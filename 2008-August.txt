From nouiz at mail.berlios.de  Fri Aug  1 20:04:12 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 1 Aug 2008 20:04:12 +0200
Subject: [Plearn-commits] r9334 - trunk/plearn_learners/generic
Message-ID: <200808011804.m71I4C33029728@sheep.berlios.de>

Author: nouiz
Date: 2008-08-01 20:04:12 +0200 (Fri, 01 Aug 2008)
New Revision: 9334

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
use the same deffinition as Denise Desjardins for the costs


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-07-31 20:52:49 UTC (rev 9333)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-08-01 18:04:12 UTC (rev 9334)
@@ -147,11 +147,8 @@
         " - 'squared_norm_reconstruction_error': | ||i||^2 - ||o||^2 |\n"
         " - 'train_time': the time spend in the train() function\n"
         " - 'type1_err': SUM[type1_err] will return the number of type 1 error(false positive).\n"
-        "                E[type1_err], will return  type1_err/NNONMISSING\n" 
-        "                you probably want false_positive_rate\n"
+        "                E[type1_err], will return the false positive rate: # false positive/# of positive\n" 
         " - 'type2_err': idem as type1_err but for the type 2 error(false negative)\n" 
-        " - 'false_positive_rate': E[false_positive_rate] return nb of false pos/nb total of neg"
-        " - 'false_negative_rate': E[false_negative_rate] return nb of false neg/nb total of pos"
         " - 'sensitivity': E[sensitivity] return nb true pos/nb total pos"
         " - 'specificity': E[specificity] return nb true neg/nb total ng"
     );
@@ -287,12 +284,6 @@
         } else if (c == "type2_err") {
             output_min = 0;
             output_max = 1;
-        } else if (c == "false_negative_rate") {
-            output_min = 0;
-            output_max = 1;
-        } else if (c == "false_positive_rate") {
-            output_min = 0;
-            output_max = 1;
         } else if (c == "sensitivity") {
             output_min = 0;
             output_max = 1;
@@ -631,57 +622,34 @@
             costs[ind_cost] = train_time;
         } else if (c == "type1_err") {
             //false positive error
+            //faux negatif/(faux negatif+vrai positif)
 #ifdef BOUNDCHECK
             PLASSERT(sub_learner_output.length()==1);
 #endif
             real target=desired_target[0];
             real out=sub_learner_output[0];
-            if( fast_is_equal(target,0) && fast_is_equal(out,1))
-                costs[ind_cost] = 1;
-            else
-                costs[ind_cost] = 0;
+            if(fast_is_equal(target,1)){
+                if (fast_is_equal(out,0))
+                    costs[ind_cost] = 1;
+                else
+                    costs[ind_cost] = 0;
+            }else
+                costs[ind_cost] = MISSING_VALUE;
         } else if (c == "type2_err") {
             //false negative error
+            //faux positif/(faux positif+ vrai negatif)
 #ifdef BOUNDCHECK
             PLASSERT(sub_learner_output.length()==1);
 #endif
             real target=desired_target[0];
             real out=sub_learner_output[0];
-            if( fast_is_equal(target,1) && fast_is_equal(out,0))
-                costs[ind_cost] = 1;
-            else
-                costs[ind_cost] = 0;
-        } else if (c == "false_positive_rate") {
-            //=1 - the specificity
-            //nb of false pos/nb total of neg
-            //should use X[test1.E[false_positive_rate]] to have the real value
-#ifdef BOUNDCHECK
-            PLASSERT(sub_learner_output.length()==1);
-#endif
-            real target=desired_target[0];
-            real out=sub_learner_output[0];
-
-            if( fast_is_equal(target,0) && fast_is_equal(out,1))
-                costs[ind_cost] = 1;                
-            else if( fast_is_equal(target, 0))
-                costs[ind_cost] = 0;
-            else
+            if(fast_is_equal(target,0)){
+                if(fast_is_equal(out,1))
+                    costs[ind_cost] = 1;
+                else
+                    costs[ind_cost] = 0;
+            }else
                 costs[ind_cost] = MISSING_VALUE;
-        } else if (c == "false_negative_rate") {
-            //nb of false neg/nb total of pos
-            //should use X[test1.E[false_nagative_rate]] to have the real value
-#ifdef BOUNDCHECK
-            PLASSERT(sub_learner_output.length()==1);
-#endif
-            real target=desired_target[0];
-            real out=sub_learner_output[0];
-
-             if( fast_is_equal(target,1) && fast_is_equal(out,0))
-                costs[ind_cost] = 1;                
-            else if( fast_is_equal(target, 1))
-                costs[ind_cost] = 0;
-            else
-                costs[ind_cost] = MISSING_VALUE;
         } else if (c == "sensitivity") {
             //nb true pos/(nb true pos + nb false neg)
             //equiv to=nb true pos/nb total pos
@@ -796,8 +764,10 @@
 
     }
     Profiler::end("AddCostToLearner::train");
-    const Profiler::Stats& stats = Profiler::getStats("AddCostToLearner::train");
-    train_time=stats.wall_duration/Profiler::ticksPerSecond();
+    if(Profiler::isActive()){
+        const Profiler::Stats& stats = Profiler::getStats("AddCostToLearner::train");
+        train_time=stats.wall_duration/Profiler::ticksPerSecond();
+    }
 }
 
 ///////////////////////////



From nouiz at mail.berlios.de  Fri Aug  1 20:06:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 1 Aug 2008 20:06:30 +0200
Subject: [Plearn-commits] r9335 - trunk/plearn_learners/generic
Message-ID: <200808011806.m71I6UnH029832@sheep.berlios.de>

Author: nouiz
Date: 2008-08-01 20:06:29 +0200 (Fri, 01 Aug 2008)
New Revision: 9335

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
In AddCostToLearner::computeOutputAndCosts, we use the possibli optimized version from the sub learner.


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-08-01 18:04:12 UTC (rev 9334)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-08-01 18:06:29 UTC (rev 9335)
@@ -775,7 +775,13 @@
 ///////////////////////////
 void AddCostToLearner::computeOutputAndCosts(const Vec& input, const Vec& target,
                                              Vec& output, Vec& costs) const {
-    PLearner::computeOutputAndCosts(input, target, output, costs);
+    PLASSERT( learner_ );
+    //done this way to use a possibly optimizer version 
+    //of computeOutputAndCosts from the sub learner as with NatGradNNet
+
+    Vec sub_costs = costs.subVec(0, learner_->nTestCosts());
+    learner_->computeOutputAndCosts(input, target, output, sub_costs);
+    computeCostsFromOutputs(input,output,target,costs,false);
 }
 
 ///////////////////////////



From nouiz at mail.berlios.de  Fri Aug  1 21:22:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 1 Aug 2008 21:22:52 +0200
Subject: [Plearn-commits] r9336 - trunk/plearn/vmat
Message-ID: <200808011922.m71JMqm8000874@sheep.berlios.de>

Author: nouiz
Date: 2008-08-01 21:22:52 +0200 (Fri, 01 Aug 2008)
New Revision: 9336

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
-added check when reloading the precomputed data matrix
-try to unlock the dir if we receive an exception. will clean partially writed file.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-08-01 18:06:29 UTC (rev 9335)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-08-01 19:22:52 UTC (rev 9336)
@@ -345,14 +345,27 @@
   
   PPath train_metadata = train_set->getMetaDataDir();
   PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
+
+  bool uptodate = train_set->isUpToDate(mean_median_mode_file_name,false)
+    && source->isUpToDate(mean_median_mode_file_name,false);
+
   train_set->lockMetaDataDir();
-  if (!train_set->isUpToDate(mean_median_mode_file_name,false)
-      ||!source->isUpToDate(mean_median_mode_file_name,false))
-    {
-      computeMeanMedianModeVectors();
-      createMeanMedianModeFile(mean_median_mode_file_name);
-    }
-  else loadMeanMedianModeFile(mean_median_mode_file_name);
+  try{
+    if (!uptodate)
+      {
+	computeMeanMedianModeVectors();
+	createMeanMedianModeFile(mean_median_mode_file_name);
+      }
+    else loadMeanMedianModeFile(mean_median_mode_file_name);
+  }catch(const PLearnError& e){
+    train_set->unlockMetaDataDir();
+
+    //we erase the file if we are creating it
+    // as it can be partilly saved.
+    if(!uptodate && isfile(mean_median_mode_file_name))
+      rm(mean_median_mode_file_name);
+    throw e;
+  }
   train_set->unlockMetaDataDir();
 }
 
@@ -369,6 +382,8 @@
     train_set->isUpToDate(file_name,true,true);
 
     mean_median_mode_file = new FileVMatrix(file_name);
+    compatibleSizeError(mean_median_mode_file);
+    PLCHECK(mean_median_mode_file->fieldNames()==fieldNames());
     mean_median_mode_file->getRow(0, variable_mean);
     mean_median_mode_file->getRow(1, variable_median);
     mean_median_mode_file->getRow(2, variable_mode);



From nouiz at mail.berlios.de  Fri Aug  1 21:41:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 1 Aug 2008 21:41:11 +0200
Subject: [Plearn-commits] r9337 - trunk/plearn/vmat
Message-ID: <200808011941.m71JfBc3004964@sheep.berlios.de>

Author: nouiz
Date: 2008-08-01 21:41:10 +0200 (Fri, 01 Aug 2008)
New Revision: 9337

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
setMetaInfoFrom this to have the comparaison work correctly.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-08-01 19:22:52 UTC (rev 9336)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-08-01 19:41:10 UTC (rev 9337)
@@ -372,6 +372,7 @@
 void MeanMedianModeImputationVMatrix::createMeanMedianModeFile(PPath file_name)
 {
     mean_median_mode_file = new FileVMatrix(file_name, 3, train_field_names);
+    mean_median_mode_file->setMetaInfoFrom(this);
     mean_median_mode_file->putRow(0, variable_mean);
     mean_median_mode_file->putRow(1, variable_median);
     mean_median_mode_file->putRow(2, variable_mode);



From nouiz at mail.berlios.de  Mon Aug  4 21:25:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Aug 2008 21:25:16 +0200
Subject: [Plearn-commits] r9338 - trunk/plearn/vmat
Message-ID: <200808041925.m74JPGnY026473@sheep.berlios.de>

Author: nouiz
Date: 2008-08-04 21:25:15 +0200 (Mon, 04 Aug 2008)
New Revision: 9338

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
bugfix in cygwin. The fgets function don't behave correctly with unix end of lines. I put the fix in comment as I'm not sure it is save in all case. But I think it is.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-01 19:41:10 UTC (rev 9337)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-04 19:25:15 UTC (rev 9338)
@@ -147,6 +147,33 @@
             int pos = int(pos_long);
             if(!fgets(buf, sizeof(buf), f))
                 break;
+
+#ifdef 0 //__CYGWIN__
+            // Bugfix for CYGWIN carriage return bug.
+            long new_pos = ftell(f);
+            long lbuf = long(strlen(buf));
+            if (lbuf+pos != new_pos)
+                if(lbuf+1+pos==new_pos && buf[lbuf-1]=='\n' && buf[lbuf-2]!='\r')
+                {
+                    //bug under windows. fgets return the good string if unix end of lines, but
+                    //change the position suppossing the use of \r\n as carrige return.
+                    //So if their is only a \n, we are a caractere too far.
+                    //if dos end of lines, return \n as end of lines in the strings and put the pos correctly.
+                    
+                    fseek(f,-1,SEEK_CUR);
+                    
+		    //if unix end of lines
+                    if(fgetc(f)!='\n')
+                    	fseek(f,-1,SEEK_CUR);
+                }
+                //in the eof case?
+                else if(lbuf-1+pos==new_pos && buf[lbuf-1]=='\n' && buf[lbuf-2]!='\r')
+                    fseek(f,+1,SEEK_CUR);
+                else
+                    PLERROR("In TextFilesVMatrix::buildId - The number of characters read "
+                            "does not match the position in the file.");
+#endif
+
             buf[sizeof(buf)-1] = '\0';         // ensure null-terminated
             lineno++;
             if(nskip>0)



From nouiz at mail.berlios.de  Mon Aug  4 21:35:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Aug 2008 21:35:18 +0200
Subject: [Plearn-commits] r9339 - in trunk: . plearn/vmat
Message-ID: <200808041935.m74JZIB6027800@sheep.berlios.de>

Author: nouiz
Date: 2008-08-04 21:35:17 +0200 (Mon, 04 Aug 2008)
New Revision: 9339

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/pymake.config.model
Log:
Added two options in pymake -march=native and -cygwin_fgets_bugfix


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-04 19:25:15 UTC (rev 9338)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-04 19:35:17 UTC (rev 9339)
@@ -148,8 +148,9 @@
             if(!fgets(buf, sizeof(buf), f))
                 break;
 
-#ifdef 0 //__CYGWIN__
+#ifdef CYGWIN_FGETS_BUGFIX
             // Bugfix for CYGWIN carriage return bug.
+            // Should be safe to enable in all case, but need to be tester more widely.
             long new_pos = ftell(f);
             long lbuf = long(strlen(buf));
             if (lbuf+pos != new_pos)

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-08-04 19:25:15 UTC (rev 9338)
+++ trunk/pymake.config.model	2008-08-04 19:35:17 UTC (rev 9339)
@@ -275,7 +275,8 @@
 
   [ '', 'nolock'],
   [ '', 'Wno-uninitialized' ],
-
+  [ '', 'march=native'],
+  [ '', 'cygwin_fgets_bugfix'],
 ]
 
 ### Using Python code snippets in C++ code
@@ -929,10 +930,17 @@
               description = 'USE WITH CARE: disable vmatrix lock',
               cpp_definitions = ['DISABLE_VMATRIX_LOCK'])
 
+pymakeOption( name = 'march=native',
+              description = 'add the compiler option -march=native',
+              compileroptions = '-march=native')
 
-cpp_variables += ['DISABLE_VMATRIX_LOCK']
+pymakeOption( name = 'cygwin_fgets_bugfix',
+              description = 'add the cpp definition CYGWIN_FGETS_BUGFIX',
+              cpp_definitions = ['CYGWIN_FGETS_BUGFIX'])
 
+cpp_variables += ['DISABLE_VMATRIX_LOCK', 'CYGWIN_FGETS_BUGFIX']
 
+
 #####  Network Setup  #######################################################
 
 # nprocesses_on_localhost = 1



From nouiz at mail.berlios.de  Mon Aug  4 21:36:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Aug 2008 21:36:54 +0200
Subject: [Plearn-commits] r9340 - trunk
Message-ID: <200808041936.m74JasLQ028022@sheep.berlios.de>

Author: nouiz
Date: 2008-08-04 21:36:53 +0200 (Mon, 04 Aug 2008)
New Revision: 9340

Modified:
   trunk/pymake.config.model
Log:
renamed option to follow the convention.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-08-04 19:35:17 UTC (rev 9339)
+++ trunk/pymake.config.model	2008-08-04 19:36:53 UTC (rev 9340)
@@ -276,7 +276,7 @@
   [ '', 'nolock'],
   [ '', 'Wno-uninitialized' ],
   [ '', 'march=native'],
-  [ '', 'cygwin_fgets_bugfix'],
+  [ '', 'cygwin-fgets-bugfix'],
 ]
 
 ### Using Python code snippets in C++ code
@@ -934,7 +934,7 @@
               description = 'add the compiler option -march=native',
               compileroptions = '-march=native')
 
-pymakeOption( name = 'cygwin_fgets_bugfix',
+pymakeOption( name = 'cygwin-fgets-bugfix',
               description = 'add the cpp definition CYGWIN_FGETS_BUGFIX',
               cpp_definitions = ['CYGWIN_FGETS_BUGFIX'])
 



From nouiz at mail.berlios.de  Mon Aug  4 23:27:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Aug 2008 23:27:28 +0200
Subject: [Plearn-commits] r9341 - trunk/plearn/vmat
Message-ID: <200808042127.m74LRSew005415@sheep.berlios.de>

Author: nouiz
Date: 2008-08-04 23:27:27 +0200 (Mon, 04 Aug 2008)
New Revision: 9341

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
Log:
put some variable private


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-08-04 19:36:53 UTC (rev 9340)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-08-04 21:27:27 UTC (rev 9341)
@@ -73,19 +73,6 @@
   //! Pairs of instruction of the form field_name : mean | median | mode | none.
   TVec< pair<string, string> >  imputation_spec;
   
-  //! The vector of variable means observed from the train set.
-  Vec                           variable_mean;
-  
-  //! The vector of variable medians observed from the train set.
-  Vec                           variable_median;
-  
-  //! The vector of variable modes observed from the train set.
-  Vec                           variable_mode;
-  
-  //! The vector of coded instruction for each variables.
-  TVec<int>                     variable_imputation_instruction;
-  
-
                         MeanMedianModeImputationVMatrix();
   virtual               ~MeanMedianModeImputationVMatrix();
 
@@ -105,6 +92,19 @@
   TVec<string>         train_field_names;
   VMat                 mean_median_mode_file;
 
+
+  //! The vector of variable means observed from the train set.
+  Vec                           variable_mean;
+  
+  //! The vector of variable medians observed from the train set.
+  Vec                           variable_median;
+  
+  //! The vector of variable modes observed from the train set.
+  Vec                           variable_mode;
+  
+  //! The vector of coded instruction for each variables.
+  TVec<int>                     variable_imputation_instruction;
+  
           void         build_();
   virtual void setMetaDataDir(const PPath& the_metadatadir);
           void         createMeanMedianModeFile(PPath file_name); 



From nouiz at mail.berlios.de  Mon Aug  4 23:28:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Aug 2008 23:28:24 +0200
Subject: [Plearn-commits] r9342 - trunk/plearn/vmat
Message-ID: <200808042128.m74LSO5T005526@sheep.berlios.de>

Author: nouiz
Date: 2008-08-04 23:28:23 +0200 (Mon, 04 Aug 2008)
New Revision: 9342

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
the option mtime can take the value -1 to give it the value unknow


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-04 21:27:27 UTC (rev 9341)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-04 21:28:23 UTC (rev 9342)
@@ -158,7 +158,8 @@
         ol, "mtime", &VMatrix::mtime_update, 
         OptionBase::buildoption|OptionBase::nosave,
         "DO NOT play with this if you don't know the implementation!\n"
-        "This add a dependency mtime to the gived value.");
+        "This add a dependency mtime to the gived value.\n"
+        " -1 or "+tostring(numeric_limits<time_t>::max())+" set permannetly that we don't know the mtime.");
 
     declareOption(
         ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::buildoption,
@@ -532,7 +533,9 @@
 {
     if(!metadatadir.isEmpty())
         setMetaDataDir(metadatadir); // make sure we perform all necessary operations
-    if(mtime_update!=0)
+    if(mtime_update == time_t(-1))
+        updateMtime(0);
+    else if(mtime_update!=0)
         updateMtime(mtime_update);
 }
 



From larocheh at mail.berlios.de  Tue Aug  5 15:31:28 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 5 Aug 2008 15:31:28 +0200
Subject: [Plearn-commits] r9343 - trunk/plearn_learners_experimental
Message-ID: <200808051331.m75DVS1k005005@sheep.berlios.de>

Author: larocheh
Date: 2008-08-05 15:31:28 +0200 (Tue, 05 Aug 2008)
New Revision: 9343

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Corrected some bugs...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-04 21:28:23 UTC (rev 9342)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-05 13:31:28 UTC (rev 9343)
@@ -354,7 +354,7 @@
         if( n_classes <= 1 )
             PLERROR("In PseudolikelihoodRBM::build_layers_and_connections(): "
                     "n_classes should be > 1");
-        if( target_layer->size != n_classes )
+        if( !target_layer || target_layer->size != n_classes )
         {
             target_layer = new RBMMultinomialLayer();
             target_layer->size = n_classes;
@@ -363,7 +363,8 @@
             target_layer->forget();
         }
         
-        if( target_connection->up_size != hidden_layer->size ||
+        if( !target_connection || 
+            target_connection->up_size != hidden_layer->size ||
             target_connection->down_size != target_layer->size )
         {
             target_connection = new RBMMatrixConnection(); 
@@ -376,7 +377,7 @@
     }
     else if ( targetsize() > 1 )
     {
-        if( target_layer->size != targetsize() )
+        if( !target_layer || target_layer->size != targetsize() )
         {
             target_layer = new RBMBinomialLayer();
             target_layer->size = targetsize();
@@ -385,7 +386,8 @@
             target_layer->forget();
         }
         
-        if( target_connection->up_size != hidden_layer->size ||
+        if( !target_connection || 
+            target_connection->up_size != hidden_layer->size ||
             target_connection->down_size != target_layer->size )
         {
             target_connection = new RBMMatrixConnection(); 
@@ -716,6 +718,7 @@
                 hidden_activation_gradient += hidden_activation_pos_i_gradient;
 
                 // Update target connections
+                w = &(target_connection->weights(0,i));
                 for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
                     *w -= learning_rate * hidden_activation_pos_i_gradient[j];
             }
@@ -743,13 +746,17 @@
             PLERROR("NNNNNNNNNNOOOOOOOOOOOOOOOOOOOOOO!!!!!!!!!!!!!!");
         }
 
-        if( !fast_is_equal(learning_rate, 0.) )
+        if( !fast_is_equal(learning_rate, 0.) &&
+            (targetsize() == 0 || generative_learning_weight > 0) )
         {
             if( decrease_ct != 0 )
                 lr = learning_rate / (1.0 + stage * decrease_ct );
             else
                 lr = learning_rate;
 
+            if( targetsize() > 0 )
+                lr *= generative_learning_weight;
+
             setLearningRate(lr);
 
             if( pseudolikelihood_context_size == 0 )
@@ -864,9 +871,6 @@
                 externalProductAcc( connection_gradient, hidden_activation_gradient,
                                     input );
 
-                if( targetsize() > 0 )
-                    lr *= generative_learning_weight;
-
                 // Hidden bias update
                 multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
                                   hidden_layer->bias);
@@ -1343,8 +1347,6 @@
 
                 externalProductAcc( connection_gradient, hidden_activation_gradient,
                                     input );
-                if( targetsize() > 0 )
-                    lr *= generative_learning_weight;
 
                 // Hidden bias update
                 multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
@@ -1375,7 +1377,8 @@
         }
             
         // CD learning
-        if( !fast_is_equal(cd_learning_rate, 0.) )
+        if( !fast_is_equal(cd_learning_rate, 0.) &&
+            (targetsize() == 0 || generative_learning_weight > 0) )
         {
 
             if( !fast_is_equal(persistent_cd_weight, 1.) )
@@ -1598,7 +1601,8 @@
             }
         }
         
-        if( !fast_is_equal(denoising_learning_rate, 0.) )
+        if( !fast_is_equal(denoising_learning_rate, 0.) &&
+            (targetsize() == 0 || generative_learning_weight > 0) )
         {
             if( denoising_decrease_ct != 0 )
                 lr = denoising_learning_rate / 
@@ -1719,11 +1723,41 @@
 void PseudolikelihoodRBM::computeOutput(const Vec& input, Vec& output) const
 {
     // Compute the output from the input.
-    if( n_classes > 1 )
+    if( targetsize() == 1 )
     {
         // Get output probabilities
-        PLERROR("n_classes > 1 not implemented yet");
+        connection->setAsDownInput( input );
+        hidden_layer->getAllActivations( 
+            (RBMMatrixConnection*) connection );
+        
+        Vec target_act = target_layer->activation;
+        Vec hidden_act = hidden_layer->activation;
+        for( int i=0 ; i<target_layer->size ; i++ )
+        {
+            target_act[i] = target_layer->bias[i];
+            // LATERAL CONNECTIONS CODE HERE!!
+            real *w = &(target_connection->weights(0,i));
+            // step from one row to the next in weights matrix
+            int m = target_connection->weights.mod();                
+            
+            for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+            {
+                // *w = weights(j,i)
+                hidden_activation_pos_i[j] = hidden_act[j] + *w;
+            }
+            target_act[i] -= hidden_layer->freeEnergyContribution(
+                hidden_activation_pos_i);
+        }
+        
+        target_layer->expectation_is_up_to_date = false;
+        target_layer->computeExpectation();
+        output << target_layer->expectation;
     }
+    else if(targetsize() > 1 )
+    {
+        PLERROR("In PseudolikelihoodRBM::computeOutput(): not implemented yet for\n"
+                "targetsize() > 1");
+    }
     else
     {
         // Get hidden layer representation
@@ -1745,12 +1779,17 @@
     costs.resize( cost_names.length() );
     costs.fill( MISSING_VALUE );
 
-    if( n_classes > 1 )
+    if( targetsize() == 1 )
     {
         costs[class_cost_index] =
             (argmax(output) == (int) round(target[0]))? 0 : 1;
         costs[nll_cost_index] = -pl_log(output[(int) round(target[0])]);
     }
+    else if( targetsize() > 1 )
+    {
+        PLERROR("In PseudolikelihoodRBM::computeCostsFromOutputs(): not implemented yet for\n"
+                "targetsize() > 1");
+    }
     else
     {        
         if( compute_input_space_nll )



From larocheh at mail.berlios.de  Tue Aug  5 15:33:38 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 5 Aug 2008 15:33:38 +0200
Subject: [Plearn-commits] r9344 - trunk/plearn_learners/online
Message-ID: <200808051333.m75DXcGn005193@sheep.berlios.de>

Author: larocheh
Date: 2008-08-05 15:33:37 +0200 (Tue, 05 Aug 2008)
New Revision: 9344

Modified:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
Log:
Corrected a bug in freeEnergyContributionGradient...


Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-08-05 13:31:28 UTC (rev 9343)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-08-05 13:33:37 UTC (rev 9344)
@@ -1068,27 +1068,27 @@
             leaf_activation = unit_activations[offset+n];
             // Add free energy of tree with activated leaf
             if( n == 0)
-                tree_free_energy = -tree_energy + leaf_activation;
+                tree_free_energy = tree_energy - leaf_activation;
             else
-                tree_free_energy = logadd( -tree_energy + leaf_activation, 
-                                           tree_free_energy );
+                tree_free_energy = -logadd( -tree_energy + leaf_activation, 
+                                            -tree_free_energy );
             tree_energies[offset+t+n] = tree_energy - leaf_activation;
 
             // Add free_energy of tree with inactivated leaf
             if( use_signed_samples )
             {
-                tree_free_energy = logadd( -tree_energy - leaf_activation, 
-                                           tree_free_energy );
+                tree_free_energy = -logadd( -tree_energy - leaf_activation, 
+                                            -tree_free_energy );
                 tree_energies[offset+t+n+1] = tree_energy + leaf_activation;
             }
             else
             {
-                tree_free_energy = logadd( -tree_energy, tree_free_energy );
+                tree_free_energy = -logadd( -tree_energy, -tree_free_energy );
                 tree_energies[offset+t+n+1] = tree_energy;
             }
         }
-        tree_free_energies[t] = -tree_free_energy;
-        result -= tree_free_energy;
+        tree_free_energies[t] = tree_free_energy;
+        result += tree_free_energy;
         offset += n_nodes_per_tree;
     }
     return result;
@@ -1114,6 +1114,12 @@
     real tree_energy_gradient = 0;
     real tree_energy_leaf_on_gradient = 0;
     real tree_energy_leaf_off_gradient = 0;
+
+    // Fills in the internal variables tree_energies and tree_free_energies.
+    // I have to do this because I can't assume the last time freeEnergyContribution was
+    // called was with the same unit_activations...
+    freeEnergyContribution(unit_activations);
+
     for( int t = 0; t<n_trees; t++ )
     {
         for( int n = 0; n < n_nodes_per_tree; n = n+2 ) // Looking only at leaves



From nouiz at mail.berlios.de  Tue Aug  5 17:37:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Aug 2008 17:37:06 +0200
Subject: [Plearn-commits] r9345 - trunk/plearn/vmat
Message-ID: <200808051537.m75Fb6rR019000@sheep.berlios.de>

Author: nouiz
Date: 2008-08-05 17:37:05 +0200 (Tue, 05 Aug 2008)
New Revision: 9345

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
corrected some messages


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-05 13:33:37 UTC (rev 9344)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-05 15:37:05 UTC (rev 9345)
@@ -370,17 +370,18 @@
 void TextFilesVMatrix::build_()
 {
     if (metadatapath != "") {
-        PLWARNING("In TextFilesVMatrix::build_: metadatapath option is deprecated. "
+        PLWARNING("In TextFilesVMatrix::build_() metadatapath option is deprecated. "
                   "You should use metadatadir instead.\n");
 
         metadatadir = metadatapath;
         setMetaDataDir(metadatapath);
     }
     if (!default_spec.empty() && !reorder_fieldspec_from_headers)
-        PLERROR("In TextFilesVMatrix::build_: when the option default_spec is used, reorder_fieldspec_from_headers must be true");
-    
+        PLERROR("In TextFilesVMatrix::build_() when the option default_spec is used, reorder_fieldspec_from_headers must be true");
+    if(getMetaDataDir().empty())
+        PLERROR("In TextFilesVMatrix::build_() We need a metadatadir");
     if(!force_mkdir(getMetaDataDir()))
-        PLERROR("In TextFilesVMatrix::build_: could not create directory '%s'",
+        PLERROR("In TextFilesVMatrix::build_() could not create directory '%s'",
                 getMetaDataDir().absolute().c_str());
     
     for(int i=0;i<txtfilenames.length();i++)
@@ -616,7 +617,7 @@
     string rowi = getTextRow(i);
     TVec<string> fields =  splitIntoFields(rowi);
     if(fields.size() != fieldspec.size())
-        PLERROR("In TextFilesVMatrix::getMapping - In getting fields of row %d, wrong number of fields: %d (should be %d):\n%s\n",i,fields.size(),fieldspec.size(),rowi.c_str());
+        PLERROR("In TextFilesVMatrix::getTextFields - In getting fields of row %d, wrong number of fields: %d (should be %d):\n%s\n",i,fields.size(),fieldspec.size(),rowi.c_str());
     for(int k=0; k<fields.size(); k++)
         fields[k] = removeblanks(fields[k]);
     return fields;



From nouiz at mail.berlios.de  Tue Aug  5 17:42:38 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Aug 2008 17:42:38 +0200
Subject: [Plearn-commits] r9346 - trunk/plearn/vmat
Message-ID: <200808051542.m75Fgc1G019323@sheep.berlios.de>

Author: nouiz
Date: 2008-08-05 17:42:37 +0200 (Tue, 05 Aug 2008)
New Revision: 9346

Modified:
   trunk/plearn/vmat/FilteredVMatrix.cc
Log:
if their is an exception while we have a lock, we do some clean up.


Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2008-08-05 15:37:05 UTC (rev 9345)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2008-08-05 15:42:37 UTC (rev 9346)
@@ -130,21 +130,29 @@
                 "directory %s", getMetaDataDir().absolute().c_str());
 
     lockMetaDataDir();
-    if(isUpToDate(idxfname))
-        indexes.open(idxfname.absolute());
-    else  // let's (re)create the index
-    {
-        computeFilteredIndices();
-        rm(idxfname);       // force remove it
-        indexes.open(idxfname.absolute(), true);
-        for (int i = 0; i < mem_indices.length(); i++)
-            indexes.append(mem_indices[i]);
-        indexes.close();
-        indexes.open(idxfname.absolute());
-        mem_indices = TVec<int>();  // Free memory.
+    try{
+        if(isUpToDate(idxfname))
+            indexes.open(idxfname.absolute());
+        else  // let's (re)create the index
+        {
+            computeFilteredIndices();
+            rm(idxfname);       // force remove it
+            indexes.open(idxfname.absolute(), true);
+            for (int i = 0; i < mem_indices.length(); i++)
+                indexes.append(mem_indices[i]);
+            indexes.close();
+            indexes.open(idxfname.absolute());
+            mem_indices = TVec<int>();  // Free memory.
+        }
+    }catch(const PLearnError& e){
+        unlockMetaDataDir();
+        //we erase the file if we are creating it
+        // as it can be partilly saved.
+        if(!isUpToDate(idxfname) && isfile(idxfname))
+            rm(idxfname);
+        throw e;
     }
     unlockMetaDataDir();
-
     length_ = indexes.length();
 }
 



From larocheh at mail.berlios.de  Tue Aug  5 18:40:53 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 5 Aug 2008 18:40:53 +0200
Subject: [Plearn-commits] r9347 - trunk/plearn_learners_experimental
Message-ID: <200808051640.m75Ger3X003140@sheep.berlios.de>

Author: larocheh
Date: 2008-08-05 18:40:50 +0200 (Tue, 05 Aug 2008)
New Revision: 9347

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Changed input_layer to RBMLayer, so that it can be used for other layers than RBMBinomialLayer. Though CD will work, pseudolikelihood will not for other layers than RBMBinomialLayer.


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-05 15:42:37 UTC (rev 9346)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-05 16:40:50 UTC (rev 9347)
@@ -95,7 +95,10 @@
 {
     declareOption(ol, "learning_rate", &PseudolikelihoodRBM::learning_rate,
                   OptionBase::buildoption,
-                  "The learning rate used for pseudolikelihood training.\n");
+                  "The learning rate used for pseudolikelihood training.\n"
+                  "Pseudolikelihood training assumes input_layer is a\n"
+                  "RBMBinomialLayer. It will work even if it isn't,\n"
+                  "but training won't be appropriate.\n");
 
     declareOption(ol, "decrease_ct", &PseudolikelihoodRBM::decrease_ct,
                   OptionBase::buildoption,

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-08-05 15:42:37 UTC (rev 9346)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-08-05 16:40:50 UTC (rev 9347)
@@ -135,7 +135,7 @@
     real generative_learning_weight;
 
     //! The binomial input layer of the RBM
-    PP<RBMBinomialLayer> input_layer;
+    PP<RBMLayer> input_layer;
 
     //! The hidden layer of the RBM
     PP<RBMLayer> hidden_layer;



From saintmlx at mail.berlios.de  Tue Aug  5 20:00:37 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 5 Aug 2008 20:00:37 +0200
Subject: [Plearn-commits] r9348 - trunk/plearn/vmat
Message-ID: <200808051800.m75I0bZ1024131@sheep.berlios.de>

Author: saintmlx
Date: 2008-08-05 20:00:36 +0200 (Tue, 05 Aug 2008)
New Revision: 9348

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- avoid force_mkdir if dir exists



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-05 16:40:50 UTC (rev 9347)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-05 18:00:36 UTC (rev 9348)
@@ -1505,7 +1505,9 @@
         return;
     PPath fname = getSFIFFilename(col,".smap");
     init_map_sr();
-    force_mkdir( getSFIFDirectory() );
+    string SFIFdir= getSFIFDirectory();
+    if(!pathexists(SFIFdir))
+        force_mkdir(SFIFdir);
     if(!isfile(fname))
         return;
 



From saintmlx at mail.berlios.de  Tue Aug  5 20:01:53 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 5 Aug 2008 20:01:53 +0200
Subject: [Plearn-commits] r9349 - in trunk: plearn/base
	python_modules/plearn/pybridge python_modules/plearn/pyext
Message-ID: <200808051801.m75I1rUo024259@sheep.berlios.de>

Author: saintmlx
Date: 2008-08-05 20:01:52 +0200 (Tue, 05 Aug 2008)
New Revision: 9349

Modified:
   trunk/plearn/base/plerror.cc
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
   trunk/python_modules/plearn/pyext/plext.cc
Log:
- minor (cosmetic) mods



Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-08-05 18:00:36 UTC (rev 9348)
+++ trunk/plearn/base/plerror.cc	2008-08-05 18:01:52 UTC (rev 9349)
@@ -64,7 +64,7 @@
     
     snprintf(message, ERROR_MSG_SIZE, "In file: \"%s\" at line %d\n", filename, linenumber);
     PLASSERT(ERROR_MSG_SIZE>=strlen(message)+strlen(msg));
-    strcat(message,msg);
+    strncat(message,msg,ERROR_MSG_SIZE);
     verrormsg(message, args);
 
     va_end(args);

Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-08-05 18:00:36 UTC (rev 9348)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-08-05 18:01:52 UTC (rev 9349)
@@ -101,9 +101,9 @@
         if attr in self._optionnames:
             return self.getOption(attr)
         else:
-            raise AttributeError, ("no attribute "
-                                   + attr + " in "
-                                   + repr(self))
+            raise AttributeError("no attribute "
+                                 + attr + " in "
+                                 + repr(self))
         
     def __del__(self):
         if hasattr(self, '_cptr'):

Modified: trunk/python_modules/plearn/pyext/plext.cc
===================================================================
--- trunk/python_modules/plearn/pyext/plext.cc	2008-08-05 18:00:36 UTC (rev 9348)
+++ trunk/python_modules/plearn/pyext/plext.cc	2008-08-05 18:01:52 UTC (rev 9349)
@@ -33,7 +33,6 @@
 
 #include <plearn/python/PythonExtension.h>
 #include <commands/plearn_full_inc.h>
-//#include <commands/myplearn_light_inc.h>
 #include <commands/PLearnCommands/plearn_main.h>
 
 using namespace PLearn;



From larocheh at mail.berlios.de  Wed Aug  6 16:43:54 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 6 Aug 2008 16:43:54 +0200
Subject: [Plearn-commits] r9350 - trunk/plearn_learners/online
Message-ID: <200808061443.m76EhsQW016499@sheep.berlios.de>

Author: larocheh
Date: 2008-08-06 16:43:53 +0200 (Wed, 06 Aug 2008)
New Revision: 9350

Modified:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
Changed computations in freeEnergyContributionGradient to log-scale to avoid numeric precision errors...


Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-08-05 18:01:52 UTC (rev 9349)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-08-06 14:43:53 UTC (rev 9350)
@@ -1120,6 +1120,16 @@
     // called was with the same unit_activations...
     freeEnergyContribution(unit_activations);
 
+    unit_activations_neg_gradient.resize(size);
+    unit_activations_neg_gradient_init.resize(size);
+    unit_activations_neg_gradient_init.fill(false);
+    if( use_signed_samples )
+    {
+        unit_activations_pos_gradient.resize(size);
+        unit_activations_pos_gradient_init.resize(size);
+        unit_activations_pos_gradient_init.fill(false);
+    }
+
     for( int t = 0; t<n_trees; t++ )
     {
         for( int n = 0; n < n_nodes_per_tree; n = n+2 ) // Looking only at leaves
@@ -1127,19 +1137,27 @@
             // Computation energy of tree
             tree_energy = 0;
             sub_tree_size = n_nodes_per_tree / 2;
-            sub_root = sub_tree_size;            
-            tree_energy_leaf_on_gradient = output_gradient * 
-                safeexp(-tree_energies[offset+t+n] + tree_free_energies[t]);
-            tree_energy_leaf_off_gradient = output_gradient * 
-                safeexp(-tree_energies[offset+t+n+1] + tree_free_energies[t]);
-            tree_energy_gradient = tree_energy_leaf_on_gradient + 
-                tree_energy_leaf_off_gradient;
+            sub_root = sub_tree_size;
+            // First do things on log-scale
+            tree_energy_leaf_on_gradient = -tree_energies[offset+t+n] + tree_free_energies[t];
+            tree_energy_leaf_off_gradient = -tree_energies[offset+t+n+1] + tree_free_energies[t];
+            tree_energy_gradient = logadd(tree_energy_leaf_on_gradient,
+                                          tree_energy_leaf_off_gradient);
             for( int d=0; d<tree_depth-1; d++ )
             {
                 if( n < sub_root )
                 {
-                    unit_activations_gradient[offset+sub_root] -= 
-                        tree_energy_gradient;
+                    if( unit_activations_neg_gradient_init[offset+sub_root] )
+                        unit_activations_neg_gradient[offset+sub_root] = 
+                            logadd(tree_energy_gradient,
+                                   unit_activations_neg_gradient[offset+sub_root]);
+                    else
+                    {
+                        unit_activations_neg_gradient[offset+sub_root] = 
+                            tree_energy_gradient;
+                        unit_activations_neg_gradient_init[offset+sub_root] = true;
+                    }
+                        
                     sub_tree_size /= 2;
                     sub_root -= sub_tree_size + 1;
                 }
@@ -1147,21 +1165,44 @@
                 {
                     if( use_signed_samples )
                     {
-                        unit_activations_gradient[offset+sub_root] += 
-                            tree_energy_gradient;
+                        if( unit_activations_pos_gradient_init[offset+sub_root] )
+                            unit_activations_pos_gradient[offset+sub_root] = 
+                                logadd(tree_energy_gradient,
+                                       unit_activations_pos_gradient[offset+sub_root]);
+                        else
+                        {
+                            unit_activations_pos_gradient[offset+sub_root] = 
+                                tree_energy_gradient;
+                            unit_activations_pos_gradient_init[offset+sub_root] = true;
+                        }
                     }
                     sub_tree_size /= 2;
                     sub_root += sub_tree_size+1;
                 }
             }
             
-            unit_activations_gradient[offset+n] -= tree_energy_leaf_on_gradient;
+            unit_activations_neg_gradient[offset+n] = 
+                tree_energy_leaf_on_gradient;
+            unit_activations_neg_gradient_init[offset+n] = true;
 
             if( use_signed_samples )
-                unit_activations_gradient[offset+n] += tree_energy_leaf_off_gradient;
+            {
+                unit_activations_pos_gradient[offset+n] = 
+                    tree_energy_leaf_off_gradient;
+                unit_activations_pos_gradient_init[offset+n] = true;
+            }
         }
         offset += n_nodes_per_tree;
     }
+
+    // Go back to linear-scale
+    for(int i=0; i<size; i++)
+        unit_activations_gradient[i] -= output_gradient * safeexp( unit_activations_neg_gradient[i] );
+
+    if( use_signed_samples )
+        for(int i=0; i<size; i++)
+            unit_activations_gradient[i] += output_gradient * 
+                safeexp( unit_activations_pos_gradient[i] );
 }
 
 int RBMWoodsLayer::getConfigurationCount()

Modified: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-08-05 18:01:52 UTC (rev 9349)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-08-06 14:43:53 UTC (rev 9350)
@@ -179,7 +179,11 @@
     // Temporary computations, for freeEnergyContribution() and its gradient variant
     mutable Vec tree_free_energies;
     mutable Vec tree_energies;
-
+    mutable Vec unit_activations_pos_gradient;
+    mutable Vec unit_activations_neg_gradient;
+    mutable TVec<bool> unit_activations_pos_gradient_init;
+    mutable TVec<bool> unit_activations_neg_gradient_init;
+    
 protected:
     //#####  Protected Member Functions  ######################################
 



From larocheh at mail.berlios.de  Wed Aug  6 16:44:26 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 6 Aug 2008 16:44:26 +0200
Subject: [Plearn-commits] r9351 - trunk/plearn_learners_experimental
Message-ID: <200808061444.m76EiQQG016705@sheep.berlios.de>

Author: larocheh
Date: 2008-08-06 16:44:26 +0200 (Wed, 06 Aug 2008)
New Revision: 9351

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Added some (currently commented out) code to do gradient verification...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-06 14:43:53 UTC (rev 9350)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-06 14:44:26 UTC (rev 9351)
@@ -667,6 +667,76 @@
 
         if( targetsize() == 1 )
         {
+
+            // For gradient verification
+            //Mat estimated_gradient(connection->up_size, connection->down_size);
+            //{
+            //    connection->setAsDownInput( input );
+            //    hidden_layer->getAllActivations( 
+            //        (RBMMatrixConnection*) connection );
+            //    
+            //    Vec target_act = target_layer->activation;
+            //    Vec hidden_act = hidden_layer->activation;
+            //    for( int i=0 ; i<target_layer->size ; i++ )
+            //    {
+            //        target_act[i] = target_layer->bias[i];
+            //        // LATERAL CONNECTIONS CODE HERE!!
+            //        real *w = &(target_connection->weights(0,i));
+            //        // step from one row to the next in weights matrix
+            //        int m = target_connection->weights.mod();                
+            //        
+            //        for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+            //        {
+            //            // *w = weights(j,i)
+            //            hidden_activation_pos_i[j] = hidden_act[j] + *w;
+            //        }
+            //        target_act[i] -= hidden_layer->freeEnergyContribution(
+            //            hidden_activation_pos_i);
+            //    }
+            //    
+            //    target_layer->expectation_is_up_to_date = false;
+            //    target_layer->computeExpectation();
+            //    real true_nll = target_layer->fpropNLL(target_one_hot);
+            //    
+            //    estimated_gradient.fill(true_nll);
+            //    
+            //    real epsilon = 1e-5;
+            //    for( int i1=0; i1<connection->up_size; i1++)
+            //        for( int j1=0; j1<connection->down_size; j1++)
+            //        {
+            //            connection->weights(i1,j1) += epsilon;
+            //            connection->setAsDownInput( input );
+            //            hidden_layer->getAllActivations( 
+            //                (RBMMatrixConnection*) connection );
+            //            
+            //            Vec target_act = target_layer->activation;
+            //            Vec hidden_act = hidden_layer->activation;
+            //            for( int i=0 ; i<target_layer->size ; i++ )
+            //            {
+            //                target_act[i] = target_layer->bias[i];
+            //                // LATERAL CONNECTIONS CODE HERE!!
+            //                real *w = &(target_connection->weights(0,i));
+            //                // step from one row to the next in weights matrix
+            //                int m = target_connection->weights.mod();                
+            //                
+            //                for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+            //                {
+            //                    // *w = weights(j,i)
+            //                    hidden_activation_pos_i[j] = hidden_act[j] + *w;
+            //                }
+            //                target_act[i] -= hidden_layer->freeEnergyContribution(
+            //                    hidden_activation_pos_i);
+            //            }
+            //            
+            //            target_layer->expectation_is_up_to_date = false;
+            //            target_layer->computeExpectation();
+            //            real nll = target_layer->fpropNLL(target_one_hot);
+            //            
+            //            estimated_gradient(i1,j1) = (nll - estimated_gradient(i1,j1) )/epsilon;
+            //            connection->weights(i1,j1) -= epsilon;
+            //        }
+            //}
+
             // Multi-class classification
 
             connection->setAsDownInput( input );
@@ -729,6 +799,11 @@
             externalProduct( connection_gradient, hidden_activation_gradient,
                              input );
 
+            //real cos_ang = dot(connection_gradient.toVec(),estimated_gradient.toVec())
+            //    / (norm(connection_gradient.toVec()) *norm(estimated_gradient.toVec()));
+            //cout << "cos_ang=" << cos_ang << endl;
+            //cout << "ang=" << acos(cos_ang) << endl;
+
             // Update target bias            
             multiplyScaledAdd(class_gradient, 1.0, -lr,
                               target_layer->bias);



From nouiz at mail.berlios.de  Thu Aug  7 18:55:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Aug 2008 18:55:06 +0200
Subject: [Plearn-commits] r9352 - trunk/plearn/base
Message-ID: <200808071655.m77Gt6Vf021352@sheep.berlios.de>

Author: nouiz
Date: 2008-08-07 18:55:03 +0200 (Thu, 07 Aug 2008)
New Revision: 9352

Modified:
   trunk/plearn/base/Object.cc
Log:
-Print a better error msg
-change the order that the option are saved for better readability


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2008-08-06 14:44:26 UTC (rev 9351)
+++ trunk/plearn/base/Object.cc	2008-08-07 16:55:03 UTC (rev 9352)
@@ -287,6 +287,9 @@
     }
 
     // There are bigger problems in the world but still it isn't always funny
+    if(optionname.empty())
+        PLERROR("There is no option named \"%s\" in a \"%s\". Meaby we forget an ')'?",
+                optionname.c_str(),classname().c_str());
     PLERROR("There is no option named \"%s\" in a \"%s\"",
             optionname.c_str(),classname().c_str());
 }
@@ -576,6 +579,17 @@
         getOptionsToRemoteTransmit():
         getOptionsToSave());
 
+    //we swap the VMatrix option at the end for better lisibility of the file
+    for(size_t i =0; i<optnames.size() - 1 ;i++)
+    {
+        if("source"==optnames[i] || "vm"==optnames[i])
+        {
+            string tmp = optnames[i];
+            optnames[i] = optnames.back();
+            optnames.back() = tmp;
+        }
+    }
+
     out.write(classname());
     out.write("(\n");
     for (size_t i = 0; i < optnames.size(); ++i) 



From nouiz at mail.berlios.de  Thu Aug  7 19:01:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Aug 2008 19:01:21 +0200
Subject: [Plearn-commits] r9353 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200808071701.m77H1Lhn027891@sheep.berlios.de>

Author: nouiz
Date: 2008-08-07 19:01:13 +0200 (Thu, 07 Aug 2008)
New Revision: 9353

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added an option 'e--tasks_filename={compact,explicit,nb0,nb1}' that change the filename where the stdin,stderr are redirected.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-08-07 16:55:03 UTC (rev 9352)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-08-07 17:01:13 UTC (rev 9353)
@@ -687,6 +687,7 @@
         self.redirect_stderr_to_stdout = False
         self.env = ''
         self.os = ''
+        self.base_tasks_log_file = []
 
         DBIBase.__init__(self, commands, **args)
         self.mem=int(self.mem)*1024
@@ -885,9 +886,19 @@
             for i in condor_datas:
                 condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
         else:
-            for task in self.tasks:
-                argstring = condor_escape_argument(' ; '.join(task.commands))
-                condor_dat.write("arguments      = %s \nqueue\n" % argstring)
+            if self.base_tasks_log_file:
+                for (task,task_log) in zip(self.tasks,self.base_tasks_log_file):
+                    argstring =condor_escape_argument(' ; '.join(task.commands))
+                    stdout_file=self.log_dir+"/condor."+task_log+".out"
+                    stderr_file=self.log_dir+"/condor."+task_log+".err"
+
+                    condor_dat.write("arguments    = %s \n" %argstring)
+                    condor_dat.write("output       = %s \n" %stdout_file)
+                    condor_dat.write("error        = %s \nqueue\n" %stderr_file)
+            else:
+                for task in self.tasks:
+                    argstring =condor_escape_argument(' ; '.join(task.commands))
+                    condor_dat.write("arguments      = %s \nqueue\n" %argstring)
         condor_dat.close()
 
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-08-07 16:55:03 UTC (rev 9352)
+++ trunk/scripts/dbidispatch	2008-08-07 17:01:13 UTC (rev 9353)
@@ -10,8 +10,14 @@
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
     all except condor options:[--[*no_]nb_proc=N]
     cluster, condor options  : [--32|--64|--3264] [--os=X] [--mem=N]
-    condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice] [--[*no_]getenv] [*--[no_]prefserver] [--rank=RANK_EXPRESSION] [--files=file1[,file2...]] [--env=VAR=VALUE[;VAR2=VALUE2]][--raw=CONDOR_EXPRESSION]
-    cluster option           : [*--[no_]cwait]  [--[*no_]force] [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
+    condor option            : [--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
+                               [--[*no_]getenv] [*--[no_]prefserver] 
+                               [--rank=RANK_EXPRESSION] 
+                               [--files=file1[,file2...]]
+                               [--env=VAR=VALUE[;VAR2=VALUE2]]
+                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,nb0,nb1}]
+    cluster option           : [*--[no_]cwait]  [--[*no_]force]
+                               [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
 An * after '[' signals the default value.
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
@@ -88,6 +94,12 @@
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
   If the CONDOR_HOME environment variable is set, then the HOME variable will
      be set to this value for jobs submitted to condor.
+  The '--tasks_filename={compact,explicit,nb0,nb1}' option will change the filename where the stdout,stderr are redirected. They have this patter condor.X.{out,error} where X=:
+      - default : same as nb0
+      - compact : will be a unic string with parameter that change of value between jobs
+      - explicit: will be a unic string that represent the full command to execute
+      - nb0     : a number from 0 to nb job -1.
+      - nb1     : a number from 1 to nb job.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -132,6 +144,7 @@
 FILE = ""
 dbi_param={}
 testmode=False
+tasks_filename=""
 
 PATH=os.getenv('PATH')
 if search_file('condor_submit',PATH):
@@ -180,6 +193,13 @@
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
+    elif argv.startswith("--tasks_filename="):
+        part = argv.split('=',1)
+        accepted_value=["compact","explicit","nb0","nb1"]
+        if part[1] not in accepted_value:
+            print "The option '"+argv+"' have an invalid value. possible value are:", accepted_value
+            sys.exit(2)
+        tasks_filename = part[1]
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice"]:
         dbi_param[argv[2:]]=True
@@ -285,8 +305,7 @@
             if res1 == []:
                 res.append(y)
             else:
-                for r in res1:
-                    res.append(y+" "+r)
+                res.extend([y+" "+r for r in res1])
         return res
 
 def generate_commands(sp):
@@ -305,19 +324,24 @@
         else:
             repl.append([arg])
     argscombination = generate_combination(repl)
-    return argscombination
+    args_modif = generate_combination([x for x in repl if len(x)>1])
 
+    return (argscombination,args_modif)
+
 #generate the command
 if FILE != "":
     FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
     commands=[]
+    choise_args = []
     for line in FD.readlines():
         line = line.rstrip()
         sp = line.split(" ")
-        commands+=generate_commands(sp)
+        (t1,t2)=generate_commands(sp)
+        commands+=t1
+        choise_args+=t2
     FD.close
 else:
-    commands=generate_commands(command_argv)
+    (commands,choise_args)=generate_commands(command_argv)
 
 if FILE == "":
     t = [x for x in sys.argv[1:] if not x[:2]=="--"]
@@ -330,6 +354,19 @@
     tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
     dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
+    if tasks_filename == "explicit":
+        dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands]
+    elif tasks_filename == "compact":
+        dbi_param["base_tasks_log_file"]=[re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in choise_args]
+    elif tasks_filename == "nb0":
+        dbi_param["base_tasks_log_file"]=map(str,range(len(commands)))
+    elif tasks_filename == "nb1":
+        dbi_param["base_tasks_log_file"]=map(str,range(1,len(commands)+1))
+    elif tasks_filename == "":
+        pass
+    else:
+        print "internal ERROR!"
+        sys.exit(2)
 else:
     dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')



From nouiz at mail.berlios.de  Thu Aug  7 19:17:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Aug 2008 19:17:58 +0200
Subject: [Plearn-commits] r9354 - trunk/scripts
Message-ID: <200808071717.m77HHwQf012883@sheep.berlios.de>

Author: nouiz
Date: 2008-08-07 19:17:57 +0200 (Thu, 07 Aug 2008)
New Revision: 9354

Modified:
   trunk/scripts/dbidispatch
Log:
corrected comment


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-08-07 17:01:13 UTC (rev 9353)
+++ trunk/scripts/dbidispatch	2008-08-07 17:17:57 UTC (rev 9354)
@@ -94,7 +94,7 @@
   The '--raw=STRING1[\nSTRING2...]' option add all the STRINGX parameter to the submit file of condor.
   If the CONDOR_HOME environment variable is set, then the HOME variable will
      be set to this value for jobs submitted to condor.
-  The '--tasks_filename={compact,explicit,nb0,nb1}' option will change the filename where the stdout,stderr are redirected. They have this patter condor.X.{out,error} where X=:
+  The '--tasks_filename={compact,explicit,nb0,nb1}' option will change the filename where the stdout, stderr are redirected. They have this pattern condor.X.{out,error} where X=:
       - default : same as nb0
       - compact : will be a unic string with parameter that change of value between jobs
       - explicit: will be a unic string that represent the full command to execute



From nouiz at mail.berlios.de  Thu Aug  7 21:52:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Aug 2008 21:52:18 +0200
Subject: [Plearn-commits] r9355 - trunk/speedtest
Message-ID: <200808071952.m77JqI1K010233@sheep.berlios.de>

Author: nouiz
Date: 2008-08-07 21:52:17 +0200 (Thu, 07 Aug 2008)
New Revision: 9355

Modified:
   trunk/speedtest/speedtest
Log:
old modif...


Modified: trunk/speedtest/speedtest
===================================================================
--- trunk/speedtest/speedtest	2008-08-07 17:17:57 UTC (rev 9354)
+++ trunk/speedtest/speedtest	2008-08-07 19:52:17 UTC (rev 9355)
@@ -1,7 +1,7 @@
 #!/bin/bash
 #nb experience executed=size(PROGEXT)*size(SIZE)
 BP=$1 #BASEPROG
-if [ "x$SIZE" == "x" -o "x$TITLE" == "x" -o "x$RESFILE" == "x" -o "x$APPENDRESULT" == "x" ]; then
+if [ "x$SIZE" == "x" -o "x$TITLE" == "x" -o "x$RESFILE" == "x" ]; then
     echo "You should not call '$0' directly! See speedtest.general on how to call this script!"
     exit
 fi
@@ -20,7 +20,7 @@
 else
     for p in "${@}"
       do
-      PROG=( "${PROG[@]}" "${BP}${p/\ /_}" )
+      PROG=( "${PROG[@]}" "${BP}${p//\ /_}" )
     done
 fi
 
@@ -52,10 +52,8 @@
 touch ${LOGFILE}
 touch $PLOTFILE
 
-if [ ! $APPENDRESULT == "YES" ]; then
-    echo -n ""  > ${DATAFILE}
-    echo -n ""  > ${LOGFILE}
-fi
+echo -n ""  > ${DATAFILE}
+echo -n ""  > ${LOGFILE}
 
 echo "BP: $BP"
 for p in "${PROG[@]}"
@@ -64,7 +62,6 @@
 done
 echo "SIZE: ${SIZE[@]}"
 echo "RESFILE: $RESFILE"
-echo "APPENDRESULT: $APPENDRESULT"
 echo -n "#SIZE " >> ${DATAFILE}
 for p in "${PROG[@]}"
   do
@@ -79,7 +76,7 @@
     do
     date
     echo -n "" > ${DATAFILE}.tmp
-    echo -n "Executing ${p} with size of ${i} " >> ${LOGFILE}
+    echo  "Executing ${p} with size of ${i} " >> ${LOGFILE}
     echo -n "Executing ${p} with size of ${i} ..."
     f "/usr/bin/time -o${DATAFILE}.tmp -a -f %e" "$p" "$i" >> ${LOGFILE} 2>>${LOGFILE}
     read time < ${DATAFILE}.tmp



From louradou at mail.berlios.de  Fri Aug  8 16:10:23 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 8 Aug 2008 16:10:23 +0200
Subject: [Plearn-commits] r9356 - trunk/plearn/vmat
Message-ID: <200808081410.m78EANVq015037@sheep.berlios.de>

Author: louradou
Date: 2008-08-08 16:10:23 +0200 (Fri, 08 Aug 2008)
New Revision: 9356

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
   trunk/plearn/vmat/ReplicateSamplesVMatrix.h
Log:
added an option bag_index to specify which index of the target
correspond to the bag information



Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-08-07 19:52:17 UTC (rev 9355)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-08-08 14:10:23 UTC (rev 9356)
@@ -61,6 +61,7 @@
 /////////////////////////////
 ReplicateSamplesVMatrix::ReplicateSamplesVMatrix():
     operate_on_bags(false),
+    bag_index(-1),
     seed(1827),
     random_gen(new PRandom())
 {}
@@ -85,6 +86,12 @@
         "account so as to preserve their integrity. The classes will also be\n"
         "reweighted so that they have the same number of bags.");
 
+    declareOption(ol, "bag_index",
+                  &ReplicateSamplesVMatrix::bag_index,
+                  OptionBase::buildoption,
+        "Index of the target corresponding to the bag information (useful\n"
+        "only when operate_on_bags is True). -1 means the last element.\n");
+
     declareOption(ol, "seed", &ReplicateSamplesVMatrix::seed,
                   OptionBase::buildoption,
         "Seed for the random number generator (to shuffle data).");
@@ -124,6 +131,10 @@
     updateMtime(indices_vmat);
     updateMtime(source);
 
+    if (bag_index < 0)
+        bag_index = source->targetsize()-1;
+    PLASSERT(bag_index < source->targetsize());
+
     // Build the vector of indices.
     indices.resize(0);
     Vec input, target;
@@ -139,7 +150,8 @@
             for (int j = 0; j < n_to_add; j++)
                 class_indices.append(TVec<int>());
         }
-        if (!operate_on_bags || int(round(target.lastElement())) &
+        
+        if (!operate_on_bags || int(round(target[bag_index])) &
                                 SumOverBagsVariable::TARGET_COLUMN_FIRST) {
             class_indices[c].append(i);
             indices.append(i);

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.h
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.h	2008-08-07 19:52:17 UTC (rev 9355)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.h	2008-08-08 14:10:23 UTC (rev 9356)
@@ -63,6 +63,7 @@
     //#####  Public Build Options  ############################################
 
     bool operate_on_bags;
+    int bag_index;
     int32_t seed;
 
 public:



From larocheh at mail.berlios.de  Fri Aug  8 16:35:41 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 8 Aug 2008 16:35:41 +0200
Subject: [Plearn-commits] r9357 - trunk/plearn_learners_experimental
Message-ID: <200808081435.m78EZfs0016641@sheep.berlios.de>

Author: larocheh
Date: 2008-08-08 16:35:40 +0200 (Fri, 08 Aug 2008)
New Revision: 9357

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Starting to implement pseudolikelihood for sparse inputs...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-08 14:10:23 UTC (rev 9356)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-08 14:35:40 UTC (rev 9357)
@@ -70,6 +70,10 @@
     fraction_of_masked_inputs( 0. ),
     only_reconstruct_masked_inputs( false ),
     n_classes( -1 ),
+    input_is_sparse( false ),
+    factorized_connection_rank( 10 ),
+    n_selected_inputs_pseudolikelihood( 1. ),
+    select_among_k_most_frequent( -1 ),
     compute_input_space_nll( false ),
     pseudolikelihood_context_size ( 0 ),
     pseudolikelihood_context_type( "uniform_random" ),
@@ -162,6 +166,30 @@
                   "If < 2, unsupervised learning will be performed.\n"
                   );
 
+    declareOption(ol, "input_is_sparse", &PseudolikelihoodRBM::input_is_sparse,
+                  OptionBase::buildoption,
+                  "Indication that the input is in a sparse format. Input is also assumed\n"
+                  "to be binary.\n"
+                  );
+
+    declareOption(ol, "factorized_connection_rank", &PseudolikelihoodRBM::factorized_connection_rank,
+                  OptionBase::buildoption,
+                  "Rank of factorized connection for sparse inputs.\n"
+                  );    
+
+    declareOption(ol, "n_selected_inputs_pseudolikelihood", 
+                  &PseudolikelihoodRBM::n_selected_inputs_pseudolikelihood,
+                  OptionBase::buildoption,
+                  "Number of randomly selected inputs for pseudolikelihood cost.\n"
+                  );    
+
+    declareOption(ol, "select_among_k_most_frequent", 
+                  &PseudolikelihoodRBM::select_among_k_most_frequent,
+                  OptionBase::buildoption,
+                  "Indication that inputs for pseudolikelihood cost are selected among the\n"
+                  "k most frequently active inputs.\n"
+                  );    
+
     declareOption(ol, "compute_input_space_nll", 
                   &PseudolikelihoodRBM::compute_input_space_nll,
                   OptionBase::buildoption,

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-08-08 14:10:23 UTC (rev 9356)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-08-08 14:35:40 UTC (rev 9357)
@@ -108,6 +108,20 @@
     //! Number of classes in the training set (for supervised learning)
     int n_classes;
     
+    //! Indication that the input is in a sparse format. Input is also assumed
+    //! to be binary
+    bool input_is_sparse;
+
+    //! Rank of factorized connection for sparse inputs
+    int factorized_connection_rank;
+
+    //! Number of randomly selected inputs for pseudolikelihood cost
+    real n_selected_inputs_pseudolikelihood;
+
+    //! Indication that inputs for pseudolikelihood cost are selected among the
+    //! k most frequently active inputs
+    int select_among_k_most_frequent;
+
     //! Indication that the input space NLL should be computed
     //! during test
     bool compute_input_space_nll;



From nouiz at mail.berlios.de  Fri Aug  8 16:56:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Aug 2008 16:56:37 +0200
Subject: [Plearn-commits] r9358 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200808081456.m78Eub3n019702@sheep.berlios.de>

Author: nouiz
Date: 2008-08-08 16:56:36 +0200 (Fri, 08 Aug 2008)
New Revision: 9358

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-added the option --set_special_env that will set the {OMP,GOTO,MKL}_NUM_THREADS env variable value to the number of cores the jobs is allocated. Default to true.
-print more explicit why the command din't worked.
-corrected the doc and code to use --no_dbilog instead of --nodbilog. The doc was contradicting and use the the first one to be consistent with other option. We accept both version.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-08-08 14:35:40 UTC (rev 9357)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-08-08 14:56:36 UTC (rev 9358)
@@ -688,6 +688,7 @@
         self.env = ''
         self.os = ''
         self.base_tasks_log_file = []
+        self.set_special_env = True
 
         DBIBase.__init__(self, commands, **args)
         self.mem=int(self.mem)*1024
@@ -803,6 +804,12 @@
     def run_all_job(self):
         if len(self.tasks)==0:
             return #no task to run
+
+        #set special environment variable
+        if self.set_special_env:
+            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) "'
+
+
         # create the bqsubmit.dat, with
         condor_datas = []
 
@@ -932,6 +939,7 @@
                     launch_dat.write('export HOME=%s\n' % condor_home)
                 if source_file:
                     launch_dat.write('source ' + source_file + '\n')
+
                 launch_dat.write(dedent('''\
                     echo "Executing on " `/bin/hostname` 1>&2
                     echo "HOSTNAME: ${HOSTNAME}" 1>&2

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-08-08 14:35:40 UTC (rev 9357)
+++ trunk/scripts/dbidispatch	2008-08-08 14:56:36 UTC (rev 9358)
@@ -15,10 +15,11 @@
                                [--rank=RANK_EXPRESSION] 
                                [--files=file1[,file2...]]
                                [--env=VAR=VALUE[;VAR2=VALUE2]]
-                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,nb0,nb1}]
+                               [--raw=CONDOR_EXPRESSION] [--tasks_filename={compact,explicit,*nb0,nb1}]
+                               [*--[no_]set_special_env]
     cluster option           : [*--[no_]cwait]  [--[*no_]force]
                                [--[*no_]interruptible] [--cpu=nb_cpu_per_node]
-An * after '[' signals the default value.
+An * after '[', '{' or ',' signals the default value.
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
@@ -27,7 +28,7 @@
 common options:
   The -h, --help print the long help(this)
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
-  The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
+  The --dbilog (--no_dbilog) tells dbi to generate (or not) an additional log
   The '--[no_]test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
   The '--testdbi' set only dbi in test mode. Not dbidispatch
   The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
@@ -100,6 +101,7 @@
       - explicit: will be a unic string that represent the full command to execute
       - nb0     : a number from 0 to nb job -1.
       - nb1     : a number from 1 to nb job.
+  The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -161,6 +163,7 @@
 if not os.path.exists(LOGDIR):
     os.mkdir(LOGDIR)
 
+
 to_parse=[]
 env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")
 if env:
@@ -172,7 +175,8 @@
     if argv == "--help" or argv == "-h":
         print LongHelp
         sys.exit(0)
-    elif argv == "--nodbilog":
+    #--nodbilog should be allowed do to bug in old version that requested it with _.
+    elif argv == "--no_dbilog" or argv == "--nodbilog":
         dbi_param["dolog"]=False
     elif argv == "--dbilog":
         dbi_param["dolog"]=True
@@ -201,8 +205,13 @@
             sys.exit(2)
         tasks_filename = part[1]
     elif argv in  ["--force", "--interruptible", "--long", 
-                   "--getenv", "--cwait", "--clean_up" ,"--nice"]:
+                   "--getenv", "--cwait", "--clean_up" ,"--nice",
+                   "--set_special_env"]:
         dbi_param[argv[2:]]=True
+    elif argv in ["--no_force", "--no_interruptible", "--no_long",
+                  "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
+                  "--no_set_special_env"]:
+        dbi_param[argv[5:]]=False
     elif argv=="--testdbi":
         dbi_param["test"]=True
     elif argv=="--no_testdbi":
@@ -213,9 +222,6 @@
     elif argv=="--no_test":
         dbi_param[argv[2:]]=True
         testmode=False
-    elif argv in ["--no_force", "--no_interruptible", "--no_long",
-                  "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice"]:
-        dbi_param[argv[5:]]=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env"]:
         param=argv.split('=')[0][2:]
@@ -228,7 +234,7 @@
             dbi_param[param]+='\n'+argv.split('=',1)[1]
         elif param=="env":
             dbi_param.setdefault(param,"")
-            dbi_param[param]+=";"+argv.split('=',1)[1]
+            dbi_param[param]+='"'+argv.split('=',1)[1]+'"'
         else:
             #otherwise we erase the old value
             dbi_param[param]=argv.split('=',1)[1]
@@ -259,6 +265,8 @@
     command_argv.remove(argv)
 
 if len(command_argv) == 0 and FILE == "":
+    print "No command or file with command to execute!"
+    print
     print ShortHelp
     sys.exit(1)
 
@@ -270,7 +278,7 @@
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
-                       "raw", "os"]
+                       "raw", "os", "set_special_env"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long","nb_proc","duree"]
 elif launch_cmd=="Local":



From nouiz at mail.berlios.de  Fri Aug  8 19:19:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Aug 2008 19:19:55 +0200
Subject: [Plearn-commits] r9359 - trunk/plearn/vmat
Message-ID: <200808081719.m78HJtSg023405@sheep.berlios.de>

Author: nouiz
Date: 2008-08-08 19:19:53 +0200 (Fri, 08 Aug 2008)
New Revision: 9359

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
typo


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-08 14:56:36 UTC (rev 9358)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-08 17:19:53 UTC (rev 9359)
@@ -104,7 +104,7 @@
 
 void TextFilesVMatrix::buildIdx()
 {
-    cerr << "Building the index file. PLease be patient..." << endl;
+    cerr << "Building the index file. Please be patient..." << endl;
 
     if(idxfile)
         fclose(idxfile);



From nouiz at mail.berlios.de  Fri Aug  8 19:53:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Aug 2008 19:53:48 +0200
Subject: [Plearn-commits] r9360 - trunk/plearn/vmat
Message-ID: <200808081753.m78Hrmnw027621@sheep.berlios.de>

Author: nouiz
Date: 2008-08-08 19:53:47 +0200 (Fri, 08 Aug 2008)
New Revision: 9360

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
Add a PLERROR earlier in the code that will fait later. This PLERROR tell a better message. i.e. when we don't have the good delimiter, we will read only 1 columns event if more are expected.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-08 17:19:53 UTC (rev 9359)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-08 17:53:47 UTC (rev 9360)
@@ -223,22 +223,31 @@
     width_ = 0;
     TVec<string> fnames;
     TVec<string> fnames_header;//field names take in the header of source file
-    if(reorder_fieldspec_from_headers || partial_match)
-    {
-        //read the fieldnames from the files.
-        for(int i=0; i<txtfiles.size(); i++)
-        {
-            FILE* f = txtfiles[i];
-            fseek(f,0,SEEK_SET);
-            if(!fgets(buf, sizeof(buf), f))
-                PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() - "
-                        "Couldn't read the fields names from file '%s'",
-                        txtfilenames[i].c_str());
-            fseek(f,0,SEEK_SET);
+    
+    //read the fieldnames from the files.
+    for(int i=0; i<txtfiles.size(); i++){
+        FILE* f = txtfiles[i];
+        fseek(f,0,SEEK_SET);
+        if(!fgets(buf, sizeof(buf), f))
+            PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() - "
+                    "Couldn't read the fields names from file '%s'",
+                    txtfilenames[i].c_str());
+        fseek(f,0,SEEK_SET);
 
-            TVec<string> fields = splitIntoFields(buf);
+        TVec<string> fields = splitIntoFields(buf);
+
+        //check that we have the good delimiter
+        if(fields.size()==1 && fieldspec.size()>1)
+            PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth() -"
+                    " We found only 1 column in the first line, but"
+                    " their is %d fieldspec. Meaby the delimiter '%s'"
+                    " is not the right one. The line is %s",
+                    fieldspec.size(),delimiter.c_str(),
+                    string(buf).c_str());
+        
+        if(reorder_fieldspec_from_headers || partial_match){
             fields.append(removeblanks(fields.pop()));
-
+            
             fnames_header.append(fields);
         }
     }



From tihocan at mail.berlios.de  Thu Aug 14 15:52:40 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 14 Aug 2008 15:52:40 +0200
Subject: [Plearn-commits] r9370 - trunk/plearn/math
Message-ID: <200808141352.m7EDqeaP008777@sheep.berlios.de>

Author: tihocan
Date: 2008-08-14 15:52:38 +0200 (Thu, 14 Aug 2008)
New Revision: 9370

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
Added a check for emptiness when computing the median of a Vec

Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-08-14 11:51:26 UTC (rev 9369)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-08-14 13:52:38 UTC (rev 9370)
@@ -2488,12 +2488,15 @@
 inline T kthOrderedElement(const TVec<T>& vec, int k)
 { return vec[positionOfkthOrderedElement(vec,k)]; }
 
-//!  returns the median value of vec
+//! Return the median value of vector.
 template<class T>
 inline T median(const TVec<T>& vec)
-{ return kthOrderedElement(vec, (vec.length()-1)/2); }
+{
+    if (vec.isEmpty())
+        PLERROR("In median - Cannot compute median of an empty vector");
+    return kthOrderedElement(vec, (vec.length()-1)/2);
+}
 
-
 //-------------- These were previouslty methods of TVec ----------------------------------
 
 



From larocheh at mail.berlios.de  Fri Aug 15 15:02:40 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 15 Aug 2008 15:02:40 +0200
Subject: [Plearn-commits] r9372 - trunk/plearn_learners_experimental
Message-ID: <200808151302.m7FD2ekZ016847@sheep.berlios.de>

Author: larocheh
Date: 2008-08-15 15:02:39 +0200 (Fri, 15 Aug 2008)
New Revision: 9372

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
- Corrected bug with sparse pseudolikelihood (forgot to set input_i)
- Added some (now commented) code to verify the gradient of pseudolikelihood


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-14 20:41:39 UTC (rev 9371)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-15 13:02:39 UTC (rev 9372)
@@ -1081,6 +1081,7 @@
                 //      p_i = exp(num_pos) / (exp(num_pos) + exp(num_neg))
 
                 Vec hidden_act = hidden_layer->activation;
+
                 real num_pos_act;
                 real num_neg_act;
                 real num_pos;
@@ -1142,6 +1143,812 @@
                             hidden_layer->size );
                 }
 
+                //Mat estimated_gradient;
+                //Mat U_estimated_gradient;
+                //{
+                //    real epsilon=1e-5;
+                //    // Empirically estimate gradient
+                //    if( input_is_sparse )
+                //    {
+                //        estimated_gradient.resize(V.length(), V.width());
+                //        U_estimated_gradient.resize(U.length(), U.width() );
+                //
+                //        int i=0;
+                //        pseudolikelihood = 0;
+                //
+                //        // Compute activations
+                //        if( input_is_sparse )
+                //        {
+                //            if( factorized_connection_rank > 0 )
+                //            {
+                //                Vx.clear();
+                //                train_set->getExtra(stage%nsamples,extra);
+                //                for( int i=0; i<extra.length(); i++ )
+                //                {
+                //                    Vx += V((int)extra[i]);
+                //                    input_is_active[(int)extra[i]] = true;
+                //                }
+                //        
+                //                product(hidden_act,U,Vx);
+                //            }
+                //            else
+                //            {
+                //                hidden_act.clear();
+                //                train_set->getExtra(stage%nsamples,extra);
+                //                for( int i=0; i<extra.length(); i++ )
+                //                {
+                //                    hidden_act += V((int)extra[i]);
+                //                    input_is_active[(int)extra[i]] = true;
+                //                }
+                //            }
+                //        }
+                //        else
+                //        {
+                //            connection->setAsDownInput( input );
+                //            hidden_layer->getAllActivations( 
+                //                (RBMMatrixConnection*) connection );
+                //        }
+                //
+                //        if( targetsize() == 1 )
+                //            productAcc( hidden_layer->activation,
+                //                        target_connection->weights,
+                //                        target_one_hot );
+                //        else if( targetsize() > 1 )
+                //            productAcc( hidden_layer->activation,
+                //                        target_connection->weights,
+                //                        target );
+                //
+                //        for( int l=0; l<input_layer->size ; l++ )
+                //        {
+                //            if( n_selected_inputs_pseudolikelihood <= inputsize() &&
+                //                n_selected_inputs_pseudolikelihood > 0 )
+                //            {
+                //                if( l >= n_selected_inputs_pseudolikelihood )
+                //                    break;
+                //                i = input_indices[l];
+                //            }
+                //            else
+                //                i = l;
+                //            
+                //            num_pos_act = input_layer->bias[i];
+                //            // LATERAL CONNECTIONS CODE HERE!
+                //            num_neg_act = 0;
+                //            if( input_is_sparse )
+                //            {
+                //                hidden_activation_pos_i << hidden_act;
+                //                hidden_activation_neg_i << hidden_act;
+                //                if( factorized_connection_rank > 0 )
+                //                    if( input_is_active[i] )
+                //                    {
+                //                        input_i = 1;
+                //                        productScaleAcc( hidden_activation_neg_i,
+                //                                         U, V(i), -1.,1.);
+                //                    }
+                //                    else
+                //                    {
+                //                        input_i = 0;
+                //                        productScaleAcc( hidden_activation_pos_i,
+                //                                         U, V(i), 1.,1.);
+                //                    }
+                //                else
+                //                    if( input_is_active[i] )
+                //                    {
+                //                        input_i = 1;
+                //                        hidden_activation_neg_i -= V(i);
+                //                    }
+                //                    else
+                //                    {
+                //                        input_i = 0;
+                //                        hidden_activation_pos_i += V(i);
+                //                    }
+                //            }
+                //            else
+                //            {
+                //                w = &(connection->weights(0,i));
+                //                input_i = input[i];
+                //                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                //                {
+                //                    a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                //                    a_neg_i[j] = a[j] - *w * input_i;
+                //                }
+                //            }
+                //            num_pos_act -= hidden_layer->freeEnergyContribution(
+                //                hidden_activation_pos_i);
+                //            num_neg_act -= hidden_layer->freeEnergyContribution(
+                //                hidden_activation_neg_i);
+                //            //num_pos = safeexp(num_pos_act);
+                //            //num_neg = safeexp(num_neg_act);
+                //            //input_probs_i = num_pos / (num_pos + num_neg);
+                //            if( input_layer->use_fast_approximations )
+                //                input_probs_i = fastsigmoid(
+                //                    num_pos_act - num_neg_act);
+                //            else
+                //            {
+                //                num_pos = safeexp(num_pos_act);
+                //                num_neg = safeexp(num_neg_act);
+                //                input_probs_i = num_pos / (num_pos + num_neg);
+                //            }
+                //            if( input_layer->use_fast_approximations )
+                //                pseudolikelihood += tabulated_softplus( 
+                //                    num_pos_act - num_neg_act ) 
+                //                    - input_i * (num_pos_act - num_neg_act);
+                //            else
+                //                pseudolikelihood += softplus( 
+                //                    num_pos_act - num_neg_act ) 
+                //                    - input_i * (num_pos_act - num_neg_act);
+                //
+                //        }
+                //
+                //        estimated_gradient.fill(pseudolikelihood);
+                //
+                //        for( int i1=0; i1<estimated_gradient.length(); i1++)
+                //            for( int j1=0; j1<estimated_gradient.width(); j1++)
+                //            {
+                //                V(i1,j1) += epsilon;
+                //                pseudolikelihood = 0;
+                //
+                //                // Compute activations
+                //                if( input_is_sparse )
+                //                {
+                //                    if( factorized_connection_rank > 0 )
+                //                    {
+                //                        Vx.clear();
+                //                        train_set->getExtra(stage%nsamples,extra);
+                //                        for( int i=0; i<extra.length(); i++ )
+                //                        {
+                //                            Vx += V((int)extra[i]);
+                //                            input_is_active[(int)extra[i]] = true;
+                //                        }
+                //        
+                //                        product(hidden_act,U,Vx);
+                //                    }
+                //                    else
+                //                    {
+                //                        hidden_act.clear();
+                //                        train_set->getExtra(stage%nsamples,extra);
+                //                        for( int i=0; i<extra.length(); i++ )
+                //                        {
+                //                            hidden_act += V((int)extra[i]);
+                //                            input_is_active[(int)extra[i]] = true;
+                //                        }
+                //                    }
+                //                }
+                //                else
+                //                {
+                //                    connection->setAsDownInput( input );
+                //                    hidden_layer->getAllActivations( 
+                //                        (RBMMatrixConnection*) connection );
+                //                }
+                //
+                //                if( targetsize() == 1 )
+                //                    productAcc( hidden_layer->activation,
+                //                                target_connection->weights,
+                //                                target_one_hot );
+                //                else if( targetsize() > 1 )
+                //                    productAcc( hidden_layer->activation,
+                //                                target_connection->weights,
+                //                                target );
+                //
+                //                for( int l=0; l<input_layer->size ; l++ )
+                //                {
+                //                    if( n_selected_inputs_pseudolikelihood <= inputsize() &&
+                //                        n_selected_inputs_pseudolikelihood > 0 )
+                //                    {
+                //                        if( l >= n_selected_inputs_pseudolikelihood )
+                //                            break;
+                //                        i = input_indices[l];
+                //                    }
+                //                    else
+                //                        i = l;
+                //            
+                //                    num_pos_act = input_layer->bias[i];
+                //                    // LATERAL CONNECTIONS CODE HERE!
+                //                    num_neg_act = 0;
+                //                    if( input_is_sparse )
+                //                    {
+                //                        hidden_activation_pos_i << hidden_act;
+                //                        hidden_activation_neg_i << hidden_act;
+                //                        if( factorized_connection_rank > 0 )
+                //                            if( input_is_active[i] )
+                //                            {
+                //                                input_i = 1;
+                //                                productScaleAcc( hidden_activation_neg_i,
+                //                                                 U, V(i), -1.,1.);
+                //                            }
+                //                            else
+                //                            {
+                //                                input_i = 0;
+                //                                productScaleAcc( hidden_activation_pos_i,
+                //                                                 U, V(i), 1.,1.);
+                //                            }
+                //                        else
+                //                            if( input_is_active[i] )
+                //                            {
+                //                                input_i = 1;
+                //                                hidden_activation_neg_i -= V(i);
+                //                            }
+                //                            else
+                //                            {
+                //                                input_i = 0;
+                //                                hidden_activation_pos_i += V(i);
+                //                            }
+                //                    }
+                //                    else
+                //                    {
+                //                        w = &(connection->weights(0,i));
+                //                        input_i = input[i];
+                //                        for( int j=0; j<hidden_layer->size; j++,w+=m )
+                //                        {
+                //                            a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                //                            a_neg_i[j] = a[j] - *w * input_i;
+                //                        }
+                //                    }
+                //                    num_pos_act -= hidden_layer->freeEnergyContribution(
+                //                        hidden_activation_pos_i);
+                //                    num_neg_act -= hidden_layer->freeEnergyContribution(
+                //                        hidden_activation_neg_i);
+                //                    //num_pos = safeexp(num_pos_act);
+                //                    //num_neg = safeexp(num_neg_act);
+                //                    //input_probs_i = num_pos / (num_pos + num_neg);
+                //                    if( input_layer->use_fast_approximations )
+                //                        input_probs_i = fastsigmoid(
+                //                            num_pos_act - num_neg_act);
+                //                    else
+                //                    {
+                //                        num_pos = safeexp(num_pos_act);
+                //                        num_neg = safeexp(num_neg_act);
+                //                        input_probs_i = num_pos / (num_pos + num_neg);
+                //                    }
+                //                    if( input_layer->use_fast_approximations )
+                //                        pseudolikelihood += tabulated_softplus( 
+                //                            num_pos_act - num_neg_act ) 
+                //                            - input_i * (num_pos_act - num_neg_act);
+                //                    else
+                //                        pseudolikelihood += softplus( 
+                //                            num_pos_act - num_neg_act ) 
+                //                            - input_i * (num_pos_act - num_neg_act);
+                //
+                //                }
+                //                V(i1,j1) -= epsilon;
+                //                estimated_gradient(i1,j1) = (pseudolikelihood - estimated_gradient(i1,j1))
+                //                    / epsilon;
+                //            }
+                //
+                //        if( factorized_connection_rank > 0 )
+                //        {
+                //
+                //        pseudolikelihood = 0;
+                //
+                //        // Compute activations
+                //        if( input_is_sparse )
+                //        {
+                //            if( factorized_connection_rank > 0 )
+                //            {
+                //                Vx.clear();
+                //                train_set->getExtra(stage%nsamples,extra);
+                //                for( int i=0; i<extra.length(); i++ )
+                //                {
+                //                    Vx += V((int)extra[i]);
+                //                    input_is_active[(int)extra[i]] = true;
+                //                }
+                //        
+                //                product(hidden_act,U,Vx);
+                //            }
+                //            else
+                //            {
+                //                hidden_act.clear();
+                //                train_set->getExtra(stage%nsamples,extra);
+                //                for( int i=0; i<extra.length(); i++ )
+                //                {
+                //                    hidden_act += V((int)extra[i]);
+                //                    input_is_active[(int)extra[i]] = true;
+                //                }
+                //            }
+                //        }
+                //        else
+                //        {
+                //            connection->setAsDownInput( input );
+                //            hidden_layer->getAllActivations( 
+                //                (RBMMatrixConnection*) connection );
+                //        }
+                //
+                //        if( targetsize() == 1 )
+                //            productAcc( hidden_layer->activation,
+                //                        target_connection->weights,
+                //                        target_one_hot );
+                //        else if( targetsize() > 1 )
+                //            productAcc( hidden_layer->activation,
+                //                        target_connection->weights,
+                //                        target );
+                //
+                //        for( int l=0; l<input_layer->size ; l++ )
+                //        {
+                //            if( n_selected_inputs_pseudolikelihood <= inputsize() &&
+                //                n_selected_inputs_pseudolikelihood > 0 )
+                //            {
+                //                if( l >= n_selected_inputs_pseudolikelihood )
+                //                    break;
+                //                i = input_indices[l];
+                //            }
+                //            else
+                //                i = l;
+                //            
+                //            num_pos_act = input_layer->bias[i];
+                //            // LATERAL CONNECTIONS CODE HERE!
+                //            num_neg_act = 0;
+                //            if( input_is_sparse )
+                //            {
+                //                hidden_activation_pos_i << hidden_act;
+                //                hidden_activation_neg_i << hidden_act;
+                //                if( factorized_connection_rank > 0 )
+                //                    if( input_is_active[i] )
+                //                    {
+                //                        input_i = 1;
+                //                        productScaleAcc( hidden_activation_neg_i,
+                //                                         U, V(i), -1.,1.);
+                //                    }
+                //                    else
+                //                    {
+                //                        input_i = 0;
+                //                        productScaleAcc( hidden_activation_pos_i,
+                //                                         U, V(i), 1.,1.);
+                //                    }
+                //                else
+                //                    if( input_is_active[i] )
+                //                    {
+                //                        input_i = 1;
+                //                        hidden_activation_neg_i -= V(i);
+                //                    }
+                //                    else
+                //                    {
+                //                        input_i = 0;
+                //                        hidden_activation_pos_i += V(i);
+                //                    }
+                //            }
+                //            else
+                //            {
+                //                w = &(connection->weights(0,i));
+                //                input_i = input[i];
+                //                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                //                {
+                //                    a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                //                    a_neg_i[j] = a[j] - *w * input_i;
+                //                }
+                //            }
+                //            num_pos_act -= hidden_layer->freeEnergyContribution(
+                //                hidden_activation_pos_i);
+                //            num_neg_act -= hidden_layer->freeEnergyContribution(
+                //                hidden_activation_neg_i);
+                //            //num_pos = safeexp(num_pos_act);
+                //            //num_neg = safeexp(num_neg_act);
+                //            //input_probs_i = num_pos / (num_pos + num_neg);
+                //            if( input_layer->use_fast_approximations )
+                //                input_probs_i = fastsigmoid(
+                //                    num_pos_act - num_neg_act);
+                //            else
+                //            {
+                //                num_pos = safeexp(num_pos_act);
+                //                num_neg = safeexp(num_neg_act);
+                //                input_probs_i = num_pos / (num_pos + num_neg);
+                //            }
+                //            if( input_layer->use_fast_approximations )
+                //                pseudolikelihood += tabulated_softplus( 
+                //                    num_pos_act - num_neg_act ) 
+                //                    - input_i * (num_pos_act - num_neg_act);
+                //            else
+                //                pseudolikelihood += softplus( 
+                //                    num_pos_act - num_neg_act ) 
+                //                    - input_i * (num_pos_act - num_neg_act);
+                //
+                //        }
+                //
+                //        U_estimated_gradient.fill(pseudolikelihood);
+                //
+                //        for( int i1=0; i1<U_estimated_gradient.length(); i1++)
+                //            for( int j1=0; j1<U_estimated_gradient.width(); j1++)
+                //            {
+                //                U(i1,j1) += epsilon;
+                //                pseudolikelihood = 0;
+                //
+                //                // Compute activations
+                //                if( input_is_sparse )
+                //                {
+                //                    if( factorized_connection_rank > 0 )
+                //                    {
+                //                        Vx.clear();
+                //                        train_set->getExtra(stage%nsamples,extra);
+                //                        for( int i=0; i<extra.length(); i++ )
+                //                        {
+                //                            Vx += V((int)extra[i]);
+                //                            input_is_active[(int)extra[i]] = true;
+                //                        }
+                //        
+                //                        product(hidden_act,U,Vx);
+                //                    }
+                //                    else
+                //                    {
+                //                        hidden_act.clear();
+                //                        train_set->getExtra(stage%nsamples,extra);
+                //                        for( int i=0; i<extra.length(); i++ )
+                //                        {
+                //                            hidden_act += V((int)extra[i]);
+                //                            input_is_active[(int)extra[i]] = true;
+                //                        }
+                //                    }
+                //                }
+                //                else
+                //                {
+                //                    connection->setAsDownInput( input );
+                //                    hidden_layer->getAllActivations( 
+                //                        (RBMMatrixConnection*) connection );
+                //                }
+                //
+                //                if( targetsize() == 1 )
+                //                    productAcc( hidden_layer->activation,
+                //                                target_connection->weights,
+                //                                target_one_hot );
+                //                else if( targetsize() > 1 )
+                //                    productAcc( hidden_layer->activation,
+                //                                target_connection->weights,
+                //                                target );
+                //
+                //                for( int l=0; l<input_layer->size ; l++ )
+                //                {
+                //                    if( n_selected_inputs_pseudolikelihood <= inputsize() &&
+                //                        n_selected_inputs_pseudolikelihood > 0 )
+                //                    {
+                //                        if( l >= n_selected_inputs_pseudolikelihood )
+                //                            break;
+                //                        i = input_indices[l];
+                //                    }
+                //                    else
+                //                        i = l;
+                //            
+                //                    num_pos_act = input_layer->bias[i];
+                //                    // LATERAL CONNECTIONS CODE HERE!
+                //                    num_neg_act = 0;
+                //                    if( input_is_sparse )
+                //                    {
+                //                        hidden_activation_pos_i << hidden_act;
+                //                        hidden_activation_neg_i << hidden_act;
+                //                        if( factorized_connection_rank > 0 )
+                //                            if( input_is_active[i] )
+                //                            {
+                //                                input_i = 1;
+                //                                productScaleAcc( hidden_activation_neg_i,
+                //                                                 U, V(i), -1.,1.);
+                //                            }
+                //                            else
+                //                            {
+                //                                input_i = 0;
+                //                                productScaleAcc( hidden_activation_pos_i,
+                //                                                 U, V(i), 1.,1.);
+                //                            }
+                //                        else
+                //                            if( input_is_active[i] )
+                //                            {
+                //                                input_i = 1;
+                //                                hidden_activation_neg_i -= V(i);
+                //                            }
+                //                            else
+                //                            {
+                //                                input_i = 0;
+                //                                hidden_activation_pos_i += V(i);
+                //                            }
+                //                    }
+                //                    else
+                //                    {
+                //                        w = &(connection->weights(0,i));
+                //                        input_i = input[i];
+                //                        for( int j=0; j<hidden_layer->size; j++,w+=m )
+                //                        {
+                //                            a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                //                            a_neg_i[j] = a[j] - *w * input_i;
+                //                        }
+                //                    }
+                //                    num_pos_act -= hidden_layer->freeEnergyContribution(
+                //                        hidden_activation_pos_i);
+                //                    num_neg_act -= hidden_layer->freeEnergyContribution(
+                //                        hidden_activation_neg_i);
+                //                    //num_pos = safeexp(num_pos_act);
+                //                    //num_neg = safeexp(num_neg_act);
+                //                    //input_probs_i = num_pos / (num_pos + num_neg);
+                //                    if( input_layer->use_fast_approximations )
+                //                        input_probs_i = fastsigmoid(
+                //                            num_pos_act - num_neg_act);
+                //                    else
+                //                    {
+                //                        num_pos = safeexp(num_pos_act);
+                //                        num_neg = safeexp(num_neg_act);
+                //                        input_probs_i = num_pos / (num_pos + num_neg);
+                //                    }
+                //                    if( input_layer->use_fast_approximations )
+                //                        pseudolikelihood += tabulated_softplus( 
+                //                            num_pos_act - num_neg_act ) 
+                //                            - input_i * (num_pos_act - num_neg_act);
+                //                    else
+                //                        pseudolikelihood += softplus( 
+                //                            num_pos_act - num_neg_act ) 
+                //                            - input_i * (num_pos_act - num_neg_act);
+                //
+                //                }
+                //                U(i1,j1) -= epsilon;
+                //                U_estimated_gradient(i1,j1) = (pseudolikelihood - U_estimated_gradient(i1,j1))
+                //                    / epsilon;
+                //            }
+                //
+                //
+                //        }
+                //    }
+                //    else
+                //    {
+                //        estimated_gradient.resize(connection->up_size, connection->down_size);
+                //
+                //        int i=0;
+                //        pseudolikelihood = 0;
+                //
+                //        // Compute activations
+                //        if( input_is_sparse )
+                //        {
+                //            if( factorized_connection_rank > 0 )
+                //            {
+                //                Vx.clear();
+                //                train_set->getExtra(stage%nsamples,extra);
+                //                for( int i=0; i<extra.length(); i++ )
+                //                {
+                //                    Vx += V((int)extra[i]);
+                //                    input_is_active[(int)extra[i]] = true;
+                //                }
+                //        
+                //                product(hidden_act,U,Vx);
+                //            }
+                //            else
+                //            {
+                //                hidden_act.clear();
+                //                train_set->getExtra(stage%nsamples,extra);
+                //                for( int i=0; i<extra.length(); i++ )
+                //                {
+                //                    hidden_act += V((int)extra[i]);
+                //                    input_is_active[(int)extra[i]] = true;
+                //                }
+                //            }
+                //        }
+                //        else
+                //        {
+                //            connection->setAsDownInput( input );
+                //            hidden_layer->getAllActivations( 
+                //                (RBMMatrixConnection*) connection );
+                //        }
+                //
+                //        if( targetsize() == 1 )
+                //            productAcc( hidden_layer->activation,
+                //                        target_connection->weights,
+                //                        target_one_hot );
+                //        else if( targetsize() > 1 )
+                //            productAcc( hidden_layer->activation,
+                //                        target_connection->weights,
+                //                        target );
+                //
+                //        for( int l=0; l<input_layer->size ; l++ )
+                //        {
+                //            if( n_selected_inputs_pseudolikelihood <= inputsize() &&
+                //                n_selected_inputs_pseudolikelihood > 0 )
+                //            {
+                //                if( l >= n_selected_inputs_pseudolikelihood )
+                //                    break;
+                //                i = input_indices[l];
+                //            }
+                //            else
+                //                i = l;
+                //            
+                //            num_pos_act = input_layer->bias[i];
+                //            // LATERAL CONNECTIONS CODE HERE!
+                //            num_neg_act = 0;
+                //            if( input_is_sparse )
+                //            {
+                //                hidden_activation_pos_i << hidden_act;
+                //                hidden_activation_neg_i << hidden_act;
+                //                if( factorized_connection_rank > 0 )
+                //                    if( input_is_active[i] )
+                //                    {
+                //                        input_i = 1;
+                //                        productScaleAcc( hidden_activation_neg_i,
+                //                                         U, V(i), -1.,1.);
+                //                    }
+                //                    else
+                //                    {
+                //                        input_i = 0;
+                //                        productScaleAcc( hidden_activation_pos_i,
+                //                                         U, V(i), 1.,1.);
+                //                    }
+                //                else
+                //                    if( input_is_active[i] )
+                //                    {
+                //                        input_i = 1;
+                //                        hidden_activation_neg_i -= V(i);
+                //                    }
+                //                    else
+                //                    {
+                //                        input_i = 0;
+                //                        hidden_activation_pos_i += V(i);
+                //                    }
+                //            }
+                //            else
+                //            {
+                //                w = &(connection->weights(0,i));
+                //                input_i = input[i];
+                //                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                //                {
+                //                    a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                //                    a_neg_i[j] = a[j] - *w * input_i;
+                //                }
+                //            }
+                //            num_pos_act -= hidden_layer->freeEnergyContribution(
+                //                hidden_activation_pos_i);
+                //            num_neg_act -= hidden_layer->freeEnergyContribution(
+                //                hidden_activation_neg_i);
+                //            //num_pos = safeexp(num_pos_act);
+                //            //num_neg = safeexp(num_neg_act);
+                //            //input_probs_i = num_pos / (num_pos + num_neg);
+                //            if( input_layer->use_fast_approximations )
+                //                input_probs_i = fastsigmoid(
+                //                    num_pos_act - num_neg_act);
+                //            else
+                //            {
+                //                num_pos = safeexp(num_pos_act);
+                //                num_neg = safeexp(num_neg_act);
+                //                input_probs_i = num_pos / (num_pos + num_neg);
+                //            }
+                //            if( input_layer->use_fast_approximations )
+                //                pseudolikelihood += tabulated_softplus( 
+                //                    num_pos_act - num_neg_act ) 
+                //                    - input_i * (num_pos_act - num_neg_act);
+                //            else
+                //                pseudolikelihood += softplus( 
+                //                    num_pos_act - num_neg_act ) 
+                //                    - input_i * (num_pos_act - num_neg_act);
+                //
+                //        }
+                //
+                //        estimated_gradient.fill(pseudolikelihood);
+                //
+                //        for( int i1=0; i1<estimated_gradient.length(); i1++)
+                //            for( int j1=0; j1<estimated_gradient.width(); j1++)
+                //            {
+                //                connection->weights(i1,j1) += epsilon;
+                //                pseudolikelihood = 0;
+                //
+                //                // Compute activations
+                //                if( input_is_sparse )
+                //                {
+                //                    if( factorized_connection_rank > 0 )
+                //                    {
+                //                        Vx.clear();
+                //                        train_set->getExtra(stage%nsamples,extra);
+                //                        for( int i=0; i<extra.length(); i++ )
+                //                        {
+                //                            Vx += V((int)extra[i]);
+                //                            input_is_active[(int)extra[i]] = true;
+                //                        }
+                //        
+                //                        product(hidden_act,U,Vx);
+                //                    }
+                //                    else
+                //                    {
+                //                        hidden_act.clear();
+                //                        train_set->getExtra(stage%nsamples,extra);
+                //                        for( int i=0; i<extra.length(); i++ )
+                //                        {
+                //                            hidden_act += V((int)extra[i]);
+                //                            input_is_active[(int)extra[i]] = true;
+                //                        }
+                //                    }
+                //                }
+                //                else
+                //                {
+                //                    connection->setAsDownInput( input );
+                //                    hidden_layer->getAllActivations( 
+                //                        (RBMMatrixConnection*) connection );
+                //                }
+                //
+                //                if( targetsize() == 1 )
+                //                    productAcc( hidden_layer->activation,
+                //                                target_connection->weights,
+                //                                target_one_hot );
+                //                else if( targetsize() > 1 )
+                //                    productAcc( hidden_layer->activation,
+                //                                target_connection->weights,
+                //                                target );
+                //
+                //                for( int l=0; l<input_layer->size ; l++ )
+                //                {
+                //                    if( n_selected_inputs_pseudolikelihood <= inputsize() &&
+                //                        n_selected_inputs_pseudolikelihood > 0 )
+                //                    {
+                //                        if( l >= n_selected_inputs_pseudolikelihood )
+                //                            break;
+                //                        i = input_indices[l];
+                //                    }
+                //                    else
+                //                        i = l;
+                //            
+                //                    num_pos_act = input_layer->bias[i];
+                //                    // LATERAL CONNECTIONS CODE HERE!
+                //                    num_neg_act = 0;
+                //                    if( input_is_sparse )
+                //                    {
+                //                        hidden_activation_pos_i << hidden_act;
+                //                        hidden_activation_neg_i << hidden_act;
+                //                        if( factorized_connection_rank > 0 )
+                //                            if( input_is_active[i] )
+                //                            {
+                //                                input_i = 1;
+                //                                productScaleAcc( hidden_activation_neg_i,
+                //                                                 U, V(i), -1.,1.);
+                //                            }
+                //                            else
+                //                            {
+                //                                input_i = 0;
+                //                                productScaleAcc( hidden_activation_pos_i,
+                //                                                 U, V(i), 1.,1.);
+                //                            }
+                //                        else
+                //                            if( input_is_active[i] )
+                //                            {
+                //                                input_i = 1;
+                //                                hidden_activation_neg_i -= V(i);
+                //                            }
+                //                            else
+                //                            {
+                //                                input_i = 0;
+                //                                hidden_activation_pos_i += V(i);
+                //                            }
+                //                    }
+                //                    else
+                //                    {
+                //                        w = &(connection->weights(0,i));
+                //                        input_i = input[i];
+                //                        for( int j=0; j<hidden_layer->size; j++,w+=m )
+                //                        {
+                //                            a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                //                            a_neg_i[j] = a[j] - *w * input_i;
+                //                        }
+                //                    }
+                //                    num_pos_act -= hidden_layer->freeEnergyContribution(
+                //                        hidden_activation_pos_i);
+                //                    num_neg_act -= hidden_layer->freeEnergyContribution(
+                //                        hidden_activation_neg_i);
+                //                    //num_pos = safeexp(num_pos_act);
+                //                    //num_neg = safeexp(num_neg_act);
+                //                    //input_probs_i = num_pos / (num_pos + num_neg);
+                //                    if( input_layer->use_fast_approximations )
+                //                        input_probs_i = fastsigmoid(
+                //                            num_pos_act - num_neg_act);
+                //                    else
+                //                    {
+                //                        num_pos = safeexp(num_pos_act);
+                //                        num_neg = safeexp(num_neg_act);
+                //                        input_probs_i = num_pos / (num_pos + num_neg);
+                //                    }
+                //                    if( input_layer->use_fast_approximations )
+                //                        pseudolikelihood += tabulated_softplus( 
+                //                            num_pos_act - num_neg_act ) 
+                //                            - input_i * (num_pos_act - num_neg_act);
+                //                    else
+                //                        pseudolikelihood += softplus( 
+                //                            num_pos_act - num_neg_act ) 
+                //                            - input_i * (num_pos_act - num_neg_act);
+                //
+                //                }
+                //                connection->weights(i1,j1) -= epsilon;
+                //                estimated_gradient(i1,j1) = (pseudolikelihood - estimated_gradient(i1,j1))
+                //                    / epsilon;
+                //            }
+                //
+                //    }
+                //}
+
+
                 // Compute activations
                 if( input_is_sparse )
                 {
@@ -1202,6 +2009,7 @@
                 V_gradients.clear();
 
                 int i=0;
+                pseudolikelihood = 0;
                 for( int l=0; l<input_layer->size ; l++ )
                 {
                     if( n_selected_inputs_pseudolikelihood <= inputsize() &&
@@ -1223,16 +2031,28 @@
                         hidden_activation_neg_i << hidden_act;
                         if( factorized_connection_rank > 0 )
                             if( input_is_active[i] )
+                            {
+                                input_i = 1;
                                 productScaleAcc( hidden_activation_neg_i,
                                                 U, V(i), -1.,1.);
+                            }
                             else
+                            {
+                                input_i = 0;
                                 productScaleAcc( hidden_activation_pos_i,
                                                 U, V(i), 1.,1.);
+                            }
                         else
                             if( input_is_active[i] )
+                            {
+                                input_i = 1;
                                 hidden_activation_neg_i -= V(i);
+                            }
                             else
+                            {
+                                input_i = 0;
                                 hidden_activation_pos_i += V(i);
+                            }
                     }
                     else
                     {
@@ -1339,6 +2159,7 @@
 
                 if( input_is_sparse )
                 {
+                    //Mat true_gradient(V.length(), V.width());
                     if( factorized_connection_rank > 0 )
                     {
                         // Factorized connection U update
@@ -1347,6 +2168,12 @@
                                             Vx );
                         multiplyScaledAdd( U_gradient, 1.0, -lr, U );
                         
+                        //real U_cos_ang = dot(U_gradient.toVec(),U_estimated_gradient.toVec())
+                        //    / (norm(U_gradient.toVec()) *norm(U_estimated_gradient.toVec()));
+                        //cout << "U_cos_ang=" << U_cos_ang << endl;
+                        //cout << "U_ang=" << acos(U_cos_ang) << endl;
+
+   
                         // Factorized connection V update
                         transposeProduct( Vx_gradient, U, 
                                           hidden_activation_gradient );
@@ -1354,6 +2181,7 @@
                         {
                             V((int)extra[e]) -= lr * Vx_gradient;
                             input_is_active[(int)extra[e]] = false;
+                            //true_gradient((int)extra[e]) += Vx_gradient;
                         }
                     }
                     else
@@ -1363,6 +2191,7 @@
                         {
                             V((int)extra[e]) -= lr * hidden_activation_gradient;
                             input_is_active[(int)extra[e]] = false;
+                            //true_gradient((int)extra[e]) += hidden_activation_gradient;
                         }
                     }
                     
@@ -1379,15 +2208,27 @@
                             i = l;
                         // Extra V gradients
                         V(i) -= lr * V_gradients(l);
-                        
+                        //true_gradient(i) += V_gradients(l);
+
                         // Input update
                         input_layer->bias[i] -= lr * input_gradient[i];
                     }
+                    
+                    //real cos_ang = dot(true_gradient.toVec(),estimated_gradient.toVec())
+                    //    / (norm(true_gradient.toVec()) *norm(estimated_gradient.toVec()));
+                    //cout << "cos_ang=" << cos_ang << endl;
+                    //cout << "ang=" << acos(cos_ang) << endl;
+
                 }
                 else
                 {
                     externalProductAcc( connection_gradient, hidden_activation_gradient,
                                         input );
+
+                    //real cos_ang = dot(connection_gradient.toVec(),estimated_gradient.toVec())
+                    //    / (norm(connection_gradient.toVec()) *norm(estimated_gradient.toVec()));
+                    //cout << "cos_ang=" << cos_ang << endl;
+                    //cout << "ang=" << acos(cos_ang) << endl;
                     
                     // Connection weights update
                     multiplyScaledAdd( connection_gradient, 1.0, -lr,



From tihocan at mail.berlios.de  Fri Aug 15 17:47:55 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Aug 2008 17:47:55 +0200
Subject: [Plearn-commits] r9376 - trunk/commands
Message-ID: <200808151547.m7FFlt30032236@sheep.berlios.de>

Author: tihocan
Date: 2008-08-15 17:47:55 +0200 (Fri, 15 Aug 2008)
New Revision: 9376

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added include of StochasticBinarizeVMatrix

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-08-15 15:47:39 UTC (rev 9375)
+++ trunk/commands/plearn_noblas_inc.h	2008-08-15 15:47:55 UTC (rev 9376)
@@ -356,6 +356,7 @@
 #include <plearn/vmat/SortRowsVMatrix.h>
 #include <plearn/vmat/SparseVMatrix.h>
 #include <plearn/vmat/SplitWiseValidationVMatrix.h>
+#include <plearn/vmat/StochasticBinarizeVMatrix.h>
 #include <plearn/vmat/SubInputVMatrix.h>
 #include <plearn/vmat/TemporaryDiskVMatrix.h>
 #include <plearn/vmat/TemporaryFileVMatrix.h>



From tihocan at mail.berlios.de  Fri Aug 15 17:48:24 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Aug 2008 17:48:24 +0200
Subject: [Plearn-commits] r9377 - trunk/plearn/vmat
Message-ID: <200808151548.m7FFmODQ032295@sheep.berlios.de>

Author: tihocan
Date: 2008-08-15 17:48:23 +0200 (Fri, 15 Aug 2008)
New Revision: 9377

Added:
   trunk/plearn/vmat/StochasticBinarizeVMatrix.cc
   trunk/plearn/vmat/StochasticBinarizeVMatrix.h
Log:
New class: StochasticBinarizeVMatrix, to stochastically binarize a dataset

Added: trunk/plearn/vmat/StochasticBinarizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/StochasticBinarizeVMatrix.cc	2008-08-15 15:47:55 UTC (rev 9376)
+++ trunk/plearn/vmat/StochasticBinarizeVMatrix.cc	2008-08-15 15:48:23 UTC (rev 9377)
@@ -0,0 +1,156 @@
+// -*- C++ -*-
+
+// StochasticBinarizeVMatrix.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file StochasticBinarizeVMatrix.cc */
+
+
+#include "StochasticBinarizeVMatrix.h"
+#include <plearn/vmat/ShiftAndRescaleVMatrix.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StochasticBinarizeVMatrix,
+    "Transform its source data into stochastically sampled binary data.",
+    "Each column of the source data is first rescaled into the [0, 1] range,\n"
+    "then when accessing a sample, each variable is taken to be 1 with\n"
+    "probability given by its real value (and 0 otherwise). Constant columns\n"
+    "are given a uniform distribution in {0, 1}.\n"
+    "Since sampling is performed every time a sample is accessed, one should\n"
+    "precompute the data if a constant dataset is desired.\n"
+    "In the current implementation, only the input part is binarized."
+);
+
+///////////////////////////////
+// StochasticBinarizeVMatrix //
+///////////////////////////////
+StochasticBinarizeVMatrix::StochasticBinarizeVMatrix():
+    rescale_to_0_1(true),
+    seed(1827),
+    random_gen(new PRandom())
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void StochasticBinarizeVMatrix::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "rescale_to_0_1",
+                  &StochasticBinarizeVMatrix::rescale_to_0_1,
+                  OptionBase::buildoption,
+        "Whether to rescale to [0,1] before sampling. If set to False, then\n"
+        "the data is assumed to already be in the [0,1] range (no check will\n"
+        "be performed to enforce it, though).");
+
+    declareOption(ol, "seed", &StochasticBinarizeVMatrix::seed,
+                  OptionBase::buildoption,
+        "Seed of random number generator.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void StochasticBinarizeVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void StochasticBinarizeVMatrix::build_()
+{
+    if (source) {
+        if (rescale_to_0_1) {
+            PP<ShiftAndRescaleVMatrix> rd =
+                new ShiftAndRescaleVMatrix(source, false);
+            rd->min_max = Vec(2);
+            rd->min_max[1] = 1;
+            rd->build();
+            rescaled_data = get_pointer(rd);
+        } else
+            rescaled_data = source;
+        setMetaInfoFromSource();
+    }
+}
+
+///////////////
+// getNewRow //
+///////////////
+void StochasticBinarizeVMatrix::getNewRow(int i, const Vec& v) const
+{
+    rescaled_data->getRow(i, v);
+    for (int j = 0; j < source->inputsize(); j++)
+        v[j] = random_gen->uniform_sample() <= v[j] ? 1 : 0;
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void StochasticBinarizeVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("StochasticBinarizeVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/StochasticBinarizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/StochasticBinarizeVMatrix.h	2008-08-15 15:47:55 UTC (rev 9376)
+++ trunk/plearn/vmat/StochasticBinarizeVMatrix.h	2008-08-15 15:48:23 UTC (rev 9377)
@@ -0,0 +1,135 @@
+// -*- C++ -*-
+
+// StochasticBinarizeVMatrix.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file StochasticBinarizeVMatrix.h */
+
+
+#ifndef StochasticBinarizeVMatrix_INC
+#define StochasticBinarizeVMatrix_INC
+
+#include <plearn/math/PRandom.h>
+#include <plearn/vmat/SourceVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class StochasticBinarizeVMatrix : public SourceVMatrix
+{
+    typedef SourceVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    bool rescale_to_0_1;
+    int seed;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    StochasticBinarizeVMatrix();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(StochasticBinarizeVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    PP<PRandom> random_gen;
+    VMat rescaled_data;
+
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! v is assumed to be the right size.
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StochasticBinarizeVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Sat Aug 16 05:18:16 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 16 Aug 2008 05:18:16 +0200
Subject: [Plearn-commits] r9381 - trunk/plearn_learners/distributions
Message-ID: <200808160318.m7G3IGPQ014988@sheep.berlios.de>

Author: tihocan
Date: 2008-08-16 05:18:16 +0200 (Sat, 16 Aug 2008)
New Revision: 9381

Modified:
   trunk/plearn_learners/distributions/RBMDistribution.cc
   trunk/plearn_learners/distributions/RBMDistribution.h
Log:
Added options to initialize Gibbs chain from sample, and to compute unnormalized densities (ie energies)

Modified: trunk/plearn_learners/distributions/RBMDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/RBMDistribution.cc	2008-08-16 03:16:40 UTC (rev 9380)
+++ trunk/plearn_learners/distributions/RBMDistribution.cc	2008-08-16 03:18:16 UTC (rev 9381)
@@ -52,7 +52,8 @@
 // RBMDistribution //
 /////////////////////
 RBMDistribution::RBMDistribution():
-    n_gibbs_chains(-1)
+    n_gibbs_chains(-1),
+    unnormalized_density(false)
 {}
 
 ////////////////////
@@ -81,6 +82,20 @@
         "absolute number of chains that are run simultaneously. Each chain\n"
         "will sample about N/n_chains samples, so as to obtain N samples.");
 
+    declareOption(ol, "unnormalized_density",
+                  &RBMDistribution::unnormalized_density,
+                  OptionBase::buildoption,
+        "If set to True, then the density will not be normalized (so the\n"
+        "partition function does not need to be computed). This means the\n"
+        "value returned by the 'log_density' method will instead be the\n"
+        "negative free energy of the visible input.");
+
+    declareOption(ol, "sample_data",
+                  &RBMDistribution::sample_data,
+                  OptionBase::buildoption,
+        "If provided, this data will be used to initialize the Gibbs\n"
+        "chains when generating samples.");
+ 
     // Now call the parent class' declareOptions().
     inherited::declareOptions(ol);
 }
@@ -140,9 +155,19 @@
 //////////////
 void RBMDistribution::generate(Vec& y) const
 {
-    work1.resize(1, 0);
+    work1.resize(0, 0);
     ports_val.fill(NULL);
     ports_val[rbm->getPortIndex("visible_sample")] = &work1;
+    if (sample_data) {
+        // Pick a random sample to initialize the Gibbs chain.
+        int init_i =
+            random_gen->uniform_multinomial_sample(sample_data->length());
+        real dummy_weight;
+        work3.resize(1, sample_data->inputsize());
+        Vec w3 = work3.toVec();
+        sample_data->getExample(init_i, w3, workv1, dummy_weight);
+        ports_val[rbm->getPortIndex("visible")] = &work2;
+    }
     rbm->fprop(ports_val);
     y.resize(work1.width());
     y << work1(0);
@@ -166,17 +191,41 @@
     if (n % n_chains > 0)
         n_gibbs_samples += 1;
     work2.resize(n_chains * n_gibbs_samples, Y.width());
-    PP<ProgressBar> pb = verbosity && n_gibbs_samples > 10
-        ? new ProgressBar("Gibbs sampling", n_gibbs_samples)
+    PP<ProgressBar> pb = verbosity && work2.length() > 10
+        ? new ProgressBar("Gibbs sampling", work2.length())
         : NULL;
-    for (int i = 0; i < n_gibbs_samples; i++) {
-        work1.resize(n_chains, 0);
+    int idx = 0;
+    for (int j = 0; j < n_chains; j++) {
         ports_val.fill(NULL);
-        ports_val[rbm->getPortIndex("visible_sample")] = &work1;
-        rbm->fprop(ports_val);
-        work2.subMatRows(i * n_chains, n_chains) << work1;
-        if (pb)
-            pb->updateone();
+        if (sample_data) {
+            // Pick a sample to initialize the Gibbs chain.
+            int init_i;
+            if (n_chains == sample_data->length())
+                // We use each sample once and only once.
+                init_i = j;
+            else
+                // Pick the sample randomly.
+                init_i = random_gen->uniform_multinomial_sample(sample_data->length());
+            real dummy_weight;
+            work3.resize(1, sample_data->inputsize());
+            Vec w3 = work3.toVec();
+            sample_data->getExample(init_i, w3, workv1, dummy_weight);
+            ports_val[rbm->getPortIndex("visible")] = &work3;
+        }
+        // Crash if not in the specific case where we have sample data and we
+        // compute only 1 sample in each chain. This is because otherwise I
+        // (Olivier D.) am not sure the chain is properly (i) restarted for
+        // each new chain, and (ii) kept intact when continuing the same chain.
+        PLCHECK(sample_data && n_gibbs_samples == 1);
+        for (int i = 0; i < n_gibbs_samples; i++) {
+            work1.resize(0, 0);
+            ports_val[rbm->getPortIndex("visible_sample")] = &work1;
+            rbm->fprop(ports_val);
+            work2(idx) << work1;
+            idx++;
+            if (pb)
+                pb->update(idx);
+        }
     }
     if (n_gibbs_samples > 1)
         // We shuffle rows to add more "randomness" since consecutive samples
@@ -194,7 +243,10 @@
     work1.resize(1, 0);
     work2.resize(1, y.length());
     work2 << y;
-    ports_val[rbm->getPortIndex("neg_log_likelihood")] = &work1;
+    if (unnormalized_density)
+        ports_val[rbm->getPortIndex("energy")] = &work1;
+    else
+        ports_val[rbm->getPortIndex("neg_log_likelihood")] = &work1;
     ports_val[rbm->getPortIndex("visible")] = &work2;
     rbm->fprop(ports_val);
     return -work1(0, 0);

Modified: trunk/plearn_learners/distributions/RBMDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/RBMDistribution.h	2008-08-16 03:16:40 UTC (rev 9380)
+++ trunk/plearn_learners/distributions/RBMDistribution.h	2008-08-16 03:18:16 UTC (rev 9381)
@@ -66,6 +66,8 @@
     PP<RBMModule> rbm;
 
     real n_gibbs_chains;
+    VMat sample_data;
+    bool unnormalized_density;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -151,8 +153,11 @@
     TVec<Mat*> ports_val;
 
     //! Temporary storage.
-    mutable Mat work1, work2;
-    
+    mutable Mat work1, work2, work3;
+
+    //! Temporary storage.
+    mutable Vec workv1;
+
     //#####  Protected Options  ###############################################
 
     // ### Declare protected option fields (such as learned parameters) here



From tihocan at mail.berlios.de  Sat Aug 16 05:16:41 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 16 Aug 2008 05:16:41 +0200
Subject: [Plearn-commits] r9380 - trunk/plearn_learners/online
Message-ID: <200808160316.m7G3GfQ1014332@sheep.berlios.de>

Author: tihocan
Date: 2008-08-16 05:16:40 +0200 (Sat, 16 Aug 2008)
New Revision: 9380

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Removed the buggy code I had added to generate more samples, it sucked

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-08-16 03:15:53 UTC (rev 9379)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-08-16 03:16:40 UTC (rev 9380)
@@ -576,7 +576,7 @@
     }
 
     Mat* hidden_activations = NULL;
-    if (positive_phase) {
+    if (positive_phase && hidden_act) {
         computePositivePhaseHiddenActivations(visible);
         hidden_activations = hidden_act;
     }
@@ -1250,7 +1250,7 @@
         {
             sampleHiddenGivenVisible(*visible_sample);
             hidden_activations_are_computed = false;
-            Gibbs_step=0;
+            Gibbs_step = 0;
             //cout << "sampling hidden from visible" << endl;
         }
         else if (visible_expectation && !visible_expectation_is_output)
@@ -1259,52 +1259,29 @@
         }
         else // sample unconditionally: Gibbs sample after k steps
         {
-            // Find out how many samples we want.
-            int n_samples = -1;
-            if (visible_sample_is_output)
-                n_samples = visible_sample->length();
-            if (visible_expectation_is_output) {
-                PLASSERT( n_samples == -1 ||
-                          n_samples == visible_expectation->length() );
-                n_samples = visible_expectation->length();
-            }
-            if (hidden_sample_is_output) {
-                PLASSERT( n_samples == -1 ||
-                          n_samples == hidden_sample->length() );
-                n_samples = hidden_sample->length();
-            }
-            // The code above to generate more than 1 sample is buggy. Olivier (D)
-            // is supposed to look into it. In the meantime, we just only
-            // generate 1 sample.
-            n_samples = 1;
-            PLCHECK( n_samples > 0 );
-
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
-            if (visible_layer->samples.length() != n_samples)
+            if (visible_layer->samples.isEmpty())
             {
                 // There are no samples already available to continue the
                 // chain: we restart it.
                 Gibbs_step = 0;
-                if (visible && !visible_is_output
-                            && visible->length() == n_samples)
+                if (visible && !visible_is_output)
                     visible_layer->samples << *visible;
-                else if (visible_layer->getExpectations().length() ==
-                                                                    n_samples)
+                else if (!visible_layer->getExpectations().isEmpty())
                     visible_layer->samples << visible_layer->getExpectations();
-                else if (hidden_layer->samples.length() == n_samples)
+                else if (!hidden_layer->samples.isEmpty())
                     sampleVisibleGivenHidden(hidden_layer->samples);
-                else if (hidden_layer->getExpectations().length() == n_samples)
+                else if (!hidden_layer->getExpectations().isEmpty())
                     sampleVisibleGivenHidden(hidden_layer->getExpectations());
                 else {
                     // There is no available data to initialize the chain: we
                     // initialize it with a zero vector.
                     Mat& zero_vector = visible_layer->samples;
                     PLASSERT( zero_vector.width() > 0 );
-                    zero_vector.resize(n_samples, zero_vector.width());
+                    zero_vector.resize(1, zero_vector.width());
                     zero_vector.clear();
                 }
-                PLASSERT( visible_layer->samples.length() == n_samples );
             }
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
@@ -1336,8 +1313,6 @@
         if (visible_sample && visible_sample_is_output)
             // provide sample of the visible units
         {
-            PLASSERT( visible_sample->length() ==
-                      visible_layer->samples.length() );
             visible_sample->resize(visible_layer->samples.length(),
                                    visible_layer->samples.width());
             *visible_sample << visible_layer->samples;
@@ -1345,8 +1320,6 @@
         if (hidden_sample && hidden_sample_is_output)
             // provide sample of the hidden units
         {
-            PLASSERT( hidden_sample->length() ==
-                      hidden_layer->samples.length() );
             hidden_sample->resize(hidden_layer->samples.length(),
                                   hidden_layer->samples.width());
             *hidden_sample << hidden_layer->samples;
@@ -1355,23 +1328,18 @@
             // provide expectation of the visible units
         {
             const Mat& to_store = visible_layer->getExpectations();
-            PLASSERT( visible_expectation->length() == to_store.length() );
             visible_expectation->resize(to_store.length(),
                                         to_store.width());
             *visible_expectation << to_store;
         }
         if (hidden && hidden_is_output)
         {
-            PLASSERT( hidden->length() ==
-                      hidden_layer->getExpectations().length() );
             hidden->resize(hidden_layer->getExpectations().length(),
                            hidden_layer->getExpectations().width());
             *hidden << hidden_layer->getExpectations();
         }
         if (hidden_act && hidden_act_is_output)
         {
-            PLASSERT( hidden_act->length() ==
-                      hidden_layer->activations.length() );
             hidden_act->resize(hidden_layer->activations.length(),
                                hidden_layer->activations.width());
             *hidden_act << hidden_layer->activations;



From ducharme at mail.berlios.de  Thu Aug 14 22:41:39 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 14 Aug 2008 22:41:39 +0200
Subject: [Plearn-commits] r9371 - trunk/plearn_learners/regressors
Message-ID: <200808142041.m7EKfd4F029526@sheep.berlios.de>

Author: ducharme
Date: 2008-08-14 22:41:39 +0200 (Thu, 14 Aug 2008)
New Revision: 9371

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
   trunk/plearn_learners/regressors/BasisSelectionRegressor.h
Log:
Utilisation d'un "template learner".


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-08-14 13:52:38 UTC (rev 9370)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-08-14 20:41:39 UTC (rev 9371)
@@ -84,9 +84,7 @@
       n_threads(0),
       thread_subtrain_length(0),
       residue_sum(0),
-      residue_sum_sq(0),
-      weights_sum(0)
-
+      residue_sum_sq(0)
 {}
 
 void BasisSelectionRegressor::declareOptions(OptionList& ol)
@@ -199,9 +197,9 @@
                   OptionBase::buildoption,
                   "EXPERIMENTAL OPTION (under development)");
 
-    declareOption(ol, "learner", &BasisSelectionRegressor::learner,
+    declareOption(ol, "learner", &BasisSelectionRegressor::template_learner,
                   OptionBase::buildoption,
-                  "The underlying learner to be trained with the extracted features.");
+                  "The underlying template learner.");
 
     declareOption(ol, "precompute_features", &BasisSelectionRegressor::precompute_features,
                   OptionBase::buildoption,
@@ -242,7 +240,11 @@
                   "will get included in the dictionary for interaction terms ONLY\n"
                   "(i.e. these interact with the other functions.)");
 
+    declareOption(ol, "true_learner", &BasisSelectionRegressor::learner,
+                  OptionBase::learntoption,
+                  "The underlying learner to be trained with the extracted features.");
 
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -255,7 +257,7 @@
 void BasisSelectionRegressor::setExperimentDirectory(const PPath& the_expdir)
 { 
     inherited::setExperimentDirectory(the_expdir);
-    learner->setExperimentDirectory(the_expdir / "SubLearner");
+    template_learner->setExperimentDirectory(the_expdir / "SubLearner");
 }
 
 
@@ -278,6 +280,7 @@
     deepCopyField(kernels, copies);
     deepCopyField(kernel_centers, copies);
     deepCopyField(learner, copies);
+    deepCopyField(template_learner, copies);
     deepCopyField(selected_functions, copies);
     deepCopyField(alphas, copies);
     deepCopyField(scores, copies);
@@ -302,17 +305,6 @@
 
 void BasisSelectionRegressor::forget()
 {
-    
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - initialize a random number generator with the seed option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
-
     selected_functions.resize(0);
     targets.resize(0);
     residue.resize(0);
@@ -1078,11 +1070,22 @@
     bool weighted = train_set->hasWeights();
 
     // set dummy training set, so that undelying learner frees reference to previous training set
+    /*
     VMat newtrainset = new MemoryVMatrix(1,nf+(weighted?2:1));
     newtrainset->defineSizes(nf,1,weighted?1:0);
     learner->setTrainingSet(newtrainset);
     learner->forget();
+    */
 
+    // Deep-copy the underlying learner
+    CopiesMap copies;
+    learner = template_learner->deepCopy(copies);
+    PP<VecStatsCollector> statscol = template_learner->getTrainStatsCollector();
+    learner->setTrainStatsCollector(statscol);
+    PPath expdir = template_learner->getExperimentDirectory();
+    learner->setExperimentDirectory(expdir);
+
+    VMat newtrainset;
     if(precompute_features)
     {
         features.resize(l,nf+(weighted?2:1), max(1,int(0.25*l*nf)), true); // enlarge width while preserving content
@@ -1102,12 +1105,9 @@
     else
         newtrainset= new RealFunctionsProcessedVMatrix(train_set, selected_functions, false, true, true);
     newtrainset->defineSizes(nf,1,weighted?1:0);
-    // perr.clearOutMap();
-    // perr << "new train set:\n" << newtrainset << endl;
     learner->setTrainingSet(newtrainset);
     learner->forget();
     learner->train();
-    // perr << "retrained learner:\n" << learner << endl;
     // resize features matrix so it contains only the features
     if(precompute_features)
         features.resize(l,nf);
@@ -1186,7 +1186,6 @@
     residue_sum = 0.;
     residue_sum_sq = 0.;
     weights.resize(l);
-    weights_sum = 0.;
 
     real w;
     for(int i=0; i<l; i++)
@@ -1195,10 +1194,9 @@
         real t = targ[0];
         targets[i] = t;
         residue[i] = t;
+        weights[i] = w;
         residue_sum += w*t;
         residue_sum_sq += w*square(t);
-        weights[i] = w;
-        weights_sum += w;
     }
 }
 
@@ -1293,13 +1291,13 @@
 void BasisSelectionRegressor::setTrainStatsCollector(PP<VecStatsCollector> statscol)
 { 
     train_stats = statscol;
-    learner->setTrainStatsCollector(statscol);
+    template_learner->setTrainStatsCollector(statscol);
 }
 
 
 TVec<string> BasisSelectionRegressor::getTrainCostNames() const
 {
-    return learner->getTrainCostNames();
+    return template_learner->getTrainCostNames();
 }
 
 

Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-08-14 13:52:38 UTC (rev 9370)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-08-14 20:41:39 UTC (rev 9371)
@@ -233,6 +233,12 @@
     void recomputeResidue();
     void computeOutputFromFeaturevec(const Vec& featurevec, Vec& output) const;
 
+protected:
+    //#####  Protected Data Members  ##########################################
+
+    // Template learner.  Each train step is done with a copy of this one.
+    PP<PLearner> template_learner;
+
 private:
     //#####  Private Data Members  ############################################
 
@@ -244,7 +250,6 @@
     Vec weights;
     double residue_sum;
     double residue_sum_sq;
-    double weights_sum;
 
     mutable Vec input;
     mutable Vec targ;



From tihocan at mail.berlios.de  Sat Aug 16 05:15:54 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 16 Aug 2008 05:15:54 +0200
Subject: [Plearn-commits] r9379 - trunk/plearn/vmat
Message-ID: <200808160315.m7G3Fs6C014273@sheep.berlios.de>

Author: tihocan
Date: 2008-08-16 05:15:53 +0200 (Sat, 16 Aug 2008)
New Revision: 9379

Modified:
   trunk/plearn/vmat/StochasticBinarizeVMatrix.cc
Log:
Fixed bug: seed was not used

Modified: trunk/plearn/vmat/StochasticBinarizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/StochasticBinarizeVMatrix.cc	2008-08-15 20:55:37 UTC (rev 9378)
+++ trunk/plearn/vmat/StochasticBinarizeVMatrix.cc	2008-08-16 03:15:53 UTC (rev 9379)
@@ -111,6 +111,7 @@
             rescaled_data = source;
         setMetaInfoFromSource();
     }
+    random_gen->manual_seed(seed);
 }
 
 ///////////////



From larocheh at mail.berlios.de  Fri Aug 15 15:35:02 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 15 Aug 2008 15:35:02 +0200
Subject: [Plearn-commits] r9374 - trunk/plearn_learners_experimental
Message-ID: <200808151335.m7FDZ2qP018764@sheep.berlios.de>

Author: larocheh
Date: 2008-08-15 15:35:01 +0200 (Fri, 15 Aug 2008)
New Revision: 9374

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Corrected bug for sparse inputs when connection is not given...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-15 13:16:32 UTC (rev 9373)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-15 13:35:01 UTC (rev 9374)
@@ -645,7 +645,8 @@
 
     input_layer->forget();
     hidden_layer->forget();
-    connection->forget();
+    if( connection )
+        connection->forget();
 
     cumulative_training_time = 0;
     //cumulative_testing_time = 0;
@@ -1057,7 +1058,7 @@
         if( !fast_exact_is_equal(learning_rate, 0.) &&
             (targetsize() == 0 || generative_learning_weight > 0) )
         {
-            if( decrease_ct != 0 )
+            if( !fast_exact_is_equal(decrease_ct, 0) )
                 lr = learning_rate / (1.0 + stage * decrease_ct );
             else
                 lr = learning_rate;
@@ -1090,7 +1091,9 @@
                 real* a_pos_i = hidden_activation_pos_i.data();
                 real* a_neg_i = hidden_activation_neg_i.data();
                 real* w, *gw;
-                int m = connection->weights.mod();
+                int m;
+                if( connection )
+                    m = connection->weights.mod();
                 real input_i, input_probs_i;
                 real pseudolikelihood = 0;
                 real* ga_pos_i = hidden_activation_pos_i_gradient.data();
@@ -2512,7 +2515,8 @@
                     cp_data = context_probs.data();
                     input_i = input[i];
 
-                    m = connection->weights.mod();
+                    if( connection ) 
+                        m = connection->weights.mod();
                     // input_i = 1
                     for( int k=0; k<n_conf; k++)
                     {
@@ -3319,7 +3323,8 @@
 {
     input_layer->setLearningRate( the_learning_rate );
     hidden_layer->setLearningRate( the_learning_rate );
-    connection->setLearningRate( the_learning_rate );
+    if( connection ) 
+        connection->setLearningRate( the_learning_rate );
     if( target_layer )
         target_layer->setLearningRate( the_learning_rate );
     if( target_connection )



From tihocan at mail.berlios.de  Fri Aug 15 17:47:40 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Aug 2008 17:47:40 +0200
Subject: [Plearn-commits] r9375 - trunk/plearn/vmat
Message-ID: <200808151547.m7FFlenr032206@sheep.berlios.de>

Author: tihocan
Date: 2008-08-15 17:47:39 +0200 (Fri, 15 Aug 2008)
New Revision: 9375

Modified:
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
Log:
When using the 'min_max' option, constant columns will now be mapped to the average of min and max instead of being nans

Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2008-08-15 13:35:01 UTC (rev 9374)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2008-08-15 15:47:39 UTC (rev 9375)
@@ -221,7 +221,8 @@
                   OptionBase::buildoption,
         "A vector of size 2 [min,max]. For each column, the elements will be\n"
         "shifted and rescaled to be in [min,max]. If set, it will override\n"
-        "the value of the 'automatic' option.");        
+        "the value of the 'automatic' option. A constant column will be set\n"
+        "to the average of 'min' and 'max'.");        
     
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -308,10 +309,18 @@
                 scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
 
                 for(int i=0 ; i<n_inputs ; ++i) { 
-                    shift[i] = (max_col[i] - min_col[i]) / (min_max[1] - min_max[0]) * 
-                                (min_max[0] - (min_max[1] - min_max[0])*min_col[i] /
-                                              (max_col[i] - min_col[i]) ); 
-                    scale[i] = (min_max[1] - min_max[0]) / (max_col[i] - min_col[i]) ; 
+                    if (fast_exact_is_equal(min_col[i], max_col[i])) {
+                        // Constant column.
+                        shift[i] = (min_max[1] + min_max[0]) / 2 - min_col[i];
+                        scale[i] = 1;
+                    } else {
+                        shift[i] = (max_col[i] - min_col[i]) /
+                            (min_max[1] - min_max[0]) * 
+                            (min_max[0] - (min_max[1] - min_max[0])*min_col[i]
+                             / (max_col[i] - min_col[i]) ); 
+                        scale[i] = (min_max[1] - min_max[0]) /
+                            (max_col[i] - min_col[i]); 
+                    }
                 }
 
             }



From louradou at mail.berlios.de  Fri Aug 15 22:55:37 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 15 Aug 2008 22:55:37 +0200
Subject: [Plearn-commits] r9378 - trunk/plearn_learners/online
Message-ID: <200808152055.m7FKtboO015256@sheep.berlios.de>

Author: louradou
Date: 2008-08-15 22:55:37 +0200 (Fri, 15 Aug 2008)
New Revision: 9378

Modified:
   trunk/plearn_learners/online/ClassErrorCostModule.cc
   trunk/plearn_learners/online/ClassErrorCostModule.h
Log:
added an option to ClassErrorCostModule to enable to
compute a weighted sum of the different types of classification errors.



Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2008-08-15 15:48:23 UTC (rev 9377)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2008-08-15 20:55:37 UTC (rev 9378)
@@ -54,7 +54,8 @@
     "you use this module inside a CombiningCostsModule, put its weight to 0.\n"
     );
 
-ClassErrorCostModule::ClassErrorCostModule()
+ClassErrorCostModule::ClassErrorCostModule():
+    error_costs(Mat())
 {
     output_size = 1;
     target_size = 1;
@@ -62,10 +63,10 @@
 
 void ClassErrorCostModule::declareOptions(OptionList& ol)
 {
-    // declareOption(ol, "myoption", &ClassErrorCostModule::myoption,
-    //               OptionBase::buildoption,
-    //               "Help text describing this option");
-    // ...
+     declareOption(ol, "error_costs", &ClassErrorCostModule::error_costs,
+                   OptionBase::buildoption,
+                   "A square matrix containing the cost for each type of error (default is 0/1 cost). "
+                   "The two dimensions correspond to: (true class, prediction).");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -75,6 +76,9 @@
 {
     PLASSERT( output_size == 1 );
     PLASSERT( target_size == 1 );
+    PLASSERT(error_costs.width() == error_costs.length());
+    if( !error_costs.isEmpty() && input_size > 1 )
+        PLASSERT(error_costs.length() == input_size);   
 }
 
 void ClassErrorCostModule::build()
@@ -107,10 +111,16 @@
     PLASSERT( input.size() == input_size );
     PLASSERT( target.size() == target_size );
 
-    if( input_size == 1 ) // is target[0] the closest integer to input[0]?
-        cost = ( round(input[0]) == round(target[0]) ) ? 0. : 1.;
-    else // is target[0] equals to argmax(input)?
-        cost = ( argmax(input) == int(round(target[0])) ) ? 0. : 1.;
+    if( error_costs.isEmpty() )
+        if( input_size == 1 ) // is target[0] the closest integer to input[0]?
+            cost = ( round(input[0]) == round(target[0]) ) ? 0. : 1.;
+        else // is target[0] equals to argmax(input)?
+            cost = ( argmax(input) == int(round(target[0])) ) ? 0. : 1.;
+    else
+        if( input_size == 1 )
+            cost = error_costs( int(round(target[0])), int(round(input[0])) );
+        else 
+            cost = error_costs( int(round(target[0])), argmax(input) );
 }
 
 void ClassErrorCostModule::fprop(const Mat& inputs, const Mat& targets,

Modified: trunk/plearn_learners/online/ClassErrorCostModule.h
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.h	2008-08-15 15:48:23 UTC (rev 9377)
+++ trunk/plearn_learners/online/ClassErrorCostModule.h	2008-08-15 20:55:37 UTC (rev 9378)
@@ -58,6 +58,8 @@
 
 public:
     //#####  Public Build Options  ############################################
+    
+    Mat error_costs;
 
 public:
     //#####  Public Member Functions  #########################################



From larocheh at mail.berlios.de  Fri Aug 15 15:16:32 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 15 Aug 2008 15:16:32 +0200
Subject: [Plearn-commits] r9373 - trunk/plearn/vmat
Message-ID: <200808151316.m7FDGWeb017635@sheep.berlios.de>

Author: larocheh
Date: 2008-08-15 15:16:32 +0200 (Fri, 15 Aug 2008)
New Revision: 9373

Modified:
   trunk/plearn/vmat/LIBSVMSparseVMatrix.cc
Log:
Made sure that inputsize, targetsize and weightsize could be defined in .vmat file and not be overriden by build()


Modified: trunk/plearn/vmat/LIBSVMSparseVMatrix.cc
===================================================================
--- trunk/plearn/vmat/LIBSVMSparseVMatrix.cc	2008-08-15 13:02:39 UTC (rev 9372)
+++ trunk/plearn/vmat/LIBSVMSparseVMatrix.cc	2008-08-15 13:16:32 UTC (rev 9373)
@@ -143,10 +143,10 @@
     }
 
     // Set sizes
-    inputsize_ = largest_input_index+1;
-    targetsize_ = 1;
-    weightsize_ = 0;
-    extrasize_ = largest_input_index+1;
+    if( inputsize_ < 0 ) inputsize_ = largest_input_index+1;
+    if( targetsize_ < 0 ) targetsize_ = 1;
+    if( weightsize_ < 0 ) weightsize_ = 0;
+    if( extrasize_ < 0 ) extrasize_ = largest_input_index+1;
 }
  
 



From nouiz at mail.berlios.de  Mon Aug 18 23:34:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 18 Aug 2008 23:34:09 +0200
Subject: [Plearn-commits] r9382 - trunk/plearn/vmat
Message-ID: <200808182134.m7ILY9Rx006844@sheep.berlios.de>

Author: nouiz
Date: 2008-08-18 23:34:08 +0200 (Mon, 18 Aug 2008)
New Revision: 9382

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/GaussianizeVMatrix.h
Log:
moved some functionnality from GaussianizeVMatrix::build_ to GaussianizeVMatrix::setMetaDataDir. So that we don't need to supply manually a metadatadir.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-08-16 03:18:16 UTC (rev 9381)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-08-18 21:34:08 UTC (rev 9382)
@@ -178,10 +178,30 @@
                 TVec<int>(col, col + the_source->extrasize() - 1, 1));
     col += the_source->extrasize();
 
+    // Obtain meta information from source.
+    setMetaInfoFromSource();
+    
+}
+
+////////////////////
+// setMetaDataDir //
+////////////////////
+void GaussianizeVMatrix::setMetaDataDir(const PPath& the_metadatadir){
+    inherited::setMetaDataDir(the_metadatadir);
+
+    VMat the_source = train_source ? train_source : source;
+    
     //to save the stats their must be a metadatadir
     if(!the_source->hasMetaDataDir() && hasMetaDataDir())
-        the_source->setMetaDataDir(getMetaDataDir()+"train_source");
+        if (train_source)
+            the_source->setMetaDataDir(getMetaDataDir()+"train_source");
+        else
+            the_source->setMetaDataDir(getMetaDataDir()+"source");
 
+    if(!the_source->hasMetaDataDir())
+        PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "
+                " train_source, source or this VMatrix should have a metadata directory!");
+    
     TVec<StatsCollector> stats = the_source->
         getPrecomputedStatsFromFile("stats_gaussianizeVMatrix.psave", -1, true);
 
@@ -225,9 +245,6 @@
     }
     if(features_to_gaussianize.size()==0)
         PLWARNING("GaussianizeVMatrix::build_() 0 variable was gaussianized");
-    // Obtain meta information from source.
-    setMetaInfoFromSource();
-    
 }
 
 ///////////////

Modified: trunk/plearn/vmat/GaussianizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.h	2008-08-16 03:18:16 UTC (rev 9381)
+++ trunk/plearn/vmat/GaussianizeVMatrix.h	2008-08-18 21:34:08 UTC (rev 9382)
@@ -125,6 +125,7 @@
     //! This does the actual building.
     // (PLEASE IMPLEMENT IN .cc)
     void build_();
+    virtual void setMetaDataDir(const PPath& the_metadatadir);
 
 private:
     //#####  Private Data Members  ############################################



From nouiz at mail.berlios.de  Tue Aug 19 17:56:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Aug 2008 17:56:58 +0200
Subject: [Plearn-commits] r9383 - trunk/plearn/vmat
Message-ID: <200808191556.m7JFuwaC009213@sheep.berlios.de>

Author: nouiz
Date: 2008-08-19 17:56:57 +0200 (Tue, 19 Aug 2008)
New Revision: 9383

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
moved some stuff from build_ to setMetaDataDir so that we don't need to always explicity set the metadatadir.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-18 21:34:08 UTC (rev 9382)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-19 15:56:57 UTC (rev 9383)
@@ -387,12 +387,20 @@
     }
     if (!default_spec.empty() && !reorder_fieldspec_from_headers)
         PLERROR("In TextFilesVMatrix::build_() when the option default_spec is used, reorder_fieldspec_from_headers must be true");
+}
+////////////////////
+// setMetaDataDir //
+////////////////////
+void TextFilesVMatrix::setMetaDataDir(const PPath& the_metadatadir){
+    inherited::setMetaDataDir(the_metadatadir);
+
     if(getMetaDataDir().empty())
-        PLERROR("In TextFilesVMatrix::build_() We need a metadatadir");
+        PLERROR("In TextFilesVMatrix::setMetaDataDir() - We need a metadatadir");
     if(!force_mkdir(getMetaDataDir()))
-        PLERROR("In TextFilesVMatrix::build_() could not create directory '%s'",
+        PLERROR("In TextFilesVMatrix::setMetaDataDir() - could not create"
+                " directory '%s'",
                 getMetaDataDir().absolute().c_str());
-    
+
     for(int i=0;i<txtfilenames.length();i++)
         updateMtime(txtfilenames[i]);
 
@@ -408,7 +416,8 @@
         txtfiles[k] = fopen(fnam.c_str(),"r");
         if(txtfiles[k]==NULL){
             perror("Can't open file");
-            PLERROR("In TextFilesVMatrix::build_ - Can't open file %s",fnam.c_str());
+            PLERROR("In TextFilesVMatrix::setMetaDataDir - Can't open file %s",
+                    fnam.c_str());
         }
     }
 
@@ -419,7 +428,7 @@
         buildIdx(); // (re)build it first!
     idxfile = fopen(idxfname.c_str(),"rb");
     if(fgetc(idxfile) != byte_order())
-        PLERROR("In TextFilesVMatrix::build_ - Wrong endianness."
+        PLERROR("In TextFilesVMatrix::setMetaDataDir - Wrong endianness."
                 " Remove the index file %s for it to be automatically rebuilt",
                 idxfname.c_str());
     fread(&length_, 4, 1, idxfile);
@@ -441,7 +450,8 @@
 
     // Sanity checking
     if (delimiter.size() != 1)
-        PLERROR("In TextFilesVMatrix::build_ - the 'delimiter' option '%s' must contain exactly one character",
+        PLERROR("In TextFilesVMatrix::setMetaDataDir - the 'delimiter' option"
+                " '%s' must contain exactly one character",
                 delimiter.c_str());
 }
 

Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2008-08-18 21:34:08 UTC (rev 9382)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2008-08-19 15:56:57 UTC (rev 9383)
@@ -179,6 +179,7 @@
 
     //! Return true iff 'ftype' is a valid type that does not need to be skipped.
     virtual bool isValidNonSkipFieldType(const string& ftype) const;
+    virtual void setMetaDataDir(const PPath& the_metadatadir);
 
 public:
 



From louradou at mail.berlios.de  Tue Aug 19 19:58:37 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 19 Aug 2008 19:58:37 +0200
Subject: [Plearn-commits] r9384 - trunk/plearn/vmat
Message-ID: <200808191758.m7JHwber013744@sheep.berlios.de>

Author: louradou
Date: 2008-08-19 19:58:37 +0200 (Tue, 19 Aug 2008)
New Revision: 9384

Modified:
   trunk/plearn/vmat/AddBagInformationVMatrix.cc
Log:
bug fixed: removing bag column when it is not in the input


Modified: trunk/plearn/vmat/AddBagInformationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AddBagInformationVMatrix.cc	2008-08-19 15:56:57 UTC (rev 9383)
+++ trunk/plearn/vmat/AddBagInformationVMatrix.cc	2008-08-19 17:58:37 UTC (rev 9384)
@@ -118,7 +118,8 @@
         if (remove_bag_info_column) {
             // We need to remove a column from the source VMat.
             width_--;
-            inputsize_--;
+            if( bag_info_idx < inputsize_ )
+                inputsize_--;
             Array<VMField> new_fields;
             for (int i = 0; i < fields.length(); i++)
                 if (i != bag_info_idx)



From nouiz at mail.berlios.de  Tue Aug 19 19:59:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Aug 2008 19:59:11 +0200
Subject: [Plearn-commits] r9385 - trunk/plearn_learners/meta
Message-ID: <200808191759.m7JHxBnH013794@sheep.berlios.de>

Author: nouiz
Date: 2008-08-19 19:59:11 +0200 (Tue, 19 Aug 2008)
New Revision: 9385

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
bugfix


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-08-19 17:58:37 UTC (rev 9384)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-08-19 17:59:11 UTC (rev 9385)
@@ -249,7 +249,7 @@
                                                Vec& output, Vec& costs) const
 {
     PLASSERT(costs.size()==nTestCosts());
-    PLASSERT_MSG(output.length()!=outputsize(),
+    PLASSERT_MSG(output.length()==outputsize(),
                  "In MultiClassAdaBoost::computeOutputAndCosts -"
                  " output don't have the good length!");
     output.resize(outputsize());



From ducharme at mail.berlios.de  Wed Aug 20 20:34:49 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 20 Aug 2008 20:34:49 +0200
Subject: [Plearn-commits] r9386 - trunk/plearn/misc
Message-ID: <200808201834.m7KIYnIH009642@sheep.berlios.de>

Author: ducharme
Date: 2008-08-20 20:34:49 +0200 (Wed, 20 Aug 2008)
New Revision: 9386

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Enclose string mapping with "" (when converting to csv file).


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-08-19 17:59:11 UTC (rev 9385)
+++ trunk/plearn/misc/vmatmain.cc	2008-08-20 18:34:49 UTC (rev 9386)
@@ -114,6 +114,7 @@
                 else if((strval=source->getValString(j,currow[j]))!=""){
                     if(strval.length()>1000-1)
                         PLERROR("a value is too big!");
+                    strval = "\"" + strval + "\"";
                     strncpy(buffer,strval.c_str(),1000);
                 }else{
                     // Normal processing



From louradou at mail.berlios.de  Wed Aug 20 20:57:29 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 20 Aug 2008 20:57:29 +0200
Subject: [Plearn-commits] r9387 - trunk/python_modules/plearn/learners
Message-ID: <200808201857.m7KIvTPM012051@sheep.berlios.de>

Author: louradou
Date: 2008-08-20 20:57:29 +0200 (Wed, 20 Aug 2008)
New Revision: 9387

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:
* function descriptions moved at the right place
* added the possibility to store outputs on the validation set
* fixed some little problems when writing results in SVM.result_filename



Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2008-08-20 18:34:49 UTC (rev 9386)
+++ trunk/python_modules/plearn/learners/SVM.py	2008-08-20 18:57:29 UTC (rev 9387)
@@ -10,12 +10,12 @@
 from libsvm import *
 
 from plearn.pyext import *
+from plearn.utilities.write_results import writeResults
 
-from numpy.numarray import *
-from math import *
+import math
 import numpy
 import random
-import fpconst
+import fpconst # for NaN
 
 class SVMHyperParamOracle__kernel(object):
     """ An oracle that gives values of hyperparameters      
@@ -100,12 +100,12 @@
         self.inputsize      = None
         self.verbosity      = 1
 
-    """ To be called when training set change 'a bit'    
-        (same kind of data, but different statistics, 
-        so that we can assume new best parameters are
-        close to previous ones.
-    """
     def semiforget(self):
+        """ To be called when training set change 'a bit'    
+            (same kind of data, but different statistics, 
+            so that we can assume new best parameters are
+            close to previous ones.
+        """
         if self.verbosity > 3:
             print "  (forget called)"
         self.trials_param_list  = []
@@ -151,19 +151,20 @@
         self.stats_are_uptodate = True
         return (self.inputsize, self.input_avgstd)
 
-    """ Return a list of values to try for hyperparameter 'C'
-        centered on a specified value 'C_value'.
-        This is an internal function (should not be called outside this class def).
-    """
     def init_C(self, C_value=None):
+        """ Return a list of values to try for hyperparameter 'C'
+            centered on a specified value 'C_value'.
+            This is an internal function (should not be called outside this class def).
+        """
         if C_value==None:
             C_value = self.C_initvalue
         return [ C_value, C_value/10., C_value*10.]
 
-    """ Return <list> of <dict>: which hyperparameter values to try FIRST
-        - 'kernel_param_list': <list> of <dict> kernel hyperparameters name->value """
     def choose_first_C_param(self, kernel_param_list=None,
                                    C_value=None):
+        """ Return <list> of <dict>: which hyperparameter values to try FIRST
+            - 'kernel_param_list': <list> of <dict> kernel hyperparameters name->value.
+        """
         # Input check
         if kernel_param_list==None:
             kernel_param_list=[{}]
@@ -183,14 +184,15 @@
                     table.append( pdict.copy() )
         return table
 
-    """ For a given hyperparameter name (default:'C'),
-        return a list of values of this hyperparameter that have been tried
-        (for a given sub-setting for other hyperparameters).
-        Values are SORTED w.r.t to respective costs: the first one gave
-        the best valid performance.
-        - 'param_name': <string> generic name of the hyperparameter.
-        - 'other_param': <dict> hyperparameters name->value. """
     def get_trials_oneparam_list(self, paramname='C', other_param=None):
+        """ For a given hyperparameter name (default:'C'),
+            return a list of values of this hyperparameter that have been tried
+            (for a given sub-setting for other hyperparameters).
+            Values are SORTED w.r.t to respective costs: the first one gave
+            the best valid performance.
+            - 'param_name': <string> generic name of the hyperparameter.
+            - 'other_param': <dict> hyperparameters name->value.
+        """
         # Input check
         if other_param==None:
             other_param={}
@@ -232,18 +234,17 @@
             sorted_param_list[j] = param_list[i]
         return sorted_param_list
 
-
-    """ A utility to give new hyperparameter values given the best value among a list
-        when the hyperparameter is multiplicative and positive (so the geometrical mean
-        is more suitable than the arithmetic mean).
-        - 'allow_to_return_already_tried_ones': <bool> Can return already tried values?
-                It is recommended to be True if and only if :
-                1. there are other hyperparams to tune, and
-                2. the hyperoptimization problem is not necessarily convex.
-    """
     def choose_new_param_geom( self,
                                crescentcosts_list,
                                allow_to_return_already_tried_ones = True ):
+        """ A utility to give new hyperparameter values given the best value among a list
+            when the hyperparameter is multiplicative and positive (so the geometrical mean
+            is more suitable than the arithmetic mean).
+            - 'allow_to_return_already_tried_ones': <bool> Can return already tried values?
+                    It is recommended to be True if and only if :
+                    1. there are other hyperparams to tune, and
+                    2. the hyperoptimization problem is not necessarily convex.
+        """
         best_value = crescentcosts_list[0]
         crescentvalues_list = sorted(crescentcosts_list)
 
@@ -296,11 +297,9 @@
         return [  geom_mean([crescentvalues_list[i-1],best_value]),
                   geom_mean([crescentvalues_list[i+1],best_value]) ]
 
-
-
-    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
-        - 'kernel_param_list': <list> of <dict> kernel hyperparameters name->value. """
     def choose_new_C_param(self, kernel_param_list=None):
+        """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+            - 'kernel_param_list': <list> of <dict> kernel hyperparameters name->value. """
         # Input check
         if kernel_param_list==None:
             kernel_param_list=[{}]
@@ -330,11 +329,11 @@
         return new_param_list
 
 
-    """ Return <bool>: whether or not we should try other 'C' for
-                       a given set of kernel hyperparameters.
-        - 'kernel_param': <dict> kernel parameters name->value.
-    """
     def should_be_tuned_again(self, kernel_param_list=None):
+        """ Return <bool>: whether or not we should try other 'C' for
+                           a given set of kernel hyperparameters.
+            - 'kernel_param': <dict> kernel parameters name->value.
+        """
         if kernel_param_list==None:
             kernel_param_list={}
         trials_C_list = self.get_trials_oneparam_list('C',kernel_param_list)
@@ -343,12 +342,12 @@
         best_C = self.best_param['C']
         return ( best_C == min(trials_C_list) or best_C == max(trials_C_list) )
 
-    """ Updates the statistics and internal trials history given new performances,
-        and returns if the trial was the best one.    
-        - 'param' is the <dict> of parameter values.
-        - the 'costs' are <dict> of corresponding performance (with None wherever missing).
-    """
     def update_trials(self, param, cost):
+        """ Updates the statistics and internal trials history given new performances,
+            and returns if the trial was the best one.    
+            - 'param' is the <dict> of parameter values.
+            - the 'costs' are <dict> of corresponding performance (with None wherever missing).
+        """
         if 'C' not in param:
             raise IndexError,"in SVMHyperParamOracle__kernel::update_trials(), " + \
                              "2nd arg (param=%s) must include 'C'" % (param)
@@ -363,7 +362,7 @@
 
         if( self.best_cost == None or cost < self.best_cost):
         # TODO: what if cost == bestcost and param <> self.best_param?
-        #       -> best param should be a list...
+        #       -> best param could be a list...
             self.best_param = param
             self.best_cost  = cost
             return True
@@ -378,19 +377,19 @@
         SVMHyperParamOracle__kernel.__init__(self)
         self.kernel_type = 'linear'
 
-    """ Return <list> of <dict>: which hyperparameter values to try FIRST
-        - 'samples': <array>(n_samples,dim) of (train/valid) samples.
-    """
     def choose_first_param(self, samples=None ):
+        """ Return <list> of <dict>: which hyperparameter values to try FIRST
+            - 'samples': <array>(n_samples,dim) of (train/valid) samples.
+        """
         # Invariance w.r.t scaling input
         d, std = self.get_input_stats( samples )
         # Note:  default C=1   [for default std=1.]
         self.C_initvalue = 1./(std*std)
         return self.choose_first_C_param()
 
-    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
-    """
     def choose_new_param(self):
+        """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+        """
         return self.choose_new_C_param()
 
     """ Return <bool>: whether or not we should try other hyperparameter values """
@@ -409,10 +408,10 @@
     def init_gamma(self, gamma_value=0.5):
         return [ gamma_value, gamma_value/9., gamma_value*9. ]
 
-    """ Return <list> of <dict>: which hyperparameter values to try FIRST.
-        - 'samples': <array>(n_samples,dim) of (train/valid) samples.
-    """
     def choose_first_param(self, samples=None ):
+        """ Return <list> of <dict>: which hyperparameter values to try FIRST.
+            - 'samples': <array>(n_samples,dim) of (train/valid) samples.
+        """
         d, std = self.get_input_stats(samples)
         # Note:  default gamma=1/4   [for default d=2, std=1.]
         gamma0 = 1./(2*self.inputsize*std*std)
@@ -422,9 +421,9 @@
             kernel_param_list.append({'gamma':g})
         return self.choose_first_C_param( kernel_param_list )
         
-    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
-    """
     def choose_new_param(self):
+        """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+        """
         best_gamma = self.best_param['gamma']
         tried_gamma = self.get_trials_oneparam_list('gamma')
         if best_gamma in tried_gamma:
@@ -440,9 +439,9 @@
             gamma_list = self.init_gamma(best_gamma)
         return self.choose_new_C_param( [{'gamma':g}  for g in gamma_list] )
 
-    """ Return <bool>: whether or not we should try other hyperparameter values.
-    """
     def should_be_tuned_again(self):
+        """ Return <bool>: whether or not we should try other hyperparameter values.
+        """
         best_gamma = self.best_param['gamma']
         tried_gamma = self.get_trials_oneparam_list('gamma')
         if best_gamma not in tried_gamma:
@@ -472,10 +471,10 @@
             coef0 = self.coef0_initvalue
         return [  coef0, coef0/10., coef0*10. ]
 
-    """ Return <list> of <dict>: which hyperparameter values to try FIRST.
-        - 'samples': <array>(n_samples,dim) of (train/valid) samples.
-    """
     def choose_first_param(self, samples=None ):
+        """ Return <list> of <dict>: which hyperparameter values to try FIRST.
+            - 'samples': <array>(n_samples,dim) of (train/valid) samples.
+        """
         degree_list  = self.init_degree()
         if samples <> None:
             d, std = self.get_input_stats(samples)
@@ -490,9 +489,9 @@
             return first_param_list
         return self.choose_first_C_param( [ {'degree':d,'coef0':self.coef0_initvalue} for d in degree_list ] )        
 
-    """ Return <list> of <dict>: which hyperparameter values to try NEXT.
-    """
     def choose_new_param(self):
+        """ Return <list> of <dict>: which hyperparameter values to try NEXT.
+        """
         best_degree = self.best_param['degree']
         tried_degrees = self.get_trials_oneparam_list('degree')
         if best_degree in tried_degrees:
@@ -545,9 +544,9 @@
                                     
         return new_param_list
 
-    """ Return <bool>: whether or not we should try other hyperparameter values.
-    """
     def should_be_tuned_again(self):
+        """ Return <bool>: whether or not we should try other hyperparameter values.
+        """
         best_degree = self.best_param['degree']
         tried_degrees = self.get_trials_oneparam_list('degree')
         if( best_degree in tried_degrees
@@ -754,6 +753,7 @@
         self.errorcosts      = None
         self.maincost_name   = None
         self.valid_stats     = None
+        self.valid_outputs   = None
         self.test_stats      = None
         self.train_stats     = None
         self.validtype       = 'simple'
@@ -818,6 +818,7 @@
         for expert in self.all_experts:
              expert.semiforget()
         self.valid_stats     = None
+        self.valid_outputs   = None
         self.test_stats      = None
         self.train_stats     = None
         self.stats_are_uptodate = False
@@ -844,34 +845,30 @@
         return dataspec[ self.trainset_key ]
     def valid_inputspec(self, dataspec):
         assert type(dataspec) == dict
-        if self.validset_key not in dataspec:
-            return None
-        return dataspec[ self.validset_key ]
+        return dataspec.get(self.validset_key)
     def test_inputspec(self, dataspec):
         assert type(dataspec) == dict
-        if self.testset_key not in dataspec:
-            return None
-        return dataspec[ self.testset_key ]
+        return dataspec.get( self.testset_key )
 
-    ## specific to libsvm
-    """ Return samples and targets in the format required
-        by libsvm, i.e. lists of float.
-        This is only a default function, so that the user can
-        define another method for any sophisticated VMatrix
-        (this can be done by changing the attribute:
-                                SVM.get_datalist ).
-        - 'input_vmat': <VMatrix> Must have the function getMat()
-                        that returns the corresponding array,
-                        as well as attributes:
-                        inputsize, targetsize, length.
-                        
-    """
+    ## specific to PLearn and libsvm
     def get_datalist(self, input_vmat ):
+        """ From a VMatrix, return samples and targets in the format required
+            by libsvm, i.e. lists of float.
+            This is only a default function, so that the user can
+            define another method for any sophisticated VMatrix
+            (this can be done by changing the attribute:
+                                    SVM.get_datalist ).
+            - 'input_vmat': <VMatrix> Must have the function getMat()
+                            that returns the corresponding array,
+                            as well as attributes:
+                            inputsize, targetsize, length.
+                            
+        """
         data_array = input_vmat.getMat()
         inputsize = input_vmat.inputsize
         targetsize = input_vmat.targetsize
         nsamples = input_vmat.length
-        assert shape(data_array)[0] == nsamples
+        assert data_array.shape[0] == nsamples
         samples   = [ [ float(x_t_i)    for x_t_i in x_t ]
                                         for x_t in data_array[:,:inputsize] ]
         assert targetsize == 1
@@ -892,10 +889,10 @@
         return samples, targets
         
 
-    """ Return a dictionary class label -> class frenquency,
-        where frequencies are estimated from 'targets', a list of class labels
-    """
     def get_class_priors(self, targets ):
+        """ Return a dictionary class label -> class frenquency,
+            where frequencies are estimated from 'targets', a list of class labels
+        """
         class_priors = {}
         for label in targets:
             if label not in class_priors:
@@ -906,9 +903,9 @@
             class_priors[ label ] *= 1./nsamples
         return class_priors
            
-    """ Return the input/target dimensions and stats estimated on a VMatrix 'vmat'.
-    """
     def get_data_stats(self, vmat=None):
+        """ Return the input/target dimensions and stats estimated on a VMatrix 'vmat'.
+        """
         if self.stats_are_uptodate:
             return ( self.nclasses, self.class_priors,
                      self.inputsize, self.input_avgstd )
@@ -985,11 +982,11 @@
             return None
         return allcosts
 
-    """ Return a svm_parameter in the format for libsvm.
-        - 'param': <dict> parameters name->value.
-    """
     ## specific to libsvm
     def get_libsvm_param(self, param ):
+        """ Return a svm_parameter in the format for libsvm.
+            - 'param': <dict> parameters name->value.
+        """
         param = param.copy()
         for pn in param:
             if param[pn] == None:
@@ -1004,9 +1001,6 @@
         return svm_parameter( svm_type = C_SVC, **param )
     
 
-    """ Write given results with corresponding parameters
-        In a PLearn format (.amat).True
-    """
     def write_results(  self, param,
                         valid_stats,
                         test_stats = None,
@@ -1015,6 +1009,9 @@
                         nvalid = None,
                         ntest = None,
                         only_stdout= False ):
+        """ Write given results with corresponding parameters
+            In a PLearn format (.amat).True
+        """
         if valid_stats==None and param == self.best_param:
                 valid_stats = self.valid_stats
         if test_stats==None and param == self.best_param:
@@ -1027,9 +1024,6 @@
         
         # If no file specified, print on stdout in a readable format
         if self.results_filename == None or self.verbosity > 0:
-            #print "\n -- Trial with parameters"
-            #for pn in param:
-            #    print "    ",pn," = ",param[pn]
             if train_costs <> None:
                 print " -- train costs: ", train_costs
             if valid_costs <> None:
@@ -1038,27 +1032,22 @@
                 print " -- test costs: ",  test_costs
             if self.results_filename == None or only_stdout:
                 return
-
-
-        # Format 'results_filename' (NO extension ".amat")
-        if self.results_filename[-5:]=='.amat':
-            self.results_filename = self.results_filename[:-5]
         
         # Format preprocessing option names to obtain one string
-        if type(self.preproc_optionnames)==str:
+        if isinstance(self.preproc_optionnames,str):
+            preproc_optionnames = self.preproc_optionnames.split()
+        elif isinstance(self.preproc_optionnames,list):
             preproc_optionnames = self.preproc_optionnames
-        elif type(self.preproc_optionnames)==list:
-            preproc_optionnames = ' '.join(self.preproc_optionnames)
         else:
-            raise TypeError, "preproc_optionnames must be of type str or list"
+            raise TypeError, "preproc_optionnames (type %s) must be of type str or list" % type(self.preproc_optionnames)
 
         # Format preprocessing option values to obtain one string
-        if type(self.preproc_optionvalues)==str:
+        if isinstance(self.preproc_optionvalues,str):
+            preproc_optionvalues = self.preproc_optionvalues.split()
+        elif isinstance(self.preproc_optionvalues,list):
             preproc_optionvalues = self.preproc_optionvalues
-        elif type(self.preproc_optionvalues)==list:
-            preproc_optionvalues = ' '.join('%s' % v for v in self.preproc_optionvalues)
         else:
-            raise TypeError, "preproc_optionvalues must be of type str or list"
+            raise TypeError, "preproc_optionvalues (type %s) must be of type str or list" % type(self.preproc_optionvalues)
         
         param_names=self.param_names
         param_values=[]
@@ -1070,16 +1059,13 @@
             else:
                 param_values.append(None)
         
-        costnames_string = ""
-        costvalues_string = ""
+        costnames = []
+        costvalues = []
         
-        all_set_names = ['valid','test']
-        if self.test_on_train:
-            all_set_names = ['train']+all_set_names
-        
         for cn in self.costnames:
-            for dataset in all_set_names:
+            for dataset in ['train','valid','test']:
                 costs = eval(dataset+'_costs')
+                if costs == None:continue
                 
                 # Special processing for the confusion matrix
                 if cn == 'confusion_matrix':
@@ -1089,48 +1075,36 @@
                         confusion_matrix=None
                     for cli in range(self.nclasses):
                         for clj in range(self.nclasses):
-                            costnames_string += "E[%s.E[cm_%d_%d]] " % \
-                                                (dataset, cli, clj)
+                            costnames.append( "E[%s.E[cm_%d_%d]]" % \
+                                                (dataset, cli, clj) )
                             if confusion_matrix <> None:
-                                costvalues_string += "%s " % confusion_matrix[cli,clj]
+                                costvalues.append( confusion_matrix[cli,clj] )
                             else:
-                                costvalues_string += "None "                                
+                                costvalues.append( None )
                     continue
 
-                costnames_string += "E[%s.E[%s]] " % (dataset, cn)
-                if costs <> None and cn in costs:
-                    costvalues_string += "%s " % costs[cn]
-                else:
-                    costvalues_string += "None "
+                costnames.append( "E[%s.E[%s]]" % (dataset, cn) )
+                costvalues.append( costs.get(cn) )
         
         for variable_name in ['ntrain','nvalid','ntest']:
             if eval(variable_name) <> None:
-                preproc_optionnames += ' %s ' % variable_name
-                preproc_optionvalues += ' %s ' % eval(variable_name)
+                preproc_optionnames += ['%s' % variable_name]
+                preproc_optionvalues += ['%s' % eval(variable_name)]
         
         # Write the result in the file specified by 'results_filename'
-        os.system('makeresults  %s %s %s %s;' % \
-                            (self.results_filename,
-                              preproc_optionnames,
-                              ' '.join(param_names),
-                              costnames_string
-                            ) \
-               + 'appendresults %s.amat %s %s %s' % \
-                            (self.results_filename,
-                              preproc_optionvalues,
-                              ' '.join(str(v) for v in param_values),
-                              costvalues_string
-                            )
-                )
+        writeResults(   [ preproc_optionnames + param_names, preproc_optionvalues + param_values ]
+                      , [ costnames, costvalues ]
+                      , self.results_filename)
 
-    """ Updates the statistics and internal trials history given new performances,
-        and returns if the trial was the best one.
-        - 'param' is the <dict> of parameter values
-        - the 'costs' are <dict> of corresponding performance (with None wherever missing) """
     def update_trials(self, param,
                       valid_stats,
                       test_stats = None,
                       train_stats= None):
+        """ Updates the statistics and internal trials history given new performances,
+            and returns if the trial was the best one.
+            - 'param' is the <dict> of parameter values
+            - the 'costs' are <dict> of corresponding performance (with None wherever missing).
+        """
 
         # Check input: enforce to give a valid cost (and not test cost)
         # or else the valid has been done and we try with the best hyperparameters
@@ -1166,13 +1140,13 @@
         return False
 
 
-    """ Return a libSVM model trained for a given set
-        of hyperparameters 'param' and some dataset
-    """
     def train( self,
                dataspec,
                param = None
              ):
+        """ Return a libSVM model trained for a given set
+            of hyperparameters 'param' and some dataset
+        """
         if self.verbosity > 3:
             print "SVM::train() called ", dataspec.keys()
         
@@ -1224,17 +1198,17 @@
         
         return dataspec
         
-    """ This function can be changed by the user to take another
-        decision than the standard ones, for instance when
-        classifying bags of data (where each bag correspond to a target)
-    """
     def predict_from_outputs(self, outputs, targets, vmat):
+        """ This function can be changed by the user to take another
+            decision than the standard ones, for instance when
+            classifying bags of data (where each bag correspond to a target)
+        """
         assert self.nclasses == len(outputs[0])
         assert type(outputs[0]) == list
         predictions = []
         for o in outputs:
-            predictions.append( array(o).argmax() )
-        return predictions, targets
+            predictions.append( numpy.array(o).argmax() )
+        return predictions, targets, outputs
 
     def get_outputs_targets( self,
                              testset ):
@@ -1304,7 +1278,7 @@
             elif self.outputs_type == 'onehot' or self.outputs_type == 'votes':
                 for x in samples:
                     output = [0]*nclasses
-                    svm_outputs = zeros(nclasses)
+                    svm_outputs = numpy.zeros(nclasses)
                     for c in range(nclasses):
                         onevsall_dict = model[c].predict_values(x)
                         svm_outputs[c] = onevsall_dict[(1,-1)]
@@ -1325,16 +1299,16 @@
         assert len(outputs) == len(targets)
         return outputs, targets
 
-    """ Return the costs obtained by a libSVM model
-        on a given dataset.
-        If 'return_outputs' is set to True, also returns a numpy array
-        containing outputs.
-    """
     def test( self,
               testset,
               teststats = None,
               return_outputs = False
              ):
+        """ Return the costs obtained by a libSVM model
+            on a given dataset.
+            If 'return_outputs' is set to True, also returns a numpy array
+            containing outputs.
+        """
         nclasses = self.nclasses
         costnames = self.costnames
         # Translation of the cost 'confusion_matrix'
@@ -1352,7 +1326,7 @@
             teststats.setFieldNames( costnames )
 
         outputs, targets = self.get_outputs_targets( testset )
-        predictions, targets = self.predict_from_outputs( outputs, targets, testset)
+        predictions, targets, outputs = self.predict_from_outputs( outputs, targets, testset)
 
         # Computing misclassification costs for the default normalized
         # classification error (= class error weighted w.r.t class priors)
@@ -1366,15 +1340,14 @@
         cm_weights = [ (class_priors.get(t,0.)!=0. and 1./class_priors.get(t,0.)) or fpconst.NaN for t in range(nclasses) ]
         if 'norm_ce' in self.costnames:
             if self.errorcosts == None:        
-                errorcosts = zeros((nclasses,nclasses))
+                errorcosts = numpy.zeros((nclasses,nclasses))
                 for classe in range(nclasses):
+                    cp= class_priors.get(classe,0.)
                     for prediction in range(classe)+range(classe+1,nclasses):
-                        cp= class_priors.get(classe,0.)
                         if cp != 0.:
                             errorcosts[classe,prediction] = 1./ ( nclasses * cp )
                         else:
-                            errorcosts[classe,prediction] = fpconst.NaN
-                            
+                            errorcosts[classe,prediction] = fpconst.NaN           
             else:
                 errorcosts = self.errorcosts
 
@@ -1405,46 +1378,46 @@
             teststats.update(statVec,1.)
 
         if return_outputs:
-            numpy_out = numpy.zeros((len(outputs), len(outputs[0])))
-            for i, out_i in enumerate(outputs):
-                numpy_out[i,:] = out_i
-            return teststats, numpy_out
+            return teststats, numpy.array(outputs)
         else:
             return teststats #, outputs, costs
 
 
     def valid( self,
                dataspec,
-               param= None):
-        if self.validset_key in dataspec:
+               param= None,
+               return_outputs = False):
+        if self.valid_inputspec(dataspec) <> None:
             self.validtype = 'simple'
-            return self.simplevalid(dataspec, param)
+            return self.simplevalid(dataspec, param, return_outputs = return_outputs)
         else:
-            return self.crossvalid(dataspec, param)
+            return self.crossvalid(dataspec, param, return_outputs = return_outputs)
 
-    """ Return the costs obtained by (simple) validation
-        for a given set of hyperparameter.
-    """
     def simplevalid( self,
                      dataspec,
                      param,
                      validstats = None,
-                     verbosity = True ):
-        if self.verbosity > 0 and verbosity:
+                     verbose = True,
+                     return_outputs = False ):
+        """ Return the costs obtained by (simple) validation
+            for a given set of hyperparameter.
+        """
+        if self.verbosity > 0 and verbose:
             print "\n** Simple Validation"
             print "   with param %s" % param
         self.train(dataspec, param)
         validset = self.valid_inputspec(dataspec)
         self.validset = validset
-        return self.test( validset, validstats )
+        return self.test( validset, validstats, return_outputs )
 
 
-    """ Return the costs obtained by cross-validation
-        for a given set of hyperparameter.
-    """
     def crossvalid( self,
                     dataspec,
-                    param ):
+                    param,
+                    return_outputs = False ):
+        """ Return the costs obtained by cross-validation
+            for a given set of hyperparameter.
+        """
         nclasses = self.nclasses
         n_fold = self.n_fold
         self.validtype = '%s-fold' % n_fold
@@ -1468,32 +1441,47 @@
             Nlastfold[c] = N[c] - Nfold[c] * (n_fold-1)
         
         validstats = None
+        if return_outputs:
+            validoutputs = numpy.zeros((trainset.length, nclasses))
         sub_trainset_class = [None]*nclasses
         sub_testset_class = [None]*nclasses
         for i in range(n_fold):
+            test_indices = []
             for c in range(nclasses):
                 if i < n_fold-1:
-                    test_indices = range( i*Nfold[c], (i+1)*Nfold[c] )
-                    train_indices = range( 0,i*Nfold[c])+range((i+1)*Nfold[c], N[c] )
+                    test_indices_class = range( i*Nfold[c], (i+1)*Nfold[c] )
+                    train_indices_class = range( 0,i*Nfold[c])+range((i+1)*Nfold[c], N[c] )
                 else:
-                    test_indices = range( i*Nfold[c], N[c] )
-                    train_indices = range( 0, N[c]-Nlastfold[c] )
+                    test_indices_class = range( i*Nfold[c], N[c] )
+                    train_indices_class = range( 0, N[c]-Nlastfold[c] )
                 sub_testset_class[c] = SelectRowsVMatrix(
                                     source = trainset_class[c],
-                                    indices = test_indices,)
+                                    indices = test_indices_class,)
                 sub_trainset_class[c] = SelectRowsVMatrix(
                                     source = trainset_class[c],
-                                    indices = train_indices,)
+                                    indices = train_indices_class,)
+                test_indices += test_indices_class
             sub_testset = ConcatRowsVMatrix( sources = sub_testset_class,
                                              fully_check_mappings = False,)
             sub_trainset = ConcatRowsVMatrix( sources = sub_trainset_class,
                                              fully_check_mappings = False,)
-            validstats = self.simplevalid({self.trainset_key:sub_trainset,
-                                           self.validset_key:sub_testset},
-                                            param,
-                                            validstats, False)
+            if return_outputs:
+                validstats, outputs = self.simplevalid(  {self.trainset_key:sub_trainset,
+                                                self.validset_key:sub_testset},
+                                                param,
+                                                validstats = validstats, verbose = False, return_outputs = True)
+                #validoutputs[test_indices,:] = outputs
+                validoutputs = numpy.array(list(validoutputs)+list(outputs))
+            else:
+                validstats = self.simplevalid(  {self.trainset_key:sub_trainset,
+                                                self.validset_key:sub_testset},
+                                                param,
+                                                validstats = validstats, verbose = False)
 
-        return validstats
+        if return_outputs:
+            return validstats, validoutputs
+        else:
+            return validstats
 
 
     def retrain_and_writeresults(self, dataspec):
@@ -1503,7 +1491,7 @@
         # Cross Validation
         if self.validset_key not in dataspec:
             if self.verbosity > 0:
-                print "\n** training model on entire train"
+                print "\n** training model on entire train\n\twith param %s" % self.best_param
             self.train( dataspec )
 
         # Simple Validation
@@ -1533,7 +1521,7 @@
                             fully_check_mappings = False,
                         )
                 if self.verbosity > 0:
-                    print "\n** re-training model on { train + valid } "
+                    print "\n** re-training model on { train + valid }\n\twith params %s" % (self.best_param)
                 self.train( {self.trainset_key: tv_set} )
 
             # CAUTION: in the case of simple validation without retraining on {train+valid},
@@ -1561,13 +1549,13 @@
                             ntest = (testset <> None and testset.length or 0) )
         return dataspec
 
-    """ THE interesting function of the class.
-        See __main__ below for usage.
-        dataspec is a dictionary which specifies train, valid, test sets.
-        The train set is mandatory, but valid and/or test sets can be missing.
-        cf. train_inputspec(), valid_inputspec(), and test_inputspec().
-    """
-    def run(self, dataspec, param = None, L0 = None):
+    def run(self, dataspec, param = None, L0 = None, save_valid_outputs = False):
+        """ THE interesting function of the class.
+            See __main__ below for usage.
+            dataspec is a dictionary which specifies train, valid, test sets.
+            The train set is mandatory, but valid and/or test sets can be missing.
+            cf. train_inputspec(), valid_inputspec(), and test_inputspec().
+        """
         random.seed(17)
         assert self.testlevel >= 0
         assert self.max_ntrials > 0
@@ -1602,7 +1590,10 @@
                 self.validset = None
                 return self.retrain_and_writeresults(dataspec)
             
-            valid_stats = self.valid(dataspec, param)
+            if save_valid_outputs:
+                valid_stats, valid_outputs = self.valid(dataspec, param, return_outputs = True)
+            else:
+                valid_stats = self.valid(dataspec, param, return_outputs = False)
 
             # No improvement measured
             if not self.update_trials( param, valid_stats ):
@@ -1617,8 +1608,11 @@
 
             # Better valid cost is obtained!
             else:
+                if save_valid_outputs:
+                    self.valid_outputs = valid_outputs
+            
                 # Simple Validation
-                if self.validset_key in dataspec:
+                if validset <> None:
                     self.best_model = self.model
                     
                 if self.testlevel > 0:
@@ -1639,7 +1633,7 @@
         and local_retrain_until_local_optimum_is_found
         and expert.should_be_tuned_again()
         and ( self.max_cost == None or expert.best_cost <= self.max_cost ) ):
-           return self.run( dataspec, None, L0 )
+           return self.run( dataspec, param = None, save_valid_outputs = save_valid_outputs, L0 = L0 )
 
         if self.testlevel == 0:
              return self.retrain_and_writeresults(dataspec)
@@ -1660,7 +1654,7 @@
     stds=[get_std_cmp(data,i) for i in range(len(data[0]))]
     while 0 in stds:
         stds.remove(0)
-    stds=array(stds)
+    stds=numpy.array(stds)
     return stds.mean(), stds.std()
 
 def get_mean_cmp(data,i):
@@ -1685,10 +1679,10 @@
             data[j][i]=(data[j][i]-mean[i])/std[i]
     return mean, std
 
-""" Geometric mean (useful to deal with multiplicative hyperparameters
-                    such as 'C', 'gamma', ...)
-"""
 def geom_mean(data):
+    """ Return geometric mean (useful to deal with multiplicative hyperparameters
+                        such as 'C', 'gamma', ...)
+    """
     if type(data[0]) == list:
         res=[]
         for coor in range(len(data[0])):
@@ -1700,17 +1694,17 @@
             prod *= value
         return prod**(1./len(data))
 
-""" Softmax function
-"""
 def softmax(output):
-    expterms = [ exp(o) for o in output ]
+    """ Softmax function
+    """
+    expterms = [ math.exp(o) for o in output ]
     S = sum(expterms)
     return [ e/S for e in expterms ]
 
 """"""
 
 """ Below:
-    Some (API) Functions that can be assigned
+    Some functions that can be assigned
     to a SVM object, to deal with bags of data
     to classify.
 
@@ -1745,16 +1739,18 @@
     assert type(outputs[0]) == list
     bag_predictions=[]
     bag_targets=[]
-    votes = zeros(nclasses)
+    bag_outputs=[]
+    sum_outputs = numpy.zeros(nclasses)
     for output, t, b in zip(outputs, targets, baginfo):
         if b in [1,3]: # beginning of a bag
-            votes = zeros(nclasses)
+            sum_outputs = numpy.zeros(nclasses)
         for o, c in zip(output, range(nclasses)):
-            votes[c] += o
+            sum_outputs[c] += o
         if b in [2,3]: # end of a bag
-            bag_predictions.append( votes.argmax() )
+            bag_predictions.append( sum_outputs.argmax() )
             bag_targets.append( t )
-    return bag_predictions, bag_targets
+            bag_outputs.append( sum_outputs )
+    return bag_predictions, bag_targets, numpy.array(bag_outputs)
 
 
 if __name__ == '__main__':
@@ -1774,7 +1770,7 @@
 
     """ <bool> weight the coeff 'C' with the inverse prior proba of each class
                (scaled so as to have average=1 on weights). Useful when classes are unbalanced. """
-    # svm.balanceC = 1
+    # svm.balanceC = True
 
     """ <list> of cost names to compute. """
     # svm.costnames = ['class_error','confusion_matrix']



From larocheh at mail.berlios.de  Wed Aug 20 20:57:42 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 20 Aug 2008 20:57:42 +0200
Subject: [Plearn-commits] r9388 - trunk/plearn_learners/online
Message-ID: <200808201857.m7KIvgx5012087@sheep.berlios.de>

Author: larocheh
Date: 2008-08-20 20:57:41 +0200 (Wed, 20 Aug 2008)
New Revision: 9388

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added some new noises


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-08-20 18:57:29 UTC (rev 9387)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-08-20 18:57:41 UTC (rev 9388)
@@ -65,7 +65,12 @@
     online( false ),
     compute_all_test_costs( false ),
     reconstruct_hidden( false ),
+    noise_type( "none" ),
     fraction_of_masked_inputs( 0 ),
+    probability_of_masked_inputs( 0 ),
+    mask_with_mean( false ),
+    gaussian_std( 1. ),
+    binary_sampling_noise_parameter( 1. ),
     unsupervised_nstages( 0 ),
     unsupervised_fine_tuning_learning_rate( 0. ),
     unsupervised_fine_tuning_decrease_ct( 0. ),
@@ -219,6 +224,16 @@
                   "reconstruct their hidden layers (inspired from CD1 in an RBM).\n"
         );
 
+    declareOption(ol, "noise_type",
+                  &StackedAutoassociatorsNet::noise_type,
+                  OptionBase::buildoption,
+                  "Type of noise that corrupts the autoassociators input. "
+                  "Choose among:\n"
+                  " - \"masking noise\"\n"
+                  " - \"binary sampling\"\n"
+                  " - \"gaussian\"\n"
+        );
+
     declareOption(ol, "fraction_of_masked_inputs",
                   &StackedAutoassociatorsNet::fraction_of_masked_inputs,
                   OptionBase::buildoption,
@@ -226,6 +241,34 @@
                   "masked, i.e. unsused to reconstruct the input.\n"
         );
 
+    declareOption(ol, "probability_of_masked_inputs",
+                  &StackedAutoassociatorsNet::probability_of_masked_inputs,
+                  OptionBase::buildoption,
+                  "Probability of masking each input component. Either this "
+                  "option.\n"
+                  "or fraction_of_masked_inputs should be > 0.\n"
+        );
+
+    declareOption(ol, "mask_with_mean",
+                  &StackedAutoassociatorsNet::mask_with_mean,
+                  OptionBase::buildoption,
+                  "Indication that inputs should be masked with the "
+                  "training set mean of that component.\n"
+        );
+
+    declareOption(ol, "gaussian_std",
+                  &StackedAutoassociatorsNet::gaussian_std,
+                  OptionBase::buildoption,
+                  "Standard deviation of Gaussian noise.\n"
+        );
+
+    declareOption(ol, "binary_sampling_noise_parameter",
+                  &StackedAutoassociatorsNet::binary_sampling_noise_parameter,
+                  OptionBase::buildoption,
+                  "Parameter \tau for corrupted input sampling:\n"
+                  "  \tilde{x}_k ~ B((x_k - 0.5) \tau + 0.5)\n"
+        );
+
     declareOption(ol, "unsupervised_nstages",
                   &StackedAutoassociatorsNet::unsupervised_nstages,
                   OptionBase::buildoption,
@@ -279,6 +322,12 @@
                   "Hidden layers for the correlation connections"
         );
 
+    declareOption(ol, "input_mean",
+                  &StackedAutoassociatorsNet::input_mean,
+                  OptionBase::learntoption,
+                  "Mean of inputs on the training set"
+        );
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -351,11 +400,31 @@
                     " - \n"
                     "fraction_of_masked_inputs should be > or equal to 0.\n");
 
+        if( probability_of_masked_inputs < 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "probability_of_masked_inputs should be > or equal to 0.\n");
+
         if( online && fraction_of_masked_inputs > 0)
             PLERROR("StackedAutoassociatorsNet::build_()"
                     " - \n"
                     "masked inputs has not been implemented for online option.\n");
 
+        if( train_set && noise_type == "masking noise" && mask_with_mean )
+        {
+            Vec input(inputsize());
+            Vec target(train_set->targetsize());
+            real weight;
+            input_mean.resize(inputsize());
+            input_mean.clear();
+            for( int l = 0; l<train_set->length(); l++ )
+            {
+                train_set->getExample(l, input, target, weight);
+                input_mean += input;
+            }
+            input_mean /= train_set->length();
+        }
+
         if( !online )
         {
             if( greedy_stages.length() == 0)
@@ -726,10 +795,9 @@
     deepCopyField(final_cost_values_0, copies);
     deepCopyField(final_cost_gradient, copies);
     deepCopyField(final_cost_gradients, copies);
-    deepCopyField(masked_autoassociator_input, copies);
-    deepCopyField(masked_autoassociator_expectations, copies);
-    deepCopyField(autoassociator_input_indices, copies);
+    deepCopyField(corrupted_autoassociator_expectations, copies);
     deepCopyField(autoassociator_expectation_indices, copies);
+    deepCopyField(input_mean, copies);
     deepCopyField(greedy_stages, copies);
 }
 
@@ -880,10 +948,6 @@
             reconstruction_expectation_gradients.resize(layers[i]->size);
             reconstruction_expectation_gradients_m.resize(minibatch_size,
                                                           layers[i]->size);
-            masked_autoassociator_input.resize(layers[i]->size);
-            autoassociator_input_indices.resize(layers[i]->size);
-            for( int j=0 ; j < autoassociator_input_indices.length() ; j++ )
-                autoassociator_input_indices[j] = j;
 
             if(reconstruct_hidden)
             {
@@ -971,14 +1035,14 @@
                     layers[i]->size );
             }
 
-            if( fraction_of_masked_inputs > 0 )
+            if( noise_type == "masking noise" && fraction_of_masked_inputs > 0 )
             {
-                masked_autoassociator_expectations.resize( n_layers-1 );
+                corrupted_autoassociator_expectations.resize( n_layers-1 );
                 autoassociator_expectation_indices.resize( n_layers-1 );
 
                 for( int i=0 ; i<n_layers-1 ; i++ )
                 {
-                    masked_autoassociator_expectations[i].resize( layers[i]->size );
+                    corrupted_autoassociator_expectations[i].resize( layers[i]->size );
                     autoassociator_expectation_indices[i].resize( layers[i]->size );
                     for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
                         autoassociator_expectation_indices[i][j] = j;
@@ -1124,25 +1188,75 @@
     }
 }
 
+void StackedAutoassociatorsNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer)
+{
+    corrupted_input.resize(input.length());
+    if( noise_type == "masking noise" )
+    {
+        if( fraction_of_masked_inputs > 0 )
+        {
+            if( probability_of_masked_inputs > 0 )
+                PLERROR("In StackedAutoassociatorsNet::corrupt_input(): fraction_of_masked_inputs and probability_of_masked_inputs can't be both > 0");
+            random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
+            corrupted_input << input;
+            if( mask_with_mean )
+                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = input_mean[autoassociator_expectation_indices[layer][j]];
+            else
+                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
+        }
+        else if( probability_of_masked_inputs > 0 )
+        {
+            if( mask_with_mean )
+                for( int j=0 ; j <input.length() ; j++)
+                    if( random_gen->uniform_sample() < probability_of_masked_inputs )
+                        corrupted_input[ j ] = input_mean[ j ];
+                    else
+                        corrupted_input[ j ] = input[ j ];
+            else
+                for( int j=0 ; j <input.length() ; j++)
+                    if( random_gen->uniform_sample() < probability_of_masked_inputs )
+                        corrupted_input[ j ] = 0;
+                    else
+                        corrupted_input[ j ] = input[ j ];
+                
+        }
+        else
+            PLERROR("In StackedAutoassociatorsNet::corrupt_input(): either fraction_of_masked_inputs or probability_of_masked_inputs should be > 0");
+
+    }
+    else if( noise_type == "binary sampling" )
+    {
+        for( int i=0; i<corrupted_input.length(); i++ )
+            corrupted_input[i] = random_gen->binomial_sample((input[i]-0.5)*binary_sampling_noise_parameter+0.5);
+    }
+    else if( noise_type == "gaussian" )
+    {
+        for( int i=0; i<corrupted_input.length(); i++ )
+            corrupted_input[i] = input[i] + 
+                random_gen->gaussian_01() * gaussian_std;
+    }
+    else if( noise_type == "none" )
+        corrupted_input << input; 
+    else
+        PLERROR("In StackedAutoassociatorsNet::corrupt_input(): noise_type %s not valid", noise_type.c_str());
+}
+
 void StackedAutoassociatorsNet::greedyStep(const Vec& input, const Vec& target,
                                            int index, Vec train_costs)
 {
     PLASSERT( index < n_layers );
 
-    if( fraction_of_masked_inputs > 0 )
-        random_gen->shuffleElements(autoassociator_input_indices);
-
     expectations[0] << input;
     if(correlation_connections.length() != 0)
     {
         for( int i=0 ; i<index + 1; i++ )
         {
-            if( i == index && fraction_of_masked_inputs > 0 )
+            if( i == index )
             {
-                masked_autoassociator_input << expectations[i];
-                for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
-                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0;
-                connections[i]->fprop( masked_autoassociator_input, correlation_activations[i] );
+                corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i );
+                connections[i]->fprop( corrupted_autoassociator_expectations[i], correlation_activations[i] );
             }
             else
                 connections[i]->fprop( expectations[i], correlation_activations[i] );
@@ -1158,12 +1272,10 @@
     {
         for( int i=0 ; i<index + 1; i++ )
         {
-            if( i == index && fraction_of_masked_inputs > 0 )
+            if( i == index )
             {
-                masked_autoassociator_input << expectations[i];
-                for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
-                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0;
-                connections[i]->fprop( masked_autoassociator_input, activations[i+1] );
+                corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i );
+                connections[i]->fprop( corrupted_autoassociator_expectations[i], activations[i+1] );
             }
             else
                 connections[i]->fprop( expectations[i], activations[i+1] );
@@ -1195,13 +1307,7 @@
                                         activation_gradients[ index + 1 ],
                                         expectation_gradients[ index + 1 ] );
 
-        if( fraction_of_masked_inputs > 0 )
-            connections[ index ]->bpropUpdate( masked_autoassociator_input,
-                                               activations[ index + 1 ],
-                                               expectation_gradients[ index ],
-                                               activation_gradients[ index + 1 ] );
-        else
-            connections[ index ]->bpropUpdate( expectations[ index ],
+        connections[ index ]->bpropUpdate( corrupted_autoassociator_expectations[index],
                                            activations[ index + 1 ],
                                            expectation_gradients[ index ],
                                            activation_gradients[ index + 1 ] );
@@ -1211,12 +1317,8 @@
                                                 reconstruction_activations);
     if(direct_connections.length() != 0)
     {
-        if( fraction_of_masked_inputs > 0 )
-            direct_connections[ index ]->fprop( masked_autoassociator_input,
-                                                direct_activations );
-        else
-            direct_connections[ index ]->fprop( expectations[ index ],
-                                                direct_activations );
+        direct_connections[ index ]->fprop( corrupted_autoassociator_expectations[index],
+                                            direct_activations );
         direct_and_reconstruction_activations.clear();
         direct_and_reconstruction_activations += direct_activations;
         direct_and_reconstruction_activations += reconstruction_activations;
@@ -1234,18 +1336,11 @@
 
         layers[ index ]->update(direct_and_reconstruction_activation_gradients);
 
-        if( fraction_of_masked_inputs > 0 )
-            direct_connections[ index ]->bpropUpdate(
-                masked_autoassociator_input,
-                direct_activations,
-                reconstruction_expectation_gradients, // Will be overwritten later
-                direct_and_reconstruction_activation_gradients);
-        else
-            direct_connections[ index ]->bpropUpdate(
-                expectations[ index ],
-                direct_activations,
-                reconstruction_expectation_gradients, // Will be overwritten later
-                direct_and_reconstruction_activation_gradients);
+        direct_connections[ index ]->bpropUpdate(
+            corrupted_autoassociator_expectations[index],
+            direct_activations,
+            reconstruction_expectation_gradients, // Will be overwritten later
+            direct_and_reconstruction_activation_gradients);
 
         reconstruction_connections[ index ]->bpropUpdate(
             expectations[ index + 1],
@@ -1357,18 +1452,11 @@
             correlation_activation_gradients [ index ],
             correlation_expectation_gradients [ index ]);
 
-        if( fraction_of_masked_inputs > 0 )
-            connections[ index ]->bpropUpdate(
-                masked_autoassociator_input,
-                correlation_activations[ index ],
-                reconstruction_expectation_gradients, //reused
-                correlation_activation_gradients [ index ]);
-        else
-            connections[ index ]->bpropUpdate(
-                expectations[ index ],
-                correlation_activations[ index ],
-                reconstruction_expectation_gradients, //reused
-                correlation_activation_gradients [ index ]);
+        connections[ index ]->bpropUpdate(
+            corrupted_autoassociator_expectations[index],
+            correlation_activations[ index ],
+            reconstruction_expectation_gradients, //reused
+            correlation_activation_gradients [ index ]);
     }
     else
     {
@@ -1378,18 +1466,11 @@
                                         reconstruction_activation_gradients,
                                         reconstruction_expectation_gradients);
 
-        if( fraction_of_masked_inputs > 0 )
-            connections[ index ]->bpropUpdate(
-                masked_autoassociator_input,
-                activations[ index + 1 ],
-                reconstruction_expectation_gradients, //reused
-                reconstruction_activation_gradients);
-        else
-            connections[ index ]->bpropUpdate(
-                expectations[ index ],
-                activations[ index + 1 ],
-                reconstruction_expectation_gradients, //reused
-                reconstruction_activation_gradients);
+        connections[ index ]->bpropUpdate(
+            corrupted_autoassociator_expectations[index],
+            activations[ index + 1 ],
+            reconstruction_expectation_gradients, //reused
+            reconstruction_activation_gradients);
     }
 
 }
@@ -1410,69 +1491,29 @@
 
     if(correlation_connections.length() != 0)
     {
-        if( fraction_of_masked_inputs > 0 )
+        
+        for( int i=0 ; i<n_layers-1; i++ )
         {
-            for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
-                random_gen->shuffleElements(autoassociator_expectation_indices[i]);
-
-            for( int i=0 ; i<n_layers-1; i++ )
-            {
-                masked_autoassociator_expectations[i] << expectations[i];
-                if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
-                    for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
-                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0;
-
-                connections[i]->fprop( masked_autoassociator_expectations[i],
-                                       correlation_activations[i] );
-                layers[i+1]->fprop( correlation_activations[i],
-                                    correlation_expectations[i] );
-                correlation_connections[i]->fprop( correlation_expectations[i],
-                                                   activations[i+1] );
-                correlation_layers[i]->fprop( activations[i+1],
-                                              expectations[i+1] );
-            }
+            corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i);
+            connections[i]->fprop( corrupted_autoassociator_expectations[i],
+                                   correlation_activations[i] );
+            layers[i+1]->fprop( correlation_activations[i],
+                                correlation_expectations[i] );
+            correlation_connections[i]->fprop( correlation_expectations[i],
+                                               activations[i+1] );
+            correlation_layers[i]->fprop( activations[i+1],
+                                          expectations[i+1] );
         }
-        else
-        {
-            for( int i=0 ; i<n_layers-1; i++ )
-            {
-                connections[i]->fprop( expectations[i], correlation_activations[i]);
-                layers[i+1]->fprop( correlation_activations[i],
-                                    correlation_expectations[i] );
-                correlation_connections[i]->fprop( correlation_expectations[i],
-                                                   activations[i+1] );
-                correlation_layers[i]->fprop( activations[i+1],
-                                              expectations[i+1] );
-            }
-        }
     }
     else
     {
-        if( fraction_of_masked_inputs > 0 )
+        for( int i=0 ; i<n_layers-1; i++ )
         {
-            for( int i=0; i<autoassociator_expectation_indices.length(); i++ )
-                random_gen->shuffleElements(autoassociator_expectation_indices[i]);
-
-            for( int i=0 ; i<n_layers-1; i++ )
-            {
-                masked_autoassociator_expectations[i] << expectations[i];
-                if( !(mask_input_layer_only_in_unsupervised_fine_tuning && i > 0) )
-                    for( int j=0 ; j < round(fraction_of_masked_inputs*layers[i]->size) ; j++)
-                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0;
-
-                connections[i]->fprop( masked_autoassociator_expectations[i],
-                                       activations[i+1] );
-                layers[i+1]->fprop(activations[i+1],expectations[i+1]);
-            }
+            corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i);
+            connections[i]->fprop( corrupted_autoassociator_expectations[i],
+                                   activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
-        else
-        {
-            for( int i=0 ; i<n_layers-1; i++ )
-            {
-                connections[i]->fprop( expectations[i], activations[i+1] );
-                layers[i+1]->fprop(activations[i+1],expectations[i+1]);
-            }
-        }
     }
     fine_tuning_reconstruction_expectations[ n_layers-1 ] <<
         expectations[ n_layers-1 ];
@@ -1554,18 +1595,11 @@
                 correlation_activation_gradients [ i ],
                 correlation_expectation_gradients [ i ]);
 
-            if( fraction_of_masked_inputs > 0 )
-                connections[ i ]->bpropUpdate(
-                    masked_autoassociator_expectations[ i ],
-                    correlation_activations[ i ],
-                    expectation_gradients[i],
-                    correlation_activation_gradients [ i ]);
-            else
-                connections[ i ]->bpropUpdate(
-                    expectations[ i ],
-                    correlation_activations[ i ],
-                    expectation_gradients[i],
-                    correlation_activation_gradients [ i ]);
+            connections[ i ]->bpropUpdate(
+                corrupted_autoassociator_expectations[ i ],
+                correlation_activations[ i ],
+                expectation_gradients[i],
+                correlation_activation_gradients [ i ]);
         }
         else
         {
@@ -1573,14 +1607,9 @@
             layers[i+1]->bpropUpdate(
                 activations[i+1],expectations[i+1],
                 activation_gradients[i+1],expectation_gradients[i+1]);
-            if( fraction_of_masked_inputs > 0 )
-                connections[i]->bpropUpdate(
-                    masked_autoassociator_expectations[i], activations[i+1],
-                    expectation_gradients[i], activation_gradients[i+1] );
-            else
-                connections[i]->bpropUpdate(
-                    expectations[i], activations[i+1],
-                    expectation_gradients[i], activation_gradients[i+1] );
+            connections[i]->bpropUpdate(
+                corrupted_autoassociator_expectations[i], activations[i+1],
+                expectation_gradients[i], activation_gradients[i+1] );
         }
     }
 }

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-08-20 18:57:29 UTC (rev 9387)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-08-20 18:57:41 UTC (rev 9388)
@@ -147,10 +147,28 @@
     //! reconstruct their hidden layers (inspired from CD1 in an RBM)
     bool reconstruct_hidden;
 
+    //! Type of noise that corrupts the autoassociators input
+    string noise_type;
+
     //! Random fraction of the autoassociators' input components that
     //! masked, i.e. unsused to reconstruct the input.
     real fraction_of_masked_inputs;
 
+    //! Probability of masking each input component. Either this option
+    //! or fraction_of_masked_inputs should be > 0.
+    real probability_of_masked_inputs;
+
+    //! Indication that inputs should be masked with the 
+    //! training set mean of that component
+    bool mask_with_mean;
+
+    //! Standard deviation of Gaussian noise
+    real gaussian_std;
+
+    //! Parameter \tau for corrupted input sampling:
+    //!   \tilde{x}_k ~ B((x_k - 0.5) \tau + 0.5)
+    real binary_sampling_noise_parameter;
+
     //! Number of samples to use for unsupervised fine-tuning
     int unsupervised_nstages;
 
@@ -373,19 +391,15 @@
     mutable Vec final_cost_gradient;
     mutable Mat final_cost_gradients;
 
-    //! Input of autoassociator where some of the components
-    //! have been masked (set to 0) randomly.
-    Vec masked_autoassociator_input;
-
     //! Layers randomly masked, for unsupervised fine-tuning.
-    TVec< Vec > masked_autoassociator_expectations;
+    TVec< Vec > corrupted_autoassociator_expectations;
 
-    //! Indices of the input components
-    TVec<int> autoassociator_input_indices;
-
     //! Indices of the expectation components
     TVec< TVec<int> > autoassociator_expectation_indices;
 
+    //! Mean of inputs on the training set
+    Vec input_mean;
+
     //! Stages of the different greedy phases
     TVec<int> greedy_stages;
 
@@ -421,6 +435,8 @@
 
     Mat remote_computeOutputsWithoutCorrelationConnections(const Mat& inputs) const;
 
+    void corrupt_input(const Vec& input, Vec& corrupted_input, int layer);
+
     //! Global storage to save memory allocations.
     mutable Vec tmp_output;
     mutable Mat tmp_output_mat;



From larocheh at mail.berlios.de  Thu Aug 21 15:48:48 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 21 Aug 2008 15:48:48 +0200
Subject: [Plearn-commits] r9389 - trunk/plearn_learners/online
Message-ID: <200808211348.m7LDmmc4028938@sheep.berlios.de>

Author: larocheh
Date: 2008-08-21 15:48:47 +0200 (Thu, 21 Aug 2008)
New Revision: 9389

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added different types of noises...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-08-20 18:57:41 UTC (rev 9388)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-08-21 13:48:47 UTC (rev 9389)
@@ -65,7 +65,7 @@
     online( false ),
     compute_all_test_costs( false ),
     reconstruct_hidden( false ),
-    noise_type( "none" ),
+    noise_type( "masking_noise" ),
     fraction_of_masked_inputs( 0 ),
     probability_of_masked_inputs( 0 ),
     mask_with_mean( false ),
@@ -229,9 +229,10 @@
                   OptionBase::buildoption,
                   "Type of noise that corrupts the autoassociators input. "
                   "Choose among:\n"
-                  " - \"masking noise\"\n"
-                  " - \"binary sampling\"\n"
+                  " - \"masking_noise\"\n"
+                  " - \"binary_sampling\"\n"
                   " - \"gaussian\"\n"
+                  " - \"none\"\n"
         );
 
     declareOption(ol, "fraction_of_masked_inputs",
@@ -322,10 +323,10 @@
                   "Hidden layers for the correlation connections"
         );
 
-    declareOption(ol, "input_mean",
-                  &StackedAutoassociatorsNet::input_mean,
+    declareOption(ol, "expectation_means",
+                  &StackedAutoassociatorsNet::expectation_means,
                   OptionBase::learntoption,
-                  "Mean of inputs on the training set"
+                  "Mean of layers on the training set for each layer"
         );
 
     // Now call the parent class' declareOptions
@@ -410,21 +411,6 @@
                     " - \n"
                     "masked inputs has not been implemented for online option.\n");
 
-        if( train_set && noise_type == "masking noise" && mask_with_mean )
-        {
-            Vec input(inputsize());
-            Vec target(train_set->targetsize());
-            real weight;
-            input_mean.resize(inputsize());
-            input_mean.clear();
-            for( int l = 0; l<train_set->length(); l++ )
-            {
-                train_set->getExample(l, input, target, weight);
-                input_mean += input;
-            }
-            input_mean /= train_set->length();
-        }
-
         if( !online )
         {
             if( greedy_stages.length() == 0)
@@ -644,6 +630,22 @@
     expectations[n_layers-1].resize( layers[n_layers-1]->size );
     activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
     expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+
+    // For denoising autoencoders
+    corrupted_autoassociator_expectations.resize( n_layers-1 );
+    if( noise_type == "masking_noise" && fraction_of_masked_inputs > 0 )
+        autoassociator_expectation_indices.resize( n_layers-1 );
+    
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        corrupted_autoassociator_expectations[i].resize( layers[i]->size );
+        if( noise_type == "masking_noise" && fraction_of_masked_inputs > 0 )
+        {
+            autoassociator_expectation_indices[i].resize( layers[i]->size );
+            for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
+                autoassociator_expectation_indices[i][j] = j;
+        }
+    }
 }
 
 void StackedAutoassociatorsNet::build_costs()
@@ -797,7 +799,7 @@
     deepCopyField(final_cost_gradients, copies);
     deepCopyField(corrupted_autoassociator_expectations, copies);
     deepCopyField(autoassociator_expectation_indices, copies);
-    deepCopyField(input_mean, copies);
+    deepCopyField(expectation_means, copies);
     deepCopyField(greedy_stages, copies);
 }
 
@@ -915,6 +917,46 @@
             MODULE_LOG << "  end_stage = " << end_stage << endl;
             MODULE_LOG << "  greedy_learning_rate = " << greedy_learning_rate << endl;
 
+            if( *this_stage == 0 && noise_type == "masking_noise" && mask_with_mean )
+            {
+                Vec in(inputsize());
+                Vec tar(train_set->targetsize());
+                real w;
+                expectation_means.resize(n_layers-1);
+                expectation_means[i].resize(expectations[i].length());
+                expectation_means[i].clear();
+                for( int l = 0; l<train_set->length(); l++ )
+                {
+                    train_set->getExample(l, in, tar, w);
+                    // Get representation
+                    expectations[0] << in;
+                    if(correlation_connections.length() != 0)
+                    {
+                        for( int j=0 ; j<i; j++ )
+                        {
+                            connections[j]->fprop( expectations[j], correlation_activations[j] );
+                            layers[j+1]->fprop( correlation_activations[j],
+                                                correlation_expectations[j] );
+                            correlation_connections[j]->fprop( correlation_expectations[j],
+                                                               activations[j+1] );
+                            correlation_layers[j]->fprop( activations[j+1],
+                                                          expectations[j+1] );
+                        }
+                    }
+                    else
+                    {
+                        for( int j=0 ; j<i; j++ )
+                        {
+                            connections[j]->fprop( expectations[j], activations[j+1] );
+                            layers[j+1]->fprop(activations[j+1],expectations[j+1]);
+                        }
+                    }
+                    
+                    expectation_means[i] += expectations[i];
+                }
+                expectation_means[i] /= train_set->length();
+            }
+
             if( report_progress && *this_stage < end_stage )
                 pb = new ProgressBar( "Training layer "+tostring(i)
                                       +" of "+classname(),
@@ -1035,20 +1077,6 @@
                     layers[i]->size );
             }
 
-            if( noise_type == "masking noise" && fraction_of_masked_inputs > 0 )
-            {
-                corrupted_autoassociator_expectations.resize( n_layers-1 );
-                autoassociator_expectation_indices.resize( n_layers-1 );
-
-                for( int i=0 ; i<n_layers-1 ; i++ )
-                {
-                    corrupted_autoassociator_expectations[i].resize( layers[i]->size );
-                    autoassociator_expectation_indices[i].resize( layers[i]->size );
-                    for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
-                        autoassociator_expectation_indices[i][j] = j;
-                }
-            }
-
             setLearningRate( unsupervised_fine_tuning_learning_rate );
             train_costs.fill(MISSING_VALUE);
             for( ; unsupervised_stage<unsupervised_nstages ; unsupervised_stage++ )
@@ -1191,27 +1219,16 @@
 void StackedAutoassociatorsNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer)
 {
     corrupted_input.resize(input.length());
-    if( noise_type == "masking noise" )
+    if( noise_type == "masking_noise" )
     {
-        if( fraction_of_masked_inputs > 0 )
+        if( probability_of_masked_inputs > 0 )
         {
-            if( probability_of_masked_inputs > 0 )
+            if( fraction_of_masked_inputs > 0 )
                 PLERROR("In StackedAutoassociatorsNet::corrupt_input(): fraction_of_masked_inputs and probability_of_masked_inputs can't be both > 0");
-            random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
-            corrupted_input << input;
             if( mask_with_mean )
-                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
-                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = input_mean[autoassociator_expectation_indices[layer][j]];
-            else
-                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
-                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
-        }
-        else if( probability_of_masked_inputs > 0 )
-        {
-            if( mask_with_mean )
                 for( int j=0 ; j <input.length() ; j++)
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
-                        corrupted_input[ j ] = input_mean[ j ];
+                        corrupted_input[ j ] = expectation_means[layer][ j ];
                     else
                         corrupted_input[ j ] = input[ j ];
             else
@@ -1223,10 +1240,19 @@
                 
         }
         else
-            PLERROR("In StackedAutoassociatorsNet::corrupt_input(): either fraction_of_masked_inputs or probability_of_masked_inputs should be > 0");
+        {
+            random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
+            corrupted_input << input;
+            if( mask_with_mean )
+                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = expectation_means[layer][autoassociator_expectation_indices[layer][j]];
+            else
+                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
+        }
 
     }
-    else if( noise_type == "binary sampling" )
+    else if( noise_type == "binary_sampling" )
     {
         for( int i=0; i<corrupted_input.length(); i++ )
             corrupted_input[i] = random_gen->binomial_sample((input[i]-0.5)*binary_sampling_noise_parameter+0.5);

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-08-20 18:57:41 UTC (rev 9388)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-08-21 13:48:47 UTC (rev 9389)
@@ -397,8 +397,8 @@
     //! Indices of the expectation components
     TVec< TVec<int> > autoassociator_expectation_indices;
 
-    //! Mean of inputs on the training set
-    Vec input_mean;
+    //! Mean of layers on the training set for each layer
+    TVec<Vec> expectation_means;
 
     //! Stages of the different greedy phases
     TVec<int> greedy_stages;



From nouiz at mail.berlios.de  Thu Aug 21 18:55:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Aug 2008 18:55:03 +0200
Subject: [Plearn-commits] r9390 - trunk/plearn_learners/meta
Message-ID: <200808211655.m7LGt3x8005822@sheep.berlios.de>

Author: nouiz
Date: 2008-08-21 18:55:02 +0200 (Thu, 21 Aug 2008)
New Revision: 9390

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
allow to build if no template gived.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-08-21 13:48:47 UTC (rev 9389)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-08-21 16:55:02 UTC (rev 9390)
@@ -231,10 +231,10 @@
         PLERROR("In Adaboost:build_(): conf_rated_adaboost and pseudo_loss_adaboost cannot both be true, a choice must be made");
 
     
-    int n;
+    int n = 0;
     if(weak_learners.size()>0)
         n=weak_learners[0]->outputsize();
-    else
+    else if(weak_learner_template)
         n=weak_learner_template->outputsize();
     weak_learner_output.resize(n);
 }



From nouiz at mail.berlios.de  Thu Aug 21 19:05:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Aug 2008 19:05:16 +0200
Subject: [Plearn-commits] r9391 - trunk/plearn/vmat
Message-ID: <200808211705.m7LH5GR4018246@sheep.berlios.de>

Author: nouiz
Date: 2008-08-21 19:05:14 +0200 (Thu, 21 Aug 2008)
New Revision: 9391

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
generalized VMatrix::compatibleSizeError to take an extra error message to display and use it.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-08-21 16:55:02 UTC (rev 9390)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-08-21 17:05:14 UTC (rev 9391)
@@ -383,7 +383,7 @@
     train_set->isUpToDate(file_name,true,true);
 
     mean_median_mode_file = new FileVMatrix(file_name);
-    compatibleSizeError(mean_median_mode_file);
+    compatibleSizeError(mean_median_mode_file, "Bad file "+file_name);
     PLCHECK(mean_median_mode_file->fieldNames()==fieldNames());
     mean_median_mode_file->getRow(0, variable_mean);
     mean_median_mode_file->getRow(1, variable_median);

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-21 16:55:02 UTC (rev 9390)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-21 17:05:14 UTC (rev 9391)
@@ -1374,8 +1374,8 @@
         || this->extrasize()  != m->extrasize() );
 }
 
-void VMatrix::compatibleSizeError(const VMat& m){
-#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::compatibleSizeError - in class %s The matrix are not compatible!\n m1."#NAME"=%d and m2."#NAME"=%d", classname().c_str(), this->NAME(), m->NAME());
+void VMatrix::compatibleSizeError(const VMat& m, string extra_msg){
+#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::compatibleSizeError - in class %s The matrix are not compatible!\n m1."#NAME"=%d and m2."#NAME"=%d. %s", classname().c_str(), this->NAME(), m->NAME(), extra_msg.c_str());
 
     if(this->width()      != m->width())
         MY_PRINT_ERROR_MST(width)

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-08-21 16:55:02 UTC (rev 9390)
+++ trunk/plearn/vmat/VMatrix.h	2008-08-21 17:05:14 UTC (rev 9391)
@@ -275,7 +275,7 @@
 
     /// generate an PLERROR iif it don't looks like the same matrix,
     /// i.e. it has same sizes and width.
-    void compatibleSizeError(const VMat& m);
+    void compatibleSizeError(const VMat& m, string extra_msg = "");
 
     /**
      *  This should be called by the build method of every VMatrix that has a



From nouiz at mail.berlios.de  Thu Aug 21 20:36:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Aug 2008 20:36:50 +0200
Subject: [Plearn-commits] r9392 - trunk/plearn/vmat
Message-ID: <200808211836.m7LIaoNR005282@sheep.berlios.de>

Author: nouiz
Date: 2008-08-21 20:36:50 +0200 (Thu, 21 Aug 2008)
New Revision: 9392

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
remove warning


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-08-21 17:05:14 UTC (rev 9391)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-08-21 18:36:50 UTC (rev 9392)
@@ -192,11 +192,12 @@
     VMat the_source = train_source ? train_source : source;
     
     //to save the stats their must be a metadatadir
-    if(!the_source->hasMetaDataDir() && hasMetaDataDir())
+    if(!the_source->hasMetaDataDir() && hasMetaDataDir()){
         if (train_source)
             the_source->setMetaDataDir(getMetaDataDir()+"train_source");
         else
             the_source->setMetaDataDir(getMetaDataDir()+"source");
+    }
 
     if(!the_source->hasMetaDataDir())
         PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "



From ducharme at mail.berlios.de  Mon Aug 25 16:38:04 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 25 Aug 2008 16:38:04 +0200
Subject: [Plearn-commits] r9393 - in trunk: commands/PLearnCommands
	plearn/misc
Message-ID: <200808251438.m7PEc4BD029703@sheep.berlios.de>

Author: ducharme
Date: 2008-08-25 16:38:03 +0200 (Mon, 25 Aug 2008)
New Revision: 9393

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
   trunk/plearn/misc/vmatmain.cc
Log:
Add a conversion vmat -> arff (Attribute-Relation File Format) format.
See http://www.cs.waikato.ac.nz/~ml/weka/arff.html for more details.


Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2008-08-21 18:36:50 UTC (rev 9392)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2008-08-25 14:38:03 UTC (rev 9393)
@@ -71,13 +71,15 @@
         "       ( will work only if your executable includes commands/PLearnCommands/VMatViewCommand.h )\n"
         "   or: vmat stats <dataset> \n"
         "       Will display basic statistics for each field \n"
-        "   or: vmat convert <source> <destination> [--cols=col1,col2,col3,...]\n"
-        "       To convert any dataset into a .amat, .pmat, .dmat, .vmat or .csv format. \n"
+        "   or: vmat convert <source> <destination> [--cols=col1,col2,col3,...] [--mat_to_mem] [--save_vmat]\n"
+        "       To convert any dataset into a .amat, .pmat, .dmat, .vmat, .csv or .arff format. \n"
         "       The extension of the destination is used to determine the format you want. \n"
         "       If the option --cols is specified, it requests to keep only the given columns\n"
         "       (no space between the commas and the columns); columns can be given either as a\n"
         "       number (zero-based) or a column name (string).  You can also specify a range,\n"
         "       such as 0-18, or any combination thereof, e.g. 5,3,8-18,Date,74-85\n"
+        "       If the option --mat_to_mem is specified, we load the original matrix into memory\n"
+        "       If the option --save_vmat is specified, we save the source vmat in the destination metadatadir\n"
         "       If .csv (Comma-Separated Value) is specified as the destination file, the \n"
         "       following additional options are also supported:\n"
         "         --skip-missings: if a row (after selecting the appropriate columns) contains\n"
@@ -86,8 +88,13 @@
         "         --delimiter=C:   use character C as the field delimiter (default = ',')\n"
         "         --convert-date:  first column is assumed to be in CYYMMDD format; it is\n"
         "                          exported as YYYYMMDD in the .csv file (19000000 is added)\n"
-        "         --mat_to_mem:    Load the original matrix into memory\n"
-        "         --save_vmat:     Save the source vmat in the destination metadatadir\n"
+        "       If .arff (Attribute-Relation File Format) is specified as the destination file, the \n"
+        "       following additional options are also supported:\n"
+        "         --skip-missings: if a row (after selecting the appropriate columns) contains\n"
+        "                          one or more missing values, it is skipped during export\n"
+        "         --precision=N:   a maximum of N digits is printed after the decimal point\n"
+        "         --date-cols=col1,col2,...:  we flag the specified columns as a date\n"
+        "                                     we also convert the date from CYYMMDD to YYYYMMDD (if necessary)\n"
         "   or: vmat gendef <source> [binnum1 binnum2 ...] \n"
         "       Generate stats for dataset (will put them in its associated metadatadir). \n"
         "   or: vmat genvmat <source_dataset> <dest_vmat> [binned{num} | onehot{num} | normalized]\n"

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-08-21 18:36:50 UTC (rev 9392)
+++ trunk/plearn/misc/vmatmain.cc	2008-08-25 14:38:03 UTC (rev 9393)
@@ -112,9 +112,10 @@
                     // necessary
                     sprintf(buffer, "%8f", currow[j] + 19000000.0);
                 else if((strval=source->getValString(j,currow[j]))!=""){
+                    search_replace(strval, "\"", "\\\"");
+                    strval = "\"" + strval + "\"";
                     if(strval.length()>1000-1)
                         PLERROR("a value is too big!");
-                    strval = "\"" + strval + "\"";
                     strncpy(buffer,strval.c_str(),1000);
                 }else{
                     // Normal processing
@@ -143,6 +144,136 @@
 }
 
   
+/**
+ * This function converts a VMat to a ARFF (Attribute-Relation File Format) file
+ * with the given name.  One can also specify whether any missing values on a row
+ * cause that row to be skipped during export.
+ * In addition, the number of significant digits after the decimal period can be specified.
+ *
+ * The 'date_columns' option (if not empty) flags the specified columns as a
+ * date.  Also, it converts the date from CYYMMDD to YYYYMMDD (if necessary).
+ */
+static void save_vmat_as_arff(VMat source, ostream& destination, TVec<string>& date_columns,
+                             bool skip_missings, int precision = 12, bool verbose = true)
+{
+    PP<ProgressBar> pb;
+    if (verbose)
+        pb = new ProgressBar(cout, "Saving to ARFF", source.length());
+
+    // First, write the ARFF specific file header
+    destination << "@relation arff-database\n";
+    TVec<string> fields = source->fieldNames();
+    int nb_fields = fields.size();
+    for (int i=0; i<nb_fields; ++i)
+    {
+        string curfield = fields[i];
+        destination << "@attribute " << curfield << " ";
+        map<string,real> string_map = source->getStringToRealMapping(i);
+        if (date_columns.contains(curfield))  // e.g. @attribute start_date date "yyyyMMdd"
+            destination << "date \"yyyyMMdd\"\n";
+        else if (string_map.empty())  // e.g. @attribute Age numeric
+            destination << "numeric\n";
+        else  // e.g. @attribute Country {"Canada", "China", "Columbia"}
+        {
+            int nb_keys = string_map.size();
+            int key_i = 0;
+            destination << "{";
+            map<string,real>::iterator it;
+            for (it = string_map.begin(); it != string_map.end(); ++it)
+            {
+                string key = it->first;
+                search_replace(key, "\"", "\\\"");
+                destination << '"' << key << '"';
+                if (key_i < nb_keys-1)
+                    destination << ", ";
+                key_i++;
+            }
+            destination << "}\n";
+        }
+    }
+    destination << "@data\n";
+
+    // Next, output each line.  Perform missing-value checks if required.
+    const string delimiter = ",";
+    const int buffer_size = 10000;
+    char buffer[buffer_size];
+    for (int i=0, n=source.length(); i<n; ++i) {
+        if (pb)
+            pb->update(i+1);
+
+        // Skip missing values?
+        Vec currow = source(i);
+        if (skip_missings && currow.hasMissing())
+            continue;
+
+        for (int j=0, m=currow.size(); j<m; ++j)
+        {
+            string strval = "";
+            // Date field
+            if (date_columns.contains(fields[j]))
+            {
+                // Date conversion: add 19000000 to convert from CYYMMDD to
+                // YYYYMMDD, and always output without trailing . if not
+                // necessary
+                real curfield = currow[j];
+                if (is_missing(curfield)  ||  curfield==0)  // missing value
+                {
+                    strval = "?";
+                    strncpy(buffer, strval.c_str(), buffer_size);
+                }
+                else
+                {
+                    if (curfield < 10000000)  // CYYMMDD format
+                        curfield += 19000000.0;
+                    sprintf(buffer, "%8d", int(curfield));
+                }
+            }
+            // String mapped field
+            else if ((strval=source->getValString(j,currow[j])) != "")
+            {
+                search_replace(strval, "\"", "\\\"");
+                strval = "\"" + strval + "\"";
+                if (strval.length()>buffer_size-1)
+                    PLERROR("a value is too big!");
+                strncpy(buffer, strval.c_str(), buffer_size);
+            }
+            // Numeric field
+            else
+            {
+                real curfield = currow[j];
+                if (is_missing(curfield))  // missing value
+                {
+                    strval = "?";
+                    strncpy(buffer, strval.c_str(), buffer_size);
+                }
+                else
+                {
+                    // Normal processing
+                    sprintf(buffer, "%#.*f", precision, currow[j]);
+
+                    // strip all trailing zeros and final period
+                    // there is always a period since sprintf includes # modifier
+                    char* period = buffer;
+                    while (*period && *period != '.')
+                        period++;
+                    for (char* last = period + strlen(period) - 1; last >= period && (*last == '0' || *last == '.'); --last)
+                    {
+                        bool should_break = *last == '.';
+                        *last = '\0';
+                        if (should_break)
+                            break;
+                    }
+                }
+            }
+            destination << buffer;
+            if (j < m-1)
+                destination << delimiter;
+        }
+        destination << "\n";
+    }
+}
+
+  
 //! Prints where m1 and m2 differ by more than tolerance
 //! returns the number of such differences, or -1 if the sizes differ
 int print_diff(ostream& out, VMat m1, VMat m2, double tolerance, int verbose)
@@ -499,13 +630,14 @@
     */
     else if(command=="convert")
     {
-        string source = argv[2];
-        string destination = argv[3];
-        bool mat_to_mem=false;
         if(argc<4)
             PLERROR("Usage: vmat convert <source> <destination> "
                     "[--mat_to_mem] [--cols=col1,col2,col3,...] [--save_vmat] [--skip-missings] [--precision=N] [--delimiter=CHAR]");
 
+        string source = argv[2];
+        string destination = argv[3];
+        bool mat_to_mem = false;
+
         /**
          * Interpret the following options:
          *
@@ -531,6 +663,7 @@
          *           ::object in the metadatadir of the destination
          */
         TVec<string> columns;
+        TVec<string> date_columns;
         bool skip_missings = false;
         int precision = 12;
         string delimiter = ",";
@@ -544,6 +677,10 @@
                 string columns_str = curopt.substr(7);
                 columns = split(columns_str, ',');
             }
+            else if (curopt.substr(0,12) == "--date-cols=") {
+                string columns_str = curopt.substr(12);
+                date_columns = split(columns_str, ',');
+            }
             else if (curopt == "--skip-missings")
                 skip_missings = true;
             else if (curopt.substr(0,12) == "--precision=") {
@@ -586,14 +723,19 @@
         else if(ext == ".csv")
         {
             if (destination == "-.csv")
-                save_vmat_as_csv(vm, cout, skip_missings, precision, delimiter, true /*verbose*/,
-                                 convert_date);
-            else {
+                save_vmat_as_csv(vm, cout, skip_missings, precision, delimiter, true /*verbose*/, convert_date);
+            else
+            {
                 ofstream out(destination.c_str());
-                save_vmat_as_csv(vm, out, skip_missings, precision, delimiter, true /*verbose*/,
-                                 convert_date);
+                save_vmat_as_csv(vm, out, skip_missings, precision, delimiter, true /*verbose*/, convert_date);
             }
-        }else if(ext == ".vmat")
+        }
+        else if(ext == ".arff")
+        {
+            ofstream out(destination.c_str());
+            save_vmat_as_arff(vm, out, date_columns, skip_missings, precision);
+        }
+        else if(ext == ".vmat")
             PLearn::save(destination,vm);
         else
         {



From tihocan at mail.berlios.de  Mon Aug 25 19:05:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 19:05:06 +0200
Subject: [Plearn-commits] r9394 - trunk/commands/PLearnCommands
Message-ID: <200808251705.m7PH56xS014553@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 19:05:05 +0200 (Mon, 25 Aug 2008)
New Revision: 9394

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
Log:
Added missing CR in help, and removed an extra one

Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2008-08-25 14:38:03 UTC (rev 9393)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2008-08-25 17:05:05 UTC (rev 9394)
@@ -81,9 +81,9 @@
         "                     Option to enable logging for the specified modules,\n"
         "                     specified a comma-separated list of modules (without spaces).\n"
         "                     Special keywords __ALL__ and __NONE__ can be specified to log\n"
-        "                     for all modules or no modules respectively."
+        "                     for all modules or no modules respectively.\n"
         "                 --servers\n"
-        "                 --global-calendars\n"
+        "                 --global-calendars"
          << endl;
 }
 



From tihocan at mail.berlios.de  Mon Aug 25 19:36:03 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 19:36:03 +0200
Subject: [Plearn-commits] r9395 - trunk/plearn/vmat
Message-ID: <200808251736.m7PHa3cj019842@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 19:36:02 +0200 (Mon, 25 Aug 2008)
New Revision: 9395

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Removed alternative to -1 to specify unknown mtime, to avoid platform-dependent scripts

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-25 17:05:05 UTC (rev 9394)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-25 17:36:02 UTC (rev 9395)
@@ -159,7 +159,8 @@
         OptionBase::buildoption|OptionBase::nosave,
         "DO NOT play with this if you don't know the implementation!\n"
         "This add a dependency mtime to the gived value.\n"
-        " -1 or "+tostring(numeric_limits<time_t>::max())+" set permannetly that we don't know the mtime.");
+        "Use -1 to set permanently that we do not know the mtime (-1 will be\n"
+        "converted to the maximum value of time_t at runtime).");
 
     declareOption(
         ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::buildoption,



From tihocan at mail.berlios.de  Mon Aug 25 20:01:20 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:01:20 +0200
Subject: [Plearn-commits] r9396 - trunk/plearn/vmat
Message-ID: <200808251801.m7PI1KdI006438@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:01:19 +0200 (Mon, 25 Aug 2008)
New Revision: 9396

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
Replaced cerr by perr

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-25 17:36:02 UTC (rev 9395)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-08-25 18:01:19 UTC (rev 9396)
@@ -104,7 +104,7 @@
 
 void TextFilesVMatrix::buildIdx()
 {
-    cerr << "Building the index file. Please be patient..." << endl;
+    perr << "Building the index file. Please be patient..." << endl;
 
     if(idxfile)
         fclose(idxfile);
@@ -205,7 +205,7 @@
     fclose(logfile);
     fclose(idxfile);
 
-    cerr << "Index file built." << endl;
+    perr << "Index file built." << endl;
 }
 
 /////////////////////////////



From tihocan at mail.berlios.de  Mon Aug 25 20:08:27 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:08:27 +0200
Subject: [Plearn-commits] r9397 - trunk/plearn/vmat
Message-ID: <200808251808.m7PI8ROZ006990@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:08:27 +0200 (Mon, 25 Aug 2008)
New Revision: 9397

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
Log:
Does not modify build option 'bag_index' (safer is somehow the source VMat is modified later)

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-08-25 18:01:19 UTC (rev 9396)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-08-25 18:08:27 UTC (rev 9397)
@@ -90,7 +90,7 @@
                   &ReplicateSamplesVMatrix::bag_index,
                   OptionBase::buildoption,
         "Index of the target corresponding to the bag information (useful\n"
-        "only when operate_on_bags is True). -1 means the last element.\n");
+        "only when operate_on_bags is True). -1 means the last element.");
 
     declareOption(ol, "seed", &ReplicateSamplesVMatrix::seed,
                   OptionBase::buildoption,
@@ -131,9 +131,7 @@
     updateMtime(indices_vmat);
     updateMtime(source);
 
-    if (bag_index < 0)
-        bag_index = source->targetsize()-1;
-    PLASSERT(bag_index < source->targetsize());
+    PLASSERT(bag_index < 0 || bag_index < source->targetsize());
 
     // Build the vector of indices.
     indices.resize(0);
@@ -142,6 +140,7 @@
     TVec< TVec<int>  > class_indices;  // Indices of samples in each class.
     map<int, int> bag_sizes; // Map a source index to the size of its bag.
     int bag_start_idx = -1;
+    int bag_idx = bag_index >= 0 ? bag_index : source->targetsize() - 1;
     for (int i = 0; i < source->length(); i++) {
         source->getExample(i, input, target, weight);
         int c = int(round(target[0]));
@@ -151,7 +150,7 @@
                 class_indices.append(TVec<int>());
         }
         
-        if (!operate_on_bags || int(round(target[bag_index])) &
+        if (!operate_on_bags || int(round(target[bag_idx])) &
                                 SumOverBagsVariable::TARGET_COLUMN_FIRST) {
             class_indices[c].append(i);
             indices.append(i);



From tihocan at mail.berlios.de  Mon Aug 25 20:28:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:28:01 +0200
Subject: [Plearn-commits] r9398 - trunk/plearn/vmat
Message-ID: <200808251828.m7PIS1kJ008824@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:28:01 +0200 (Mon, 25 Aug 2008)
New Revision: 9398

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
- Fixed indent
- Added documentation about new parameter 'extra_msg' in 'compatibleSizeError'
- Changed this parameter from 'string' to 'const string&' to save string copy and be consistent with the rest of PLearn code


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-25 18:08:27 UTC (rev 9397)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-25 18:28:01 UTC (rev 9398)
@@ -1375,8 +1375,14 @@
         || this->extrasize()  != m->extrasize() );
 }
 
-void VMatrix::compatibleSizeError(const VMat& m, string extra_msg){
-#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::compatibleSizeError - in class %s The matrix are not compatible!\n m1."#NAME"=%d and m2."#NAME"=%d. %s", classname().c_str(), this->NAME(), m->NAME(), extra_msg.c_str());
+/////////////////////////
+// compatibleSizeError //
+/////////////////////////
+void VMatrix::compatibleSizeError(const VMat& m, const string& extra_msg) {
+#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::compatibleSizeError " \
+        " - in class %s - The matrices are not compatible!\n"               \
+        "m1."#NAME"=%d and m2."#NAME"=%d. %s",                              \
+        classname().c_str(), this->NAME(), m->NAME(), extra_msg.c_str());
 
     if(this->width()      != m->width())
         MY_PRINT_ERROR_MST(width)

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-08-25 18:08:27 UTC (rev 9397)
+++ trunk/plearn/vmat/VMatrix.h	2008-08-25 18:28:01 UTC (rev 9398)
@@ -273,9 +273,11 @@
     /// width and length.
     bool looksTheSameAs(const VMat& m);
 
-    /// generate an PLERROR iif it don't looks like the same matrix,
-    /// i.e. it has same sizes and width.
-    void compatibleSizeError(const VMat& m, string extra_msg = "");
+    //! Generate a PLERROR iff 'm' does not look like the same matrix,
+    //! i.e. it does not have same sizes and width.
+    //! If an 'extra_msg' is provided, this message is appended to the error
+    //! message displayed when there is a size mismatch.
+    void compatibleSizeError(const VMat& m, const string& extra_msg = "");
 
     /**
      *  This should be called by the build method of every VMatrix that has a



From tihocan at mail.berlios.de  Mon Aug 25 20:29:20 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:29:20 +0200
Subject: [Plearn-commits] r9399 - trunk/plearn/base
Message-ID: <200808251829.m7PITKEx008887@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:29:20 +0200 (Mon, 25 Aug 2008)
New Revision: 9399

Modified:
   trunk/plearn/base/Object.cc
Log:
Fixed typo in error message (and hopefully made message more explicit at the same time)

Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2008-08-25 18:28:01 UTC (rev 9398)
+++ trunk/plearn/base/Object.cc	2008-08-25 18:29:20 UTC (rev 9399)
@@ -288,8 +288,8 @@
 
     // There are bigger problems in the world but still it isn't always funny
     if(optionname.empty())
-        PLERROR("There is no option named \"%s\" in a \"%s\". Meaby we forget an ')'?",
-                optionname.c_str(),classname().c_str());
+        PLERROR("Found an empty option name in a \"%s\". "
+                "Maybe you forgot a ')'?", classname().c_str());
     PLERROR("There is no option named \"%s\" in a \"%s\"",
             optionname.c_str(),classname().c_str());
 }



From tihocan at mail.berlios.de  Mon Aug 25 20:29:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:29:50 +0200
Subject: [Plearn-commits] r9400 - trunk/plearn_learners/online
Message-ID: <200808251829.m7PIToep008925@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:29:50 +0200 (Mon, 25 Aug 2008)
New Revision: 9400

Modified:
   trunk/plearn_learners/online/ClassErrorCostModule.cc
Log:
Added missing deep copy statement

Modified: trunk/plearn_learners/online/ClassErrorCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ClassErrorCostModule.cc	2008-08-25 18:29:20 UTC (rev 9399)
+++ trunk/plearn_learners/online/ClassErrorCostModule.cc	2008-08-25 18:29:50 UTC (rev 9400)
@@ -81,18 +81,23 @@
         PLASSERT(error_costs.length() == input_size);   
 }
 
+///////////
+// build //
+///////////
 void ClassErrorCostModule::build()
 {
     inherited::build();
     build_();
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void ClassErrorCostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // deepCopyField(trainvec, copies);
+    deepCopyField(error_costs, copies);
 }
 
 ///////////
@@ -105,6 +110,9 @@
     fprop( input, target, cost[0] );
 }
 
+///////////
+// fprop //
+///////////
 void ClassErrorCostModule::fprop(const Vec& input, const Vec& target,
                                  real& cost) const
 {



From tihocan at mail.berlios.de  Mon Aug 25 20:30:27 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:30:27 +0200
Subject: [Plearn-commits] r9401 - trunk/plearn_learners/generic
Message-ID: <200808251830.m7PIURnE009123@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:30:23 +0200 (Mon, 25 Aug 2008)
New Revision: 9401

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
Added documentation on new option 'add_sub_learner_costs'

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2008-08-25 18:29:50 UTC (rev 9400)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2008-08-25 18:30:23 UTC (rev 9401)
@@ -176,9 +176,13 @@
     virtual void forget();
 
     //! Computes our and from the sub_learner costs from already computed output. 
+    //! If 'add_sub_learner_costs' is true, then the underlying learner will be
+    //! used to compute its own cost on the given input, output and target.
+    //! Otherwise, these costs are not computed (they are assumed to be already
+    //! given in the first part of the 'costs' vector).
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                          const Vec& target, Vec& costs,
-                                         const bool add_sub_learner_costs) const;
+                                         bool add_sub_learner_costs) const;
 
     //! Computes our and from the sub_learner costs from already computed output. 
     void computeCostsFromOutputs(const Vec& input, const Vec& output, 



From tihocan at mail.berlios.de  Mon Aug 25 20:31:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Aug 2008 20:31:50 +0200
Subject: [Plearn-commits] r9402 - in trunk: commands/PLearnCommands
	plearn/sys
Message-ID: <200808251831.m7PIVo1D009227@sheep.berlios.de>

Author: tihocan
Date: 2008-08-25 20:31:49 +0200 (Mon, 25 Aug 2008)
New Revision: 9402

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/plearn/sys/Profiler.cc
   trunk/plearn/sys/Profiler.h
Log:
Renamed disactivate into deactivate, which is the proper English word

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2008-08-25 18:30:23 UTC (rev 9401)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2008-08-25 18:31:49 UTC (rev 9402)
@@ -353,7 +353,7 @@
 
     if(findpos( command_line, "--profile" )!=-1){
         Profiler::pl_profile_end("Prog");
-        Profiler::pl_profile_disactivate();
+        Profiler::pl_profile_deactivate();
         Profiler::pl_profile_report(cerr);
         Profiler::pl_profile_reportwall(cerr);
     }

Modified: trunk/plearn/sys/Profiler.cc
===================================================================
--- trunk/plearn/sys/Profiler.cc	2008-08-25 18:30:23 UTC (rev 9401)
+++ trunk/plearn/sys/Profiler.cc	2008-08-25 18:31:49 UTC (rev 9402)
@@ -127,9 +127,9 @@
 // call Profiler::activate if PL_PROFILE is set
 void Profiler::pl_profile_activate(){
     Profiler::activate();}
-// call Profiler::disactivate if PL_PROFILE is set
-void Profiler::pl_profile_disactivate(){
-    Profiler::disactivate();}
+// call Profiler::deactivate if PL_PROFILE is set
+void Profiler::pl_profile_deactivate(){
+    Profiler::deactivate();}
 // call Profiler::report if PL_PROFILE is set
 void Profiler::pl_profile_report(ostream& out){
     Profiler::report(out);}

Modified: trunk/plearn/sys/Profiler.h
===================================================================
--- trunk/plearn/sys/Profiler.h	2008-08-25 18:30:23 UTC (rev 9401)
+++ trunk/plearn/sys/Profiler.h	2008-08-25 18:31:49 UTC (rev 9402)
@@ -134,7 +134,7 @@
     }
 
     //! Disable profiling
-    static void disactivate() { active=false; }
+    static void deactivate() { active=false; }
 
     //! Return activation status
     static bool isActive() { return active; }
@@ -175,11 +175,11 @@
     static inline void pl_profile_activate() {}
 #endif
 
-    //!  call disactivate() if if PL_PROFILE is set
+    //!  call deactivate() if if PL_PROFILE is set
 #if defined(PROFILE) && defined(PL_PROFILE)
-    static void pl_profile_disactivate();
+    static void pl_profile_deactivate();
 #else
-    static inline void pl_profile_disactivate() {}
+    static inline void pl_profile_deactivate() {}
 #endif
 
     //!  call report() if if PL_PROFILE is set



From tihocan at mail.berlios.de  Tue Aug 26 16:59:59 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Aug 2008 16:59:59 +0200
Subject: [Plearn-commits] r9403 - trunk/plearn/vmat
Message-ID: <200808261459.m7QExxqQ001465@sheep.berlios.de>

Author: tihocan
Date: 2008-08-26 16:59:59 +0200 (Tue, 26 Aug 2008)
New Revision: 9403

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Fred just told me that time_t may be signed on some systems

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-25 18:31:49 UTC (rev 9402)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-26 14:59:59 UTC (rev 9403)
@@ -159,8 +159,7 @@
         OptionBase::buildoption|OptionBase::nosave,
         "DO NOT play with this if you don't know the implementation!\n"
         "This add a dependency mtime to the gived value.\n"
-        "Use -1 to set permanently that we do not know the mtime (-1 will be\n"
-        "converted to the maximum value of time_t at runtime).");
+        "Use -1 to set permanently that we do not know the mtime.");
 
     declareOption(
         ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::buildoption,



From nouiz at mail.berlios.de  Wed Aug 27 16:38:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Aug 2008 16:38:17 +0200
Subject: [Plearn-commits] r9404 - trunk/plearn/vmat
Message-ID: <200808271438.m7REcH93008756@sheep.berlios.de>

Author: nouiz
Date: 2008-08-27 16:38:16 +0200 (Wed, 27 Aug 2008)
New Revision: 9404

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
bugfix for last modif


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-08-26 14:59:59 UTC (rev 9403)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-08-27 14:38:16 UTC (rev 9404)
@@ -180,7 +180,8 @@
 
     // Obtain meta information from source.
     setMetaInfoFromSource();
-    
+    if(values.size()==0)
+        setMetaDataDir(getMetaDataDir());
 }
 
 ////////////////////
@@ -189,6 +190,9 @@
 void GaussianizeVMatrix::setMetaDataDir(const PPath& the_metadatadir){
     inherited::setMetaDataDir(the_metadatadir);
 
+    if(features_to_gaussianize.size()==0)
+        return;
+
     VMat the_source = train_source ? train_source : source;
     
     //to save the stats their must be a metadatadir



From nouiz at mail.berlios.de  Wed Aug 27 16:55:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Aug 2008 16:55:45 +0200
Subject: [Plearn-commits] r9405 - trunk/plearn/vmat
Message-ID: <200808271455.m7REtjLi011528@sheep.berlios.de>

Author: nouiz
Date: 2008-08-27 16:55:44 +0200 (Wed, 27 Aug 2008)
New Revision: 9405

Modified:
   trunk/plearn/vmat/SelectRowsVMatrix.cc
Log:
added a warning


Modified: trunk/plearn/vmat/SelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-08-27 14:38:16 UTC (rev 9404)
+++ trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-08-27 14:55:44 UTC (rev 9405)
@@ -204,6 +204,9 @@
         selected_indices << indices;
     }
 
+    if(selected_indices.length()==source.length() && source.length()>0)
+        PLWARNING("In SelectRowsVMatrix::build_() - We select all row!");
+
     length_ = selected_indices.length();
     if (source) {
         string error_msg =



From nouiz at mail.berlios.de  Wed Aug 27 17:49:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Aug 2008 17:49:08 +0200
Subject: [Plearn-commits] r9406 - in trunk/plearn/vmat/test: . .pytest
	.pytest/PL_SelectRowsVMatrix/expected_results
	.pytest/PL_SelectRowsVMatrix2
	.pytest/PL_SelectRowsVMatrix2/expected_results
Message-ID: <200808271549.m7RFn8Yi016179@sheep.berlios.de>

Author: nouiz
Date: 2008-08-27 17:49:08 +0200 (Wed, 27 Aug 2008)
New Revision: 9406

Added:
   trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2/
   trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2/expected_results/
   trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2/expected_results/RUN.log
   trunk/plearn/vmat/test/selectrowsvmat2.vmat
Modified:
   trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log
   trunk/plearn/vmat/test/pytest.config
Log:
fixed one test from last commit of SelectRowsVMatrix and added a test


Modified: trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log	2008-08-27 14:55:44 UTC (rev 9405)
+++ trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix/expected_results/RUN.log	2008-08-27 15:49:08 UTC (rev 9406)
@@ -1,3 +1,4 @@
+ WARNING: In SelectRowsVMatrix::build_() - We select all row!
 2 x 3
 inputsize: 1
 targetsize: 0


Property changes on: trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2/expected_results/RUN.log	2008-08-27 14:55:44 UTC (rev 9405)
+++ trunk/plearn/vmat/test/.pytest/PL_SelectRowsVMatrix2/expected_results/RUN.log	2008-08-27 15:49:08 UTC (rev 9406)
@@ -0,0 +1,5 @@
+1 x 3
+inputsize: 1
+targetsize: 0
+weightsize: 2
+extrasize: 0

Modified: trunk/plearn/vmat/test/pytest.config
===================================================================
--- trunk/plearn/vmat/test/pytest.config	2008-08-27 14:55:44 UTC (rev 9405)
+++ trunk/plearn/vmat/test/pytest.config	2008-08-27 15:49:08 UTC (rev 9406)
@@ -108,6 +108,23 @@
     )
 
 Test(
+    name = "PL_SelectRowsVMatrix2",
+    description = "Test sizes of SelectRowsVMatrix",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "vmat info selectrowsvmat2.vmat",
+    resources = [ "selectrowsvmat2.vmat" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+Test(
     name = "PL_ProcessingVMatrixSizes",
     description = "Test that sizes from the source VMat can be acquired automatically",
     category = "General",

Added: trunk/plearn/vmat/test/selectrowsvmat2.vmat
===================================================================
--- trunk/plearn/vmat/test/selectrowsvmat2.vmat	2008-08-27 14:55:44 UTC (rev 9405)
+++ trunk/plearn/vmat/test/selectrowsvmat2.vmat	2008-08-27 15:49:08 UTC (rev 9406)
@@ -0,0 +1,13 @@
+SelectRowsVMatrix(
+  source =
+	MemoryVMatrix(
+		data = 2 3 [ 0 0 0 0 0 0 ]
+		inputsize = 2
+		targetsize = 1
+		weightsize = 0
+	) 
+  indices = [ 0 ]
+  inputsize =   1
+  targetsize =  0
+  weightsize =  2
+)



From nouiz at mail.berlios.de  Wed Aug 27 19:46:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Aug 2008 19:46:27 +0200
Subject: [Plearn-commits] r9407 - trunk/plearn/vmat
Message-ID: <200808271746.m7RHkRxL016589@sheep.berlios.de>

Author: nouiz
Date: 2008-08-27 19:46:26 +0200 (Wed, 27 Aug 2008)
New Revision: 9407

Modified:
   trunk/plearn/vmat/SelectRowsVMatrix.cc
   trunk/plearn/vmat/SelectRowsVMatrix.h
Log:
added an option to SelectRowsVMatrix to remove warning. This correct one test.


Modified: trunk/plearn/vmat/SelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-08-27 15:49:08 UTC (rev 9406)
+++ trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-08-27 17:46:26 UTC (rev 9407)
@@ -56,14 +56,16 @@
       obtained_targetsize_from_source(false),
       obtained_weightsize_from_source(false),
       obtained_extrasize_from_source(false),
+      warn_if_all_rows_selected(true),
       rows_to_remove(false)
 {}
 
-SelectRowsVMatrix::SelectRowsVMatrix(VMat the_source, TVec<int> the_indices, bool the_rows_to_remove)
+SelectRowsVMatrix::SelectRowsVMatrix(VMat the_source, TVec<int> the_indices, bool the_rows_to_remove, bool warn)
     : obtained_inputsize_from_source(false),
       obtained_targetsize_from_source(false),
       obtained_weightsize_from_source(false),
       obtained_extrasize_from_source(false),
+      warn_if_all_rows_selected(warn),
       indices(the_indices),
       rows_to_remove(the_rows_to_remove)
 {
@@ -72,11 +74,12 @@
 }
 
 //! Here the indices will be copied locally into an integer vector
-SelectRowsVMatrix::SelectRowsVMatrix(VMat the_source, Vec the_indices, bool the_rows_to_remove)
+SelectRowsVMatrix::SelectRowsVMatrix(VMat the_source, Vec the_indices, bool the_rows_to_remove, bool warn)
     : obtained_inputsize_from_source(false),
       obtained_targetsize_from_source(false),
       obtained_weightsize_from_source(false),
       obtained_extrasize_from_source(false),
+      warn_if_all_rows_selected(warn),
       rows_to_remove(the_rows_to_remove)
 {
     source = the_source;
@@ -135,6 +138,9 @@
     declareOption(ol, "obtained_extrasize_from_source", &SelectRowsVMatrix::obtained_extrasize_from_source, OptionBase::learntoption,
                   "Set to 1 if the extrasize was obtained from the source VMat.");
 
+    declareOption(ol, "warn_if_all_rows_selected", &SelectRowsVMatrix::warn_if_all_rows_selected, OptionBase::buildoption,
+                  "If true, we generate a warning if we select all row.");
+
     inherited::declareOptions(ol);
 
     // Hide unused options.
@@ -203,8 +209,8 @@
         selected_indices.resize(indices.length());
         selected_indices << indices;
     }
-
-    if(selected_indices.length()==source.length() && source.length()>0)
+    //we don't display the warning for SortRowsVMatrix as it always select all row!
+    if(warn_if_all_rows_selected && selected_indices.length()==source.length() && source.length()>0)
         PLWARNING("In SelectRowsVMatrix::build_() - We select all row!");
 
     length_ = selected_indices.length();

Modified: trunk/plearn/vmat/SelectRowsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.h	2008-08-27 15:49:08 UTC (rev 9406)
+++ trunk/plearn/vmat/SelectRowsVMatrix.h	2008-08-27 17:46:26 UTC (rev 9407)
@@ -67,6 +67,8 @@
     bool obtained_weightsize_from_source;
     bool obtained_extrasize_from_source;
 
+    bool warn_if_all_rows_selected;
+
     TVec<int> selected_indices;
 
 public:
@@ -88,10 +90,10 @@
 
     //! Also copies the original fieldinfos upon construction
     //! Here the indices will be shared for efficiency. But you should not modify them afterwards!
-    SelectRowsVMatrix(VMat the_source, TVec<int> the_indices, bool the_rows_to_remove = false);
+    SelectRowsVMatrix(VMat the_source, TVec<int> the_indices, bool the_rows_to_remove = false, bool warn = true);
 
     //! Here the indices will be copied locally into an integer vector
-    SelectRowsVMatrix(VMat the_source, Vec the_indices, bool the_rows_to_remove = false);
+    SelectRowsVMatrix(VMat the_source, Vec the_indices, bool the_rows_to_remove = false, bool warn = true);
 
     PLEARN_DECLARE_OBJECT(SelectRowsVMatrix);
 



From nouiz at mail.berlios.de  Wed Aug 27 19:47:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Aug 2008 19:47:26 +0200
Subject: [Plearn-commits] r9408 - trunk/plearn/vmat
Message-ID: <200808271747.m7RHlQMJ016702@sheep.berlios.de>

Author: nouiz
Date: 2008-08-27 19:47:25 +0200 (Wed, 27 Aug 2008)
New Revision: 9408

Modified:
   trunk/plearn/vmat/RepeatSplitter.cc
   trunk/plearn/vmat/SortRowsVMatrix.cc
Log:
removed useless warning that made some test fail.


Modified: trunk/plearn/vmat/RepeatSplitter.cc
===================================================================
--- trunk/plearn/vmat/RepeatSplitter.cc	2008-08-27 17:46:26 UTC (rev 9407)
+++ trunk/plearn/vmat/RepeatSplitter.cc	2008-08-27 17:47:25 UTC (rev 9408)
@@ -307,7 +307,8 @@
         int shuffle_indice = k / child_splits;
         if (shuffle_indice != last_n) {
             // We have to reshuffle the dataset, according to indices.
-            VMat m = new SelectRowsVMatrix(dataset, indices(shuffle_indice));
+            VMat m = new SelectRowsVMatrix(dataset, indices(shuffle_indice),
+                                           false, false);
             to_repeat->setDataSet(m);
             last_n = shuffle_indice;
         }

Modified: trunk/plearn/vmat/SortRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SortRowsVMatrix.cc	2008-08-27 17:46:26 UTC (rev 9407)
+++ trunk/plearn/vmat/SortRowsVMatrix.cc	2008-08-27 17:47:25 UTC (rev 9408)
@@ -60,7 +60,9 @@
 SortRowsVMatrix::SortRowsVMatrix():
     ignore_missing_fields(false),
     increasing_order(true)
-{}
+{
+    warn_if_all_rows_selected = false;
+}
 
 ////////////////////
 // declareOptions //



From larocheh at mail.berlios.de  Wed Aug 27 20:28:35 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 27 Aug 2008 20:28:35 +0200
Subject: [Plearn-commits] r9409 - trunk/plearn_learners/online
Message-ID: <200808271828.m7RISZlB026836@sheep.berlios.de>

Author: larocheh
Date: 2008-08-27 20:28:35 +0200 (Wed, 27 Aug 2008)
New Revision: 9409

Modified:
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
Log:
Implemented expectation_is_not_up_to_date()



Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-08-27 17:47:25 UTC (rev 9408)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-08-27 18:28:35 UTC (rev 9409)
@@ -189,7 +189,14 @@
     }
 }
 
+void RBMMixedLayer::expectation_is_not_up_to_date()
+{
+    for( int i=0 ; i<n_layers ; i++ )
+        sub_layers[i]->expectation_is_not_up_to_date();
 
+    expectation_is_up_to_date = false;
+}
+
 ////////////////////
 // generateSample //
 ////////////////////

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2008-08-27 17:47:25 UTC (rev 9408)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2008-08-27 18:28:35 UTC (rev 9409)
@@ -104,6 +104,9 @@
     virtual void getAllActivations( PP<RBMConnection> rbmc, int offset=0,
                                     bool minibatch = false );
 
+    //change the flag of expectation_is_up_to_date to false
+    virtual void expectation_is_not_up_to_date();
+
     //! compute a sample, and update the sample field
     virtual void generateSample();
 



From larocheh at mail.berlios.de  Wed Aug 27 20:29:47 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 27 Aug 2008 20:29:47 +0200
Subject: [Plearn-commits] r9410 - trunk/plearn_learners_experimental
Message-ID: <200808271829.m7RITl26026965@sheep.berlios.de>

Author: larocheh
Date: 2008-08-27 20:29:46 +0200 (Wed, 27 Aug 2008)
New Revision: 9410

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added CD for sparse inputs and corrected bug with sparse input classification and pseudolikelihood which wasn't using the hidden layer bias...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-27 18:28:35 UTC (rev 9409)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-27 18:29:46 UTC (rev 9410)
@@ -73,6 +73,7 @@
     input_is_sparse( false ),
     factorized_connection_rank( -1 ),
     n_selected_inputs_pseudolikelihood( -1 ),
+    n_selected_inputs_cd( -1 ),
     //select_among_k_most_frequent( -1 ),
     compute_input_space_nll( false ),
     pseudolikelihood_context_size ( 0 ),
@@ -184,6 +185,15 @@
                   "This option is ignored for pseudolikelihood_context_size > 0.\n"
                   );    
 
+    declareOption(ol, "n_selected_inputs_cd", 
+                  &PseudolikelihoodRBM::n_selected_inputs_cd,
+                  OptionBase::buildoption,
+                  "Number of randomly selected inputs for CD in sparse "
+                  "input case.\n"
+                  "Note that CD for sparse inputs assumes RBMBinomialLayer in "
+                  "input.\n"
+                  );    
+
     //declareOption(ol, "select_among_k_most_frequent", 
     //              &PseudolikelihoodRBM::select_among_k_most_frequent,
     //              OptionBase::buildoption,
@@ -465,6 +475,11 @@
         }
         input_is_active.resize( inputsize() );
         input_is_active.clear();
+        hidden_act_non_selected.resize( hidden_layer->size );
+        // CD option
+        pos_hidden.resize( hidden_layer->size );
+        pos_input_sparse.resize( input_layer->size );
+        pos_input_sparse.clear();
     }
     else
     {
@@ -624,6 +639,9 @@
     deepCopyField(V_gradients, copies);
     deepCopyField(input_is_active, copies);
     deepCopyField(input_indices, copies);
+    deepCopyField(input_is_selected, copies);
+    deepCopyField(hidden_act_non_selected, copies);
+    deepCopyField(pos_input_sparse, copies);
     deepCopyField(persistent_gibbs_chain_is_started, copies);
 }
 
@@ -928,6 +946,7 @@
                         input_is_active[(int)extra[i]] = true;
                     }
                 }
+                hidden_act += hidden_layer->bias;
             }
             else
             {
@@ -1184,6 +1203,7 @@
                 //                    input_is_active[(int)extra[i]] = true;
                 //                }
                 //            }
+                //            hidden_act += hidden_layer->bias;
                 //        }
                 //        else
                 //        {
@@ -1315,6 +1335,7 @@
                 //                            input_is_active[(int)extra[i]] = true;
                 //                        }
                 //                    }
+                //                    hidden_act += hidden_layer->bias;
                 //                }
                 //                else
                 //                {
@@ -1447,6 +1468,7 @@
                 //                    input_is_active[(int)extra[i]] = true;
                 //                }
                 //            }
+                //            hidden_act += hidden_layer->bias;
                 //        }
                 //        else
                 //        {
@@ -1578,6 +1600,7 @@
                 //                            input_is_active[(int)extra[i]] = true;
                 //                        }
                 //                    }
+                //                    hidden_act += hidden_layer->bias;
                 //                }
                 //                else
                 //                {
@@ -1715,6 +1738,7 @@
                 //                    input_is_active[(int)extra[i]] = true;
                 //                }
                 //            }
+                //            hidden_act += hidden_layer->bias;
                 //        }
                 //        else
                 //        {
@@ -1846,6 +1870,7 @@
                 //                            input_is_active[(int)extra[i]] = true;
                 //                        }
                 //                    }
+                //                    hidden_act += hidden_layer->bias;
                 //                }
                 //                else
                 //                {
@@ -1951,7 +1976,6 @@
                 //    }
                 //}
 
-
                 // Compute activations
                 if( input_is_sparse )
                 {
@@ -1977,6 +2001,7 @@
                             input_is_active[(int)extra[i]] = true;
                         }
                     }
+                    hidden_act += hidden_layer->bias;
                 }
                 else
                 {
@@ -2741,9 +2766,193 @@
             (targetsize() == 0 || generative_learning_weight > 0) )
         {
             if( input_is_sparse )
+            {
                 PLERROR("In PseudolikelihoodRBM::train(): CD is not implemented "
                         "for sparse inputs");
 
+                // Randomly select inputs
+                if( n_selected_inputs_cd > inputsize() &&
+                    n_selected_inputs_cd <= 0 )
+                    PLERROR("In PseudolikelihoodRBM::train(): "
+                            "n_selected_inputs_cd should be > 0 and "
+                            "<= inputsize()" );
+
+                if ( input_indices.length() == 0 )
+                {
+                    input_indices.resize(inputsize());
+                    for( int i=0; i<input_indices.length(); i++ )
+                        input_indices[i] = i;
+                        
+                }
+                 
+                // Randomly selected inputs
+                int tmp;
+                int k;
+                for (int j = 0; j < n_selected_inputs_cd; j++) 
+                {
+                    k = j + 
+                        random_gen->uniform_multinomial_sample(
+                            inputsize() - j);
+                        
+                    tmp = input_indices[j];
+                    input_indices[j] = input_indices[k];
+                    input_indices[k] = tmp;
+                }
+
+                if( factorized_connection_rank > 0 )
+                    PLERROR("In PseudolikelihoodRBM::train(): factorized "
+                            "connection is not implemented for CD and "
+                            "sparse inputs" );
+
+                if( !fast_exact_is_equal(persistent_cd_weight, 0) )
+                    PLERROR("In PseudolikelihoodRBM::train(): persistent CD "
+                            "cannot be used for sparse inputs" );
+
+                if( use_mean_field_cd )
+                    PLERROR("In PseudolikelihoodRBM::train(): MF-CD "
+                            "is not implemented for sparse inputs" );
+
+                if( !fast_exact_is_equal(cd_decrease_ct, 0) )
+                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                else
+                    lr = cd_learning_rate;
+
+                if( targetsize() > 0 )
+                    lr *= generative_learning_weight;
+
+                setLearningRate(lr);
+
+                // Positive phase
+                if( targetsize() > 0 )
+                    pos_target = target_one_hot;
+
+                Vec hidden_act = hidden_layer->activation;
+                hidden_act.clear();
+                hidden_act_non_selected.clear();
+                train_set->getExtra(stage%nsamples,extra);
+                input_is_selected.resize( extra.length() );
+                input_is_selected.clear();
+                for( int i=0; i<extra.length(); i++ )
+                {
+                    hidden_act += V((int)extra[i]);
+                    if( input_indices.subVec(0,n_selected_inputs_cd).find((int)extra[i]) >= 0 )
+                    {
+                        input_is_selected[i] = true;
+                        pos_input_sparse[(int)extra[i]] = 1;
+                    }
+                    else
+                        hidden_act_non_selected += V((int)extra[i]);                        
+                }
+                hidden_act += hidden_layer->bias;
+                hidden_act_non_selected += hidden_layer->bias;
+
+                if( targetsize() == 1 )
+                    productAcc( hidden_layer->activation,
+                                target_connection->weights,
+                                target_one_hot );
+                else if( targetsize() > 1 )
+                    productAcc( hidden_layer->activation,
+                                target_connection->weights,
+                                target );
+
+                hidden_layer->expectation_is_not_up_to_date();
+                hidden_layer->computeExpectation();
+                //pos_hidden.resize( hidden_layer->size );
+                pos_hidden << hidden_layer->expectation;
+                    
+                // Negative phase
+                real *w;
+                Vec input_act = input_layer->activation;
+                Vec input_sample = input_layer->sample;
+                Vec hidden_sample = hidden_layer->sample;
+                int in;
+                for(int i=0; i<cd_n_gibbs; i++)
+                {
+                    // Down pass
+                    hidden_layer->generateSample();
+                    for (int j = 0; j < n_selected_inputs_cd; j++) 
+                    {
+                        in = input_indices[j];
+                        w = V[in];
+                        input_act[in] = input_layer->bias[in];
+                        for( int k=0; k<hidden_layer->size; k++ )
+                            input_act[in] += w[k] * hidden_sample[k];
+                        
+                        if( input_layer->use_fast_approximations )
+                        {
+                            input_sample[in] = random_gen->binomial_sample(
+                                fastsigmoid( input_act[in] ));
+                        }
+                        else
+                        {
+                            input_sample[in] = random_gen->binomial_sample(
+                                fastsigmoid( input_act[in] ));
+                        }
+                    }
+
+                    // Up pass
+                    hidden_act << hidden_act_non_selected;
+                    for (int j = 0; j < n_selected_inputs_cd; j++) 
+                    {
+                        in = input_indices[j];
+                        if( fast_exact_is_equal(input_sample[in], 1) )
+                            hidden_act += V(in);
+                    }
+
+                    if( targetsize() > 0 )
+                    {
+                        // Down-up pass for target
+                        target_connection->setAsUpInput( 
+                            hidden_layer->sample );
+                        target_layer->getAllActivations( 
+                            (RBMMatrixConnection*) target_connection );
+                        target_layer->computeExpectation();
+                        target_layer->generateSample();
+                        productAcc( hidden_act,
+                                    target_connection->weights,
+                                    target_layer->sample );
+                    }
+                    
+                    hidden_layer->expectation_is_not_up_to_date();
+                    hidden_layer->computeExpectation();
+                }
+
+                neg_hidden = hidden_layer->expectation;
+                    
+                hidden_layer->update(pos_hidden,neg_hidden);
+                if( targetsize() > 0 )
+                {
+                    neg_target = target_layer->sample;
+                    target_layer->update(pos_target,neg_target);
+                    target_connection->update(pos_target,pos_hidden,
+                                              neg_target,neg_hidden);
+                }
+
+                // Selected inputs connection update
+                for (int j = 0; j < n_selected_inputs_cd; j++) 
+                {
+                    in = input_indices[j];
+                    w = V[in];
+                    for( int k=0; k<hidden_layer->size; k++ )
+                        w[k] += lr * (pos_hidden[k] * pos_input_sparse[in] - 
+                                    neg_hidden[k] * input_sample[in]);
+                    input_layer->bias[in] += lr * ( pos_input_sparse[in] - 
+                                                    input_sample[in]);
+                }
+                
+                // Non-selected inputs connection update
+                hidden_activation_gradient << neg_hidden;
+                hidden_activation_gradient -= pos_hidden;
+                hidden_activation_gradient *= -lr;
+                for( int i=0; i<extra.length(); i++ )
+                {
+                    if( input_is_selected[i] = true )
+                        pos_input_sparse[(int)extra[i]] = 0;
+                    else
+                        V((int)extra[i]) += hidden_activation_gradient;
+                }
+            }
+            
             if( !fast_exact_is_equal(persistent_cd_weight, 1.) )
             {
                 if( !fast_exact_is_equal(cd_decrease_ct, 0) )
@@ -3163,7 +3372,8 @@
                 for( int e=0; e<extra.length(); e++ )
                     hidden_act += V((int)extra[e]);
             }
-            
+            hidden_act += hidden_layer->bias;
+
             for( int i=0 ; i<target_layer->size ; i++ )
             {
                 target_act[i] = target_layer->bias[i];

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-08-27 18:28:35 UTC (rev 9409)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-08-27 18:29:46 UTC (rev 9410)
@@ -118,6 +118,9 @@
     //! Number of randomly selected inputs for pseudolikelihood cost
     int n_selected_inputs_pseudolikelihood;
 
+    //! Number of randomly selected inputs for CD in sparse input case
+    int n_selected_inputs_cd;
+
     ////! Indication that inputs for pseudolikelihood cost are selected among the
     ////! k most frequently active inputs
     //int select_among_k_most_frequent;
@@ -315,6 +318,9 @@
     Mat V_gradients;
     TVec<bool> input_is_active;
     TVec<int> input_indices;
+    TVec<bool> input_is_selected;
+    Vec hidden_act_non_selected;
+    Vec pos_input_sparse;
 
     //! Keeps the index of the NLL cost in train_costs
     int nll_cost_index;



From tihocan at mail.berlios.de  Fri Aug 29 17:02:14 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 29 Aug 2008 17:02:14 +0200
Subject: [Plearn-commits] r9411 - trunk/plearn_learners/generic
Message-ID: <200808291502.m7TF2EWC000386@sheep.berlios.de>

Author: tihocan
Date: 2008-08-29 17:02:14 +0200 (Fri, 29 Aug 2008)
New Revision: 9411

Modified:
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/PLearner.h
Log:
Fixed remote version of computeOutputs

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-08-27 18:29:46 UTC (rev 9410)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-08-29 15:02:14 UTC (rev 9411)
@@ -379,11 +379,11 @@
          RetDoc ("Computed output (will have width outputsize)")));
 
     declareMethod(
-        rmm, "computeOutputs", &PLearner::computeOutputs,
+        rmm, "computeOutputs", &PLearner::remote_computeOutputs,
         (BodyDoc("On a trained learner, this computes the output from the input, one\n"
                  "batch of examples at a time (one example per row of the arg. matrices.\n"),
          ArgDoc ("inputs", "Input matrix (batch_size x inputsize)"),
-         ArgDoc ("outputs", "Resulting output matrix (batch_size x outputsize)")));
+         RetDoc ("Resulting output matrix (batch_size x outputsize)")));
 
     declareMethod(
         rmm, "use", &PLearner::remote_use,
@@ -1328,6 +1328,16 @@
     return tmp_output;
 }
 
+///////////////////////////
+// remote_computeOutputs //
+///////////////////////////
+Mat PLearner::remote_computeOutputs(const Mat& input) const
+{
+    Mat out(input.length(), outputsize() >= 0 ? outputsize() : 0);
+    computeOutputs(input, out);
+    return out;
+}
+
 ///////////////////////////////////
 // remote_computeOutputsAndCosts //
 ///////////////////////////////////

Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2008-08-27 18:29:46 UTC (rev 9410)
+++ trunk/plearn_learners/generic/PLearner.h	2008-08-29 15:02:14 UTC (rev 9411)
@@ -675,6 +675,7 @@
     // List of methods that are called by Remote Method Invocation.  Our
     // convention is to have them start with the remote_ prefix.
     Vec remote_computeOutput(const Vec& input) const;
+    Mat remote_computeOutputs(const Mat& input) const;
     pair<Mat, Mat> remote_computeOutputsAndCosts(const Mat& input,
                                                  const Mat& target) const;
     void remote_use(VMat inputs, string output_fname) const;



From larocheh at mail.berlios.de  Fri Aug 29 17:34:26 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 29 Aug 2008 17:34:26 +0200
Subject: [Plearn-commits] r9412 - trunk/plearn_learners/online
Message-ID: <200808291534.m7TFYQjx004258@sheep.berlios.de>

Author: larocheh
Date: 2008-08-29 17:34:25 +0200 (Fri, 29 Aug 2008)
New Revision: 9412

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:

Added option to corrupt input only during greedy layer-wise learning...



Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-08-29 15:02:14 UTC (rev 9411)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-08-29 15:34:25 UTC (rev 9412)
@@ -74,6 +74,7 @@
     unsupervised_nstages( 0 ),
     unsupervised_fine_tuning_learning_rate( 0. ),
     unsupervised_fine_tuning_decrease_ct( 0. ),
+    mask_input_layer_only( false ),
     mask_input_layer_only_in_unsupervised_fine_tuning( false ),
     train_stats_window( -1 ),
     n_layers( 0 ),
@@ -287,6 +288,12 @@
                   "The decrease constant of the learning rate used during\n"
                   "unsupervised fine tuning gradient descent.\n");
 
+    declareOption(ol, "mask_input_layer_only",
+                  &StackedAutoassociatorsNet::mask_input_layer_only,
+                  OptionBase::buildoption,
+                  "Indication that only the input layer should be masked\n"
+                  "during greedy layer-wise learning.\n");
+
     declareOption(ol, "mask_input_layer_only_in_unsupervised_fine_tuning",
                   &StackedAutoassociatorsNet::mask_input_layer_only_in_unsupervised_fine_tuning,
                   OptionBase::buildoption,
@@ -1219,6 +1226,12 @@
 void StackedAutoassociatorsNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer)
 {
     corrupted_input.resize(input.length());
+    if( mask_input_layer_only && layer != 0 )
+    {
+        corrupted_input << input; 
+        return;
+    }
+    
     if( noise_type == "masking_noise" )
     {
         if( probability_of_masked_inputs > 0 )
@@ -1515,6 +1528,9 @@
     // fprop
     expectations[0] << input;
 
+    bool old_mask_input_layer_only = mask_input_layer_only;
+    mask_input_layer_only = mask_input_layer_only_in_unsupervised_fine_tuning;
+
     if(correlation_connections.length() != 0)
     {
         
@@ -1638,6 +1654,8 @@
                 expectation_gradients[i], activation_gradients[i+1] );
         }
     }
+
+    mask_input_layer_only = old_mask_input_layer_only;
 }
 
 void StackedAutoassociatorsNet::unsupervisedFineTuningStep(const Mat& inputs,

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-08-29 15:02:14 UTC (rev 9411)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-08-29 15:34:25 UTC (rev 9412)
@@ -181,6 +181,10 @@
     real unsupervised_fine_tuning_decrease_ct;
 
     //! Indication that only the input layer should be masked
+    //! during greedy layer-wise learning
+    bool mask_input_layer_only;
+
+    //! Indication that only the input layer should be masked
     //! during unsupervised fine-tuning
     bool mask_input_layer_only_in_unsupervised_fine_tuning;
 



From chapados at mail.berlios.de  Fri Aug 29 19:55:53 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 29 Aug 2008 19:55:53 +0200
Subject: [Plearn-commits] r9413 - trunk/plearn/base
Message-ID: <200808291755.m7THtr8j009106@sheep.berlios.de>

Author: chapados
Date: 2008-08-29 19:55:53 +0200 (Fri, 29 Aug 2008)
New Revision: 9413

Modified:
   trunk/plearn/base/ParentableObject.cc
Log:
Fix for 64-bit clean

Modified: trunk/plearn/base/ParentableObject.cc
===================================================================
--- trunk/plearn/base/ParentableObject.cc	2008-08-29 15:34:25 UTC (rev 9412)
+++ trunk/plearn/base/ParentableObject.cc	2008-08-29 17:55:53 UTC (rev 9413)
@@ -170,12 +170,12 @@
         
     case UniqueParent:
         if (m_parent)         // Because of test above, m_parent != parent
-            PLERROR("ParentableObject::setParent: for object at 0x%x (%s),\n"
-                    "trying to override existing parent at 0x%x (%s) with\n"
-                    "new parent at 0x%x (%s) -- parenting mode set to UniqueParent.",
-                    this, classname().c_str(),
-                    m_parent, m_parent->classname().c_str(),
-                    parent, (parent? parent->classname().c_str() : "NULL"));
+            PLERROR("ParentableObject::setParent: for object at 0x%p (%s),\n"
+                    "trying to override existing parent at 0x%p (%s) with\n"
+                    "new parent at 0x%p (%s) -- parenting mode set to UniqueParent.",
+                    (void*)this, classname().c_str(),
+                    (void*)m_parent, m_parent->classname().c_str(),
+                    (void*)parent, (parent? parent->classname().c_str() : "NULL"));
         break;
     }
 



From larocheh at mail.berlios.de  Fri Aug 29 20:31:01 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 29 Aug 2008 20:31:01 +0200
Subject: [Plearn-commits] r9414 - trunk/plearn_learners_experimental
Message-ID: <200808291831.m7TIV1Y5013485@sheep.berlios.de>

Author: larocheh
Date: 2008-08-29 20:31:00 +0200 (Fri, 29 Aug 2008)
New Revision: 9414

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
Log:
Debugged sparse inputs CD option...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-29 17:55:53 UTC (rev 9413)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-08-29 18:31:00 UTC (rev 9414)
@@ -2767,11 +2767,8 @@
         {
             if( input_is_sparse )
             {
-                PLERROR("In PseudolikelihoodRBM::train(): CD is not implemented "
-                        "for sparse inputs");
-
                 // Randomly select inputs
-                if( n_selected_inputs_cd > inputsize() &&
+                if( n_selected_inputs_cd > inputsize() ||
                     n_selected_inputs_cd <= 0 )
                     PLERROR("In PseudolikelihoodRBM::train(): "
                             "n_selected_inputs_cd should be > 0 and "
@@ -2952,223 +2949,225 @@
                         V((int)extra[i]) += hidden_activation_gradient;
                 }
             }
-            
-            if( !fast_exact_is_equal(persistent_cd_weight, 1.) )
+            else
             {
-                if( !fast_exact_is_equal(cd_decrease_ct, 0) )
-                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
-                else
-                    lr = cd_learning_rate;
+                if( !fast_exact_is_equal(persistent_cd_weight, 1.) )
+                {
+                    if( !fast_exact_is_equal(cd_decrease_ct, 0) )
+                        lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                    else
+                        lr = cd_learning_rate;
 
-                if( targetsize() > 0 )
-                    lr *= generative_learning_weight;
+                    if( targetsize() > 0 )
+                        lr *= generative_learning_weight;
                     
-                lr *= (1-persistent_cd_weight);
+                    lr *= (1-persistent_cd_weight);
 
-                setLearningRate(lr);
+                    setLearningRate(lr);
 
-                // Positive phase
-                pos_input = input;
-                if( targetsize() > 0 )
-                    pos_target = target_one_hot;
-                connection->setAsDownInput( input );
-                hidden_layer->getAllActivations( 
-                    (RBMMatrixConnection*) connection );
-                if( targetsize() == 1 )
-                    productAcc( hidden_layer->activation,
-                                target_connection->weights,
-                                target_one_hot );
-                else if( targetsize() > 1 )
-                    productAcc( hidden_layer->activation,
-                                target_connection->weights,
-                                target );
+                    // Positive phase
+                    pos_input = input;
+                    if( targetsize() > 0 )
+                        pos_target = target_one_hot;
+                    connection->setAsDownInput( input );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    if( targetsize() == 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target_one_hot );
+                    else if( targetsize() > 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target );
                         
-                hidden_layer->computeExpectation();
-                //pos_hidden.resize( hidden_layer->size );
-                pos_hidden << hidden_layer->expectation;
+                    hidden_layer->computeExpectation();
+                    //pos_hidden.resize( hidden_layer->size );
+                    pos_hidden << hidden_layer->expectation;
                     
-                // Negative phase
-                for(int i=0; i<cd_n_gibbs; i++)
-                {
-                    if( use_mean_field_cd )
+                    // Negative phase
+                    for(int i=0; i<cd_n_gibbs; i++)
                     {
-                        connection->setAsUpInput( hidden_layer->expectation );
-                    }
-                    else
-                    {
-                        hidden_layer->generateSample();
-                        connection->setAsUpInput( hidden_layer->sample );
-                    }
-                    input_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-                    input_layer->computeExpectation();
-                    // LATERAL CONNECTIONS CODE HERE!
+                        if( use_mean_field_cd )
+                        {
+                            connection->setAsUpInput( hidden_layer->expectation );
+                        }
+                        else
+                        {
+                            hidden_layer->generateSample();
+                            connection->setAsUpInput( hidden_layer->sample );
+                        }
+                        input_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        input_layer->computeExpectation();
+                        // LATERAL CONNECTIONS CODE HERE!
 
-                    if( use_mean_field_cd )
-                    {
-                        connection->setAsDownInput( input_layer->expectation );
+                        if( use_mean_field_cd )
+                        {
+                            connection->setAsDownInput( input_layer->expectation );
+                        }
+                        else
+                        {
+                            input_layer->generateSample();
+                            connection->setAsDownInput( input_layer->sample );
+                        }
+
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+
+                        if( targetsize() > 0 )
+                        {
+                            if( use_mean_field_cd )
+                                target_connection->setAsUpInput( 
+                                    hidden_layer->expectation );
+                            else
+                                target_connection->setAsUpInput( 
+                                    hidden_layer->sample );
+                            target_layer->getAllActivations( 
+                                (RBMMatrixConnection*) target_connection );
+                            target_layer->computeExpectation();
+                            if( use_mean_field_cd )
+                                productAcc( hidden_layer->activation,
+                                            target_connection->weights,
+                                            target_layer->expectation );
+                            else
+                            {
+                                target_layer->generateSample();
+                                productAcc( hidden_layer->activation,
+                                            target_connection->weights,
+                                            target_layer->sample );
+                            }   
+                        }
+                        
+                        hidden_layer->computeExpectation();
                     }
+                    
+                    if( use_mean_field_cd )
+                        neg_input = input_layer->expectation;
                     else
-                    {
-                        input_layer->generateSample();
-                        connection->setAsDownInput( input_layer->sample );
-                    }
+                        neg_input = input_layer->sample;
 
-                    hidden_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-
+                    neg_hidden = hidden_layer->expectation;
+                    
+                    input_layer->update(pos_input,neg_input);
+                    hidden_layer->update(pos_hidden,neg_hidden);
+                    connection->update(pos_input,pos_hidden,
+                                       neg_input,neg_hidden);
                     if( targetsize() > 0 )
                     {
                         if( use_mean_field_cd )
-                            target_connection->setAsUpInput( 
-                                hidden_layer->expectation );
+                            neg_target = target_layer->expectation;
                         else
-                            target_connection->setAsUpInput( 
-                                hidden_layer->sample );
-                        target_layer->getAllActivations( 
-                            (RBMMatrixConnection*) target_connection );
-                        target_layer->computeExpectation();
-                        if( use_mean_field_cd )
-                            productAcc( hidden_layer->activation,
-                                        target_connection->weights,
-                                        target_layer->expectation );
-                        else
-                        {
-                            target_layer->generateSample();
-                            productAcc( hidden_layer->activation,
-                                        target_connection->weights,
-                                        target_layer->sample );
-                        }   
+                            neg_target = target_layer->sample;
+                        target_layer->update(pos_target,neg_target);
+                        target_connection->update(pos_target,pos_hidden,
+                                                  neg_target,neg_hidden);
                     }
-                        
-                    hidden_layer->computeExpectation();
                 }
-                    
-                if( use_mean_field_cd )
-                    neg_input = input_layer->expectation;
-                else
-                    neg_input = input_layer->sample;
 
-                neg_hidden = hidden_layer->expectation;
-                    
-                input_layer->update(pos_input,neg_input);
-                hidden_layer->update(pos_hidden,neg_hidden);
-                connection->update(pos_input,pos_hidden,
-                                   neg_input,neg_hidden);
-                if( targetsize() > 0 )
+                if( !fast_exact_is_equal(persistent_cd_weight, 0.) )
                 {
                     if( use_mean_field_cd )
-                        neg_target = target_layer->expectation;
-                    else
-                        neg_target = target_layer->sample;
-                    target_layer->update(pos_target,neg_target);
-                    target_connection->update(pos_target,pos_hidden,
-                                              neg_target,neg_hidden);
-                }
-            }
+                        PLERROR("In PseudolikelihoodRBM::train(): Persistent "
+                                "Contrastive Divergence was not implemented for "
+                                "MF-CD");
 
-            if( !fast_exact_is_equal(persistent_cd_weight, 0.) )
-            {
-                if( use_mean_field_cd )
-                    PLERROR("In PseudolikelihoodRBM::train(): Persistent "
-                            "Contrastive Divergence was not implemented for "
-                            "MF-CD");
-
-                if( !fast_exact_is_equal(cd_decrease_ct, 0) )
-                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
-                else
-                    lr = cd_learning_rate;
+                    if( !fast_exact_is_equal(cd_decrease_ct, 0) )
+                        lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                    else
+                        lr = cd_learning_rate;
                     
-                if( targetsize() > 0 )
-                    lr *= generative_learning_weight;
+                    if( targetsize() > 0 )
+                        lr *= generative_learning_weight;
 
-                lr *= persistent_cd_weight;
+                    lr *= persistent_cd_weight;
 
-                setLearningRate(lr);
+                    setLearningRate(lr);
 
-                int chain_i = stage % n_gibbs_chains;
+                    int chain_i = stage % n_gibbs_chains;
 
-                if( !persistent_gibbs_chain_is_started[chain_i] )
-                {  
-                    // Start gibbs chain
-                    connection->setAsDownInput( input );
-                    hidden_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-                    if( targetsize() == 1 )
-                        productAcc( hidden_layer->activation,
-                                    target_connection->weights,
-                                    target_one_hot );
-                    else if( targetsize() > 1 )
-                        productAcc( hidden_layer->activation,
-                                    target_connection->weights,
-                                    target );
+                    if( !persistent_gibbs_chain_is_started[chain_i] )
+                    {  
+                        // Start gibbs chain
+                        connection->setAsDownInput( input );
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        if( targetsize() == 1 )
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target_one_hot );
+                        else if( targetsize() > 1 )
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target );
                         
-                    hidden_layer->computeExpectation();
-                    hidden_layer->generateSample();
-                    pers_cd_hidden[chain_i] << hidden_layer->sample;
-                    persistent_gibbs_chain_is_started[chain_i] = true;
-                }
+                        hidden_layer->computeExpectation();
+                        hidden_layer->generateSample();
+                        pers_cd_hidden[chain_i] << hidden_layer->sample;
+                        persistent_gibbs_chain_is_started[chain_i] = true;
+                    }
 
-                if( fast_exact_is_equal(persistent_cd_weight, 1.) )
-                {
-                    // Hidden positive sample was not computed previously
-                    connection->setAsDownInput( input );
-                    hidden_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-                    if( targetsize() == 1 )
-                        productAcc( hidden_layer->activation,
-                                    target_connection->weights,
-                                    target_one_hot );
-                    else if( targetsize() > 1 )
-                        productAcc( hidden_layer->activation,
-                                    target_connection->weights,
-                                    target );
+                    if( fast_exact_is_equal(persistent_cd_weight, 1.) )
+                    {
+                        // Hidden positive sample was not computed previously
+                        connection->setAsDownInput( input );
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        if( targetsize() == 1 )
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target_one_hot );
+                        else if( targetsize() > 1 )
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target );
                             
-                    hidden_layer->computeExpectation();
-                    pos_hidden << hidden_layer->expectation;
-                }
+                        hidden_layer->computeExpectation();
+                        pos_hidden << hidden_layer->expectation;
+                    }
 
-                hidden_layer->sample << pers_cd_hidden[chain_i];
-                // Prolonged Gibbs chain
-                for(int i=0; i<cd_n_gibbs; i++)
-                {
-                    connection->setAsUpInput( hidden_layer->sample );
-                    input_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-                    input_layer->computeExpectation();
-                    // LATERAL CONNECTIONS CODE HERE!
-                    input_layer->generateSample();
-                    connection->setAsDownInput( input_layer->sample );
-                    hidden_layer->getAllActivations( 
-                        (RBMMatrixConnection*) connection );
-                    if( targetsize() > 0 )
+                    hidden_layer->sample << pers_cd_hidden[chain_i];
+                    // Prolonged Gibbs chain
+                    for(int i=0; i<cd_n_gibbs; i++)
                     {
-                        target_connection->setAsUpInput( hidden_layer->sample );
-                        target_layer->getAllActivations( 
-                            (RBMMatrixConnection*) target_connection );
-                        target_layer->computeExpectation();
-                        target_layer->generateSample();
-                        productAcc( hidden_layer->activation,
-                                    target_connection->weights,
-                                    target_layer->sample );
+                        connection->setAsUpInput( hidden_layer->sample );
+                        input_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        input_layer->computeExpectation();
+                        // LATERAL CONNECTIONS CODE HERE!
+                        input_layer->generateSample();
+                        connection->setAsDownInput( input_layer->sample );
+                        hidden_layer->getAllActivations( 
+                            (RBMMatrixConnection*) connection );
+                        if( targetsize() > 0 )
+                        {
+                            target_connection->setAsUpInput( hidden_layer->sample );
+                            target_layer->getAllActivations( 
+                                (RBMMatrixConnection*) target_connection );
+                            target_layer->computeExpectation();
+                            target_layer->generateSample();
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target_layer->sample );
+                        }
+                        hidden_layer->computeExpectation();
+                        hidden_layer->generateSample();
                     }
-                    hidden_layer->computeExpectation();
-                    hidden_layer->generateSample();
-                }
 
-                pers_cd_hidden[chain_i] << hidden_layer->sample;
+                    pers_cd_hidden[chain_i] << hidden_layer->sample;
 
-                input_layer->update(input, input_layer->sample);
-                hidden_layer->update(pos_hidden,hidden_layer->expectation);
-                connection->update(input,pos_hidden,
-                                   input_layer->sample,
-                                   hidden_layer->expectation);
-                if( targetsize() > 0 )
-                {
-                    target_layer->update(target_one_hot, target_layer->sample);
-                    target_connection->update(target_one_hot,pos_hidden,
-                                              target_layer->sample,
-                                              hidden_layer->expectation);
+                    input_layer->update(input, input_layer->sample);
+                    hidden_layer->update(pos_hidden,hidden_layer->expectation);
+                    connection->update(input,pos_hidden,
+                                       input_layer->sample,
+                                       hidden_layer->expectation);
+                    if( targetsize() > 0 )
+                    {
+                        target_layer->update(target_one_hot, target_layer->sample);
+                        target_connection->update(target_one_hot,pos_hidden,
+                                                  target_layer->sample,
+                                                  hidden_layer->expectation);
+                    }
                 }
             }
         }



From nouiz at mail.berlios.de  Fri Aug 29 20:40:12 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Aug 2008 20:40:12 +0200
Subject: [Plearn-commits] r9415 - trunk/plearn/io
Message-ID: <200808291840.m7TIeCmO013853@sheep.berlios.de>

Author: nouiz
Date: 2008-08-29 20:40:11 +0200 (Fri, 29 Aug 2008)
New Revision: 9415

Modified:
   trunk/plearn/io/openFile.cc
   trunk/plearn/io/openFile.h
Log:
Added an optional parameter to openFile: err_if_dont_exist that default to true(so don't change behavior if you don't use it.)
if err_if_dont_exist==false and the file don't existe we return an empty PStream
if err_if_dont_exist==true and the file don't we do a PLERROR.


Modified: trunk/plearn/io/openFile.cc
===================================================================
--- trunk/plearn/io/openFile.cc	2008-08-29 18:31:00 UTC (rev 9414)
+++ trunk/plearn/io/openFile.cc	2008-08-29 18:40:11 UTC (rev 9415)
@@ -65,9 +65,13 @@
  *  "r" for opening the file for reading, "w" for writing (overwrites the
  *  file if it exists), or "a" for appending to the file (creating it if
  *  it doesn't exist). The default is to open the file for reading ("r").
+ *
+ *  @param err_if_dont_exist if true, will generate a PLERROR if the file
+ *  don't exist. Else, will return an empty PStream witch st->good() will
+ *  return false.
  */
 PStream openFile(const PPath& filepath_, PStream::mode_t io_formatting,
-                 const string& openmode)
+                 const string& openmode, bool err_if_dont_exist)
 {
     const PPath filepath = filepath_.absolute();
     
@@ -80,8 +84,10 @@
     if (openmode == "r")
     {
         fd = PR_Open(filepath.c_str(), PR_RDONLY, 0666);
-        if (!fd)
+        if (!fd && err_if_dont_exist)
             PLERROR("openFile(\"%s\",\"%s\") failed.",filepath.c_str(), openmode.c_str());
+        else if(!fd)
+            return new PrPStreamBuf(0, 0);
         st = new PrPStreamBuf(fd, 0, true, false);
     }
     else if (openmode == "w")

Modified: trunk/plearn/io/openFile.h
===================================================================
--- trunk/plearn/io/openFile.h	2008-08-29 18:31:00 UTC (rev 9414)
+++ trunk/plearn/io/openFile.h	2008-08-29 18:40:11 UTC (rev 9415)
@@ -53,7 +53,8 @@
 
 PStream openFile(const PPath& filepath_, 
                  PStream::mode_t io_formatting,
-                 const string& openmode = "r"); 
+                 const string& openmode = "r",
+                 bool err_if_dont_exist = true); 
 
 } // end of namespace PLearn
 



From nouiz at mail.berlios.de  Fri Aug 29 20:43:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Aug 2008 20:43:22 +0200
Subject: [Plearn-commits] r9416 - trunk/plearn/vmat
Message-ID: <200808291843.m7TIhMmh014062@sheep.berlios.de>

Author: nouiz
Date: 2008-08-29 20:43:21 +0200 (Fri, 29 Aug 2008)
New Revision: 9416

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
-remove a race condition in VMatrix::lockMetaDataDir by using the previous commit modification in openFile. We check that a file exist, if it exist then we open it. During this time the file can be deleted. In that case we where doing a PLERROR. Now we handle it gracefully.
-Added a PLERROR in VMatrix::getPrecomputedStatsFromFile if we reload data with not the expected width.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-08-29 18:40:11 UTC (rev 9415)
+++ trunk/plearn/vmat/VMatrix.cc	2008-08-29 18:43:21 UTC (rev 9416)
@@ -1439,7 +1439,11 @@
     while (isfile(lockfile) && (max_lock_age == 0 || mtime(lockfile) + max_lock_age > time(0))) {
         // There is a lock file, and it is not older than 'max_lock_age'.
         string bywho;
-        try{ bywho = loadFileAsString(lockfile); }
+        try{ 
+            PStream st = openFile(lockfile, PStream::raw_ascii, "r", false);
+            if(st.good())
+                st.read(bywho, streamsize(filesize(lockfile)));
+        }
         catch(const PLearnError& e) {
             PLERROR("In VMatrix::lockMetaDataDir - Catching exceptions is"
                     " dangerous in PLearn (memory"
@@ -1650,9 +1654,13 @@
         uptodate = isUpToDate(statsfile);
     }
     try{
-        if (uptodate)
+        if (uptodate){
             PLearn::load(statsfile, stats);
-        else
+            if(stats.length()!=width())
+                PLERROR("In VMatrix::getPrecomputedStatsFromFile() for class %s -"
+                        " bad file %s. Delete it to have it recreated.",
+                        classname().c_str(), statsfile.c_str());
+        } else
         {
             VMat vm = const_cast<VMatrix*>(this);
             stats = PLearn::computeStats(vm, maxnvalues, progress_bar);



