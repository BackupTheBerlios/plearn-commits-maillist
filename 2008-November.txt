From nouiz at mail.berlios.de  Tue Nov  4 20:29:43 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 20:29:43 +0100
Subject: [Plearn-commits] r9632 - trunk/plearn/vmat
Message-ID: <200811041929.mA4JTh0Y002460@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 20:29:42 +0100 (Tue, 04 Nov 2008)
New Revision: 9632

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
added the handling of Y in the postal code. Also, print when the postal code is invalid instead of telling it is not handled.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-11-04 19:28:31 UTC (rev 9631)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-11-04 19:29:42 UTC (rev 9632)
@@ -686,6 +686,8 @@
         val = 160 + second_digit;
     else if(first_char=='X')
         val = 170 + second_digit;
+    else if(first_char=='Y')
+        val = 180 + second_digit;
     else if(first_char=='0' || first_char=='1' || first_char=='2' || first_char=='3' ||
             first_char=='4' || first_char=='5' || first_char=='6' || first_char=='7' ||
             first_char=='8' || first_char=='9') {
@@ -694,8 +696,18 @@
         val = 260 + first_digit * 10 + second_digit;
     }
     else {
+        //http://en.wikipedia.org/wiki/Canadian_postal_code
+        //No postal code includes the letters D, F, I, O, Q, or U,
+        //as the OCR equipment used in automated sorting could easily
+        //confuse them with other letters and digits, 
         if (display_warning) {
-            string errmsg = "Currently only some postal codes are supported: ";
+            string errmsg;
+            if(first_char=='D' ||first_char=='F' ||first_char=='I' ||
+               first_char=='O' ||first_char=='Q' ||first_char=='U')
+                errmsg = "Postal code don't use letters D, F, I, O, Q, or U: ";
+            else
+                errmsg = "Currently only some postal codes are supported: ";
+
             errmsg += "can't process " + strval + ", value will be set to 0.";
             PLWARNING(errmsg.c_str());
         }



From nouiz at mail.berlios.de  Tue Nov  4 20:28:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 20:28:31 +0100
Subject: [Plearn-commits] r9631 - trunk/plearn/vmat
Message-ID: <200811041928.mA4JSVwk002363@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 20:28:31 +0100 (Tue, 04 Nov 2008)
New Revision: 9631

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
Log:
removed useless warning when their is a missing column with a skip instruction.


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-10-30 22:28:50 UTC (rev 9630)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-11-04 19:28:31 UTC (rev 9631)
@@ -176,9 +176,14 @@
             continue;
         } else if (source_col >= source->width()) 
         {
-            PLWARNING("In MissingInstructionVMatrix::build_() - missing_instructions '%d': no field with this name: '%s'" 
-                    ,ins_col,(missing_instructions[ins_col].first).c_str());
-            missing_field++;
+            if(missing_instructions[ins_col].second!="skip"){
+                //if the instruction is skip, we don't care that it is missing in the source!
+                PLWARNING("In MissingInstructionVMatrix::build_() - missing_instructions '%d': no field with this name: '%s'."
+                          " It have '%s' as spec"
+                          ,ins_col,(missing_instructions[ins_col].first).c_str(),
+                          (missing_instructions[ins_col].second).c_str());
+                missing_field++;
+            }
             continue;
         }
         if (missing_instructions[ins_col].second == "skip")



From nouiz at mail.berlios.de  Tue Nov  4 21:16:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:16:34 +0100
Subject: [Plearn-commits] r9633 - trunk/plearn/vmat
Message-ID: <200811042016.mA4KGYNt008985@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:16:33 +0100 (Tue, 04 Nov 2008)
New Revision: 9633

Modified:
   trunk/plearn/vmat/DichotomizeVMatrix.cc
Log:
corrected message.


Modified: trunk/plearn/vmat/DichotomizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-11-04 19:29:42 UTC (rev 9632)
+++ trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-11-04 20:16:33 UTC (rev 9633)
@@ -163,7 +163,7 @@
             if (discrete_variable_instructions[ins_col].first == source_names[source_col]) break;
         }
         if (source_col >= source->width()) 
-            PLERROR("In DichotomizeDond2DiscreteVariables::build_() -  "
+            PLERROR("In DichotomizeVMatrix::build_() -  "
                     "no field with this name in the source data set: %s",
                     (discrete_variable_instructions[ins_col].first).c_str());
         else instruction_index[source_col] = ins_col;
@@ -227,7 +227,7 @@
                discrete_variable_instructions[instruction_index[source_col]].second;
            if (instruction_ptr.size() == 0) 
            {
-               PLWARNING("In DichotomizeDond2DiscreteVariables::build_() -"
+               PLWARNING("In DichotomizeVMatrix::build_() -"
                          "instruction for field %s have no range!",
                          discrete_variable_instructions[instruction_index[source_col]].first.c_str());
                continue;



From nouiz at mail.berlios.de  Tue Nov  4 21:20:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:20:23 +0100
Subject: [Plearn-commits] r9634 - trunk/plearn/vmat
Message-ID: <200811042020.mA4KKNST009525@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:20:23 +0100 (Tue, 04 Nov 2008)
New Revision: 9634

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
   trunk/plearn/vmat/MissingInstructionVMatrix.h
Log:
added option MissingInstructionVMatrix.missing_instruction_error and MissingInstructionVMatrix.missing_field_error that default to true. Default behavior not changed.


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-11-04 20:16:33 UTC (rev 9633)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-11-04 20:20:23 UTC (rev 9634)
@@ -50,7 +50,9 @@
     );
 
 MissingInstructionVMatrix::MissingInstructionVMatrix():
-    default_instruction("")
+    default_instruction(""),
+    missing_instruction_error(true),
+    missing_field_error(true)
 /* ### Initialize all fields to their default value */
 {
     // ...
@@ -120,6 +122,14 @@
                   "If some field in the source matrix have no instruction," 
                   " we will use this instruction. We will warn about field"
                   " with empty instruction then will stop.");
+   declareOption(ol, "missing_instruction_error",
+                  &MissingInstructionVMatrix::missing_instruction_error,
+                  OptionBase::buildoption,
+                 "If true will generate an error if some field have field without instruction" );
+   declareOption(ol, "missing_field_error",
+                  &MissingInstructionVMatrix::missing_field_error ,
+                  OptionBase::buildoption,
+                 "If true will generate an error if some instruction reference missing field" );
 }
 
 void MissingInstructionVMatrix::build_()
@@ -223,9 +233,9 @@
             missing_instruction++;
         }   
     }
-    if(missing_instruction)
+    if(missing_instruction && missing_instruction_error)
         PLERROR("In MissingInstructionVMatrix::build_ - Their have been %d field in the source matrix that have no instruction",missing_instruction);
-    if(missing_field)
+    if(missing_field && missing_field_error)
         PLERROR("In MissingInstructionVMatrix::build_ - Their have been %d instruction that have no correcponding field in the source matrix",missing_field);
 
     // Copy the appropriate VMFields

Modified: trunk/plearn/vmat/MissingInstructionVMatrix.h
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.h	2008-11-04 20:16:33 UTC (rev 9633)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.h	2008-11-04 20:20:23 UTC (rev 9634)
@@ -72,6 +72,10 @@
     //! this instruction. Will warn about field with empty instruction then stop.
     string default_instruction;
 
+    //! If true will generate an error if some field have field without instruction
+    bool missing_instruction_error;
+    //! If true will generate an error if some instruction reference missing field
+    bool missing_field_error;
 public:
     //#####  Public Member Functions  #########################################
 



From nouiz at mail.berlios.de  Tue Nov  4 21:21:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:21:48 +0100
Subject: [Plearn-commits] r9635 - trunk/plearn/vmat
Message-ID: <200811042021.mA4KLmn8009697@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:21:47 +0100 (Tue, 04 Nov 2008)
New Revision: 9635

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
   trunk/plearn/vmat/VariableDeletionVMatrix.h
Log:
added option VariableDeletionVMatrix.save_deleted_columns that will save the selected columns for deletion to a file.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-11-04 20:20:23 UTC (rev 9634)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-11-04 20:21:47 UTC (rev 9635)
@@ -42,6 +42,8 @@
 #include "VariableDeletionVMatrix.h"
 #include <plearn/vmat/SubVMatrix.h>
 #include <plearn/vmat/VMat_computeStats.h>
+#include <plearn/io/fileutils.h>
+#include <plearn/io/load_and_save.h>
 
 namespace PLearn {
 using namespace std;
@@ -123,6 +125,13 @@
         "If greater than or equal to 1, the integer portion will be\n"
         "interpreted as the number of samples to use.");
 
+    declareOption(ol, "save_deleted_columns",
+                  &VariableDeletionVMatrix::save_deleted_columns,
+                  OptionBase::buildoption,
+                  "If not empty will save the deleted culumns in this file."
+                  "If present, will verify that it have the same content then"
+                  " the calculated data.");
+
     declareOption(ol, "complete_dataset",
                   &VariableDeletionVMatrix::complete_dataset,
                   OptionBase::learntoption,
@@ -327,6 +336,19 @@
     // We have modified the selected columns, so the parent class must be
     // re-built.
     inherited::build();
+
+    if(!save_deleted_columns.empty()){
+        if(isfile(save_deleted_columns)){
+            TVec<int> indices2;
+            PLearn::load(save_deleted_columns, indices2);
+            if(indices!=indices2)
+                PLERROR("In VariableDeletionVMatrix::build_() - the calculated"
+                        " indices(%d) differ from the saved indices(%d) in file '%s'! ",
+                        indices2.length(), indices.length(), save_deleted_columns.c_str());
+        }else{
+            PLearn::save(save_deleted_columns,indices);
+        }
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/vmat/VariableDeletionVMatrix.h
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.h	2008-11-04 20:20:23 UTC (rev 9634)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.h	2008-11-04 20:21:47 UTC (rev 9635)
@@ -59,12 +59,12 @@
     real    max_constant_threshold;
     int     number_of_train_samples;
     VMat    train_set;
+    string  save_deleted_columns;
 
     // Deprecated.
     VMat       complete_dataset;
     real       deletion_threshold;
     int        remove_columns_with_constant_value;
-
 public:
 
     VariableDeletionVMatrix();



From nouiz at mail.berlios.de  Tue Nov  4 21:30:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:30:22 +0100
Subject: [Plearn-commits] r9636 - trunk/plearn/vmat
Message-ID: <200811042030.mA4KUMY1012542@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:30:21 +0100 (Tue, 04 Nov 2008)
New Revision: 9636

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
in a corner case, change an error for a warning as this don't change de final matrix.


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-04 20:21:47 UTC (rev 9635)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-04 20:30:21 UTC (rev 9636)
@@ -340,9 +340,13 @@
                     ok = true;
                     indices.append(the_index);
                 }
-                if (!ok)
+                if (!ok && !inverse_fields_selection)
                     PLERROR("In SelectColumnsVMatrix::build_ - Unknown field \"%s\" in source VMat;\n"
                             "    (you may want to use the 'extend_with_missing' option)", the_field.c_str());
+                else if(!ok)
+                    PLWARNING("In SelectColumnsVMatrix::build_ - Unknown field \"%s\" in source VMat;\n"
+                              " as we want to ignore this field, we ignore this error.", the_field.c_str());
+                    
             } else
                 indices.append(the_index);
             if(extend_with_missing && the_index == -1)



From nouiz at mail.berlios.de  Tue Nov  4 21:31:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:31:21 +0100
Subject: [Plearn-commits] r9637 - trunk/plearn/vmat
Message-ID: <200811042031.mA4KVLox012706@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:31:20 +0100 (Tue, 04 Nov 2008)
New Revision: 9637

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/GaussianizeVMatrix.h
Log:
added the option save_and_reuse_stats to force the compute in memory of the stats.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-11-04 20:30:21 UTC (rev 9636)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-11-04 20:31:20 UTC (rev 9637)
@@ -39,6 +39,7 @@
 
 #include "GaussianizeVMatrix.h"
 #include <plearn/math/pl_erf.h>
+#include "VMat_computeStats.h"
 
 namespace PLearn {
 using namespace std;
@@ -80,7 +81,8 @@
     gaussianize_weight(false),
     gaussianize_extra(false),
     gaussianize_binary(false),
-    threshold_ratio(10)
+    threshold_ratio(10),
+    save_and_reuse_stats(true)
 {}
 
 ////////////////////
@@ -113,6 +115,11 @@
                   OptionBase::buildoption,
         "Whether or not to Gaussianize the extra part.");
 
+    declareOption(ol, "save_and_reuse_stats",
+                  &GaussianizeVMatrix::save_and_reuse_stats,
+                  OptionBase::buildoption,
+        "If true, will save and reuse the stats of the source.");
+
     declareOption(ol, "gaussianize_binary",
                   &GaussianizeVMatrix::gaussianize_binary,
                   OptionBase::buildoption,
@@ -207,8 +214,12 @@
         PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "
                 " train_source, source or this VMatrix should have a metadata directory!");
     
-    TVec<StatsCollector> stats = the_source->
-        getPrecomputedStatsFromFile("stats_gaussianizeVMatrix.psave", -1, true);
+    TVec<StatsCollector> stats;
+    if(save_and_reuse_stats)
+        stats = the_source->
+            getPrecomputedStatsFromFile("stats_gaussianizeVMatrix.psave", -1, true);
+    else
+        stats = PLearn::computeStats(the_source, -1, true);
 
     // See which dimensions violate the Gaussian assumption and will be
     // actually Gaussianized, and store the corresponding list of values.

Modified: trunk/plearn/vmat/GaussianizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.h	2008-11-04 20:30:21 UTC (rev 9636)
+++ trunk/plearn/vmat/GaussianizeVMatrix.h	2008-11-04 20:31:20 UTC (rev 9637)
@@ -70,6 +70,7 @@
     bool gaussianize_extra;
     bool gaussianize_binary;
     real threshold_ratio;
+    bool save_and_reuse_stats;
     VMat train_source;
 
 public:



From nouiz at mail.berlios.de  Tue Nov  4 21:43:44 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:43:44 +0100
Subject: [Plearn-commits] r9638 - trunk/plearn/vmat
Message-ID: <200811042043.mA4Khi5B014708@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:43:44 +0100 (Tue, 04 Nov 2008)
New Revision: 9638

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
   trunk/plearn/vmat/MissingInstructionVMatrix.h
Log:
-corrected comment
-removed MissingInstructionVMatrix.missing_instruction_error option as we should use the default_instruction if we want that behavior. Changed message to tell this.
-code indentation.


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-11-04 20:31:20 UTC (rev 9637)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-11-04 20:43:44 UTC (rev 9638)
@@ -51,7 +51,6 @@
 
 MissingInstructionVMatrix::MissingInstructionVMatrix():
     default_instruction(""),
-    missing_instruction_error(true),
     missing_field_error(true)
 /* ### Initialize all fields to their default value */
 {
@@ -122,10 +121,6 @@
                   "If some field in the source matrix have no instruction," 
                   " we will use this instruction. We will warn about field"
                   " with empty instruction then will stop.");
-   declareOption(ol, "missing_instruction_error",
-                  &MissingInstructionVMatrix::missing_instruction_error,
-                  OptionBase::buildoption,
-                 "If true will generate an error if some field have field without instruction" );
    declareOption(ol, "missing_field_error",
                   &MissingInstructionVMatrix::missing_field_error ,
                   OptionBase::buildoption,
@@ -188,7 +183,8 @@
         {
             if(missing_instructions[ins_col].second!="skip"){
                 //if the instruction is skip, we don't care that it is missing in the source!
-                PLWARNING("In MissingInstructionVMatrix::build_() - missing_instructions '%d': no field with this name: '%s'."
+                PLWARNING("In MissingInstructionVMatrix::build_() -"
+                          " missing_instructions '%d': no field with this name: '%s'."
                           " It have '%s' as spec"
                           ,ins_col,(missing_instructions[ins_col].first).c_str(),
                           (missing_instructions[ins_col].second).c_str());
@@ -207,8 +203,13 @@
         else if (missing_instructions[ins_col].second == "present")
             ins[source_col] = "present";
         else if (missing_instructions[ins_col].second.empty())
-            PLWARNING("In MergeDond2Files::build_() - merge instruction empty for field '%s', we keep the previous instruction who could be the default_instruction",(missing_instructions[source_col].first).c_str());
-        else PLERROR("In MergeDond2Files::build_() - unsupported merge instruction: '%s'", 
+            PLWARNING("In MissingInstructionVMatrix::build_() -"
+                      " merge instruction empty for field '%s',"
+                      " we keep the previous instruction who could be"
+                      " the default_instruction",
+                      (missing_instructions[source_col].first).c_str());
+        else PLERROR("In MissingInstructionVMatrix::build_() -"
+                     " unsupported merge instruction: '%s'", 
                      (missing_instructions[ins_col].second).c_str());
         if (ins[source_col] == "skip"){
             if(source_col<source->inputsize())
@@ -226,17 +227,23 @@
     int missing_instruction = 0;
     for (int col = 0; col < source->width(); col++)
     {
-        if(ins[col] == "")
+        if(ins[col].empty())
         {
-            PLWARNING("In MissingInstructionVMatrix::build_ - their is no instruction for the field '%s'",
+            PLWARNING("In MissingInstructionVMatrix::build_ -"
+                      " their is no instruction for the field '%s'",
                     source_names[col].c_str());
             missing_instruction++;
         }   
     }
-    if(missing_instruction && missing_instruction_error)
-        PLERROR("In MissingInstructionVMatrix::build_ - Their have been %d field in the source matrix that have no instruction",missing_instruction);
+    if(missing_instruction)
+        PLERROR("In MissingInstructionVMatrix::build_ -"
+                " Their have been %d field in the source"
+                " matrix that have no instruction. Do you want"
+                " to set the default_instruction option?",missing_instruction);
     if(missing_field && missing_field_error)
-        PLERROR("In MissingInstructionVMatrix::build_ - Their have been %d instruction that have no correcponding field in the source matrix",missing_field);
+        PLERROR("In MissingInstructionVMatrix::build_ - Their have been %d"
+                " instruction that have no correcponding field in the"
+                " source matrix",missing_field);
 
     // Copy the appropriate VMFields
     fieldinfos.resize(width());

Modified: trunk/plearn/vmat/MissingInstructionVMatrix.h
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.h	2008-11-04 20:31:20 UTC (rev 9637)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.h	2008-11-04 20:43:44 UTC (rev 9638)
@@ -72,8 +72,6 @@
     //! this instruction. Will warn about field with empty instruction then stop.
     string default_instruction;
 
-    //! If true will generate an error if some field have field without instruction
-    bool missing_instruction_error;
     //! If true will generate an error if some instruction reference missing field
     bool missing_field_error;
 public:



From nouiz at mail.berlios.de  Tue Nov  4 21:52:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Nov 2008 21:52:30 +0100
Subject: [Plearn-commits] r9639 - trunk/plearn/vmat
Message-ID: <200811042052.mA4KqU8Y016114@sheep.berlios.de>

Author: nouiz
Date: 2008-11-04 21:52:30 +0100 (Tue, 04 Nov 2008)
New Revision: 9639

Modified:
   trunk/plearn/vmat/DichotomizeVMatrix.cc
   trunk/plearn/vmat/DichotomizeVMatrix.h
Log:
added the option DichotomizeVMatrix missing_field_error that default to true(old behavior) that if false will generate a warning instead of an error if some instruction revert to variable that don't exist in the source.


Modified: trunk/plearn/vmat/DichotomizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-11-04 20:43:44 UTC (rev 9638)
+++ trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-11-04 20:52:30 UTC (rev 9639)
@@ -52,7 +52,8 @@
     );
 
 DichotomizeVMatrix::DichotomizeVMatrix():
-    verbose(3)
+    verbose(3),
+    missing_field_error(true)
 /* ### Initialize all fields to their default value */
 {
     // ...
@@ -126,8 +127,13 @@
     declareOption(ol, "instruction_index", &DichotomizeVMatrix::instruction_index,
                   OptionBase::learntoption,
                   "An array that point each columns of the source matrix to its instruction.");
-//instruction_index
 
+    declareOption(ol, "missing_field_error", &DichotomizeVMatrix::missing_field_error,
+                  OptionBase::buildoption,
+                  "If true we will generate an error is a field is"
+                  " in the instruction but not in the source."
+                  " Otherwise will generate a warning.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -162,10 +168,16 @@
         {
             if (discrete_variable_instructions[ins_col].first == source_names[source_col]) break;
         }
-        if (source_col >= source->width()) 
-            PLERROR("In DichotomizeVMatrix::build_() -  "
-                    "no field with this name in the source data set: %s",
-                    (discrete_variable_instructions[ins_col].first).c_str());
+        if (source_col >= source->width()){
+            if(missing_field_error)
+                PLERROR("In DichotomizeVMatrix::build_() -  "
+                        "no field with this name in the source data set: %s",
+                        (discrete_variable_instructions[ins_col].first).c_str());
+            else
+                PLWARNING("In DichotomizeVMatrix::build_() -  "
+                        "no field with this name in the source data set: %s",
+                        (discrete_variable_instructions[ins_col].first).c_str());
+        }
         else instruction_index[source_col] = ins_col;
     }
 

Modified: trunk/plearn/vmat/DichotomizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.h	2008-11-04 20:43:44 UTC (rev 9638)
+++ trunk/plearn/vmat/DichotomizeVMatrix.h	2008-11-04 20:52:30 UTC (rev 9639)
@@ -66,7 +66,7 @@
     TVec< pair<string, TVec< pair<real, real> > > > discrete_variable_instructions;
     
     int verbose;
-
+    bool missing_field_error;
 public:
     //#####  Public Member Functions  #########################################
 



From nouiz at mail.berlios.de  Wed Nov  5 16:02:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 16:02:05 +0100
Subject: [Plearn-commits] r9640 - trunk/plearn/misc
Message-ID: <200811051502.mA5F25qp018819@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 16:02:04 +0100 (Wed, 05 Nov 2008)
New Revision: 9640

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
reuse old VMatrix in some case.


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2008-11-04 20:52:30 UTC (rev 9639)
+++ trunk/plearn/misc/viewVMat.cc	2008-11-05 15:02:04 UTC (rev 9640)
@@ -854,13 +854,25 @@
             case (int)'<': case (int)'>':
                 // Sort by increasing or decreasing order.
             {
-                PP<SortRowsVMatrix> new_vm = new SortRowsVMatrix();
-                new_vm->source = vm_showed;
+                PP<SortRowsVMatrix> new_vm;
+                if(vm_showed->classname()!="SortRowsVMatrix" 
+                   || ((PP<SortRowsVMatrix>)vm_showed)->sort_columns[0]!=curj){
+                //if it is a SortRowsVMatrix and we sort on a new column
+                // we can't reuse the last SortRowsVMatrix as it don't suport 
+                // different order on each row.
+                    new_vm = new SortRowsVMatrix();
+                    new_vm->source = vm_showed;
+                    vm_showed = get_pointer(new_vm);
+                } else 
+                    //in the case where we sort multiple time on the same column
+                    //we reuse the last VMatrix.
+                    new_vm= (PP<SortRowsVMatrix>)vm_showed;
                 new_vm->sort_columns = TVec<int>(1, curj);
                 if (key == (int)'>')
                     new_vm->increasing_order = false;
+                else                    
+                    new_vm->increasing_order = true;
                 new_vm->build();
-                vm_showed = get_pointer(new_vm);
             }
             break;
             ///////////////////////////////////////////////////////////////



From nouiz at mail.berlios.de  Wed Nov  5 16:04:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 16:04:35 +0100
Subject: [Plearn-commits] r9641 - trunk/plearn/vmat
Message-ID: <200811051504.mA5F4Z5c018968@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 16:04:35 +0100 (Wed, 05 Nov 2008)
New Revision: 9641

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
Log:
added option MeanMedianModeImputationVMatrix.missing_field_error that if false will generate a warning instead of an error.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-05 15:02:04 UTC (rev 9640)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-05 15:04:35 UTC (rev 9641)
@@ -56,7 +56,8 @@
   );
 
 MeanMedianModeImputationVMatrix::MeanMedianModeImputationVMatrix()
-: number_of_train_samples_to_use(0.0)
+  : number_of_train_samples_to_use(0.0),
+    missing_field_error(true)
 {
 }
 
@@ -86,6 +87,12 @@
 		" -err   : make it an error to have a missing value in this field"
 		);
 
+  declareOption(ol, "missing_field_error", &MeanMedianModeImputationVMatrix::missing_field_error,
+		OptionBase::buildoption, 
+                "If True, will generate an error if some field in the"
+		" imputation_spec are present but not in the source. Otherwise"
+		" will generate a warning..");
+
   declareOption(ol, "variable_mean", &MeanMedianModeImputationVMatrix::variable_mean, OptionBase::learntoption, 
                 "The vector of variable means observed from the train set.");
 
@@ -322,9 +329,12 @@
     }
     imputation_spec = save_imputation_spec;
 
-    if(nofields.length()>0)
+    if(nofields.length()>0 && missing_field_error)
       PLERROR("In MeanMedianModeImputationVMatrix::build_() Their is %d fields in the imputation_spec that are not in train set: %s",nofields.length(),
 	      tostring(nofields).c_str());
+    else if(nofields.length()>0)
+      PLWARNING("In MeanMedianModeImputationVMatrix::build_() Their is %d fields in the imputation_spec that are not in train set: %s",nofields.length(),
+		tostring(nofields).c_str());
     TVec<string> no_instruction;
     for(int i = 0;i<variable_imputation_instruction.size();i++)
       if(variable_imputation_instruction[i]==0)

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-11-05 15:02:04 UTC (rev 9640)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-11-05 15:04:35 UTC (rev 9641)
@@ -72,7 +72,12 @@
   
   //! Pairs of instruction of the form field_name : mean | median | mode | none.
   TVec< pair<string, string> >  imputation_spec;
-  
+
+  //!if true will generate an error if the imputation_spec reference a
+  //! field not in the source. Otherwise generate a warning.
+  bool                          missing_field_error;
+
+
                         MeanMedianModeImputationVMatrix();
   virtual               ~MeanMedianModeImputationVMatrix();
 



From nouiz at mail.berlios.de  Wed Nov  5 16:06:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 16:06:58 +0100
Subject: [Plearn-commits] r9642 - trunk/plearn_learners/regressors
Message-ID: <200811051506.mA5F6wrI019124@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 16:06:58 +0100 (Wed, 05 Nov 2008)
New Revision: 9642

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
Log:
small optimization.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-11-05 15:04:35 UTC (rev 9641)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-11-05 15:06:58 UTC (rev 9642)
@@ -187,30 +187,34 @@
 
 void RegressionTreeLeave::getOutputAndError(Vec& output, Vec& error)const
 {
-    if(length==0){        
-        output.clear();
+    if(length>0){
+        output[0] = weighted_targets_sum / weights_sum;
+        if (missing_leave != true)
+        {
+            //we put the most frequent case first as an optimisation
+            output[1] = 1.0;
+            error[0] = ((weights_sum * output[0] * output[0]) - 
+                        (2.0 * weighted_targets_sum * output[0]) + weighted_squared_targets_sum)
+                * loss_function_factor;
+            if (error[0] < 1E-10) {error[0] = 0.0;} //PLWARNING("E[0] <1e-10: %f",error[0]);}
+            error[1] = 0.0;
+            real weights_sum_factor  = weights_sum * loss_function_factor;
+            if (error[0] > weights_sum_factor) error[2] = weights_sum_factor;
+            else error[2] = error[0];
+        }
+        else
+        {
+            output[1] = 0.0;
+            error[0] = 0.0;
+            error[1] = weights_sum;
+            error[2] = 0.0;
+        }
+    }else{
         output[0]=MISSING_VALUE;
+        output[1] = 0.0;
         error.clear();
         return;
     }
-        
-    output[0] = weighted_targets_sum / weights_sum;
-    if (missing_leave == true)
-    {
-        output[1] = 0.0;
-        error[0] = 0.0;
-        error[1] = weights_sum;
-        error[2] = 0.0;
-    }
-    else
-    {
-        output[1] = 1.0;
-        error[0] = ((weights_sum * output[0] * output[0]) - (2.0 * weighted_targets_sum * output[0]) + weighted_squared_targets_sum) * loss_function_factor;
-        if (error[0] < 1E-10) error[0] = 0.0;
-        error[1] = 0.0;
-        if (error[0] > weights_sum * loss_function_factor) error[2] = weights_sum * loss_function_factor;
-        else error[2] = error[0];
-    }
 }
 
 void RegressionTreeLeave::printStats()



From nouiz at mail.berlios.de  Wed Nov  5 16:09:05 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 16:09:05 +0100
Subject: [Plearn-commits] r9643 - trunk/plearn_learners/meta
Message-ID: <200811051509.mA5F95hd019469@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 16:09:04 +0100 (Wed, 05 Nov 2008)
New Revision: 9643

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
added cost train_time, total_train_time, test_time, total_test_time in MultiClassAdaBoost.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-05 15:06:58 UTC (rev 9642)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-05 15:09:04 UTC (rev 9643)
@@ -39,6 +39,8 @@
 
 #include "MultiClassAdaBoost.h"
 #include <plearn/vmat/ProcessingVMatrix.h>
+#include <plearn/vmat/SubVMatrix.h>
+#include <plearn/vmat/MemoryVMatrix.h>
 #ifdef _OPENMP
 #include <omp.h>
 #endif
@@ -53,6 +55,10 @@
     " and 2 vs other.");
 
 MultiClassAdaBoost::MultiClassAdaBoost():
+    train_time(0),
+    total_train_time(0),
+    test_time(0),
+    total_test_time(0),
     forward_sub_learner_test_costs(false)
 /* ### Initialize all fields to their default value here */
 {
@@ -104,7 +110,6 @@
 
 void MultiClassAdaBoost::build_()
 {
-    //reuse object to same allocation time
     sub_target_tmp.resize(2);
     for(int i=0;i<sub_target_tmp.size();i++)
         sub_target_tmp[i].resize(1);
@@ -126,7 +131,14 @@
         target_prg1= "@"+targetname+" 1 0 ifelse :"+targetname;
         target_prg2= "@"+targetname+" 2 - 0 1 ifelse :"+targetname;
         weight_prg = "1 :weight";
+        if(learner1->getTrainingSet()){
+            subcosts2.resize(learner2->nTestCosts());
+            subcosts1.resize(learner1->nTestCosts());
+        }
     }
+
+    Profiler::activate();
+    Profiler::reset("MultiClassAdaBoost::test");
 }
 
 // ### Nothing to add here, simply calls build_
@@ -182,6 +194,8 @@
 
 void MultiClassAdaBoost::train()
 {
+    Profiler::start("MultiClassAdaBoost::train");
+
     learner1->nstages = nstages;
     learner2->nstages = nstages;
 
@@ -190,15 +204,15 @@
 #ifdef _OPENMP
     //the AdaBoost and the weak learner should not print anything as this will cause race condition on the printing
     if(omp_get_max_threads()>1){
-        PLCHECK(learner1->verbosity==0);
-        PLCHECK(learner2->verbosity==0);
-        PLCHECK(learner1->report_progress==false);
-        PLCHECK(learner2->report_progress==false);
-        
-        PLCHECK(learner1->weak_learner_template->verbosity==0);
-        PLCHECK(learner2->weak_learner_template->verbosity==0);
-        PLCHECK(learner1->weak_learner_template->report_progress==false);
-        PLCHECK(learner2->weak_learner_template->report_progress==false);
+      PLCHECK(learner1->verbosity==0);
+      PLCHECK(learner2->verbosity==0);
+      PLCHECK(learner1->report_progress==false);
+      PLCHECK(learner2->report_progress==false);
+      
+      PLCHECK(learner1->weak_learner_template->verbosity==0);
+      PLCHECK(learner2->weak_learner_template->verbosity==0);
+      PLCHECK(learner1->weak_learner_template->report_progress==false);
+      PLCHECK(learner2->weak_learner_template->report_progress==false);
     }
 #pragma omp parallel sections default(none)
 {
@@ -222,6 +236,19 @@
         train_stats->append(*(v),"sublearner1.");
     if(v=learner2->getTrainStatsCollector())
         train_stats->append(*(v),"sublearner2.");
+
+    Profiler::end("MultiClassAdaBoost::train");
+    const Profiler::Stats& stats = Profiler::getStats("MultiClassAdaBoost::train");
+    real tmp=stats.wall_duration/Profiler::ticksPerSecond();
+    train_time=tmp - total_train_time;
+    total_train_time=tmp;
+
+    //we get the test_time here as we want the test time for all dataset.
+    //if we put it in the test function, we would have it for one dataset.
+    const Profiler::Stats& stats_test = Profiler::getStats("MultiClassAdaBoost::test");
+    tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
+    test_time=tmp-total_test_time;
+    total_test_time=tmp;  
 }
 
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
@@ -232,9 +259,9 @@
 #ifdef _OPENMP
 #pragma omp parallel sections default(none)
 {
-#pragma omp section 
+#pragma omp section
     learner1->computeOutput(input, output1);
-#pragma omp section 
+#pragma omp section
     learner2->computeOutput(input, output2);
 }
 
@@ -266,19 +293,15 @@
     PLASSERT_MSG(output.length()==outputsize(),
                  "In MultiClassAdaBoost::computeOutputAndCosts -"
                  " output don't have the good length!");
-    output.resize(outputsize());
 
-    subcosts1.resize(learner1->nTestCosts());
-    subcosts2.resize(learner1->nTestCosts());
-
     getSubLearnerTarget(target, sub_target_tmp);
 #ifdef _OPENMP
 #pragma omp parallel sections default(none)
 {
-#pragma omp section 
+#pragma omp section
     learner1->computeOutputAndCosts(input, sub_target_tmp[0],
                                     output1, subcosts1);
-#pragma omp section 
+#pragma omp section
     learner2->computeOutputAndCosts(input, sub_target_tmp[1],
                                     output2, subcosts2);
 }
@@ -320,9 +343,12 @@
 
     costs[4]=costs[5]=costs[6]=0;
     costs[out+4]=1;
-
+    costs[7]=train_time;
+    costs[8]=total_train_time;
+    costs[9]=test_time;
+    costs[10]=total_test_time;
     if(forward_sub_learner_test_costs){
-        costs.resize(7);
+        costs.resize(7+4);
         subcosts1+=subcosts2;
         costs.append(subcosts1);
     }
@@ -351,6 +377,10 @@
 
     costs[4]=costs[5]=costs[6]=0;
     costs[out+4]=1;
+    costs[7]=train_time;
+    costs[8]=total_train_time;
+    costs[9]=test_time;
+    costs[10]=total_test_time;
 
     if(forward_sub_learner_test_costs){
         costs.resize(7);
@@ -393,6 +423,10 @@
     names.append("class0");
     names.append("class1");
     names.append("class2");
+    names.append("train_time");
+    names.append("total_train_time");
+    names.append("test_time");
+    names.append("total_test_time");
     if(forward_sub_learner_test_costs){
         TVec<string> subcosts=learner1->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
@@ -455,11 +489,13 @@
 
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
-    //can't be parallized as the training_set is meaby not thread save...
     if(!learner1->getTrainingSet())
         learner1->setTrainingSet(vmat1, call_forget);
     if(!learner2->getTrainingSet())
         learner2->setTrainingSet(vmat2, call_forget);
+    subcosts2.resize(learner2->nTestCosts());
+    subcosts1.resize(learner1->nTestCosts());
+
 }
 
 void MultiClassAdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-11-05 15:06:58 UTC (rev 9642)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-11-05 15:09:04 UTC (rev 9643)
@@ -64,6 +64,15 @@
     mutable Vec subcosts1;
     mutable Vec subcosts2;
 
+    //! The time it took for the last execution of the train() function
+    real train_time;
+    //! The total time passed in training
+    real total_train_time;
+
+    //! The time it took for the last execution of the test() function
+    real test_time;
+    //! The total time passed in test()
+    real total_test_time;
 public:
     //#####  Public Build Options  ############################################
 



From nouiz at mail.berlios.de  Wed Nov  5 19:34:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 19:34:34 +0100
Subject: [Plearn-commits] r9644 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost: .
	.pytest/PL_MultiClassAdaBoost/expected_results/expdir
	.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
	.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata
	.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata
Message-ID: <200811051834.mA5IYYB2018450@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 19:34:33 +0100 (Wed, 05 Nov 2008)
New Revision: 9644

Removed:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/train_stats.psave
Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn
Log:
corrected test for last commit.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-11-05 18:34:33 UTC (rev 9644)
@@ -269,9 +269,7 @@
 ;
 root = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *10 ->RegressionTreeLeave(
+leave = *10 ->RegressionTreeLeave(
 id = 1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -283,8 +281,7 @@
 weighted_squared_targets_sum = 0.746666666666667589 ;
 loss_function_factor = 2  )
 ;
-leave = *10  ;
-leave_output = 2 [ 0.746666666666665813 1 ] ;
+leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.378311111111112819 0 0.378311111111112819 ] ;
 split_col = 2 ;
 split_balance = 70 ;
@@ -305,9 +302,7 @@
 ;
 left_node = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *13 ->RegressionTreeLeave(
+leave = *13 ->RegressionTreeLeave(
 id = 3 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -319,8 +314,7 @@
 weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
-leave = *13  ;
-leave_output = 2 [ 0.100000000000000089 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.048000000000000001 0 0.048000000000000001 ] ;
 split_col = 2 ;
 split_balance = 24 ;
@@ -341,9 +335,7 @@
 ;
 left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *16 ->RegressionTreeLeave(
+leave = *16 ->RegressionTreeLeave(
 id = 6 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -355,7 +347,6 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-leave = *16  ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -405,9 +396,7 @@
 left_leave = *16  ;
 right_node = *20 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *21 ->RegressionTreeLeave(
+leave = *21 ->RegressionTreeLeave(
 id = 7 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -419,8 +408,7 @@
 weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
-leave = *21  ;
-leave_output = 2 [ 0.5 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0266666666666666684 0 0.0266666666666666684 ] ;
 split_col = 2 ;
 split_balance = 2 ;
@@ -441,9 +429,7 @@
 ;
 left_node = *23 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *24 ->RegressionTreeLeave(
+leave = *24 ->RegressionTreeLeave(
 id = 15 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -455,8 +441,7 @@
 weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
-leave = *24  ;
-leave_output = 2 [ 0.800000000000000044 1 ] ;
+leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
 split_col = 2 ;
 split_balance = 1 ;
@@ -505,9 +490,7 @@
 left_leave = *24  ;
 right_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *29 ->RegressionTreeLeave(
+leave = *29 ->RegressionTreeLeave(
 id = 16 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -519,7 +502,6 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-leave = *29  ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -573,9 +555,7 @@
 left_leave = *13  ;
 right_node = *33 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *34 ->RegressionTreeLeave(
+leave = *34 ->RegressionTreeLeave(
 id = 4 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -587,13 +567,12 @@
 weighted_squared_targets_sum = 0.720000000000000751 ;
 loss_function_factor = 2  )
 ;
-leave = *34  ;
-leave_output = 2 [ 0.981818181818181701 1 ] ;
+leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.026181818181818306 0 0.026181818181818306 ] ;
 split_col = 4 ;
 split_balance = 88 ;
 split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.0218181818181818338 ;
+after_split_error = 0.0218181818181818199 ;
 missing_node = *0 ;
 missing_leave = *35 ->RegressionTreeLeave(
 id = 8 ;
@@ -614,10 +593,10 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666673822 ;
+weights_sum = 0.00666666666666668271 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666673822 ;
-weighted_squared_targets_sum = 0.00666666666666673822 ;
+weighted_targets_sum = 0.00666666666666668271 ;
+weighted_squared_targets_sum = 0.00666666666666668271 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
@@ -760,9 +739,7 @@
 ;
 root = *44 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *45 ->RegressionTreeLeave(
+leave = *45 ->RegressionTreeLeave(
 id = 1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -774,8 +751,7 @@
 weighted_squared_targets_sum = 0.49333333333333268 ;
 loss_function_factor = 2  )
 ;
-leave = *45  ;
-leave_output = 2 [ 0.493333333333331459 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.499911111111112305 0 0.499911111111112305 ] ;
 split_col = 2 ;
 split_balance = 24 ;
@@ -796,9 +772,7 @@
 ;
 left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *48 ->RegressionTreeLeave(
+leave = *48 ->RegressionTreeLeave(
 id = 3 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -810,8 +784,7 @@
 weighted_squared_targets_sum = 0.0866666666666666696 ;
 loss_function_factor = 2  )
 ;
-leave = *48  ;
-leave_output = 2 [ 0.149425287356321879 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.147432950191570877 0 0.147432950191570877 ] ;
 split_col = 1 ;
 split_balance = 31 ;
@@ -832,9 +805,7 @@
 ;
 left_node = *50 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *51 ->RegressionTreeLeave(
+leave = *51 ->RegressionTreeLeave(
 id = 6 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -846,8 +817,7 @@
 weighted_squared_targets_sum = 0.00666666666666666709 ;
 loss_function_factor = 2  )
 ;
-leave = *51  ;
-leave_output = 2 [ 0.0169491525423729021 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.013107344632768362 0 0.013107344632768362 ] ;
 split_col = 3 ;
 split_balance = 53 ;
@@ -896,9 +866,7 @@
 left_leave = *51  ;
 right_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *56 ->RegressionTreeLeave(
+leave = *56 ->RegressionTreeLeave(
 id = 7 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -910,8 +878,7 @@
 weighted_squared_targets_sum = 0.0800000000000000017 ;
 loss_function_factor = 2  )
 ;
-leave = *56  ;
-leave_output = 2 [ 0.428571428571428825 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0914285714285713869 0 0.0914285714285713869 ] ;
 split_col = 2 ;
 split_balance = 18 ;
@@ -932,9 +899,7 @@
 ;
 left_node = *58 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *59 ->RegressionTreeLeave(
+leave = *59 ->RegressionTreeLeave(
 id = 15 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -946,8 +911,7 @@
 weighted_squared_targets_sum = 0.0533333333333333368 ;
 loss_function_factor = 2  )
 ;
-leave = *59  ;
-leave_output = 2 [ 0.34782608695652184 1 ] ;
+leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0695652173913043348 0 0.0695652173913043348 ] ;
 split_col = 2 ;
 split_balance = 15 ;
@@ -996,9 +960,7 @@
 left_leave = *59  ;
 right_node = *63 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *64 ->RegressionTreeLeave(
+leave = *64 ->RegressionTreeLeave(
 id = 16 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1010,8 +972,7 @@
 weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
-leave = *64  ;
-leave_output = 2 [ 0.800000000000000044 1 ] ;
+leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
 split_col = 2 ;
 split_balance = 1 ;
@@ -1037,7 +998,7 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666666189 ;
+weights_sum = 0.00666666666666666536 ;
 targets_sum = 1 ;
 weighted_targets_sum = 0.00666666666666666536 ;
 weighted_squared_targets_sum = 0.00666666666666666536 ;
@@ -1064,9 +1025,7 @@
 left_leave = *48  ;
 right_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *69 ->RegressionTreeLeave(
+leave = *69 ->RegressionTreeLeave(
 id = 4 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1078,8 +1037,7 @@
 weighted_squared_targets_sum = 0.406666666666666177 ;
 loss_function_factor = 2  )
 ;
-leave = *69  ;
-leave_output = 2 [ 0.968253968253968256 1 ] ;
+leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0258201058201057432 0 0.0258201058201057432 ] ;
 split_col = 2 ;
 split_balance = 47 ;
@@ -1105,10 +1063,10 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666668271 ;
+weights_sum = 0.00666666666666665495 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666668271 ;
-weighted_squared_targets_sum = 0.00666666666666668271 ;
+weighted_targets_sum = 0.00666666666666665495 ;
+weighted_squared_targets_sum = 0.00666666666666665495 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-11-05 18:34:33 UTC (rev 9644)
@@ -5,6 +5,10 @@
 class0	0
 class1	0
 class2	0
+train_time	0
+total_train_time	0
+test_time	0
+total_test_time	0
 sum_sublearner.binary_class_error	0
 sum_sublearner.exp_neg_margin	0
 sum_sublearner.class_error	0

Deleted: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_stats.psave	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_stats.psave	2008-11-05 18:34:33 UTC (rev 9644)
@@ -1,486 +0,0 @@
-*1 ->VecStatsCollector(
-maxnvalues = 0 ;
-fieldnames = 22 [ "class_error" "linear_class_error" "square_class_error" "conflict" "class0" "class1" "class2" "sum_sublearner.binary_class_error" "sum_sublearner.exp_neg_margin" "sum_sublearner.class_error" "sum_sublearner.avg_weight_class_0" "sum_sublearner.avg_weight_class_1" "sum_sublearner.weighted_weak_learner.mse" "sum_sublearner.weighted_weak_learner.base_confidence" "sum_sublearner.weighted_weak_learner.base_reward_l2" "sum_sublearner.weighted_weak_learner.base_reward_l1" "sum_sublearner.weighted_weak_learner.class_error" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x1" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x2" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x3" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x4" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_y1" ] ;
-compute_covariance = 0 ;
-epsilon = 0 ;
-window = -1 ;
-full_update_frequency = -1 ;
-window_nan_code = 0 ;
-no_removal_warnings = 0 ;
-stats = 22 [ StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 15 ;
-sumsquare_ = 15 ;
-sumcube_ = 15 ;
-sumfourth_ = 15 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 141 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 15 ;
-sumsquare_ = 15 ;
-sumcube_ = 15 ;
-sumfourth_ = 15 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 141 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 15 ;
-sumsquare_ = 15 ;
-sumcube_ = 15 ;
-sumfourth_ = 15 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 141 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 35 ;
-sumsquare_ = 35 ;
-sumcube_ = 35 ;
-sumfourth_ = 35 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 145 ;
-first_ = 0 ;
-last_ = 1 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = -103 ;
-sumsquare_ = 103 ;
-sumcube_ = -103 ;
-sumfourth_ = 103 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 148 ;
-agemax_ = 149 ;
-first_ = 1 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 68 ;
-sumsquare_ = 68 ;
-sumcube_ = 68 ;
-sumfourth_ = 68 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 148 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 15 ;
-sumsquare_ = 15 ;
-sumcube_ = 15 ;
-sumfourth_ = 15 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 141 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 57.7268015224626438 ;
-sumsquare_ = 256.10470275066541 ;
-sumcube_ = 1323.4839129605939 ;
-sumfourth_ = 7735.67317571679087 ;
-min_ = 0.437741055166937176 ;
-max_ = 7.29488391230979349 ;
-agmemin_ = 149 ;
-agemax_ = 105 ;
-first_ = 0.437741055166937176 ;
-last_ = 0.437741055166937176 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 15 ;
-sumsquare_ = 15 ;
-sumcube_ = 15 ;
-sumfourth_ = 15 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 149 ;
-agemax_ = 141 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 150 ;
-nnonmissing_ = 0 ;
-sumsquarew_ = 0 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = nan ;
-max_ = nan ;
-agmemin_ = nan ;
-agemax_ = nan ;
-first_ = nan ;
-last_ = nan ;
-binary_ = -1 ;
-integer_ = -1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 150 ;
-nnonmissing_ = 0 ;
-sumsquarew_ = 0 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = nan ;
-max_ = nan ;
-agmemin_ = nan ;
-agemax_ = nan ;
-first_ = nan ;
-last_ = nan ;
-binary_ = -1 ;
-integer_ = -1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 20.4918126593811607 ;
-sumsquare_ = 29.254876048119641 ;
-sumcube_ = 43.9580698251016742 ;
-sumfourth_ = 69.7007002426492619 ;
-min_ = 0 ;
-max_ = 1.94591014905531323 ;
-agmemin_ = 149 ;
-agemax_ = 105 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 3.16708366673991559 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 3.16708366673991559 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = -40.9836253187623214 ;
-sumsquare_ = 117.019504192478593 ;
-sumcube_ = -351.66455860081345 ;
-sumfourth_ = 1115.21120388238842 ;
-min_ = -0.724736631370711093 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 105 ;
-agemax_ = 149 ;
-first_ = 3.16708366673991559 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = -40.9836253187623214 ;
-sumsquare_ = 117.019504192478593 ;
-sumcube_ = -351.66455860081345 ;
-sumfourth_ = 1115.21120388238842 ;
-min_ = -0.724736631370711093 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 105 ;
-agemax_ = 149 ;
-first_ = 3.16708366673991559 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 20.4918126593811607 ;
-sumsquare_ = 29.254876048119641 ;
-sumcube_ = 43.9580698251016742 ;
-sumfourth_ = 69.7007002426492619 ;
-min_ = 0 ;
-max_ = 1.94591014905531323 ;
-agmemin_ = 149 ;
-agemax_ = 105 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 149 ;
-agemax_ = 149 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = -76.9339316141300174 ;
-sumsquare_ = 93.9496798985337023 ;
-sumcube_ = -114.728861087034772 ;
-sumfourth_ = 140.103846873602322 ;
-min_ = 0 ;
-max_ = 1.22117351768460214 ;
-agmemin_ = 148 ;
-agemax_ = 149 ;
-first_ = 1.22117351768460214 ;
-last_ = 1.22117351768460214 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = -627.206680570492381 ;
-sumsquare_ = 2809.0382900802183 ;
-sumcube_ = -12982.9982919796639 ;
-sumfourth_ = 61788.1752789862949 ;
-min_ = 3.16708366673991559 ;
-max_ = 9.00481411390585507 ;
-agmemin_ = 140 ;
-agemax_ = 149 ;
-first_ = 9.00481411390585507 ;
-last_ = 5.11299381579522816 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = -48.8576653395287863 ;
-sumsquare_ = 256.875215049057431 ;
-sumcube_ = 70.0667038794879176 ;
-sumfourth_ = 661.190924265666354 ;
-min_ = 0 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 148 ;
-agemax_ = 145 ;
-first_ = 1.22117351768460214 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 150 ;
-sumsquarew_ = 150 ;
-sum_ = 219.887846843250685 ;
-sumsquare_ = 427.881992826202236 ;
-sumcube_ = 832.61991243851935 ;
-sumfourth_ = 1620.20353791965613 ;
-min_ = 0 ;
-max_ = 1.94591014905531323 ;
-agmemin_ = 149 ;
-agemax_ = 148 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-] ;
-cov = 0  0  [ 
-]
-;
-sum_cross = 0  0  [ 
-]
-;
-sum_cross_weights = 0  0  [ 
-]
-;
-sum_cross_square_weights = 0  0  [ 
-]
-;
-sum_non_missing_weights = 0 ;
-sum_non_missing_square_weights = 0  )

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-11-05 18:34:33 UTC (rev 9644)
@@ -5,6 +5,10 @@
 class0	0
 class1	0
 class2	0
+train_time	0
+total_train_time	0
+test_time	0
+total_test_time	0
 sum_sublearner.binary_class_error	0
 sum_sublearner.exp_neg_margin	0
 sum_sublearner.class_error	0

Deleted: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_stats.psave	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_stats.psave	2008-11-05 18:34:33 UTC (rev 9644)
@@ -1,486 +0,0 @@
-*1 ->VecStatsCollector(
-maxnvalues = 0 ;
-fieldnames = 22 [ "class_error" "linear_class_error" "square_class_error" "conflict" "class0" "class1" "class2" "sum_sublearner.binary_class_error" "sum_sublearner.exp_neg_margin" "sum_sublearner.class_error" "sum_sublearner.avg_weight_class_0" "sum_sublearner.avg_weight_class_1" "sum_sublearner.weighted_weak_learner.mse" "sum_sublearner.weighted_weak_learner.base_confidence" "sum_sublearner.weighted_weak_learner.base_reward_l2" "sum_sublearner.weighted_weak_learner.base_reward_l1" "sum_sublearner.weighted_weak_learner.class_error" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x1" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x2" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x3" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_x4" "sum_sublearner.weighted_weak_learner.SPLIT_VAR_y1" ] ;
-compute_covariance = 0 ;
-epsilon = 0 ;
-window = -1 ;
-full_update_frequency = -1 ;
-window_nan_code = 0 ;
-no_removal_warnings = 0 ;
-stats = 22 [ StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -39 ;
-sumsquare_ = 39 ;
-sumcube_ = -39 ;
-sumfourth_ = 39 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 48 ;
-agemax_ = 49 ;
-first_ = 1 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -38 ;
-sumsquare_ = 40 ;
-sumcube_ = -38 ;
-sumfourth_ = 40 ;
-min_ = 0 ;
-max_ = 2 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 1 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -36 ;
-sumsquare_ = 48 ;
-sumcube_ = -12 ;
-sumfourth_ = 120 ;
-min_ = 0 ;
-max_ = 4 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 1 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -34 ;
-sumsquare_ = 34 ;
-sumcube_ = -34 ;
-sumfourth_ = 34 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 48 ;
-agemax_ = 49 ;
-first_ = 1 ;
-last_ = 1 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 15 ;
-sumsquare_ = 15 ;
-sumcube_ = 15 ;
-sumfourth_ = 15 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 49 ;
-agemax_ = 48 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 19 ;
-sumsquare_ = 19 ;
-sumcube_ = 19 ;
-sumfourth_ = 19 ;
-min_ = 0 ;
-max_ = 1 ;
-agmemin_ = 49 ;
-agemax_ = 47 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -38 ;
-sumsquare_ = 40 ;
-sumcube_ = -38 ;
-sumfourth_ = 40 ;
-min_ = 0 ;
-max_ = 2 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 1 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -279.375737460878668 ;
-sumsquare_ = 1899.95920013866203 ;
-sumcube_ = -12757.6924101691002 ;
-sumfourth_ = 87117.962449798797 ;
-min_ = 0.437741055166937176 ;
-max_ = 10.3911649915626327 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 7.29488391230979349 ;
-last_ = 0.437741055166937176 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -38 ;
-sumsquare_ = 40 ;
-sumcube_ = -38 ;
-sumfourth_ = 40 ;
-min_ = 0 ;
-max_ = 2 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 1 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 50 ;
-nnonmissing_ = 0 ;
-sumsquarew_ = 0 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = nan ;
-max_ = nan ;
-agmemin_ = nan ;
-agemax_ = nan ;
-first_ = nan ;
-last_ = nan ;
-binary_ = -1 ;
-integer_ = -1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 50 ;
-nnonmissing_ = 0 ;
-sumsquarew_ = 0 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = nan ;
-max_ = nan ;
-agmemin_ = nan ;
-agemax_ = nan ;
-first_ = nan ;
-last_ = nan ;
-binary_ = -1 ;
-integer_ = -1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -77.5682688209554385 ;
-sumsquare_ = 151.268323519358859 ;
-sumcube_ = -287.065953431055107 ;
-sumfourth_ = 562.512684046600498 ;
-min_ = 0 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 1.94591014905531323 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 3.16708366673991559 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 3.16708366673991559 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 155.136537641910934 ;
-sumsquare_ = 605.073294077435435 ;
-sumcube_ = 2296.52762744844085 ;
-sumfourth_ = 9000.20294474560978 ;
-min_ = -3.16708366673991559 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 1 ;
-agemax_ = 48 ;
-first_ = -0.724736631370711093 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 155.136537641910934 ;
-sumsquare_ = 605.073294077435435 ;
-sumcube_ = 2296.52762744844085 ;
-sumfourth_ = 9000.20294474560978 ;
-min_ = -3.16708366673991559 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 1 ;
-agemax_ = 48 ;
-first_ = -0.724736631370711093 ;
-last_ = 3.16708366673991559 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -77.5682688209554385 ;
-sumsquare_ = 151.268323519358859 ;
-sumcube_ = -287.065953431055107 ;
-sumfourth_ = 562.512684046600498 ;
-min_ = 0 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 48 ;
-agemax_ = 1 ;
-first_ = 1.94591014905531323 ;
-last_ = 0 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 49 ;
-agemax_ = 49 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -18.3176027652690259 ;
-sumsquare_ = 22.3689714044127861 ;
-sumcube_ = -27.3163954969130316 ;
-sumfourth_ = 33.3580587794291077 ;
-min_ = 0 ;
-max_ = 1.22117351768460214 ;
-agmemin_ = 47 ;
-agemax_ = 49 ;
-first_ = 1.22117351768460214 ;
-last_ = 1.22117351768460214 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -21.4739617998642416 ;
-sumsquare_ = 48.208806675653527 ;
-sumcube_ = -63.4330204884323621 ;
-sumfourth_ = 148.126424077304165 ;
-min_ = 3.16708366673991559 ;
-max_ = 7.05890396485054161 ;
-agmemin_ = 46 ;
-agemax_ = 0 ;
-first_ = 5.11299381579522816 ;
-last_ = 7.05890396485054161 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = -98.6361931590509897 ;
-sumsquare_ = 288.626136885233507 ;
-sumcube_ = -867.862578938199249 ;
-sumfourth_ = 2658.61345294527882 ;
-min_ = 0 ;
-max_ = 3.16708366673991559 ;
-agmemin_ = 48 ;
-agemax_ = 49 ;
-first_ = 3.16708366673991559 ;
-last_ = 1.22117351768460214 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 50 ;
-sumsquarew_ = 50 ;
-sum_ = 68.1068552169359691 ;
-sumsquare_ = 132.529820786876485 ;
-sumcube_ = 257.891123321664736 ;
-sumfourth_ = 501.832954222902686 ;
-min_ = 0 ;
-max_ = 1.94591014905531323 ;
-agmemin_ = 49 ;
-agemax_ = 48 ;
-first_ = 0 ;
-last_ = 1.94591014905531323 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-] ;
-cov = 0  0  [ 
-]
-;
-sum_cross = 0  0  [ 
-]
-;
-sum_cross_weights = 0  0  [ 
-]
-;
-sum_cross_square_weights = 0  0  [ 
-]
-;
-sum_non_missing_weights = 0 ;
-sum_non_missing_square_weights = 0  )

Deleted: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/train_stats.psave	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/train_stats.psave	2008-11-05 18:34:33 UTC (rev 9644)
@@ -1,192 +0,0 @@
-*1 ->VecStatsCollector(
-maxnvalues = 0 ;
-fieldnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[linear_class_error]]" "E[test1.E[square_class_error]]" "E[test1.E[conflict]]" "E[test2.E[class_error]]" "E[test2.E[linear_class_error]]" "E[test2.E[square_class_error]]" "E[test2.E[conflict]]" ] ;
-compute_covariance = 0 ;
-epsilon = 0 ;
-window = -1 ;
-full_update_frequency = -1 ;
-window_nan_code = 0 ;
-no_removal_warnings = 0 ;
-stats = 8 [ StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0.100000000000000006 ;
-max_ = 0.100000000000000006 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0.100000000000000006 ;
-last_ = 0.100000000000000006 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0.100000000000000006 ;
-max_ = 0.100000000000000006 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0.100000000000000006 ;
-last_ = 0.100000000000000006 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0.100000000000000006 ;
-max_ = 0.100000000000000006 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0.100000000000000006 ;
-last_ = 0.100000000000000006 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0.220000000000000001 ;
-max_ = 0.220000000000000001 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0.220000000000000001 ;
-last_ = 0.220000000000000001 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0.239999999999999991 ;
-max_ = 0.239999999999999991 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0.239999999999999991 ;
-last_ = 0.239999999999999991 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0.280000000000000027 ;
-max_ = 0.280000000000000027 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0.280000000000000027 ;
-last_ = 0.280000000000000027 ;
-binary_ = 0 ;
-integer_ = 0 ;
-counts = {};
-more_than_maxnvalues = 1  )
-StatsCollector(
-epsilon = 0 ;
-maxnvalues = 0 ;
-no_removal_warnings = 0 ;
-nmissing_ = 0 ;
-nnonmissing_ = 1 ;
-sumsquarew_ = 1 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 0 ;
-max_ = 0 ;
-agmemin_ = 0 ;
-agemax_ = 0 ;
-first_ = 0 ;
-last_ = 0 ;
-binary_ = 1 ;
-integer_ = 1 ;
-counts = {};
-more_than_maxnvalues = 1  )
-] ;
-cov = 0  0  [ 
-]
-;
-sum_cross = 0  0  [ 
-]
-;
-sum_cross_weights = 0  0  [ 
-]
-;
-sum_cross_square_weights = 0  0  [ 
-]
-;
-sum_non_missing_weights = 0 ;
-sum_non_missing_square_weights = 0  )

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/experiment.plearn	2008-11-05 18:34:33 UTC (rev 9644)
@@ -70,6 +70,7 @@
             save_initial_learners = 0,
             save_initial_tester = 0,
             save_learners = 0,
+            save_split_stats = 1,
             save_test_confidence = 0,
             save_test_costs = 0,
             save_test_names = 0,
@@ -97,6 +98,7 @@
     provide_learner_expdir = 1,
     save_learners = 1,
     save_split_stats = 0,
+    save_stat_collectors = 0,
     save_test_confidence = 0,
     save_test_costs = 1,
     save_test_outputs = 1,

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-11-05 18:34:33 UTC (rev 9644)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9601"
+__REVISION__ = "PL9643"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/test_cost_names.txt	2008-11-05 18:34:33 UTC (rev 9644)
@@ -5,6 +5,10 @@
 class0
 class1
 class2
+train_time
+total_train_time
+test_time
+total_test_time
 sum_sublearner.binary_class_error
 sum_sublearner.exp_neg_margin
 sum_sublearner.class_error

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2008-11-05 18:34:33 UTC (rev 9644)
@@ -393,7 +393,7 @@
 perf_evaluators = {};
 report_stats = 1 ;
 save_initial_tester = 1 ;
-save_stat_collectors = 1 ;
+save_stat_collectors = 0 ;
 save_split_stats = 0 ;
 save_learners = 1 ;
 save_initial_learners = 0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn	2008-11-05 15:09:04 UTC (rev 9643)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/PL_MutiClassAdaBoost.pyplearn	2008-11-05 18:34:33 UTC (rev 9644)
@@ -59,6 +59,7 @@
         provide_learner_expdir = 1  ,
         save_test_confidence = 0  ,
         save_test_names = 0,
+        save_split_stats = 1
         ),
     strategy = [
 
@@ -96,7 +97,8 @@
     save_test_outputs = 1,
     save_test_confidence = 0,
     save_learners = 1,
-    save_split_stats = 0#not need as their is only one split
+    save_split_stats = 0,#not need as their is only one split
+    save_stat_collectors = 0
     )
 
 def main():



From nouiz at mail.berlios.de  Wed Nov  5 22:23:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 22:23:16 +0100
Subject: [Plearn-commits] r9645 - trunk/plearn/vmat
Message-ID: <200811052123.mA5LNGNl015402@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 22:23:16 +0100 (Wed, 05 Nov 2008)
New Revision: 9645

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
bugfix.


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-05 18:34:33 UTC (rev 9644)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-05 21:23:16 UTC (rev 9645)
@@ -239,8 +239,8 @@
             for(int i=0;i<source.width();i++)
             {
                 bool found=false;
-                for(int j=0;j<indices.size();j++)
-                    if(indices[j]==i){
+                for(int j=0;j<sel_indices.size();j++)
+                    if(sel_indices[j]==i){
                         found=true;
                         break;
                     }



From nouiz at mail.berlios.de  Wed Nov  5 22:24:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 5 Nov 2008 22:24:17 +0100
Subject: [Plearn-commits] r9646 - trunk/plearn/vmat
Message-ID: <200811052124.mA5LOH2i015446@sheep.berlios.de>

Author: nouiz
Date: 2008-11-05 22:24:17 +0100 (Wed, 05 Nov 2008)
New Revision: 9646

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
transformed a warning in an error, as if we continue, we will crash when we try to get a row.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-05 21:23:16 UTC (rev 9645)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-05 21:24:17 UTC (rev 9646)
@@ -330,18 +330,23 @@
     imputation_spec = save_imputation_spec;
 
     if(nofields.length()>0 && missing_field_error)
-      PLERROR("In MeanMedianModeImputationVMatrix::build_() Their is %d fields in the imputation_spec that are not in train set: %s",nofields.length(),
+      PLERROR("In MeanMedianModeImputationVMatrix::build_() Their is %d"
+	      " fields in the imputation_spec that are not in train set:"
+	      " %s",nofields.length(),
 	      tostring(nofields).c_str());
     else if(nofields.length()>0)
-      PLWARNING("In MeanMedianModeImputationVMatrix::build_() Their is %d fields in the imputation_spec that are not in train set: %s",nofields.length(),
+      PLWARNING("In MeanMedianModeImputationVMatrix::build_() Their is %d"
+		" fields in the imputation_spec that are not in train set:"
+		" %s",nofields.length(),
 		tostring(nofields).c_str());
     TVec<string> no_instruction;
     for(int i = 0;i<variable_imputation_instruction.size();i++)
       if(variable_imputation_instruction[i]==0)
 	no_instruction.append(train_field_names[i]);
     if(no_instruction.size()>0)
-      PLWARNING("In MeanMedianModeImputationVMatrix::build_() In the source VMatrix their is %d field(s) that do not have instruction: '%s'.",
-		no_instruction.size(),tostring(no_instruction).c_str());
+      PLERROR("In MeanMedianModeImputationVMatrix::build_() In the source"
+	      " VMatrix their is %d field(s) that do not have instruction: '%s'.",
+	      no_instruction.size(),tostring(no_instruction).c_str());
 
 }
 void MeanMedianModeImputationVMatrix::setMetaDataDir(const PPath& the_metadatadir)



From ducharme at mail.berlios.de  Wed Nov  5 22:28:16 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 5 Nov 2008 22:28:16 +0100
Subject: [Plearn-commits] r9647 - trunk/plearn_learners/regressors
Message-ID: <200811052128.mA5LSGjY015951@sheep.berlios.de>

Author: ducharme
Date: 2008-11-05 22:28:16 +0100 (Wed, 05 Nov 2008)
New Revision: 9647

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
   trunk/plearn_learners/regressors/BasisSelectionRegressor.h
Log:
Ajout de l'option "use_all_basis".


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-11-05 21:24:17 UTC (rev 9646)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-11-05 21:28:16 UTC (rev 9647)
@@ -83,6 +83,7 @@
       precompute_features(true),
       n_threads(0),
       thread_subtrain_length(0),
+      use_all_basis(false),
       residue_sum(0),
       residue_sum_sq(0)
 {}
@@ -214,6 +215,11 @@
                   OptionBase::buildoption,
                   "Preload thread_subtrain_length data when using multi-threading.");
 
+    declareOption(ol, "use_all_basis", &BasisSelectionRegressor::use_all_basis,
+                  OptionBase::buildoption,
+                  "If true, we use the underlying learner on all basis functions generated by the BSR.\n"
+                  "In this special way, all interaction terms are shut down and only 1 stage of training is necessary");
+
     //#####  Public Learnt Options  ############################################
 
     declareOption(ol, "selected_functions", &BasisSelectionRegressor::selected_functions,
@@ -251,6 +257,11 @@
 
 void BasisSelectionRegressor::build_()
 {
+    if (use_all_basis)
+    {
+        PLASSERT_MSG(nstages == 1, "\"nstages\" must be 1 when \"use_all_basis\" is true");
+        PLASSERT_MSG(!consider_interaction_terms, "\"consider_interaction_terms\" must be false when \"use_all_basis\" is true");
+    }
 }
 
 
@@ -594,6 +605,13 @@
         interaction_candidate_functions.resize(max_interaction_terms);
     }
     candidate_functions.append(interaction_candidate_functions);
+
+    // If use_all_basis, append all candidate_functions to selected_functions
+    if (use_all_basis)
+    {
+        while (candidate_functions.length() > 0)
+            appendFunctionToSelection(0);
+    }
 }
 
 void BasisSelectionRegressor::addInteractionFunction(RealFunc& f1, RealFunc& f2, TVec<RealFunc>& all_functions)
@@ -632,7 +650,7 @@
 TVec<RealFunc> BasisSelectionRegressor::buildTopCandidateFunctions()
 {
     // The scores matrix should match (in size) the candidate_functions matrix
-    assert(scores.length() == candidate_functions.length());
+    PLASSERT(scores.length() == candidate_functions.length());
 
     sortRows(scores, 1, false);
     TVec<RealFunc> top_best_functions;

Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-11-05 21:24:17 UTC (rev 9646)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2008-11-05 21:28:16 UTC (rev 9647)
@@ -88,6 +88,7 @@
     bool precompute_features;
     int n_threads;
     int thread_subtrain_length;
+    bool use_all_basis;
 
     //#####  Public Learnt Options  ############################################
     TVec<RealFunc> selected_functions;



From ducharme at mail.berlios.de  Wed Nov  5 22:52:35 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 5 Nov 2008 22:52:35 +0100
Subject: [Plearn-commits] r9648 - trunk/plearn/vmat
Message-ID: <200811052152.mA5LqZHP018287@sheep.berlios.de>

Author: ducharme
Date: 2008-11-05 22:52:35 +0100 (Wed, 05 Nov 2008)
New Revision: 9648

Modified:
   trunk/plearn/vmat/VMat_linalg.cc
Log:
Correction to "outputwise_sum_squared_Y" to take care of the weights.


Modified: trunk/plearn/vmat/VMat_linalg.cc
===================================================================
--- trunk/plearn/vmat/VMat_linalg.cc	2008-11-05 21:28:16 UTC (rev 9647)
+++ trunk/plearn/vmat/VMat_linalg.cc	2008-11-05 21:52:35 UTC (rev 9648)
@@ -319,7 +319,7 @@
             externalProductScaleAcc(XtY, x,y,gamma_i);
             sum_squared_Y += gamma_i * dot(y,y);
             sum_gammas += gamma_i;
-            y *= y;                                //!< element-wise square
+            y *= gamma_i*y;                                //!< element-wise square
             outputwise_sum_squared_Y += y;
         }
     }



From ducharme at mail.berlios.de  Thu Nov  6 17:07:35 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 6 Nov 2008 17:07:35 +0100
Subject: [Plearn-commits] r9649 - trunk/plearn/math
Message-ID: <200811061607.mA6G7ZY4013495@sheep.berlios.de>

Author: ducharme
Date: 2008-11-06 17:07:34 +0100 (Thu, 06 Nov 2008)
New Revision: 9649

Modified:
   trunk/plearn/math/TMat_sort.h
Log:
Can now sort elements in reverse order.


Modified: trunk/plearn/math/TMat_sort.h
===================================================================
--- trunk/plearn/math/TMat_sort.h	2008-11-05 21:52:35 UTC (rev 9648)
+++ trunk/plearn/math/TMat_sort.h	2008-11-06 16:07:34 UTC (rev 9649)
@@ -110,8 +110,12 @@
 
 //! Sorts the elements of vec in place
 template<class T>
-inline void sortElements(const TVec<T>& vec) 
-{ sort(vec.begin(),vec.end()); }
+inline void sortElements(const TVec<T>& vec, bool reverse_elems=false)
+{
+    sort(vec.begin(), vec.end());
+    if (reverse_elems)
+        reverse(vec.begin(), vec.end());
+}
 
 
 //! Uses partial_sort.



From nouiz at mail.berlios.de  Thu Nov  6 17:34:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Nov 2008 17:34:40 +0100
Subject: [Plearn-commits] r9650 - trunk/plearn/vmat
Message-ID: <200811061634.mA6GYeL5015406@sheep.berlios.de>

Author: nouiz
Date: 2008-11-06 17:34:39 +0100 (Thu, 06 Nov 2008)
New Revision: 9650

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
Log:
added that option default_instruction.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-06 16:07:34 UTC (rev 9649)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-06 16:34:39 UTC (rev 9650)
@@ -93,6 +93,11 @@
 		" imputation_spec are present but not in the source. Otherwise"
 		" will generate a warning..");
 
+  declareOption(ol, "default_instruction", &MeanMedianModeImputationVMatrix::default_instruction,
+		OptionBase::buildoption, 
+                "The default instruction to use. If empty(default), will generate"
+		" an error is some source variable don't have an one in imputation_spec.");
+
   declareOption(ol, "variable_mean", &MeanMedianModeImputationVMatrix::variable_mean, OptionBase::learntoption, 
                 "The vector of variable means observed from the train set.");
 
@@ -275,7 +280,18 @@
     variable_median.resize(train_width);
     variable_mode.resize(train_width);
     variable_imputation_instruction.resize(train_width);
-    variable_imputation_instruction.clear();
+
+    if(default_instruction.empty()) variable_imputation_instruction.clear();
+    else if (default_instruction == "mean") variable_imputation_instruction.fill(1);
+    else if (default_instruction == "median") variable_imputation_instruction.fill(2);
+    else if (default_instruction == "mode") variable_imputation_instruction.fill(3);
+    else if (default_instruction == "none") variable_imputation_instruction.fill(4);
+    else if (default_instruction == "err") variable_imputation_instruction.fill(5);
+    else
+      PLERROR("In MeanMedianModeImputationVMatrix: unsupported default_imputation instruction: %s ",
+	      default_instruction.c_str());
+
+
     TVec<string> nofields;
     
     //We sho

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-11-06 16:07:34 UTC (rev 9649)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-11-06 16:34:39 UTC (rev 9650)
@@ -76,8 +76,8 @@
   //!if true will generate an error if the imputation_spec reference a
   //! field not in the source. Otherwise generate a warning.
   bool                          missing_field_error;
+  string                        default_instruction;
 
-
                         MeanMedianModeImputationVMatrix();
   virtual               ~MeanMedianModeImputationVMatrix();
 



From nouiz at mail.berlios.de  Thu Nov  6 17:35:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Nov 2008 17:35:57 +0100
Subject: [Plearn-commits] r9651 - trunk/plearn/vmat
Message-ID: <200811061635.mA6GZvix015545@sheep.berlios.de>

Author: nouiz
Date: 2008-11-06 17:35:57 +0100 (Thu, 06 Nov 2008)
New Revision: 9651

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
generate a warning instead of an error and erase the old file.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-11-06 16:34:39 UTC (rev 9650)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-11-06 16:35:57 UTC (rev 9651)
@@ -341,10 +341,13 @@
         if(isfile(save_deleted_columns)){
             TVec<int> indices2;
             PLearn::load(save_deleted_columns, indices2);
-            if(indices!=indices2)
-                PLERROR("In VariableDeletionVMatrix::build_() - the calculated"
-                        " indices(%d) differ from the saved indices(%d) in file '%s'! ",
+            if(indices!=indices2){
+                PLWARNING("In VariableDeletionVMatrix::build_() - the calculated"
+                          " indices(%d) differ from the saved indices(%d) in file '%s'."
+                          " We overwrite it.",
                         indices2.length(), indices.length(), save_deleted_columns.c_str());
+                PLearn::save(save_deleted_columns,indices);
+            }
         }else{
             PLearn::save(save_deleted_columns,indices);
         }



From nouiz at mail.berlios.de  Thu Nov  6 18:27:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Nov 2008 18:27:11 +0100
Subject: [Plearn-commits] r9652 - trunk/plearn/math
Message-ID: <200811061727.mA6HRBp4019300@sheep.berlios.de>

Author: nouiz
Date: 2008-11-06 18:27:10 +0100 (Thu, 06 Nov 2008)
New Revision: 9652

Modified:
   trunk/plearn/math/TMat_decl.h
Log:
better error.


Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2008-11-06 16:35:57 UTC (rev 9651)
+++ trunk/plearn/math/TMat_decl.h	2008-11-06 17:27:10 UTC (rev 9652)
@@ -524,7 +524,14 @@
                         for(int j=0; j<width_; j++)
                         {
                             in.skipBlanksAndCommentsAndSeparators();
-                            in >> ptr[j];
+                            try{
+                                in >> ptr[j];
+                            }
+                            catch(const PLearnError& e) {
+                                PLERROR("In TMat::read() - Error while reading a serialised TMat<T>."
+                                        " Did you set correctly the mat size?\n"
+                                        "%s",e.message().c_str());
+                            }
                         }
                     in.skipBlanksAndCommentsAndSeparators();
                     c = in.get();



From nouiz at mail.berlios.de  Thu Nov  6 20:21:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Nov 2008 20:21:39 +0100
Subject: [Plearn-commits] r9653 - trunk/plearn_learners/hyper
Message-ID: <200811061921.mA6JLdSa020130@sheep.berlios.de>

Author: nouiz
Date: 2008-11-06 20:21:39 +0100 (Thu, 06 Nov 2008)
New Revision: 9653

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
added a new warning to help user to use correctly the auto_save mechanism.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-06 17:27:10 UTC (rev 9652)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-06 19:21:39 UTC (rev 9653)
@@ -196,6 +196,11 @@
 
     if (call_forget)
     {
+        if(reloaded)
+            PLWARNING("In HyperLearner::setTrainingSet() - we where asked to"
+                      " forget after having reloaded a previous version."
+                      " To don't do this, in the PTester that include this"
+                      " HyperLearner set call_forget_in_run = 0.");
         build();
         forget();
     }



From nouiz at mail.berlios.de  Thu Nov  6 22:59:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Nov 2008 22:59:31 +0100
Subject: [Plearn-commits] r9654 - trunk/plearn_learners/meta
Message-ID: <200811062159.mA6LxVCr001605@sheep.berlios.de>

Author: nouiz
Date: 2008-11-06 22:59:31 +0100 (Thu, 06 Nov 2008)
New Revision: 9654

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
bugfix when reloading the learner.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-06 19:21:39 UTC (rev 9653)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-06 21:59:31 UTC (rev 9654)
@@ -115,8 +115,10 @@
         sub_target_tmp[i].resize(1);
     
     if(learner_template){
-        learner1= ::PLearn::deepCopy(learner_template);
-        learner2= ::PLearn::deepCopy(learner_template);
+        if(!learner1)
+            learner1 = ::PLearn::deepCopy(learner_template);
+        if(!learner2)
+            learner2 = ::PLearn::deepCopy(learner_template);
     }
     if(learner1)
         output1.resize(learner1->outputsize());



From nouiz at mail.berlios.de  Fri Nov  7 17:57:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 17:57:40 +0100
Subject: [Plearn-commits] r9655 - trunk/plearn_learners/meta
Message-ID: <200811071657.mA7GveY8024973@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 17:57:39 +0100 (Fri, 07 Nov 2008)
New Revision: 9655

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
small bugfix and code refactoring.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-06 21:59:31 UTC (rev 9654)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-07 16:57:39 UTC (rev 9655)
@@ -128,15 +128,12 @@
         train_stats=new VecStatsCollector();
 
     if(train_set){
-        targetname = train_set->fieldName(train_set->inputsize());
-        input_prg  = "[%0:%"+tostring(train_set->inputsize()-1)+"]";
-        target_prg1= "@"+targetname+" 1 0 ifelse :"+targetname;
-        target_prg2= "@"+targetname+" 2 - 0 1 ifelse :"+targetname;
-        weight_prg = "1 :weight";
-        if(learner1->getTrainingSet()){
-            subcosts2.resize(learner2->nTestCosts());
-            subcosts1.resize(learner1->nTestCosts());
-        }
+        if(learner1 && learner2)
+            if(! learner1->getTrainingSet()
+               || ! learner2->getTrainingSet()
+               || targetname.empty()
+                )
+                setTrainingSet(train_set);
     }
 
     Profiler::activate();
@@ -385,7 +382,7 @@
     costs[10]=total_test_time;
 
     if(forward_sub_learner_test_costs){
-        costs.resize(7);
+        costs.resize(7+4);
         subcosts1.resize(learner1->nTestCosts());
         subcosts2.resize(learner1->nTestCosts());
         getSubLearnerTarget(target, sub_target_tmp);
@@ -477,7 +474,6 @@
 void MultiClassAdaBoost::setTrainingSet(VMat training_set, bool call_forget)
 { 
     PLCHECK(learner1 && learner2);
-    inherited::setTrainingSet(training_set, call_forget);
 
     targetname = training_set->fieldName(training_set->inputsize());
     input_prg  = "[%0:%"+tostring(training_set->inputsize()-1)+"]";
@@ -498,12 +494,15 @@
     subcosts2.resize(learner2->nTestCosts());
     subcosts1.resize(learner1->nTestCosts());
 
+    inherited::setTrainingSet(training_set, call_forget);
 }
 
 void MultiClassAdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,
                               VMat testoutputs, VMat testcosts) const
 {
     Profiler::pl_profile_start("MultiClassAdaBoost::test");
+    subcosts1.resize(learner1->nTestCosts());
+    subcosts2.resize(learner2->nTestCosts());
     inherited::test(testset,test_stats,testoutputs,testcosts);
     Profiler::pl_profile_end("MultiClassAdaBoost::test");
 }



From nouiz at mail.berlios.de  Fri Nov  7 18:08:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 18:08:07 +0100
Subject: [Plearn-commits] r9656 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results:
	. expdir expdir/Split0 expdir/Split0/LearnerExpdir
Message-ID: <200811071708.mA7H87sD025824@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 18:08:05 +0100 (Fri, 07 Nov 2008)
New Revision: 9656

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
Log:
fixed test following the bugfix in commit r9654.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2008-11-07 16:57:39 UTC (rev 9655)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2008-11-07 17:08:05 UTC (rev 9656)
@@ -1,121 +1,121 @@
 HyperLearner: starting the optimization
 split_cols: 2 2 2 
-split_values: 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 
-weak learner at stage 0 has average loss = 0.02
-split_cols: 2 1 2 
-split_values: 0.991025168386145405 0.482293993618237549 0.891579732096156263 
-weak learner at stage 0 has average loss = 0.08
-split_cols: 4 3 2 
-split_values: 3.23307269844974599e-10 0.698651129400676418 0.000537488498421501149 
-weak learner at stage 1 has average loss = 0.0306122
-split_cols: 4 1 1 
-split_values: 1.54709578481515564e-13 0.588552410435003615 0.568316236782665074 
-weak learner at stage 1 has average loss = 0.13587
-split_cols: 2 0 3 
-split_values: 0.000528285193333644099 0.624507340564582236 0.406995257960652612 
-weak learner at stage 2 has average loss = 0.136257
-split_cols: 1 3 2 
-split_values: 0.526575453102100632 0.924226804347039965 0.995245802370935517 
-weak learner at stage 2 has average loss = 0.204444
-split_cols: 2 3 1 
-split_values: 0.000528285193333644099 0.406995257960652612 0.399391565505198165 
-weak learner at stage 3 has average loss = 0.119855
-split_cols: 2 0 3 
-split_values: 0.997650553369808346 0.441781515973124872 0.310957185430505323 
-weak learner at stage 3 has average loss = 0.179198
-split_cols: 2 1 2 
-split_values: 0.196634593877310471 0.36967671457248541 0.00125079586853901747 
-weak learner at stage 4 has average loss = 0.103659
-split_cols: 3 3 2 
-split_values: 0.141930657011306749 0.0564509465831030677 0.997650553369808346 
-weak learner at stage 4 has average loss = 0.27129
-split_cols: 0 3 1 
-split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
-weak learner at stage 5 has average loss = 0.0730531
-split_cols: 2 4 4 
-split_values: 0.00363231682035125569 0.999999964757892545 0.999999999538717876 
-weak learner at stage 5 has average loss = 0.217388
-split_cols: 2 0 3 
-split_values: 0.00125079586853901747 0.624507340564582236 0.763338630405507312 
-weak learner at stage 6 has average loss = 0.140932
-split_cols: 1 1 1 
-split_values: 0.564858493389006289 0.479480044756096346 0.379258691801199865 
-weak learner at stage 6 has average loss = 0.262423
-split_cols: 2 3 1 
-split_values: 0.000357032461916012567 0.130712305658957473 0.399391565505198165 
-weak learner at stage 7 has average loss = 0.214419
-split_cols: 4 1 0 
-split_values: 0.999999999999999334 0.569140400436275673 0.318872618050356049 
-weak learner at stage 7 has average loss = 0.26615
+split_values: 0.00124141380660278133 0.000276121091179526434 0.000981625552665510437 
+weak learner at stage 0 has average loss = 0.05
+split_cols: 2 2 2 
+split_values: 0.883141897664061037 0.997650553369808346 0.12373287907043895 
+weak learner at stage 0 has average loss = 0.095
 split_cols: 1 3 1 
-split_values: 0.401581050193795197 0.360834998492078562 0.370088477642079638 
-weak learner at stage 8 has average loss = 0.135994
-split_cols: 2 1 1 
-split_values: 0.0696715340410815898 0.502718698860307178 0.526575453102100632 
-weak learner at stage 8 has average loss = 0.270747
-split_cols: 2 0 1 
-split_values: 0.00125079586853901747 0.624507340564582236 0.370088477642079638 
-weak learner at stage 9 has average loss = 0.154551
-split_cols: 1 1 1 
-split_values: 0.502718698860307178 0.479480044756096346 0.6805672568090122 
-weak learner at stage 9 has average loss = 0.210479
-split_cols: 2 2 2 
-split_values: 0.000357032461916012567 0.000981625552665510437 0.00125079586853901747 
-weak learner at stage 10 has average loss = 0.108164
+split_values: 0.444470151470817032 0.700873003833307751 0.531511881898726335 
+weak learner at stage 1 has average loss = 0.0894737
+split_cols: 1 2 3 
+split_values: 0.479480044756096346 0.997650553369808346 0.496260749748818786 
+weak learner at stage 1 has average loss = 0.111951
 split_cols: 1 1 2 
-split_values: 0.662011169718969006 0.6805672568090122 0.122353510242232788 
-weak learner at stage 10 has average loss = 0.272979
-split_cols: 1 1 1 
-split_values: 0.401581050193795197 0.332158208341527428 0.272305619535323673 
-weak learner at stage 11 has average loss = 0.132455
-split_cols: 3 0 1 
-split_values: 0.141930657011306749 0.40739065223433224 0.574273867344056166 
-weak learner at stage 11 has average loss = 0.315862
-split_cols: 4 1 3 
-split_values: 7.04158953368505536e-14 0.384853138944362905 0.662373884583906003 
-weak learner at stage 12 has average loss = 0.110589
-split_cols: 1 1 2 
-split_values: 0.662011169718969006 0.6805672568090122 0.00363231682035125569 
-weak learner at stage 12 has average loss = 0.327692
-split_cols: 3 2 0 
-split_values: 0.357489402445920146 0.0015353418386078177 0.624507340564582236 
-weak learner at stage 13 has average loss = 0.181377
-split_cols: 1 4 1 
-split_values: 0.379258691801199865 8.80684414283905426e-14 0.479480044756096346 
-weak learner at stage 13 has average loss = 0.350098
+split_values: 0.531511881898726335 0.530046165578986317 0.207390222439248872 
+weak learner at stage 2 has average loss = 0.216933
 split_cols: 2 2 2 
-split_values: 0.000981625552665510437 0.000357032461916012567 0.196634593877310471 
-weak learner at stage 14 has average loss = 0.12825
+split_values: 0.12373287907043895 0.997650553369808346 0.627448174543832948 
+weak learner at stage 2 has average loss = 0.221948
+split_cols: 3 3 2 
+split_values: 0.685897385955671401 0.729013358245854448 5.59499955390674319e-05 
+weak learner at stage 3 has average loss = 0.145754
+split_cols: 2 4 3 
+split_values: 0.442618283127769407 0.999999999999997558 0.0434674056274751697 
+weak learner at stage 3 has average loss = 0.182351
 split_cols: 1 4 4 
-split_values: 0.564858493389006289 0.999999999999999334 0.999999991749624728 
-weak learner at stage 14 has average loss = 0.30729
-split_cols: 2 3 0 
-split_values: 0.000528285193333644099 0.406995257960652612 0.624507340564582236 
-weak learner at stage 15 has average loss = 0.207133
+split_values: 0.531511881898726335 5.26800825184636778e-13 2.22044604925031308e-16 
+weak learner at stage 4 has average loss = 0.216539
+split_cols: 1 1 4 
+split_values: 0.6805672568090122 0.662011169718969006 1.54709578481515564e-13 
+weak learner at stage 4 has average loss = 0.380743
 split_cols: 1 1 1 
-split_values: 0.526575453102100632 0.544178240629241028 0.502718698860307178 
-weak learner at stage 15 has average loss = 0.329174
+split_values: 0.370269696184077013 0.384853138944362905 0.387128434225619933 
+weak learner at stage 5 has average loss = 0.249281
+split_cols: 1 3 3 
+split_values: 0.6805672568090122 0.119889252557545484 0.119702025390566957 
+weak learner at stage 5 has average loss = 0.273722
+split_cols: 2 0 0 
+split_values: 0.000528285193333644099 0.176199208821572029 0.463905694672786861 
+weak learner at stage 6 has average loss = 0.248753
+split_cols: 3 2 1 
+split_values: 0.711118874226568165 0.216049487742465185 0.488019810363248041 
+weak learner at stage 6 has average loss = 0.295528
+split_cols: 0 1 2 
+split_values: 0.441781515973124872 0.332158208341527428 0.971644395191448185 
+weak learner at stage 7 has average loss = 0.22173
+split_cols: 2 2 3 
+split_values: 0.997650553369808346 0.960331052047352696 0.642420030204781112 
+weak learner at stage 7 has average loss = 0.323144
+split_cols: 1 3 0 
+split_values: 0.272330343687464782 0.1320001318823463 0.59777085177647038 
+weak learner at stage 8 has average loss = 0.267509
+split_cols: 1 1 4 
+split_values: 0.479480044756096346 0.408499973451678211 1.54709578481515564e-13 
+weak learner at stage 8 has average loss = 0.262643
+split_cols: 3 0 2 
+split_values: 0.729013358245854448 0.176199208821572029 0.000276121091179526434 
+weak learner at stage 9 has average loss = 0.357595
+split_cols: 1 4 1 
+split_values: 0.6805672568090122 0.999999964757892545 0.531511881898726335 
+weak learner at stage 9 has average loss = 0.360215
+split_cols: 0 1 3 
+split_values: 0.556206773297698298 0.342206108309510149 0.429657342218349281 
+weak learner at stage 10 has average loss = 0.261544
+split_cols: 1 2 1 
+split_values: 0.531511881898726335 0.0689879291310910303 0.502718698860307178 
+weak learner at stage 10 has average loss = 0.316711
+split_cols: 2 0 2 
+split_values: 0.00468965205373939042 0.498163758700666093 0.00298229498924867942 
+weak learner at stage 11 has average loss = 0.32727
+split_cols: 1 4 2 
+split_values: 0.502718698860307178 0.264644704110722606 0.941974890824293754 
+weak learner at stage 11 has average loss = 0.293512
+split_cols: 1 3 0 
+split_values: 0.272330343687464782 0.357489402445920146 0.440322558328621938 
+weak learner at stage 12 has average loss = 0.25763
+split_cols: 2 1 1 
+split_values: 0.997650553369808346 0.531511881898726335 0.537120305926493513 
+weak learner at stage 12 has average loss = 0.373473
+split_cols: 0 1 1 
+split_values: 0.710330525060918871 0.370269696184077013 0.292229839503947653 
+weak learner at stage 13 has average loss = 0.233226
 split_cols: 2 2 2 
-split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
-weak learner at stage 16 has average loss = 0.151516
+split_values: 0.997650553369808346 0.995010648391392527 0.991025168386145405 
+weak learner at stage 13 has average loss = 0.388996
+split_cols: 2 0 0 
+split_values: 0.000357032461916012567 0.176199208821572029 0.59777085177647038 
+weak learner at stage 14 has average loss = 0.284814
+split_cols: 1 1 0 
+split_values: 0.537120305926493513 0.546046099205379054 0.62302042132145119 
+weak learner at stage 14 has average loss = 0.304367
+split_cols: 0 1 4 
+split_values: 0.531293973028163946 0.444470151470817032 6.63358257213531033e-15 
+weak learner at stage 15 has average loss = 0.239493
+split_cols: 3 1 3 
+split_values: 0.711118874226568165 0.569140400436275673 0.122140892972254167 
+weak learner at stage 15 has average loss = 0.338407
+split_cols: 3 0 1 
+split_values: 0.729013358245854448 0.59777085177647038 0.370088477642079638 
+weak learner at stage 16 has average loss = 0.33676
+split_cols: 2 1 0 
+split_values: 0.0707071480833099397 0.568316236782665074 0.227845488698554116 
+weak learner at stage 16 has average loss = 0.274603
+split_cols: 3 1 4 
+split_values: 0.729013358245854448 0.429003557741430064 2.22044604925031308e-16 
+weak learner at stage 17 has average loss = 0.282723
+split_cols: 4 1 1 
+split_values: 0.999999999999999334 0.569140400436275673 0.662011169718969006 
+weak learner at stage 17 has average loss = 0.379669
+split_cols: 2 0 2 
+split_values: 0.000276121091179526434 0.176199208821572029 0.0613580224726868739 
+weak learner at stage 18 has average loss = 0.300147
+split_cols: 2 3 2 
+split_values: 0.997650553369808346 0.935854568621468541 0.997472335875476235 
+weak learner at stage 18 has average loss = 0.340843
+split_cols: 2 4 4 
+split_values: 0.196634593877310471 2.39832292447950124e-06 8.26838597589585333e-14 
+weak learner at stage 19 has average loss = 0.344061
 split_cols: 2 2 2 
-split_values: 0.995010648391392527 0.991025168386145405 0.977100614750671781 
-weak learner at stage 16 has average loss = 0.306159
-split_cols: 1 3 2 
-split_values: 0.36967671457248541 0.406995257960652612 0.000254178900377460826 
-weak learner at stage 17 has average loss = 0.179798
-split_cols: 2 2 1 
-split_values: 0.885239426681956321 0.627448174543832948 0.482293993618237549 
-weak learner at stage 17 has average loss = 0.348588
-split_cols: 0 3 1 
-split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
-weak learner at stage 18 has average loss = 0.126962
-split_cols: 2 4 3 
-split_values: 0.997650553369808346 0.999999999999998335 0.782913532932723921 
-weak learner at stage 18 has average loss = 0.283838
-split_cols: 2 2 2 
-split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
-weak learner at stage 19 has average loss = 0.174564
-split_cols: 1 1 1 
-split_values: 0.662011169718969006 0.6805672568090122 0.502718698860307178 
-weak learner at stage 19 has average loss = 0.34564
+split_values: 0.997650553369808346 0.00501121229176337835 0.00468965205373939042 
+weak learner at stage 19 has average loss = 0.364412

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-11-07 16:57:39 UTC (rev 9655)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-11-07 17:08:05 UTC (rev 9656)
@@ -233,13 +233,13 @@
 learner = *5 ->MultiClassAdaBoost(
 random_gen = *0 ;
 seed = 1827 ;
-stage = 1 ;
+stage = 12 ;
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 1 ;
+nstages = 12 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;
@@ -247,7 +247,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 learner1 = *6 ->AdaBoost(
-weak_learners = 1 [ *7 ->RegressionTree(
+weak_learners = 12 [ *7 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
@@ -274,19 +274,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 150 ;
-weights_sum = 1.00000000000000244 ;
-targets_sum = 112 ;
-weighted_targets_sum = 0.746666666666667589 ;
-weighted_squared_targets_sum = 0.746666666666667589 ;
+length = 200 ;
+weights_sum = 1.00000000000000067 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.715000000000000524 ;
+weighted_squared_targets_sum = 0.715000000000000524 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.378311111111112819 0 0.378311111111112819 ] ;
+leave_error = 3 [ 0.407550000000000079 0 0.407550000000000079 ] ;
 split_col = 2 ;
-split_balance = 70 ;
-split_feature_value = 0.00125079586853901747 ;
-after_split_error = 0.074181818181818418 ;
+split_balance = 88 ;
+split_feature_value = 0.00124141380660278133 ;
+after_split_error = 0.120168650793650938 ;
 missing_node = *0 ;
 missing_leave = *11 ->RegressionTreeLeave(
 id = 2 ;
@@ -307,19 +307,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.266666666666666441 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
+length = 56 ;
+weights_sum = 0.280000000000000138 ;
+targets_sum = 6 ;
+weighted_targets_sum = 0.0300000000000000024 ;
+weighted_squared_targets_sum = 0.0300000000000000024 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.048000000000000001 0 0.048000000000000001 ] ;
+leave_error = 3 [ 0.0535714285714285754 0 0.0535714285714285754 ] ;
 split_col = 2 ;
-split_balance = 24 ;
-split_feature_value = 0.000357032461916012567 ;
-after_split_error = 0.0266666666666666684 ;
+split_balance = 34 ;
+split_feature_value = 0.000276121091179526434 ;
+after_split_error = 0.0370505050505050421 ;
 missing_node = *0 ;
 missing_leave = *14 ->RegressionTreeLeave(
 id = 5 ;
@@ -340,19 +340,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 32 ;
-weights_sum = 0.21333333333333318 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
+length = 45 ;
+weights_sum = 0.225000000000000117 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0050000000000000001 ;
+weighted_squared_targets_sum = 0.0050000000000000001 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.113038628061597313 ;
-after_split_error = 0 ;
+leave_error = 3 [ 0.0097777777777777776 0 0.0097777777777777776 ] ;
+split_col = 4 ;
+split_balance = 39 ;
+split_feature_value = 2.00395255944840756e-14 ;
+after_split_error = 0.00666666666666666623 ;
 missing_node = *0 ;
 missing_leave = *17 ->RegressionTreeLeave(
 id = 11 ;
@@ -373,7 +373,7 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666665495 ;
+weights_sum = 0.00499999999999998449 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
@@ -385,11 +385,11 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 31 ;
-weights_sum = 0.206666666666666526 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
+length = 44 ;
+weights_sum = 0.220000000000000112 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0050000000000000001 ;
+weighted_squared_targets_sum = 0.0050000000000000001 ;
 loss_function_factor = 2  )
  )
 ;
@@ -401,19 +401,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
+length = 11 ;
+weights_sum = 0.0549999999999999933 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0250000000000000014 ;
+weighted_squared_targets_sum = 0.0250000000000000014 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0266666666666666684 0 0.0266666666666666684 ] ;
+leave_error = 3 [ 0.0272727272727272679 0 0.0272727272727272679 ] ;
 split_col = 2 ;
-split_balance = 2 ;
+split_balance = 3 ;
 split_feature_value = 0.000981625552665510437 ;
-after_split_error = 0.0106666666666666646 ;
+after_split_error = 0.0142857142857142835 ;
 missing_node = *0 ;
 missing_leave = *22 ->RegressionTreeLeave(
 id = 14 ;
@@ -434,19 +434,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
+length = 7 ;
+weights_sum = 0.0350000000000000033 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0250000000000000014 ;
+weighted_squared_targets_sum = 0.0250000000000000014 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
+leave_error = 3 [ 0.0142857142857142835 0 0.0142857142857142835 ] ;
 split_col = 2 ;
 split_balance = 1 ;
 split_feature_value = 0.000528285193333644099 ;
-after_split_error = 0.00666666666666666449 ;
+after_split_error = 0.0100000000000000002 ;
 missing_node = *0 ;
 missing_leave = *25 ->RegressionTreeLeave(
 id = 17 ;
@@ -467,10 +467,10 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666536 ;
-weighted_squared_targets_sum = 0.00666666666666666536 ;
+weights_sum = 0.00499999999999999837 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.73472347597680709e-18 ;
+weighted_squared_targets_sum = -1.73472347597680709e-18 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
@@ -479,11 +479,11 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0200000000000000004 ;
-weighted_squared_targets_sum = 0.0200000000000000004 ;
+length = 6 ;
+weights_sum = 0.0300000000000000024 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0250000000000000014 ;
+weighted_squared_targets_sum = 0.0250000000000000014 ;
 loss_function_factor = 2  )
  )
 ;
@@ -495,7 +495,7 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 3 ;
+length = 4 ;
 weights_sum = 0.0200000000000000004 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
@@ -504,9 +504,9 @@
 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
-split_col = 4 ;
-split_balance = 1 ;
-split_feature_value = 3.42448291945629535e-13 ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.336930458627675344 ;
 after_split_error = 0 ;
 missing_node = *0 ;
 missing_leave = *30 ->RegressionTreeLeave(
@@ -528,7 +528,7 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666666536 ;
+weights_sum = 0.00499999999999999837 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
@@ -540,8 +540,8 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
+length = 3 ;
+weights_sum = 0.0149999999999999994 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
@@ -560,19 +560,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 110 ;
-weights_sum = 0.73333333333333417 ;
-targets_sum = 108 ;
-weighted_targets_sum = 0.720000000000000751 ;
-weighted_squared_targets_sum = 0.720000000000000751 ;
+length = 144 ;
+weights_sum = 0.720000000000000528 ;
+targets_sum = 137 ;
+weighted_targets_sum = 0.685000000000000497 ;
+weighted_squared_targets_sum = 0.685000000000000497 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.026181818181818306 0 0.026181818181818306 ] ;
+leave_error = 3 [ 0.0665972222222224186 0 0.0665972222222224186 ] ;
 split_col = 4 ;
-split_balance = 88 ;
-split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.0218181818181818199 ;
+split_balance = 96 ;
+split_feature_value = 1.82065853449042692e-07 ;
+after_split_error = 0.0549166666666665859 ;
 missing_node = *0 ;
 missing_leave = *35 ->RegressionTreeLeave(
 id = 8 ;
@@ -593,10 +593,10 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666668271 ;
+weights_sum = 0.00499999999999995674 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666668271 ;
-weighted_squared_targets_sum = 0.00666666666666668271 ;
+weighted_targets_sum = 0.00499999999999995674 ;
+weighted_squared_targets_sum = 0.00499999999999995674 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
@@ -605,11 +605,11 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 109 ;
-weights_sum = 0.72666666666666746 ;
-targets_sum = 107 ;
-weighted_targets_sum = 0.713333333333334041 ;
-weighted_squared_targets_sum = 0.713333333333334041 ;
+length = 143 ;
+weights_sum = 0.715000000000000524 ;
+targets_sum = 136 ;
+weighted_targets_sum = 0.680000000000000493 ;
+weighted_squared_targets_sum = 0.680000000000000493 ;
 loss_function_factor = 2  )
  )
 ;
@@ -623,11 +623,11 @@
 ;
 first_leave = *10  ;
 split_cols = 3 [ 2 2 2 ] ;
-split_values = 3 [ 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 ] ;
+split_values = 3 [ 0.00124141380660278133 0.000276121091179526434 0.000981625552665510437 ] ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 4 ;
-n_examples = 150 ;
+n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
@@ -639,13 +639,4325 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
+*39 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *40 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *41 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *42 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.423684210526316163 ;
+weighted_squared_targets_sum = 0.423684210526316163 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.488351800554016724 0 0.488351800554016724 ] ;
+split_col = 1 ;
+split_balance = 46 ;
+split_feature_value = 0.444470151470817032 ;
+after_split_error = 0.204382894399834314 ;
+missing_node = *0 ;
+missing_leave = *43 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *44 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *45 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 77 ;
+weights_sum = 0.581578947368420307 ;
+targets_sum = 23 ;
+weighted_targets_sum = 0.0605263157894735601 ;
+weighted_squared_targets_sum = 0.0605263157894735601 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.108454393903310103 0 0.108454393903310103 ] ;
+split_col = 3 ;
+split_balance = 57 ;
+split_feature_value = 0.700873003833307751 ;
+after_split_error = 0.0642055375405336803 ;
+missing_node = *0 ;
+missing_leave = *46 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *47 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *48 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 67 ;
+weights_sum = 0.555263157894736126 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.0342105263157894066 ;
+weighted_squared_targets_sum = 0.0342105263157894066 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0642055375405336803 0 0.0642055375405336803 ] ;
+split_col = 2 ;
+split_balance = 65 ;
+split_feature_value = 0.207390222439248872 ;
+after_split_error = 0.059548872180451011 ;
+missing_node = *0 ;
+missing_leave = *49 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *50 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00263157894736847043 ;
+targets_sum = 0 ;
+weighted_targets_sum = 6.07153216591882483e-18 ;
+weighted_squared_targets_sum = 6.07153216591882483e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *51 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 66 ;
+weights_sum = 0.552631578947367919 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.0342105263157894066 ;
+weighted_squared_targets_sum = 0.0342105263157894066 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *48  ;
+right_node = *52 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *53 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 10 ;
+weights_sum = 0.026315789473684157 ;
+targets_sum = 10 ;
+weighted_targets_sum = 0.026315789473684157 ;
+weighted_squared_targets_sum = 0.026315789473684157 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 4 ;
+split_balance = 0 ;
+split_feature_value = 2.1363039115485094e-10 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *54 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *55 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00263157894736841839 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00263157894736841839 ;
+weighted_squared_targets_sum = 0.00263157894736841839 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *56 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 9 ;
+weights_sum = 0.0236842105263157417 ;
+targets_sum = 9 ;
+weighted_targets_sum = 0.0236842105263157417 ;
+weighted_squared_targets_sum = 0.0236842105263157417 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *53   )
+;
+left_leave = *45  ;
+right_node = *57 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *58 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 123 ;
+weights_sum = 0.41842105263157936 ;
+targets_sum = 120 ;
+weighted_targets_sum = 0.363157894736842568 ;
+weighted_squared_targets_sum = 0.363157894736842568 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0959285004965242383 0 0.0959285004965242383 ] ;
+split_col = 1 ;
+split_balance = 71 ;
+split_feature_value = 0.531511881898726335 ;
+after_split_error = 0.07308998302207112 ;
+missing_node = *0 ;
+missing_leave = *59 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *60 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *61 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 26 ;
+weights_sum = 0.163157894736841752 ;
+targets_sum = 23 ;
+weighted_targets_sum = 0.10789473684210503 ;
+weighted_squared_targets_sum = 0.10789473684210503 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0730899830220711477 0 0.0730899830220711477 ] ;
+split_col = 1 ;
+split_balance = 24 ;
+split_feature_value = 0.530046165578986317 ;
+after_split_error = 0.0100367197062423141 ;
+missing_node = *0 ;
+missing_leave = *62 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *63 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0499999999999998918 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0499999999999998918 ;
+weighted_squared_targets_sum = 0.0499999999999998918 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *64 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 25 ;
+weights_sum = 0.11315789473684186 ;
+targets_sum = 22 ;
+weighted_targets_sum = 0.0578947368421051448 ;
+weighted_squared_targets_sum = 0.0578947368421051448 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *61  ;
+right_node = *65 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *66 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 97 ;
+weights_sum = 0.255263157894736969 ;
+targets_sum = 97 ;
+weighted_targets_sum = 0.255263157894736969 ;
+weighted_squared_targets_sum = 0.255263157894736969 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.715222895179832507 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *67 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *68 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0026315789473684288 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0026315789473684288 ;
+weighted_squared_targets_sum = 0.0026315789473684288 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *69 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 96 ;
+weights_sum = 0.25263157894736854 ;
+targets_sum = 96 ;
+weighted_targets_sum = 0.25263157894736854 ;
+weighted_squared_targets_sum = 0.25263157894736854 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *66   )
+;
+right_leave = *58   )
+;
+priority_queue = *70 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *60  *52  *47  *65  ]  )
+;
+first_leave = *42  ;
+split_cols = 3 [ 1 3 1 ] ;
+split_values = 3 [ 0.444470151470817032 0.700873003833307751 0.531511881898726335 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*71 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *72 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *73 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *74 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.000000000000002 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.405049302958179425 ;
+weighted_squared_targets_sum = 0.405049302958179425 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.481968730262545497 0 0.481968730262545497 ] ;
+split_col = 1 ;
+split_balance = 6 ;
+split_feature_value = 0.531511881898726335 ;
+after_split_error = 0.366557859017760723 ;
+missing_node = *0 ;
+missing_leave = *75 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *76 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *77 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 103 ;
+weights_sum = 0.859826589595377389 ;
+targets_sum = 46 ;
+weighted_targets_sum = 0.264875892553554315 ;
+weighted_squared_targets_sum = 0.264875892553554315 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.366557859017760723 0 0.366557859017760723 ] ;
+split_col = 1 ;
+split_balance = 101 ;
+split_feature_value = 0.530046165578986317 ;
+after_split_error = 0.287996283877794745 ;
+missing_node = *0 ;
+missing_leave = *78 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *79 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *80 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 102 ;
+weights_sum = 0.58041482488949403 ;
+targets_sum = 46 ;
+weighted_targets_sum = 0.264875892553554315 ;
+weighted_squared_targets_sum = 0.264875892553554315 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.287996283877794745 0 0.287996283877794745 ] ;
+split_col = 2 ;
+split_balance = 54 ;
+split_feature_value = 0.207390222439248872 ;
+after_split_error = 0.257105796718174184 ;
+missing_node = *0 ;
+missing_leave = *81 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *82 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *83 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 78 ;
+weights_sum = 0.532471948316900257 ;
+targets_sum = 22 ;
+weighted_targets_sum = 0.216933015980959681 ;
+weighted_squared_targets_sum = 0.216933015980959681 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.257105796718174184 0 0.257105796718174184 ] ;
+split_col = 2 ;
+split_balance = 68 ;
+split_feature_value = 0.09124051799518762 ;
+after_split_error = 0.228781330125952642 ;
+missing_node = *0 ;
+missing_leave = *84 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *85 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00144508670520230014 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.38777878078144568e-17 ;
+weighted_squared_targets_sum = -1.38777878078144568e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *86 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 77 ;
+weights_sum = 0.531026861611698209 ;
+targets_sum = 22 ;
+weighted_targets_sum = 0.216933015980959626 ;
+weighted_squared_targets_sum = 0.216933015980959626 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *83  ;
+right_node = *87 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *88 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 24 ;
+weights_sum = 0.0479428765725944325 ;
+targets_sum = 24 ;
+weighted_targets_sum = 0.0479428765725944325 ;
+weighted_squared_targets_sum = 0.0479428765725944325 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 4 ;
+split_balance = 0 ;
+split_feature_value = 0.0395302655963922223 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *89 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *90 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00144508670520231402 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00144508670520231402 ;
+weighted_squared_targets_sum = 0.00144508670520231402 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *91 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 23 ;
+weights_sum = 0.046497789867392135 ;
+targets_sum = 23 ;
+weighted_targets_sum = 0.046497789867392135 ;
+weighted_squared_targets_sum = 0.046497789867392135 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *88   )
+;
+left_leave = *80  ;
+right_node = *92 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *93 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.279411764705883414 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
+missing_node = *0 ;
+missing_leave = *94 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *95 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *96 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *93   )
+;
+left_leave = *77  ;
+right_node = *97 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *98 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 97 ;
+weights_sum = 0.140173410404624416 ;
+targets_sum = 97 ;
+weighted_targets_sum = 0.140173410404624416 ;
+weighted_squared_targets_sum = 0.140173410404624416 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.715222895179832507 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *99 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *100 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00144508670520231228 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00144508670520231228 ;
+weighted_squared_targets_sum = 0.00144508670520231228 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *101 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 96 ;
+weights_sum = 0.13872832369942209 ;
+targets_sum = 96 ;
+weighted_targets_sum = 0.13872832369942209 ;
+weighted_squared_targets_sum = 0.13872832369942209 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *98   )
+;
+priority_queue = *102 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 3 ;
+nodes = 4 [ *82  *97  *87  *0 ]  )
+;
+first_leave = *74  ;
+split_cols = 3 [ 1 1 2 ] ;
+split_values = 3 [ 0.531511881898726335 0.530046165578986317 0.207390222439248872 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*103 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *104 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *105 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *106 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999999112 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.620115067303516065 ;
+weighted_squared_targets_sum = 0.620115067303516065 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.471144741213342932 0 0.471144741213342932 ] ;
+split_col = 3 ;
+split_balance = 52 ;
+split_feature_value = 0.685897385955671401 ;
+after_split_error = 0.358804093433449267 ;
+missing_node = *0 ;
+missing_leave = *107 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *108 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *109 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 126 ;
+weights_sum = 0.718608054507068772 ;
+targets_sum = 72 ;
+weighted_targets_sum = 0.55219468064688626 ;
+weighted_squared_targets_sum = 0.55219468064688626 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.255751599937544039 0 0.255751599937544039 ] ;
+split_col = 2 ;
+split_balance = 62 ;
+split_feature_value = 5.59499955390674319e-05 ;
+after_split_error = 0.21938800334806241 ;
+missing_node = *0 ;
+missing_leave = *110 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *111 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *112 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 32 ;
+weights_sum = 0.0295267042987408246 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.088465696311848363 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *113 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *114 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000922709509335650768 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *115 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 31 ;
+weights_sum = 0.0286039947894051738 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *112  ;
+right_node = *116 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *117 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 94 ;
+weights_sum = 0.689081350208326171 ;
+targets_sum = 72 ;
+weighted_targets_sum = 0.552194680646884484 ;
+weighted_squared_targets_sum = 0.552194680646884484 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.219388003348062632 0 0.219388003348062632 ] ;
+split_col = 4 ;
+split_balance = 34 ;
+split_feature_value = 5.26800825184636778e-13 ;
+after_split_error = 0.205540286541139705 ;
+missing_node = *0 ;
+missing_leave = *118 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *119 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000922709509335605665 ;
+targets_sum = 0 ;
+weighted_targets_sum = -9.71445146547011973e-17 ;
+weighted_squared_targets_sum = -9.71445146547011973e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *120 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 93 ;
+weights_sum = 0.688158640698992574 ;
+targets_sum = 72 ;
+weighted_targets_sum = 0.552194680646886593 ;
+weighted_squared_targets_sum = 0.552194680646886593 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *117   )
+;
+left_leave = *109  ;
+right_node = *121 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *122 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 74 ;
+weights_sum = 0.281391945492931617 ;
+targets_sum = 71 ;
+weighted_targets_sum = 0.0679203866566303327 ;
+weighted_squared_targets_sum = 0.0679203866566303327 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.103052493495905007 0 0.103052493495905007 ] ;
+split_col = 3 ;
+split_balance = 54 ;
+split_feature_value = 0.729013358245854448 ;
+after_split_error = 0.0170267165996853057 ;
+missing_node = *0 ;
+missing_leave = *123 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *124 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *125 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 10 ;
+weights_sum = 0.222338536895449967 ;
+targets_sum = 7 ;
+weighted_targets_sum = 0.00886697805914869221 ;
+weighted_squared_targets_sum = 0.00886697805914869221 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0170267165996853195 0 0.0170267165996853195 ] ;
+split_col = 4 ;
+split_balance = 2 ;
+split_feature_value = 0.999999999998233857 ;
+after_split_error = 0.00655910265440538646 ;
+missing_node = *0 ;
+missing_leave = *126 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *127 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00333072100313478586 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0033307210031347876 ;
+weighted_squared_targets_sum = 0.0033307210031347876 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *128 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 9 ;
+weights_sum = 0.219007815892315189 ;
+targets_sum = 6 ;
+weighted_targets_sum = 0.00553625705601390461 ;
+weighted_squared_targets_sum = 0.00553625705601390461 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *125  ;
+right_node = *129 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *130 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 64 ;
+weights_sum = 0.0590534085974816492 ;
+targets_sum = 64 ;
+weighted_targets_sum = 0.0590534085974816492 ;
+weighted_squared_targets_sum = 0.0590534085974816492 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.912877564103069084 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *131 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *132 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000922709509335650768 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000922709509335650768 ;
+weighted_squared_targets_sum = 0.000922709509335650768 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *133 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 63 ;
+weights_sum = 0.0581306990881459984 ;
+targets_sum = 63 ;
+weighted_targets_sum = 0.0581306990881459984 ;
+weighted_squared_targets_sum = 0.0581306990881459984 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *130   )
+;
+right_leave = *122   )
+;
+priority_queue = *134 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *116  *124  *111  *129  ]  )
+;
+first_leave = *106  ;
+split_cols = 3 [ 3 3 2 ] ;
+split_values = 3 [ 0.685897385955671401 0.729013358245854448 5.59499955390674319e-05 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*135 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *136 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *137 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *138 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999998113 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.388188071299955917 ;
+weighted_squared_targets_sum = 0.388188071299955917 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.47499618520075193 0 0.47499618520075193 ] ;
+split_col = 1 ;
+split_balance = 6 ;
+split_feature_value = 0.531511881898726335 ;
+after_split_error = 0.420255978931199037 ;
+missing_node = *0 ;
+missing_leave = *139 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *140 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *141 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 103 ;
+weights_sum = 0.931861594499045709 ;
+targets_sum = 46 ;
+weighted_targets_sum = 0.320049665799003458 ;
+weighted_squared_targets_sum = 0.320049665799003458 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.420255978931199037 0 0.420255978931199037 ] ;
+split_col = 4 ;
+split_balance = 35 ;
+split_feature_value = 5.26800825184636778e-13 ;
+after_split_error = 0.359957974049163687 ;
+missing_node = *0 ;
+missing_leave = *142 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *143 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *144 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 69 ;
+weights_sum = 0.608213401749990279 ;
+targets_sum = 18 ;
+weighted_targets_sum = 0.288696195727700777 ;
+weighted_squared_targets_sum = 0.288696195727700777 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.303325778691394821 0 0.303325778691394821 ] ;
+split_col = 4 ;
+split_balance = 53 ;
+split_feature_value = 2.22044604925031308e-16 ;
+after_split_error = 0.244130964831055619 ;
+missing_node = *0 ;
+missing_leave = *145 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *146 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *147 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 61 ;
+weights_sum = 0.477849528090813769 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.171758999966496545 ;
+weighted_squared_targets_sum = 0.171758999966496545 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.220043339667620352 0 0.220043339667620352 ] ;
+split_col = 3 ;
+split_balance = 23 ;
+split_feature_value = 0.360834998492078562 ;
+after_split_error = 0.172300210882511851 ;
+missing_node = *0 ;
+missing_leave = *148 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *149 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00316530503489540968 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.71303943252709701e-17 ;
+weighted_squared_targets_sum = -1.71303943252709701e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *150 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 60 ;
+weights_sum = 0.474684223055918453 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.171758999966496545 ;
+weighted_squared_targets_sum = 0.171758999966496545 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *147  ;
+right_node = *151 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *152 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 8 ;
+weights_sum = 0.130363873659176677 ;
+targets_sum = 6 ;
+weighted_targets_sum = 0.116937195761204205 ;
+weighted_squared_targets_sum = 0.116937195761204205 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0240876251634352945 0 0.0240876251634352945 ] ;
+split_col = 3 ;
+split_balance = 4 ;
+split_feature_value = 0.675953685167010931 ;
+after_split_error = 0.00718915029260175191 ;
+missing_node = *0 ;
+missing_leave = *153 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *154 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0198391156319297941 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0198391156319297941 ;
+weighted_squared_targets_sum = 0.0198391156319297941 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *155 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 7 ;
+weights_sum = 0.110524758027246886 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0970980801292744139 ;
+weighted_squared_targets_sum = 0.0970980801292744139 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *152   )
+;
+left_leave = *144  ;
+right_node = *156 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *157 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 34 ;
+weights_sum = 0.32364819274905543 ;
+targets_sum = 28 ;
+weighted_targets_sum = 0.0313534700713027983 ;
+weighted_squared_targets_sum = 0.0313534700713027983 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0566321953577688594 0 0.0566321953577688594 ] ;
+split_col = 1 ;
+split_balance = 22 ;
+split_feature_value = 0.429003557741430064 ;
+after_split_error = 0.0429142711928514817 ;
+missing_node = *0 ;
+missing_leave = *158 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *159 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00316530503489537152 ;
+targets_sum = 0 ;
+weighted_targets_sum = -6.28837260041592572e-18 ;
+weighted_squared_targets_sum = -6.28837260041592572e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *160 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 33 ;
+weights_sum = 0.320482887714160003 ;
+targets_sum = 28 ;
+weighted_targets_sum = 0.0313534700713028053 ;
+weighted_squared_targets_sum = 0.0313534700713028053 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *157   )
+;
+left_leave = *141  ;
+right_node = *161 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *162 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 97 ;
+weights_sum = 0.0681384055009519318 ;
+targets_sum = 97 ;
+weighted_targets_sum = 0.0681384055009519318 ;
+weighted_squared_targets_sum = 0.0681384055009519318 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.715222895179832507 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *163 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *164 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000540072255951422981 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000540072255951422981 ;
+weighted_squared_targets_sum = 0.000540072255951422981 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *165 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 96 ;
+weights_sum = 0.0675983332450005719 ;
+targets_sum = 96 ;
+weighted_targets_sum = 0.0675983332450005719 ;
+weighted_squared_targets_sum = 0.0675983332450005719 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *162   )
+;
+priority_queue = *166 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *146  *151  *156  *161  ]  )
+;
+first_leave = *138  ;
+split_cols = 3 [ 1 4 4 ] ;
+split_values = 3 [ 0.531511881898726335 5.26800825184636778e-13 2.22044604925031308e-16 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*167 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *168 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *169 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *170 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999999778 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.58711124673282844 ;
+weighted_squared_targets_sum = 0.58711124673282844 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.484823261385304405 0 0.484823261385304405 ] ;
+split_col = 1 ;
+split_balance = 98 ;
+split_feature_value = 0.370269696184077013 ;
+after_split_error = 0.417855418629112019 ;
+missing_node = *0 ;
+missing_leave = *171 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *172 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *173 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 51 ;
+weights_sum = 0.389073469268640726 ;
+targets_sum = 11 ;
+weighted_targets_sum = 0.317642474223453875 ;
+weighted_squared_targets_sum = 0.317642474223453875 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.116633591311414597 0 0.116633591311414597 ] ;
+split_col = 3 ;
+split_balance = 17 ;
+split_feature_value = 0.379758326487598441 ;
+after_split_error = 0.0817399725429753532 ;
+missing_node = *0 ;
+missing_leave = *174 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *175 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00202007862074767616 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.99493199737332816e-17 ;
+weighted_squared_targets_sum = -1.99493199737332816e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *176 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 50 ;
+weights_sum = 0.387053390647893014 ;
+targets_sum = 11 ;
+weighted_targets_sum = 0.317642474223453875 ;
+weighted_squared_targets_sum = 0.317642474223453875 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *173  ;
+right_node = *177 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *178 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 149 ;
+weights_sum = 0.610926530731358608 ;
+targets_sum = 132 ;
+weighted_targets_sum = 0.269468772509374177 ;
+weighted_squared_targets_sum = 0.269468772509374177 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.301221827317697977 0 0.301221827317697977 ] ;
+split_col = 1 ;
+split_balance = 137 ;
+split_feature_value = 0.384853138944362905 ;
+after_split_error = 0.26619311422357661 ;
+missing_node = *0 ;
+missing_leave = *179 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *180 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *181 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 6 ;
+weights_sum = 0.0798170787980364282 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000344670862959180939 ;
+weighted_squared_targets_sum = 0.000344670862959180939 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.000686364969425591089 0 0.000686364969425591089 ] ;
+split_col = 4 ;
+split_balance = 4 ;
+split_feature_value = 8.62643290133746632e-14 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *182 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *183 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000344670862959175085 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *184 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 5 ;
+weights_sum = 0.0794724079350772467 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000344670862959180939 ;
+weighted_squared_targets_sum = 0.000344670862959180939 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *181  ;
+right_node = *185 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *186 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 143 ;
+weights_sum = 0.531109451933322152 ;
+targets_sum = 131 ;
+weighted_targets_sum = 0.269124101646414926 ;
+weighted_squared_targets_sum = 0.269124101646414926 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.265506749254151686 0 0.265506749254151686 ] ;
+split_col = 1 ;
+split_balance = 139 ;
+split_feature_value = 0.387128434225619933 ;
+after_split_error = 0.211625644627081194 ;
+missing_node = *0 ;
+missing_leave = *187 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *188 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *189 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 2 ;
+weights_sum = 0.0916190712905839699 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0916190712905839699 ;
+weighted_squared_targets_sum = 0.0916190712905839699 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.307088750247426767 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *190 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *191 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.045809535645291985 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.045809535645291985 ;
+weighted_squared_targets_sum = 0.045809535645291985 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *192 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.045809535645291985 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.045809535645291985 ;
+weighted_squared_targets_sum = 0.045809535645291985 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *189  ;
+right_node = *193 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *194 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 141 ;
+weights_sum = 0.439490380642737988 ;
+targets_sum = 129 ;
+weighted_targets_sum = 0.177505030355829679 ;
+weighted_squared_targets_sum = 0.177505030355829679 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.211625644627081055 0 0.211625644627081055 ] ;
+split_col = 2 ;
+split_balance = 27 ;
+split_feature_value = 0.96944116901060462 ;
+after_split_error = 0.173303874638066624 ;
+missing_node = *0 ;
+missing_leave = *195 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *196 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0383814937942055859 ;
+targets_sum = 0 ;
+weighted_targets_sum = -2.08166817117216851e-17 ;
+weighted_squared_targets_sum = -2.08166817117216851e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *197 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 140 ;
+weights_sum = 0.401108886848531354 ;
+targets_sum = 129 ;
+weighted_targets_sum = 0.177505030355830928 ;
+weighted_squared_targets_sum = 0.177505030355830928 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *194   )
+;
+right_leave = *186   )
+;
+right_leave = *178   )
+;
+priority_queue = *198 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *193  *172  *188  *180  ]  )
+;
+first_leave = *170  ;
+split_cols = 3 [ 1 1 1 ] ;
+split_values = 3 [ 0.370269696184077013 0.384853138944362905 0.387128434225619933 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*199 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *200 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *201 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *202 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999996114 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.629305386150195267 ;
+weighted_squared_targets_sum = 0.629305386150195267 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.466560234225094739 0 0.466560234225094739 ] ;
+split_col = 2 ;
+split_balance = 102 ;
+split_feature_value = 0.000528285193333644099 ;
+after_split_error = 0.411043969618775706 ;
+missing_node = *0 ;
+missing_leave = *203 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *204 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *205 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 49 ;
+weights_sum = 0.292780721380140052 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.108435533463665898 ;
+weighted_squared_targets_sum = 0.108435533463665898 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.136549761192975516 0 0.136549761192975516 ] ;
+split_col = 0 ;
+split_balance = 43 ;
+split_feature_value = 0.176199208821572029 ;
+after_split_error = 0.0942057438390707658 ;
+missing_node = *0 ;
+missing_leave = *206 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *207 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *208 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 3 ;
+weights_sum = 0.0487973376442767809 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0474146759776301538 ;
+weighted_squared_targets_sum = 0.0474146759776301538 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.00268696851408774251 0 0.00268696851408774251 ] ;
+split_col = 4 ;
+split_balance = 1 ;
+split_feature_value = 1.41553435639707459e-14 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *209 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *210 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000691330833323314958 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *211 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 2 ;
+weights_sum = 0.0481060068109534639 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0474146759776301538 ;
+weighted_squared_targets_sum = 0.0474146759776301538 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *208  ;
+right_node = *212 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *213 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 46 ;
+weights_sum = 0.243983383735863368 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0610208574860357442 ;
+weighted_squared_targets_sum = 0.0610208574860357442 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0915187753249830788 0 0.0915187753249830788 ] ;
+split_col = 3 ;
+split_balance = 38 ;
+split_feature_value = 0.429778803348615179 ;
+after_split_error = 0.0691576696932000023 ;
+missing_node = *0 ;
+missing_leave = *214 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *215 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000691330833323316259 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *216 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 45 ;
+weights_sum = 0.243292052902540079 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0610208574860357442 ;
+weighted_squared_targets_sum = 0.0610208574860357442 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *213   )
+;
+left_leave = *205  ;
+right_node = *217 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *218 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 151 ;
+weights_sum = 0.707219278619855785 ;
+targets_sum = 140 ;
+weighted_targets_sum = 0.520869852686528523 ;
+weighted_squared_targets_sum = 0.520869852686528523 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.274494208425800079 0 0.274494208425800079 ] ;
+split_col = 0 ;
+split_balance = 29 ;
+split_feature_value = 0.463905694672786861 ;
+after_split_error = 0.243132135257500215 ;
+missing_node = *0 ;
+missing_leave = *219 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *220 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *221 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 61 ;
+weights_sum = 0.186240969298665071 ;
+targets_sum = 59 ;
+weighted_targets_sum = 0.183550111862743892 ;
+weighted_squared_targets_sum = 0.183550111862743892 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.00530395847089876193 0 0.00530395847089876193 ] ;
+split_col = 2 ;
+split_balance = 53 ;
+split_feature_value = 0.00123333066520273094 ;
+after_split_error = 0.00499154125417236583 ;
+missing_node = *0 ;
+missing_leave = *222 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *223 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0305104287430178409 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0305104287430178478 ;
+weighted_squared_targets_sum = 0.0305104287430178478 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *224 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 60 ;
+weights_sum = 0.15573054055564714 ;
+targets_sum = 58 ;
+weighted_targets_sum = 0.153039683119725933 ;
+weighted_squared_targets_sum = 0.153039683119725933 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *221  ;
+right_node = *225 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *226 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 90 ;
+weights_sum = 0.520978309321192157 ;
+targets_sum = 81 ;
+weighted_targets_sum = 0.337319740823786074 ;
+weighted_squared_targets_sum = 0.337319740823786074 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.237828176786601342 0 0.237828176786601342 ] ;
+split_col = 1 ;
+split_balance = 66 ;
+split_feature_value = 0.370088477642079638 ;
+after_split_error = 0.210976697344030684 ;
+missing_node = *0 ;
+missing_leave = *227 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *228 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00405181518469544758 ;
+targets_sum = 0 ;
+weighted_targets_sum = -3.40439482160448392e-17 ;
+weighted_squared_targets_sum = -3.40439482160448392e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *229 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 89 ;
+weights_sum = 0.516926494136497161 ;
+targets_sum = 81 ;
+weighted_targets_sum = 0.337319740823786685 ;
+weighted_squared_targets_sum = 0.337319740823786685 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *226   )
+;
+right_leave = *218   )
+;
+priority_queue = *230 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *225  *212  *220  *207  ]  )
+;
+first_leave = *202  ;
+split_cols = 3 [ 2 0 0 ] ;
+split_values = 3 [ 0.000528285193333644099 0.176199208821572029 0.463905694672786861 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*231 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *232 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *233 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *234 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999999778 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.5008809963217008 ;
+weighted_squared_targets_sum = 0.5008809963217008 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.499998447690962133 0 0.499998447690962133 ] ;
+split_col = 0 ;
+split_balance = 20 ;
+split_feature_value = 0.441781515973124872 ;
+after_split_error = 0.4387891171211033 ;
+missing_node = *0 ;
+missing_leave = *235 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *236 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *237 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 90 ;
+weights_sum = 0.272068498653787016 ;
+targets_sum = 59 ;
+weighted_targets_sum = 0.21412745187236959 ;
+weighted_squared_targets_sum = 0.21412745187236959 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0912032724663986438 0 0.0912032724663986438 ] ;
+split_col = 3 ;
+split_balance = 10 ;
+split_feature_value = 0.1320001318823463 ;
+after_split_error = 0.0588319084659889788 ;
+missing_node = *0 ;
+missing_leave = *238 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *239 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000460122158750065052 ;
+targets_sum = 0 ;
+weighted_targets_sum = -3.46944695195361419e-18 ;
+weighted_squared_targets_sum = -3.46944695195361419e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *240 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 89 ;
+weights_sum = 0.271608376495036774 ;
+targets_sum = 59 ;
+weighted_targets_sum = 0.214127451872369506 ;
+weighted_squared_targets_sum = 0.214127451872369506 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *237  ;
+right_node = *241 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *242 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 110 ;
+weights_sum = 0.727931501346212873 ;
+targets_sum = 84 ;
+weighted_targets_sum = 0.286753544449331044 ;
+weighted_squared_targets_sum = 0.286753544449331044 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.347585844654703879 0 0.347585844654703879 ] ;
+split_col = 1 ;
+split_balance = 56 ;
+split_feature_value = 0.332158208341527428 ;
+after_split_error = 0.302374292810142387 ;
+missing_node = *0 ;
+missing_leave = *243 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *244 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *245 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 27 ;
+weights_sum = 0.199072132905354138 ;
+targets_sum = 8 ;
+weighted_targets_sum = 0.135599770894035354 ;
+weighted_squared_targets_sum = 0.135599770894035354 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0864695386664701138 0 0.0864695386664701138 ] ;
+split_col = 1 ;
+split_balance = 9 ;
+split_feature_value = 0.272330343687464782 ;
+after_split_error = 0.0454699356707378433 ;
+missing_node = *0 ;
+missing_leave = *246 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *247 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00814425570578567772 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *248 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 26 ;
+weights_sum = 0.190927877199568524 ;
+targets_sum = 8 ;
+weighted_targets_sum = 0.135599770894035326 ;
+weighted_squared_targets_sum = 0.135599770894035326 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *245  ;
+right_node = *249 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *250 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 83 ;
+weights_sum = 0.528859368440858457 ;
+targets_sum = 76 ;
+weighted_targets_sum = 0.151153773555296023 ;
+weighted_squared_targets_sum = 0.151153773555296023 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.215904754143672328 0 0.215904754143672328 ] ;
+split_col = 2 ;
+split_balance = 37 ;
+split_feature_value = 0.971644395191448185 ;
+after_split_error = 0.158529236007423413 ;
+missing_node = *0 ;
+missing_leave = *251 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *252 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *253 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 23 ;
+weights_sum = 0.478022548707630446 ;
+targets_sum = 16 ;
+weighted_targets_sum = 0.100316953822065513 ;
+weighted_squared_targets_sum = 0.100316953822065513 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.158529236007423413 0 0.158529236007423413 ] ;
+split_col = 1 ;
+split_balance = 3 ;
+split_feature_value = 0.429987773183261757 ;
+after_split_error = 0.144586261367377789 ;
+missing_node = *0 ;
+missing_leave = *254 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *255 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0512377972104179208 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *256 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 22 ;
+weights_sum = 0.426784751497212567 ;
+targets_sum = 16 ;
+weighted_targets_sum = 0.100316953822065499 ;
+weighted_squared_targets_sum = 0.100316953822065499 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *253  ;
+right_node = *257 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *258 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 60 ;
+weights_sum = 0.0508368197332301694 ;
+targets_sum = 60 ;
+weighted_targets_sum = 0.0508368197332301694 ;
+weighted_squared_targets_sum = 0.0508368197332301694 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.901066733516976193 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *259 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *260 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00166476917446302408 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00166476917446302408 ;
+weighted_squared_targets_sum = 0.00166476917446302408 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *261 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 59 ;
+weights_sum = 0.0491720505587671525 ;
+targets_sum = 59 ;
+weighted_targets_sum = 0.0491720505587671525 ;
+weighted_squared_targets_sum = 0.0491720505587671525 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *258   )
+;
+right_leave = *250   )
+;
+right_leave = *242   )
+;
+priority_queue = *262 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *244  *236  *252  *257  ]  )
+;
+first_leave = *234  ;
+split_cols = 3 [ 0 1 2 ] ;
+split_values = 3 [ 0.441781515973124872 0.332158208341527428 0.971644395191448185 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*263 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *264 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *265 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *266 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000067 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.48355655532398456 ;
+weighted_squared_targets_sum = 0.48355655532398456 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.499459226254373945 0 0.499459226254373945 ] ;
+split_col = 1 ;
+split_balance = 136 ;
+split_feature_value = 0.272330343687464782 ;
+after_split_error = 0.441602821067810181 ;
+missing_node = *0 ;
+missing_leave = *267 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *268 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *269 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 32 ;
+weights_sum = 0.138506311028580187 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.00822378328030563301 ;
+weighted_squared_targets_sum = 0.00822378328030563301 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0154709957323335683 0 0.0154709957323335683 ] ;
+split_col = 3 ;
+split_balance = 26 ;
+split_feature_value = 0.700873003833307751 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *270 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *271 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00103757138473049566 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *272 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 31 ;
+weights_sum = 0.137468739643849619 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.00822378328030563301 ;
+weighted_squared_targets_sum = 0.00822378328030563301 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *269  ;
+right_node = *273 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *274 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 168 ;
+weights_sum = 0.861493688971420868 ;
+targets_sum = 140 ;
+weighted_targets_sum = 0.475332772043679297 ;
+weighted_squared_targets_sum = 0.475332772043679297 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.426131825335476444 0 0.426131825335476444 ] ;
+split_col = 3 ;
+split_balance = 110 ;
+split_feature_value = 0.1320001318823463 ;
+after_split_error = 0.391676956726106418 ;
+missing_node = *0 ;
+missing_leave = *275 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *276 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *277 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 29 ;
+weights_sum = 0.136791871263686748 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.0309515017211229419 ;
+weighted_squared_targets_sum = 0.0309515017211229419 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0478963895997318451 0 0.0478963895997318451 ] ;
+split_col = 0 ;
+split_balance = 19 ;
+split_feature_value = 0.176199208821572029 ;
+after_split_error = 0.017426739380946122 ;
+missing_node = *0 ;
+missing_leave = *278 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *279 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00103757138473049154 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *280 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 28 ;
+weights_sum = 0.135754299878956319 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.0309515017211229385 ;
+weighted_squared_targets_sum = 0.0309515017211229385 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *277  ;
+right_node = *281 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *282 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 139 ;
+weights_sum = 0.724701817707733786 ;
+targets_sum = 127 ;
+weighted_targets_sum = 0.444381270322556088 ;
+weighted_squared_targets_sum = 0.444381270322556088 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.343780567126374281 0 0.343780567126374281 ] ;
+split_col = 0 ;
+split_balance = 17 ;
+split_feature_value = 0.59777085177647038 ;
+after_split_error = 0.31078862576163474 ;
+missing_node = *0 ;
+missing_leave = *283 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *284 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *285 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 78 ;
+weights_sum = 0.518544265715617958 ;
+targets_sum = 70 ;
+weighted_targets_sum = 0.367296075830272795 ;
+weighted_squared_targets_sum = 0.367296075830272795 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.214264703302247117 0 0.214264703302247117 ] ;
+split_col = 0 ;
+split_balance = 56 ;
+split_feature_value = 0.548110156284199013 ;
+after_split_error = 0.180073536407568624 ;
+missing_node = *0 ;
+missing_leave = *286 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *287 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0130459421710816051 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0130459421710815635 ;
+weighted_squared_targets_sum = 0.0130459421710815635 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *288 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 77 ;
+weights_sum = 0.505498323544536388 ;
+targets_sum = 69 ;
+weighted_targets_sum = 0.354250133659191224 ;
+weighted_squared_targets_sum = 0.354250133659191224 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *285  ;
+right_node = *289 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *290 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 61 ;
+weights_sum = 0.206157551992115801 ;
+targets_sum = 57 ;
+weighted_targets_sum = 0.0770851944922828491 ;
+weighted_squared_targets_sum = 0.0770851944922828491 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0965239224593876788 0 0.0965239224593876788 ] ;
+split_col = 3 ;
+split_balance = 47 ;
+split_feature_value = 0.734861316589340507 ;
+after_split_error = 0.0596029356202125576 ;
+missing_node = *0 ;
+missing_leave = *291 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *292 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0183652243421758739 ;
+targets_sum = 0 ;
+weighted_targets_sum = 6.5052130349130266e-19 ;
+weighted_squared_targets_sum = 6.5052130349130266e-19 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *293 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 60 ;
+weights_sum = 0.187792327649939778 ;
+targets_sum = 57 ;
+weighted_targets_sum = 0.0770851944922828491 ;
+weighted_squared_targets_sum = 0.0770851944922828491 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *290   )
+;
+right_leave = *282   )
+;
+right_leave = *274   )
+;
+priority_queue = *294 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *289  *284  *276  *268  ]  )
+;
+first_leave = *266  ;
+split_cols = 3 [ 1 3 0 ] ;
+split_values = 3 [ 0.272330343687464782 0.1320001318823463 0.59777085177647038 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*295 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *296 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *297 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *298 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999999223 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.468019277192740835 ;
+weighted_squared_targets_sum = 0.468019277192740835 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.497954466737450141 0 0.497954466737450141 ] ;
+split_col = 3 ;
+split_balance = 72 ;
+split_feature_value = 0.729013358245854448 ;
+after_split_error = 0.452327897424011416 ;
+missing_node = *0 ;
+missing_leave = *299 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *300 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *301 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 136 ;
+weights_sum = 0.925402114634777728 ;
+targets_sum = 79 ;
+weighted_targets_sum = 0.39342139182752045 ;
+weighted_squared_targets_sum = 0.39342139182752045 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.452327897424011416 0 0.452327897424011416 ] ;
+split_col = 0 ;
+split_balance = 122 ;
+split_feature_value = 0.176199208821572029 ;
+after_split_error = 0.430982516459485931 ;
+missing_node = *0 ;
+missing_leave = *302 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *303 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *304 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 7 ;
+weights_sum = 0.0443821012400811982 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0401042069557288258 ;
+weighted_squared_targets_sum = 0.0401042069557288258 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.00773111470258469602 0 0.00773111470258469602 ] ;
+split_col = 4 ;
+split_balance = 3 ;
+split_feature_value = 1.41553435639707459e-14 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *305 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *306 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00213894714217618617 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *307 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 6 ;
+weights_sum = 0.0422431540979050085 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0401042069557288258 ;
+weighted_squared_targets_sum = 0.0401042069557288258 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *304  ;
+right_node = *308 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *309 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 129 ;
+weights_sum = 0.881020013394697377 ;
+targets_sum = 74 ;
+weighted_targets_sum = 0.353317184871791368 ;
+weighted_squared_targets_sum = 0.353317184871791368 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.423251401756901291 0 0.423251401756901291 ] ;
+split_col = 2 ;
+split_balance = 45 ;
+split_feature_value = 0.000276121091179526434 ;
+after_split_error = 0.38776620905643866 ;
+missing_node = *0 ;
+missing_leave = *310 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *311 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *312 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 42 ;
+weights_sum = 0.0980441724909268952 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.107157403318795141 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *313 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *314 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000708248235409786038 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *315 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 41 ;
+weights_sum = 0.0973359242555170978 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *312  ;
+right_node = *316 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *317 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 87 ;
+weights_sum = 0.782975840903769482 ;
+targets_sum = 74 ;
+weighted_targets_sum = 0.353317184871791201 ;
+weighted_squared_targets_sum = 0.353317184871791201 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.387766209056438271 0 0.387766209056438271 ] ;
+split_col = 2 ;
+split_balance = 21 ;
+split_feature_value = 0.09124051799518762 ;
+after_split_error = 0.364099940337007988 ;
+missing_node = *0 ;
+missing_leave = *318 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *319 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0125361376813889237 ;
+targets_sum = 0 ;
+weighted_targets_sum = -2.25514051876984922e-17 ;
+weighted_squared_targets_sum = -2.25514051876984922e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *320 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 86 ;
+weights_sum = 0.770439703222381311 ;
+targets_sum = 74 ;
+weighted_targets_sum = 0.353317184871791479 ;
+weighted_squared_targets_sum = 0.353317184871791479 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *317   )
+;
+right_leave = *309   )
+;
+left_leave = *301  ;
+right_node = *321 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *322 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 64 ;
+weights_sum = 0.0745978853652202734 ;
+targets_sum = 64 ;
+weighted_targets_sum = 0.0745978853652202734 ;
+weighted_squared_targets_sum = 0.0745978853652202734 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.912877564103069084 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *323 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *324 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000663799852083516665 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000663799852083516665 ;
+weighted_squared_targets_sum = 0.000663799852083516665 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *325 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 63 ;
+weights_sum = 0.073934085513136738 ;
+targets_sum = 63 ;
+weighted_targets_sum = 0.073934085513136738 ;
+weighted_squared_targets_sum = 0.073934085513136738 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *322   )
+;
+priority_queue = *326 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *316  *303  *311  *321  ]  )
+;
+first_leave = *298  ;
+split_cols = 3 [ 3 0 2 ] ;
+split_values = 3 [ 0.729013358245854448 0.176199208821572029 0.000276121091179526434 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*327 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *328 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *329 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *330 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.99999999999999778 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.583294061806513442 ;
+weighted_squared_targets_sum = 0.583294061806513442 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.486124198535543894 0 0.486124198535543894 ] ;
+split_col = 0 ;
+split_balance = 34 ;
+split_feature_value = 0.556206773297698298 ;
+after_split_error = 0.456703256187367457 ;
+missing_node = *0 ;
+missing_leave = *331 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *332 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *333 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 117 ;
+weights_sum = 0.563200197084261611 ;
+targets_sum = 73 ;
+weighted_targets_sum = 0.268354366925050547 ;
+weighted_squared_targets_sum = 0.268354366925050547 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.280977053994277137 0 0.280977053994277137 ] ;
+split_col = 1 ;
+split_balance = 61 ;
+split_feature_value = 0.342206108309510149 ;
+after_split_error = 0.233696696643850843 ;
+missing_node = *0 ;
+missing_leave = *334 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *335 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *336 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 28 ;
+weights_sum = 0.0878787087915239573 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.113038628061597313 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *337 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *338 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000551247517281962642 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *339 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 27 ;
+weights_sum = 0.0873274612742419715 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *336  ;
+right_node = *340 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *341 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 89 ;
+weights_sum = 0.475321488292738348 ;
+targets_sum = 73 ;
+weighted_targets_sum = 0.268354366925050991 ;
+weighted_squared_targets_sum = 0.268354366925050991 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.23369669664385101 0 0.23369669664385101 ] ;
+split_col = 3 ;
+split_balance = 31 ;
+split_feature_value = 0.429657342218349281 ;
+after_split_error = 0.19590829899505241 ;
+missing_node = *0 ;
+missing_leave = *342 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *343 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *344 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 60 ;
+weights_sum = 0.350512323495811906 ;
+targets_sum = 46 ;
+weighted_targets_sum = 0.239591198964361374 ;
+weighted_squared_targets_sum = 0.239591198964361374 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.151639320135247724 0 0.151639320135247724 ] ;
+split_col = 3 ;
+split_balance = 36 ;
+split_feature_value = 0.32100875926977368 ;
+after_split_error = 0.125773950295607173 ;
+missing_node = *0 ;
+missing_leave = *345 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *346 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000551247517281989313 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.21430643318376497e-17 ;
+weighted_squared_targets_sum = -1.21430643318376497e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *347 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 59 ;
+weights_sum = 0.349961075978529823 ;
+targets_sum = 46 ;
+weighted_targets_sum = 0.239591198964361429 ;
+weighted_squared_targets_sum = 0.239591198964361429 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *344  ;
+right_node = *348 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *349 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 29 ;
+weights_sum = 0.124809164796925928 ;
+targets_sum = 27 ;
+weighted_targets_sum = 0.0287631679606891143 ;
+weighted_squared_targets_sum = 0.0287631679606891143 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0442689788598046585 0 0.0442689788598046585 ] ;
+split_col = 4 ;
+split_balance = 25 ;
+split_feature_value = 0.000988084490728707854 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *350 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *351 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0480229984181184538 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *352 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 28 ;
+weights_sum = 0.0767861663788075716 ;
+targets_sum = 27 ;
+weighted_targets_sum = 0.0287631679606891177 ;
+weighted_squared_targets_sum = 0.0287631679606891177 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *349   )
+;
+right_leave = *341   )
+;
+left_leave = *333  ;
+right_node = *353 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *354 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 83 ;
+weights_sum = 0.436799802915736723 ;
+targets_sum = 70 ;
+weighted_targets_sum = 0.314939694881462673 ;
+weighted_squared_targets_sum = 0.314939694881462673 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.175726202193090209 0 0.175726202193090209 ] ;
+split_col = 1 ;
+split_balance = 59 ;
+split_feature_value = 0.272330343687464782 ;
+after_split_error = 0.141013965640128303 ;
+missing_node = *0 ;
+missing_leave = *355 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *356 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00975719306829215052 ;
+targets_sum = 0 ;
+weighted_targets_sum = 9.21571846612678769e-18 ;
+weighted_squared_targets_sum = 9.21571846612678769e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *357 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 82 ;
+weights_sum = 0.427042609847444965 ;
+targets_sum = 70 ;
+weighted_targets_sum = 0.314939694881463006 ;
+weighted_squared_targets_sum = 0.314939694881463006 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *354   )
+;
+priority_queue = *358 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *348  *353  *343  *335  ]  )
+;
+first_leave = *330  ;
+split_cols = 3 [ 0 1 3 ] ;
+split_values = 3 [ 0.556206773297698298 0.342206108309510149 0.429657342218349281 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*359 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *360 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *361 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *362 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000022 ;
+targets_sum = 143 ;
+weighted_targets_sum = 0.43045380638063907 ;
+weighted_squared_targets_sum = 0.43045380638063907 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.490326653906116794 0 0.490326653906116794 ] ;
+split_col = 2 ;
+split_balance = 72 ;
+split_feature_value = 0.00468965205373939042 ;
+after_split_error = 0.451357995059023742 ;
+missing_node = *0 ;
+missing_leave = *363 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *364 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *365 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 64 ;
+weights_sum = 0.56695653307663807 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.174884078978449986 ;
+weighted_squared_targets_sum = 0.174884078978449986 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.241878260598555217 0 0.241878260598555217 ] ;
+split_col = 2 ;
+split_balance = 60 ;
+split_feature_value = 0.00298229498924867942 ;
+after_split_error = 0.202259529121453174 ;
+missing_node = *0 ;
+missing_leave = *366 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *367 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *368 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 62 ;
+weights_sum = 0.414680026810623992 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.174884078978449986 ;
+weighted_squared_targets_sum = 0.174884078978449986 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.202259529121453174 0 0.202259529121453174 ] ;
+split_col = 2 ;
+split_balance = 50 ;
+split_feature_value = 0.00124141380660278133 ;
+after_split_error = 0.139174612029951961 ;
+missing_node = *0 ;
+missing_leave = *369 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *370 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.018653033765306741 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.38777878078144568e-17 ;
+weighted_squared_targets_sum = -1.38777878078144568e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *371 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 61 ;
+weights_sum = 0.396026993045317244 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.174884078978449986 ;
+weighted_squared_targets_sum = 0.174884078978449986 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *368  ;
+right_node = *372 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *373 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 2 ;
+weights_sum = 0.152276506266014106 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.503622972889967047 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *374 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *375 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0604699175539979983 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *376 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0918065887120161073 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *373   )
+;
+left_leave = *365  ;
+right_node = *377 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *378 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 136 ;
+weights_sum = 0.433043466923362541 ;
+targets_sum = 131 ;
+weighted_targets_sum = 0.25556972740218914 ;
+weighted_squared_targets_sum = 0.25556972740218914 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.209479734460468858 0 0.209479734460468858 ] ;
+split_col = 0 ;
+split_balance = 20 ;
+split_feature_value = 0.498163758700666093 ;
+after_split_error = 0.163975871224280589 ;
+missing_node = *0 ;
+missing_leave = *379 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *380 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *381 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 58 ;
+weights_sum = 0.103183666511191624 ;
+targets_sum = 58 ;
+weighted_targets_sum = 0.103183666511191624 ;
+weighted_squared_targets_sum = 0.103183666511191624 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.341824886709230169 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *382 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *383 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0295918401082008453 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0295918401082008453 ;
+weighted_squared_targets_sum = 0.0295918401082008453 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *384 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 57 ;
+weights_sum = 0.0735918264029907648 ;
+targets_sum = 57 ;
+weighted_targets_sum = 0.0735918264029907648 ;
+weighted_squared_targets_sum = 0.0735918264029907648 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *381  ;
+right_node = *385 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *386 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 78 ;
+weights_sum = 0.329859800412171167 ;
+targets_sum = 73 ;
+weighted_targets_sum = 0.152386060890997432 ;
+weighted_squared_targets_sum = 0.152386060890997432 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.16397587122428095 0 0.16397587122428095 ] ;
+split_col = 3 ;
+split_balance = 74 ;
+split_feature_value = 0.502522189560202448 ;
+after_split_error = 0.129401693962108599 ;
+missing_node = *0 ;
+missing_leave = *387 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *388 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000349819418088731515 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000349819418088741924 ;
+weighted_squared_targets_sum = 0.000349819418088741924 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *389 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 77 ;
+weights_sum = 0.329509980994081686 ;
+targets_sum = 72 ;
+weighted_targets_sum = 0.152036241472908812 ;
+weighted_squared_targets_sum = 0.152036241472908812 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *386   )
+;
+right_leave = *378   )
+;
+priority_queue = *390 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *367  *380  *385  *372  ]  )
+;
+first_leave = *362  ;
+split_cols = 3 [ 2 0 2 ] ;
+split_values = 3 [ 0.00468965205373939042 0.498163758700666093 0.00298229498924867942 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
 ] ;
-voting_weights = 1 [ 1.94591014905531323 ] ;
-sum_voting_weights = 1.94591014905531323 ;
-initial_sum_weights = 150 ;
-example_weights = 150 [ 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.0034013605442176956!
 2 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562!
  0.00340136054421769562 0.00340136054421769562 0.0034013605442!
 1769562 
0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00!
 340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 ] ;
-learners_error = 1 [ 0.0200000000000000004 ] ;
-weak_learner_template = *39 ->RegressionTree(
+voting_weights = 12 [ 1.47221948958322013 1.16003912522078267 0.641814808237945456 0.884150884771987666 0.642974871060007169 0.551226131392252627 0.552637175041885365 0.627805490448808512 0.503649765393419879 0.292908770120374085 0.518978458401263443 0.360278953433196047 ] ;
+sum_voting_weights = 8.20868392310514317 ;
+initial_sum_weights = 200 ;
+example_weights = 200 [ 0.0107210847922966195 0.000444852046205364597 0.000400878651114653479 0.000444852046205364597 0.000783250216693656275 0.000400878651114653479 0.000444852046205364597 0.0155121670150422022 0.000141982221496269483 0.00156142278670138657 0.000277409648641513277 0.000534450558532895722 0.000444852046205364597 0.000444852046205364597 0.0183768887024829339 0.000444852046205364597 0.00152431698991743725 0.000444852046205364597 0.000444852046205364597 0.0342152206303120571 0.000444852046205364597 0.000291855774090214878 0.000444852046205364597 0.000444852046205364597 0.000458701252646065362 0.000444852046205364597 0.00564938782923639967 0.000141982221496269483 0.0126053372449833178 0.00150505870040525342 0.000444852046205364597 0.00160952034070969323 0.00160952034070969323 0.000277409648641513277 0.00445194603151248623 0.000444852046205364597 0.000141982221496269483 0.000141982221496269483 0.0128804392476061045 0.000277409648641513277 0.000444852046205364597!
  0.0289620228084313154 0.000216411965650798458 0.000277409648641513277 0.0249194539764500851 0.000277409648641513277 0.000277409648641513277 0.00160952034070969323 0.000458701252646065362 0.000277409648641513277 0.00171053510157377414 0.0352691750340703991 0.000444852046205364597 0.00370641536237075473 0.000223149338253895665 0.000444852046205364597 0.000141982221496269483 0.000141982221496269483 0.00234950638321515873 0.00026008277447326782 0.00026008277447326782 0.000388775237040830544 0.0219938506241258089 0.00162586792396260242 0.00445194603151248623 0.000141982221496269483 0.000444852046205364597 0.0203124271385637344 0.000277409648641513277 0.000277409648641513277 0.00160952034070969323 0.000277409648641513277 0.000141982221496269483 0.0104372732594916959 0.000783250216693656275 0.0128804392476061045 0.000444852046205364597 0.000400878651114653479 0.00185433460581665185 0.000513706243382991445 0.00482959765650080395 0.00460351795648042232 0.00185433460581665185 0.0018!
 5433460581665185 0.000400878651114653479 0.0004587012526460653!
 62 0.000
400878651114653479 0.000277409648641513277 0.0107210847922966195 0.000141982221496269483 0.000277409648641513277 0.000783250216693656275 0.000223149338253895665 0.000783250216693656275 0.000534450558532895722 0.000277409648641513277 0.000444852046205364597 0.000444852046205364597 0.00564938782923639967 0.000444852046205364597 0.024098975020124206 0.00160952034070969323 0.0153835192099978247 0.000400878651114653479 0.000141982221496269483 0.00370641536237075473 0.000388775237040830544 0.000444852046205364597 0.000444852046205364597 0.000277409648641513277 0.000141982221496269483 0.0388996079922467736 0.0138636879903672253 0.000444852046205364597 0.000444852046205364597 0.000444852046205364597 0.000444852046205364597 0.00370641536237075473 0.000141982221496269483 0.000388775237040830544 0.000444852046205364597 0.000444852046205364597 0.000444852046205364597 0.00185433460581665185 0.000141982221496269483 0.000216411965650798458 0.0377182269229495437 0.00507753272439342464 0.000!
 141982221496269483 0.000141982221496269483 0.00160952034070969323 0.00160952034070969323 0.000444852046205364597 0.000400878651114653479 0.000759602158435389329 0.000400878651114653479 0.00160952034070969323 0.00234950638321515873 0.000141982221496269483 0.000141982221496269483 0.000277409648641513277 0.000400878651114653479 0.000277409648641513277 0.000400878651114653479 0.00160952034070969323 0.00459054474130646695 0.000444852046205364597 0.000444852046205364597 0.0352691750340703991 0.0155121670150422022 0.0305100172191348808 0.00509094875908995374 0.000223149338253895665 0.068234364306498721 0.000223149338253895665 0.00482959765650080395 0.00564938782923639967 0.00152431698991743725 0.0452101131499583614 0.000277409648641513277 0.0241670804860386786 0.000400878651114653479 0.000400878651114653479 0.0388996079922467736 0.000444852046205364597 0.00160952034070969323 0.000388775237040830544 0.00551513524977442857 0.000534450558532895722 0.0376308173372357849 0.000783250216!
 693656275 0.000141982221496269483 0.000444852046205364597 0.00!
 15243169
8991743725 0.00564938782923639967 0.000277409648641513277 0.000444852046205364597 0.000444852046205364597 0.0241670804860386786 0.000388775237040830544 0.000141982221496269483 0.00459054474130646695 0.00459054474130646695 0.000277409648641513277 0.000783250216693656275 0.000388775237040830544 0.00171053510157377414 0.000277409648641513277 0.000458701252646065362 0.000141982221496269483 0.00150505870040525342 0.000216411965650798458 0.00289235883362637782 0.000444852046205364597 0.000141982221496269483 0.00185433460581665185 0.044943684781780735 0.000277409648641513277 0.0432685162660981712 0.0138636879903672253 ] ;
+learners_error = 12 [ 0.0499999999999999958 0.0894736842105261221 0.216933015980959626 0.145753647620590565 0.21653914793577192 0.24928069626397617 0.248752945086009536 0.221730362614801613 0.267508669379056652 0.357595079156143969 0.261544400526413423 0.327270139869447474 ] ;
+weak_learner_template = *391 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -653,7 +4965,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *40 ->RegressionTreeLeave(
+leave_template = *392 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -675,7 +4987,7 @@
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
-n_examples = 150 ;
+n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
@@ -702,13 +5014,13 @@
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 1 ;
-n_examples = 150 ;
+stage = 12 ;
+n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
-nstages = 1 ;
+nstages = 12 ;
 report_progress = 1 ;
 verbosity = 2 ;
 nservers = 0 ;
@@ -716,8 +5028,8 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
-learner2 = *41 ->AdaBoost(
-weak_learners = 1 [ *42 ->RegressionTree(
+learner2 = *393 ->AdaBoost(
+weak_learners = 12 [ *394 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
@@ -725,7 +5037,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *43 ->RegressionTreeLeave(
+leave_template = *395 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -737,28 +5049,28 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-root = *44 ->RegressionTreeNode(
+root = *396 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *45 ->RegressionTreeLeave(
+leave = *397 ->RegressionTreeLeave(
 id = 1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 150 ;
-weights_sum = 1.00000000000000244 ;
-targets_sum = 74 ;
-weighted_targets_sum = 0.49333333333333268 ;
-weighted_squared_targets_sum = 0.49333333333333268 ;
+length = 200 ;
+weights_sum = 1.00000000000000067 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.460000000000000298 ;
+weighted_squared_targets_sum = 0.460000000000000298 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499911111111112305 0 0.499911111111112305 ] ;
+leave_error = 3 [ 0.496800000000000352 0 0.496800000000000352 ] ;
 split_col = 2 ;
-split_balance = 24 ;
-split_feature_value = 0.991025168386145405 ;
-after_split_error = 0.173253056011676648 ;
+split_balance = 14 ;
+split_feature_value = 0.883141897664061037 ;
+after_split_error = 0.171677218370013096 ;
 missing_node = *0 ;
-missing_leave = *46 ->RegressionTreeLeave(
+missing_leave = *398 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -770,28 +5082,28 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *47 ->RegressionTreeNode(
+left_node = *399 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *48 ->RegressionTreeLeave(
+leave = *400 ->RegressionTreeLeave(
 id = 3 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 87 ;
-weights_sum = 0.579999999999999849 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.0866666666666666696 ;
-weighted_squared_targets_sum = 0.0866666666666666696 ;
+length = 107 ;
+weights_sum = 0.535000000000000364 ;
+targets_sum = 9 ;
+weighted_targets_sum = 0.0449999999999999983 ;
+weighted_squared_targets_sum = 0.0449999999999999983 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.147432950191570877 0 0.147432950191570877 ] ;
-split_col = 1 ;
-split_balance = 31 ;
-split_feature_value = 0.482293993618237549 ;
-after_split_error = 0.104535916061339731 ;
+leave_error = 3 [ 0.0824299065420560778 0 0.0824299065420560778 ] ;
+split_col = 2 ;
+split_balance = 65 ;
+split_feature_value = 0.12373287907043895 ;
+after_split_error = 0.0662015503875969108 ;
 missing_node = *0 ;
-missing_leave = *49 ->RegressionTreeLeave(
+missing_leave = *401 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -803,28 +5115,185 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *50 ->RegressionTreeNode(
+left_node = *402 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *51 ->RegressionTreeLeave(
+leave = *403 ->RegressionTreeLeave(
 id = 6 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 59 ;
-weights_sum = 0.393333333333332869 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666709 ;
-weighted_squared_targets_sum = 0.00666666666666666709 ;
+length = 86 ;
+weights_sum = 0.430000000000000271 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0100000000000000002 ;
+weighted_squared_targets_sum = 0.0100000000000000002 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.013107344632768362 0 0.013107344632768362 ] ;
+leave_error = 3 [ 0.0195348837209302316 0 0.0195348837209302316 ] ;
+split_col = 4 ;
+split_balance = 82 ;
+split_feature_value = 0.999999999999998335 ;
+after_split_error = 0.0148809523809523801 ;
+missing_node = *0 ;
+missing_leave = *404 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *405 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00499999999999995674 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *406 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 85 ;
+weights_sum = 0.425000000000000266 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0100000000000000002 ;
+weighted_squared_targets_sum = 0.0100000000000000002 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *403  ;
+right_node = *407 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *408 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 21 ;
+weights_sum = 0.105000000000000024 ;
+targets_sum = 7 ;
+weighted_targets_sum = 0.0350000000000000033 ;
+weighted_squared_targets_sum = 0.0350000000000000033 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0466666666666666757 0 0.0466666666666666757 ] ;
 split_col = 3 ;
-split_balance = 53 ;
-split_feature_value = 0.924226804347039965 ;
-after_split_error = 0.00888888888888889062 ;
+split_balance = 17 ;
+split_feature_value = 0.14842107991859671 ;
+after_split_error = 0.0368421052631578982 ;
 missing_node = *0 ;
-missing_leave = *52 ->RegressionTreeLeave(
+missing_leave = *409 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *410 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00499999999999999837 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.73472347597680709e-18 ;
+weighted_squared_targets_sum = -1.73472347597680709e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *411 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 20 ;
+weights_sum = 0.100000000000000019 ;
+targets_sum = 7 ;
+weighted_targets_sum = 0.0350000000000000033 ;
+weighted_squared_targets_sum = 0.0350000000000000033 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *408   )
+;
+left_leave = *400  ;
+right_node = *412 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *413 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 93 ;
+weights_sum = 0.465000000000000302 ;
+targets_sum = 83 ;
+weighted_targets_sum = 0.415000000000000258 ;
+weighted_squared_targets_sum = 0.415000000000000258 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0892473118279570876 0 0.0892473118279570876 ] ;
+split_col = 2 ;
+split_balance = 39 ;
+split_feature_value = 0.997650553369808346 ;
+after_split_error = 0.0629629629629629844 ;
+missing_node = *0 ;
+missing_leave = *414 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *415 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *416 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 27 ;
+weights_sum = 0.135000000000000037 ;
+targets_sum = 17 ;
+weighted_targets_sum = 0.0850000000000000061 ;
+weighted_squared_targets_sum = 0.0850000000000000061 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0629629629629629844 0 0.0629629629629629844 ] ;
+split_col = 3 ;
+split_balance = 13 ;
+split_feature_value = 0.496260749748818786 ;
+after_split_error = 0.0500000000000000097 ;
+missing_node = *0 ;
+missing_leave = *417 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -837,55 +5306,839 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *53 ->RegressionTreeLeave(
+left_leave = *418 ->RegressionTreeLeave(
 id = 12 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666668271 ;
+weights_sum = 0.00499999999999999837 ;
 targets_sum = 0 ;
+weighted_targets_sum = -1.73472347597680709e-18 ;
+weighted_squared_targets_sum = -1.73472347597680709e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *419 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 26 ;
+weights_sum = 0.130000000000000032 ;
+targets_sum = 17 ;
+weighted_targets_sum = 0.0850000000000000061 ;
+weighted_squared_targets_sum = 0.0850000000000000061 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *416  ;
+right_node = *420 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *421 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 66 ;
+weights_sum = 0.330000000000000182 ;
+targets_sum = 66 ;
+weighted_targets_sum = 0.330000000000000182 ;
+weighted_squared_targets_sum = 0.330000000000000182 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.890956512155314684 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *422 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
+left_node = *0 ;
+left_leave = *423 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00499999999999999837 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00499999999999999837 ;
+weighted_squared_targets_sum = 0.00499999999999999837 ;
+loss_function_factor = 2  )
+;
 right_node = *0 ;
-right_leave = *54 ->RegressionTreeLeave(
+right_leave = *424 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 65 ;
+weights_sum = 0.325000000000000178 ;
+targets_sum = 65 ;
+weighted_targets_sum = 0.325000000000000178 ;
+weighted_squared_targets_sum = 0.325000000000000178 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *421   )
+;
+right_leave = *413   )
+;
+priority_queue = *425 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *415  *407  *402  *420  ]  )
+;
+first_leave = *397  ;
+split_cols = 3 [ 2 2 2 ] ;
+split_values = 3 [ 0.883141897664061037 0.997650553369808346 0.12373287907043895 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*426 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *427 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *428 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *429 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000333 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.466123873218958873 ;
+weighted_squared_targets_sum = 0.466123873218958873 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.497704816068631095 0 0.497704816068631095 ] ;
+split_col = 1 ;
+split_balance = 28 ;
+split_feature_value = 0.479480044756096346 ;
+after_split_error = 0.354065326145222459 ;
+missing_node = *0 ;
+missing_leave = *430 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *431 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *432 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 86 ;
+weights_sum = 0.355335853445769012 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0373655132305902998 ;
+weighted_squared_targets_sum = 0.0373655132305902998 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0668726492923903265 0 0.0668726492923903265 ] ;
+split_col = 2 ;
+split_balance = 66 ;
+split_feature_value = 0.449086344763339085 ;
+after_split_error = 0.0555251526606571899 ;
+missing_node = *0 ;
+missing_leave = *433 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *434 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00276243093922654221 ;
+targets_sum = 0 ;
+weighted_targets_sum = 8.67361737988403547e-19 ;
+weighted_squared_targets_sum = 8.67361737988403547e-19 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *435 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 85 ;
+weights_sum = 0.352573422506542222 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0373655132305902998 ;
+weighted_squared_targets_sum = 0.0373655132305902998 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *432  ;
+right_node = *436 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *437 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 114 ;
+weights_sum = 0.644664146554232986 ;
+targets_sum = 87 ;
+weighted_targets_sum = 0.428758359988368309 ;
+weighted_squared_targets_sum = 0.428758359988368309 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.287192676852833673 0 0.287192676852833673 ] ;
+split_col = 2 ;
+split_balance = 14 ;
+split_feature_value = 0.997650553369808346 ;
+after_split_error = 0.232544890067519716 ;
+missing_node = *0 ;
+missing_leave = *438 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *439 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *440 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 50 ;
+weights_sum = 0.467868566443733702 ;
+targets_sum = 23 ;
+weighted_targets_sum = 0.251962779877871412 ;
+weighted_squared_targets_sum = 0.251962779877871412 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.232544890067519661 0 0.232544890067519661 ] ;
+split_col = 3 ;
+split_balance = 18 ;
+split_feature_value = 0.496260749748818786 ;
+after_split_error = 0.124392796099083422 ;
+missing_node = *0 ;
+missing_leave = *441 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *442 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *443 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 34 ;
+weights_sum = 0.282349520209363103 ;
+targets_sum = 15 ;
+weighted_targets_sum = 0.229863332364059342 ;
+weighted_squared_targets_sum = 0.229863332364059342 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0854589732064111596 0 0.0854589732064111596 ] ;
+split_col = 2 ;
+split_balance = 12 ;
+split_feature_value = 0.0707071480833099397 ;
+after_split_error = 0.0712656098953515088 ;
+missing_node = *0 ;
+missing_leave = *444 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *445 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.026315789473684216 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0263157894736842125 ;
+weighted_squared_targets_sum = 0.0263157894736842125 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *446 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 33 ;
+weights_sum = 0.256033730735679033 ;
+targets_sum = 14 ;
+weighted_targets_sum = 0.203547542890375188 ;
+weighted_squared_targets_sum = 0.203547542890375188 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *443  ;
+right_node = *447 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *448 ->RegressionTreeLeave(
 id = 13 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 58 ;
-weights_sum = 0.386666666666666214 ;
+length = 16 ;
+weights_sum = 0.185519046234370516 ;
+targets_sum = 8 ;
+weighted_targets_sum = 0.0220994475138121607 ;
+weighted_squared_targets_sum = 0.0220994475138121607 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0389338228926722069 0 0.0389338228926722069 ] ;
+split_col = 1 ;
+split_balance = 12 ;
+split_feature_value = 0.502718698860307178 ;
+after_split_error = 0.0300966627692143031 ;
+missing_node = *0 ;
+missing_leave = *449 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *450 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00276243093922652313 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666709 ;
-weighted_squared_targets_sum = 0.00666666666666666709 ;
+weighted_targets_sum = 0.00276243093922651792 ;
+weighted_squared_targets_sum = 0.00276243093922651792 ;
 loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *451 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 15 ;
+weights_sum = 0.182756615295143976 ;
+targets_sum = 7 ;
+weighted_targets_sum = 0.0193370165745856415 ;
+weighted_squared_targets_sum = 0.0193370165745856415 ;
+loss_function_factor = 2  )
  )
 ;
-left_leave = *51  ;
-right_node = *55 ->RegressionTreeNode(
+right_leave = *448   )
+;
+left_leave = *440  ;
+right_node = *452 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *56 ->RegressionTreeLeave(
-id = 7 ;
+leave = *453 ->RegressionTreeLeave(
+id = 10 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 28 ;
-weights_sum = 0.186666666666666564 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.0800000000000000017 ;
-weighted_squared_targets_sum = 0.0800000000000000017 ;
+length = 64 ;
+weights_sum = 0.176795580110497258 ;
+targets_sum = 64 ;
+weighted_targets_sum = 0.176795580110497258 ;
+weighted_squared_targets_sum = 0.176795580110497258 ;
 loss_function_factor = 2  )
 ;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.887813788792009673 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *454 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *455 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00276243093922651792 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00276243093922651792 ;
+weighted_squared_targets_sum = 0.00276243093922651792 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *456 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 63 ;
+weights_sum = 0.174033149171270746 ;
+targets_sum = 63 ;
+weighted_targets_sum = 0.174033149171270746 ;
+weighted_squared_targets_sum = 0.174033149171270746 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *453   )
+;
+right_leave = *437   )
+;
+priority_queue = *457 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *442  *447  *431  *452  ]  )
+;
+first_leave = *429  ;
+split_cols = 3 [ 1 2 3 ] ;
+split_values = 3 [ 0.479480044756096346 0.997650553369808346 0.496260749748818786 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*458 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *459 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *460 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *461 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999996669 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.494546432611265629 ;
+weighted_squared_targets_sum = 0.494546432611265629 ;
+loss_function_factor = 2  )
+;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0914285714285713869 0 0.0914285714285713869 ] ;
+leave_error = 3 [ 0.499940517205471446 0 0.499940517205471446 ] ;
 split_col = 2 ;
+split_balance = 28 ;
+split_feature_value = 0.12373287907043895 ;
+after_split_error = 0.374456126030871472 ;
+missing_node = *0 ;
+missing_leave = *462 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *463 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *464 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 86 ;
+weights_sum = 0.289669498805058434 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.029633267845448584 ;
+weighted_squared_targets_sum = 0.029633267845448584 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0532035531067966555 0 0.0532035531067966555 ] ;
+split_col = 4 ;
+split_balance = 82 ;
+split_feature_value = 0.999999999999998335 ;
+after_split_error = 0.0414247279866019683 ;
+missing_node = *0 ;
+missing_leave = *465 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *466 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00155533726260640944 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *467 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 85 ;
+weights_sum = 0.288114161542452729 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.029633267845448584 ;
+weighted_squared_targets_sum = 0.029633267845448584 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *464  ;
+right_node = *468 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *469 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 114 ;
+weights_sum = 0.710330501194937791 ;
+targets_sum = 90 ;
+weighted_targets_sum = 0.464913164765817233 ;
+weighted_squared_targets_sum = 0.464913164765817233 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.321252572924072166 0 0.321252572924072166 ] ;
+split_col = 2 ;
 split_balance = 18 ;
-split_feature_value = 0.891579732096156263 ;
-after_split_error = 0.0802318840579709924 ;
+split_feature_value = 0.997650553369808346 ;
+after_split_error = 0.285312501485439296 ;
 missing_node = *0 ;
-missing_leave = *57 ->RegressionTreeLeave(
+missing_leave = *470 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *471 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *472 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 48 ;
+weights_sum = 0.586113591712805415 ;
+targets_sum = 24 ;
+weighted_targets_sum = 0.340696255283681304 ;
+weighted_squared_targets_sum = 0.340696255283681304 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.285312501485439296 0 0.285312501485439296 ] ;
+split_col = 2 ;
+split_balance = 22 ;
+split_feature_value = 0.627448174543832948 ;
+after_split_error = 0.242168283091306363 ;
+missing_node = *0 ;
+missing_leave = *473 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *474 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *475 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 13 ;
+weights_sum = 0.234849973209501545 ;
+targets_sum = 6 ;
+weighted_targets_sum = 0.191615637146088869 ;
+weighted_squared_targets_sum = 0.191615637146088869 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0705503580704158195 0 0.0705503580704158195 ] ;
+split_col = 2 ;
+split_balance = 7 ;
+split_feature_value = 0.442618283127769407 ;
+after_split_error = 0.0438336932282397465 ;
+missing_node = *0 ;
+missing_leave = *476 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *477 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00155533726260643546 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *478 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 12 ;
+weights_sum = 0.233294635946895118 ;
+targets_sum = 6 ;
+weighted_targets_sum = 0.191615637146088869 ;
+weighted_squared_targets_sum = 0.191615637146088869 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *475  ;
+right_node = *479 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *480 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 35 ;
+weights_sum = 0.351263618503304009 ;
+targets_sum = 18 ;
+weighted_targets_sum = 0.149080618137592491 ;
+weighted_squared_targets_sum = 0.149080618137592491 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.171617925020890516 0 0.171617925020890516 ] ;
+split_col = 1 ;
+split_balance = 25 ;
+split_feature_value = 0.6805672568090122 ;
+after_split_error = 0.156790033037366772 ;
+missing_node = *0 ;
+missing_leave = *481 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *482 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0148166339227243232 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *483 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 34 ;
+weights_sum = 0.336446984580579589 ;
+targets_sum = 18 ;
+weighted_targets_sum = 0.149080618137592491 ;
+weighted_squared_targets_sum = 0.149080618137592491 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *480   )
+;
+left_leave = *472  ;
+right_node = *484 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *485 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 66 ;
+weights_sum = 0.124216909482135207 ;
+targets_sum = 66 ;
+weighted_targets_sum = 0.124216909482135207 ;
+weighted_squared_targets_sum = 0.124216909482135207 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.890956512155314684 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *486 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -897,28 +6150,263 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *58 ->RegressionTreeNode(
+left_node = *0 ;
+left_leave = *487 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0123376623376623314 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0123376623376623314 ;
+weighted_squared_targets_sum = 0.0123376623376623314 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *488 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 65 ;
+weights_sum = 0.111879247144472962 ;
+targets_sum = 65 ;
+weighted_targets_sum = 0.111879247144472962 ;
+weighted_squared_targets_sum = 0.111879247144472962 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *485   )
+;
+right_leave = *469   )
+;
+priority_queue = *489 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *474  *479  *463  *484  ]  )
+;
+first_leave = *461  ;
+split_cols = 3 [ 2 2 2 ] ;
+split_values = 3 [ 0.12373287907043895 0.997650553369808346 0.627448174543832948 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*490 ->RegressionTree(
 missing_is_valid = 0 ;
-leave = *59 ->RegressionTreeLeave(
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *491 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *492 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *493 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000355 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.605566360137504645 ;
+weighted_squared_targets_sum = 0.605566360137504645 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.477711487214639852 0 0.477711487214639852 ] ;
+split_col = 2 ;
+split_balance = 8 ;
+split_feature_value = 0.442618283127769407 ;
+after_split_error = 0.347257837360016919 ;
+missing_node = *0 ;
+missing_leave = *494 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *495 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *496 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 96 ;
+weights_sum = 0.359826664214057179 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0953220381361555985 ;
+weighted_squared_targets_sum = 0.0953220381361555985 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.140140365135299066 0 0.140140365135299066 ] ;
+split_col = 4 ;
+split_balance = 90 ;
+split_feature_value = 0.999999999999997558 ;
+after_split_error = 0.100440458626433402 ;
+missing_node = *0 ;
+missing_leave = *497 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *498 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *499 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 93 ;
+weights_sum = 0.308997896214625589 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.0524218313759988244 ;
+weighted_squared_targets_sum = 0.0524218313759988244 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0870568205858950062 0 0.0870568205858950062 ] ;
+split_col = 0 ;
+split_balance = 27 ;
+split_feature_value = 0.313878478919910275 ;
+after_split_error = 0.0767778848771243361 ;
+missing_node = *0 ;
+missing_leave = *500 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *501 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000999507556359108696 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *502 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 92 ;
+weights_sum = 0.307998388658267952 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.0524218313759988244 ;
+weighted_squared_targets_sum = 0.0524218313759988244 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *499  ;
+right_node = *503 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *504 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 3 ;
+weights_sum = 0.0508287679994313749 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0429002067601567741 ;
+weighted_squared_targets_sum = 0.0429002067601567741 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0133836380405383953 0 0.0133836380405383953 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.0434674056274751697 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *505 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *506 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *507 ->RegressionTreeLeave(
 id = 15 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.153333333333333294 ;
-targets_sum = 8 ;
-weighted_targets_sum = 0.0533333333333333368 ;
-weighted_squared_targets_sum = 0.0533333333333333368 ;
+length = 1 ;
+weights_sum = 0.00792856123927459903 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0695652173913043348 0 0.0695652173913043348 ] ;
-split_col = 2 ;
-split_balance = 15 ;
-split_feature_value = 0.808283414109232878 ;
-after_split_error = 0.0617543859649122839 ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
+missing_leave = *508 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -931,55 +6419,1562 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *61 ->RegressionTreeLeave(
+left_leave = *509 ->RegressionTreeLeave(
 id = 18 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *510 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *507  ;
+right_node = *511 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *512 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 2 ;
+weights_sum = 0.0429002067601567741 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0429002067601567741 ;
+weighted_squared_targets_sum = 0.0429002067601567741 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 4 ;
+split_balance = 0 ;
+split_feature_value = 0.999999999999999223 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *513 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *514 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666665495 ;
+weights_sum = 0.00952162461584205033 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666189 ;
-weighted_squared_targets_sum = 0.00666666666666666189 ;
+weighted_targets_sum = 0.00952162461584205033 ;
+weighted_squared_targets_sum = 0.00952162461584205033 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
-right_leave = *62 ->RegressionTreeLeave(
+right_leave = *515 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0333785821443147238 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0333785821443147238 ;
+weighted_squared_targets_sum = 0.0333785821443147238 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *512   )
+;
+right_leave = *504   )
+;
+left_leave = *496  ;
+right_node = *516 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *517 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 104 ;
+weights_sum = 0.640173335785946263 ;
+targets_sum = 87 ;
+weighted_targets_sum = 0.510244322001350059 ;
+weighted_squared_targets_sum = 0.510244322001350059 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.20711747222471788 0 0.20711747222471788 ] ;
+split_col = 3 ;
+split_balance = 32 ;
+split_feature_value = 0.701784786855022658 ;
+after_split_error = 0.197450279487326852 ;
+missing_node = *0 ;
+missing_leave = *518 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *519 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0095216246158420486 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.73472347597680709e-18 ;
+weighted_squared_targets_sum = -1.73472347597680709e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *520 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 103 ;
+weights_sum = 0.630651711170102658 ;
+targets_sum = 87 ;
+weighted_targets_sum = 0.510244322001348283 ;
+weighted_squared_targets_sum = 0.510244322001348283 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *517   )
+;
+priority_queue = *521 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 3 ;
+nodes = 4 [ *498  *516  *511  *0 ]  )
+;
+first_leave = *493  ;
+split_cols = 3 [ 2 4 3 ] ;
+split_values = 3 [ 0.442618283127769407 0.999999999999997558 0.0434674056274751697 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*522 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *523 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *524 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *525 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000311 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.481991913175921327 ;
+weighted_squared_targets_sum = 0.481991913175921327 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.499351417617874327 0 0.499351417617874327 ] ;
+split_col = 1 ;
+split_balance = 104 ;
+split_feature_value = 0.6805672568090122 ;
+after_split_error = 0.467958413356260372 ;
+missing_node = *0 ;
+missing_leave = *526 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *527 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *528 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 152 ;
+weights_sum = 0.944736246107295674 ;
+targets_sum = 44 ;
+weighted_targets_sum = 0.426728159283215502 ;
+weighted_squared_targets_sum = 0.426728159283215502 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.467958413356260372 0 0.467958413356260372 ] ;
+split_col = 1 ;
+split_balance = 140 ;
+split_feature_value = 0.662011169718969006 ;
+after_split_error = 0.436756428867164981 ;
+missing_node = *0 ;
+missing_leave = *529 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *530 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *531 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 146 ;
+weights_sum = 0.869077754543573944 ;
+targets_sum = 42 ;
+weighted_targets_sum = 0.425505743103313239 ;
+weighted_squared_targets_sum = 0.425505743103313239 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.43435109772621916 0 0.43435109772621916 ] ;
+split_col = 4 ;
+split_balance = 12 ;
+split_feature_value = 1.54709578481515564e-13 ;
+after_split_error = 0.405394917300886115 ;
+missing_node = *0 ;
+missing_leave = *532 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *533 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *534 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 67 ;
+weights_sum = 0.073747726070205466 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00484838832911889048 ;
+weighted_squared_targets_sum = 0.00484838832911889048 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.00905928257828314022 0 0.00905928257828314022 ] ;
+split_col = 3 ;
+split_balance = 65 ;
+split_feature_value = 0.987251230798804613 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *535 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *536 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000611208089951136837 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *537 ->RegressionTreeLeave(
 id = 19 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 22 ;
-weights_sum = 0.14666666666666664 ;
-targets_sum = 7 ;
-weighted_targets_sum = 0.0466666666666666688 ;
-weighted_squared_targets_sum = 0.0466666666666666688 ;
+length = 66 ;
+weights_sum = 0.0731365179802543619 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00484838832911889048 ;
+weighted_squared_targets_sum = 0.00484838832911889048 ;
 loss_function_factor = 2  )
  )
 ;
-left_leave = *59  ;
-right_node = *63 ->RegressionTreeNode(
+left_leave = *534  ;
+right_node = *538 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *64 ->RegressionTreeLeave(
+leave = *539 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 79 ;
+weights_sum = 0.795330028473368422 ;
+targets_sum = 41 ;
+weighted_targets_sum = 0.420657354774194359 ;
+weighted_squared_targets_sum = 0.420657354774194359 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.39633563472260358 0 0.39633563472260358 ] ;
+split_col = 1 ;
+split_balance = 71 ;
+split_feature_value = 0.408499973451678211 ;
+after_split_error = 0.357693407009168129 ;
+missing_node = *0 ;
+missing_leave = *540 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *541 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0169962936620895629 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0169962936620895316 ;
+weighted_squared_targets_sum = 0.0169962936620895316 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *542 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 78 ;
+weights_sum = 0.778333734811277878 ;
+targets_sum = 40 ;
+weighted_targets_sum = 0.403661061112105091 ;
+weighted_squared_targets_sum = 0.403661061112105091 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *539   )
+;
+left_leave = *531  ;
+right_node = *543 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *544 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 6 ;
+weights_sum = 0.0756584915637215361 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.00122241617990227541 ;
+weighted_squared_targets_sum = 0.00122241617990227541 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.00240533114094590265 0 0.00240533114094590265 ] ;
+split_col = 3 ;
+split_balance = 2 ;
+split_feature_value = 0.738796073466493564 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *545 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *546 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0261079804907304314 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *547 ->RegressionTreeLeave(
 id = 16 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 5 ;
-weights_sum = 0.0333333333333333329 ;
+weights_sum = 0.0495505110729911186 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.00122241617990227541 ;
+weighted_squared_targets_sum = 0.00122241617990227541 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *544   )
+;
+left_leave = *528  ;
+right_node = *548 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *549 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 48 ;
+weights_sum = 0.055263753892705908 ;
+targets_sum = 48 ;
+weighted_targets_sum = 0.055263753892705908 ;
+weighted_squared_targets_sum = 0.055263753892705908 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.752591011543306765 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *550 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *551 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000611208089951137705 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.000611208089951137705 ;
+weighted_squared_targets_sum = 0.000611208089951137705 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *552 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 47 ;
+weights_sum = 0.0546525458027547484 ;
+targets_sum = 47 ;
+weighted_targets_sum = 0.0546525458027547484 ;
+weighted_squared_targets_sum = 0.0546525458027547484 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *549   )
+;
+priority_queue = *553 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *538  *533  *543  *548  ]  )
+;
+first_leave = *525  ;
+split_cols = 3 [ 1 1 4 ] ;
+split_values = 3 [ 0.6805672568090122 0.662011169718969006 1.54709578481515564e-13 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*554 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *555 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *556 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *557 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000222 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.392240446759400896 ;
+weighted_squared_targets_sum = 0.392240446759400896 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.476775757370773656 0 0.476775757370773656 ] ;
+split_col = 1 ;
+split_balance = 104 ;
+split_feature_value = 0.6805672568090122 ;
+after_split_error = 0.44227268942828013 ;
+missing_node = *0 ;
+missing_leave = *558 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *559 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *560 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 152 ;
+weights_sum = 0.955378948829799124 ;
+targets_sum = 44 ;
+weighted_targets_sum = 0.347619395589199798 ;
+weighted_squared_targets_sum = 0.347619395589199798 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.44227268942828013 0 0.44227268942828013 ] ;
+split_col = 3 ;
+split_balance = 86 ;
+split_feature_value = 0.119889252557545484 ;
+after_split_error = 0.411853948410690174 ;
+missing_node = *0 ;
+missing_leave = *561 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *562 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *563 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 33 ;
+weights_sum = 0.133519136616596423 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0903779594501199074 ;
+weighted_squared_targets_sum = 0.0903779594501199074 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0584037862943701247 0 0.0584037862943701247 ] ;
+split_col = 3 ;
+split_balance = 31 ;
+split_feature_value = 0.119702025390566957 ;
+after_split_error = 0.0238499860947938014 ;
+missing_node = *0 ;
+missing_leave = *564 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *565 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *566 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 32 ;
+weights_sum = 0.0596216744560958689 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.016480497289619353 ;
+weighted_squared_targets_sum = 0.016480497289619353 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0238499860947938014 0 0.0238499860947938014 ] ;
+split_col = 2 ;
+split_balance = 30 ;
+split_feature_value = 0.0544487023778579271 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *567 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *568 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000493501536473626512 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *569 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 31 ;
+weights_sum = 0.0591281729196222322 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.016480497289619353 ;
+weighted_squared_targets_sum = 0.016480497289619353 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *566  ;
+right_node = *570 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *571 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0738974621605005544 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0738974621605005544 ;
+weighted_squared_targets_sum = 0.0738974621605005544 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
+missing_node = *0 ;
+missing_leave = *572 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *573 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *574 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *571   )
+;
+left_leave = *563  ;
+right_node = *575 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *576 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 119 ;
+weights_sum = 0.821859812213203367 ;
+targets_sum = 42 ;
+weighted_targets_sum = 0.257241436139079571 ;
+weighted_squared_targets_sum = 0.257241436139079571 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.353450162116320021 0 0.353450162116320021 ] ;
+split_col = 3 ;
+split_balance = 37 ;
+split_feature_value = 0.711118874226568165 ;
+after_split_error = 0.327041816880288938 ;
+missing_node = *0 ;
+missing_leave = *577 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *578 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000493501536473595287 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.73472347597680709e-18 ;
+weighted_squared_targets_sum = -1.73472347597680709e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *579 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 118 ;
+weights_sum = 0.82136631067673016 ;
+targets_sum = 42 ;
+weighted_targets_sum = 0.257241436139079516 ;
+weighted_squared_targets_sum = 0.257241436139079516 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *576   )
+;
+left_leave = *560  ;
+right_node = *580 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *581 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 48 ;
+weights_sum = 0.044621051170201001 ;
+targets_sum = 48 ;
+weighted_targets_sum = 0.044621051170201001 ;
+weighted_squared_targets_sum = 0.044621051170201001 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.752591011543306765 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *582 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *583 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00049350153647362391 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00049350153647362391 ;
+weighted_squared_targets_sum = 0.00049350153647362391 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *584 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 47 ;
+weights_sum = 0.044127549633727399 ;
+targets_sum = 47 ;
+weighted_targets_sum = 0.044127549633727399 ;
+weighted_squared_targets_sum = 0.044127549633727399 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *581   )
+;
+priority_queue = *585 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 3 ;
+nodes = 4 [ *575  *580  *565  *0 ]  )
+;
+first_leave = *557  ;
+split_cols = 3 [ 1 3 3 ] ;
+split_values = 3 [ 0.6805672568090122 0.119889252557545484 0.119702025390566957 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*586 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *587 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *588 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *589 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999997446 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.581593069366817539 ;
+weighted_squared_targets_sum = 0.581593069366817539 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.486685142062601717 0 0.486685142062601717 ] ;
+split_col = 3 ;
+split_balance = 62 ;
+split_feature_value = 0.711118874226568165 ;
+after_split_error = 0.4668341465742194 ;
+missing_node = *0 ;
+missing_leave = *590 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *591 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *592 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 131 ;
+weights_sum = 0.499014213036646614 ;
+targets_sum = 32 ;
+weighted_targets_sum = 0.240409908482151852 ;
+weighted_squared_targets_sum = 0.240409908482151852 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.249175416518530701 0 0.249175416518530701 ] ;
+split_col = 2 ;
+split_balance = 51 ;
+split_feature_value = 0.216049487742465185 ;
+after_split_error = 0.222023797905294529 ;
+missing_node = *0 ;
+missing_leave = *593 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *594 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *595 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 91 ;
+weights_sum = 0.260424623188368565 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.166579008471686968 ;
+weighted_squared_targets_sum = 0.166579008471686968 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.120055540505579844 0 0.120055540505579844 ] ;
+split_col = 1 ;
+split_balance = 57 ;
+split_feature_value = 0.488019810363248041 ;
+after_split_error = 0.0902535175820549929 ;
+missing_node = *0 ;
+missing_leave = *596 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *597 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *598 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 74 ;
+weights_sum = 0.0319515719573615839 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.174966118866960257 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *599 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *600 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000339746964136894815 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *601 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 73 ;
+weights_sum = 0.0316118249932247214 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *598  ;
+right_node = *602 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *603 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 17 ;
+weights_sum = 0.228473051231006974 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.166579008471686996 ;
+weighted_squared_targets_sum = 0.166579008471686996 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0902535175820549929 0 0.0902535175820549929 ] ;
+split_col = 4 ;
+split_balance = 3 ;
+split_feature_value = 0.999999858495719041 ;
+after_split_error = 0.0721715137430611886 ;
+missing_node = *0 ;
+missing_leave = *604 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *605 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0508740836064100685 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0508740836064100962 ;
+weighted_squared_targets_sum = 0.0508740836064100962 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *606 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 16 ;
+weights_sum = 0.177598967624596898 ;
 targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
+weighted_targets_sum = 0.11570492486527692 ;
+weighted_squared_targets_sum = 0.11570492486527692 ;
 loss_function_factor = 2  )
+ )
 ;
+right_leave = *603   )
+;
+left_leave = *595  ;
+right_node = *607 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *608 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 40 ;
+weights_sum = 0.238589589848277717 ;
+targets_sum = 27 ;
+weighted_targets_sum = 0.0738309000104650087 ;
+weighted_squared_targets_sum = 0.0738309000104650087 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.101968257399714685 0 0.101968257399714685 ] ;
+split_col = 1 ;
+split_balance = 0 ;
+split_feature_value = 0.6805672568090122 ;
+after_split_error = 0.0797705176182461895 ;
+missing_node = *0 ;
+missing_leave = *609 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *610 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00193709325952056127 ;
+targets_sum = 0 ;
+weighted_targets_sum = 3.46944695195361419e-18 ;
+weighted_squared_targets_sum = 3.46944695195361419e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *611 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 39 ;
+weights_sum = 0.236652496588757283 ;
+targets_sum = 27 ;
+weighted_targets_sum = 0.0738309000104649255 ;
+weighted_squared_targets_sum = 0.0738309000104649255 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *608   )
+;
+left_leave = *592  ;
+right_node = *612 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *613 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 69 ;
+weights_sum = 0.500985786963352053 ;
+targets_sum = 60 ;
+weighted_targets_sum = 0.341183160884665881 ;
+weighted_squared_targets_sum = 0.341183160884665881 ;
+loss_function_factor = 2  )
+;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
+leave_error = 3 [ 0.217658730055688643 0 0.217658730055688643 ] ;
 split_col = 2 ;
+split_balance = 59 ;
+split_feature_value = 0.941974890824293754 ;
+after_split_error = 0.190762293116890191 ;
+missing_node = *0 ;
+missing_leave = *614 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *615 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.014512417713875448 ;
+targets_sum = 0 ;
+weighted_targets_sum = 1.38777878078144568e-17 ;
+weighted_squared_targets_sum = 1.38777878078144568e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *616 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 68 ;
+weights_sum = 0.486473369249476373 ;
+targets_sum = 60 ;
+weighted_targets_sum = 0.341183160884665659 ;
+weighted_squared_targets_sum = 0.341183160884665659 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *613   )
+;
+priority_queue = *617 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *612  *607  *597  *602  ]  )
+;
+first_leave = *589  ;
+split_cols = 3 [ 3 2 1 ] ;
+split_values = 3 [ 0.711118874226568165 0.216049487742465185 0.488019810363248041 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*618 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *619 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *620 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *621 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999997891 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.485298425293867908 ;
+weighted_squared_targets_sum = 0.485298425293867908 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.499567727402319084 0 0.499567727402319084 ] ;
+split_col = 2 ;
+split_balance = 68 ;
+split_feature_value = 0.997650553369808346 ;
+after_split_error = 0.475282190087886691 ;
+missing_node = *0 ;
+missing_leave = *622 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *623 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *624 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 134 ;
+weights_sum = 0.956172859182146451 ;
+targets_sum = 26 ;
+weighted_targets_sum = 0.441471284476015913 ;
+weighted_squared_targets_sum = 0.441471284476015913 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.475282190087886691 0 0.475282190087886691 ] ;
+split_col = 2 ;
+split_balance = 90 ;
+split_feature_value = 0.960331052047352696 ;
+after_split_error = 0.438548801813753442 ;
+missing_node = *0 ;
+missing_leave = *625 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *626 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *627 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 112 ;
+weights_sum = 0.506925038213422607 ;
+targets_sum = 14 ;
+weighted_targets_sum = 0.300190306601266166 ;
+weighted_squared_targets_sum = 0.300190306601266166 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.24484788791062112 0 0.24484788791062112 ] ;
+split_col = 3 ;
+split_balance = 90 ;
+split_feature_value = 0.642420030204781112 ;
+after_split_error = 0.20052880731762443 ;
+missing_node = *0 ;
+missing_leave = *628 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *629 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *630 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 101 ;
+weights_sum = 0.372563674989123572 ;
+targets_sum = 10 ;
+weighted_targets_sum = 0.173845879643221707 ;
+weighted_squared_targets_sum = 0.173845879643221707 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.185451627476449721 0 0.185451627476449721 ] ;
+split_col = 1 ;
+split_balance = 97 ;
+split_feature_value = 0.689386369193649928 ;
+after_split_error = 0.172654263127876573 ;
+missing_node = *0 ;
+missing_leave = *631 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *632 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000241135741523314907 ;
+targets_sum = 0 ;
+weighted_targets_sum = 6.93889390390722838e-18 ;
+weighted_squared_targets_sum = 6.93889390390722838e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *633 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 100 ;
+weights_sum = 0.372322539247600282 ;
+targets_sum = 10 ;
+weighted_targets_sum = 0.173845879643221735 ;
+weighted_squared_targets_sum = 0.173845879643221735 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *630  ;
+right_node = *634 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *635 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 11 ;
+weights_sum = 0.134361363224298869 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.126344426958044487 ;
+weighted_squared_targets_sum = 0.126344426958044487 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0150771798411747093 0 0.0150771798411747093 ] ;
+split_col = 4 ;
 split_balance = 1 ;
-split_feature_value = 0.982696507149771858 ;
-after_split_error = 0.00666666666666666709 ;
+split_feature_value = 1.54709578481515564e-13 ;
+after_split_error = 0.00811482509180619371 ;
 missing_node = *0 ;
-missing_leave = *65 ->RegressionTreeLeave(
+missing_leave = *636 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -992,59 +7987,904 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *66 ->RegressionTreeLeave(
+left_leave = *637 ->RegressionTreeLeave(
 id = 21 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666666536 ;
+weights_sum = 0.000241135741523304065 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *638 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 10 ;
+weights_sum = 0.13412022748277555 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.126344426958044487 ;
+weighted_squared_targets_sum = 0.126344426958044487 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *635   )
+;
+left_leave = *627  ;
+right_node = *639 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *640 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 22 ;
+weights_sum = 0.449247820968723677 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.141280977874749913 ;
+weighted_squared_targets_sum = 0.141280977874749913 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.193700913903132377 0 0.193700913903132377 ] ;
+split_col = 4 ;
+split_balance = 14 ;
+split_feature_value = 0.500140901567539986 ;
+after_split_error = 0.15162762203233901 ;
+missing_node = *0 ;
+missing_leave = *641 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *642 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0245534076066462811 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *643 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 21 ;
+weights_sum = 0.424694413362077361 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.141280977874749913 ;
+weighted_squared_targets_sum = 0.141280977874749913 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *640   )
+;
+left_leave = *624  ;
+right_node = *644 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *645 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 66 ;
+weights_sum = 0.0438271408178518979 ;
+targets_sum = 66 ;
+weighted_targets_sum = 0.0438271408178518979 ;
+weighted_squared_targets_sum = 0.0438271408178518979 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.890956512155314684 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *646 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *647 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00825470079355741099 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666536 ;
-weighted_squared_targets_sum = 0.00666666666666666536 ;
+weighted_targets_sum = 0.00825470079355741099 ;
+weighted_squared_targets_sum = 0.00825470079355741099 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
-right_leave = *67 ->RegressionTreeLeave(
+right_leave = *648 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 65 ;
+weights_sum = 0.0355724400242945493 ;
+targets_sum = 65 ;
+weighted_targets_sum = 0.0355724400242945493 ;
+weighted_squared_targets_sum = 0.0355724400242945493 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *645   )
+;
+priority_queue = *649 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *639  *634  *629  *644  ]  )
+;
+first_leave = *621  ;
+split_cols = 3 [ 2 2 3 ] ;
+split_values = 3 [ 0.997650553369808346 0.960331052047352696 0.642420030204781112 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*650 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *651 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *652 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *653 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000089 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.613302730417886077 ;
+weighted_squared_targets_sum = 0.613302730417886077 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.474324982559704345 0 0.474324982559704345 ] ;
+split_col = 1 ;
+split_balance = 28 ;
+split_feature_value = 0.479480044756096346 ;
+after_split_error = 0.437989797099442746 ;
+missing_node = *0 ;
+missing_leave = *654 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *655 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *656 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 86 ;
+weights_sum = 0.206039904666722368 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0718488715484882495 ;
+weighted_squared_targets_sum = 0.0718488715484882495 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0935884174191052853 0 0.0935884174191052853 ] ;
+split_col = 1 ;
+split_balance = 46 ;
+split_feature_value = 0.408499973451678211 ;
+after_split_error = 0.0525521160654311703 ;
+missing_node = *0 ;
+missing_leave = *657 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *658 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *659 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 66 ;
+weights_sum = 0.102064478050155519 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.0680996870826439948 ;
+weighted_squared_targets_sum = 0.0680996870826439948 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0453241260995489587 0 0.0453241260995489587 ] ;
+split_col = 4 ;
+split_balance = 58 ;
+split_feature_value = 1.54709578481515564e-13 ;
+after_split_error = 0.0109025982794831913 ;
+missing_node = *0 ;
+missing_leave = *660 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *661 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *662 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 62 ;
+weights_sum = 0.0397728989845104811 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00609782455841582742 ;
+weighted_squared_targets_sum = 0.00609782455841582742 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0103258601251013694 0 0.0103258601251013694 ] ;
+split_col = 3 ;
+split_balance = 60 ;
+split_feature_value = 0.987251230798804613 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *663 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *664 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000178129223983526105 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *665 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 61 ;
+weights_sum = 0.0395947697605269905 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00609782455841582742 ;
+weighted_squared_targets_sum = 0.00609782455841582742 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *662  ;
+right_node = *666 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *667 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 4 ;
+weights_sum = 0.0622915790656450516 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.0620018625242281657 ;
+weighted_squared_targets_sum = 0.0620018625242281657 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.000576738154381820078 0 0.000576738154381820078 ] ;
+split_col = 3 ;
+split_balance = 2 ;
+split_feature_value = 0.624616292049510191 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *668 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *669 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0131429736748032988 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0131429736748032988 ;
+weighted_squared_targets_sum = 0.0131429736748032988 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *670 ->RegressionTreeLeave(
 id = 22 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
+length = 3 ;
+weights_sum = 0.0491486053908417528 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.0488588888494248669 ;
+weighted_squared_targets_sum = 0.0488588888494248669 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *667   )
+;
+left_leave = *659  ;
+right_node = *671 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *672 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 20 ;
+weights_sum = 0.103975426616566904 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00374918446584425252 ;
+weighted_squared_targets_sum = 0.00374918446584425252 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.00722798996588220992 0 0.00722798996588220992 ] ;
+split_col = 4 ;
+split_balance = 18 ;
+split_feature_value = 0.497196075517891301 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *673 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *674 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000178129223983522093 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *675 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 19 ;
+weights_sum = 0.103797297392583393 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00374918446584425252 ;
+weighted_squared_targets_sum = 0.00374918446584425252 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *672   )
+;
+left_leave = *656  ;
+right_node = *676 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *677 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 114 ;
+weights_sum = 0.793960095333278271 ;
+targets_sum = 87 ;
+weighted_targets_sum = 0.541453858869398008 ;
+weighted_squared_targets_sum = 0.541453858869398008 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.344401379680337461 0 0.344401379680337461 ] ;
+split_col = 1 ;
+split_balance = 100 ;
+split_feature_value = 0.495842598540276014 ;
+after_split_error = 0.322496386301985238 ;
+missing_node = *0 ;
+missing_leave = *678 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *679 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0558697575303407773 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0558697575303407357 ;
+weighted_squared_targets_sum = 0.0558697575303407357 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *680 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 113 ;
+weights_sum = 0.738090337802937424 ;
+targets_sum = 86 ;
+weighted_targets_sum = 0.485584101339056939 ;
+weighted_squared_targets_sum = 0.485584101339056939 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *677   )
+;
+priority_queue = *681 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *676  *671  *661  *666  ]  )
+;
+first_leave = *653  ;
+split_cols = 3 [ 1 1 4 ] ;
+split_values = 3 [ 0.479480044756096346 0.408499973451678211 1.54709578481515564e-13 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*682 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *683 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *684 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *685 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000155 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.427947843726230792 ;
+weighted_squared_targets_sum = 0.427947843726230792 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.489616973552601253 0 0.489616973552601253 ] ;
+split_col = 1 ;
+split_balance = 104 ;
+split_feature_value = 0.6805672568090122 ;
+after_split_error = 0.464965066090214818 ;
+missing_node = *0 ;
+missing_leave = *686 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *687 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *688 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 152 ;
+weights_sum = 0.963701247437870001 ;
+targets_sum = 44 ;
+weighted_targets_sum = 0.391649091164100738 ;
+weighted_squared_targets_sum = 0.391649091164100738 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.464965066090214874 0 0.464965066090214874 ] ;
+split_col = 4 ;
+split_balance = 54 ;
+split_feature_value = 0.999999964757892545 ;
+after_split_error = 0.44091836246908811 ;
+missing_node = *0 ;
+missing_leave = *689 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *690 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *691 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 103 ;
+weights_sum = 0.39841617504778909 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.214924998806458073 ;
+weighted_squared_targets_sum = 0.214924998806458073 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.197968071100192128 0 0.197968071100192128 ] ;
+split_col = 1 ;
+split_balance = 99 ;
+split_feature_value = 0.552069282377009474 ;
+after_split_error = 0.173757841628789905 ;
+missing_node = *0 ;
+missing_leave = *692 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *693 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000120788990143721729 ;
+targets_sum = 0 ;
+weighted_targets_sum = -6.93889390390722838e-18 ;
+weighted_squared_targets_sum = -6.93889390390722838e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *694 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 102 ;
+weights_sum = 0.398295386057646639 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.214924998806458073 ;
+weighted_squared_targets_sum = 0.214924998806458073 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *691  ;
+right_node = *695 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *696 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 49 ;
+weights_sum = 0.565285072390081855 ;
+targets_sum = 32 ;
+weighted_targets_sum = 0.176724092357642637 ;
+weighted_squared_targets_sum = 0.176724092357642637 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.24295029136889601 0 0.24295029136889601 ] ;
+split_col = 1 ;
+split_balance = 39 ;
+split_feature_value = 0.531511881898726335 ;
+after_split_error = 0.216740446543295801 ;
+missing_node = *0 ;
+missing_leave = *697 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *698 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *699 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 5 ;
+weights_sum = 0.113281643080132152 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.000961483719544341709 ;
+weighted_squared_targets_sum = 0.000961483719544341709 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.00190664615493684798 0 0.00190664615493684798 ] ;
+split_col = 3 ;
+split_balance = 1 ;
+split_feature_value = 0.92637587089929152 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *700 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *701 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0561600796802939131 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *702 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
 length = 4 ;
-weights_sum = 0.0266666666666666684 ;
+weights_sum = 0.057121563399838246 ;
 targets_sum = 3 ;
-weighted_targets_sum = 0.0200000000000000004 ;
-weighted_squared_targets_sum = 0.0200000000000000004 ;
+weighted_targets_sum = 0.000961483719544341709 ;
+weighted_squared_targets_sum = 0.000961483719544341709 ;
 loss_function_factor = 2  )
  )
 ;
-right_leave = *64   )
+left_leave = *699  ;
+right_node = *703 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *704 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 44 ;
+weights_sum = 0.452003429309950133 ;
+targets_sum = 29 ;
+weighted_targets_sum = 0.175762608638098256 ;
+weighted_squared_targets_sum = 0.175762608638098256 ;
+loss_function_factor = 2  )
 ;
-right_leave = *56   )
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.214833800388359003 0 0.214833800388359003 ] ;
+split_col = 1 ;
+split_balance = 30 ;
+split_feature_value = 0.544178240629241028 ;
+after_split_error = 0.172423668717861467 ;
+missing_node = *0 ;
+missing_leave = *705 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
 ;
-left_leave = *48  ;
-right_node = *68 ->RegressionTreeNode(
+left_node = *0 ;
+left_leave = *706 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0365602083497618274 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *707 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 43 ;
+weights_sum = 0.415443220960188264 ;
+targets_sum = 29 ;
+weighted_targets_sum = 0.175762608638098228 ;
+weighted_squared_targets_sum = 0.175762608638098228 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *704   )
+;
+right_leave = *696   )
+;
+left_leave = *688  ;
+right_node = *708 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *69 ->RegressionTreeLeave(
+leave = *709 ->RegressionTreeLeave(
 id = 4 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 63 ;
-weights_sum = 0.419999999999999485 ;
-targets_sum = 61 ;
-weighted_targets_sum = 0.406666666666666177 ;
-weighted_squared_targets_sum = 0.406666666666666177 ;
+length = 48 ;
+weights_sum = 0.0362987525621300128 ;
+targets_sum = 48 ;
+weighted_targets_sum = 0.0362987525621300128 ;
+weighted_squared_targets_sum = 0.0362987525621300128 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0258201058201057432 0 0.0258201058201057432 ] ;
-split_col = 2 ;
-split_balance = 47 ;
-split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.0200000000000000039 ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.752591011543306765 ;
+after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *70 ->RegressionTreeLeave(
+missing_leave = *710 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1057,47 +8897,47 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *71 ->RegressionTreeLeave(
+left_leave = *711 ->RegressionTreeLeave(
 id = 9 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00666666666666665495 ;
+weights_sum = 0.000287934265741774937 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666665495 ;
-weighted_squared_targets_sum = 0.00666666666666665495 ;
+weighted_targets_sum = 0.000287934265741774937 ;
+weighted_squared_targets_sum = 0.000287934265741774937 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
-right_leave = *72 ->RegressionTreeLeave(
+right_leave = *712 ->RegressionTreeLeave(
 id = 10 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 62 ;
-weights_sum = 0.413333333333332831 ;
-targets_sum = 60 ;
-weighted_targets_sum = 0.399999999999999523 ;
-weighted_squared_targets_sum = 0.399999999999999523 ;
+length = 47 ;
+weights_sum = 0.0360108182963882542 ;
+targets_sum = 47 ;
+weighted_targets_sum = 0.0360108182963882542 ;
+weighted_squared_targets_sum = 0.0360108182963882542 ;
 loss_function_factor = 2  )
  )
 ;
-right_leave = *69   )
+right_leave = *709   )
 ;
-priority_queue = *73 ->RegressionTreeQueue(
+priority_queue = *713 ->RegressionTreeQueue(
 verbosity = 2 ;
 maximum_number_of_nodes = 4 ;
 next_available_node = 4 ;
-nodes = 4 [ *58  *50  *68  *63  ]  )
+nodes = 4 [ *703  *690  *698  *708  ]  )
 ;
-first_leave = *45  ;
-split_cols = 3 [ 2 1 2 ] ;
-split_values = 3 [ 0.991025168386145405 0.482293993618237549 0.891579732096156263 ] ;
+first_leave = *685  ;
+split_cols = 3 [ 1 4 1 ] ;
+split_values = 3 [ 0.6805672568090122 0.999999964757892545 0.531511881898726335 ] ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 4 ;
-n_examples = 150 ;
+n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
@@ -1109,13 +8949,797 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
+*714 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *715 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *716 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *717 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 0.999999999999998668 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.441638038891628504 ;
+weighted_squared_targets_sum = 0.441638038891628504 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.493187762991169343 0 0.493187762991169343 ] ;
+split_col = 1 ;
+split_balance = 6 ;
+split_feature_value = 0.531511881898726335 ;
+after_split_error = 0.464460608552705823 ;
+missing_node = *0 ;
+missing_leave = *718 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *719 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *720 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 103 ;
+weights_sum = 0.429335299645965118 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.130288208924507709 ;
+weighted_squared_targets_sum = 0.130288208924507709 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.181500612068526523 0 0.181500612068526523 ] ;
+split_col = 1 ;
+split_balance = 87 ;
+split_feature_value = 0.502718698860307178 ;
+after_split_error = 0.148202035155245299 ;
+missing_node = *0 ;
+missing_leave = *721 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *722 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *723 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 95 ;
+weights_sum = 0.298035239982574751 ;
+targets_sum = 11 ;
+weighted_targets_sum = 0.129398478251990745 ;
+weighted_squared_targets_sum = 0.129398478251990745 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.146434632002289067 0 0.146434632002289067 ] ;
+split_col = 4 ;
+split_balance = 81 ;
+split_feature_value = 0.264644704110722606 ;
+after_split_error = 0.110345584607090541 ;
+missing_node = *0 ;
+missing_leave = *724 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *725 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.00016766222960705129 ;
+targets_sum = 0 ;
+weighted_targets_sum = -1.38777878078144568e-17 ;
+weighted_squared_targets_sum = -1.38777878078144568e-17 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *726 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 94 ;
+weights_sum = 0.297867577752968959 ;
+targets_sum = 11 ;
+weighted_targets_sum = 0.129398478251990745 ;
+weighted_squared_targets_sum = 0.129398478251990745 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *723  ;
+right_node = *727 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *728 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 8 ;
+weights_sum = 0.13130005966339045 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.000889730672516971508 ;
+weighted_squared_targets_sum = 0.000889730672516971508 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.00176740315295624019 0 0.00176740315295624019 ] ;
+split_col = 4 ;
+split_balance = 4 ;
+split_feature_value = 0.999999999998003708 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *729 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *730 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0144763715039025256 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *731 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 7 ;
+weights_sum = 0.116823688159487946 ;
+targets_sum = 2 ;
+weighted_targets_sum = 0.000889730672516971508 ;
+weighted_squared_targets_sum = 0.000889730672516971508 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *728   )
+;
+left_leave = *720  ;
+right_node = *732 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *733 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 97 ;
+weights_sum = 0.570664700354033827 ;
+targets_sum = 79 ;
+weighted_targets_sum = 0.311349829967120295 ;
+weighted_squared_targets_sum = 0.311349829967120295 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.282959996484179466 0 0.282959996484179466 ] ;
+split_col = 2 ;
+split_balance = 83 ;
+split_feature_value = 0.0689879291310910303 ;
+after_split_error = 0.233209680328619551 ;
+missing_node = *0 ;
+missing_leave = *734 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *735 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *736 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 7 ;
+weights_sum = 0.0728921172508161241 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 4 ;
+split_balance = 1 ;
+split_feature_value = 0.999999932728100394 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *737 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *738 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0144763715039025239 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *739 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 6 ;
+weights_sum = 0.0584157457469136054 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *736  ;
+right_node = *740 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *741 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 90 ;
+weights_sum = 0.497772583103216981 ;
+targets_sum = 79 ;
+weighted_targets_sum = 0.311349829967120351 ;
+weighted_squared_targets_sum = 0.311349829967120351 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.233209680328619218 0 0.233209680328619218 ] ;
+split_col = 1 ;
+split_balance = 76 ;
+split_feature_value = 0.544178240629241028 ;
+after_split_error = 0.210355535310927766 ;
+missing_node = *0 ;
+missing_leave = *742 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *743 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0166030527017196824 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.016603052701719738 ;
+weighted_squared_targets_sum = 0.016603052701719738 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *744 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 89 ;
+weights_sum = 0.481169530401498124 ;
+targets_sum = 78 ;
+weighted_targets_sum = 0.294746777265401327 ;
+weighted_squared_targets_sum = 0.294746777265401327 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *741   )
+;
+right_leave = *733   )
+;
+priority_queue = *745 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *722  *727  *740  *735  ]  )
+;
+first_leave = *717  ;
+split_cols = 3 [ 1 2 1 ] ;
+split_values = 3 [ 0.531511881898726335 0.0689879291310910303 0.502718698860307178 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+*746 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 4 ;
+compute_train_stats = 0 ;
+complexity_penalty_factor = 0 ;
+output_confidence_target = 0 ;
+multiclass_outputs = 3 [ 0 1 2 ] ;
+leave_template = *747 ->RegressionTreeLeave(
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+root = *748 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *749 ->RegressionTreeLeave(
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 200 ;
+weights_sum = 1.00000000000000244 ;
+targets_sum = 92 ;
+weighted_targets_sum = 0.433521170121874355 ;
+weighted_squared_targets_sum = 0.433521170121874355 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.491161130356071396 0 0.491161130356071396 ] ;
+split_col = 1 ;
+split_balance = 10 ;
+split_feature_value = 0.502718698860307178 ;
+after_split_error = 0.456009411249177987 ;
+missing_node = *0 ;
+missing_leave = *750 ->RegressionTreeLeave(
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *751 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *752 ->RegressionTreeLeave(
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 95 ;
+weights_sum = 0.327685562949602771 ;
+targets_sum = 11 ;
+weighted_targets_sum = 0.204284811315166598 ;
+weighted_squared_targets_sum = 0.204284811315166598 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.153860298493941283 0 0.153860298493941283 ] ;
+split_col = 4 ;
+split_balance = 81 ;
+split_feature_value = 0.264644704110722606 ;
+after_split_error = 0.124344699950523135 ;
+missing_node = *0 ;
+missing_leave = *753 ->RegressionTreeLeave(
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *754 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *755 ->RegressionTreeLeave(
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 88 ;
+weights_sum = 0.231894852396504175 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.112937949520379388 ;
+weighted_squared_targets_sum = 0.112937949520379388 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.11586931364179763 0 0.11586931364179763 ] ;
+split_col = 2 ;
+split_balance = 78 ;
+split_feature_value = 0.941974890824293754 ;
+after_split_error = 0.0862489807289095883 ;
+missing_node = *0 ;
+missing_leave = *756 ->RegressionTreeLeave(
+id = 11 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *757 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *758 ->RegressionTreeLeave(
+id = 12 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 83 ;
+weights_sum = 0.144124552913856202 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.0986153382941301337 ;
+weighted_squared_targets_sum = 0.0986153382941301337 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.0622781684937041558 0 0.0622781684937041558 ] ;
+split_col = 4 ;
+split_balance = 75 ;
+split_feature_value = 1.46268193282061976e-05 ;
+after_split_error = 0.0317752728200500489 ;
+missing_node = *0 ;
+missing_leave = *759 ->RegressionTreeLeave(
+id = 17 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *760 ->RegressionTreeLeave(
+id = 18 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.000122687633122823522 ;
+targets_sum = 0 ;
+weighted_targets_sum = 3.46944695195361419e-18 ;
+weighted_squared_targets_sum = 3.46944695195361419e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *761 ->RegressionTreeLeave(
+id = 19 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 82 ;
+weights_sum = 0.144001865280733138 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.0986153382941301337 ;
+weighted_squared_targets_sum = 0.0986153382941301337 ;
+loss_function_factor = 2  )
+ )
+;
+left_leave = *758  ;
+right_node = *762 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *763 ->RegressionTreeLeave(
+id = 13 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 5 ;
+weights_sum = 0.0877702994826478622 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0143226112262492579 ;
+weighted_squared_targets_sum = 0.0143226112262492579 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.0239708122352054361 0 0.0239708122352054361 ] ;
+split_col = 3 ;
+split_balance = 3 ;
+split_feature_value = 0.987251230798804613 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *764 ->RegressionTreeLeave(
+id = 20 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *765 ->RegressionTreeLeave(
+id = 21 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0124925465023531211 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *766 ->RegressionTreeLeave(
+id = 22 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 4 ;
+weights_sum = 0.0752777529802947376 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0143226112262492579 ;
+weighted_squared_targets_sum = 0.0143226112262492579 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *763   )
+;
+left_leave = *755  ;
+right_node = *767 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *768 ->RegressionTreeLeave(
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 7 ;
+weights_sum = 0.0957907105530985958 ;
+targets_sum = 6 ;
+weighted_targets_sum = 0.0913468617947872097 ;
+weighted_squared_targets_sum = 0.0913468617947872097 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 1 1 ] ;
+leave_error = 3 [ 0.00847538630872543552 0 0.00847538630872543552 ] ;
+split_col = 3 ;
+split_balance = 5 ;
+split_feature_value = 0.306301132450967406 ;
+after_split_error = 0 ;
+missing_node = *0 ;
+missing_leave = *769 ->RegressionTreeLeave(
+id = 14 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *770 ->RegressionTreeLeave(
+id = 15 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0353792403406492165 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.0353792403406492303 ;
+weighted_squared_targets_sum = 0.0353792403406492303 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *771 ->RegressionTreeLeave(
+id = 16 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 6 ;
+weights_sum = 0.0604114702124493724 ;
+targets_sum = 5 ;
+weighted_targets_sum = 0.0559676214541379724 ;
+weighted_squared_targets_sum = 0.0559676214541379724 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *768   )
+;
+left_leave = *752  ;
+right_node = *772 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+leave = *773 ->RegressionTreeLeave(
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 105 ;
+weights_sum = 0.672314437050399616 ;
+targets_sum = 81 ;
+weighted_targets_sum = 0.229236358806707563 ;
+weighted_squared_targets_sum = 0.229236358806707563 ;
+loss_function_factor = 2  )
+;
+leave_output = 2 [ 0 1 ] ;
+leave_error = 3 [ 0.302149112755236982 0 0.302149112755236982 ] ;
+split_col = 1 ;
+split_balance = 89 ;
+split_feature_value = 0.531511881898726335 ;
+after_split_error = 0.278035902305286187 ;
+missing_node = *0 ;
+missing_leave = *774 ->RegressionTreeLeave(
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 2  )
+;
+left_node = *0 ;
+left_leave = *775 ->RegressionTreeLeave(
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 1 ;
+weights_sum = 0.0105931536290697573 ;
+targets_sum = 0 ;
+weighted_targets_sum = 5.20417042793042128e-18 ;
+weighted_squared_targets_sum = 5.20417042793042128e-18 ;
+loss_function_factor = 2  )
+;
+right_node = *0 ;
+right_leave = *776 ->RegressionTreeLeave(
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+length = 104 ;
+weights_sum = 0.661721283421328188 ;
+targets_sum = 81 ;
+weighted_targets_sum = 0.22923635880670773 ;
+weighted_squared_targets_sum = 0.22923635880670773 ;
+loss_function_factor = 2  )
+ )
+;
+right_leave = *773   )
+;
+priority_queue = *777 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 4 ;
+next_available_node = 4 ;
+nodes = 4 [ *757  *762  *772  *767  ]  )
+;
+first_leave = *749  ;
+split_cols = 3 [ 1 4 2 ] ;
+split_values = 3 [ 0.502718698860307178 0.264644704110722606 0.941974890824293754 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 4 ;
+n_examples = 200 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 4 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
 ] ;
-voting_weights = 1 [ 1.22117351768460214 ] ;
-sum_voting_weights = 1.22117351768460214 ;
-initial_sum_weights = 150 ;
-example_weights = 150 [ 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971!
 132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579!
 71132 0.0036231884057971132 0.0036231884057971132 0.0036231884!
 05797113
2 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579711!
 32 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 ] ;
-learners_error = 1 [ 0.0800000000000000017 ] ;
-weak_learner_template = *74 ->RegressionTree(
+voting_weights = 12 [ 1.12702902604969268 1.03548207574539641 0.627174476979685869 0.750250393230790458 0.243196868623840629 0.487910101857012224 0.434343534989996505 0.369680722849932031 0.516138335783265179 0.287214970759119059 0.384464196250010937 0.439193765713115358 ] ;
+sum_voting_weights = 6.70207846883185709 ;
+initial_sum_weights = 200 ;
+example_weights = 200 [ 0.000208999323974944171 0.00055454680971253373 0.000280504045088514136 0.00055454680971253373 0.000208999323974944171 0.00970647716624035983 0.00055454680971253373 0.000339924914881385677 0.0110653149853990503 0.00778205130351132708 0.000208999323974944171 0.024398678070881339 0.0323001053964686105 0.00055454680971253373 0.000339924914881385677 0.000117671997982472966 0.000208999323974944171 0.00055454680971253373 0.0490383661954016739 0.000437769476772936251 0.000117671997982472966 0.0205716909579260823 0.000117671997982472966 0.000117671997982472966 0.000117671997982472966 0.00055454680971253373 0.0076104231728418353 0.0171298405614496337 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.016300265773906121 0.00119640871801693599 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.0105321055927624831 0.0105321055927624831 0.000208999323974944171 0.000208999323974944171 0.00055454680971253373 0.0002089993!
 23974944171 0.00259525858356851125 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.00623231395217586567 0.000117671997982472966 0.000208999323974944171 0.0323001053964686105 0.000208999323974944171 0.00055454680971253373 0.0017777693464267458 0.000117671997982472966 0.00055454680971253373 0.00910668071110512685 0.000280504045088514136 0.000117671997982472966 0.000208999323974944171 0.000208999323974944171 0.00422103304611871639 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.0319239733177390586 0.0323001053964686105 0.000437769476772936251 0.000208999323974944171 0.000208999323974944171 0.0143798490176562967 0.000208999323974944171 0.026377266187366858 0.000339924914881385677 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.00205966483174763919 0.00749705170508035805 0.0110250694567599825 0.00132191707442976783 0.000339924914881385677 0.0250388131247752874 0.0180455184510893!
 603 0.000280504045088514136 0.000117671997982472966 0.00970647!
 71662403
5983 0.000208999323974944171 0.000208999323974944171 0.000280504045088514136 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.000208999323974944171 0.00884130168746423764 0.000208999323974944171 0.00055454680971253373 0.000117671997982472966 0.0143798490176562967 0.00055454680971253373 0.000208999323974944171 0.00119640871801693599 0.00153462513281898624 0.000280504045088514136 0.000280504045088514136 0.0017777693464267458 0.019621017607701189 0.000117671997982472966 0.000117671997982472966 0.000208999323974944171 0.000280504045088514136 0.000208999323974944171 0.000437769476772936251 0.000117671997982472966 0.0323001053964686105 0.000117671997982472966 0.000117671997982472966 0.00119162560508736365 0.026377266187366858 0.00422103304611871639 0.000117671997982472966 0.00055454680971253373 0.000117671997982472966 0.000339924914881385677 0.000280504045088514136 0.0387900741831750592 0.00119162560508736365 0.0330808940330893156 0.00910668071110512685 0.!
 00205966483174763919 0.00027985234944076417 0.016300265773906121 0.00055454680971253373 0.000280504045088514136 0.000208999323974944171 0.000280504045088514136 0.0143798490176562967 0.000117671997982472966 0.0171298405614496337 0.0279351648715520373 0.000208999323974944171 0.000280504045088514136 0.000208999323974944171 0.000280504045088514136 0.0227297487201944676 0.000208999323974944171 0.00055454680971253373 0.000117671997982472966 0.000208999323974944171 0.000339924914881385677 0.000208999323974944171 0.0265375112954981986 0.000117671997982472966 0.000208999323974944171 0.000117671997982472966 0.0205716909579260823 0.00778205130351132708 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.000339924914881385677 0.0126482917697524257 0.000280504045088514136 0.000208999323974944171 0.000117671997982472966 0.000497050724460643459 0.00422103304611871639 0.000208999323974944171 0.0036582072611220668 0.000954322272458555136 0.000208999323974944171 0.00205!
 966483174763919 0.000901937257518955929 0.00020899932397494417!
 1 0.0211
476558591618367 0.000208999323974944171 0.000901937257518955929 0.000117671997982472966 0.000339924914881385677 0.00749705170508035805 0.00970647716624035983 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.00749705170508035805 0.0323001053964686105 0.000208999323974944171 0.000117671997982472966 0.00205966483174763919 0.000208999323974944171 0.000280504045088514136 0.0206965325387320695 0.00749705170508035805 0.000280504045088514136 0.00757013044178810914 0.000437769476772936251 0.000208999323974944171 0.0227297487201944676 0.000208999323974944171 ] ;
+learners_error = 12 [ 0.095000000000000015 0.111951148589706381 0.221948222046453703 0.182350845160595049 0.380743478208193542 0.273721933428698827 0.2955275688484712 0.323143793784225974 0.262642962029557558 0.36021526859897407 0.316710962060604451 0.293512033410994522 ] ;
+weak_learner_template = *778 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -1123,7 +9747,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *75 ->RegressionTreeLeave(
+leave_template = *779 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -1145,7 +9769,7 @@
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
-n_examples = 150 ;
+n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
@@ -1172,13 +9796,13 @@
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 1 ;
-n_examples = 150 ;
+stage = 12 ;
+n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
-nstages = 1 ;
+nstages = 12 ;
 report_progress = 1 ;
 verbosity = 2 ;
 nservers = 0 ;
@@ -1187,7 +9811,7 @@
 use_a_separate_random_generator_for_testing = 1827  )
 ;
 forward_sub_learner_test_costs = 1 ;
-learner_template = *76 ->AdaBoost(
+learner_template = *780 ->AdaBoost(
 weak_learners = []
 ;
 voting_weights = []
@@ -1198,7 +9822,7 @@
 ;
 learners_error = []
 ;
-weak_learner_template = *77 ->RegressionTree(
+weak_learner_template = *781 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -1206,7 +9830,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *78 ->RegressionTreeLeave(
+leave_template = *782 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -1296,10 +9920,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *79 ->HyperOptimize(
+strategy = 1 [ *783 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *80 ->EarlyStoppingOracle(
+oracle = *784 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;
@@ -1313,8 +9937,8 @@
 max_degraded_steps = 120 ;
 min_n_steps = 2 ;
 nreturned = 20 ;
-best_objective = 0.220000000000000001 ;
-best_step = 2 ;
+best_objective = 0 ;
+best_step = 12 ;
 met_early_stopping = 0  )
 ;
 provide_tester_expdir = 0 ;
@@ -1327,8 +9951,8 @@
 auto_save = 0 ;
 auto_save_diff_time = 10800 ;
 auto_save_test = 0 ;
-best_objective = 0.220000000000000001 ;
-best_results = 8 [ 0.100000000000000006 0.100000000000000006 0.100000000000000006 0 0.220000000000000001 0.239999999999999991 0.280000000000000027 0 ] ;
+best_objective = 0 ;
+best_results = 8 [ 0.00666666666666666709 0.00666666666666666709 0.00666666666666666709 0 0 0 0 0 ] ;
 best_learner = *5  ;
 trialnum = 20 ;
 option_vals = []

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-11-07 16:57:39 UTC (rev 9655)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-11-07 17:08:05 UTC (rev 9656)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9643"
+__REVISION__ = "PL9654"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1



From nouiz at mail.berlios.de  Fri Nov  7 19:19:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 19:19:04 +0100
Subject: [Plearn-commits] r9657 - trunk/plearn_learners/meta
Message-ID: <200811071819.mA7IJ4Rr021796@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 19:19:03 +0100 (Fri, 07 Nov 2008)
New Revision: 9657

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
another bugfix about the training set...


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-07 17:08:05 UTC (rev 9656)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-07 18:19:03 UTC (rev 9657)
@@ -475,22 +475,28 @@
 { 
     PLCHECK(learner1 && learner2);
 
+    bool training_set_has_changed = !train_set || !(train_set->looksTheSameAs(training_set));
+
     targetname = training_set->fieldName(training_set->inputsize());
     input_prg  = "[%0:%"+tostring(training_set->inputsize()-1)+"]";
     target_prg1= "@"+targetname+" 1 0 ifelse :"+targetname;
     target_prg2= "@"+targetname+" 2 - 0 1 ifelse :"+targetname;
-
-    VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
-                                       target_prg1,  weight_prg);
-    VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
-                                       target_prg2,  weight_prg);
-
+    
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
-    if(!learner1->getTrainingSet())
+    if(training_set_has_changed || !learner1->getTrainingSet()){
+        VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
+                                           target_prg1,  weight_prg);
         learner1->setTrainingSet(vmat1, call_forget);
-    if(!learner2->getTrainingSet())
+    }
+    if(training_set_has_changed || !learner2->getTrainingSet()){
+        VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
+                                           target_prg2,  weight_prg);
         learner2->setTrainingSet(vmat2, call_forget);
+    }
+
+    //we do it here as RegressionTree need a trainingSet to know
+    // the number of test.
     subcosts2.resize(learner2->nTestCosts());
     subcosts1.resize(learner1->nTestCosts());
 
@@ -501,8 +507,11 @@
                               VMat testoutputs, VMat testcosts) const
 {
     Profiler::pl_profile_start("MultiClassAdaBoost::test");
+
+    //we need this in case we reload the learner without training it.
     subcosts1.resize(learner1->nTestCosts());
     subcosts2.resize(learner2->nTestCosts());
+
     inherited::test(testset,test_stats,testoutputs,testcosts);
     Profiler::pl_profile_end("MultiClassAdaBoost::test");
 }



From nouiz at mail.berlios.de  Fri Nov  7 19:29:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 19:29:00 +0100
Subject: [Plearn-commits] r9658 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results:
	. expdir expdir/Split0 expdir/Split0/LearnerExpdir
Message-ID: <200811071829.mA7IT0qe015249@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 19:28:59 +0100 (Fri, 07 Nov 2008)
New Revision: 9658

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
Log:
fixed again the test


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2008-11-07 18:19:03 UTC (rev 9657)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2008-11-07 18:28:59 UTC (rev 9658)
@@ -1,121 +1,121 @@
 HyperLearner: starting the optimization
 split_cols: 2 2 2 
-split_values: 0.00124141380660278133 0.000276121091179526434 0.000981625552665510437 
-weak learner at stage 0 has average loss = 0.05
+split_values: 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 
+weak learner at stage 0 has average loss = 0.02
+split_cols: 2 1 2 
+split_values: 0.991025168386145405 0.482293993618237549 0.891579732096156263 
+weak learner at stage 0 has average loss = 0.08
+split_cols: 4 3 2 
+split_values: 3.23307269844974599e-10 0.698651129400676418 0.000537488498421501149 
+weak learner at stage 1 has average loss = 0.0306122
+split_cols: 4 1 1 
+split_values: 1.54709578481515564e-13 0.588552410435003615 0.568316236782665074 
+weak learner at stage 1 has average loss = 0.13587
+split_cols: 2 0 3 
+split_values: 0.000528285193333644099 0.624507340564582236 0.406995257960652612 
+weak learner at stage 2 has average loss = 0.136257
+split_cols: 1 3 2 
+split_values: 0.526575453102100632 0.924226804347039965 0.995245802370935517 
+weak learner at stage 2 has average loss = 0.204444
+split_cols: 2 3 1 
+split_values: 0.000528285193333644099 0.406995257960652612 0.399391565505198165 
+weak learner at stage 3 has average loss = 0.119855
+split_cols: 2 0 3 
+split_values: 0.997650553369808346 0.441781515973124872 0.310957185430505323 
+weak learner at stage 3 has average loss = 0.179198
+split_cols: 2 1 2 
+split_values: 0.196634593877310471 0.36967671457248541 0.00125079586853901747 
+weak learner at stage 4 has average loss = 0.103659
+split_cols: 3 3 2 
+split_values: 0.141930657011306749 0.0564509465831030677 0.997650553369808346 
+weak learner at stage 4 has average loss = 0.27129
+split_cols: 0 3 1 
+split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
+weak learner at stage 5 has average loss = 0.0730531
+split_cols: 2 4 4 
+split_values: 0.00363231682035125569 0.999999964757892545 0.999999999538717876 
+weak learner at stage 5 has average loss = 0.217388
+split_cols: 2 0 3 
+split_values: 0.00125079586853901747 0.624507340564582236 0.763338630405507312 
+weak learner at stage 6 has average loss = 0.140932
+split_cols: 1 1 1 
+split_values: 0.564858493389006289 0.479480044756096346 0.379258691801199865 
+weak learner at stage 6 has average loss = 0.262423
+split_cols: 2 3 1 
+split_values: 0.000357032461916012567 0.130712305658957473 0.399391565505198165 
+weak learner at stage 7 has average loss = 0.214419
+split_cols: 4 1 0 
+split_values: 0.999999999999999334 0.569140400436275673 0.318872618050356049 
+weak learner at stage 7 has average loss = 0.26615
+split_cols: 1 3 1 
+split_values: 0.401581050193795197 0.360834998492078562 0.370088477642079638 
+weak learner at stage 8 has average loss = 0.135994
+split_cols: 2 1 1 
+split_values: 0.0696715340410815898 0.502718698860307178 0.526575453102100632 
+weak learner at stage 8 has average loss = 0.270747
+split_cols: 2 0 1 
+split_values: 0.00125079586853901747 0.624507340564582236 0.370088477642079638 
+weak learner at stage 9 has average loss = 0.154551
+split_cols: 1 1 1 
+split_values: 0.502718698860307178 0.479480044756096346 0.6805672568090122 
+weak learner at stage 9 has average loss = 0.210479
 split_cols: 2 2 2 
-split_values: 0.883141897664061037 0.997650553369808346 0.12373287907043895 
-weak learner at stage 0 has average loss = 0.095
-split_cols: 1 3 1 
-split_values: 0.444470151470817032 0.700873003833307751 0.531511881898726335 
-weak learner at stage 1 has average loss = 0.0894737
-split_cols: 1 2 3 
-split_values: 0.479480044756096346 0.997650553369808346 0.496260749748818786 
-weak learner at stage 1 has average loss = 0.111951
+split_values: 0.000357032461916012567 0.000981625552665510437 0.00125079586853901747 
+weak learner at stage 10 has average loss = 0.108164
 split_cols: 1 1 2 
-split_values: 0.531511881898726335 0.530046165578986317 0.207390222439248872 
-weak learner at stage 2 has average loss = 0.216933
+split_values: 0.662011169718969006 0.6805672568090122 0.122353510242232788 
+weak learner at stage 10 has average loss = 0.272979
+split_cols: 1 1 1 
+split_values: 0.401581050193795197 0.332158208341527428 0.272305619535323673 
+weak learner at stage 11 has average loss = 0.132455
+split_cols: 3 0 1 
+split_values: 0.141930657011306749 0.40739065223433224 0.574273867344056166 
+weak learner at stage 11 has average loss = 0.315862
+split_cols: 4 1 3 
+split_values: 7.04158953368505536e-14 0.384853138944362905 0.662373884583906003 
+weak learner at stage 12 has average loss = 0.110589
+split_cols: 1 1 2 
+split_values: 0.662011169718969006 0.6805672568090122 0.00363231682035125569 
+weak learner at stage 12 has average loss = 0.327692
+split_cols: 3 2 0 
+split_values: 0.357489402445920146 0.0015353418386078177 0.624507340564582236 
+weak learner at stage 13 has average loss = 0.181377
+split_cols: 1 4 1 
+split_values: 0.379258691801199865 8.80684414283905426e-14 0.479480044756096346 
+weak learner at stage 13 has average loss = 0.350098
 split_cols: 2 2 2 
-split_values: 0.12373287907043895 0.997650553369808346 0.627448174543832948 
-weak learner at stage 2 has average loss = 0.221948
-split_cols: 3 3 2 
-split_values: 0.685897385955671401 0.729013358245854448 5.59499955390674319e-05 
-weak learner at stage 3 has average loss = 0.145754
-split_cols: 2 4 3 
-split_values: 0.442618283127769407 0.999999999999997558 0.0434674056274751697 
-weak learner at stage 3 has average loss = 0.182351
+split_values: 0.000981625552665510437 0.000357032461916012567 0.196634593877310471 
+weak learner at stage 14 has average loss = 0.12825
 split_cols: 1 4 4 
-split_values: 0.531511881898726335 5.26800825184636778e-13 2.22044604925031308e-16 
-weak learner at stage 4 has average loss = 0.216539
-split_cols: 1 1 4 
-split_values: 0.6805672568090122 0.662011169718969006 1.54709578481515564e-13 
-weak learner at stage 4 has average loss = 0.380743
+split_values: 0.564858493389006289 0.999999999999999334 0.999999991749624728 
+weak learner at stage 14 has average loss = 0.30729
+split_cols: 2 3 0 
+split_values: 0.000528285193333644099 0.406995257960652612 0.624507340564582236 
+weak learner at stage 15 has average loss = 0.207133
 split_cols: 1 1 1 
-split_values: 0.370269696184077013 0.384853138944362905 0.387128434225619933 
-weak learner at stage 5 has average loss = 0.249281
-split_cols: 1 3 3 
-split_values: 0.6805672568090122 0.119889252557545484 0.119702025390566957 
-weak learner at stage 5 has average loss = 0.273722
-split_cols: 2 0 0 
-split_values: 0.000528285193333644099 0.176199208821572029 0.463905694672786861 
-weak learner at stage 6 has average loss = 0.248753
-split_cols: 3 2 1 
-split_values: 0.711118874226568165 0.216049487742465185 0.488019810363248041 
-weak learner at stage 6 has average loss = 0.295528
-split_cols: 0 1 2 
-split_values: 0.441781515973124872 0.332158208341527428 0.971644395191448185 
-weak learner at stage 7 has average loss = 0.22173
-split_cols: 2 2 3 
-split_values: 0.997650553369808346 0.960331052047352696 0.642420030204781112 
-weak learner at stage 7 has average loss = 0.323144
-split_cols: 1 3 0 
-split_values: 0.272330343687464782 0.1320001318823463 0.59777085177647038 
-weak learner at stage 8 has average loss = 0.267509
-split_cols: 1 1 4 
-split_values: 0.479480044756096346 0.408499973451678211 1.54709578481515564e-13 
-weak learner at stage 8 has average loss = 0.262643
-split_cols: 3 0 2 
-split_values: 0.729013358245854448 0.176199208821572029 0.000276121091179526434 
-weak learner at stage 9 has average loss = 0.357595
-split_cols: 1 4 1 
-split_values: 0.6805672568090122 0.999999964757892545 0.531511881898726335 
-weak learner at stage 9 has average loss = 0.360215
-split_cols: 0 1 3 
-split_values: 0.556206773297698298 0.342206108309510149 0.429657342218349281 
-weak learner at stage 10 has average loss = 0.261544
-split_cols: 1 2 1 
-split_values: 0.531511881898726335 0.0689879291310910303 0.502718698860307178 
-weak learner at stage 10 has average loss = 0.316711
-split_cols: 2 0 2 
-split_values: 0.00468965205373939042 0.498163758700666093 0.00298229498924867942 
-weak learner at stage 11 has average loss = 0.32727
-split_cols: 1 4 2 
-split_values: 0.502718698860307178 0.264644704110722606 0.941974890824293754 
-weak learner at stage 11 has average loss = 0.293512
-split_cols: 1 3 0 
-split_values: 0.272330343687464782 0.357489402445920146 0.440322558328621938 
-weak learner at stage 12 has average loss = 0.25763
-split_cols: 2 1 1 
-split_values: 0.997650553369808346 0.531511881898726335 0.537120305926493513 
-weak learner at stage 12 has average loss = 0.373473
-split_cols: 0 1 1 
-split_values: 0.710330525060918871 0.370269696184077013 0.292229839503947653 
-weak learner at stage 13 has average loss = 0.233226
+split_values: 0.526575453102100632 0.544178240629241028 0.502718698860307178 
+weak learner at stage 15 has average loss = 0.329174
 split_cols: 2 2 2 
-split_values: 0.997650553369808346 0.995010648391392527 0.991025168386145405 
-weak learner at stage 13 has average loss = 0.388996
-split_cols: 2 0 0 
-split_values: 0.000357032461916012567 0.176199208821572029 0.59777085177647038 
-weak learner at stage 14 has average loss = 0.284814
-split_cols: 1 1 0 
-split_values: 0.537120305926493513 0.546046099205379054 0.62302042132145119 
-weak learner at stage 14 has average loss = 0.304367
-split_cols: 0 1 4 
-split_values: 0.531293973028163946 0.444470151470817032 6.63358257213531033e-15 
-weak learner at stage 15 has average loss = 0.239493
-split_cols: 3 1 3 
-split_values: 0.711118874226568165 0.569140400436275673 0.122140892972254167 
-weak learner at stage 15 has average loss = 0.338407
-split_cols: 3 0 1 
-split_values: 0.729013358245854448 0.59777085177647038 0.370088477642079638 
-weak learner at stage 16 has average loss = 0.33676
-split_cols: 2 1 0 
-split_values: 0.0707071480833099397 0.568316236782665074 0.227845488698554116 
-weak learner at stage 16 has average loss = 0.274603
-split_cols: 3 1 4 
-split_values: 0.729013358245854448 0.429003557741430064 2.22044604925031308e-16 
-weak learner at stage 17 has average loss = 0.282723
-split_cols: 4 1 1 
-split_values: 0.999999999999999334 0.569140400436275673 0.662011169718969006 
-weak learner at stage 17 has average loss = 0.379669
-split_cols: 2 0 2 
-split_values: 0.000276121091179526434 0.176199208821572029 0.0613580224726868739 
-weak learner at stage 18 has average loss = 0.300147
-split_cols: 2 3 2 
-split_values: 0.997650553369808346 0.935854568621468541 0.997472335875476235 
-weak learner at stage 18 has average loss = 0.340843
-split_cols: 2 4 4 
-split_values: 0.196634593877310471 2.39832292447950124e-06 8.26838597589585333e-14 
-weak learner at stage 19 has average loss = 0.344061
+split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
+weak learner at stage 16 has average loss = 0.151516
 split_cols: 2 2 2 
-split_values: 0.997650553369808346 0.00501121229176337835 0.00468965205373939042 
-weak learner at stage 19 has average loss = 0.364412
+split_values: 0.995010648391392527 0.991025168386145405 0.977100614750671781 
+weak learner at stage 16 has average loss = 0.306159
+split_cols: 1 3 2 
+split_values: 0.36967671457248541 0.406995257960652612 0.000254178900377460826 
+weak learner at stage 17 has average loss = 0.179798
+split_cols: 2 2 1 
+split_values: 0.885239426681956321 0.627448174543832948 0.482293993618237549 
+weak learner at stage 17 has average loss = 0.348588
+split_cols: 0 3 1 
+split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
+weak learner at stage 18 has average loss = 0.126962
+split_cols: 2 4 3 
+split_values: 0.997650553369808346 0.999999999999998335 0.782913532932723921 
+weak learner at stage 18 has average loss = 0.283838
+split_cols: 2 2 2 
+split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
+weak learner at stage 19 has average loss = 0.174564
+split_cols: 1 1 1 
+split_values: 0.662011169718969006 0.6805672568090122 0.502718698860307178 
+weak learner at stage 19 has average loss = 0.34564

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-11-07 18:19:03 UTC (rev 9657)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-11-07 18:28:59 UTC (rev 9658)
@@ -233,13 +233,13 @@
 learner = *5 ->MultiClassAdaBoost(
 random_gen = *0 ;
 seed = 1827 ;
-stage = 12 ;
+stage = 1 ;
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 12 ;
+nstages = 1 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;
@@ -247,7 +247,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
 learner1 = *6 ->AdaBoost(
-weak_learners = 12 [ *7 ->RegressionTree(
+weak_learners = 1 [ *7 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
@@ -274,19 +274,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000067 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.715000000000000524 ;
-weighted_squared_targets_sum = 0.715000000000000524 ;
+length = 150 ;
+weights_sum = 1.00000000000000244 ;
+targets_sum = 112 ;
+weighted_targets_sum = 0.746666666666667589 ;
+weighted_squared_targets_sum = 0.746666666666667589 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.407550000000000079 0 0.407550000000000079 ] ;
+leave_error = 3 [ 0.378311111111112819 0 0.378311111111112819 ] ;
 split_col = 2 ;
-split_balance = 88 ;
-split_feature_value = 0.00124141380660278133 ;
-after_split_error = 0.120168650793650938 ;
+split_balance = 70 ;
+split_feature_value = 0.00125079586853901747 ;
+after_split_error = 0.074181818181818418 ;
 missing_node = *0 ;
 missing_leave = *11 ->RegressionTreeLeave(
 id = 2 ;
@@ -307,19 +307,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 56 ;
-weights_sum = 0.280000000000000138 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0300000000000000024 ;
-weighted_squared_targets_sum = 0.0300000000000000024 ;
+length = 40 ;
+weights_sum = 0.266666666666666441 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.0266666666666666684 ;
+weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0535714285714285754 0 0.0535714285714285754 ] ;
+leave_error = 3 [ 0.048000000000000001 0 0.048000000000000001 ] ;
 split_col = 2 ;
-split_balance = 34 ;
-split_feature_value = 0.000276121091179526434 ;
-after_split_error = 0.0370505050505050421 ;
+split_balance = 24 ;
+split_feature_value = 0.000357032461916012567 ;
+after_split_error = 0.0266666666666666684 ;
 missing_node = *0 ;
 missing_leave = *14 ->RegressionTreeLeave(
 id = 5 ;
@@ -340,19 +340,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 45 ;
-weights_sum = 0.225000000000000117 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0050000000000000001 ;
-weighted_squared_targets_sum = 0.0050000000000000001 ;
+length = 32 ;
+weights_sum = 0.21333333333333318 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0097777777777777776 0 0.0097777777777777776 ] ;
-split_col = 4 ;
-split_balance = 39 ;
-split_feature_value = 2.00395255944840756e-14 ;
-after_split_error = 0.00666666666666666623 ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 0.113038628061597313 ;
+after_split_error = 0 ;
 missing_node = *0 ;
 missing_leave = *17 ->RegressionTreeLeave(
 id = 11 ;
@@ -373,7 +373,7 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00499999999999998449 ;
+weights_sum = 0.00666666666666665495 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
@@ -385,11 +385,11 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 44 ;
-weights_sum = 0.220000000000000112 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0050000000000000001 ;
-weighted_squared_targets_sum = 0.0050000000000000001 ;
+length = 31 ;
+weights_sum = 0.206666666666666526 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
  )
 ;
@@ -401,19 +401,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 11 ;
-weights_sum = 0.0549999999999999933 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0250000000000000014 ;
-weighted_squared_targets_sum = 0.0250000000000000014 ;
+length = 8 ;
+weights_sum = 0.0533333333333333368 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.0266666666666666684 ;
+weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0272727272727272679 0 0.0272727272727272679 ] ;
+leave_error = 3 [ 0.0266666666666666684 0 0.0266666666666666684 ] ;
 split_col = 2 ;
-split_balance = 3 ;
+split_balance = 2 ;
 split_feature_value = 0.000981625552665510437 ;
-after_split_error = 0.0142857142857142835 ;
+after_split_error = 0.0106666666666666646 ;
 missing_node = *0 ;
 missing_leave = *22 ->RegressionTreeLeave(
 id = 14 ;
@@ -434,19 +434,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0350000000000000033 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0250000000000000014 ;
-weighted_squared_targets_sum = 0.0250000000000000014 ;
+length = 5 ;
+weights_sum = 0.0333333333333333329 ;
+targets_sum = 4 ;
+weighted_targets_sum = 0.0266666666666666684 ;
+weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0142857142857142835 0 0.0142857142857142835 ] ;
+leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
 split_col = 2 ;
 split_balance = 1 ;
 split_feature_value = 0.000528285193333644099 ;
-after_split_error = 0.0100000000000000002 ;
+after_split_error = 0.00666666666666666449 ;
 missing_node = *0 ;
 missing_leave = *25 ->RegressionTreeLeave(
 id = 17 ;
@@ -467,10 +467,10 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00499999999999999837 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
+weights_sum = 0.00666666666666666189 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00666666666666666536 ;
+weighted_squared_targets_sum = 0.00666666666666666536 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
@@ -479,11 +479,11 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0300000000000000024 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0250000000000000014 ;
-weighted_squared_targets_sum = 0.0250000000000000014 ;
+length = 4 ;
+weights_sum = 0.0266666666666666684 ;
+targets_sum = 3 ;
+weighted_targets_sum = 0.0200000000000000004 ;
+weighted_squared_targets_sum = 0.0200000000000000004 ;
 loss_function_factor = 2  )
  )
 ;
@@ -495,7 +495,7 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 4 ;
+length = 3 ;
 weights_sum = 0.0200000000000000004 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
@@ -504,9 +504,9 @@
 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.336930458627675344 ;
+split_col = 4 ;
+split_balance = 1 ;
+split_feature_value = 3.42448291945629535e-13 ;
 after_split_error = 0 ;
 missing_node = *0 ;
 missing_leave = *30 ->RegressionTreeLeave(
@@ -528,7 +528,7 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00499999999999999837 ;
+weights_sum = 0.00666666666666666536 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
@@ -540,8 +540,8 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0149999999999999994 ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
 targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
@@ -560,19 +560,19 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 144 ;
-weights_sum = 0.720000000000000528 ;
-targets_sum = 137 ;
-weighted_targets_sum = 0.685000000000000497 ;
-weighted_squared_targets_sum = 0.685000000000000497 ;
+length = 110 ;
+weights_sum = 0.73333333333333417 ;
+targets_sum = 108 ;
+weighted_targets_sum = 0.720000000000000751 ;
+weighted_squared_targets_sum = 0.720000000000000751 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0665972222222224186 0 0.0665972222222224186 ] ;
+leave_error = 3 [ 0.026181818181818306 0 0.026181818181818306 ] ;
 split_col = 4 ;
-split_balance = 96 ;
-split_feature_value = 1.82065853449042692e-07 ;
-after_split_error = 0.0549166666666665859 ;
+split_balance = 88 ;
+split_feature_value = 1.54709578481515564e-13 ;
+after_split_error = 0.0218181818181818199 ;
 missing_node = *0 ;
 missing_leave = *35 ->RegressionTreeLeave(
 id = 8 ;
@@ -593,10 +593,10 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00499999999999995674 ;
+weights_sum = 0.00666666666666668271 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00499999999999995674 ;
-weighted_squared_targets_sum = 0.00499999999999995674 ;
+weighted_targets_sum = 0.00666666666666668271 ;
+weighted_squared_targets_sum = 0.00666666666666668271 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
@@ -605,11 +605,11 @@
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 143 ;
-weights_sum = 0.715000000000000524 ;
-targets_sum = 136 ;
-weighted_targets_sum = 0.680000000000000493 ;
-weighted_squared_targets_sum = 0.680000000000000493 ;
+length = 109 ;
+weights_sum = 0.72666666666666746 ;
+targets_sum = 107 ;
+weighted_targets_sum = 0.713333333333334041 ;
+weighted_squared_targets_sum = 0.713333333333334041 ;
 loss_function_factor = 2  )
  )
 ;
@@ -623,11 +623,11 @@
 ;
 first_leave = *10  ;
 split_cols = 3 [ 2 2 2 ] ;
-split_values = 3 [ 0.00124141380660278133 0.000276121091179526434 0.000981625552665510437 ] ;
+split_values = 3 [ 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 ] ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 4 ;
-n_examples = 200 ;
+n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
@@ -639,4325 +639,13 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*39 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *40 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *41 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *42 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.423684210526316163 ;
-weighted_squared_targets_sum = 0.423684210526316163 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.488351800554016724 0 0.488351800554016724 ] ;
-split_col = 1 ;
-split_balance = 46 ;
-split_feature_value = 0.444470151470817032 ;
-after_split_error = 0.204382894399834314 ;
-missing_node = *0 ;
-missing_leave = *43 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *44 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *45 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 77 ;
-weights_sum = 0.581578947368420307 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.0605263157894735601 ;
-weighted_squared_targets_sum = 0.0605263157894735601 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.108454393903310103 0 0.108454393903310103 ] ;
-split_col = 3 ;
-split_balance = 57 ;
-split_feature_value = 0.700873003833307751 ;
-after_split_error = 0.0642055375405336803 ;
-missing_node = *0 ;
-missing_leave = *46 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *47 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *48 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 67 ;
-weights_sum = 0.555263157894736126 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.0342105263157894066 ;
-weighted_squared_targets_sum = 0.0342105263157894066 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0642055375405336803 0 0.0642055375405336803 ] ;
-split_col = 2 ;
-split_balance = 65 ;
-split_feature_value = 0.207390222439248872 ;
-after_split_error = 0.059548872180451011 ;
-missing_node = *0 ;
-missing_leave = *49 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *50 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00263157894736847043 ;
-targets_sum = 0 ;
-weighted_targets_sum = 6.07153216591882483e-18 ;
-weighted_squared_targets_sum = 6.07153216591882483e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *51 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.552631578947367919 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.0342105263157894066 ;
-weighted_squared_targets_sum = 0.0342105263157894066 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *48  ;
-right_node = *52 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *53 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 10 ;
-weights_sum = 0.026315789473684157 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.026315789473684157 ;
-weighted_squared_targets_sum = 0.026315789473684157 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 4 ;
-split_balance = 0 ;
-split_feature_value = 2.1363039115485094e-10 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *54 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *55 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00263157894736841839 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00263157894736841839 ;
-weighted_squared_targets_sum = 0.00263157894736841839 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *56 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 9 ;
-weights_sum = 0.0236842105263157417 ;
-targets_sum = 9 ;
-weighted_targets_sum = 0.0236842105263157417 ;
-weighted_squared_targets_sum = 0.0236842105263157417 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *53   )
-;
-left_leave = *45  ;
-right_node = *57 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *58 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 123 ;
-weights_sum = 0.41842105263157936 ;
-targets_sum = 120 ;
-weighted_targets_sum = 0.363157894736842568 ;
-weighted_squared_targets_sum = 0.363157894736842568 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0959285004965242383 0 0.0959285004965242383 ] ;
-split_col = 1 ;
-split_balance = 71 ;
-split_feature_value = 0.531511881898726335 ;
-after_split_error = 0.07308998302207112 ;
-missing_node = *0 ;
-missing_leave = *59 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *60 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *61 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 26 ;
-weights_sum = 0.163157894736841752 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.10789473684210503 ;
-weighted_squared_targets_sum = 0.10789473684210503 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0730899830220711477 0 0.0730899830220711477 ] ;
-split_col = 1 ;
-split_balance = 24 ;
-split_feature_value = 0.530046165578986317 ;
-after_split_error = 0.0100367197062423141 ;
-missing_node = *0 ;
-missing_leave = *62 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *63 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0499999999999998918 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0499999999999998918 ;
-weighted_squared_targets_sum = 0.0499999999999998918 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *64 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 25 ;
-weights_sum = 0.11315789473684186 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.0578947368421051448 ;
-weighted_squared_targets_sum = 0.0578947368421051448 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *61  ;
-right_node = *65 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *66 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 97 ;
-weights_sum = 0.255263157894736969 ;
-targets_sum = 97 ;
-weighted_targets_sum = 0.255263157894736969 ;
-weighted_squared_targets_sum = 0.255263157894736969 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.715222895179832507 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *67 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *68 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0026315789473684288 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0026315789473684288 ;
-weighted_squared_targets_sum = 0.0026315789473684288 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *69 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 96 ;
-weights_sum = 0.25263157894736854 ;
-targets_sum = 96 ;
-weighted_targets_sum = 0.25263157894736854 ;
-weighted_squared_targets_sum = 0.25263157894736854 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *66   )
-;
-right_leave = *58   )
-;
-priority_queue = *70 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *60  *52  *47  *65  ]  )
-;
-first_leave = *42  ;
-split_cols = 3 [ 1 3 1 ] ;
-split_values = 3 [ 0.444470151470817032 0.700873003833307751 0.531511881898726335 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*71 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *72 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *73 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *74 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.000000000000002 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.405049302958179425 ;
-weighted_squared_targets_sum = 0.405049302958179425 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.481968730262545497 0 0.481968730262545497 ] ;
-split_col = 1 ;
-split_balance = 6 ;
-split_feature_value = 0.531511881898726335 ;
-after_split_error = 0.366557859017760723 ;
-missing_node = *0 ;
-missing_leave = *75 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *76 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *77 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 103 ;
-weights_sum = 0.859826589595377389 ;
-targets_sum = 46 ;
-weighted_targets_sum = 0.264875892553554315 ;
-weighted_squared_targets_sum = 0.264875892553554315 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.366557859017760723 0 0.366557859017760723 ] ;
-split_col = 1 ;
-split_balance = 101 ;
-split_feature_value = 0.530046165578986317 ;
-after_split_error = 0.287996283877794745 ;
-missing_node = *0 ;
-missing_leave = *78 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *79 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *80 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 102 ;
-weights_sum = 0.58041482488949403 ;
-targets_sum = 46 ;
-weighted_targets_sum = 0.264875892553554315 ;
-weighted_squared_targets_sum = 0.264875892553554315 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.287996283877794745 0 0.287996283877794745 ] ;
-split_col = 2 ;
-split_balance = 54 ;
-split_feature_value = 0.207390222439248872 ;
-after_split_error = 0.257105796718174184 ;
-missing_node = *0 ;
-missing_leave = *81 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *82 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *83 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 78 ;
-weights_sum = 0.532471948316900257 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.216933015980959681 ;
-weighted_squared_targets_sum = 0.216933015980959681 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.257105796718174184 0 0.257105796718174184 ] ;
-split_col = 2 ;
-split_balance = 68 ;
-split_feature_value = 0.09124051799518762 ;
-after_split_error = 0.228781330125952642 ;
-missing_node = *0 ;
-missing_leave = *84 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *85 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00144508670520230014 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.38777878078144568e-17 ;
-weighted_squared_targets_sum = -1.38777878078144568e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *86 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 77 ;
-weights_sum = 0.531026861611698209 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.216933015980959626 ;
-weighted_squared_targets_sum = 0.216933015980959626 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *83  ;
-right_node = *87 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *88 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.0479428765725944325 ;
-targets_sum = 24 ;
-weighted_targets_sum = 0.0479428765725944325 ;
-weighted_squared_targets_sum = 0.0479428765725944325 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 4 ;
-split_balance = 0 ;
-split_feature_value = 0.0395302655963922223 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *89 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *90 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00144508670520231402 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00144508670520231402 ;
-weighted_squared_targets_sum = 0.00144508670520231402 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *91 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.046497789867392135 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.046497789867392135 ;
-weighted_squared_targets_sum = 0.046497789867392135 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *88   )
-;
-left_leave = *80  ;
-right_node = *92 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *93 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.279411764705883414 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *94 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *95 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *96 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *93   )
-;
-left_leave = *77  ;
-right_node = *97 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *98 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 97 ;
-weights_sum = 0.140173410404624416 ;
-targets_sum = 97 ;
-weighted_targets_sum = 0.140173410404624416 ;
-weighted_squared_targets_sum = 0.140173410404624416 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.715222895179832507 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *99 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *100 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00144508670520231228 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00144508670520231228 ;
-weighted_squared_targets_sum = 0.00144508670520231228 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *101 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 96 ;
-weights_sum = 0.13872832369942209 ;
-targets_sum = 96 ;
-weighted_targets_sum = 0.13872832369942209 ;
-weighted_squared_targets_sum = 0.13872832369942209 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *98   )
-;
-priority_queue = *102 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 3 ;
-nodes = 4 [ *82  *97  *87  *0 ]  )
-;
-first_leave = *74  ;
-split_cols = 3 [ 1 1 2 ] ;
-split_values = 3 [ 0.531511881898726335 0.530046165578986317 0.207390222439248872 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*103 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *104 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *105 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *106 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999999112 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.620115067303516065 ;
-weighted_squared_targets_sum = 0.620115067303516065 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.471144741213342932 0 0.471144741213342932 ] ;
-split_col = 3 ;
-split_balance = 52 ;
-split_feature_value = 0.685897385955671401 ;
-after_split_error = 0.358804093433449267 ;
-missing_node = *0 ;
-missing_leave = *107 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *108 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *109 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 126 ;
-weights_sum = 0.718608054507068772 ;
-targets_sum = 72 ;
-weighted_targets_sum = 0.55219468064688626 ;
-weighted_squared_targets_sum = 0.55219468064688626 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.255751599937544039 0 0.255751599937544039 ] ;
-split_col = 2 ;
-split_balance = 62 ;
-split_feature_value = 5.59499955390674319e-05 ;
-after_split_error = 0.21938800334806241 ;
-missing_node = *0 ;
-missing_leave = *110 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *111 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *112 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 32 ;
-weights_sum = 0.0295267042987408246 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.088465696311848363 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *113 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *114 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000922709509335650768 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *115 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 31 ;
-weights_sum = 0.0286039947894051738 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *112  ;
-right_node = *116 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *117 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.689081350208326171 ;
-targets_sum = 72 ;
-weighted_targets_sum = 0.552194680646884484 ;
-weighted_squared_targets_sum = 0.552194680646884484 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.219388003348062632 0 0.219388003348062632 ] ;
-split_col = 4 ;
-split_balance = 34 ;
-split_feature_value = 5.26800825184636778e-13 ;
-after_split_error = 0.205540286541139705 ;
-missing_node = *0 ;
-missing_leave = *118 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *119 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000922709509335605665 ;
-targets_sum = 0 ;
-weighted_targets_sum = -9.71445146547011973e-17 ;
-weighted_squared_targets_sum = -9.71445146547011973e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *120 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 93 ;
-weights_sum = 0.688158640698992574 ;
-targets_sum = 72 ;
-weighted_targets_sum = 0.552194680646886593 ;
-weighted_squared_targets_sum = 0.552194680646886593 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *117   )
-;
-left_leave = *109  ;
-right_node = *121 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *122 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 74 ;
-weights_sum = 0.281391945492931617 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.0679203866566303327 ;
-weighted_squared_targets_sum = 0.0679203866566303327 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.103052493495905007 0 0.103052493495905007 ] ;
-split_col = 3 ;
-split_balance = 54 ;
-split_feature_value = 0.729013358245854448 ;
-after_split_error = 0.0170267165996853057 ;
-missing_node = *0 ;
-missing_leave = *123 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *124 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *125 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 10 ;
-weights_sum = 0.222338536895449967 ;
-targets_sum = 7 ;
-weighted_targets_sum = 0.00886697805914869221 ;
-weighted_squared_targets_sum = 0.00886697805914869221 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0170267165996853195 0 0.0170267165996853195 ] ;
-split_col = 4 ;
-split_balance = 2 ;
-split_feature_value = 0.999999999998233857 ;
-after_split_error = 0.00655910265440538646 ;
-missing_node = *0 ;
-missing_leave = *126 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *127 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00333072100313478586 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0033307210031347876 ;
-weighted_squared_targets_sum = 0.0033307210031347876 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *128 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 9 ;
-weights_sum = 0.219007815892315189 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.00553625705601390461 ;
-weighted_squared_targets_sum = 0.00553625705601390461 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *125  ;
-right_node = *129 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *130 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 64 ;
-weights_sum = 0.0590534085974816492 ;
-targets_sum = 64 ;
-weighted_targets_sum = 0.0590534085974816492 ;
-weighted_squared_targets_sum = 0.0590534085974816492 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.912877564103069084 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *131 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *132 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000922709509335650768 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000922709509335650768 ;
-weighted_squared_targets_sum = 0.000922709509335650768 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *133 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 63 ;
-weights_sum = 0.0581306990881459984 ;
-targets_sum = 63 ;
-weighted_targets_sum = 0.0581306990881459984 ;
-weighted_squared_targets_sum = 0.0581306990881459984 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *130   )
-;
-right_leave = *122   )
-;
-priority_queue = *134 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *116  *124  *111  *129  ]  )
-;
-first_leave = *106  ;
-split_cols = 3 [ 3 3 2 ] ;
-split_values = 3 [ 0.685897385955671401 0.729013358245854448 5.59499955390674319e-05 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*135 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *136 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *137 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *138 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999998113 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.388188071299955917 ;
-weighted_squared_targets_sum = 0.388188071299955917 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.47499618520075193 0 0.47499618520075193 ] ;
-split_col = 1 ;
-split_balance = 6 ;
-split_feature_value = 0.531511881898726335 ;
-after_split_error = 0.420255978931199037 ;
-missing_node = *0 ;
-missing_leave = *139 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *140 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *141 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 103 ;
-weights_sum = 0.931861594499045709 ;
-targets_sum = 46 ;
-weighted_targets_sum = 0.320049665799003458 ;
-weighted_squared_targets_sum = 0.320049665799003458 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.420255978931199037 0 0.420255978931199037 ] ;
-split_col = 4 ;
-split_balance = 35 ;
-split_feature_value = 5.26800825184636778e-13 ;
-after_split_error = 0.359957974049163687 ;
-missing_node = *0 ;
-missing_leave = *142 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *143 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *144 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 69 ;
-weights_sum = 0.608213401749990279 ;
-targets_sum = 18 ;
-weighted_targets_sum = 0.288696195727700777 ;
-weighted_squared_targets_sum = 0.288696195727700777 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.303325778691394821 0 0.303325778691394821 ] ;
-split_col = 4 ;
-split_balance = 53 ;
-split_feature_value = 2.22044604925031308e-16 ;
-after_split_error = 0.244130964831055619 ;
-missing_node = *0 ;
-missing_leave = *145 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *146 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *147 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 61 ;
-weights_sum = 0.477849528090813769 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.171758999966496545 ;
-weighted_squared_targets_sum = 0.171758999966496545 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.220043339667620352 0 0.220043339667620352 ] ;
-split_col = 3 ;
-split_balance = 23 ;
-split_feature_value = 0.360834998492078562 ;
-after_split_error = 0.172300210882511851 ;
-missing_node = *0 ;
-missing_leave = *148 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *149 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00316530503489540968 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.71303943252709701e-17 ;
-weighted_squared_targets_sum = -1.71303943252709701e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *150 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 60 ;
-weights_sum = 0.474684223055918453 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.171758999966496545 ;
-weighted_squared_targets_sum = 0.171758999966496545 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *147  ;
-right_node = *151 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *152 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 8 ;
-weights_sum = 0.130363873659176677 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.116937195761204205 ;
-weighted_squared_targets_sum = 0.116937195761204205 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0240876251634352945 0 0.0240876251634352945 ] ;
-split_col = 3 ;
-split_balance = 4 ;
-split_feature_value = 0.675953685167010931 ;
-after_split_error = 0.00718915029260175191 ;
-missing_node = *0 ;
-missing_leave = *153 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *154 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0198391156319297941 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0198391156319297941 ;
-weighted_squared_targets_sum = 0.0198391156319297941 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *155 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.110524758027246886 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0970980801292744139 ;
-weighted_squared_targets_sum = 0.0970980801292744139 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *152   )
-;
-left_leave = *144  ;
-right_node = *156 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *157 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 34 ;
-weights_sum = 0.32364819274905543 ;
-targets_sum = 28 ;
-weighted_targets_sum = 0.0313534700713027983 ;
-weighted_squared_targets_sum = 0.0313534700713027983 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0566321953577688594 0 0.0566321953577688594 ] ;
-split_col = 1 ;
-split_balance = 22 ;
-split_feature_value = 0.429003557741430064 ;
-after_split_error = 0.0429142711928514817 ;
-missing_node = *0 ;
-missing_leave = *158 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *159 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00316530503489537152 ;
-targets_sum = 0 ;
-weighted_targets_sum = -6.28837260041592572e-18 ;
-weighted_squared_targets_sum = -6.28837260041592572e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *160 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 33 ;
-weights_sum = 0.320482887714160003 ;
-targets_sum = 28 ;
-weighted_targets_sum = 0.0313534700713028053 ;
-weighted_squared_targets_sum = 0.0313534700713028053 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *157   )
-;
-left_leave = *141  ;
-right_node = *161 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *162 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 97 ;
-weights_sum = 0.0681384055009519318 ;
-targets_sum = 97 ;
-weighted_targets_sum = 0.0681384055009519318 ;
-weighted_squared_targets_sum = 0.0681384055009519318 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.715222895179832507 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *163 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *164 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000540072255951422981 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000540072255951422981 ;
-weighted_squared_targets_sum = 0.000540072255951422981 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *165 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 96 ;
-weights_sum = 0.0675983332450005719 ;
-targets_sum = 96 ;
-weighted_targets_sum = 0.0675983332450005719 ;
-weighted_squared_targets_sum = 0.0675983332450005719 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *162   )
-;
-priority_queue = *166 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *146  *151  *156  *161  ]  )
-;
-first_leave = *138  ;
-split_cols = 3 [ 1 4 4 ] ;
-split_values = 3 [ 0.531511881898726335 5.26800825184636778e-13 2.22044604925031308e-16 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*167 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *168 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *169 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *170 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999999778 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.58711124673282844 ;
-weighted_squared_targets_sum = 0.58711124673282844 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.484823261385304405 0 0.484823261385304405 ] ;
-split_col = 1 ;
-split_balance = 98 ;
-split_feature_value = 0.370269696184077013 ;
-after_split_error = 0.417855418629112019 ;
-missing_node = *0 ;
-missing_leave = *171 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *172 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *173 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 51 ;
-weights_sum = 0.389073469268640726 ;
-targets_sum = 11 ;
-weighted_targets_sum = 0.317642474223453875 ;
-weighted_squared_targets_sum = 0.317642474223453875 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.116633591311414597 0 0.116633591311414597 ] ;
-split_col = 3 ;
-split_balance = 17 ;
-split_feature_value = 0.379758326487598441 ;
-after_split_error = 0.0817399725429753532 ;
-missing_node = *0 ;
-missing_leave = *174 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *175 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00202007862074767616 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.99493199737332816e-17 ;
-weighted_squared_targets_sum = -1.99493199737332816e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *176 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 50 ;
-weights_sum = 0.387053390647893014 ;
-targets_sum = 11 ;
-weighted_targets_sum = 0.317642474223453875 ;
-weighted_squared_targets_sum = 0.317642474223453875 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *173  ;
-right_node = *177 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *178 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.610926530731358608 ;
-targets_sum = 132 ;
-weighted_targets_sum = 0.269468772509374177 ;
-weighted_squared_targets_sum = 0.269468772509374177 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.301221827317697977 0 0.301221827317697977 ] ;
-split_col = 1 ;
-split_balance = 137 ;
-split_feature_value = 0.384853138944362905 ;
-after_split_error = 0.26619311422357661 ;
-missing_node = *0 ;
-missing_leave = *179 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *180 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *181 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0798170787980364282 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000344670862959180939 ;
-weighted_squared_targets_sum = 0.000344670862959180939 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.000686364969425591089 0 0.000686364969425591089 ] ;
-split_col = 4 ;
-split_balance = 4 ;
-split_feature_value = 8.62643290133746632e-14 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *182 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *183 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000344670862959175085 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *184 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0794724079350772467 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000344670862959180939 ;
-weighted_squared_targets_sum = 0.000344670862959180939 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *181  ;
-right_node = *185 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *186 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 143 ;
-weights_sum = 0.531109451933322152 ;
-targets_sum = 131 ;
-weighted_targets_sum = 0.269124101646414926 ;
-weighted_squared_targets_sum = 0.269124101646414926 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.265506749254151686 0 0.265506749254151686 ] ;
-split_col = 1 ;
-split_balance = 139 ;
-split_feature_value = 0.387128434225619933 ;
-after_split_error = 0.211625644627081194 ;
-missing_node = *0 ;
-missing_leave = *187 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *188 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *189 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0916190712905839699 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0916190712905839699 ;
-weighted_squared_targets_sum = 0.0916190712905839699 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.307088750247426767 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *190 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *191 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.045809535645291985 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.045809535645291985 ;
-weighted_squared_targets_sum = 0.045809535645291985 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *192 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.045809535645291985 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.045809535645291985 ;
-weighted_squared_targets_sum = 0.045809535645291985 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *189  ;
-right_node = *193 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *194 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 141 ;
-weights_sum = 0.439490380642737988 ;
-targets_sum = 129 ;
-weighted_targets_sum = 0.177505030355829679 ;
-weighted_squared_targets_sum = 0.177505030355829679 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.211625644627081055 0 0.211625644627081055 ] ;
-split_col = 2 ;
-split_balance = 27 ;
-split_feature_value = 0.96944116901060462 ;
-after_split_error = 0.173303874638066624 ;
-missing_node = *0 ;
-missing_leave = *195 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *196 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0383814937942055859 ;
-targets_sum = 0 ;
-weighted_targets_sum = -2.08166817117216851e-17 ;
-weighted_squared_targets_sum = -2.08166817117216851e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *197 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 140 ;
-weights_sum = 0.401108886848531354 ;
-targets_sum = 129 ;
-weighted_targets_sum = 0.177505030355830928 ;
-weighted_squared_targets_sum = 0.177505030355830928 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *194   )
-;
-right_leave = *186   )
-;
-right_leave = *178   )
-;
-priority_queue = *198 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *193  *172  *188  *180  ]  )
-;
-first_leave = *170  ;
-split_cols = 3 [ 1 1 1 ] ;
-split_values = 3 [ 0.370269696184077013 0.384853138944362905 0.387128434225619933 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*199 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *200 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *201 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *202 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999996114 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.629305386150195267 ;
-weighted_squared_targets_sum = 0.629305386150195267 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.466560234225094739 0 0.466560234225094739 ] ;
-split_col = 2 ;
-split_balance = 102 ;
-split_feature_value = 0.000528285193333644099 ;
-after_split_error = 0.411043969618775706 ;
-missing_node = *0 ;
-missing_leave = *203 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *204 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *205 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 49 ;
-weights_sum = 0.292780721380140052 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.108435533463665898 ;
-weighted_squared_targets_sum = 0.108435533463665898 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.136549761192975516 0 0.136549761192975516 ] ;
-split_col = 0 ;
-split_balance = 43 ;
-split_feature_value = 0.176199208821572029 ;
-after_split_error = 0.0942057438390707658 ;
-missing_node = *0 ;
-missing_leave = *206 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *207 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *208 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0487973376442767809 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0474146759776301538 ;
-weighted_squared_targets_sum = 0.0474146759776301538 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.00268696851408774251 0 0.00268696851408774251 ] ;
-split_col = 4 ;
-split_balance = 1 ;
-split_feature_value = 1.41553435639707459e-14 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *209 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *210 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000691330833323314958 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *211 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0481060068109534639 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0474146759776301538 ;
-weighted_squared_targets_sum = 0.0474146759776301538 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *208  ;
-right_node = *212 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *213 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 46 ;
-weights_sum = 0.243983383735863368 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0610208574860357442 ;
-weighted_squared_targets_sum = 0.0610208574860357442 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0915187753249830788 0 0.0915187753249830788 ] ;
-split_col = 3 ;
-split_balance = 38 ;
-split_feature_value = 0.429778803348615179 ;
-after_split_error = 0.0691576696932000023 ;
-missing_node = *0 ;
-missing_leave = *214 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *215 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000691330833323316259 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *216 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 45 ;
-weights_sum = 0.243292052902540079 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0610208574860357442 ;
-weighted_squared_targets_sum = 0.0610208574860357442 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *213   )
-;
-left_leave = *205  ;
-right_node = *217 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *218 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 151 ;
-weights_sum = 0.707219278619855785 ;
-targets_sum = 140 ;
-weighted_targets_sum = 0.520869852686528523 ;
-weighted_squared_targets_sum = 0.520869852686528523 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.274494208425800079 0 0.274494208425800079 ] ;
-split_col = 0 ;
-split_balance = 29 ;
-split_feature_value = 0.463905694672786861 ;
-after_split_error = 0.243132135257500215 ;
-missing_node = *0 ;
-missing_leave = *219 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *220 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *221 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 61 ;
-weights_sum = 0.186240969298665071 ;
-targets_sum = 59 ;
-weighted_targets_sum = 0.183550111862743892 ;
-weighted_squared_targets_sum = 0.183550111862743892 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.00530395847089876193 0 0.00530395847089876193 ] ;
-split_col = 2 ;
-split_balance = 53 ;
-split_feature_value = 0.00123333066520273094 ;
-after_split_error = 0.00499154125417236583 ;
-missing_node = *0 ;
-missing_leave = *222 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *223 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0305104287430178409 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0305104287430178478 ;
-weighted_squared_targets_sum = 0.0305104287430178478 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *224 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 60 ;
-weights_sum = 0.15573054055564714 ;
-targets_sum = 58 ;
-weighted_targets_sum = 0.153039683119725933 ;
-weighted_squared_targets_sum = 0.153039683119725933 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *221  ;
-right_node = *225 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *226 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 90 ;
-weights_sum = 0.520978309321192157 ;
-targets_sum = 81 ;
-weighted_targets_sum = 0.337319740823786074 ;
-weighted_squared_targets_sum = 0.337319740823786074 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.237828176786601342 0 0.237828176786601342 ] ;
-split_col = 1 ;
-split_balance = 66 ;
-split_feature_value = 0.370088477642079638 ;
-after_split_error = 0.210976697344030684 ;
-missing_node = *0 ;
-missing_leave = *227 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *228 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00405181518469544758 ;
-targets_sum = 0 ;
-weighted_targets_sum = -3.40439482160448392e-17 ;
-weighted_squared_targets_sum = -3.40439482160448392e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *229 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.516926494136497161 ;
-targets_sum = 81 ;
-weighted_targets_sum = 0.337319740823786685 ;
-weighted_squared_targets_sum = 0.337319740823786685 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *226   )
-;
-right_leave = *218   )
-;
-priority_queue = *230 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *225  *212  *220  *207  ]  )
-;
-first_leave = *202  ;
-split_cols = 3 [ 2 0 0 ] ;
-split_values = 3 [ 0.000528285193333644099 0.176199208821572029 0.463905694672786861 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*231 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *232 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *233 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *234 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999999778 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.5008809963217008 ;
-weighted_squared_targets_sum = 0.5008809963217008 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.499998447690962133 0 0.499998447690962133 ] ;
-split_col = 0 ;
-split_balance = 20 ;
-split_feature_value = 0.441781515973124872 ;
-after_split_error = 0.4387891171211033 ;
-missing_node = *0 ;
-missing_leave = *235 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *236 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *237 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 90 ;
-weights_sum = 0.272068498653787016 ;
-targets_sum = 59 ;
-weighted_targets_sum = 0.21412745187236959 ;
-weighted_squared_targets_sum = 0.21412745187236959 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0912032724663986438 0 0.0912032724663986438 ] ;
-split_col = 3 ;
-split_balance = 10 ;
-split_feature_value = 0.1320001318823463 ;
-after_split_error = 0.0588319084659889788 ;
-missing_node = *0 ;
-missing_leave = *238 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *239 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000460122158750065052 ;
-targets_sum = 0 ;
-weighted_targets_sum = -3.46944695195361419e-18 ;
-weighted_squared_targets_sum = -3.46944695195361419e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *240 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.271608376495036774 ;
-targets_sum = 59 ;
-weighted_targets_sum = 0.214127451872369506 ;
-weighted_squared_targets_sum = 0.214127451872369506 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *237  ;
-right_node = *241 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *242 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 110 ;
-weights_sum = 0.727931501346212873 ;
-targets_sum = 84 ;
-weighted_targets_sum = 0.286753544449331044 ;
-weighted_squared_targets_sum = 0.286753544449331044 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.347585844654703879 0 0.347585844654703879 ] ;
-split_col = 1 ;
-split_balance = 56 ;
-split_feature_value = 0.332158208341527428 ;
-after_split_error = 0.302374292810142387 ;
-missing_node = *0 ;
-missing_leave = *243 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *244 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *245 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 27 ;
-weights_sum = 0.199072132905354138 ;
-targets_sum = 8 ;
-weighted_targets_sum = 0.135599770894035354 ;
-weighted_squared_targets_sum = 0.135599770894035354 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0864695386664701138 0 0.0864695386664701138 ] ;
-split_col = 1 ;
-split_balance = 9 ;
-split_feature_value = 0.272330343687464782 ;
-after_split_error = 0.0454699356707378433 ;
-missing_node = *0 ;
-missing_leave = *246 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *247 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00814425570578567772 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *248 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 26 ;
-weights_sum = 0.190927877199568524 ;
-targets_sum = 8 ;
-weighted_targets_sum = 0.135599770894035326 ;
-weighted_squared_targets_sum = 0.135599770894035326 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *245  ;
-right_node = *249 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *250 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 83 ;
-weights_sum = 0.528859368440858457 ;
-targets_sum = 76 ;
-weighted_targets_sum = 0.151153773555296023 ;
-weighted_squared_targets_sum = 0.151153773555296023 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.215904754143672328 0 0.215904754143672328 ] ;
-split_col = 2 ;
-split_balance = 37 ;
-split_feature_value = 0.971644395191448185 ;
-after_split_error = 0.158529236007423413 ;
-missing_node = *0 ;
-missing_leave = *251 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *252 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *253 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.478022548707630446 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.100316953822065513 ;
-weighted_squared_targets_sum = 0.100316953822065513 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.158529236007423413 0 0.158529236007423413 ] ;
-split_col = 1 ;
-split_balance = 3 ;
-split_feature_value = 0.429987773183261757 ;
-after_split_error = 0.144586261367377789 ;
-missing_node = *0 ;
-missing_leave = *254 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *255 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0512377972104179208 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *256 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 22 ;
-weights_sum = 0.426784751497212567 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.100316953822065499 ;
-weighted_squared_targets_sum = 0.100316953822065499 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *253  ;
-right_node = *257 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *258 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 60 ;
-weights_sum = 0.0508368197332301694 ;
-targets_sum = 60 ;
-weighted_targets_sum = 0.0508368197332301694 ;
-weighted_squared_targets_sum = 0.0508368197332301694 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.901066733516976193 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *259 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *260 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00166476917446302408 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00166476917446302408 ;
-weighted_squared_targets_sum = 0.00166476917446302408 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *261 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 59 ;
-weights_sum = 0.0491720505587671525 ;
-targets_sum = 59 ;
-weighted_targets_sum = 0.0491720505587671525 ;
-weighted_squared_targets_sum = 0.0491720505587671525 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *258   )
-;
-right_leave = *250   )
-;
-right_leave = *242   )
-;
-priority_queue = *262 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *244  *236  *252  *257  ]  )
-;
-first_leave = *234  ;
-split_cols = 3 [ 0 1 2 ] ;
-split_values = 3 [ 0.441781515973124872 0.332158208341527428 0.971644395191448185 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*263 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *264 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *265 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *266 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000067 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.48355655532398456 ;
-weighted_squared_targets_sum = 0.48355655532398456 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499459226254373945 0 0.499459226254373945 ] ;
-split_col = 1 ;
-split_balance = 136 ;
-split_feature_value = 0.272330343687464782 ;
-after_split_error = 0.441602821067810181 ;
-missing_node = *0 ;
-missing_leave = *267 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *268 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *269 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 32 ;
-weights_sum = 0.138506311028580187 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.00822378328030563301 ;
-weighted_squared_targets_sum = 0.00822378328030563301 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0154709957323335683 0 0.0154709957323335683 ] ;
-split_col = 3 ;
-split_balance = 26 ;
-split_feature_value = 0.700873003833307751 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *270 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *271 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00103757138473049566 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *272 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 31 ;
-weights_sum = 0.137468739643849619 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.00822378328030563301 ;
-weighted_squared_targets_sum = 0.00822378328030563301 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *269  ;
-right_node = *273 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *274 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 168 ;
-weights_sum = 0.861493688971420868 ;
-targets_sum = 140 ;
-weighted_targets_sum = 0.475332772043679297 ;
-weighted_squared_targets_sum = 0.475332772043679297 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.426131825335476444 0 0.426131825335476444 ] ;
-split_col = 3 ;
-split_balance = 110 ;
-split_feature_value = 0.1320001318823463 ;
-after_split_error = 0.391676956726106418 ;
-missing_node = *0 ;
-missing_leave = *275 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *276 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *277 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 29 ;
-weights_sum = 0.136791871263686748 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.0309515017211229419 ;
-weighted_squared_targets_sum = 0.0309515017211229419 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0478963895997318451 0 0.0478963895997318451 ] ;
-split_col = 0 ;
-split_balance = 19 ;
-split_feature_value = 0.176199208821572029 ;
-after_split_error = 0.017426739380946122 ;
-missing_node = *0 ;
-missing_leave = *278 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *279 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00103757138473049154 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *280 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 28 ;
-weights_sum = 0.135754299878956319 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.0309515017211229385 ;
-weighted_squared_targets_sum = 0.0309515017211229385 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *277  ;
-right_node = *281 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *282 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 139 ;
-weights_sum = 0.724701817707733786 ;
-targets_sum = 127 ;
-weighted_targets_sum = 0.444381270322556088 ;
-weighted_squared_targets_sum = 0.444381270322556088 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.343780567126374281 0 0.343780567126374281 ] ;
-split_col = 0 ;
-split_balance = 17 ;
-split_feature_value = 0.59777085177647038 ;
-after_split_error = 0.31078862576163474 ;
-missing_node = *0 ;
-missing_leave = *283 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *284 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *285 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 78 ;
-weights_sum = 0.518544265715617958 ;
-targets_sum = 70 ;
-weighted_targets_sum = 0.367296075830272795 ;
-weighted_squared_targets_sum = 0.367296075830272795 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.214264703302247117 0 0.214264703302247117 ] ;
-split_col = 0 ;
-split_balance = 56 ;
-split_feature_value = 0.548110156284199013 ;
-after_split_error = 0.180073536407568624 ;
-missing_node = *0 ;
-missing_leave = *286 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *287 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0130459421710816051 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0130459421710815635 ;
-weighted_squared_targets_sum = 0.0130459421710815635 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *288 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 77 ;
-weights_sum = 0.505498323544536388 ;
-targets_sum = 69 ;
-weighted_targets_sum = 0.354250133659191224 ;
-weighted_squared_targets_sum = 0.354250133659191224 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *285  ;
-right_node = *289 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *290 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 61 ;
-weights_sum = 0.206157551992115801 ;
-targets_sum = 57 ;
-weighted_targets_sum = 0.0770851944922828491 ;
-weighted_squared_targets_sum = 0.0770851944922828491 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0965239224593876788 0 0.0965239224593876788 ] ;
-split_col = 3 ;
-split_balance = 47 ;
-split_feature_value = 0.734861316589340507 ;
-after_split_error = 0.0596029356202125576 ;
-missing_node = *0 ;
-missing_leave = *291 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *292 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0183652243421758739 ;
-targets_sum = 0 ;
-weighted_targets_sum = 6.5052130349130266e-19 ;
-weighted_squared_targets_sum = 6.5052130349130266e-19 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *293 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 60 ;
-weights_sum = 0.187792327649939778 ;
-targets_sum = 57 ;
-weighted_targets_sum = 0.0770851944922828491 ;
-weighted_squared_targets_sum = 0.0770851944922828491 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *290   )
-;
-right_leave = *282   )
-;
-right_leave = *274   )
-;
-priority_queue = *294 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *289  *284  *276  *268  ]  )
-;
-first_leave = *266  ;
-split_cols = 3 [ 1 3 0 ] ;
-split_values = 3 [ 0.272330343687464782 0.1320001318823463 0.59777085177647038 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*295 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *296 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *297 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *298 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999999223 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.468019277192740835 ;
-weighted_squared_targets_sum = 0.468019277192740835 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.497954466737450141 0 0.497954466737450141 ] ;
-split_col = 3 ;
-split_balance = 72 ;
-split_feature_value = 0.729013358245854448 ;
-after_split_error = 0.452327897424011416 ;
-missing_node = *0 ;
-missing_leave = *299 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *300 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *301 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 136 ;
-weights_sum = 0.925402114634777728 ;
-targets_sum = 79 ;
-weighted_targets_sum = 0.39342139182752045 ;
-weighted_squared_targets_sum = 0.39342139182752045 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.452327897424011416 0 0.452327897424011416 ] ;
-split_col = 0 ;
-split_balance = 122 ;
-split_feature_value = 0.176199208821572029 ;
-after_split_error = 0.430982516459485931 ;
-missing_node = *0 ;
-missing_leave = *302 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *303 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *304 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0443821012400811982 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0401042069557288258 ;
-weighted_squared_targets_sum = 0.0401042069557288258 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.00773111470258469602 0 0.00773111470258469602 ] ;
-split_col = 4 ;
-split_balance = 3 ;
-split_feature_value = 1.41553435639707459e-14 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *305 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *306 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00213894714217618617 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *307 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0422431540979050085 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0401042069557288258 ;
-weighted_squared_targets_sum = 0.0401042069557288258 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *304  ;
-right_node = *308 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *309 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 129 ;
-weights_sum = 0.881020013394697377 ;
-targets_sum = 74 ;
-weighted_targets_sum = 0.353317184871791368 ;
-weighted_squared_targets_sum = 0.353317184871791368 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.423251401756901291 0 0.423251401756901291 ] ;
-split_col = 2 ;
-split_balance = 45 ;
-split_feature_value = 0.000276121091179526434 ;
-after_split_error = 0.38776620905643866 ;
-missing_node = *0 ;
-missing_leave = *310 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *311 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *312 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 42 ;
-weights_sum = 0.0980441724909268952 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.107157403318795141 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *313 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *314 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000708248235409786038 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *315 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.0973359242555170978 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *312  ;
-right_node = *316 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *317 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 87 ;
-weights_sum = 0.782975840903769482 ;
-targets_sum = 74 ;
-weighted_targets_sum = 0.353317184871791201 ;
-weighted_squared_targets_sum = 0.353317184871791201 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.387766209056438271 0 0.387766209056438271 ] ;
-split_col = 2 ;
-split_balance = 21 ;
-split_feature_value = 0.09124051799518762 ;
-after_split_error = 0.364099940337007988 ;
-missing_node = *0 ;
-missing_leave = *318 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *319 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0125361376813889237 ;
-targets_sum = 0 ;
-weighted_targets_sum = -2.25514051876984922e-17 ;
-weighted_squared_targets_sum = -2.25514051876984922e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *320 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 86 ;
-weights_sum = 0.770439703222381311 ;
-targets_sum = 74 ;
-weighted_targets_sum = 0.353317184871791479 ;
-weighted_squared_targets_sum = 0.353317184871791479 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *317   )
-;
-right_leave = *309   )
-;
-left_leave = *301  ;
-right_node = *321 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *322 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 64 ;
-weights_sum = 0.0745978853652202734 ;
-targets_sum = 64 ;
-weighted_targets_sum = 0.0745978853652202734 ;
-weighted_squared_targets_sum = 0.0745978853652202734 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.912877564103069084 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *323 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *324 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000663799852083516665 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000663799852083516665 ;
-weighted_squared_targets_sum = 0.000663799852083516665 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *325 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 63 ;
-weights_sum = 0.073934085513136738 ;
-targets_sum = 63 ;
-weighted_targets_sum = 0.073934085513136738 ;
-weighted_squared_targets_sum = 0.073934085513136738 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *322   )
-;
-priority_queue = *326 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *316  *303  *311  *321  ]  )
-;
-first_leave = *298  ;
-split_cols = 3 [ 3 0 2 ] ;
-split_values = 3 [ 0.729013358245854448 0.176199208821572029 0.000276121091179526434 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*327 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *328 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *329 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *330 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.99999999999999778 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.583294061806513442 ;
-weighted_squared_targets_sum = 0.583294061806513442 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.486124198535543894 0 0.486124198535543894 ] ;
-split_col = 0 ;
-split_balance = 34 ;
-split_feature_value = 0.556206773297698298 ;
-after_split_error = 0.456703256187367457 ;
-missing_node = *0 ;
-missing_leave = *331 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *332 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *333 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 117 ;
-weights_sum = 0.563200197084261611 ;
-targets_sum = 73 ;
-weighted_targets_sum = 0.268354366925050547 ;
-weighted_squared_targets_sum = 0.268354366925050547 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.280977053994277137 0 0.280977053994277137 ] ;
-split_col = 1 ;
-split_balance = 61 ;
-split_feature_value = 0.342206108309510149 ;
-after_split_error = 0.233696696643850843 ;
-missing_node = *0 ;
-missing_leave = *334 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *335 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *336 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 28 ;
-weights_sum = 0.0878787087915239573 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.113038628061597313 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *337 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *338 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000551247517281962642 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *339 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 27 ;
-weights_sum = 0.0873274612742419715 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *336  ;
-right_node = *340 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *341 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.475321488292738348 ;
-targets_sum = 73 ;
-weighted_targets_sum = 0.268354366925050991 ;
-weighted_squared_targets_sum = 0.268354366925050991 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.23369669664385101 0 0.23369669664385101 ] ;
-split_col = 3 ;
-split_balance = 31 ;
-split_feature_value = 0.429657342218349281 ;
-after_split_error = 0.19590829899505241 ;
-missing_node = *0 ;
-missing_leave = *342 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *343 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *344 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 60 ;
-weights_sum = 0.350512323495811906 ;
-targets_sum = 46 ;
-weighted_targets_sum = 0.239591198964361374 ;
-weighted_squared_targets_sum = 0.239591198964361374 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.151639320135247724 0 0.151639320135247724 ] ;
-split_col = 3 ;
-split_balance = 36 ;
-split_feature_value = 0.32100875926977368 ;
-after_split_error = 0.125773950295607173 ;
-missing_node = *0 ;
-missing_leave = *345 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *346 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000551247517281989313 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.21430643318376497e-17 ;
-weighted_squared_targets_sum = -1.21430643318376497e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *347 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 59 ;
-weights_sum = 0.349961075978529823 ;
-targets_sum = 46 ;
-weighted_targets_sum = 0.239591198964361429 ;
-weighted_squared_targets_sum = 0.239591198964361429 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *344  ;
-right_node = *348 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *349 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 29 ;
-weights_sum = 0.124809164796925928 ;
-targets_sum = 27 ;
-weighted_targets_sum = 0.0287631679606891143 ;
-weighted_squared_targets_sum = 0.0287631679606891143 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0442689788598046585 0 0.0442689788598046585 ] ;
-split_col = 4 ;
-split_balance = 25 ;
-split_feature_value = 0.000988084490728707854 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *350 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *351 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0480229984181184538 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *352 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 28 ;
-weights_sum = 0.0767861663788075716 ;
-targets_sum = 27 ;
-weighted_targets_sum = 0.0287631679606891177 ;
-weighted_squared_targets_sum = 0.0287631679606891177 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *349   )
-;
-right_leave = *341   )
-;
-left_leave = *333  ;
-right_node = *353 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *354 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 83 ;
-weights_sum = 0.436799802915736723 ;
-targets_sum = 70 ;
-weighted_targets_sum = 0.314939694881462673 ;
-weighted_squared_targets_sum = 0.314939694881462673 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.175726202193090209 0 0.175726202193090209 ] ;
-split_col = 1 ;
-split_balance = 59 ;
-split_feature_value = 0.272330343687464782 ;
-after_split_error = 0.141013965640128303 ;
-missing_node = *0 ;
-missing_leave = *355 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *356 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00975719306829215052 ;
-targets_sum = 0 ;
-weighted_targets_sum = 9.21571846612678769e-18 ;
-weighted_squared_targets_sum = 9.21571846612678769e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *357 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 82 ;
-weights_sum = 0.427042609847444965 ;
-targets_sum = 70 ;
-weighted_targets_sum = 0.314939694881463006 ;
-weighted_squared_targets_sum = 0.314939694881463006 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *354   )
-;
-priority_queue = *358 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *348  *353  *343  *335  ]  )
-;
-first_leave = *330  ;
-split_cols = 3 [ 0 1 3 ] ;
-split_values = 3 [ 0.556206773297698298 0.342206108309510149 0.429657342218349281 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*359 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *360 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *361 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *362 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000022 ;
-targets_sum = 143 ;
-weighted_targets_sum = 0.43045380638063907 ;
-weighted_squared_targets_sum = 0.43045380638063907 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.490326653906116794 0 0.490326653906116794 ] ;
-split_col = 2 ;
-split_balance = 72 ;
-split_feature_value = 0.00468965205373939042 ;
-after_split_error = 0.451357995059023742 ;
-missing_node = *0 ;
-missing_leave = *363 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *364 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *365 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 64 ;
-weights_sum = 0.56695653307663807 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.174884078978449986 ;
-weighted_squared_targets_sum = 0.174884078978449986 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.241878260598555217 0 0.241878260598555217 ] ;
-split_col = 2 ;
-split_balance = 60 ;
-split_feature_value = 0.00298229498924867942 ;
-after_split_error = 0.202259529121453174 ;
-missing_node = *0 ;
-missing_leave = *366 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *367 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *368 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 62 ;
-weights_sum = 0.414680026810623992 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.174884078978449986 ;
-weighted_squared_targets_sum = 0.174884078978449986 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.202259529121453174 0 0.202259529121453174 ] ;
-split_col = 2 ;
-split_balance = 50 ;
-split_feature_value = 0.00124141380660278133 ;
-after_split_error = 0.139174612029951961 ;
-missing_node = *0 ;
-missing_leave = *369 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *370 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.018653033765306741 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.38777878078144568e-17 ;
-weighted_squared_targets_sum = -1.38777878078144568e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *371 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 61 ;
-weights_sum = 0.396026993045317244 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.174884078978449986 ;
-weighted_squared_targets_sum = 0.174884078978449986 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *368  ;
-right_node = *372 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *373 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.152276506266014106 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.503622972889967047 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *374 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *375 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0604699175539979983 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *376 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0918065887120161073 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *373   )
-;
-left_leave = *365  ;
-right_node = *377 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *378 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 136 ;
-weights_sum = 0.433043466923362541 ;
-targets_sum = 131 ;
-weighted_targets_sum = 0.25556972740218914 ;
-weighted_squared_targets_sum = 0.25556972740218914 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.209479734460468858 0 0.209479734460468858 ] ;
-split_col = 0 ;
-split_balance = 20 ;
-split_feature_value = 0.498163758700666093 ;
-after_split_error = 0.163975871224280589 ;
-missing_node = *0 ;
-missing_leave = *379 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *380 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *381 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 58 ;
-weights_sum = 0.103183666511191624 ;
-targets_sum = 58 ;
-weighted_targets_sum = 0.103183666511191624 ;
-weighted_squared_targets_sum = 0.103183666511191624 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.341824886709230169 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *382 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *383 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0295918401082008453 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0295918401082008453 ;
-weighted_squared_targets_sum = 0.0295918401082008453 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *384 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 57 ;
-weights_sum = 0.0735918264029907648 ;
-targets_sum = 57 ;
-weighted_targets_sum = 0.0735918264029907648 ;
-weighted_squared_targets_sum = 0.0735918264029907648 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *381  ;
-right_node = *385 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *386 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 78 ;
-weights_sum = 0.329859800412171167 ;
-targets_sum = 73 ;
-weighted_targets_sum = 0.152386060890997432 ;
-weighted_squared_targets_sum = 0.152386060890997432 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.16397587122428095 0 0.16397587122428095 ] ;
-split_col = 3 ;
-split_balance = 74 ;
-split_feature_value = 0.502522189560202448 ;
-after_split_error = 0.129401693962108599 ;
-missing_node = *0 ;
-missing_leave = *387 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *388 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000349819418088731515 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000349819418088741924 ;
-weighted_squared_targets_sum = 0.000349819418088741924 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *389 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 77 ;
-weights_sum = 0.329509980994081686 ;
-targets_sum = 72 ;
-weighted_targets_sum = 0.152036241472908812 ;
-weighted_squared_targets_sum = 0.152036241472908812 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *386   )
-;
-right_leave = *378   )
-;
-priority_queue = *390 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *367  *380  *385  *372  ]  )
-;
-first_leave = *362  ;
-split_cols = 3 [ 2 0 2 ] ;
-split_values = 3 [ 0.00468965205373939042 0.498163758700666093 0.00298229498924867942 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
 ] ;
-voting_weights = 12 [ 1.47221948958322013 1.16003912522078267 0.641814808237945456 0.884150884771987666 0.642974871060007169 0.551226131392252627 0.552637175041885365 0.627805490448808512 0.503649765393419879 0.292908770120374085 0.518978458401263443 0.360278953433196047 ] ;
-sum_voting_weights = 8.20868392310514317 ;
-initial_sum_weights = 200 ;
-example_weights = 200 [ 0.0107210847922966195 0.000444852046205364597 0.000400878651114653479 0.000444852046205364597 0.000783250216693656275 0.000400878651114653479 0.000444852046205364597 0.0155121670150422022 0.000141982221496269483 0.00156142278670138657 0.000277409648641513277 0.000534450558532895722 0.000444852046205364597 0.000444852046205364597 0.0183768887024829339 0.000444852046205364597 0.00152431698991743725 0.000444852046205364597 0.000444852046205364597 0.0342152206303120571 0.000444852046205364597 0.000291855774090214878 0.000444852046205364597 0.000444852046205364597 0.000458701252646065362 0.000444852046205364597 0.00564938782923639967 0.000141982221496269483 0.0126053372449833178 0.00150505870040525342 0.000444852046205364597 0.00160952034070969323 0.00160952034070969323 0.000277409648641513277 0.00445194603151248623 0.000444852046205364597 0.000141982221496269483 0.000141982221496269483 0.0128804392476061045 0.000277409648641513277 0.000444852046205364597!
  0.0289620228084313154 0.000216411965650798458 0.000277409648641513277 0.0249194539764500851 0.000277409648641513277 0.000277409648641513277 0.00160952034070969323 0.000458701252646065362 0.000277409648641513277 0.00171053510157377414 0.0352691750340703991 0.000444852046205364597 0.00370641536237075473 0.000223149338253895665 0.000444852046205364597 0.000141982221496269483 0.000141982221496269483 0.00234950638321515873 0.00026008277447326782 0.00026008277447326782 0.000388775237040830544 0.0219938506241258089 0.00162586792396260242 0.00445194603151248623 0.000141982221496269483 0.000444852046205364597 0.0203124271385637344 0.000277409648641513277 0.000277409648641513277 0.00160952034070969323 0.000277409648641513277 0.000141982221496269483 0.0104372732594916959 0.000783250216693656275 0.0128804392476061045 0.000444852046205364597 0.000400878651114653479 0.00185433460581665185 0.000513706243382991445 0.00482959765650080395 0.00460351795648042232 0.00185433460581665185 0.0018!
 5433460581665185 0.000400878651114653479 0.0004587012526460653!
 62 0.000
400878651114653479 0.000277409648641513277 0.0107210847922966195 0.000141982221496269483 0.000277409648641513277 0.000783250216693656275 0.000223149338253895665 0.000783250216693656275 0.000534450558532895722 0.000277409648641513277 0.000444852046205364597 0.000444852046205364597 0.00564938782923639967 0.000444852046205364597 0.024098975020124206 0.00160952034070969323 0.0153835192099978247 0.000400878651114653479 0.000141982221496269483 0.00370641536237075473 0.000388775237040830544 0.000444852046205364597 0.000444852046205364597 0.000277409648641513277 0.000141982221496269483 0.0388996079922467736 0.0138636879903672253 0.000444852046205364597 0.000444852046205364597 0.000444852046205364597 0.000444852046205364597 0.00370641536237075473 0.000141982221496269483 0.000388775237040830544 0.000444852046205364597 0.000444852046205364597 0.000444852046205364597 0.00185433460581665185 0.000141982221496269483 0.000216411965650798458 0.0377182269229495437 0.00507753272439342464 0.000!
 141982221496269483 0.000141982221496269483 0.00160952034070969323 0.00160952034070969323 0.000444852046205364597 0.000400878651114653479 0.000759602158435389329 0.000400878651114653479 0.00160952034070969323 0.00234950638321515873 0.000141982221496269483 0.000141982221496269483 0.000277409648641513277 0.000400878651114653479 0.000277409648641513277 0.000400878651114653479 0.00160952034070969323 0.00459054474130646695 0.000444852046205364597 0.000444852046205364597 0.0352691750340703991 0.0155121670150422022 0.0305100172191348808 0.00509094875908995374 0.000223149338253895665 0.068234364306498721 0.000223149338253895665 0.00482959765650080395 0.00564938782923639967 0.00152431698991743725 0.0452101131499583614 0.000277409648641513277 0.0241670804860386786 0.000400878651114653479 0.000400878651114653479 0.0388996079922467736 0.000444852046205364597 0.00160952034070969323 0.000388775237040830544 0.00551513524977442857 0.000534450558532895722 0.0376308173372357849 0.000783250216!
 693656275 0.000141982221496269483 0.000444852046205364597 0.00!
 15243169
8991743725 0.00564938782923639967 0.000277409648641513277 0.000444852046205364597 0.000444852046205364597 0.0241670804860386786 0.000388775237040830544 0.000141982221496269483 0.00459054474130646695 0.00459054474130646695 0.000277409648641513277 0.000783250216693656275 0.000388775237040830544 0.00171053510157377414 0.000277409648641513277 0.000458701252646065362 0.000141982221496269483 0.00150505870040525342 0.000216411965650798458 0.00289235883362637782 0.000444852046205364597 0.000141982221496269483 0.00185433460581665185 0.044943684781780735 0.000277409648641513277 0.0432685162660981712 0.0138636879903672253 ] ;
-learners_error = 12 [ 0.0499999999999999958 0.0894736842105261221 0.216933015980959626 0.145753647620590565 0.21653914793577192 0.24928069626397617 0.248752945086009536 0.221730362614801613 0.267508669379056652 0.357595079156143969 0.261544400526413423 0.327270139869447474 ] ;
-weak_learner_template = *391 ->RegressionTree(
+voting_weights = 1 [ 1.94591014905531323 ] ;
+sum_voting_weights = 1.94591014905531323 ;
+initial_sum_weights = 1 ;
+example_weights = 150 [ 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.0034013605442176956!
 2 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562!
  0.00340136054421769562 0.00340136054421769562 0.0034013605442!
 1769562 
0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00!
 340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 ] ;
+learners_error = 1 [ 0.0200000000000000004 ] ;
+weak_learner_template = *39 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -4965,7 +653,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *392 ->RegressionTreeLeave(
+leave_template = *40 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -4990,7 +678,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 1 ;
+weightsize = 0 ;
 forget_when_training_set_changes = 1 ;
 nstages = 4 ;
 report_progress = 1 ;
@@ -5010,17 +698,17 @@
 save_often = 0 ;
 compute_training_error = 0 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 1 ;
+modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 12 ;
-n_examples = 200 ;
+stage = 1 ;
+n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 1 ;
+weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 12 ;
+nstages = 1 ;
 report_progress = 1 ;
 verbosity = 2 ;
 nservers = 0 ;
@@ -5028,8 +716,8 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
-learner2 = *393 ->AdaBoost(
-weak_learners = 12 [ *394 ->RegressionTree(
+learner2 = *41 ->AdaBoost(
+weak_learners = 1 [ *42 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
@@ -5037,7 +725,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *395 ->RegressionTreeLeave(
+leave_template = *43 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -5049,28 +737,28 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-root = *396 ->RegressionTreeNode(
+root = *44 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *397 ->RegressionTreeLeave(
+leave = *45 ->RegressionTreeLeave(
 id = 1 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000067 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.460000000000000298 ;
-weighted_squared_targets_sum = 0.460000000000000298 ;
+length = 150 ;
+weights_sum = 1.00000000000000244 ;
+targets_sum = 74 ;
+weighted_targets_sum = 0.49333333333333268 ;
+weighted_squared_targets_sum = 0.49333333333333268 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.496800000000000352 0 0.496800000000000352 ] ;
+leave_error = 3 [ 0.499911111111112305 0 0.499911111111112305 ] ;
 split_col = 2 ;
-split_balance = 14 ;
-split_feature_value = 0.883141897664061037 ;
-after_split_error = 0.171677218370013096 ;
+split_balance = 24 ;
+split_feature_value = 0.991025168386145405 ;
+after_split_error = 0.173253056011676648 ;
 missing_node = *0 ;
-missing_leave = *398 ->RegressionTreeLeave(
+missing_leave = *46 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -5082,28 +770,28 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *399 ->RegressionTreeNode(
+left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *400 ->RegressionTreeLeave(
+leave = *48 ->RegressionTreeLeave(
 id = 3 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 107 ;
-weights_sum = 0.535000000000000364 ;
-targets_sum = 9 ;
-weighted_targets_sum = 0.0449999999999999983 ;
-weighted_squared_targets_sum = 0.0449999999999999983 ;
+length = 87 ;
+weights_sum = 0.579999999999999849 ;
+targets_sum = 13 ;
+weighted_targets_sum = 0.0866666666666666696 ;
+weighted_squared_targets_sum = 0.0866666666666666696 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0824299065420560778 0 0.0824299065420560778 ] ;
-split_col = 2 ;
-split_balance = 65 ;
-split_feature_value = 0.12373287907043895 ;
-after_split_error = 0.0662015503875969108 ;
+leave_error = 3 [ 0.147432950191570877 0 0.147432950191570877 ] ;
+split_col = 1 ;
+split_balance = 31 ;
+split_feature_value = 0.482293993618237549 ;
+after_split_error = 0.104535916061339731 ;
 missing_node = *0 ;
-missing_leave = *401 ->RegressionTreeLeave(
+missing_leave = *49 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -5115,185 +803,28 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *402 ->RegressionTreeNode(
+left_node = *50 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *403 ->RegressionTreeLeave(
+leave = *51 ->RegressionTreeLeave(
 id = 6 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 86 ;
-weights_sum = 0.430000000000000271 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0100000000000000002 ;
-weighted_squared_targets_sum = 0.0100000000000000002 ;
+length = 59 ;
+weights_sum = 0.393333333333332869 ;
+targets_sum = 1 ;
+weighted_targets_sum = 0.00666666666666666709 ;
+weighted_squared_targets_sum = 0.00666666666666666709 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0195348837209302316 0 0.0195348837209302316 ] ;
-split_col = 4 ;
-split_balance = 82 ;
-split_feature_value = 0.999999999999998335 ;
-after_split_error = 0.0148809523809523801 ;
-missing_node = *0 ;
-missing_leave = *404 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *405 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00499999999999995674 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *406 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 85 ;
-weights_sum = 0.425000000000000266 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0100000000000000002 ;
-weighted_squared_targets_sum = 0.0100000000000000002 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *403  ;
-right_node = *407 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *408 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 21 ;
-weights_sum = 0.105000000000000024 ;
-targets_sum = 7 ;
-weighted_targets_sum = 0.0350000000000000033 ;
-weighted_squared_targets_sum = 0.0350000000000000033 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0466666666666666757 0 0.0466666666666666757 ] ;
+leave_error = 3 [ 0.013107344632768362 0 0.013107344632768362 ] ;
 split_col = 3 ;
-split_balance = 17 ;
-split_feature_value = 0.14842107991859671 ;
-after_split_error = 0.0368421052631578982 ;
+split_balance = 53 ;
+split_feature_value = 0.924226804347039965 ;
+after_split_error = 0.00888888888888889062 ;
 missing_node = *0 ;
-missing_leave = *409 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *410 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00499999999999999837 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *411 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.100000000000000019 ;
-targets_sum = 7 ;
-weighted_targets_sum = 0.0350000000000000033 ;
-weighted_squared_targets_sum = 0.0350000000000000033 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *408   )
-;
-left_leave = *400  ;
-right_node = *412 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *413 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 93 ;
-weights_sum = 0.465000000000000302 ;
-targets_sum = 83 ;
-weighted_targets_sum = 0.415000000000000258 ;
-weighted_squared_targets_sum = 0.415000000000000258 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0892473118279570876 0 0.0892473118279570876 ] ;
-split_col = 2 ;
-split_balance = 39 ;
-split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.0629629629629629844 ;
-missing_node = *0 ;
-missing_leave = *414 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *415 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *416 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 27 ;
-weights_sum = 0.135000000000000037 ;
-targets_sum = 17 ;
-weighted_targets_sum = 0.0850000000000000061 ;
-weighted_squared_targets_sum = 0.0850000000000000061 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0629629629629629844 0 0.0629629629629629844 ] ;
-split_col = 3 ;
-split_balance = 13 ;
-split_feature_value = 0.496260749748818786 ;
-after_split_error = 0.0500000000000000097 ;
-missing_node = *0 ;
-missing_leave = *417 ->RegressionTreeLeave(
+missing_leave = *52 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -5306,839 +837,55 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *418 ->RegressionTreeLeave(
+left_leave = *53 ->RegressionTreeLeave(
 id = 12 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00499999999999999837 ;
+weights_sum = 0.00666666666666668271 ;
 targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *419 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 26 ;
-weights_sum = 0.130000000000000032 ;
-targets_sum = 17 ;
-weighted_targets_sum = 0.0850000000000000061 ;
-weighted_squared_targets_sum = 0.0850000000000000061 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *416  ;
-right_node = *420 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *421 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.330000000000000182 ;
-targets_sum = 66 ;
-weighted_targets_sum = 0.330000000000000182 ;
-weighted_squared_targets_sum = 0.330000000000000182 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.890956512155314684 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *422 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
 weighted_targets_sum = 0 ;
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *0 ;
-left_leave = *423 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00499999999999999837 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00499999999999999837 ;
-weighted_squared_targets_sum = 0.00499999999999999837 ;
-loss_function_factor = 2  )
-;
 right_node = *0 ;
-right_leave = *424 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 65 ;
-weights_sum = 0.325000000000000178 ;
-targets_sum = 65 ;
-weighted_targets_sum = 0.325000000000000178 ;
-weighted_squared_targets_sum = 0.325000000000000178 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *421   )
-;
-right_leave = *413   )
-;
-priority_queue = *425 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *415  *407  *402  *420  ]  )
-;
-first_leave = *397  ;
-split_cols = 3 [ 2 2 2 ] ;
-split_values = 3 [ 0.883141897664061037 0.997650553369808346 0.12373287907043895 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*426 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *427 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *428 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *429 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000333 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.466123873218958873 ;
-weighted_squared_targets_sum = 0.466123873218958873 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.497704816068631095 0 0.497704816068631095 ] ;
-split_col = 1 ;
-split_balance = 28 ;
-split_feature_value = 0.479480044756096346 ;
-after_split_error = 0.354065326145222459 ;
-missing_node = *0 ;
-missing_leave = *430 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *431 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *432 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 86 ;
-weights_sum = 0.355335853445769012 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0373655132305902998 ;
-weighted_squared_targets_sum = 0.0373655132305902998 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0668726492923903265 0 0.0668726492923903265 ] ;
-split_col = 2 ;
-split_balance = 66 ;
-split_feature_value = 0.449086344763339085 ;
-after_split_error = 0.0555251526606571899 ;
-missing_node = *0 ;
-missing_leave = *433 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *434 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00276243093922654221 ;
-targets_sum = 0 ;
-weighted_targets_sum = 8.67361737988403547e-19 ;
-weighted_squared_targets_sum = 8.67361737988403547e-19 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *435 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 85 ;
-weights_sum = 0.352573422506542222 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0373655132305902998 ;
-weighted_squared_targets_sum = 0.0373655132305902998 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *432  ;
-right_node = *436 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *437 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 114 ;
-weights_sum = 0.644664146554232986 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.428758359988368309 ;
-weighted_squared_targets_sum = 0.428758359988368309 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.287192676852833673 0 0.287192676852833673 ] ;
-split_col = 2 ;
-split_balance = 14 ;
-split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.232544890067519716 ;
-missing_node = *0 ;
-missing_leave = *438 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *439 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *440 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 50 ;
-weights_sum = 0.467868566443733702 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.251962779877871412 ;
-weighted_squared_targets_sum = 0.251962779877871412 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.232544890067519661 0 0.232544890067519661 ] ;
-split_col = 3 ;
-split_balance = 18 ;
-split_feature_value = 0.496260749748818786 ;
-after_split_error = 0.124392796099083422 ;
-missing_node = *0 ;
-missing_leave = *441 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *442 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *443 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 34 ;
-weights_sum = 0.282349520209363103 ;
-targets_sum = 15 ;
-weighted_targets_sum = 0.229863332364059342 ;
-weighted_squared_targets_sum = 0.229863332364059342 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0854589732064111596 0 0.0854589732064111596 ] ;
-split_col = 2 ;
-split_balance = 12 ;
-split_feature_value = 0.0707071480833099397 ;
-after_split_error = 0.0712656098953515088 ;
-missing_node = *0 ;
-missing_leave = *444 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *445 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.026315789473684216 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0263157894736842125 ;
-weighted_squared_targets_sum = 0.0263157894736842125 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *446 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 33 ;
-weights_sum = 0.256033730735679033 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.203547542890375188 ;
-weighted_squared_targets_sum = 0.203547542890375188 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *443  ;
-right_node = *447 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *448 ->RegressionTreeLeave(
+right_leave = *54 ->RegressionTreeLeave(
 id = 13 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 16 ;
-weights_sum = 0.185519046234370516 ;
-targets_sum = 8 ;
-weighted_targets_sum = 0.0220994475138121607 ;
-weighted_squared_targets_sum = 0.0220994475138121607 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0389338228926722069 0 0.0389338228926722069 ] ;
-split_col = 1 ;
-split_balance = 12 ;
-split_feature_value = 0.502718698860307178 ;
-after_split_error = 0.0300966627692143031 ;
-missing_node = *0 ;
-missing_leave = *449 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *450 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00276243093922652313 ;
+length = 58 ;
+weights_sum = 0.386666666666666214 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00276243093922651792 ;
-weighted_squared_targets_sum = 0.00276243093922651792 ;
+weighted_targets_sum = 0.00666666666666666709 ;
+weighted_squared_targets_sum = 0.00666666666666666709 ;
 loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *451 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 15 ;
-weights_sum = 0.182756615295143976 ;
-targets_sum = 7 ;
-weighted_targets_sum = 0.0193370165745856415 ;
-weighted_squared_targets_sum = 0.0193370165745856415 ;
-loss_function_factor = 2  )
  )
 ;
-right_leave = *448   )
-;
-left_leave = *440  ;
-right_node = *452 ->RegressionTreeNode(
+left_leave = *51  ;
+right_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *453 ->RegressionTreeLeave(
-id = 10 ;
+leave = *56 ->RegressionTreeLeave(
+id = 7 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 64 ;
-weights_sum = 0.176795580110497258 ;
-targets_sum = 64 ;
-weighted_targets_sum = 0.176795580110497258 ;
-weighted_squared_targets_sum = 0.176795580110497258 ;
+length = 28 ;
+weights_sum = 0.186666666666666564 ;
+targets_sum = 12 ;
+weighted_targets_sum = 0.0800000000000000017 ;
+weighted_squared_targets_sum = 0.0800000000000000017 ;
 loss_function_factor = 2  )
 ;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.887813788792009673 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *454 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *455 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00276243093922651792 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00276243093922651792 ;
-weighted_squared_targets_sum = 0.00276243093922651792 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *456 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 63 ;
-weights_sum = 0.174033149171270746 ;
-targets_sum = 63 ;
-weighted_targets_sum = 0.174033149171270746 ;
-weighted_squared_targets_sum = 0.174033149171270746 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *453   )
-;
-right_leave = *437   )
-;
-priority_queue = *457 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *442  *447  *431  *452  ]  )
-;
-first_leave = *429  ;
-split_cols = 3 [ 1 2 3 ] ;
-split_values = 3 [ 0.479480044756096346 0.997650553369808346 0.496260749748818786 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*458 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *459 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *460 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *461 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999996669 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.494546432611265629 ;
-weighted_squared_targets_sum = 0.494546432611265629 ;
-loss_function_factor = 2  )
-;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499940517205471446 0 0.499940517205471446 ] ;
+leave_error = 3 [ 0.0914285714285713869 0 0.0914285714285713869 ] ;
 split_col = 2 ;
-split_balance = 28 ;
-split_feature_value = 0.12373287907043895 ;
-after_split_error = 0.374456126030871472 ;
-missing_node = *0 ;
-missing_leave = *462 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *463 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *464 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 86 ;
-weights_sum = 0.289669498805058434 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.029633267845448584 ;
-weighted_squared_targets_sum = 0.029633267845448584 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0532035531067966555 0 0.0532035531067966555 ] ;
-split_col = 4 ;
-split_balance = 82 ;
-split_feature_value = 0.999999999999998335 ;
-after_split_error = 0.0414247279866019683 ;
-missing_node = *0 ;
-missing_leave = *465 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *466 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00155533726260640944 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *467 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 85 ;
-weights_sum = 0.288114161542452729 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.029633267845448584 ;
-weighted_squared_targets_sum = 0.029633267845448584 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *464  ;
-right_node = *468 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *469 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 114 ;
-weights_sum = 0.710330501194937791 ;
-targets_sum = 90 ;
-weighted_targets_sum = 0.464913164765817233 ;
-weighted_squared_targets_sum = 0.464913164765817233 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.321252572924072166 0 0.321252572924072166 ] ;
-split_col = 2 ;
 split_balance = 18 ;
-split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.285312501485439296 ;
+split_feature_value = 0.891579732096156263 ;
+after_split_error = 0.0802318840579709924 ;
 missing_node = *0 ;
-missing_leave = *470 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *471 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *472 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.586113591712805415 ;
-targets_sum = 24 ;
-weighted_targets_sum = 0.340696255283681304 ;
-weighted_squared_targets_sum = 0.340696255283681304 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.285312501485439296 0 0.285312501485439296 ] ;
-split_col = 2 ;
-split_balance = 22 ;
-split_feature_value = 0.627448174543832948 ;
-after_split_error = 0.242168283091306363 ;
-missing_node = *0 ;
-missing_leave = *473 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *474 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *475 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 13 ;
-weights_sum = 0.234849973209501545 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.191615637146088869 ;
-weighted_squared_targets_sum = 0.191615637146088869 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0705503580704158195 0 0.0705503580704158195 ] ;
-split_col = 2 ;
-split_balance = 7 ;
-split_feature_value = 0.442618283127769407 ;
-after_split_error = 0.0438336932282397465 ;
-missing_node = *0 ;
-missing_leave = *476 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *477 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00155533726260643546 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *478 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 12 ;
-weights_sum = 0.233294635946895118 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.191615637146088869 ;
-weighted_squared_targets_sum = 0.191615637146088869 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *475  ;
-right_node = *479 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *480 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 35 ;
-weights_sum = 0.351263618503304009 ;
-targets_sum = 18 ;
-weighted_targets_sum = 0.149080618137592491 ;
-weighted_squared_targets_sum = 0.149080618137592491 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.171617925020890516 0 0.171617925020890516 ] ;
-split_col = 1 ;
-split_balance = 25 ;
-split_feature_value = 0.6805672568090122 ;
-after_split_error = 0.156790033037366772 ;
-missing_node = *0 ;
-missing_leave = *481 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *482 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0148166339227243232 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *483 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 34 ;
-weights_sum = 0.336446984580579589 ;
-targets_sum = 18 ;
-weighted_targets_sum = 0.149080618137592491 ;
-weighted_squared_targets_sum = 0.149080618137592491 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *480   )
-;
-left_leave = *472  ;
-right_node = *484 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *485 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.124216909482135207 ;
-targets_sum = 66 ;
-weighted_targets_sum = 0.124216909482135207 ;
-weighted_squared_targets_sum = 0.124216909482135207 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.890956512155314684 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *486 ->RegressionTreeLeave(
+missing_leave = *57 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -6150,263 +897,28 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *0 ;
-left_leave = *487 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0123376623376623314 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0123376623376623314 ;
-weighted_squared_targets_sum = 0.0123376623376623314 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *488 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 65 ;
-weights_sum = 0.111879247144472962 ;
-targets_sum = 65 ;
-weighted_targets_sum = 0.111879247144472962 ;
-weighted_squared_targets_sum = 0.111879247144472962 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *485   )
-;
-right_leave = *469   )
-;
-priority_queue = *489 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *474  *479  *463  *484  ]  )
-;
-first_leave = *461  ;
-split_cols = 3 [ 2 2 2 ] ;
-split_values = 3 [ 0.12373287907043895 0.997650553369808346 0.627448174543832948 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*490 ->RegressionTree(
+left_node = *58 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *491 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *492 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *493 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000355 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.605566360137504645 ;
-weighted_squared_targets_sum = 0.605566360137504645 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.477711487214639852 0 0.477711487214639852 ] ;
-split_col = 2 ;
-split_balance = 8 ;
-split_feature_value = 0.442618283127769407 ;
-after_split_error = 0.347257837360016919 ;
-missing_node = *0 ;
-missing_leave = *494 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *495 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *496 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 96 ;
-weights_sum = 0.359826664214057179 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0953220381361555985 ;
-weighted_squared_targets_sum = 0.0953220381361555985 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.140140365135299066 0 0.140140365135299066 ] ;
-split_col = 4 ;
-split_balance = 90 ;
-split_feature_value = 0.999999999999997558 ;
-after_split_error = 0.100440458626433402 ;
-missing_node = *0 ;
-missing_leave = *497 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *498 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *499 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 93 ;
-weights_sum = 0.308997896214625589 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0524218313759988244 ;
-weighted_squared_targets_sum = 0.0524218313759988244 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0870568205858950062 0 0.0870568205858950062 ] ;
-split_col = 0 ;
-split_balance = 27 ;
-split_feature_value = 0.313878478919910275 ;
-after_split_error = 0.0767778848771243361 ;
-missing_node = *0 ;
-missing_leave = *500 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *501 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000999507556359108696 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *502 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 92 ;
-weights_sum = 0.307998388658267952 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0524218313759988244 ;
-weighted_squared_targets_sum = 0.0524218313759988244 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *499  ;
-right_node = *503 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *504 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0508287679994313749 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0429002067601567741 ;
-weighted_squared_targets_sum = 0.0429002067601567741 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0133836380405383953 0 0.0133836380405383953 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.0434674056274751697 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *505 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *506 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *507 ->RegressionTreeLeave(
+leave = *59 ->RegressionTreeLeave(
 id = 15 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00792856123927459903 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
+length = 23 ;
+weights_sum = 0.153333333333333294 ;
+targets_sum = 8 ;
+weighted_targets_sum = 0.0533333333333333368 ;
+weighted_squared_targets_sum = 0.0533333333333333368 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
+leave_error = 3 [ 0.0695652173913043348 0 0.0695652173913043348 ] ;
+split_col = 2 ;
+split_balance = 15 ;
+split_feature_value = 0.808283414109232878 ;
+after_split_error = 0.0617543859649122839 ;
 missing_node = *0 ;
-missing_leave = *508 ->RegressionTreeLeave(
+missing_leave = *60 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -6419,1562 +931,55 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *509 ->RegressionTreeLeave(
+left_leave = *61 ->RegressionTreeLeave(
 id = 18 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *510 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *507  ;
-right_node = *511 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *512 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0429002067601567741 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0429002067601567741 ;
-weighted_squared_targets_sum = 0.0429002067601567741 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 4 ;
-split_balance = 0 ;
-split_feature_value = 0.999999999999999223 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *513 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *514 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.00952162461584205033 ;
+weights_sum = 0.00666666666666665495 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00952162461584205033 ;
-weighted_squared_targets_sum = 0.00952162461584205033 ;
+weighted_targets_sum = 0.00666666666666666189 ;
+weighted_squared_targets_sum = 0.00666666666666666189 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
-right_leave = *515 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0333785821443147238 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0333785821443147238 ;
-weighted_squared_targets_sum = 0.0333785821443147238 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *512   )
-;
-right_leave = *504   )
-;
-left_leave = *496  ;
-right_node = *516 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *517 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 104 ;
-weights_sum = 0.640173335785946263 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.510244322001350059 ;
-weighted_squared_targets_sum = 0.510244322001350059 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.20711747222471788 0 0.20711747222471788 ] ;
-split_col = 3 ;
-split_balance = 32 ;
-split_feature_value = 0.701784786855022658 ;
-after_split_error = 0.197450279487326852 ;
-missing_node = *0 ;
-missing_leave = *518 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *519 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0095216246158420486 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *520 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 103 ;
-weights_sum = 0.630651711170102658 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.510244322001348283 ;
-weighted_squared_targets_sum = 0.510244322001348283 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *517   )
-;
-priority_queue = *521 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 3 ;
-nodes = 4 [ *498  *516  *511  *0 ]  )
-;
-first_leave = *493  ;
-split_cols = 3 [ 2 4 3 ] ;
-split_values = 3 [ 0.442618283127769407 0.999999999999997558 0.0434674056274751697 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*522 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *523 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *524 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *525 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000311 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.481991913175921327 ;
-weighted_squared_targets_sum = 0.481991913175921327 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499351417617874327 0 0.499351417617874327 ] ;
-split_col = 1 ;
-split_balance = 104 ;
-split_feature_value = 0.6805672568090122 ;
-after_split_error = 0.467958413356260372 ;
-missing_node = *0 ;
-missing_leave = *526 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *527 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *528 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 152 ;
-weights_sum = 0.944736246107295674 ;
-targets_sum = 44 ;
-weighted_targets_sum = 0.426728159283215502 ;
-weighted_squared_targets_sum = 0.426728159283215502 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.467958413356260372 0 0.467958413356260372 ] ;
-split_col = 1 ;
-split_balance = 140 ;
-split_feature_value = 0.662011169718969006 ;
-after_split_error = 0.436756428867164981 ;
-missing_node = *0 ;
-missing_leave = *529 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *530 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *531 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 146 ;
-weights_sum = 0.869077754543573944 ;
-targets_sum = 42 ;
-weighted_targets_sum = 0.425505743103313239 ;
-weighted_squared_targets_sum = 0.425505743103313239 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.43435109772621916 0 0.43435109772621916 ] ;
-split_col = 4 ;
-split_balance = 12 ;
-split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.405394917300886115 ;
-missing_node = *0 ;
-missing_leave = *532 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *533 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *534 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 67 ;
-weights_sum = 0.073747726070205466 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00484838832911889048 ;
-weighted_squared_targets_sum = 0.00484838832911889048 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.00905928257828314022 0 0.00905928257828314022 ] ;
-split_col = 3 ;
-split_balance = 65 ;
-split_feature_value = 0.987251230798804613 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *535 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *536 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000611208089951136837 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *537 ->RegressionTreeLeave(
+right_leave = *62 ->RegressionTreeLeave(
 id = 19 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.0731365179802543619 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00484838832911889048 ;
-weighted_squared_targets_sum = 0.00484838832911889048 ;
+length = 22 ;
+weights_sum = 0.14666666666666664 ;
+targets_sum = 7 ;
+weighted_targets_sum = 0.0466666666666666688 ;
+weighted_squared_targets_sum = 0.0466666666666666688 ;
 loss_function_factor = 2  )
  )
 ;
-left_leave = *534  ;
-right_node = *538 ->RegressionTreeNode(
+left_leave = *59  ;
+right_node = *63 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *539 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 79 ;
-weights_sum = 0.795330028473368422 ;
-targets_sum = 41 ;
-weighted_targets_sum = 0.420657354774194359 ;
-weighted_squared_targets_sum = 0.420657354774194359 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.39633563472260358 0 0.39633563472260358 ] ;
-split_col = 1 ;
-split_balance = 71 ;
-split_feature_value = 0.408499973451678211 ;
-after_split_error = 0.357693407009168129 ;
-missing_node = *0 ;
-missing_leave = *540 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *541 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0169962936620895629 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0169962936620895316 ;
-weighted_squared_targets_sum = 0.0169962936620895316 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *542 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 78 ;
-weights_sum = 0.778333734811277878 ;
-targets_sum = 40 ;
-weighted_targets_sum = 0.403661061112105091 ;
-weighted_squared_targets_sum = 0.403661061112105091 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *539   )
-;
-left_leave = *531  ;
-right_node = *543 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *544 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0756584915637215361 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.00122241617990227541 ;
-weighted_squared_targets_sum = 0.00122241617990227541 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.00240533114094590265 0 0.00240533114094590265 ] ;
-split_col = 3 ;
-split_balance = 2 ;
-split_feature_value = 0.738796073466493564 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *545 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *546 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0261079804907304314 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *547 ->RegressionTreeLeave(
+leave = *64 ->RegressionTreeLeave(
 id = 16 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 5 ;
-weights_sum = 0.0495505110729911186 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.00122241617990227541 ;
-weighted_squared_targets_sum = 0.00122241617990227541 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *544   )
-;
-left_leave = *528  ;
-right_node = *548 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *549 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.055263753892705908 ;
-targets_sum = 48 ;
-weighted_targets_sum = 0.055263753892705908 ;
-weighted_squared_targets_sum = 0.055263753892705908 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.752591011543306765 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *550 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *551 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000611208089951137705 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.000611208089951137705 ;
-weighted_squared_targets_sum = 0.000611208089951137705 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *552 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 47 ;
-weights_sum = 0.0546525458027547484 ;
-targets_sum = 47 ;
-weighted_targets_sum = 0.0546525458027547484 ;
-weighted_squared_targets_sum = 0.0546525458027547484 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *549   )
-;
-priority_queue = *553 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *538  *533  *543  *548  ]  )
-;
-first_leave = *525  ;
-split_cols = 3 [ 1 1 4 ] ;
-split_values = 3 [ 0.6805672568090122 0.662011169718969006 1.54709578481515564e-13 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*554 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *555 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *556 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *557 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000222 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.392240446759400896 ;
-weighted_squared_targets_sum = 0.392240446759400896 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.476775757370773656 0 0.476775757370773656 ] ;
-split_col = 1 ;
-split_balance = 104 ;
-split_feature_value = 0.6805672568090122 ;
-after_split_error = 0.44227268942828013 ;
-missing_node = *0 ;
-missing_leave = *558 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *559 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *560 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 152 ;
-weights_sum = 0.955378948829799124 ;
-targets_sum = 44 ;
-weighted_targets_sum = 0.347619395589199798 ;
-weighted_squared_targets_sum = 0.347619395589199798 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.44227268942828013 0 0.44227268942828013 ] ;
-split_col = 3 ;
-split_balance = 86 ;
-split_feature_value = 0.119889252557545484 ;
-after_split_error = 0.411853948410690174 ;
-missing_node = *0 ;
-missing_leave = *561 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *562 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *563 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 33 ;
-weights_sum = 0.133519136616596423 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0903779594501199074 ;
-weighted_squared_targets_sum = 0.0903779594501199074 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0584037862943701247 0 0.0584037862943701247 ] ;
-split_col = 3 ;
-split_balance = 31 ;
-split_feature_value = 0.119702025390566957 ;
-after_split_error = 0.0238499860947938014 ;
-missing_node = *0 ;
-missing_leave = *564 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *565 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *566 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 32 ;
-weights_sum = 0.0596216744560958689 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.016480497289619353 ;
-weighted_squared_targets_sum = 0.016480497289619353 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0238499860947938014 0 0.0238499860947938014 ] ;
-split_col = 2 ;
-split_balance = 30 ;
-split_feature_value = 0.0544487023778579271 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *567 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *568 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000493501536473626512 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *569 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 31 ;
-weights_sum = 0.0591281729196222322 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.016480497289619353 ;
-weighted_squared_targets_sum = 0.016480497289619353 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *566  ;
-right_node = *570 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *571 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0738974621605005544 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0738974621605005544 ;
-weighted_squared_targets_sum = 0.0738974621605005544 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *572 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *573 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *574 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *571   )
-;
-left_leave = *563  ;
-right_node = *575 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *576 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 119 ;
-weights_sum = 0.821859812213203367 ;
-targets_sum = 42 ;
-weighted_targets_sum = 0.257241436139079571 ;
-weighted_squared_targets_sum = 0.257241436139079571 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.353450162116320021 0 0.353450162116320021 ] ;
-split_col = 3 ;
-split_balance = 37 ;
-split_feature_value = 0.711118874226568165 ;
-after_split_error = 0.327041816880288938 ;
-missing_node = *0 ;
-missing_leave = *577 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *578 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000493501536473595287 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *579 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 118 ;
-weights_sum = 0.82136631067673016 ;
-targets_sum = 42 ;
-weighted_targets_sum = 0.257241436139079516 ;
-weighted_squared_targets_sum = 0.257241436139079516 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *576   )
-;
-left_leave = *560  ;
-right_node = *580 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *581 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.044621051170201001 ;
-targets_sum = 48 ;
-weighted_targets_sum = 0.044621051170201001 ;
-weighted_squared_targets_sum = 0.044621051170201001 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.752591011543306765 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *582 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *583 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00049350153647362391 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00049350153647362391 ;
-weighted_squared_targets_sum = 0.00049350153647362391 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *584 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 47 ;
-weights_sum = 0.044127549633727399 ;
-targets_sum = 47 ;
-weighted_targets_sum = 0.044127549633727399 ;
-weighted_squared_targets_sum = 0.044127549633727399 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *581   )
-;
-priority_queue = *585 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 3 ;
-nodes = 4 [ *575  *580  *565  *0 ]  )
-;
-first_leave = *557  ;
-split_cols = 3 [ 1 3 3 ] ;
-split_values = 3 [ 0.6805672568090122 0.119889252557545484 0.119702025390566957 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*586 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *587 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *588 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *589 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999997446 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.581593069366817539 ;
-weighted_squared_targets_sum = 0.581593069366817539 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.486685142062601717 0 0.486685142062601717 ] ;
-split_col = 3 ;
-split_balance = 62 ;
-split_feature_value = 0.711118874226568165 ;
-after_split_error = 0.4668341465742194 ;
-missing_node = *0 ;
-missing_leave = *590 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *591 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *592 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 131 ;
-weights_sum = 0.499014213036646614 ;
-targets_sum = 32 ;
-weighted_targets_sum = 0.240409908482151852 ;
-weighted_squared_targets_sum = 0.240409908482151852 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.249175416518530701 0 0.249175416518530701 ] ;
-split_col = 2 ;
-split_balance = 51 ;
-split_feature_value = 0.216049487742465185 ;
-after_split_error = 0.222023797905294529 ;
-missing_node = *0 ;
-missing_leave = *593 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *594 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *595 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 91 ;
-weights_sum = 0.260424623188368565 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.166579008471686968 ;
-weighted_squared_targets_sum = 0.166579008471686968 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.120055540505579844 0 0.120055540505579844 ] ;
-split_col = 1 ;
-split_balance = 57 ;
-split_feature_value = 0.488019810363248041 ;
-after_split_error = 0.0902535175820549929 ;
-missing_node = *0 ;
-missing_leave = *596 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *597 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *598 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 74 ;
-weights_sum = 0.0319515719573615839 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.174966118866960257 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *599 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *600 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000339746964136894815 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *601 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 73 ;
-weights_sum = 0.0316118249932247214 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *598  ;
-right_node = *602 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *603 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.228473051231006974 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.166579008471686996 ;
-weighted_squared_targets_sum = 0.166579008471686996 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0902535175820549929 0 0.0902535175820549929 ] ;
-split_col = 4 ;
-split_balance = 3 ;
-split_feature_value = 0.999999858495719041 ;
-after_split_error = 0.0721715137430611886 ;
-missing_node = *0 ;
-missing_leave = *604 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *605 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0508740836064100685 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0508740836064100962 ;
-weighted_squared_targets_sum = 0.0508740836064100962 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *606 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 16 ;
-weights_sum = 0.177598967624596898 ;
+weights_sum = 0.0333333333333333329 ;
 targets_sum = 4 ;
-weighted_targets_sum = 0.11570492486527692 ;
-weighted_squared_targets_sum = 0.11570492486527692 ;
+weighted_targets_sum = 0.0266666666666666684 ;
+weighted_squared_targets_sum = 0.0266666666666666684 ;
 loss_function_factor = 2  )
- )
 ;
-right_leave = *603   )
-;
-left_leave = *595  ;
-right_node = *607 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *608 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.238589589848277717 ;
-targets_sum = 27 ;
-weighted_targets_sum = 0.0738309000104650087 ;
-weighted_squared_targets_sum = 0.0738309000104650087 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.101968257399714685 0 0.101968257399714685 ] ;
-split_col = 1 ;
-split_balance = 0 ;
-split_feature_value = 0.6805672568090122 ;
-after_split_error = 0.0797705176182461895 ;
-missing_node = *0 ;
-missing_leave = *609 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *610 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00193709325952056127 ;
-targets_sum = 0 ;
-weighted_targets_sum = 3.46944695195361419e-18 ;
-weighted_squared_targets_sum = 3.46944695195361419e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *611 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 39 ;
-weights_sum = 0.236652496588757283 ;
-targets_sum = 27 ;
-weighted_targets_sum = 0.0738309000104649255 ;
-weighted_squared_targets_sum = 0.0738309000104649255 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *608   )
-;
-left_leave = *592  ;
-right_node = *612 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *613 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 69 ;
-weights_sum = 0.500985786963352053 ;
-targets_sum = 60 ;
-weighted_targets_sum = 0.341183160884665881 ;
-weighted_squared_targets_sum = 0.341183160884665881 ;
-loss_function_factor = 2  )
-;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.217658730055688643 0 0.217658730055688643 ] ;
+leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
 split_col = 2 ;
-split_balance = 59 ;
-split_feature_value = 0.941974890824293754 ;
-after_split_error = 0.190762293116890191 ;
-missing_node = *0 ;
-missing_leave = *614 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *615 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.014512417713875448 ;
-targets_sum = 0 ;
-weighted_targets_sum = 1.38777878078144568e-17 ;
-weighted_squared_targets_sum = 1.38777878078144568e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *616 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 68 ;
-weights_sum = 0.486473369249476373 ;
-targets_sum = 60 ;
-weighted_targets_sum = 0.341183160884665659 ;
-weighted_squared_targets_sum = 0.341183160884665659 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *613   )
-;
-priority_queue = *617 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *612  *607  *597  *602  ]  )
-;
-first_leave = *589  ;
-split_cols = 3 [ 3 2 1 ] ;
-split_values = 3 [ 0.711118874226568165 0.216049487742465185 0.488019810363248041 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*618 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *619 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *620 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *621 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999997891 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.485298425293867908 ;
-weighted_squared_targets_sum = 0.485298425293867908 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499567727402319084 0 0.499567727402319084 ] ;
-split_col = 2 ;
-split_balance = 68 ;
-split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.475282190087886691 ;
-missing_node = *0 ;
-missing_leave = *622 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *623 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *624 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 134 ;
-weights_sum = 0.956172859182146451 ;
-targets_sum = 26 ;
-weighted_targets_sum = 0.441471284476015913 ;
-weighted_squared_targets_sum = 0.441471284476015913 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.475282190087886691 0 0.475282190087886691 ] ;
-split_col = 2 ;
-split_balance = 90 ;
-split_feature_value = 0.960331052047352696 ;
-after_split_error = 0.438548801813753442 ;
-missing_node = *0 ;
-missing_leave = *625 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *626 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *627 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 112 ;
-weights_sum = 0.506925038213422607 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.300190306601266166 ;
-weighted_squared_targets_sum = 0.300190306601266166 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.24484788791062112 0 0.24484788791062112 ] ;
-split_col = 3 ;
-split_balance = 90 ;
-split_feature_value = 0.642420030204781112 ;
-after_split_error = 0.20052880731762443 ;
-missing_node = *0 ;
-missing_leave = *628 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *629 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *630 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 101 ;
-weights_sum = 0.372563674989123572 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.173845879643221707 ;
-weighted_squared_targets_sum = 0.173845879643221707 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.185451627476449721 0 0.185451627476449721 ] ;
-split_col = 1 ;
-split_balance = 97 ;
-split_feature_value = 0.689386369193649928 ;
-after_split_error = 0.172654263127876573 ;
-missing_node = *0 ;
-missing_leave = *631 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *632 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000241135741523314907 ;
-targets_sum = 0 ;
-weighted_targets_sum = 6.93889390390722838e-18 ;
-weighted_squared_targets_sum = 6.93889390390722838e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *633 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 100 ;
-weights_sum = 0.372322539247600282 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.173845879643221735 ;
-weighted_squared_targets_sum = 0.173845879643221735 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *630  ;
-right_node = *634 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *635 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 11 ;
-weights_sum = 0.134361363224298869 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.126344426958044487 ;
-weighted_squared_targets_sum = 0.126344426958044487 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0150771798411747093 0 0.0150771798411747093 ] ;
-split_col = 4 ;
 split_balance = 1 ;
-split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.00811482509180619371 ;
+split_feature_value = 0.982696507149771858 ;
+after_split_error = 0.00666666666666666709 ;
 missing_node = *0 ;
-missing_leave = *636 ->RegressionTreeLeave(
+missing_leave = *65 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -7987,904 +992,59 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *637 ->RegressionTreeLeave(
+left_leave = *66 ->RegressionTreeLeave(
 id = 21 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.000241135741523304065 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *638 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 10 ;
-weights_sum = 0.13412022748277555 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.126344426958044487 ;
-weighted_squared_targets_sum = 0.126344426958044487 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *635   )
-;
-left_leave = *627  ;
-right_node = *639 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *640 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 22 ;
-weights_sum = 0.449247820968723677 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.141280977874749913 ;
-weighted_squared_targets_sum = 0.141280977874749913 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.193700913903132377 0 0.193700913903132377 ] ;
-split_col = 4 ;
-split_balance = 14 ;
-split_feature_value = 0.500140901567539986 ;
-after_split_error = 0.15162762203233901 ;
-missing_node = *0 ;
-missing_leave = *641 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *642 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0245534076066462811 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *643 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 21 ;
-weights_sum = 0.424694413362077361 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.141280977874749913 ;
-weighted_squared_targets_sum = 0.141280977874749913 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *640   )
-;
-left_leave = *624  ;
-right_node = *644 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *645 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.0438271408178518979 ;
-targets_sum = 66 ;
-weighted_targets_sum = 0.0438271408178518979 ;
-weighted_squared_targets_sum = 0.0438271408178518979 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.890956512155314684 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *646 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *647 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00825470079355741099 ;
+weights_sum = 0.00666666666666666536 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.00825470079355741099 ;
-weighted_squared_targets_sum = 0.00825470079355741099 ;
+weighted_targets_sum = 0.00666666666666666536 ;
+weighted_squared_targets_sum = 0.00666666666666666536 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
-right_leave = *648 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 65 ;
-weights_sum = 0.0355724400242945493 ;
-targets_sum = 65 ;
-weighted_targets_sum = 0.0355724400242945493 ;
-weighted_squared_targets_sum = 0.0355724400242945493 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *645   )
-;
-priority_queue = *649 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *639  *634  *629  *644  ]  )
-;
-first_leave = *621  ;
-split_cols = 3 [ 2 2 3 ] ;
-split_values = 3 [ 0.997650553369808346 0.960331052047352696 0.642420030204781112 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*650 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *651 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *652 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *653 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000089 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.613302730417886077 ;
-weighted_squared_targets_sum = 0.613302730417886077 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.474324982559704345 0 0.474324982559704345 ] ;
-split_col = 1 ;
-split_balance = 28 ;
-split_feature_value = 0.479480044756096346 ;
-after_split_error = 0.437989797099442746 ;
-missing_node = *0 ;
-missing_leave = *654 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *655 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *656 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 86 ;
-weights_sum = 0.206039904666722368 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0718488715484882495 ;
-weighted_squared_targets_sum = 0.0718488715484882495 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0935884174191052853 0 0.0935884174191052853 ] ;
-split_col = 1 ;
-split_balance = 46 ;
-split_feature_value = 0.408499973451678211 ;
-after_split_error = 0.0525521160654311703 ;
-missing_node = *0 ;
-missing_leave = *657 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *658 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *659 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.102064478050155519 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0680996870826439948 ;
-weighted_squared_targets_sum = 0.0680996870826439948 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0453241260995489587 0 0.0453241260995489587 ] ;
-split_col = 4 ;
-split_balance = 58 ;
-split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.0109025982794831913 ;
-missing_node = *0 ;
-missing_leave = *660 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *661 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *662 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 62 ;
-weights_sum = 0.0397728989845104811 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00609782455841582742 ;
-weighted_squared_targets_sum = 0.00609782455841582742 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0103258601251013694 0 0.0103258601251013694 ] ;
-split_col = 3 ;
-split_balance = 60 ;
-split_feature_value = 0.987251230798804613 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *663 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *664 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000178129223983526105 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *665 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 61 ;
-weights_sum = 0.0395947697605269905 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00609782455841582742 ;
-weighted_squared_targets_sum = 0.00609782455841582742 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *662  ;
-right_node = *666 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *667 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0622915790656450516 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0620018625242281657 ;
-weighted_squared_targets_sum = 0.0620018625242281657 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.000576738154381820078 0 0.000576738154381820078 ] ;
-split_col = 3 ;
-split_balance = 2 ;
-split_feature_value = 0.624616292049510191 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *668 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *669 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0131429736748032988 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0131429736748032988 ;
-weighted_squared_targets_sum = 0.0131429736748032988 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *670 ->RegressionTreeLeave(
+right_leave = *67 ->RegressionTreeLeave(
 id = 22 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0491486053908417528 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0488588888494248669 ;
-weighted_squared_targets_sum = 0.0488588888494248669 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *667   )
-;
-left_leave = *659  ;
-right_node = *671 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *672 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.103975426616566904 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00374918446584425252 ;
-weighted_squared_targets_sum = 0.00374918446584425252 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.00722798996588220992 0 0.00722798996588220992 ] ;
-split_col = 4 ;
-split_balance = 18 ;
-split_feature_value = 0.497196075517891301 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *673 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *674 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000178129223983522093 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *675 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.103797297392583393 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00374918446584425252 ;
-weighted_squared_targets_sum = 0.00374918446584425252 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *672   )
-;
-left_leave = *656  ;
-right_node = *676 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *677 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 114 ;
-weights_sum = 0.793960095333278271 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.541453858869398008 ;
-weighted_squared_targets_sum = 0.541453858869398008 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.344401379680337461 0 0.344401379680337461 ] ;
-split_col = 1 ;
-split_balance = 100 ;
-split_feature_value = 0.495842598540276014 ;
-after_split_error = 0.322496386301985238 ;
-missing_node = *0 ;
-missing_leave = *678 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *679 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0558697575303407773 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0558697575303407357 ;
-weighted_squared_targets_sum = 0.0558697575303407357 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *680 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 113 ;
-weights_sum = 0.738090337802937424 ;
-targets_sum = 86 ;
-weighted_targets_sum = 0.485584101339056939 ;
-weighted_squared_targets_sum = 0.485584101339056939 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *677   )
-;
-priority_queue = *681 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *676  *671  *661  *666  ]  )
-;
-first_leave = *653  ;
-split_cols = 3 [ 1 1 4 ] ;
-split_values = 3 [ 0.479480044756096346 0.408499973451678211 1.54709578481515564e-13 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*682 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *683 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *684 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *685 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000155 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.427947843726230792 ;
-weighted_squared_targets_sum = 0.427947843726230792 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.489616973552601253 0 0.489616973552601253 ] ;
-split_col = 1 ;
-split_balance = 104 ;
-split_feature_value = 0.6805672568090122 ;
-after_split_error = 0.464965066090214818 ;
-missing_node = *0 ;
-missing_leave = *686 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *687 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *688 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 152 ;
-weights_sum = 0.963701247437870001 ;
-targets_sum = 44 ;
-weighted_targets_sum = 0.391649091164100738 ;
-weighted_squared_targets_sum = 0.391649091164100738 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.464965066090214874 0 0.464965066090214874 ] ;
-split_col = 4 ;
-split_balance = 54 ;
-split_feature_value = 0.999999964757892545 ;
-after_split_error = 0.44091836246908811 ;
-missing_node = *0 ;
-missing_leave = *689 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *690 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *691 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 103 ;
-weights_sum = 0.39841617504778909 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.214924998806458073 ;
-weighted_squared_targets_sum = 0.214924998806458073 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.197968071100192128 0 0.197968071100192128 ] ;
-split_col = 1 ;
-split_balance = 99 ;
-split_feature_value = 0.552069282377009474 ;
-after_split_error = 0.173757841628789905 ;
-missing_node = *0 ;
-missing_leave = *692 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *693 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000120788990143721729 ;
-targets_sum = 0 ;
-weighted_targets_sum = -6.93889390390722838e-18 ;
-weighted_squared_targets_sum = -6.93889390390722838e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *694 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 102 ;
-weights_sum = 0.398295386057646639 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.214924998806458073 ;
-weighted_squared_targets_sum = 0.214924998806458073 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *691  ;
-right_node = *695 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *696 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 49 ;
-weights_sum = 0.565285072390081855 ;
-targets_sum = 32 ;
-weighted_targets_sum = 0.176724092357642637 ;
-weighted_squared_targets_sum = 0.176724092357642637 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.24295029136889601 0 0.24295029136889601 ] ;
-split_col = 1 ;
-split_balance = 39 ;
-split_feature_value = 0.531511881898726335 ;
-after_split_error = 0.216740446543295801 ;
-missing_node = *0 ;
-missing_leave = *697 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *698 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *699 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.113281643080132152 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.000961483719544341709 ;
-weighted_squared_targets_sum = 0.000961483719544341709 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.00190664615493684798 0 0.00190664615493684798 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.92637587089929152 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *700 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *701 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0561600796802939131 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *702 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
 length = 4 ;
-weights_sum = 0.057121563399838246 ;
+weights_sum = 0.0266666666666666684 ;
 targets_sum = 3 ;
-weighted_targets_sum = 0.000961483719544341709 ;
-weighted_squared_targets_sum = 0.000961483719544341709 ;
+weighted_targets_sum = 0.0200000000000000004 ;
+weighted_squared_targets_sum = 0.0200000000000000004 ;
 loss_function_factor = 2  )
  )
 ;
-left_leave = *699  ;
-right_node = *703 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *704 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 44 ;
-weights_sum = 0.452003429309950133 ;
-targets_sum = 29 ;
-weighted_targets_sum = 0.175762608638098256 ;
-weighted_squared_targets_sum = 0.175762608638098256 ;
-loss_function_factor = 2  )
+right_leave = *64   )
 ;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.214833800388359003 0 0.214833800388359003 ] ;
-split_col = 1 ;
-split_balance = 30 ;
-split_feature_value = 0.544178240629241028 ;
-after_split_error = 0.172423668717861467 ;
-missing_node = *0 ;
-missing_leave = *705 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
+right_leave = *56   )
 ;
-left_node = *0 ;
-left_leave = *706 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0365602083497618274 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *707 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 43 ;
-weights_sum = 0.415443220960188264 ;
-targets_sum = 29 ;
-weighted_targets_sum = 0.175762608638098228 ;
-weighted_squared_targets_sum = 0.175762608638098228 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *704   )
-;
-right_leave = *696   )
-;
-left_leave = *688  ;
-right_node = *708 ->RegressionTreeNode(
+left_leave = *48  ;
+right_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *709 ->RegressionTreeLeave(
+leave = *69 ->RegressionTreeLeave(
 id = 4 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.0362987525621300128 ;
-targets_sum = 48 ;
-weighted_targets_sum = 0.0362987525621300128 ;
-weighted_squared_targets_sum = 0.0362987525621300128 ;
+length = 63 ;
+weights_sum = 0.419999999999999485 ;
+targets_sum = 61 ;
+weighted_targets_sum = 0.406666666666666177 ;
+weighted_squared_targets_sum = 0.406666666666666177 ;
 loss_function_factor = 2  )
 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 0.752591011543306765 ;
-after_split_error = 0 ;
+leave_error = 3 [ 0.0258201058201057432 0 0.0258201058201057432 ] ;
+split_col = 2 ;
+split_balance = 47 ;
+split_feature_value = 0.997650553369808346 ;
+after_split_error = 0.0200000000000000039 ;
 missing_node = *0 ;
-missing_leave = *710 ->RegressionTreeLeave(
+missing_leave = *70 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -8897,47 +1057,47 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *711 ->RegressionTreeLeave(
+left_leave = *71 ->RegressionTreeLeave(
 id = 9 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
 length = 1 ;
-weights_sum = 0.000287934265741774937 ;
+weights_sum = 0.00666666666666665495 ;
 targets_sum = 1 ;
-weighted_targets_sum = 0.000287934265741774937 ;
-weighted_squared_targets_sum = 0.000287934265741774937 ;
+weighted_targets_sum = 0.00666666666666665495 ;
+weighted_squared_targets_sum = 0.00666666666666665495 ;
 loss_function_factor = 2  )
 ;
 right_node = *0 ;
-right_leave = *712 ->RegressionTreeLeave(
+right_leave = *72 ->RegressionTreeLeave(
 id = 10 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
 verbosity = 2 ;
-length = 47 ;
-weights_sum = 0.0360108182963882542 ;
-targets_sum = 47 ;
-weighted_targets_sum = 0.0360108182963882542 ;
-weighted_squared_targets_sum = 0.0360108182963882542 ;
+length = 62 ;
+weights_sum = 0.413333333333332831 ;
+targets_sum = 60 ;
+weighted_targets_sum = 0.399999999999999523 ;
+weighted_squared_targets_sum = 0.399999999999999523 ;
 loss_function_factor = 2  )
  )
 ;
-right_leave = *709   )
+right_leave = *69   )
 ;
-priority_queue = *713 ->RegressionTreeQueue(
+priority_queue = *73 ->RegressionTreeQueue(
 verbosity = 2 ;
 maximum_number_of_nodes = 4 ;
 next_available_node = 4 ;
-nodes = 4 [ *703  *690  *698  *708  ]  )
+nodes = 4 [ *58  *50  *68  *63  ]  )
 ;
-first_leave = *685  ;
-split_cols = 3 [ 1 4 1 ] ;
-split_values = 3 [ 0.6805672568090122 0.999999964757892545 0.531511881898726335 ] ;
+first_leave = *45  ;
+split_cols = 3 [ 2 1 2 ] ;
+split_values = 3 [ 0.991025168386145405 0.482293993618237549 0.891579732096156263 ] ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 4 ;
-n_examples = 200 ;
+n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
 weightsize = 1 ;
@@ -8949,797 +1109,13 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*714 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *715 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *716 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *717 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 0.999999999999998668 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.441638038891628504 ;
-weighted_squared_targets_sum = 0.441638038891628504 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.493187762991169343 0 0.493187762991169343 ] ;
-split_col = 1 ;
-split_balance = 6 ;
-split_feature_value = 0.531511881898726335 ;
-after_split_error = 0.464460608552705823 ;
-missing_node = *0 ;
-missing_leave = *718 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *719 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *720 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 103 ;
-weights_sum = 0.429335299645965118 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.130288208924507709 ;
-weighted_squared_targets_sum = 0.130288208924507709 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.181500612068526523 0 0.181500612068526523 ] ;
-split_col = 1 ;
-split_balance = 87 ;
-split_feature_value = 0.502718698860307178 ;
-after_split_error = 0.148202035155245299 ;
-missing_node = *0 ;
-missing_leave = *721 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *722 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *723 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 95 ;
-weights_sum = 0.298035239982574751 ;
-targets_sum = 11 ;
-weighted_targets_sum = 0.129398478251990745 ;
-weighted_squared_targets_sum = 0.129398478251990745 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.146434632002289067 0 0.146434632002289067 ] ;
-split_col = 4 ;
-split_balance = 81 ;
-split_feature_value = 0.264644704110722606 ;
-after_split_error = 0.110345584607090541 ;
-missing_node = *0 ;
-missing_leave = *724 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *725 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00016766222960705129 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.38777878078144568e-17 ;
-weighted_squared_targets_sum = -1.38777878078144568e-17 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *726 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.297867577752968959 ;
-targets_sum = 11 ;
-weighted_targets_sum = 0.129398478251990745 ;
-weighted_squared_targets_sum = 0.129398478251990745 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *723  ;
-right_node = *727 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *728 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 8 ;
-weights_sum = 0.13130005966339045 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.000889730672516971508 ;
-weighted_squared_targets_sum = 0.000889730672516971508 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.00176740315295624019 0 0.00176740315295624019 ] ;
-split_col = 4 ;
-split_balance = 4 ;
-split_feature_value = 0.999999999998003708 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *729 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *730 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0144763715039025256 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *731 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.116823688159487946 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.000889730672516971508 ;
-weighted_squared_targets_sum = 0.000889730672516971508 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *728   )
-;
-left_leave = *720  ;
-right_node = *732 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *733 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 97 ;
-weights_sum = 0.570664700354033827 ;
-targets_sum = 79 ;
-weighted_targets_sum = 0.311349829967120295 ;
-weighted_squared_targets_sum = 0.311349829967120295 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.282959996484179466 0 0.282959996484179466 ] ;
-split_col = 2 ;
-split_balance = 83 ;
-split_feature_value = 0.0689879291310910303 ;
-after_split_error = 0.233209680328619551 ;
-missing_node = *0 ;
-missing_leave = *734 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *735 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *736 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0728921172508161241 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = 4 ;
-split_balance = 1 ;
-split_feature_value = 0.999999932728100394 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *737 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *738 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0144763715039025239 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *739 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0584157457469136054 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *736  ;
-right_node = *740 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *741 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 90 ;
-weights_sum = 0.497772583103216981 ;
-targets_sum = 79 ;
-weighted_targets_sum = 0.311349829967120351 ;
-weighted_squared_targets_sum = 0.311349829967120351 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.233209680328619218 0 0.233209680328619218 ] ;
-split_col = 1 ;
-split_balance = 76 ;
-split_feature_value = 0.544178240629241028 ;
-after_split_error = 0.210355535310927766 ;
-missing_node = *0 ;
-missing_leave = *742 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *743 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0166030527017196824 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.016603052701719738 ;
-weighted_squared_targets_sum = 0.016603052701719738 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *744 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.481169530401498124 ;
-targets_sum = 78 ;
-weighted_targets_sum = 0.294746777265401327 ;
-weighted_squared_targets_sum = 0.294746777265401327 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *741   )
-;
-right_leave = *733   )
-;
-priority_queue = *745 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *722  *727  *740  *735  ]  )
-;
-first_leave = *717  ;
-split_cols = 3 [ 1 2 1 ] ;
-split_values = 3 [ 0.531511881898726335 0.0689879291310910303 0.502718698860307178 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-*746 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 4 ;
-compute_train_stats = 0 ;
-complexity_penalty_factor = 0 ;
-output_confidence_target = 0 ;
-multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *747 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *748 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *749 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 200 ;
-weights_sum = 1.00000000000000244 ;
-targets_sum = 92 ;
-weighted_targets_sum = 0.433521170121874355 ;
-weighted_squared_targets_sum = 0.433521170121874355 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.491161130356071396 0 0.491161130356071396 ] ;
-split_col = 1 ;
-split_balance = 10 ;
-split_feature_value = 0.502718698860307178 ;
-after_split_error = 0.456009411249177987 ;
-missing_node = *0 ;
-missing_leave = *750 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *751 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *752 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 95 ;
-weights_sum = 0.327685562949602771 ;
-targets_sum = 11 ;
-weighted_targets_sum = 0.204284811315166598 ;
-weighted_squared_targets_sum = 0.204284811315166598 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.153860298493941283 0 0.153860298493941283 ] ;
-split_col = 4 ;
-split_balance = 81 ;
-split_feature_value = 0.264644704110722606 ;
-after_split_error = 0.124344699950523135 ;
-missing_node = *0 ;
-missing_leave = *753 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *754 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *755 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 88 ;
-weights_sum = 0.231894852396504175 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.112937949520379388 ;
-weighted_squared_targets_sum = 0.112937949520379388 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.11586931364179763 0 0.11586931364179763 ] ;
-split_col = 2 ;
-split_balance = 78 ;
-split_feature_value = 0.941974890824293754 ;
-after_split_error = 0.0862489807289095883 ;
-missing_node = *0 ;
-missing_leave = *756 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *757 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *758 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 83 ;
-weights_sum = 0.144124552913856202 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0986153382941301337 ;
-weighted_squared_targets_sum = 0.0986153382941301337 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0622781684937041558 0 0.0622781684937041558 ] ;
-split_col = 4 ;
-split_balance = 75 ;
-split_feature_value = 1.46268193282061976e-05 ;
-after_split_error = 0.0317752728200500489 ;
-missing_node = *0 ;
-missing_leave = *759 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *760 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.000122687633122823522 ;
-targets_sum = 0 ;
-weighted_targets_sum = 3.46944695195361419e-18 ;
-weighted_squared_targets_sum = 3.46944695195361419e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *761 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 82 ;
-weights_sum = 0.144001865280733138 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0986153382941301337 ;
-weighted_squared_targets_sum = 0.0986153382941301337 ;
-loss_function_factor = 2  )
- )
-;
-left_leave = *758  ;
-right_node = *762 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *763 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0877702994826478622 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0143226112262492579 ;
-weighted_squared_targets_sum = 0.0143226112262492579 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0239708122352054361 0 0.0239708122352054361 ] ;
-split_col = 3 ;
-split_balance = 3 ;
-split_feature_value = 0.987251230798804613 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *764 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *765 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0124925465023531211 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *766 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0752777529802947376 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0143226112262492579 ;
-weighted_squared_targets_sum = 0.0143226112262492579 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *763   )
-;
-left_leave = *755  ;
-right_node = *767 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *768 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0957907105530985958 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0913468617947872097 ;
-weighted_squared_targets_sum = 0.0913468617947872097 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.00847538630872543552 0 0.00847538630872543552 ] ;
-split_col = 3 ;
-split_balance = 5 ;
-split_feature_value = 0.306301132450967406 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *769 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *770 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0353792403406492165 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0353792403406492303 ;
-weighted_squared_targets_sum = 0.0353792403406492303 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *771 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0604114702124493724 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.0559676214541379724 ;
-weighted_squared_targets_sum = 0.0559676214541379724 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *768   )
-;
-left_leave = *752  ;
-right_node = *772 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-leave = *773 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 105 ;
-weights_sum = 0.672314437050399616 ;
-targets_sum = 81 ;
-weighted_targets_sum = 0.229236358806707563 ;
-weighted_squared_targets_sum = 0.229236358806707563 ;
-loss_function_factor = 2  )
-;
-leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.302149112755236982 0 0.302149112755236982 ] ;
-split_col = 1 ;
-split_balance = 89 ;
-split_feature_value = 0.531511881898726335 ;
-after_split_error = 0.278035902305286187 ;
-missing_node = *0 ;
-missing_leave = *774 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-left_node = *0 ;
-left_leave = *775 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0105931536290697573 ;
-targets_sum = 0 ;
-weighted_targets_sum = 5.20417042793042128e-18 ;
-weighted_squared_targets_sum = 5.20417042793042128e-18 ;
-loss_function_factor = 2  )
-;
-right_node = *0 ;
-right_leave = *776 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 104 ;
-weights_sum = 0.661721283421328188 ;
-targets_sum = 81 ;
-weighted_targets_sum = 0.22923635880670773 ;
-weighted_squared_targets_sum = 0.22923635880670773 ;
-loss_function_factor = 2  )
- )
-;
-right_leave = *773   )
-;
-priority_queue = *777 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *757  *762  *772  *767  ]  )
-;
-first_leave = *749  ;
-split_cols = 3 [ 1 4 2 ] ;
-split_values = 3 [ 0.502718698860307178 0.264644704110722606 0.941974890824293754 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 4 ;
-n_examples = 200 ;
-inputsize = 5 ;
-targetsize = 1 ;
-weightsize = 1 ;
-forget_when_training_set_changes = 1 ;
-nstages = 4 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
 ] ;
-voting_weights = 12 [ 1.12702902604969268 1.03548207574539641 0.627174476979685869 0.750250393230790458 0.243196868623840629 0.487910101857012224 0.434343534989996505 0.369680722849932031 0.516138335783265179 0.287214970759119059 0.384464196250010937 0.439193765713115358 ] ;
-sum_voting_weights = 6.70207846883185709 ;
-initial_sum_weights = 200 ;
-example_weights = 200 [ 0.000208999323974944171 0.00055454680971253373 0.000280504045088514136 0.00055454680971253373 0.000208999323974944171 0.00970647716624035983 0.00055454680971253373 0.000339924914881385677 0.0110653149853990503 0.00778205130351132708 0.000208999323974944171 0.024398678070881339 0.0323001053964686105 0.00055454680971253373 0.000339924914881385677 0.000117671997982472966 0.000208999323974944171 0.00055454680971253373 0.0490383661954016739 0.000437769476772936251 0.000117671997982472966 0.0205716909579260823 0.000117671997982472966 0.000117671997982472966 0.000117671997982472966 0.00055454680971253373 0.0076104231728418353 0.0171298405614496337 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.016300265773906121 0.00119640871801693599 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.0105321055927624831 0.0105321055927624831 0.000208999323974944171 0.000208999323974944171 0.00055454680971253373 0.0002089993!
 23974944171 0.00259525858356851125 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.00623231395217586567 0.000117671997982472966 0.000208999323974944171 0.0323001053964686105 0.000208999323974944171 0.00055454680971253373 0.0017777693464267458 0.000117671997982472966 0.00055454680971253373 0.00910668071110512685 0.000280504045088514136 0.000117671997982472966 0.000208999323974944171 0.000208999323974944171 0.00422103304611871639 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.0319239733177390586 0.0323001053964686105 0.000437769476772936251 0.000208999323974944171 0.000208999323974944171 0.0143798490176562967 0.000208999323974944171 0.026377266187366858 0.000339924914881385677 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.00205966483174763919 0.00749705170508035805 0.0110250694567599825 0.00132191707442976783 0.000339924914881385677 0.0250388131247752874 0.0180455184510893!
 603 0.000280504045088514136 0.000117671997982472966 0.00970647!
 71662403
5983 0.000208999323974944171 0.000208999323974944171 0.000280504045088514136 0.000208999323974944171 0.000208999323974944171 0.000117671997982472966 0.000208999323974944171 0.00884130168746423764 0.000208999323974944171 0.00055454680971253373 0.000117671997982472966 0.0143798490176562967 0.00055454680971253373 0.000208999323974944171 0.00119640871801693599 0.00153462513281898624 0.000280504045088514136 0.000280504045088514136 0.0017777693464267458 0.019621017607701189 0.000117671997982472966 0.000117671997982472966 0.000208999323974944171 0.000280504045088514136 0.000208999323974944171 0.000437769476772936251 0.000117671997982472966 0.0323001053964686105 0.000117671997982472966 0.000117671997982472966 0.00119162560508736365 0.026377266187366858 0.00422103304611871639 0.000117671997982472966 0.00055454680971253373 0.000117671997982472966 0.000339924914881385677 0.000280504045088514136 0.0387900741831750592 0.00119162560508736365 0.0330808940330893156 0.00910668071110512685 0.!
 00205966483174763919 0.00027985234944076417 0.016300265773906121 0.00055454680971253373 0.000280504045088514136 0.000208999323974944171 0.000280504045088514136 0.0143798490176562967 0.000117671997982472966 0.0171298405614496337 0.0279351648715520373 0.000208999323974944171 0.000280504045088514136 0.000208999323974944171 0.000280504045088514136 0.0227297487201944676 0.000208999323974944171 0.00055454680971253373 0.000117671997982472966 0.000208999323974944171 0.000339924914881385677 0.000208999323974944171 0.0265375112954981986 0.000117671997982472966 0.000208999323974944171 0.000117671997982472966 0.0205716909579260823 0.00778205130351132708 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.000339924914881385677 0.0126482917697524257 0.000280504045088514136 0.000208999323974944171 0.000117671997982472966 0.000497050724460643459 0.00422103304611871639 0.000208999323974944171 0.0036582072611220668 0.000954322272458555136 0.000208999323974944171 0.00205!
 966483174763919 0.000901937257518955929 0.00020899932397494417!
 1 0.0211
476558591618367 0.000208999323974944171 0.000901937257518955929 0.000117671997982472966 0.000339924914881385677 0.00749705170508035805 0.00970647716624035983 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.000208999323974944171 0.00749705170508035805 0.0323001053964686105 0.000208999323974944171 0.000117671997982472966 0.00205966483174763919 0.000208999323974944171 0.000280504045088514136 0.0206965325387320695 0.00749705170508035805 0.000280504045088514136 0.00757013044178810914 0.000437769476772936251 0.000208999323974944171 0.0227297487201944676 0.000208999323974944171 ] ;
-learners_error = 12 [ 0.095000000000000015 0.111951148589706381 0.221948222046453703 0.182350845160595049 0.380743478208193542 0.273721933428698827 0.2955275688484712 0.323143793784225974 0.262642962029557558 0.36021526859897407 0.316710962060604451 0.293512033410994522 ] ;
-weak_learner_template = *778 ->RegressionTree(
+voting_weights = 1 [ 1.22117351768460214 ] ;
+sum_voting_weights = 1.22117351768460214 ;
+initial_sum_weights = 1 ;
+example_weights = 150 [ 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971!
 132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579!
 71132 0.0036231884057971132 0.0036231884057971132 0.0036231884!
 05797113
2 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579711!
 32 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 ] ;
+learners_error = 1 [ 0.0800000000000000017 ] ;
+weak_learner_template = *74 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -9747,7 +1123,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *779 ->RegressionTreeLeave(
+leave_template = *75 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -9772,7 +1148,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 1 ;
+weightsize = 0 ;
 forget_when_training_set_changes = 1 ;
 nstages = 4 ;
 report_progress = 1 ;
@@ -9792,17 +1168,17 @@
 save_often = 0 ;
 compute_training_error = 0 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 1 ;
+modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 12 ;
-n_examples = 200 ;
+stage = 1 ;
+n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 1 ;
+weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 12 ;
+nstages = 1 ;
 report_progress = 1 ;
 verbosity = 2 ;
 nservers = 0 ;
@@ -9811,7 +1187,7 @@
 use_a_separate_random_generator_for_testing = 1827  )
 ;
 forward_sub_learner_test_costs = 1 ;
-learner_template = *780 ->AdaBoost(
+learner_template = *76 ->AdaBoost(
 weak_learners = []
 ;
 voting_weights = []
@@ -9822,7 +1198,7 @@
 ;
 learners_error = []
 ;
-weak_learner_template = *781 ->RegressionTree(
+weak_learner_template = *77 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -9830,7 +1206,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *782 ->RegressionTreeLeave(
+leave_template = *78 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -9920,10 +1296,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *783 ->HyperOptimize(
+strategy = 1 [ *79 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *784 ->EarlyStoppingOracle(
+oracle = *80 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;
@@ -9937,8 +1313,8 @@
 max_degraded_steps = 120 ;
 min_n_steps = 2 ;
 nreturned = 20 ;
-best_objective = 0 ;
-best_step = 12 ;
+best_objective = 0.220000000000000001 ;
+best_step = 2 ;
 met_early_stopping = 0  )
 ;
 provide_tester_expdir = 0 ;
@@ -9951,8 +1327,8 @@
 auto_save = 0 ;
 auto_save_diff_time = 10800 ;
 auto_save_test = 0 ;
-best_objective = 0 ;
-best_results = 8 [ 0.00666666666666666709 0.00666666666666666709 0.00666666666666666709 0 0 0 0 0 ] ;
+best_objective = 0.220000000000000001 ;
+best_results = 8 [ 0.100000000000000006 0.100000000000000006 0.100000000000000006 0 0.220000000000000001 0.239999999999999991 0.280000000000000027 0 ] ;
 best_learner = *5  ;
 trialnum = 20 ;
 option_vals = []

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-11-07 18:19:03 UTC (rev 9657)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-11-07 18:28:59 UTC (rev 9658)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9654"
+__REVISION__ = "PL9656"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1



From nouiz at mail.berlios.de  Fri Nov  7 19:53:42 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 19:53:42 +0100
Subject: [Plearn-commits] r9659 - trunk/plearn_learners/hyper
Message-ID: <200811071853.mA7Irgcn028592@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 19:53:42 +0100 (Fri, 07 Nov 2008)
New Revision: 9659

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
added some check in debug mode.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-07 18:28:59 UTC (rev 9658)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-07 18:53:42 UTC (rev 9659)
@@ -40,6 +40,7 @@
 
 #include "HyperLearner.h"
 #include <plearn/base/stringutils.h>
+#include <plearn/base/PLearnDiff.h>
 #include <plearn/io/load_and_save.h>
 #include <plearn/vmat/FileVMatrix.h>
 
@@ -346,6 +347,13 @@
         perr << "In HyperLearner::auto_save() - We save the hlearner"
              << endl;
     PLearn::save(tmp, this);
+
+#ifdef BOUNDCHECK
+    HyperLearner n;
+    PLearn::load(tmp,n);
+    PLCHECK(PLearn::diff(&n,this));
+#endif
+
     mvforce(tmp,f);
 }
 



From nouiz at mail.berlios.de  Fri Nov  7 20:23:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 20:23:26 +0100
Subject: [Plearn-commits] r9660 - trunk/plearn_learners/regressors
Message-ID: <200811071923.mA7JNQup030610@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 20:23:25 +0100 (Fri, 07 Nov 2008)
New Revision: 9660

Modified:
   trunk/plearn_learners/regressors/RegressionTreeQueue.cc
Log:
bugfix


Modified: trunk/plearn_learners/regressors/RegressionTreeQueue.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-11-07 18:53:42 UTC (rev 9659)
+++ trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-11-07 19:23:25 UTC (rev 9660)
@@ -53,14 +53,16 @@
 
 RegressionTreeQueue::RegressionTreeQueue()    
     : verbosity(0),
-      maximum_number_of_nodes(400)
+      maximum_number_of_nodes(400),
+      next_available_node(0)
 {
     build();
 }
 RegressionTreeQueue::RegressionTreeQueue(int verbosity_,
                                          int maximum_number_of_nodes_)
     : verbosity(verbosity_),
-      maximum_number_of_nodes(maximum_number_of_nodes_)
+      maximum_number_of_nodes(maximum_number_of_nodes_),
+      next_available_node(0)
 {
     build();
 }
@@ -99,7 +101,6 @@
 
 void RegressionTreeQueue::build_()
 {
-    next_available_node = 0;
     nodes.resize(maximum_number_of_nodes);
 }
 



From nouiz at mail.berlios.de  Fri Nov  7 20:37:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 20:37:49 +0100
Subject: [Plearn-commits] r9661 - trunk/plearn_learners/meta
Message-ID: <200811071937.mA7JbnrN000126@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 20:37:48 +0100 (Fri, 07 Nov 2008)
New Revision: 9661

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
serialize the time costs.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-07 19:23:25 UTC (rev 9660)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-11-07 19:37:48 UTC (rev 9661)
@@ -106,8 +106,24 @@
                   OptionBase::buildoption,
                   "The template to use for learner1 and learner2.\n");
 
-}
+    declareOption(ol, "train_time",
+                  &MultiClassAdaBoost::train_time, OptionBase::learntoption,
+                  "The time spent in the last call to train() in second.");
 
+    declareOption(ol, "total_train_time",
+                  &MultiClassAdaBoost::total_train_time, OptionBase::learntoption,
+                  "The total time spent in the train() function in second.");
+
+    declareOption(ol, "test_time",
+                  &MultiClassAdaBoost::test_time, OptionBase::learntoption,
+                  "The time spent in the last call to test() in second.");
+
+    declareOption(ol, "total_test_time",
+                  &MultiClassAdaBoost::total_test_time, OptionBase::learntoption,
+                  "The total time spent in the test() function in second.");
+
+ }
+
 void MultiClassAdaBoost::build_()
 {
     sub_target_tmp.resize(2);



From nouiz at mail.berlios.de  Fri Nov  7 20:43:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 20:43:03 +0100
Subject: [Plearn-commits] r9662 - trunk/plearn_learners/generic
Message-ID: <200811071943.mA7Jh3fF000854@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 20:43:02 +0100 (Fri, 07 Nov 2008)
New Revision: 9662

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
made the profiler AddCostToLearner::{train,test}_time stats keep the correct value.


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-11-07 19:37:48 UTC (rev 9661)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-11-07 19:43:02 UTC (rev 9662)
@@ -797,16 +797,16 @@
     Profiler::end("AddCostToLearner::train");
     if(Profiler::isActive()){
         const Profiler::Stats& stats = Profiler::getStats("AddCostToLearner::train");
-        train_time=stats.wall_duration/Profiler::ticksPerSecond();
-        total_train_time+=train_time;
-        Profiler::reset("AddCostToLearner::train");
+        real tmp=stats.wall_duration/Profiler::ticksPerSecond();
+        train_time=tmp - total_train_time;
+        total_train_time=tmp;
 
         //we get the test_time here as we want the test time for all dataset.
         //if we put it in the test function, we would have it for one dataset.
         const Profiler::Stats& stats_test = Profiler::getStats("AddCostToLearner::test");
-        test_time=stats_test.wall_duration/Profiler::ticksPerSecond();
-        total_test_time+=test_time;
-        Profiler::reset("AddCostToLearner::test");
+        tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
+        test_time=tmp-total_test_time;
+        total_test_time=tmp;  
     }
 }
 



From nouiz at mail.berlios.de  Fri Nov  7 21:44:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 7 Nov 2008 21:44:01 +0100
Subject: [Plearn-commits] r9663 - trunk/plearn_learners/hyper
Message-ID: <200811072044.mA7Ki1IU006536@sheep.berlios.de>

Author: nouiz
Date: 2008-11-07 21:44:01 +0100 (Fri, 07 Nov 2008)
New Revision: 9663

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
bugfix in auto_save



Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-11-07 19:43:02 UTC (rev 9662)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-11-07 20:44:01 UTC (rev 9663)
@@ -358,8 +358,13 @@
 {
 //in the case when auto_save is true. This function can be called even
 //if the optimisation is finished. We must not redo it in this case.
-    if(trialnum>0&&!option_vals&&resultsmat.length()==trialnum+1)
+    if(trialnum>0&&!option_vals&&resultsmat.length()==trialnum+1){
+        hlearner->setLearner(best_learner);
+        if (!best_results.isEmpty() && resultsmat->get(resultsmat.length()-1,0)!=-1)
+            reportResult(-1,best_results);
+
         return best_results;
+    }
     TVec<string> option_names;
     option_names = oracle->getOptionNames();
 



From nouiz at mail.berlios.de  Mon Nov 10 18:15:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Nov 2008 18:15:50 +0100
Subject: [Plearn-commits] r9664 - trunk/plearn/vmat
Message-ID: <200811101715.mAAHFoj9015516@sheep.berlios.de>

Author: nouiz
Date: 2008-11-10 18:15:49 +0100 (Mon, 10 Nov 2008)
New Revision: 9664

Modified:
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
   trunk/plearn/vmat/MissingIndicatorVMatrix.h
Log:
added option MissingIndicatorVMatrix::save_fields_with_missing to don't force the use of a train_set if used with the option MissingIndicatorVMatrix::fields.


Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-11-07 20:44:01 UTC (rev 9663)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-11-10 17:15:49 UTC (rev 9664)
@@ -42,6 +42,7 @@
 
 #include "MissingIndicatorVMatrix.h"
 #include <plearn/math/TMat_maths.h>
+#include <plearn/io/load_and_save.h>
 
 namespace PLearn {
 using namespace std;
@@ -89,6 +90,13 @@
   declareOption(ol, "fields", &MissingIndicatorVMatrix::fields, OptionBase::buildoption,
 		"The names of the fields to extract if the train_set is not provided.");
 
+  declareOption(ol, "save_fields_with_missing",
+		&MissingIndicatorVMatrix::save_fields_with_missing,
+		OptionBase::buildoption,
+		"The file name where we save the name of the fields that we"
+		" add an indicator. It can be reused with the options fields"
+		" to redo the same processing.");
+
   inherited::declareOptions(ol);
 }
 
@@ -236,6 +244,7 @@
     TVec<string> new_field_names(width_);
     source_field_names = source->fieldNames();
     int new_col = 0;
+    TVec<string> miss;
     for (int source_col = 0; source_col < source_inputsize; source_col++)
     {
       new_field_names[new_col] = source_field_names[source_col];
@@ -246,8 +255,10 @@
           new_field_names[new_col] = source_field_names[source_col] + "_MI";
           source_rel_pos[new_col] = -1;
           new_col += 1;
+	  miss->append(source->fieldName(source_col));
       }
     }
+    PLCHECK(miss->size()==added_colomns);
     for (int source_col = source_inputsize; source_col < source_width; source_col++)
     {
       new_field_names[new_col] = source_field_names[source_col];
@@ -260,6 +271,10 @@
     weightsize_ = source->weightsize();
     source_input.resize(source_inputsize);
     declareFieldNames(new_field_names);
+
+    if(!save_fields_with_missing.empty())
+      PLearn::save(save_fields_with_missing,miss);
+      
 }
 
 } // end of namespcae PLearn

Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.h
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.h	2008-11-07 20:44:01 UTC (rev 9663)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.h	2008-11-10 17:15:49 UTC (rev 9664)
@@ -70,6 +70,7 @@
 
   //!
   TVec<string> fields; 
+  string       save_fields_with_missing;
 
                         MissingIndicatorVMatrix();
                         MissingIndicatorVMatrix(VMat the_source, VMat the_train_set, real the_number_of_train_samples_to_use);



From nouiz at mail.berlios.de  Mon Nov 10 18:17:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Nov 2008 18:17:00 +0100
Subject: [Plearn-commits] r9665 - trunk/plearn/vmat
Message-ID: <200811101717.mAAHH0M4015609@sheep.berlios.de>

Author: nouiz
Date: 2008-11-10 18:16:59 +0100 (Mon, 10 Nov 2008)
New Revision: 9665

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
Log:
added option SelectColumnsVMatrix::save_fields to allow replicating the exact same matrix.


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-10 17:15:49 UTC (rev 9664)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-10 17:16:59 UTC (rev 9665)
@@ -40,6 +40,7 @@
  ******************************************************* */
 
 #include "SelectColumnsVMatrix.h"
+#include <plearn/io/load_and_save.h>
 
 namespace PLearn {
 using namespace std;
@@ -152,6 +153,12 @@
                   "The names of the fields to extract (will override 'indices' if provided). Can\n"
                   "be a range of the form Field_1-Field_n, if 'extend_with_missing' is false.");
 
+    declareOption(ol, "save_fields", &SelectColumnsVMatrix::save_fields,
+                  OptionBase::buildoption,
+                  "The filename where to save the filename of this VMatrix."
+        );
+
+
     declareOption(ol, "fields_partial_match", &SelectColumnsVMatrix::fields_partial_match, OptionBase::buildoption,
                   "If set to 1, then a field will be kept iff it contains one of the strings from 'fields'.");
 
@@ -305,6 +312,9 @@
 
         sinput.resize(width());
         sinput.fill(MISSING_VALUE);
+
+        if(!save_fields.empty())
+            PLearn::save(save_fields,fieldNames());
     }
 }
 

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-11-10 17:15:49 UTC (rev 9664)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-11-10 17:16:59 UTC (rev 9665)
@@ -70,6 +70,7 @@
     bool extend_with_missing;
     TVec<int> indices;
     TVec<string> fields;
+    string save_fields;
     bool fields_partial_match;
     bool inverse_fields_selection;
     bool warn_non_selected_field;



From nouiz at mail.berlios.de  Mon Nov 10 18:26:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Nov 2008 18:26:22 +0100
Subject: [Plearn-commits] r9666 - trunk/plearn/vmat
Message-ID: <200811101726.mAAHQMnX016531@sheep.berlios.de>

Author: nouiz
Date: 2008-11-10 18:26:21 +0100 (Mon, 10 Nov 2008)
New Revision: 9666

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
Log:
better error msg.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-11-10 17:16:59 UTC (rev 9665)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-11-10 17:26:21 UTC (rev 9666)
@@ -152,7 +152,10 @@
         return;
 
     if (train_source) {
-        source->compatibleSizeError(train_source);
+        source->compatibleSizeError(train_source,
+                                    "In GaussianizeVMatrix::build_ -"
+                                    " The source and the train_source"
+                                    " option are not compatible.");
     }
 
     VMat the_source = train_source ? train_source : source;

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-11-10 17:16:59 UTC (rev 9665)
+++ trunk/plearn/vmat/VMatrix.cc	2008-11-10 17:26:21 UTC (rev 9666)
@@ -1380,7 +1380,7 @@
 void VMatrix::compatibleSizeError(const VMat& m, const string& extra_msg) {
 #define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::compatibleSizeError " \
         " - in class %s - The matrices are not compatible!\n"               \
-        "m1."#NAME"=%d and m2."#NAME"=%d. %s",                              \
+        "m1."#NAME"=%d and m2."#NAME"=%d. \n%s",                            \
         classname().c_str(), this->NAME(), m->NAME(), extra_msg.c_str());
 
     if(this->width()      != m->width())



From ducharme at mail.berlios.de  Mon Nov 10 21:04:30 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 10 Nov 2008 21:04:30 +0100
Subject: [Plearn-commits] r9667 - trunk/plearn_learners/regressors
Message-ID: <200811102004.mAAK4USt026343@sheep.berlios.de>

Author: ducharme
Date: 2008-11-10 21:04:30 +0100 (Mon, 10 Nov 2008)
New Revision: 9667

Added:
   trunk/plearn_learners/regressors/PruningLinearRegressor.cc
   trunk/plearn_learners/regressors/PruningLinearRegressor.h
Log:
LinearRegressor for which we prune the regression coefficients with the smallest t-ratios.


Added: trunk/plearn_learners/regressors/PruningLinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/PruningLinearRegressor.cc	2008-11-10 17:26:21 UTC (rev 9666)
+++ trunk/plearn_learners/regressors/PruningLinearRegressor.cc	2008-11-10 20:04:30 UTC (rev 9667)
@@ -0,0 +1,270 @@
+// -*- C++ -*-
+
+// PruningLinearRegressor.cc
+//
+// Copyright (C) 2008 Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Rejean Ducharme
+
+/*! \file PruningLinearRegressor.cc */
+
+#include "PruningLinearRegressor.h"
+#include <plearn/math/plapack.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PruningLinearRegressor,
+    "Same as LinearRegressor, but adding the pruning of the regression coefficients",
+    "This class permits to reduce the degree of freedom of a LinearRegressor by\n"
+    "pruning some regression coefficients.  Several pruning methods are supported:\n"
+    "  - minimum t-ratio: keep only the coefficients for which the t-ratio exceeds a threshold\n"
+    "  - absolute max: keep only a maximum number of coefficient (with best t-ratios)\n"
+    "  - relative max: keep only a maximum fraction of coefficient (with best t-ratios)\n"
+    );
+
+PruningLinearRegressor::PruningLinearRegressor()
+    : pruning_method("max_number"),
+      min_t_ratio(0.05),
+      max_number(50),
+      max_fraction(0.5)
+{ }
+
+void PruningLinearRegressor::declareOptions(OptionList& ol)
+{
+    //#####  Build Options  ####################################################
+
+    declareOption(ol, "pruning_method", &PruningLinearRegressor::pruning_method,
+                  OptionBase::buildoption,
+                  "The pruning method:\n"
+                  " - \"max_number\"    = keep only the weights with the k-best t-ratio\n"
+                  " - \"max_fraction\"  = same as \"max_number\", but using a fraction rather than a hard threshold\n"
+                  " - \"min_t_ratio\"   = keep only the weights with t-ratio > min_t_ratio");
+
+    declareOption(ol, "min_t_ratio", &PruningLinearRegressor::min_t_ratio,
+                  OptionBase::buildoption,
+                  "Minimum t-ratio for not pruning a coefficient");
+
+    declareOption(ol, "max_number", &PruningLinearRegressor::max_number,
+                  OptionBase::buildoption,
+                  "Maximum number of coefficients (the default)");
+
+    declareOption(ol, "max_fraction", &PruningLinearRegressor::max_fraction,
+                  OptionBase::buildoption,
+                  "Maximum fraction (in [0,1]) of coefficients");
+
+    //#####  Learnt Options  ###################################################
+
+    declareOption(ol, "t_ratio", &PruningLinearRegressor::t_ratio,
+                  OptionBase::learntoption,
+                  "t-ratio statistics for the estimator b (regression coefficients)\n"
+                  "Saved as a learned option to allow computing statistical significance\n"
+                  "of the coefficients when the model is reloaded and used in test mode.");
+
+    declareOption(ol, "input_indices", &PruningLinearRegressor::input_indices,
+                  OptionBase::learntoption,
+                  "Indices of inputs kept for regression");
+
+    inherited::declareOptions(ol);
+}
+
+void PruningLinearRegressor::build_()
+{
+    if (pruning_method == "max_number")
+    {
+        if (max_number < 1)
+            PLERROR("\"max_number\" should be strictly positive");
+    }
+    else if (pruning_method == "max_fraction")
+    {
+        if (max_fraction <= 0.0  ||  max_fraction >= 1.0)
+            PLERROR("\"max_fraction\" should be in range ]0,1[");
+    }
+    else if (pruning_method == "min_t_ratio")
+    {
+        if (min_t_ratio <= 0.0)
+            PLERROR("\"min_t_ratio\" should be strictly positive");
+    }
+    else
+        PLERROR("Pruning method \"%s\" not supported", pruning_method.c_str());
+}
+
+void PruningLinearRegressor::build()
+{
+    inherited::build();
+    build_();
+}
+
+void PruningLinearRegressor::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(t_ratio, copies);
+    deepCopyField(input_indices, copies);
+}
+
+void PruningLinearRegressor::forget()
+{
+    inherited::forget();
+    t_ratio.resize(0);
+}
+
+void PruningLinearRegressor::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set, call_forget);
+    if (targetsize() > 1)
+        PLERROR("PruningLinearRegressor works only with single target problems");
+}
+
+void PruningLinearRegressor::train()
+{
+    // train with all coefficients
+    inherited::train();
+
+    // find the dataset indices corresponding to coefficients not pruned
+    newDatasetIndices();
+
+    // Add target and (possibly) weight indices
+    TVec<int> all_indices = input_indices.copy();
+    all_indices.append(inputsize());        // target index = inputsize
+    if (weightsize())
+        all_indices.append(inputsize()+1);  // weight index = target index + 1
+
+    // Build new training set
+    VMat new_trainset = train_set.columns(all_indices);
+    int new_inputsize = input_indices.length();
+    new_trainset->defineSizes(new_inputsize, 1, weightsize());
+
+    // Train with this new training set
+    setTrainingSet(new_trainset);
+    inherited::train();
+}
+
+void PruningLinearRegressor::computeOutput(const Vec& input, Vec& output) const
+{
+    Vec actual_input(input_indices.length());
+    selectElements(input, input_indices, actual_input);
+    inherited::computeOutput(actual_input, output);
+}
+
+void PruningLinearRegressor::computeTRatio()
+{
+    // We wish to compute the t-ratio of the estimator coefficients.
+    // For that purpose, we use the following formula:
+    //
+    //    t = |b| / sigma_b
+    //
+    // where b is the coefficients vector and sigma_b is the stderr matrix
+    // of the estimator of b.  The latter is computed as:
+    //
+    //     sigma_b = s^2 * inverse(X'X)
+    //
+    // where s^2 is the residual variance (estimated using the
+    // LinearRegressor::computeResidualsVariance method)
+    // and X is the matrix of regressors..
+
+    const int ninputs = weights.length();
+    Mat sigma_b(ninputs, ninputs);
+    t_ratio.resize(ninputs);
+
+    // We compute the estimator
+    PLASSERT(resid_variance.length() == 1);
+    PLASSERT(weights.width() == 1);
+    real residual_variance = resid_variance[0];
+ 
+    Mat XtX_copy = XtX.copy();  // matInvert overwrite the input matrix
+    Mat XtX_inverse(XtX.length(), XtX.width());
+    matInvert(XtX_copy, XtX_inverse);
+    for (int i=0; i<ninputs; i++)
+    {
+        real sigma_b = sqrt(residual_variance*XtX_inverse(i,i));
+        t_ratio[i] = abs(weights(i,0)) / sigma_b;
+    }
+}
+
+void PruningLinearRegressor::newDatasetIndices()
+{
+    // Compute first the t-ratios
+    computeTRatio();
+
+    // Sort all t-ratios
+    int nb_weights = weights.length();
+    Vec t_ratio_sort = t_ratio.copy();
+    sortElements(t_ratio_sort, true);
+
+    // Find the t-ratio threshold
+    real t_ratio_threshold = 0.0;
+    if (pruning_method == "max_number")
+    {
+        int keep_n_weights = min(max_number, nb_weights);
+        if (include_bias)
+            ++keep_n_weights;
+        t_ratio_threshold = t_ratio_sort[keep_n_weights-1];
+    }
+    else if (pruning_method == "max_fraction")
+    {
+        int keep_n_weights = max_fraction*nb_weights;
+        t_ratio_threshold = t_ratio_sort[keep_n_weights-1];
+    }
+    else if (pruning_method == "min_t_ratio")
+    {
+        t_ratio_threshold = min_t_ratio;
+    }
+
+    // Find kept (not pruned) coefficient indices
+    input_indices.resize(0);
+    int offset = include_bias ? 1 : 0;
+    for (int i=0; i<nb_weights; i++)
+    {
+        if (t_ratio[i] >= t_ratio_threshold)
+        {
+            int data_index = i - offset;
+            if (data_index >= 0)  // = -1 for bias
+                input_indices.append(data_index);
+        }
+    }
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/regressors/PruningLinearRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/PruningLinearRegressor.h	2008-11-10 17:26:21 UTC (rev 9666)
+++ trunk/plearn_learners/regressors/PruningLinearRegressor.h	2008-11-10 20:04:30 UTC (rev 9667)
@@ -0,0 +1,158 @@
+// -*- C++ -*-
+
+// PruningLinearRegressor.h
+//
+// Copyright (C) 2008 Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Rejean Ducharme
+
+/*! \file PruningLinearRegressor.h */
+
+
+#ifndef PruningLinearRegressor_INC
+#define PruningLinearRegressor_INC
+
+#include <plearn_learners/regressors/LinearRegressor.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class PruningLinearRegressor : public LinearRegressor
+{
+    typedef LinearRegressor inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    string pruning_method;
+    real min_t_ratio;
+    int max_number;
+    real max_fraction;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    PruningLinearRegressor();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(PruningLinearRegressor);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+    //! t-ratio statistics for the estimator b (regression coefficients)
+    //! Saved as a learned option to allow computing statistical significance
+    //! of the weights when the model is reloaded and used in test mode.
+    Vec t_ratio;
+
+    //! Indices of inputs kept for regression
+    TVec<int> input_indices;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Utility function to compute the t-ratio for the estimator b
+    //! (regression coefficients)
+    void computeTRatio();
+
+    //! Find the dataset indices corresponding to coefficients not pruned
+    void newDatasetIndices();
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PruningLinearRegressor);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From ducharme at mail.berlios.de  Mon Nov 10 21:28:43 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 10 Nov 2008 21:28:43 +0100
Subject: [Plearn-commits] r9668 - trunk/plearn_learners/meta
Message-ID: <200811102028.mAAKShNE028374@sheep.berlios.de>

Author: ducharme
Date: 2008-11-10 21:28:43 +0100 (Mon, 10 Nov 2008)
New Revision: 9668

Modified:
   trunk/plearn_learners/meta/BaggingLearner.cc
   trunk/plearn_learners/meta/BaggingLearner.h
Log:
Add methods "setTrainStatsCollector" and "setExperimentDirectory".


Modified: trunk/plearn_learners/meta/BaggingLearner.cc
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.cc	2008-11-10 20:04:30 UTC (rev 9667)
+++ trunk/plearn_learners/meta/BaggingLearner.cc	2008-11-10 20:28:43 UTC (rev 9668)
@@ -460,8 +460,27 @@
     return outputnames;
 }
 
+////////////////////////////
+// setTrainStatsCollector //
+////////////////////////////
+void BaggingLearner::setTrainStatsCollector(PP<VecStatsCollector> statscol)
+{
+    //train_stats = statscol;
+    template_learner->setTrainStatsCollector(statscol);
+}
 
 
+////////////////////////////
+// setExperimentDirectory //
+////////////////////////////
+void BaggingLearner::setExperimentDirectory(const PPath& the_expdir)
+{
+    inherited::setExperimentDirectory(the_expdir);
+    template_learner->setExperimentDirectory(the_expdir / "BaggingSubLearner");
+}
+
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/BaggingLearner.h
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.h	2008-11-10 20:04:30 UTC (rev 9667)
+++ trunk/plearn_learners/meta/BaggingLearner.h	2008-11-10 20:28:43 UTC (rev 9668)
@@ -100,6 +100,8 @@
 
     virtual TVec<string> getOutputNames() const;
 
+    virtual void setTrainStatsCollector(PP<VecStatsCollector> statscol);
+
     //#####  PLearn::Object Protocol  #########################################
     PLEARN_DECLARE_OBJECT(BaggingLearner);
 
@@ -107,6 +109,8 @@
 
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
+    virtual void setExperimentDirectory(const PPath& the_expdir);
+
 protected:
     //#####  Protected Options  ###############################################
 



From ducharme at mail.berlios.de  Mon Nov 10 21:30:04 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 10 Nov 2008 21:30:04 +0100
Subject: [Plearn-commits] r9669 - trunk/python_modules/plearn/parallel
Message-ID: <200811102030.mAAKU4KG028613@sheep.berlios.de>

Author: ducharme
Date: 2008-11-10 21:30:04 +0100 (Mon, 10 Nov 2008)
New Revision: 9669

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Update machine list at ApSTAT.


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2008-11-10 20:28:43 UTC (rev 9668)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2008-11-10 20:30:04 UTC (rev 9669)
@@ -43,7 +43,10 @@
 # Used only for clusters of type 'ssh'. Do not enter the same machine more
 # than once: use the MAX_LOADAVG map to allow for higher maximum load
 # average than the default of 2.
-SSH_MACHINES_MAP = { 'apstat.com': [ 'embla',
+SSH_MACHINES_MAP = { 'apstat.com': [ 'lodur',
+                                     'garm',
+                                     'mimir',
+                                     'embla',
                                      'valhalla',
                                      'inari', 
                                      'kamado',
@@ -62,8 +65,13 @@
                      }
 
 # To override the default of 1
-MAX_LOADAVG = { 'inari'     : 4 ,
-                'kamado'    : 4 }
+MAX_LOADAVG = { 'inari'     : 4,
+                'kamado'    : 4,
+                'lodur'     : 4,
+                'mimir'     : 4,
+                'garm'      : 4,
+                'nai'       : 8
+              }
 
 # Do not perform a new query for the loadavg until recently launched
 # processes are likely to have started. 



From nouiz at mail.berlios.de  Tue Nov 11 22:03:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Nov 2008 22:03:33 +0100
Subject: [Plearn-commits] r9670 - trunk/plearn/vmat
Message-ID: <200811112103.mABL3Xle001975@sheep.berlios.de>

Author: nouiz
Date: 2008-11-11 22:03:33 +0100 (Tue, 11 Nov 2008)
New Revision: 9670

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/GaussianizeVMatrix.h
Log:
added option GaussianizeVMatrix::stats_file_to_use to replace the train_set. This make doing complex vmat scripts a little more simple.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-11-10 20:30:04 UTC (rev 9669)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-11-11 21:03:33 UTC (rev 9670)
@@ -40,6 +40,8 @@
 #include "GaussianizeVMatrix.h"
 #include <plearn/math/pl_erf.h>
 #include "VMat_computeStats.h"
+#include <plearn/io/load_and_save.h>
+#include <plearn/io/fileutils.h>
 
 namespace PLearn {
 using namespace std;
@@ -130,6 +132,12 @@
         "An optional VMat that will be used instead of 'source' to compute\n"
         "the transformation parameters from the distribution statistics.");
 
+    declareOption(ol, "stats_file_to_use",
+                  &GaussianizeVMatrix::stats_file_to_use,
+                  OptionBase::buildoption,
+                  "The filename of the statistics to use instead of the"
+                  " train_source.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -213,12 +221,18 @@
             the_source->setMetaDataDir(getMetaDataDir()+"source");
     }
 
-    if(!the_source->hasMetaDataDir())
+    if(!the_source->hasMetaDataDir() && stats_file_to_use.empty())
         PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "
                 " train_source, source or this VMatrix should have a metadata directory!");
     
     TVec<StatsCollector> stats;
-    if(save_and_reuse_stats)
+    if(!stats_file_to_use.empty()){
+        if(!isfile(stats_file_to_use))
+            PLERROR("In GaussianizeVMatrix::setMetaDataDir() - "
+                    "stats_file_to_use = '%s' is not a file.",
+                    stats_file_to_use.c_str());
+         PLearn::load(stats_file_to_use, stats);
+    } else if(save_and_reuse_stats)
         stats = the_source->
             getPrecomputedStatsFromFile("stats_gaussianizeVMatrix.psave", -1, true);
     else

Modified: trunk/plearn/vmat/GaussianizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.h	2008-11-10 20:30:04 UTC (rev 9669)
+++ trunk/plearn/vmat/GaussianizeVMatrix.h	2008-11-11 21:03:33 UTC (rev 9670)
@@ -72,6 +72,7 @@
     real threshold_ratio;
     bool save_and_reuse_stats;
     VMat train_source;
+    string stats_file_to_use;
 
 public:
     //#####  Public Member Functions  #########################################



From nouiz at mail.berlios.de  Tue Nov 11 22:04:42 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Nov 2008 22:04:42 +0100
Subject: [Plearn-commits] r9671 - trunk/plearn/math
Message-ID: <200811112104.mABL4gaa002039@sheep.berlios.de>

Author: nouiz
Date: 2008-11-11 22:04:42 +0100 (Tue, 11 Nov 2008)
New Revision: 9671

Modified:
   trunk/plearn/math/stats_utils.cc
Log:
added critical section for code that are not thread safe.


Modified: trunk/plearn/math/stats_utils.cc
===================================================================
--- trunk/plearn/math/stats_utils.cc	2008-11-11 21:03:33 UTC (rev 9670)
+++ trunk/plearn/math/stats_utils.cc	2008-11-11 21:04:42 UTC (rev 9671)
@@ -317,8 +317,10 @@
     {
         row1->resize(m1->length());
         row2->resize(m2->length());
-        m1->getColumn(col,row1);
-        m2->getColumn(col,row2);
+#pragma omp critical
+        m1->getColumn(col,row1);//not threadsafe!
+#pragma omp critical
+        m2->getColumn(col,row2);//not threadsafe!
         remove_missing_inplace(row1);
         remove_missing_inplace(row2);
         real D;



From nouiz at mail.berlios.de  Tue Nov 11 22:06:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Nov 2008 22:06:21 +0100
Subject: [Plearn-commits] r9672 - trunk/plearn_learners/regressors
Message-ID: <200811112106.mABL6LIU002143@sheep.berlios.de>

Author: nouiz
Date: 2008-11-11 22:06:20 +0100 (Tue, 11 Nov 2008)
New Revision: 9672

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
added comment.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-11-11 21:04:42 UTC (rev 9671)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-11-11 21:06:20 UTC (rev 9672)
@@ -50,7 +50,9 @@
 //!used to limit the memory used by limiting the length of the dataset.
 //!work with unsigned int, uint16_t, but fail with uint8_t???
 #define RTR_type uint32_t
+//!The type for the leave id
 #define RTR_type_id int16_t
+
 namespace PLearn {
 using namespace std;
 



From nouiz at mail.berlios.de  Wed Nov 12 16:00:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Nov 2008 16:00:21 +0100
Subject: [Plearn-commits] r9673 - trunk/python_modules/plearn
Message-ID: <200811121500.mACF0Lot002568@sheep.berlios.de>

Author: nouiz
Date: 2008-11-12 16:00:21 +0100 (Wed, 12 Nov 2008)
New Revision: 9673

Added:
   trunk/python_modules/plearn/collectres.py
Log:
Added a symlic to allow importing collectres.


Added: trunk/python_modules/plearn/collectres.py
===================================================================
--- trunk/python_modules/plearn/collectres.py	2008-11-11 21:06:20 UTC (rev 9672)
+++ trunk/python_modules/plearn/collectres.py	2008-11-12 15:00:21 UTC (rev 9673)
@@ -0,0 +1 @@
+link ../../scripts/collectres
\ No newline at end of file


Property changes on: trunk/python_modules/plearn/collectres.py
___________________________________________________________________
Name: svn:special
   + *



From nouiz at mail.berlios.de  Wed Nov 12 21:52:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Nov 2008 21:52:02 +0100
Subject: [Plearn-commits] r9674 - trunk/plearn/vmat
Message-ID: <200811122052.mACKq25J012382@sheep.berlios.de>

Author: nouiz
Date: 2008-11-12 21:52:01 +0100 (Wed, 12 Nov 2008)
New Revision: 9674

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
   trunk/plearn/vmat/VariableDeletionVMatrix.h
Log:
added option VariableDeletionVMatrix::warn_removed_var that print the reason why a variable is removed.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-11-12 15:00:21 UTC (rev 9673)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-11-12 20:52:01 UTC (rev 9674)
@@ -69,6 +69,7 @@
     min_non_missing_threshold(0),
     max_constant_threshold(0),
     number_of_train_samples(0),
+    warn_removed_var(false),
     deletion_threshold(-1),
     remove_columns_with_constant_value(-1)
 {}
@@ -125,6 +126,11 @@
         "If greater than or equal to 1, the integer portion will be\n"
         "interpreted as the number of samples to use.");
 
+    declareOption(ol, "warn_removed_var",
+                  &VariableDeletionVMatrix::warn_removed_var,
+                  OptionBase::buildoption,
+                  "If true, will print a warning about variable that are removed");
+
     declareOption(ol, "save_deleted_columns",
                   &VariableDeletionVMatrix::save_deleted_columns,
                   OptionBase::buildoption,
@@ -282,6 +288,11 @@
             if (stats[i].nnonmissing() >= min_non_missing 
                 && stats[i].nnonmissing() > 0)
                 indices.append(i);
+            else if (warn_removed_var)
+                PLWARNING("In VariableDeletionVMatrix::build_() var '%s'"
+                          " have too many missing value: %f/%f",
+                          source->fieldName(i).c_str(), stats[i].nmissing(),
+                          stats[i].n());
     } else
         for (int i = 0; i < is; i++)
             indices.append(i);
@@ -293,6 +304,10 @@
             StatsCollector stat = stats[i];
             if(!(stat.min()==stat.max() && stat.nnonmissing()>0))
                 final_indices.append(i);
+            else if (warn_removed_var)
+                PLWARNING("In VariableDeletionVMatrix::build_() var '%s'"
+                          " is constant with value: %f",
+                          source->fieldName(i).c_str(), stat.min());
         }
         indices.resize(final_indices.length());
         indices << final_indices; 
@@ -304,13 +319,19 @@
             map<real, StatsCollectorCounts>* counts = stats[i].getCounts();
             map<real, StatsCollectorCounts>::const_iterator it;
             bool ok = true;
+            int n;
             for (it = counts->begin(); ok && it != counts->end(); it++) {
-                int n = int(round(it->second.n));
+                n = int(round(it->second.n));
                 if (n >= max_constant_absolute)
                     ok = false;
             }
             if (ok)
                 final_indices.append(i);
+            else if (warn_removed_var)
+                PLWARNING("In VariableDeletionVMatrix::build_() var '%s'"
+                          " is too constant. Value %f happen %d/%f",
+                          source->fieldName(i).c_str(), it->first, 
+                          n, stats[i].nnonmissing());
         }
         indices.resize(final_indices.length());
         indices << final_indices;

Modified: trunk/plearn/vmat/VariableDeletionVMatrix.h
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.h	2008-11-12 15:00:21 UTC (rev 9673)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.h	2008-11-12 20:52:01 UTC (rev 9674)
@@ -58,6 +58,7 @@
     real    min_non_missing_threshold;
     real    max_constant_threshold;
     int     number_of_train_samples;
+    bool    warn_removed_var;
     VMat    train_set;
     string  save_deleted_columns;
 



From plearner at mail.berlios.de  Wed Nov 12 23:21:02 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:21:02 +0100
Subject: [Plearn-commits] r9675 - in trunk/plearn/ker: . EXPERIMENTAL
Message-ID: <200811122221.mACML2K7023277@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:21:01 +0100 (Wed, 12 Nov 2008)
New Revision: 9675

Added:
   trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc
   trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.h
Modified:
   trunk/plearn/ker/Kernel.cc
   trunk/plearn/ker/Kernel.h
Log:
Added new experimental trainable kernel


Added: trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc	2008-11-12 20:52:01 UTC (rev 9674)
+++ trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc	2008-11-12 22:21:01 UTC (rev 9675)
@@ -0,0 +1,175 @@
+// -*- C++ -*-
+
+// PartsDistanceKernel.cc
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/*! \file PartsDistanceKernel.h */
+
+/* *******************************************************      
+ * $Id: PartsDistanceKernel.cc 7675 2007-06-29 19:50:49Z tihocan $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#include "PartsDistanceKernel.h"
+
+#include <plearn/vmat/VMat_basic_stats.h>
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    PartsDistanceKernel,
+    "Implements a parts distance",
+    "");
+
+////////////////////
+// PartsDistanceKernel //
+////////////////////
+PartsDistanceKernel::PartsDistanceKernel()
+{
+    partsize = 0.9;
+    n = 2;
+    standardize = true;
+    min_stddev = 1e-8;
+    epsilon = 1e-8;
+    pow_distance = false;
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PartsDistanceKernel::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "n", &PartsDistanceKernel::n, OptionBase::buildoption, 
+                  "This class implements a Ln parts distance (L2 is the default).");
+
+    declareOption(ol, "pow_distance", &PartsDistanceKernel::pow_distance, OptionBase::buildoption, 
+                  "If set to 1 (true), the distance computed will be elevated to power n.");
+
+    declareOption(ol, "standardize", &PartsDistanceKernel::standardize, OptionBase::buildoption, 
+                  "If set to 1 (true), the inverse standard deviation inv_stddev will be used to scale the vector differences.");
+
+    declareOption(ol, "min_stddev", &PartsDistanceKernel::min_stddev, OptionBase::buildoption, 
+                  "When method train computes inv_stddev, it will set it to FLT_MAX for \n"
+                  "any component for which the standard deviation is below min_stddev");
+
+    declareOption(ol, "epsilon", &PartsDistanceKernel::epsilon, OptionBase::buildoption, 
+                  "This is added to the absolute value of the elementwise difference between the 2 vectors.\n" 
+                  "It's especially important for this to be non-zero when standardizing.");
+
+    declareOption(ol, "partsize", &PartsDistanceKernel::partsize, OptionBase::buildoption, 
+                  "This determines the number of elements (the size of the part) that will be used to compute the distance.\n"
+                  "If >=1 it's interpreted as anabsolute number of elements. If it's <1 it's taken to mean a fraction of all the elements.\n");
+
+    declareOption(ol, "inv_stddev", &PartsDistanceKernel::inv_stddev, OptionBase::learntoption, 
+                  "This is computed when calling method train, and will be used when evaluating distances only if standardize is true.\n"
+                  "It crresponds to the inverse of the standard deviation of inputs in the dataset passed to train. But see also min_stddev.\n");
+
+    inherited::declareOptions(ol);
+}
+
+void PartsDistanceKernel::train(VMat data)
+{    
+    Vec meanvec;
+    Vec stddev;
+    computeInputMeanAndStddev(data, meanvec, stddev, 0.0);
+    int l = stddev.length();
+    for(int i=0; i<l; i++)
+    {
+        if(stddev[i]<min_stddev)
+            inv_stddev[i] = FLT_MAX;
+        else
+            inv_stddev[i] = 1/stddev[i]; 
+    }
+}
+
+
+//////////////
+// evaluate //
+//////////////
+real PartsDistanceKernel::evaluate(const Vec& x1, const Vec& x2) const {
+    int l = x1.length();
+    if(x2.length() != l)
+        PLERROR("vectors x1 and x2 must have the same size");
+    
+    if(standardize && l!=inv_stddev.length())
+        PLERROR("In PartsDistanceKernel::evaluate, size of vectors (%d) does not match size of inv_stddev (%d). Make sure you called train on the kernel with appropriate dataset",l,inv_stddev.length());
+        
+    elementdist.resize(l);
+    for(int i; i<l; i++)
+    {
+        real d = FLT_MAX;
+        if( !(standardize && inv_stddev[i]>=FLT_MAX) )
+        {
+            d = abs(x1[i]-x2[i])+epsilon;
+            if(standardize)
+                d *= inv_stddev[i];
+            
+            if(fast_exact_is_equal(n,2))
+                d *= d;
+            else if(!fast_exact_is_equal(n,1))
+                d = mypow(d,n);
+        }
+        elementdist[i] = d;     
+    }
+    
+    sortElements(elementdist);
+    int ps = (partsize>=1 ? (int)partsize :(int)(partsize*l));
+    real res = 0;
+    for(int i=0; i<ps; i++)
+    {
+        real d = elementdist[i];
+        if(d<FLT_MAX)
+            res += d;
+    }
+
+    if(!pow_distance)
+        res = mypow(res, 1/n);
+
+    return res;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.h
===================================================================
--- trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.h	2008-11-12 20:52:01 UTC (rev 9674)
+++ trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.h	2008-11-12 22:21:01 UTC (rev 9675)
@@ -0,0 +1,99 @@
+// -*- C++ -*-
+
+// PartsDistanceKernel.h
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/*! \file PartsDistanceKernel.h */
+
+
+/* *******************************************************      
+ * $Id: PartsDistanceKernel.h 7664 2007-06-28 19:47:30Z nouiz $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#ifndef PartsDistanceKernel_INC
+#define PartsDistanceKernel_INC
+
+#include <plearn/ker/Kernel.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This class implements an Ln distance (defaults to L2 i.e. euclidean distance).
+class PartsDistanceKernel: public Kernel
+{
+
+private:
+
+    typedef Kernel inherited;
+
+public:
+
+    real partsize;
+    real n;  //!<  1 for L1, 2 for L2, etc...
+    bool standardize;
+    real min_stddev;
+    real epsilon;
+    bool pow_distance;
+
+    Vec inv_stddev;
+
+    PartsDistanceKernel();
+
+    virtual void train(VMat data);
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+    
+    PLEARN_DECLARE_OBJECT(PartsDistanceKernel);
+
+protected:
+    mutable Vec elementdist;
+
+    static void declareOptions(OptionList& ol);
+};
+
+DECLARE_OBJECT_PTR(PartsDistanceKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2008-11-12 20:52:01 UTC (rev 9674)
+++ trunk/plearn/ker/Kernel.cc	2008-11-12 22:21:01 UTC (rev 9675)
@@ -167,6 +167,12 @@
 }
 
 ////////////////////////////
+// train                  //
+////////////////////////////
+void Kernel::train(VMat data)
+{}
+
+////////////////////////////
 // setDataForKernelMatrix //
 ////////////////////////////
 void Kernel::setDataForKernelMatrix(VMat the_data)

Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2008-11-12 20:52:01 UTC (rev 9674)
+++ trunk/plearn/ker/Kernel.h	2008-11-12 22:21:01 UTC (rev 9675)
@@ -94,6 +94,12 @@
     //!  ** Subclasses must override this method **
     virtual real evaluate(const Vec& x1, const Vec& x2) const = 0; //!<  returns K(x1,x2) 
 
+    /** Subclasses may override this method for kernels that can be trained
+     *  on a dataset prior to being used (e.g. a Mahalanobis distance which could learn the empirical covariance)
+     * Note: in general only the input and weight part of the vmat are used 
+     */
+    virtual void train(VMat data);
+
     //!  ** Subclasses may override these methods to provide efficient kernel matrix access **
 
     /**    



From plearner at mail.berlios.de  Wed Nov 12 23:21:58 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:21:58 +0100
Subject: [Plearn-commits] r9676 - trunk/plearn/vmat
Message-ID: <200811122221.mACMLwgw023359@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:21:58 +0100 (Wed, 12 Nov 2008)
New Revision: 9676

Modified:
   trunk/plearn/vmat/FileVMatrix.cc
Log:
Added explicit cast to remove compiler warning


Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-11-12 22:21:01 UTC (rev 9675)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-11-12 22:21:58 UTC (rev 9676)
@@ -283,7 +283,7 @@
         if(info.size!=expectedsize)
             PLWARNING("In FileVMatrix::build_() - The file '%s' have a size"
                       " of %ld, expected %ld",
-                      filename_.c_str(), info.size, expectedsize);
+                      filename_.c_str(), (long int)info.size, (long int)expectedsize);
 #endif
         
     }



From plearner at mail.berlios.de  Wed Nov 12 23:22:48 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:22:48 +0100
Subject: [Plearn-commits] r9677 - trunk/doc
Message-ID: <200811122222.mACMMm2f023438@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:22:48 +0100 (Wed, 12 Nov 2008)
New Revision: 9677

Modified:
   trunk/doc/installation_guide.tex
Log:
minor update to installation_guide 


Modified: trunk/doc/installation_guide.tex
===================================================================
--- trunk/doc/installation_guide.tex	2008-11-12 22:21:58 UTC (rev 9676)
+++ trunk/doc/installation_guide.tex	2008-11-12 22:22:48 UTC (rev 9677)
@@ -580,10 +580,11 @@
 If it doesn't work, you may have to adapt the configuration file to your system
 (PLearn/.pymake/config)
 
-If it does work, you can try with more dependencies to have more fonctionality \
+You can try with more dependencies to have more fonctionality \
 with the commands \verb!pymake plearn_lapack.cc!, \verb!pymake plearn_curses.cc!\
- or even \verb!pymake plearn_python.cc!
+ or even \verb!pymake plearn_python.cc! or \verb!pymake plearn_full.cc!
 
+To compile the plearn python extension module, do \verb!make_plearn_python_ext!
 
 \section{Installation on Windows with cygwin}
 \label{sec:windows}



From plearner at mail.berlios.de  Wed Nov 12 23:23:36 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:23:36 +0100
Subject: [Plearn-commits] r9678 - in trunk/examples: . PythonExtension
Message-ID: <200811122223.mACMNaS3023482@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:23:36 +0100 (Wed, 12 Nov 2008)
New Revision: 9678

Added:
   trunk/examples/PythonExtension/
   trunk/examples/PythonExtension/logistic_regression.py
Log:
Added simple demo for using plearn as python extension 


Added: trunk/examples/PythonExtension/logistic_regression.py
===================================================================
--- trunk/examples/PythonExtension/logistic_regression.py	2008-11-12 22:22:48 UTC (rev 9677)
+++ trunk/examples/PythonExtension/logistic_regression.py	2008-11-12 22:23:36 UTC (rev 9678)
@@ -0,0 +1,46 @@
+# logistic_regression.py
+# Author: Pascal Vincent (2008)
+
+# Simple demo using the plearn python extension module to train a simple logistic regression
+# (uses NNet with no hidden layer)
+# For this to work, the plearn python extension module must have been compiled:
+# make_plearn_python_ext
+
+
+import plearn.pyext.plext as pl
+from numpy import array
+from math import exp
+
+def sigmoid(x):
+    return exp(x)/(1+exp(x))
+
+
+traindata = array([
+    [.1,   1],
+    [-2.5, 1],
+    [2,  0],
+    [2.5,  0]])
+
+learner = pl.NNet(output_transfer_func = "sigmoid",
+                  cost_funcs = ["stable_cross_entropy"],
+                  noutputs = 1,
+                  batch_size = 0,
+                  optimizer = pl.ConjGradientOptimizer(),
+                  nstages = 1000,
+                  verbosity = 3)
+trainset = pl.MemoryVMatrix(data = traindata, inputsize=1, targetsize=1, weightsize=0)
+#learner.setExperimentDirectory('myexp')
+learner.setTrainingSet(trainset, True)
+learner.train()
+params = learner.wout.value
+print "Learnt parameters:",params
+b, w = params.flat
+print "----------"
+x = 1.2
+print "Computation using computeOutput:", learner.computeOutput(array([x]))
+print "Computation by hand using learnt parameters:",sigmoid(w*x+b)
+print "Outputs:"
+print learner.computeOutputs(traindata[:,0:1])
+outputs, costs = learner.computeOutputsAndCosts(traindata[:,0:1],traindata[:,1:2])
+print "Costs:",costs
+



From plearner at mail.berlios.de  Wed Nov 12 23:25:54 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:25:54 +0100
Subject: [Plearn-commits] r9679 - trunk/plearn_learners/classifiers
Message-ID: <200811122225.mACMPsNI023613@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:25:53 +0100 (Wed, 12 Nov 2008)
New Revision: 9679

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
Log:
Minor change to KNNClassifier so it does invoke train() on the kernel (some kernels may require a pretraining on the data set before they can be evaluated).


Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-12 22:23:36 UTC (rev 9678)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-12 22:25:53 UTC (rev 9679)
@@ -200,6 +200,9 @@
     knn->copy_target = true;
     knn->copy_weight = true;
     knn->copy_index  = false;
+    // we call train on the knn->distance_kernel. We do it here in KNNClassifier::setTrainingSet (rather than in KNNClassifier::train)
+    // in case the knn->setTrainingSet that is invoked just afterwards calls distance_kernel computations already.
+    knn->distance_kernel->train(training_set);
     knn->setTrainingSet(training_set,call_forget);
     knn_costs.resize(num_neighbors); // Changed for compatibility with HyperLearner
     //knn_costs.resize(knn->nTestCosts());



From plearner at mail.berlios.de  Wed Nov 12 23:27:37 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:27:37 +0100
Subject: [Plearn-commits] r9680 - trunk/plearn_learners/unsupervised
Message-ID: <200811122227.mACMRbrL023715@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:27:37 +0100 (Wed, 12 Nov 2008)
New Revision: 9680

Modified:
   trunk/plearn_learners/unsupervised/NormalizationLearner.cc
   trunk/plearn_learners/unsupervised/NormalizationLearner.h
Log:
Important upgrade to NormalizationLearner that can now also chose to exclude some input features based on their marginal statistics 
(too small standard deviation, or too large fration of missing values).


Modified: trunk/plearn_learners/unsupervised/NormalizationLearner.cc
===================================================================
--- trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-12 22:25:53 UTC (rev 9679)
+++ trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-12 22:27:37 UTC (rev 9680)
@@ -44,17 +44,23 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     NormalizationLearner,
-    "This learner performs normalization of its input (subtracting the mean and dividing by stddev)",
-    "NormalizationLearner produces as output a normalized version of its input\n"
+    "This learner can perform normalization of its input (subtracting the mean and dividing by stddev) "
+    "and can also exclude input components that have too low standard deviation, or too many missing values.",
+    "NormalizationLearner produces as output a possibly normalized version of its input\n"
     "obtained by subtracting the mean and dividing by the standard deviation.\n"
-    "It also has a simple policy switch for the handling of missing values.\n"
+    "It can also exclude input components whose standard deviation is below a specified value,\n"
+    "or whose missing values exceed a certain proportion of times.",
+    "It also has a simple policy switch for deciding to keep missing values as is or replace them by 0.\n"
     "It is typically used as an early preprocessing step in a ChainedLearner \n"
     "NOTE: you may also consider using PCA(normalize=1), if you wan to obtain \n"
     "decorrelated 'sphered' data." );
 
 NormalizationLearner::NormalizationLearner()
     :min_allowed_stddev(1e-6),
-     set_missing_to_zero(true)
+     remove_components_with_stddev_smaller_than(-1),
+     remove_components_whose_missing_proportion_exceeds(1),
+     set_missing_to_zero(true),
+     do_normalize(true)
 {
 }
 
@@ -65,11 +71,28 @@
                   "If the empirical standard deviation is lower than this, we'll use this value to \n"
                   "compute inv_stddev (this is to prevent getting too large or even infinite values for inv_stddev");
 
+    declareOption(ol, "remove_components_with_stddev_smaller_than", &NormalizationLearner::remove_components_with_stddev_smaller_than,
+                  OptionBase::buildoption,
+                  "Components of the input whose stddev is strictly below that value will be excluded from the output\n");
+
+    declareOption(ol, "remove_components_whose_missing_proportion_exceeds", &NormalizationLearner::remove_components_whose_missing_proportion_exceeds,
+                  OptionBase::buildoption,
+                  "Components of the input that are missing more than that given fraction of times will be excluded from the output\n"
+                  "The default 1 means no component will be excluded for being missing.\n"
+                  "At the other extreme 0 means any component that was missing at east once wil be excluded\n"
+                  "0.75 would exclude components that are missing more than 75\% of the time\n");
+
+    declareOption(ol, "do_normalize", &NormalizationLearner::do_normalize,
+                  OptionBase::buildoption,
+                  "If true (the default) then subtract mean and divide by stddev.\n"
+                  "It can be useful to set this to false if all you want to do is remove components with small\n"
+                  "stddev (see option remove_components with_small_stddev) but leave the others untouched.");
+
     declareOption(ol, "set_missing_to_zero", &NormalizationLearner::set_missing_to_zero,
                   OptionBase::buildoption,
                   "How to handle missing values: \n"
-                  "  If true (the default), missing values will be replaced by \n"
-                  "  the post-normalization mean, which is 0. \n"
+                  "  If true (the default), missing values will be replaced by 0\n"
+                  "  (this corresponds to post-normalization mean if indeed we d_normakize) \n"
                   "  If false missing values will be left as missing values. \n");
 
     declareOption(ol, "meanvec", &NormalizationLearner::meanvec,
@@ -80,10 +103,16 @@
                   OptionBase::learntoption,
                   "The vector of factors by which to multiply (input-meanvec)\n");
 
+    declareOption(ol, "kept_components", &NormalizationLearner::kept_components,
+                  OptionBase::learntoption,
+                  "The indices of the input components kept in the final output\n");
+
     declareOption(ol, "inputnames", &NormalizationLearner::inputnames,
                   OptionBase::learntoption,
                   "We store the inputnames, which are also the outputnames\n");
 
+
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -100,6 +129,14 @@
     // ###    options have been modified.
     // ### You should assume that the parent class' build_() has already been
     // ### called.
+
+    int d = meanvec.length();
+    if(d>0 && kept_components.length()==0) // fill uninitialized kept_components
+    {
+        kept_components.resize(d);
+        for(int k=0; k<d; k++)
+            kept_components[k] = k;
+    }
 }
 
 // ### Nothing to add here, simply calls build_
@@ -127,7 +164,7 @@
 
 int NormalizationLearner::outputsize() const
 {
-    return meanvec.length();
+    return kept_components.length();
 }
 
 void NormalizationLearner::forget()
@@ -169,8 +206,18 @@
         st.getMean(meanvec);
         Vec stddev = st.getStdDev();
         inv_stddev.resize(n);
+        kept_components.resize(n);
+        kept_components.resize(0);
         for(int k=0; k<n; k++)
-            inv_stddev[k] = 1.0/max(min_allowed_stddev, stddev[k]);
+        {
+            const StatsCollector& stk = st.stat[k];
+            real sd = stk.stddev();
+            inv_stddev[k] = 1/max(min_allowed_stddev,sd);
+            double missing_proportion = (double)st.nmissing()/(double)l;
+            if( (missing_proportion<=remove_components_whose_missing_proportion_exceeds)
+                && (sd>=remove_components_with_stddev_smaller_than) )
+                kept_components.append(k);
+        }
         ++stage;
         train_stats->finalize(); 
     }
@@ -182,19 +229,22 @@
     int n = meanvec.length();
     if(input.length()!=n)
         PLERROR("length of input differs from length of meanvec!");
-    output.resize(n);
+    int n2 = kept_components.length();
+    output.resize(n2);
     real* p_input = input.data();
     real* p_output = output.data();
     real* p_meanvec = meanvec.data();
     real* p_inv_stddev = inv_stddev.data();
+    int*  p_kept_components = kept_components.data();
 
-    for(int k=0; k<n; k++)
+    for(int k=0; k<n2; k++)
     {
-        real val = p_input[k];
+        int pos = p_kept_components[k];
+        real val = p_input[pos];
         if(is_missing(val))
             p_output[k] = set_missing_to_zero?0.:val;
-        else
-            p_output[k] = p_inv_stddev[k]*(val - p_meanvec[k]);
+        else if(do_normalize)
+            p_output[k] = p_inv_stddev[pos]*(val - p_meanvec[pos]);
     }
 }
 
@@ -216,9 +266,20 @@
 
 TVec<string> NormalizationLearner::getOutputNames() const
 {
-    return inputnames;
+    TVec<string> outnames;
+    if(kept_components.length()==inputnames.length())
+        outnames = inputnames;
+    else
+    {
+        int n2 = kept_components.length();
+        outnames.resize(n2);
+        for(int k=0; k<n2; k++)
+            outnames[k] = inputnames[kept_components[k]];
+    }
+    return outnames;
 }
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/unsupervised/NormalizationLearner.h
===================================================================
--- trunk/plearn_learners/unsupervised/NormalizationLearner.h	2008-11-12 22:25:53 UTC (rev 9679)
+++ trunk/plearn_learners/unsupervised/NormalizationLearner.h	2008-11-12 22:27:37 UTC (rev 9680)
@@ -66,12 +66,16 @@
 
     // Build options
     real min_allowed_stddev;
+    double remove_components_with_stddev_smaller_than;
+    double remove_components_whose_missing_proportion_exceeds;
     bool set_missing_to_zero;
+    bool do_normalize;
 
     // Learnt option
     Vec meanvec;
     Vec inv_stddev;
     TVec<string> inputnames;
+    TVec<int> kept_components;
 
 public:
     //#####  Public Member Functions  #########################################



From plearner at mail.berlios.de  Wed Nov 12 23:28:29 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:28:29 +0100
Subject: [Plearn-commits] r9681 - trunk/python_modules/plearn/pyplearn
Message-ID: <200811122228.mACMSTh7023756@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:28:28 +0100 (Wed, 12 Nov 2008)
New Revision: 9681

Modified:
   trunk/python_modules/plearn/pyplearn/tutorial.py
Log:
minor fix


Modified: trunk/python_modules/plearn/pyplearn/tutorial.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/tutorial.py	2008-11-12 22:27:37 UTC (rev 9680)
+++ trunk/python_modules/plearn/pyplearn/tutorial.py	2008-11-12 22:28:28 UTC (rev 9681)
@@ -710,8 +710,8 @@
 #  Imports
 #
 import inspect, os
-from pyplearn                  import *
-from pyplearn                  import _plargs_storage_readonly
+from plearn.pyplearn                  import *
+from plearn.pyplearn                  import _plargs_storage_readonly
 from plearn_repr               import plearn_repr
 from PyPLearnObject            import PLOption, PyPLearnObject
 from plearn.utilities.Tutorial import *



From plearner at mail.berlios.de  Wed Nov 12 23:28:53 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 12 Nov 2008 23:28:53 +0100
Subject: [Plearn-commits] r9682 - trunk/commands/EXPERIMENTAL
Message-ID: <200811122228.mACMSrXQ023799@sheep.berlios.de>

Author: plearner
Date: 2008-11-12 23:28:53 +0100 (Wed, 12 Nov 2008)
New Revision: 9682

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
Log:
some more experimental stuff


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-11-12 22:28:28 UTC (rev 9681)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-11-12 22:28:53 UTC (rev 9682)
@@ -150,7 +150,7 @@
 // //#include <plearn_learners/generic/DistRepNNet.h>
 // #include <plearn_learners/generic/NNet.h>
 // #include <plearn_learners/generic/SelectInputSubsetLearner.h>
-// #include <plearn_learners/generic/ChainedLearners.h>
+#include <plearn_learners/generic/ChainedLearners.h>
 // #include <plearn_learners/generic/StackedLearner.h>
 // #include <plearn_learners/generic/TestingLearner.h>
 // #include <plearn_learners/generic/VPLPreprocessedLearner.h>
@@ -357,7 +357,7 @@
 #include <plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h>
 
 
-// Stuff used for DeepReconstrctorNet experiments
+// Stuff used for DeepReconstructorNet experiments
 #include <plearn/var/Variable.h>
 #include <plearn/var/SquareVariable.h>
 #include <plearn/math/TVec_impl.h>
@@ -391,12 +391,13 @@
 // Stuff used for transformationLearner experiments
 #include <plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h>
 
-// Stuff used for local Gaussian classifier
+// Stuff used for local Gaussian classifier and for knn with PartsDistanceKernel
 #include <plearn_learners/classifiers/KNNClassifier.h>
+#include <plearn/ker/EXPERIMENTAL/PartsDistanceKernel.h>
 #include <plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h>
 
 // Stuff used for DiverseComponentAnalysis
-#include <plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h>
+// #include <plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h>
 
 
 using namespace PLearn;



From plearner at mail.berlios.de  Thu Nov 13 00:31:22 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 13 Nov 2008 00:31:22 +0100
Subject: [Plearn-commits] r9683 - in trunk: commands/EXPERIMENTAL
	plearn_learners/unsupervised
Message-ID: <200811122331.mACNVMEV029365@sheep.berlios.de>

Author: plearner
Date: 2008-11-13 00:31:19 +0100 (Thu, 13 Nov 2008)
New Revision: 9683

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn_learners/unsupervised/NormalizationLearner.cc
Log:
Fixed NormalizationLearner typos that prevented compilation


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-11-12 22:28:53 UTC (rev 9682)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-11-12 23:31:19 UTC (rev 9683)
@@ -134,6 +134,10 @@
 //  * PLearner *
 //  ************/
 
+// // Unsupervised preprocessing
+// #include <plearn_learners/unsupervised/UniformizeLearner.h>
+#include <plearn_learners/unsupervised/NormalizationLearner.h>
+
 // // Classifiers
 // #include <plearn_learners/classifiers/BinaryStump.h>
 // #include <plearn_learners/classifiers/ClassifierFromConditionalPDistribution.h>
@@ -171,8 +175,6 @@
 // #include <plearn_learners/regressors/KNNRegressor.h>
 // #include <plearn_learners/regressors/RankLearner.h>
 // #include <plearn_learners/regressors/RegressorFromDistribution.h>
-// // Unsupervised
-// #include <plearn_learners/unsupervised/UniformizeLearner.h>
 
 // // PDistribution
 // #include <plearn_learners/distributions/SpiralDistribution.h>

Modified: trunk/plearn_learners/unsupervised/NormalizationLearner.cc
===================================================================
--- trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-12 22:28:53 UTC (rev 9682)
+++ trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-12 23:31:19 UTC (rev 9683)
@@ -49,7 +49,7 @@
     "NormalizationLearner produces as output a possibly normalized version of its input\n"
     "obtained by subtracting the mean and dividing by the standard deviation.\n"
     "It can also exclude input components whose standard deviation is below a specified value,\n"
-    "or whose missing values exceed a certain proportion of times.",
+    "or whose missing values exceed a certain proportion of times."
     "It also has a simple policy switch for deciding to keep missing values as is or replace them by 0.\n"
     "It is typically used as an early preprocessing step in a ChainedLearner \n"
     "NOTE: you may also consider using PCA(normalize=1), if you wan to obtain \n"
@@ -204,16 +204,15 @@
         st.finalize();
 
         st.getMean(meanvec);
-        Vec stddev = st.getStdDev();
         inv_stddev.resize(n);
         kept_components.resize(n);
         kept_components.resize(0);
         for(int k=0; k<n; k++)
         {
-            const StatsCollector& stk = st.stat[k];
+            const StatsCollector& stk = st.stats[k];
             real sd = stk.stddev();
             inv_stddev[k] = 1/max(min_allowed_stddev,sd);
-            double missing_proportion = (double)st.nmissing()/(double)l;
+            double missing_proportion = (double)stk.nmissing()/(double)l;
             if( (missing_proportion<=remove_components_whose_missing_proportion_exceeds)
                 && (sd>=remove_components_with_stddev_smaller_than) )
                 kept_components.append(k);



From plearner at mail.berlios.de  Thu Nov 13 00:37:06 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 13 Nov 2008 00:37:06 +0100
Subject: [Plearn-commits] r9684 - trunk/plearn_learners/unsupervised
Message-ID: <200811122337.mACNb6Ft003500@sheep.berlios.de>

Author: plearner
Date: 2008-11-13 00:37:05 +0100 (Thu, 13 Nov 2008)
New Revision: 9684

Modified:
   trunk/plearn_learners/unsupervised/NormalizationLearner.cc
Log:
typo fix


Modified: trunk/plearn_learners/unsupervised/NormalizationLearner.cc
===================================================================
--- trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-12 23:31:19 UTC (rev 9683)
+++ trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-12 23:37:05 UTC (rev 9684)
@@ -80,7 +80,7 @@
                   "Components of the input that are missing more than that given fraction of times will be excluded from the output\n"
                   "The default 1 means no component will be excluded for being missing.\n"
                   "At the other extreme 0 means any component that was missing at east once wil be excluded\n"
-                  "0.75 would exclude components that are missing more than 75\% of the time\n");
+                  "0.75 would exclude components that are missing more than 75 percent of the time\n");
 
     declareOption(ol, "do_normalize", &NormalizationLearner::do_normalize,
                   OptionBase::buildoption,



From plearner at mail.berlios.de  Thu Nov 13 00:49:26 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 13 Nov 2008 00:49:26 +0100
Subject: [Plearn-commits] r9685 - trunk/plearn/io
Message-ID: <200811122349.mACNnQxt018745@sheep.berlios.de>

Author: plearner
Date: 2008-11-13 00:49:25 +0100 (Thu, 13 Nov 2008)
New Revision: 9685

Modified:
   trunk/plearn/io/PStream.cc
Log:
Handling \ escape characters insmartReadUntilNext


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2008-11-12 23:37:05 UTC (rev 9684)
+++ trunk/plearn/io/PStream.cc	2008-11-12 23:49:25 UTC (rev 9685)
@@ -386,7 +386,12 @@
                 break;
             case '"':
                 smartReadUntilNext("\"", characters_read, ignore_brackets, false);
-                characters_read+= '"';
+                characters_read+= '"';          
+                break; 
+            case '\\':   // we consider backslash an escape character, 
+                c=get(); // thus the next character is read and appended without being interpreted
+                if(c!=EOF)
+                    characters_read += static_cast<char>(c);
                 break;
             }
         }



From ducharme at mail.berlios.de  Thu Nov 13 17:29:59 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 13 Nov 2008 17:29:59 +0100
Subject: [Plearn-commits] r9686 - trunk/plearn_learners/regressors
Message-ID: <200811131629.mADGTxTi016340@sheep.berlios.de>

Author: ducharme
Date: 2008-11-13 17:29:59 +0100 (Thu, 13 Nov 2008)
New Revision: 9686

Modified:
   trunk/plearn_learners/regressors/AutoLinearRegressor.cc
Log:
Bug fix.


Modified: trunk/plearn_learners/regressors/AutoLinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/AutoLinearRegressor.cc	2008-11-12 23:49:25 UTC (rev 9685)
+++ trunk/plearn_learners/regressors/AutoLinearRegressor.cc	2008-11-13 16:29:59 UTC (rev 9686)
@@ -176,11 +176,7 @@
         int ninputs = train_set->inputsize();
         int ntargets = train_set->targetsize();
         int nweights = train_set->weightsize();
-        
-        // extended inputs
-        int insize = ninputs; 
 
-
         Mat tset = train_set->toMatCopy();
         int l = tset.length();
         
@@ -202,6 +198,7 @@
             columnMean(Y, mean_target);
         Y -= mean_target;
 
+        int insize = ninputs + (include_bias ? 1 : 0); 
         //weights.resize(insize, ntargets);
         weights.resize(ntargets, insize);
         real best_GCV;



From nouiz at mail.berlios.de  Fri Nov 14 20:31:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Nov 2008 20:31:01 +0100
Subject: [Plearn-commits] r9687 - trunk/plearn_learners/hyper
Message-ID: <200811141931.mAEJV1LG014093@sheep.berlios.de>

Author: nouiz
Date: 2008-11-14 20:31:00 +0100 (Fri, 14 Nov 2008)
New Revision: 9687

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
added timing of auto_load and auto_save.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-13 16:29:59 UTC (rev 9686)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-14 19:31:00 UTC (rev 9687)
@@ -43,6 +43,7 @@
 #include <plearn/base/PLearnDiff.h>
 #include <plearn/io/load_and_save.h>
 #include <plearn/vmat/FileVMatrix.h>
+#include <plearn/sys/Profiler.h>
 
 namespace PLearn {
 
@@ -337,6 +338,7 @@
 ///////////////
 void HyperLearner::auto_save()
 {
+    Profiler::pl_profile_start("HyperLearner::auto_save");
     if(expdir.isEmpty())
         PLERROR("In HyperLearner::auto_save - we can't auto_save as"
                 " we don't have any expdir");
@@ -355,6 +357,7 @@
 #endif
 
     mvforce(tmp,f);
+    Profiler::pl_profile_end("HyperLearner::auto_save");
 }
 
 ///////////////
@@ -374,12 +377,14 @@
     PPath f = expdir/"hyper_learner_auto_save.psave";
     bool isf=isfile(f);
     if(stage==0 && !reloading && !reloaded && isf){
+        Profiler::pl_profile_start("HyperLearner::auto_load");
         if(verbosity>0)
             PLWARNING("In HyperLearner::auto_load() - reloading from file %s",f.c_str());
         reloading = true;
         PLearn::load(f,*this);
         reloading = false;
         reloaded = true;
+        Profiler::pl_profile_end("HyperLearner::auto_load");
     }
     else if(!isf && verbosity>1)
         PLWARNING("In HyperLearner::auto_load() - no file to reload.");



From ducharme at mail.berlios.de  Fri Nov 14 22:37:43 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 14 Nov 2008 22:37:43 +0100
Subject: [Plearn-commits] r9688 - in trunk/plearn_learners: generic meta
	regressors
Message-ID: <200811142137.mAELbhrs026287@sheep.berlios.de>

Author: ducharme
Date: 2008-11-14 22:37:42 +0100 (Fri, 14 Nov 2008)
New Revision: 9688

Modified:
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/StackedLearner.cc
   trunk/plearn_learners/meta/BaggingLearner.cc
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
Log:
Meta learners must call inherited::setTrainStatsCollector inside their own method.


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-11-14 19:31:00 UTC (rev 9687)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-11-14 21:37:42 UTC (rev 9688)
@@ -129,7 +129,7 @@
 void PLearner::declareOptions(OptionList& ol)
 {
     declareOption(
-        ol, "expdir", &PLearner::expdir, OptionBase::buildoption | OptionBase::nosave, 
+        ol, "expdir", &PLearner::expdir, OptionBase::buildoption | OptionBase::nosave | OptionBase::remotetransmit, 
         "Path of the directory associated with this learner, in which\n"
         "it should save any file it wishes to create. \n"
         "The directory will be created if it does not already exist.\n"
@@ -1242,8 +1242,8 @@
     if (!train_stats)
         train_stats = new VecStatsCollector();
 
-    // Set names of train_stats costs
-    train_stats->setFieldNames(getTrainCostNames());
+    // Meta learners may need to set the stats_collector of their sub-learners
+    setTrainStatsCollector(train_stats);
 
     // Everything is fine.
     return true;

Modified: trunk/plearn_learners/generic/StackedLearner.cc
===================================================================
--- trunk/plearn_learners/generic/StackedLearner.cc	2008-11-14 19:31:00 UTC (rev 9687)
+++ trunk/plearn_learners/generic/StackedLearner.cc	2008-11-14 21:37:42 UTC (rev 9688)
@@ -152,7 +152,7 @@
 
 void StackedLearner::setTrainStatsCollector(PP<VecStatsCollector> statscol)
 {
-    train_stats = statscol;
+    inherited::setTrainStatsCollector(statscol);
     if (combiner)
         combiner->setTrainStatsCollector(statscol);
 }

Modified: trunk/plearn_learners/meta/BaggingLearner.cc
===================================================================
--- trunk/plearn_learners/meta/BaggingLearner.cc	2008-11-14 19:31:00 UTC (rev 9687)
+++ trunk/plearn_learners/meta/BaggingLearner.cc	2008-11-14 21:37:42 UTC (rev 9688)
@@ -465,7 +465,7 @@
 ////////////////////////////
 void BaggingLearner::setTrainStatsCollector(PP<VecStatsCollector> statscol)
 {
-    //train_stats = statscol;
+    inherited::setTrainStatsCollector(statscol);
     template_learner->setTrainStatsCollector(statscol);
 }
 

Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-11-14 19:31:00 UTC (rev 9687)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2008-11-14 21:37:42 UTC (rev 9688)
@@ -1308,7 +1308,7 @@
 
 void BasisSelectionRegressor::setTrainStatsCollector(PP<VecStatsCollector> statscol)
 { 
-    train_stats = statscol;
+    inherited::setTrainStatsCollector(statscol);
     template_learner->setTrainStatsCollector(statscol);
 }
 



From ducharme at mail.berlios.de  Fri Nov 14 23:06:35 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 14 Nov 2008 23:06:35 +0100
Subject: [Plearn-commits] r9689 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200811142206.mAEM6ZRj028897@sheep.berlios.de>

Author: ducharme
Date: 2008-11-14 23:06:35 +0100 (Fri, 14 Nov 2008)
New Revision: 9689

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
Update following last commit of PLearner.cc ("expdir" has now attribute "OptionBase::remotetransmit")


Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2008-11-14 21:37:42 UTC (rev 9688)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2008-11-14 22:06:35 UTC (rev 9689)
@@ -87,6 +87,7 @@
 20 	
 ]
 ;
+expdir = "" ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 10 ;



From nouiz at mail.berlios.de  Tue Nov 18 16:14:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Nov 2008 16:14:11 +0100
Subject: [Plearn-commits] r9690 - trunk/python_modules/plearn/parallel
Message-ID: <200811181514.mAIFEBH1019887@sheep.berlios.de>

Author: nouiz
Date: 2008-11-18 16:14:10 +0100 (Tue, 18 Nov 2008)
New Revision: 9690

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
in the cluster and condor backend the os is transformed automatically to lower or upper case.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-11-14 22:06:35 UTC (rev 9689)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 15:14:10 UTC (rev 9690)
@@ -398,6 +398,9 @@
         self.mem=None
         self.os=None
         DBIBase.__init__(self, commands, **args)
+
+        self.os = self.os.lower()
+
         self.pre_tasks=["echo '[DBI] executing on host' $HOSTNAME"]+self.pre_tasks
         self.post_tasks=["echo '[DBI] exit status' $?"]+self.post_tasks
         self.add_commands(commands)
@@ -722,6 +725,8 @@
 
         DBIBase.__init__(self, commands, **args)
         self.mem=int(self.mem)*1024
+
+        self.os = self.os.upper()
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
 
@@ -893,6 +898,7 @@
                 else:
                     out.write(line_header()+
                               "renew the launch file "+self.launch_file+"\n")
+                    out.flush()
                     self.make_launch_script(bash_exec, True)
                 out.flush()
                 #we do this as in some case(with dagman) the log file can 



From nouiz at mail.berlios.de  Tue Nov 18 17:50:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Nov 2008 17:50:14 +0100
Subject: [Plearn-commits] r9691 - trunk/plearn/misc
Message-ID: <200811181650.mAIGoE4p029341@sheep.berlios.de>

Author: nouiz
Date: 2008-11-18 17:50:13 +0100 (Tue, 18 Nov 2008)
New Revision: 9691

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
in plearn vmat view added the KEYS f and F. It toggle the display of the filename at the bottom of the screen. 


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2008-11-18 15:14:10 UTC (rev 9690)
+++ trunk/plearn/misc/viewVMat.cc	2008-11-18 16:50:13 UTC (rev 9691)
@@ -179,6 +179,9 @@
     int hide_sameval = 0;
     bool transposed = false;
 
+    //! if true we will display the filename at the bottom of the screan instead
+    //! of the normal other information.
+    bool display_filename = false;
     int namewidth = 0;
     for(int j=0; j<vm->width(); j++)
         namewidth = max(namewidth, (int) vm->fieldName(j).size());
@@ -210,6 +213,8 @@
 
             int nj = transposed ? LINES-3 : (COLS-leftcolwidth)/valwidth;
             int ni = transposed ? (COLS-leftcolwidth)/valwidth : LINES-4;
+            if(display_filename && filename.length()>(size_t)COLS)
+                ni -= filename.length()/COLS;
 
             int endj = min(vm_showed->width(), startj+nj);
             int endi = min(vm_showed->length(), starti+ni);
@@ -314,10 +319,14 @@
 
             string strval = vm_showed->getString(curi, curj);
             mvprintw(0,0,"Cols[%d-%d]", 0, vm_showed.width()-1);
-            mvprintw(LINES-1,0," %dx%d   line= %d   col= %d     %s = %s (%f)",
-                     vm_showed->length(), vm_showed->width(),
-                     curi, curj, vm_showed->fieldName(curj).c_str(),
-                     strval.c_str(), vm_showed(curi,curj));
+            if(display_filename){
+                mvprintw(LINES-filename.length()/COLS-1,0,"%s",filename.c_str());
+            }else
+                mvprintw(LINES-1,0,
+                         " %dx%d   line= %d   col= %d     %s = %s (%f)",
+                         vm_showed->length(), vm_showed->width(),
+                         curi, curj, vm_showed->fieldName(curj).c_str(),
+                         strval.c_str(), vm_showed(curi,curj));
 
             refresh();
             if (!onError)
@@ -960,6 +969,7 @@
                 mvprintw(vStartHelp++,10," - 'r' or 'R': show only a range or a set of columns");
                 mvprintw(vStartHelp++,10," - 'x' or 'X': hide the currently selected column");
                 mvprintw(vStartHelp++,10," - 'a' or 'A': show the original VMat");
+                mvprintw(vStartHelp++,10," - 'f' or 'F': toggle the display of the filename");
                 mvprintw(vStartHelp++,10," - 'i' or 'I': insert a new column with default value");
                 mvprintw(vStartHelp++,10," - 'l' or 'L': prompt for a line number and go to that line");
                 mvprintw(vStartHelp++,10," - 'c' or 'C': prompt for a column number and go to that column");
@@ -981,6 +991,9 @@
 
                 break;
 
+            case (int)'f': case (int)'F':
+                display_filename = !display_filename;
+
             case (int)'q': case (int)'Q':
                 break;
 



From nouiz at mail.berlios.de  Tue Nov 18 21:18:12 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Nov 2008 21:18:12 +0100
Subject: [Plearn-commits] r9692 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200811182018.mAIKICFn012725@sheep.berlios.de>

Author: nouiz
Date: 2008-11-18 21:18:12 +0100 (Tue, 18 Nov 2008)
New Revision: 9692

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added the dbidispatch option --universe for the condor backend
the --universe=local can be used to test our script as this start immediatly the ALL the jobs on the local host.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 16:50:13 UTC (rev 9691)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 20:18:12 UTC (rev 9692)
@@ -722,8 +722,15 @@
         self.condor_submit_dag_exec = "condor_submit_dag"
         self.pkdilly = False
         self.launch_file = None
+        self.universe = "vanilla"
 
         DBIBase.__init__(self, commands, **args)
+
+        valid_universe = ["standard", "vanilla", "grid", "java", "scheduler", "local", "parallel", "vm"]
+        if not self.universe in valid_universe:
+            print "[DBI] ERROR: the universe option have an invalid value",self.universe,". Valid values are:",valid_universe
+
+        #transform from meg to kilo
         self.mem=int(self.mem)*1024
 
         self.os = self.os.upper()
@@ -1027,7 +1034,7 @@
 
         condor_submit_fd.write( dedent('''\
                 executable     = %s
-                universe       = vanilla
+                universe       = %s
                 requirements   = %s
                 output         = $(stdout)
                 error          = $(stderr)
@@ -1035,7 +1042,7 @@
                 getenv         = %s
                 nice_user      = %s
                 arguments      = $(args)
-                ''' % (self.launch_file,self.req,
+                ''' % (self.launch_file, self.universe, self.req,
                        self.log_file,str(self.getenv),str(self.nice))))
         if self.mem>0:
             #condor need value in Kb
@@ -1129,14 +1136,14 @@
 
         condor_submit_fd.write( dedent('''\
                 executable     = %s
-                universe       = vanilla
+                universe       = %s
                 requirements   = %s
                 output         = %s/condor.$(Process).out
                 error          = %s/condor.$(Process).error
                 log            = %s
                 getenv         = %s
                 nice_user      = %s
-                ''' % (self.launch_file,self.req,
+                ''' % (self.launch_file, self.universe, self.req,
                        self.log_dir,
                        self.log_dir,
                        self.log_file,str(self.getenv),str(self.nice))))

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-11-18 16:50:13 UTC (rev 9691)
+++ trunk/scripts/dbidispatch	2008-11-18 20:18:12 UTC (rev 9692)
@@ -20,6 +20,7 @@
                               [--tasks_filename={compact,explicit,*nb0,nb1,sh}+]
                               [*--[no_]abs_path] [--[*no_]pkdilly]
                               [*--[no_]set_special_env]
+                              [--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}]
     cluster option           :[*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible]
 An * after '[', '{' or ',' signals the default value.
@@ -152,6 +153,11 @@
     executable to the absolute path or not. Default True.
   The '--[no_]pkdilly': will use the pkdilly tool to make condor more 
       kerberos friendly.
+  The '--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}'
+      The universe parameter is passed to condor. You can use the 'local' 
+       universe to test your script as this make the all the jobs start 
+       immediately without being preempted on the local host. Take care to 
+       don't send too much jobs.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -279,7 +285,8 @@
         dbi_param[argv[2:]]=True
         testmode=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
-                                "--req", "--files", "--raw", "--rank", "--env"]:
+                                "--req", "--files", "--raw", "--rank", "--env",
+                                "--universe"]:
         param=argv.split('=')[0][2:]
         if param in ["req", "files", "rank"]:
             #param that we happend to if defined more then one time
@@ -332,7 +339,8 @@
                        "duree","cpu","mem","os"]
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
-                       "raw", "os", "set_special_env", "mem", "cpu", "pkdilly"]
+                       "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
+                       "universe"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long", "duree"]
 



From nouiz at mail.berlios.de  Tue Nov 18 21:38:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Nov 2008 21:38:02 +0100
Subject: [Plearn-commits] r9693 - trunk/python_modules/plearn/parallel
Message-ID: <200811182038.mAIKc2nI015899@sheep.berlios.de>

Author: nouiz
Date: 2008-11-18 21:38:00 +0100 (Tue, 18 Nov 2008)
New Revision: 9693

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
dbidispatch with condor. If we are in the local universe, we don't allow running more jobs then the number of cores on the local machine to don't kill it.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 20:18:12 UTC (rev 9692)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 20:38:00 UTC (rev 9693)
@@ -729,6 +729,12 @@
         valid_universe = ["standard", "vanilla", "grid", "java", "scheduler", "local", "parallel", "vm"]
         if not self.universe in valid_universe:
             print "[DBI] ERROR: the universe option have an invalid value",self.universe,". Valid values are:",valid_universe
+            sys.exit(1)
+        if self.universe=="local":
+            n=subprocess.Popen("cat /proc/cpuinfo |grep processor|wc -l", shell = True, stdout=PIPE).stdout.readline()
+            if len(commands)>int(n):
+                print "[DBI] ERROR we refuse to start more jobs on the local universe then the total number of core. Start less jobs or use another universe."
+                sys.exit(1)
 
         #transform from meg to kilo
         self.mem=int(self.mem)*1024



From ducharme at mail.berlios.de  Tue Nov 18 21:49:44 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 18 Nov 2008 21:49:44 +0100
Subject: [Plearn-commits] r9694 - trunk/plearn_learners/regressors
Message-ID: <200811182049.mAIKni2W017136@sheep.berlios.de>

Author: ducharme
Date: 2008-11-18 21:49:44 +0100 (Tue, 18 Nov 2008)
New Revision: 9694

Modified:
   trunk/plearn_learners/regressors/PruningLinearRegressor.cc
   trunk/plearn_learners/regressors/PruningLinearRegressor.h
Log:
Get rid of (unnecessary) "forget" method.


Modified: trunk/plearn_learners/regressors/PruningLinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/PruningLinearRegressor.cc	2008-11-18 20:38:00 UTC (rev 9693)
+++ trunk/plearn_learners/regressors/PruningLinearRegressor.cc	2008-11-18 20:49:44 UTC (rev 9694)
@@ -131,12 +131,6 @@
     deepCopyField(input_indices, copies);
 }
 
-void PruningLinearRegressor::forget()
-{
-    inherited::forget();
-    t_ratio.resize(0);
-}
-
 void PruningLinearRegressor::setTrainingSet(VMat training_set, bool call_forget)
 {
     inherited::setTrainingSet(training_set, call_forget);

Modified: trunk/plearn_learners/regressors/PruningLinearRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/PruningLinearRegressor.h	2008-11-18 20:38:00 UTC (rev 9693)
+++ trunk/plearn_learners/regressors/PruningLinearRegressor.h	2008-11-18 20:49:44 UTC (rev 9694)
@@ -78,11 +78,6 @@
 
     //#####  PLearner Member Functions  #######################################
 
-    //! (Re-)initializes the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!).
-    virtual void forget();
-
     //! The role of the train method is to bring the learner up to
     //! stage==nstages, updating the train_stats collector with training costs
     //! measured on-line in the process.



From ducharme at mail.berlios.de  Tue Nov 18 21:51:10 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 18 Nov 2008 21:51:10 +0100
Subject: [Plearn-commits] r9695 - trunk/plearn_learners/regressors
Message-ID: <200811182051.mAIKpAut017286@sheep.berlios.de>

Author: ducharme
Date: 2008-11-18 21:51:09 +0100 (Tue, 18 Nov 2008)
New Revision: 9695

Modified:
   trunk/plearn_learners/regressors/LinearRegressor.cc
   trunk/plearn_learners/regressors/LinearRegressor.h
Log:
Little code cleanup.


Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2008-11-18 20:49:44 UTC (rev 9694)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2008-11-18 20:51:09 UTC (rev 9695)
@@ -179,9 +179,6 @@
     // ### that you wish to be deepCopied rather than 
     // ### shallow-copied.
     // ### ex:
-    deepCopyField(extendedinput, copies);
-    deepCopyField(input, copies);
-    deepCopyField(target, copies);
     deepCopyField(train_costs, copies);
     deepCopyField(XtX, copies);
     deepCopyField(XtY, copies);
@@ -221,26 +218,18 @@
     resid_variance.resize(0);
 }
 
-
 void LinearRegressor::train()
 {
     if(targetsize()<=0)
         PLERROR("In LinearRegressor::train() -  Targetsize (%d) must be "
                 "positive", targetsize());
+
     // Preparatory buffer allocation
     bool recompute_XXXY = (XtX.length()==0);
-    extendedinput.resize(effective_inputsize());
-    input = extendedinput;
-    if (include_bias) {
-        input = extendedinput.subVec(1,inputsize());
-        extendedinput[0]=1.0;
-    }
-    target.resize(targetsize());  // the train_set's targetsize()
-    weights.resize(extendedinput.length(),target.length());
     if (recompute_XXXY)
     {
-        XtX.resize(extendedinput.length(),extendedinput.length());
-        XtY.resize(extendedinput.length(),target.length());
+        XtX.resize(effective_inputsize(), effective_inputsize());
+        XtY.resize(effective_inputsize(), targetsize());
     }
     if(!train_stats)  // make a default stats collector, in case there's none
         train_stats = new VecStatsCollector();
@@ -251,12 +240,13 @@
     // Compute training inputs and targets; take into account optional bias
     real squared_error=0;
     Vec outputwise_sum_squared_Y;
-    VMat trainset_inputs  = train_set.subMatColumns(0,inputsize());
+    VMat trainset_inputs  = train_set.subMatColumns(0, inputsize());
     VMat trainset_targets = train_set.subMatColumns(inputsize(), targetsize());
     if (include_bias)                          // prepend a first column of ones
         trainset_inputs = new ExtendedVMatrix(trainset_inputs,0,0,1,0,1.0);
 
     // Choose proper function depending on whether the dataset is weighted
+    weights.resize(effective_inputsize(), targetsize());
     if (train_set->weightsize()<=0)
     {
         squared_error =
@@ -312,18 +302,14 @@
     }
   
     // Compute the output from the input
-    int nout = outputsize();
-    output.resize(nout);
-    if (input.length()==0) 
-    {
-        extendedinput.resize(effective_inputsize());
-        input = extendedinput;
-        if (include_bias) {
-            input = extendedinput.subVec(1,inputsize());
-            extendedinput[0] = 1.0;
-        }
+    extendedinput.resize(effective_inputsize());
+    input = extendedinput;
+    if (include_bias) {
+        input = extendedinput.subVec(1,inputsize());
+        extendedinput[0] = 1.0;
     }
     input << actual_input;
+    output.resize(outputsize());
     transposeProduct(output,weights,extendedinput);
 }
 

Modified: trunk/plearn_learners/regressors/LinearRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.h	2008-11-18 20:49:44 UTC (rev 9694)
+++ trunk/plearn_learners/regressors/LinearRegressor.h	2008-11-18 20:51:09 UTC (rev 9695)
@@ -52,7 +52,6 @@
     // Global storage used to save memory allocations.
     mutable Vec extendedinput; //!<  length 1+inputsize(), first element is 1.0 (used by the use method)
     mutable Vec input; //!<  extendedinput.subVec(1,inputsize())
-    Vec target;
     Vec train_costs;
 
 protected:



From ducharme at mail.berlios.de  Tue Nov 18 21:52:25 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 18 Nov 2008 21:52:25 +0100
Subject: [Plearn-commits] r9696 - trunk/plearn_learners/regressors
Message-ID: <200811182052.mAIKqPVr017424@sheep.berlios.de>

Author: ducharme
Date: 2008-11-18 21:52:25 +0100 (Tue, 18 Nov 2008)
New Revision: 9696

Modified:
   trunk/plearn_learners/regressors/AutoLinearRegressor.cc
Log:
Add a column of 1 to the training set when include_bias==true.


Modified: trunk/plearn_learners/regressors/AutoLinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/AutoLinearRegressor.cc	2008-11-18 20:51:09 UTC (rev 9695)
+++ trunk/plearn_learners/regressors/AutoLinearRegressor.cc	2008-11-18 20:52:25 UTC (rev 9696)
@@ -184,6 +184,12 @@
         Mat Y = tset.subMatColumns(ninputs, ntargets);
         Vec gamma; // the weights
 
+        if (include_bias)
+        {
+            Mat col_ones = Mat(l, 1, 1.0);
+            X = hconcat(col_ones, X);
+        }
+
         mean_target.resize(ntargets);
         mean_target.fill(0);
 



From nouiz at mail.berlios.de  Tue Nov 18 21:52:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Nov 2008 21:52:58 +0100
Subject: [Plearn-commits] r9697 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200811182052.mAIKqwS5017514@sheep.berlios.de>

Author: nouiz
Date: 2008-11-18 21:52:57 +0100 (Tue, 18 Nov 2008)
New Revision: 9697

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
add an environment variable in each condor CONDOR_JOB_LOGDIR. It contain the path to the condor log directory.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 20:52:25 UTC (rev 9696)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-11-18 20:52:57 UTC (rev 9697)
@@ -995,6 +995,7 @@
                     echo "PYTHONPATH: $PYTHONPATH" 1>&2
                     echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
                     echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
+                    echo "CONDOR_JOB_LOGDIR: $CONDOR_JOB_LOGDIR" 1>&2
                     pwd 1>&2
                     echo "nb args: $#" 1>&2
                     echo "Running: command: \\"$@\\"" 1>&2
@@ -1019,6 +1020,7 @@
                 echo "PYTHONPATH: $PYTHONPATH"
                 echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
                 echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"
+                echo "CONDOR_JOB_LOGDIR: $CONDOR_JOB_LOGDIR"
                 pwd
                 echo "Running command: $argv"
                 $argv
@@ -1243,7 +1245,9 @@
             return #no task to run
 
         if self.set_special_env:
-            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) "'
+            self.env+='" OMP_NUM_THREADS=$$(CPUS) GOTO_NUM_THREADS=$$(CPUS) MKL_NUM_THREADS=$$(CPUS) CONDOR_JOB_LOGDIR=%s"'%self.log_dir
+        else:
+            self.env+='" CONDOR_JOB_LOGDIR=%s"'%self.log_dir
 
         if not self.req:
             self.req = "True"

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-11-18 20:52:25 UTC (rev 9696)
+++ trunk/scripts/dbidispatch	2008-11-18 20:52:57 UTC (rev 9697)
@@ -103,6 +103,8 @@
     in the environment of the submitted jobs.
   The 'CONDOR_LOCAL_SOURCE' environment variable define a file that will be
     sourced before the jobs execute.
+  The 'CONDOR_JOB_LOGDIR' for each jobs the environment variable is set to 
+    the condor log directory. 
   The '--[no_]getenv' option is forwarded to condor. If True, the current 
     environnement variable will be forwarded to the execution node.
   The '--req=\"CONDOR_REQUIREMENT\"' add requirement for condor. 



From ducharme at mail.berlios.de  Tue Nov 18 21:57:25 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 18 Nov 2008 21:57:25 +0100
Subject: [Plearn-commits] r9698 - trunk/python_modules/plearn/parallel
Message-ID: <200811182057.mAIKvPk8018004@sheep.berlios.de>

Author: ducharme
Date: 2008-11-18 21:57:25 +0100 (Tue, 18 Nov 2008)
New Revision: 9698

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
- Check if it runs under a 32 or 64-bit machine (at ApSTAT)
- Correct bug in method "availableMachinesCount" (take into account free machine cpus)


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2008-11-18 20:52:57 UTC (rev 9697)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2008-11-18 20:57:25 UTC (rev 9698)
@@ -40,21 +40,26 @@
                      '## UNKNOWN DOMAINE ##': 'OnHostTask'
                      }
 
+# Figure out if we are running on a 32bit or 64 bit machine.
+if sys.maxint > 2147483647:
+    apstat_machines = [ 'nai' ]
+else:
+    apstat_machines = [ 'lodur',
+                        'garm',
+                        'mimir',
+                        'embla',
+                        'valhalla',
+                        'inari', 
+                        'kamado',
+                        'loki',
+                        'odin',
+                        'midgard',
+                        'vili' ]
+
 # Used only for clusters of type 'ssh'. Do not enter the same machine more
 # than once: use the MAX_LOADAVG map to allow for higher maximum load
-# average than the default of 2.
-SSH_MACHINES_MAP = { 'apstat.com': [ 'lodur',
-                                     'garm',
-                                     'mimir',
-                                     'embla',
-                                     'valhalla',
-                                     'inari', 
-                                     'kamado',
-                                     'loki',
-                                     'odin',
-                                     'midgard',
-                                     'vili'
-                                     ],
+# average than the default of 1.
+SSH_MACHINES_MAP = { 'apstat.com': apstat_machines,
 
                      'iro.umontreal.ca' : [ 'lhmm',    'lknn',    'lmfa',      'lmlp',
                                             'lsom',    'lsvm',    'currie',    'dirac',
@@ -181,7 +186,7 @@
         """Returns the number of machines currently available for cluster job dispatch."""
         avail = 0
         for am in cls.listAvailableMachines():
-            avail += 1
+            avail += int(MAX_LOADAVG.get(am, 1.) - cls.getMachineLoad(am))
         return avail
     availableMachinesCount = classmethod(availableMachinesCount)
 
@@ -352,14 +357,18 @@
             #print "Saved %f at %s (now %s)"%(loadavg, t, cur_t)
             if cur_t < t+LOADAVG_DELAY:
                 return loadavg
+        return cls.getMachineLoad(machine)
+    getLoadAvg = classmethod(getLoadAvg)
 
+    def getMachineLoad(cls, machine, command = lambda host: 'ssh -x %s cat /proc/loadavg' % host):
         # Query for the load average
         #print "NEW QUERY!"
         p = os.popen( command(machine) )
         line = p.readline()
         return float(line.split()[0]) # Take the last minute average
-    getLoadAvg = classmethod(getLoadAvg)
+    getMachineLoad = classmethod(getMachineLoad)
     
+
     def listAvailableMachines(cls):
         for m in cls._machines:
             try:
@@ -668,3 +677,6 @@
 if __name__ == "__main__":
     dispatch = Dispatch( program = "time", logdir=None )
     dispatch.start({ "_constant_args_" : "ls -lah" }) 
+
+
+# vim: filetype=python:expandtab:shiftwidth=4:tabstop=8:softtabstop=4 :



From plearner at mail.berlios.de  Wed Nov 19 00:15:12 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 19 Nov 2008 00:15:12 +0100
Subject: [Plearn-commits] r9699 - in trunk: plearn/ker/EXPERIMENTAL
	plearn_learners/classifiers plearn_learners/unsupervised
Message-ID: <200811182315.mAINFCMP011175@sheep.berlios.de>

Author: plearner
Date: 2008-11-19 00:15:10 +0100 (Wed, 19 Nov 2008)
New Revision: 9699

Modified:
   trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc
   trunk/plearn_learners/classifiers/KNNClassifier.cc
   trunk/plearn_learners/unsupervised/NormalizationLearner.cc
Log:
Bug fixes


Modified: trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc	2008-11-18 20:57:25 UTC (rev 9698)
+++ trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc	2008-11-18 23:15:10 UTC (rev 9699)
@@ -99,16 +99,20 @@
 
 void PartsDistanceKernel::train(VMat data)
 {    
-    Vec meanvec;
-    Vec stddev;
-    computeInputMeanAndStddev(data, meanvec, stddev, 0.0);
-    int l = stddev.length();
-    for(int i=0; i<l; i++)
+    if(standardize)
     {
-        if(stddev[i]<min_stddev)
-            inv_stddev[i] = FLT_MAX;
-        else
-            inv_stddev[i] = 1/stddev[i]; 
+        Vec meanvec;
+        Vec stddev;    
+        computeInputMeanAndStddev(data, meanvec, stddev, 0.0);
+        int l = stddev.length();
+        inv_stddev.resize(l);
+        for(int i=0; i<l; i++)
+        {
+            if(stddev[i]<min_stddev)
+                inv_stddev[i] = FLT_MAX;
+            else
+                inv_stddev[i] = 1/stddev[i]; 
+        }
     }
 }
 
@@ -125,7 +129,7 @@
         PLERROR("In PartsDistanceKernel::evaluate, size of vectors (%d) does not match size of inv_stddev (%d). Make sure you called train on the kernel with appropriate dataset",l,inv_stddev.length());
         
     elementdist.resize(l);
-    for(int i; i<l; i++)
+    for(int i=0; i<l; i++)
     {
         real d = FLT_MAX;
         if( !(standardize && inv_stddev[i]>=FLT_MAX) )
@@ -143,7 +147,9 @@
     }
     
     sortElements(elementdist);
-    int ps = (partsize>=1 ? (int)partsize :(int)(partsize*l));
+    int ps = (partsize>=1 ? (int)partsize :(int)(partsize*l+0.5));
+    if(ps>l)
+        ps = l;
     real res = 0;
     for(int i=0; i<ps; i++)
     {

Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-18 20:57:25 UTC (rev 9698)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-18 23:15:10 UTC (rev 9699)
@@ -200,9 +200,6 @@
     knn->copy_target = true;
     knn->copy_weight = true;
     knn->copy_index  = false;
-    // we call train on the knn->distance_kernel. We do it here in KNNClassifier::setTrainingSet (rather than in KNNClassifier::train)
-    // in case the knn->setTrainingSet that is invoked just afterwards calls distance_kernel computations already.
-    knn->distance_kernel->train(training_set);
     knn->setTrainingSet(training_set,call_forget);
     knn_costs.resize(num_neighbors); // Changed for compatibility with HyperLearner
     //knn_costs.resize(knn->nTestCosts());
@@ -218,6 +215,7 @@
 void KNNClassifier::train()
 {
     PLASSERT( knn );
+    knn->distance_kernel->train(train_set);
     knn->train();
 }
 

Modified: trunk/plearn_learners/unsupervised/NormalizationLearner.cc
===================================================================
--- trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-18 20:57:25 UTC (rev 9698)
+++ trunk/plearn_learners/unsupervised/NormalizationLearner.cc	2008-11-18 23:15:10 UTC (rev 9699)
@@ -241,9 +241,14 @@
         int pos = p_kept_components[k];
         real val = p_input[pos];
         if(is_missing(val))
-            p_output[k] = set_missing_to_zero?0.:val;
+        {
+            if(set_missing_to_zero)
+                val = 0;
+        }
         else if(do_normalize)
-            p_output[k] = p_inv_stddev[pos]*(val - p_meanvec[pos]);
+            val = p_inv_stddev[pos]*(val - p_meanvec[pos]);
+
+        p_output[k] = val;
     }
 }
 



From nouiz at mail.berlios.de  Wed Nov 19 19:30:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 19 Nov 2008 19:30:27 +0100
Subject: [Plearn-commits] r9700 - in trunk: commands plearn/vmat
Message-ID: <200811191830.mAJIURNg012419@sheep.berlios.de>

Author: nouiz
Date: 2008-11-19 19:30:17 +0100 (Wed, 19 Nov 2008)
New Revision: 9700

Added:
   trunk/plearn/vmat/AutoVMatrixSaveSource.cc
   trunk/plearn/vmat/AutoVMatrixSaveSource.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
added the file AutoVMatrixSaveSource that is the same as AutoVMatrix except that it save the vm and reuse it if present instead of checking the filename option.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-11-18 23:15:10 UTC (rev 9699)
+++ trunk/commands/plearn_noblas_inc.h	2008-11-19 18:30:17 UTC (rev 9700)
@@ -300,6 +300,7 @@
 #include <plearn/vmat/AppendNeighborsVMatrix.h>
 #include <plearn/vmat/AsciiVMatrix.h>
 #include <plearn/vmat/AutoVMatrix.h>
+#include <plearn/vmat/AutoVMatrixSaveSource.h>
 #include <plearn/vmat/BootstrapVMatrix.h>
 #include <plearn/vmat/CenteredVMatrix.h>
 #include <plearn/vmat/ClassSubsetVMatrix.h>

Added: trunk/plearn/vmat/AutoVMatrixSaveSource.cc
===================================================================
--- trunk/plearn/vmat/AutoVMatrixSaveSource.cc	2008-11-18 23:15:10 UTC (rev 9699)
+++ trunk/plearn/vmat/AutoVMatrixSaveSource.cc	2008-11-19 18:30:17 UTC (rev 9700)
@@ -0,0 +1,88 @@
+// -*- C++ -*-
+
+// AutoVMatrixSaveSource.cc
+//
+// Copyright (C) 2008 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file AutoVMatrixSaveSource.cc */
+
+
+#include "AutoVMatrixSaveSource.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    AutoVMatrixSaveSource,
+    "ONE LINE USER DESCRIPTION",
+    "MULTI LINE\nHELP FOR USERS"
+    );
+
+AutoVMatrixSaveSource::AutoVMatrixSaveSource(){}
+
+// ### Nothing to add here, simply calls build_
+void AutoVMatrixSaveSource::build()
+{
+    build_();
+}
+
+void AutoVMatrixSaveSource::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+void AutoVMatrixSaveSource::declareOptions(OptionList& ol)
+{
+    inherited::declareOptions(ol);
+    redeclareOption(ol, "vm", &AutoVMatrixSaveSource::vm, OptionBase::buildoption, "");
+}
+
+void AutoVMatrixSaveSource::build_()
+{
+    if(!vm)
+        inherited::build();
+}
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/AutoVMatrixSaveSource.h
===================================================================
--- trunk/plearn/vmat/AutoVMatrixSaveSource.h	2008-11-18 23:15:10 UTC (rev 9699)
+++ trunk/plearn/vmat/AutoVMatrixSaveSource.h	2008-11-19 18:30:17 UTC (rev 9700)
@@ -0,0 +1,107 @@
+// -*- C++ -*-
+
+// AutoVMatrixSaveSource.h
+//
+// Copyright (C) 2008 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file AutoVMatrixSaveSource.h */
+
+
+#ifndef AutoVMatrixSaveSource_INC
+#define AutoVMatrixSaveSource_INC
+
+#include <plearn/vmat/AutoVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class AutoVMatrixSaveSource : public AutoVMatrix
+{
+    typedef AutoVMatrix inherited;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    AutoVMatrixSaveSource();
+
+    //#####  PLearn::Object Protocol  #########################################
+    PLEARN_DECLARE_OBJECT(AutoVMatrixSaveSource);
+
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(AutoVMatrixSaveSource);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Wed Nov 19 22:25:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 19 Nov 2008 22:25:10 +0100
Subject: [Plearn-commits] r9701 - trunk/plearn/misc
Message-ID: <200811192125.mAJLPA6S014046@sheep.berlios.de>

Author: nouiz
Date: 2008-11-19 22:25:10 +0100 (Wed, 19 Nov 2008)
New Revision: 9701

Modified:
   trunk/plearn/misc/Redirect.cc
   trunk/plearn/misc/Redirect.h
Log:
added the option Redirect.active


Modified: trunk/plearn/misc/Redirect.cc
===================================================================
--- trunk/plearn/misc/Redirect.cc	2008-11-19 18:30:17 UTC (rev 9700)
+++ trunk/plearn/misc/Redirect.cc	2008-11-19 21:25:10 UTC (rev 9701)
@@ -46,7 +46,8 @@
 namespace PLearn {
 using namespace std;
 
-Redirect::Redirect()
+Redirect::Redirect():
+    active(true)
     /* ### Initialize all fields to their default value */
 {
     // ...
@@ -67,6 +68,9 @@
     // ### OptionBase::tuningoption. Another possible flag to be combined with
     // ### is OptionBase::nosave
 
+    declareOption(ol, "active", &Redirect::active, OptionBase::buildoption,
+                  "Will do the redirect only if true. Else do nothing.");
+
     declareOption(ol, "what", &Redirect::what, OptionBase::buildoption,
                   "The string perr or pout. Indicated what will be redirected.");
 
@@ -107,6 +111,8 @@
 /////////
 void Redirect::run() {
     //do the redirection
+    if(!active)
+        return;
     if(what=="perr"){
         //the old is closed automatically
         perr=openFile(filename,PStream::raw_ascii,"w",false,true);

Modified: trunk/plearn/misc/Redirect.h
===================================================================
--- trunk/plearn/misc/Redirect.h	2008-11-19 18:30:17 UTC (rev 9700)
+++ trunk/plearn/misc/Redirect.h	2008-11-19 21:25:10 UTC (rev 9701)
@@ -72,7 +72,7 @@
 
     PPath filename;
     string what;
-    
+    bool active;
 
     // ****************
     // * Constructors *



From nouiz at mail.berlios.de  Thu Nov 20 18:05:53 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Nov 2008 18:05:53 +0100
Subject: [Plearn-commits] r9702 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200811201705.mAKH5rhL002003@sheep.berlios.de>

Author: nouiz
Date: 2008-11-20 18:05:52 +0100 (Thu, 20 Nov 2008)
New Revision: 9702

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added the option --task_filename=... to bqtools.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-11-19 21:25:10 UTC (rev 9701)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-11-20 17:05:52 UTC (rev 9702)
@@ -176,6 +176,8 @@
         self.dolog = False
         self.temp_files = []
         self.arch = 0 # TODO, we should put the local arch: 32,64 or 3264 bits
+        self.base_tasks_log_file = []
+
         for key in args.keys():
             self.__dict__[key] = args[key]
 
@@ -627,13 +629,18 @@
         # and another one containing the log_file name associated
         tasks_file = open( 'tasks', 'w' )
         logfiles_file = open( 'logfiles', 'w' )
-        for task in self.tasks:
-            tasks_file.write( ';'.join(task.commands) + '\n' )
-            logfiles_file.write( task.log_file + '\n' )
+        if self.base_tasks_log_file:
+            for task,base in zip(self.tasks,self.base_tasks_log_file):
+                tasks_file.write( ';'.join(task.commands) + '\n' )
+                logfiles_file.write( os.path.join(self.log_dir,base) + '\n' )
+        else:
+            for task in self.tasks:
+                tasks_file.write( ';'.join(task.commands) + '\n' )
+                logfiles_file.write( task.log_file + '\n' )
         tasks_file.close()
         logfiles_file.close()
 
-        # create the bqsubmit.dat, with
+        # Create the bqsubmit.dat, with
         bqsubmit_dat = open( 'bqsubmit.dat', 'w' )
         bqsubmit_dat.write( dedent('''\
                 batchName = dbi_%s
@@ -712,7 +719,6 @@
         self.stdouts = ''
         self.stderrs = ''
         self.abs_path = True
-        self.base_tasks_log_file = []
         self.set_special_env = True
         self.nb_proc = -1 # < 0   mean unlimited
         self.source_file = ''
@@ -1089,7 +1095,7 @@
                 condor_dag_fd.write("JOB %d %s\n"%(i,self.condor_submit_file))
                 condor_dag_fd.write('VARS %d args="%s"\n'%(i,argstring))
                 s=os.path.join(self.log_dir,
-                               "condor"+self.base_tasks_log_file[i])
+                               "condor."+self.base_tasks_log_file[i])
                 condor_dag_fd.write('VARS %d stdout="%s"\n'%(i,s+".out"))
                 condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(i,s+".err"))
         elif self.stdouts and self.stderrs:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-11-19 21:25:10 UTC (rev 9701)
+++ trunk/scripts/dbidispatch	2008-11-20 17:05:52 UTC (rev 9702)
@@ -11,6 +11,7 @@
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
     cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
                               [--cpu=nb_cpu_per_node]
+    bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only)}+]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                               [--[*no_]getenv] [*--[no_]prefserver] 
                               [--rank=RANK_EXPRESSION] 
@@ -91,6 +92,19 @@
   The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
     will be reserved for each job.
 
+bqtools and condor options:
+  The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the
+    filename where the stdout, stderr are redirected. We can put many option 
+    separated by comma. They will apper in the filename in order separated by a 
+    dot. For all except sh, they have this pattern condor.X.{out,error} where X=:
+      - default : same as nb0
+      - compact : a unic string with parameter that change of value between jobs
+      - explicit: a unic string that represent the full command to execute
+      - nb0     : a number from 0 to nb job -1.
+      - nb1     : a number from 1 to nb job.
+      - sh      : (condor only)parse the command for > and 2> redirection command.
+                  If one or both of them are missing, they are redirected
+                  to /dev/null
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
     This must be enabled if there is not nb_proc available nodes. Otherwise 
@@ -139,18 +153,6 @@
   The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the condor submit file.
   The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, 
     MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
-  The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the
-    filename where the stdout, stderr are redirected. We can put many option 
-    separated by comma. They will apper in the filename in order separated by a 
-    dot. For all except sh, they have this pattern condor.X.{out,error} where X=:
-      - default : same as nb0
-      - compact : a unic string with parameter that change of value between jobs
-      - explicit: a unic string that represent the full command to execute
-      - nb0     : a number from 0 to nb job -1.
-      - nb1     : a number from 1 to nb job.
-      - sh      : parse the command for > and 2> redirection command.
-                  If one or both of them are missing, they are redirected
-                  to /dev/null
   The '--[no_]abs_path' option will tell condor to change the path to the 
     executable to the absolute path or not. Default True.
   The '--[no_]pkdilly': will use the pkdilly tool to make condor more 
@@ -208,7 +210,7 @@
 FILE = ""
 dbi_param={}
 testmode=False
-tasks_filename = ["nb0"]
+tasks_filename = []
 
 PATH=os.getenv('PATH')
 if search_file('condor_submit',PATH):
@@ -368,11 +370,19 @@
         if source_with_kerb:
             dbi_param['copy_local_source_file']=True
 
+if launch_cmd=="Bqtools" and not tasks_filename:
+    tasks_filename = ["explicit"]
+elif launch_cmd=="Condor" and not tasks_filename:
+    tasks_filename = ["nb0"]
+elif tasks_filename:
+    print "WARNING: The parameter --tasks_filename={} is only supported by condor and bqtools.",
+    print "It will be ignored"
+    
 print "\n\nThe jobs will be launched on the system:", launch_cmd
 print "With options: ",dbi_param
 for i in dbi_param:
     if i not in valid_dbi_param:
-        print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end"
+        print "WARNING: The parameter",i,"is not valid for the",launch_cmd,"back-end. It will be ignored."
 print "With the command to be expanded:"," ".join(command_argv),"\n\n"
 
 def generate_combination(repl,sep=" "):
@@ -467,7 +477,7 @@
 dbi_param[n]=[""]*len(commands)
 
 def merge_pattern(new_list):
-    return [x+'.'+y for (x,y) in  zip(dbi_param[n], new_list)]
+    return [x+'.'+y if x else y for (x,y) in  zip(dbi_param[n], new_list)]
 for pattern in tasks_filename:
     if pattern == "explicit":
         dbi_param[n]=merge_pattern([re.sub( '[^a-zA-Z=0-9-]', '_', x ) for x in commands])



From plearner at mail.berlios.de  Thu Nov 20 20:36:43 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 20 Nov 2008 20:36:43 +0100
Subject: [Plearn-commits] r9703 - trunk/plearn_learners/classifiers
Message-ID: <200811201936.mAKJahhg008634@sheep.berlios.de>

Author: plearner
Date: 2008-11-20 20:36:41 +0100 (Thu, 20 Nov 2008)
New Revision: 9703

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
   trunk/plearn_learners/classifiers/KNNClassifier.h
Log:
Added multi_k option that allows computing costs for several values of k (without having to research the neighbours among all training points for each different value of k).



Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-20 17:05:52 UTC (rev 9702)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-20 19:36:41 UTC (rev 9703)
@@ -142,6 +142,25 @@
         "(default), and the 'use_knn_costs_as_weights' is false, the\n"
         "rectangular kernel is used.");
   
+    declareOption(
+        ol, "multi_k", &KNNClassifier::multi_k, OptionBase::buildoption,
+        "This can be used if you wish to simultaneously compute the costs for\n"
+        "several values of k, efficiently, while doing neighbors search a\n"
+        "single time. Specify in increasing order, the values of k (number \n"
+        "the number of neighbors) you are interested in. This will result \n"
+        "in computing and making available extra costs in addition to \n"
+        "class_error and neglogprob. For each such specified k, \n"
+        "there will be a class_error_k and neglogprob_k.\n"
+        "Note that these will however only be computed correctly for values\n"
+        "of k that are less or equal to the global K determined by the other\n"
+        "options. So if you specify a multi_k list, you should probably set \n"
+        "kmin to the last and largest k of the list.\n"
+        "On a technical note, these costs will be computed correctly \n"
+        "only if the call to computeCostsFromOutputs follows the \n"
+        "computeOutput corresponding to the same input (this is usually\n"
+        "the case, and a warning is issued if it isn't).");
+
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -157,6 +176,10 @@
     if (kmin <= 0)
         PLERROR("KNNClassifier::build_: the 'kmin' option must be strictly positive");
 
+    for(int k=0; k<multi_k.length()-1; k++)
+        if(multi_k[k]>multi_k[k+1])
+            PLERROR("values in option multi_k *must* be in increaisng order");
+
 }
 
 // ### Nothing to add here, simply calls build_
@@ -229,11 +252,23 @@
     knn->computeOutputAndCosts(input, knn_targets, knn_output, knn_costs);
     real* output_data = knn_output.data();
   
+    int n_multi_k = multi_k.length();
+    if(n_multi_k>0)
+    {
+        // First remember the input so we can verify computeCostsFromOutputs is called on the same
+        multi_k_input.resize(input.length());
+        multi_k_input << input; 
+        // Then initialize the multi_k_output matrix.
+        multi_k_output.resize(n_multi_k, outputsize());
+        multi_k_output.fill(0);
+        // TODO: need to sort the knn output ?...
+    }
+
     // Cumulate the class weights.  Compute the kernel if it's required.
     class_weights.resize(nclasses);
     class_weights.fill(0.0);
     real total_weight = 0.0;
-    for (int i=0, n=knn->num_neighbors ; i<n ; ++i) {
+    for (int i=0, n=knn->num_neighbors, multi_pos=0 ; i<n ; ++i) {
         real w = -1.0;                           //!< safety net
         if (kernel) {
             Vec cur_input(inputsize, output_data);
@@ -252,6 +287,16 @@
         PLASSERT( w >= 0.0 );
         class_weights[nn_class] += w;
         total_weight += w;
+        if(multi_pos<n_multi_k && multi_k[multi_pos]==i+1) // we want to keep the output for k==i+1
+        {
+            if (total_weight >= 1e-6)
+            {
+                Vec output_k = multi_k_output(multi_pos);
+                output_k << class_weights;
+                output_k *= 1/total_weight;
+            }
+            ++multi_pos;
+        }
     }
 
     // If the total weight is too small, output zero probability for all classes
@@ -271,17 +316,40 @@
 void KNNClassifier::computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                             const Vec& target, Vec& costs) const
 {
-    costs.resize(nTestCosts());
+    int n_multi_k = multi_k.length();
+    costs.resize(2*(1+n_multi_k));
+    int t = int(target[0]);
     int sel_class = argmax(output);
-    costs[0] = sel_class != int(target[0]);
-    costs[1] = -pl_log(1e-10+output[int(target[0])]);
+    costs[0] = sel_class != t; 
+    costs[1] = -pl_log(1e-10+output[t]);
+
+    if(n_multi_k>0 && input!=multi_k_input)
+        PLWARNING("In computeCostsFromOutputs: input appears different from multi_k_input. "
+                  "This probably means that computeOutput was called on a different input "
+                  "before calling computeCostsFromOutputs. As a consequence, the extra costs "
+                  "requested through the multi_k option will be incorrect");
+        
+    for(int k=0; k<n_multi_k; k++)
+    {
+        Vec output_k = multi_k_output(k);
+        int sel_class = argmax(output_k);
+        costs[2+2*k] = sel_class != t; 
+        costs[3+2*k] = -pl_log(1e-10+output_k[t]);
+    }
 }
 
 TVec<string> KNNClassifier::getTestCostNames() const
 {
-    static TVec<string> costs(2);
+    int n_multi_k = multi_k.length();
+    static TVec<string> costs(2*(1+n_multi_k));
     costs[0] = "class_error";
     costs[1] = "neglogprob";
+    for(int k=0; k<n_multi_k; k++)
+    {
+        string kstr = tostring(multi_k[k]);
+        costs[2+2*k] = "class_error_"+kstr;
+        costs[3+2*k] = "neglogprob_"+kstr;
+    }
     return costs;
 }
 

Modified: trunk/plearn_learners/classifiers/KNNClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.h	2008-11-20 17:05:52 UTC (rev 9702)
+++ trunk/plearn_learners/classifiers/KNNClassifier.h	2008-11-20 19:36:41 UTC (rev 9703)
@@ -104,6 +104,17 @@
 
     //! Internal use: temporary buffer for cumulating class weights
     mutable Vec class_weights;
+
+    //! Internal use: this is used when a multi_k option is provided 
+    //! to temporarily store the outputs the classifier would give for 
+    //! all values of k given in multi_k.
+    //! These outputs are computed by the computeOutput method, for
+    //! consumption by the computeCostsFromOutputs method (whuch must be 
+    //! called right after). 
+    mutable Mat multi_k_output;
+
+    //! Internal use to remember the input used in computeOutput when using multi_k option.
+    mutable Vec multi_k_input;
   
 public:
     //#####  Public Build Options  ############################################
@@ -139,6 +150,11 @@
     //! (default), and the 'use_knn_costs_as_weights' is false, the
     //! rectangular kernel is used.
     Ker kernel;
+    
+    //! This can be used if you wish to simultaneously compute the costs for several
+    //! values of k, efficiently, while doing neighbors search a single time.
+    //! (see corresponding declareOption in .cc for more detailed info).
+    TVec<int> multi_k;
 
 public:
     //#####  Object Methods  ##################################################



From plearner at mail.berlios.de  Thu Nov 20 20:37:59 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Thu, 20 Nov 2008 20:37:59 +0100
Subject: [Plearn-commits] r9704 - trunk/plearn/ker/EXPERIMENTAL
Message-ID: <200811201937.mAKJbx21008715@sheep.berlios.de>

Author: plearner
Date: 2008-11-20 20:37:59 +0100 (Thu, 20 Nov 2008)
New Revision: 9704

Modified:
   trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc
Log:
further developments on parts distance 


Modified: trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc	2008-11-20 19:36:41 UTC (rev 9703)
+++ trunk/plearn/ker/EXPERIMENTAL/PartsDistanceKernel.cc	2008-11-20 19:37:59 UTC (rev 9704)
@@ -116,11 +116,12 @@
     }
 }
 
-
 //////////////
 // evaluate //
 //////////////
-real PartsDistanceKernel::evaluate(const Vec& x1, const Vec& x2) const {
+
+real PartsDistanceKernel::evaluate(const Vec& x1, const Vec& x2) const 
+{
     int l = x1.length();
     if(x2.length() != l)
         PLERROR("vectors x1 and x2 must have the same size");
@@ -129,28 +130,53 @@
         PLERROR("In PartsDistanceKernel::evaluate, size of vectors (%d) does not match size of inv_stddev (%d). Make sure you called train on the kernel with appropriate dataset",l,inv_stddev.length());
         
     elementdist.resize(l);
+
+    const real* px1 = x1.data();
+    const real* px2 = x2.data();
+    real* pelementdist = elementdist.data();
+    real* pinv_stddev = 0;
+    if(standardize)
+        pinv_stddev = inv_stddev.data();
+
+    char specialn = 0;
+    if(n==2)
+        specialn = 2;
+    else if(n==1)
+        specialn = 1;
+        
     for(int i=0; i<l; i++)
     {
-        real d = FLT_MAX;
-        if( !(standardize && inv_stddev[i]>=FLT_MAX) )
+        real d;
+        if(standardize && pinv_stddev[i]>=FLT_MAX )
+            d = FLT_MAX;
+        else
         {
-            d = abs(x1[i]-x2[i])+epsilon;
+            d = fabs(px1[i]-px2[i])+epsilon;                
             if(standardize)
-                d *= inv_stddev[i];
+                d *= pinv_stddev[i];
             
-            if(fast_exact_is_equal(n,2))
+            switch(specialn)
+            {
+            case 2:
                 d *= d;
-            else if(!fast_exact_is_equal(n,1))
+                break;
+            case 1:
+                break;
+            default:
                 d = mypow(d,n);
+            }
         }
-        elementdist[i] = d;     
+        pelementdist[i] = d;
     }
     
-    sortElements(elementdist);
     int ps = (partsize>=1 ? (int)partsize :(int)(partsize*l+0.5));
-    if(ps>l)
+    if(ps<l)
+        sortElements(elementdist);
+    else
         ps = l;
+
     real res = 0;
+
     for(int i=0; i<ps; i++)
     {
         real d = elementdist[i];



From nouiz at mail.berlios.de  Thu Nov 20 21:12:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Nov 2008 21:12:06 +0100
Subject: [Plearn-commits] r9705 - trunk/plearn/base
Message-ID: <200811202012.mAKKC6Qb010990@sheep.berlios.de>

Author: nouiz
Date: 2008-11-20 21:12:05 +0100 (Thu, 20 Nov 2008)
New Revision: 9705

Modified:
   trunk/plearn/base/plerror.cc
   trunk/plearn/base/plerror.h
Log:
added the macro PLWARN_ERR(bool warn, msg, ...), if warn is true, will generate a warning, else an error.
in gdb to breakpoint all error and warning, breakpoint fct verrormsg and vwarningmsg. Otherwise you won't break at PLWARN_ERR


Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-11-20 19:37:59 UTC (rev 9704)
+++ trunk/plearn/base/plerror.cc	2008-11-20 20:12:05 UTC (rev 9705)
@@ -107,10 +107,16 @@
 #endif
 
 
-void  warningmsg(const char* msg, ...)
+void warningmsg(const char* msg, ...)
 {
     va_list args;
     va_start(args,msg);
+    vwarningmsg(msg, args);
+    va_end(args);
+}
+
+void vwarningmsg(const char* msg, va_list args)
+{
     char message[ERROR_MSG_SIZE];
 
 #if !defined(ULTRIX) && !defined(_MINGW_) && !defined(WIN32)
@@ -119,12 +125,37 @@
     vsprintf(message,msg,args);
 #endif
 
-    va_end(args);
-
     // *error_stream <<" WARNING: "<<message<<endl;
     NORMAL_LOG << " WARNING: " << message << endl;
 }
 
+void warn_err(bool warn, const char* msg, ...)
+{
+    va_list args;
+    va_start(args,msg);
+    if(warn) vwarningmsg(msg,args);
+    else verrormsg(msg, args);
+    va_end(args);
+}
+
+void warn_err2(const char* filename, const int linenumber, bool warn, const char* msg,...)
+{
+    va_list args;
+    va_start(args,msg);
+
+    if(warn) vwarningmsg(msg,args);
+    else{
+        char message[ERROR_MSG_SIZE];
+    
+        snprintf(message, ERROR_MSG_SIZE, "In file: \"%s\" at line %d\n",
+                 PPath(filename).basename().c_str(), linenumber);
+        PLASSERT(ERROR_MSG_SIZE>=strlen(message)+strlen(msg));
+        strncat(message,msg,ERROR_MSG_SIZE);
+        verrormsg(message, args);
+    }
+    va_end(args);
+}
+
 void  deprecationmsg(const char* msg, ...)
 {
     va_list args;

Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2008-11-20 19:37:59 UTC (rev 9704)
+++ trunk/plearn/base/plerror.h	2008-11-20 20:12:05 UTC (rev 9705)
@@ -68,6 +68,8 @@
 #define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
 #define PLWARNING warningmsg
 #define PLDEPRECATED deprecationmsg
+//#define PLWARN_ERR warn_err //Use if the compiler don't like variadic macros
+#define PLWARN_ERR(...) warn_err2(__FILE__,__LINE__,__VA_ARGS__)
 
 void errormsg2(const char* filename, const int linenumber, const char* msg, ...)
     __attribute__((noreturn))
@@ -81,6 +83,12 @@
     __attribute__((noreturn));
 void warningmsg(const char* msg, ...)
     __attribute__((format(printf, 1, 2)));
+//vwarningmsg: internal warningmsg that takes a single va_list
+void vwarningmsg(const char* msg, va_list args);
+void warn_err(bool warn, const char* msg, ...)
+    __attribute__((format(printf, 2, 3)));    
+void warn_err2(const char* filename, const int linenumber, bool warn, const char* msg,...)
+    __attribute__((format(printf, 4, 5)));    
 void deprecationmsg(const char* msg, ...)
     __attribute__((format(printf, 1, 2)));
 void exitmsg(const char* msg, ...)



From nouiz at mail.berlios.de  Thu Nov 20 21:24:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Nov 2008 21:24:48 +0100
Subject: [Plearn-commits] r9706 - trunk/plearn/vmat
Message-ID: <200811202024.mAKKOmlR012360@sheep.berlios.de>

Author: nouiz
Date: 2008-11-20 21:24:47 +0100 (Thu, 20 Nov 2008)
New Revision: 9706

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
use the new PLWARN_ERR and apply this to regex specification too.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-20 20:12:05 UTC (rev 9705)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-11-20 20:24:47 UTC (rev 9706)
@@ -91,7 +91,7 @@
 		OptionBase::buildoption, 
                 "If True, will generate an error if some field in the"
 		" imputation_spec are present but not in the source. Otherwise"
-		" will generate a warning..");
+		" will generate a warning. This also applies for regex spec.");
 
   declareOption(ol, "default_instruction", &MeanMedianModeImputationVMatrix::default_instruction,
 		OptionBase::buildoption, 
@@ -327,10 +327,12 @@
 	      expended = true;
 	    }
 	  }
-	  if(!expended)
-	    PLERROR("In MeanMedianModeImputationVMatrix::build_() - "
-		    "Don't have find any partial match to %s",
-		    imputation_spec[spec_col].first.c_str());
+	  if(!expended){
+	    PLWARN_ERR(!missing_field_error,
+		       "In MeanMedianModeImputationVMatrix::build_() - "
+		       "Didn't found partial match for '%s'",
+		       imputation_spec[spec_col].first.c_str());
+	  }
 	  continue;
 	}
 	
@@ -345,16 +347,13 @@
     }
     imputation_spec = save_imputation_spec;
 
-    if(nofields.length()>0 && missing_field_error)
-      PLERROR("In MeanMedianModeImputationVMatrix::build_() Their is %d"
-	      " fields in the imputation_spec that are not in train set:"
-	      " %s",nofields.length(),
-	      tostring(nofields).c_str());
-    else if(nofields.length()>0)
-      PLWARNING("In MeanMedianModeImputationVMatrix::build_() Their is %d"
-		" fields in the imputation_spec that are not in train set:"
-		" %s",nofields.length(),
-		tostring(nofields).c_str());
+    if(nofields.length()>0)
+      PLWARN_ERR(!missing_field_error,
+		 "In MeanMedianModeImputationVMatrix::build_() Their is %d"
+		 " fields in the imputation_spec that are not in train set:"
+		 " '%s'",nofields.length(),
+		 tostring(nofields).c_str());
+
     TVec<string> no_instruction;
     for(int i = 0;i<variable_imputation_instruction.size();i++)
       if(variable_imputation_instruction[i]==0)



From nouiz at mail.berlios.de  Thu Nov 20 21:44:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Nov 2008 21:44:47 +0100
Subject: [Plearn-commits] r9707 - in trunk/plearn/base/test/.pytest:
	PL_assert/expected_results PL_check/expected_results
Message-ID: <200811202044.mAKKilik014251@sheep.berlios.de>

Author: nouiz
Date: 2008-11-20 21:44:46 +0100 (Thu, 20 Nov 2008)
New Revision: 9707

Modified:
   trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
   trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
Log:
corrected test following the addition of PLWARN_ERR


Modified: trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-11-20 20:24:47 UTC (rev 9706)
+++ trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-11-20 20:44:46 UTC (rev 9707)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 175
+FATAL ERROR: In file: "plerror.cc" at line 206
 Assertion failed: 3+8 == 123+46
 Function: int main()
     File: assertions.cc

Modified: trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-11-20 20:24:47 UTC (rev 9706)
+++ trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-11-20 20:44:46 UTC (rev 9707)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 189
+FATAL ERROR: In file: "plerror.cc" at line 220
 Check failed: ein == stein
 Function: void PLearn::PLCheckTest::perform()
     File: PLCheckTest.cc



From nouiz at mail.berlios.de  Fri Nov 21 17:22:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 17:22:20 +0100
Subject: [Plearn-commits] r9708 - in trunk/commands: . PLearnCommands
Message-ID: <200811211622.mALGMKGD029787@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 17:22:19 +0100 (Fri, 21 Nov 2008)
New Revision: 9708

Added:
   trunk/commands/PLearnCommands/ReadModifWriteCommand.cc
   trunk/commands/PLearnCommands/ReadModifWriteCommand.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
added plearn command ReadModifWriteCommand that allow to read a plearn file, make modifications to it and write it to a new file.
read_modif_write <sourcefile> <destfile> 'modification string'...


Added: trunk/commands/PLearnCommands/ReadModifWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadModifWriteCommand.cc	2008-11-20 20:44:46 UTC (rev 9707)
+++ trunk/commands/PLearnCommands/ReadModifWriteCommand.cc	2008-11-21 16:22:19 UTC (rev 9708)
@@ -0,0 +1,119 @@
+// -*- C++ -*-
+
+// ReadModifWriteCommand.cc
+//
+// Copyright (C) 2008 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file ReadModifWriteCommand.cc */
+
+
+#include "ReadModifWriteCommand.h"
+#include <plearn/base/Object.h>
+#include <plearn/base/stringutils.h>      //!< For extract_extension.
+#include <plearn/io/fileutils.h>        //!< For readFileAndMacroProcess.
+#include <plearn/io/load_and_save.h>
+#include <plearn/io/openFile.h>
+#include <plearn/io/openString.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'ReadModifWriteCommand' command in the command registry
+PLearnCommandRegistry ReadModifWriteCommand::reg_(new ReadModifWriteCommand);
+
+ReadModifWriteCommand::ReadModifWriteCommand()
+    : PLearnCommand(
+        "read_modif_write",
+        "used to make modification to serialized plearn file",
+        "read_modif_write <sourcefile> <destfile> 'modification string'...\n"
+        "Reads an Object (in PLearn serialization format) from the <sourcefile> and writes it to the <destfile> after appling the modification string (in PLearn serialisation format too)\n"
+        "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
+        "Otherwise (ex: .plearn .vmat) it will. \n"
+        )
+{}
+
+//! The actual implementation of the 'ReadModifWriteCommand' command
+void ReadModifWriteCommand::run(const vector<string>& args)
+{
+    if(args.size()<3)
+        PLERROR("read_and_write takes 3 arguments: "
+                "<sourcefile> <destfile> 'modification string' ...");
+    string source = args[0];
+    string dest = args[1];
+
+    string ext = extract_extension(source);
+    PP<Object> o;
+
+    //read the file
+    if(ext==".psave") // may be binay. Don't macro-process
+    {
+        PLearn::load(source,o);
+    }
+    else
+    {
+        map<string, string> vars;
+        string script = readFileAndMacroProcess(source, vars);
+        PStream in = openString(script,PStream::plearn_ascii);
+        o = readObject(in);
+    }
+
+    //modif the object
+    string left;
+    string right;
+    for(int i=2; i<args.size();i++){
+        split_on_first(args[i], "=", left, right);
+        cout <<left<<endl<<right<<endl;
+        o->setOption(left, right);
+    }
+    //write the file
+    PStream out = openFile(dest,PStream::plearn_ascii,"w");
+    if(!out)
+        PLERROR("Could not open file %s for writing",dest.c_str());
+    out << *o;
+
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/commands/PLearnCommands/ReadModifWriteCommand.h
===================================================================
--- trunk/commands/PLearnCommands/ReadModifWriteCommand.h	2008-11-20 20:44:46 UTC (rev 9707)
+++ trunk/commands/PLearnCommands/ReadModifWriteCommand.h	2008-11-21 16:22:19 UTC (rev 9708)
@@ -0,0 +1,86 @@
+// -*- C++ -*-
+
+// ReadModifWriteCommand.h
+//
+// Copyright (C) 2008 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file ReadModifWriteCommand.h */
+
+
+#ifndef ReadModifWriteCommand_INC
+#define ReadModifWriteCommand_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ReadModifWriteCommand : public PLearnCommand
+{
+    typedef PLearnCommand inherited;
+
+public:
+    ReadModifWriteCommand();
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-11-20 20:44:46 UTC (rev 9707)
+++ trunk/commands/plearn_noblas_inc.h	2008-11-21 16:22:19 UTC (rev 9708)
@@ -77,6 +77,7 @@
 #include <commands/PLearnCommands/LearnerCommand.h>
 #include <commands/PLearnCommands/PairwiseDiffsCommand.h>
 #include <commands/PLearnCommands/ReadAndWriteCommand.h>
+#include <commands/PLearnCommands/ReadModifWriteCommand.h>
 #include <commands/PLearnCommands/RunCommand.h>
 #include <commands/PLearnCommands/ServerCommand.h>
 #include <commands/PLearnCommands/TestDependenciesCommand.h>



From nouiz at mail.berlios.de  Fri Nov 21 17:22:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 17:22:37 +0100
Subject: [Plearn-commits] r9709 - trunk/commands
Message-ID: <200811211622.mALGMb5n029850@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 17:22:37 +0100 (Fri, 21 Nov 2008)
New Revision: 9709

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
added missing include.


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-11-21 16:22:19 UTC (rev 9708)
+++ trunk/commands/plearn_desjardins.cc	2008-11-21 16:22:37 UTC (rev 9709)
@@ -57,6 +57,7 @@
 #include <commands/PLearnCommands/LearnerCommand.h>
 //#include <commands/PLearnCommands/PairwiseDiffsCommand.h>
 #include <commands/PLearnCommands/ReadAndWriteCommand.h>
+#include <commands/PLearnCommands/ReadModifWriteCommand.h>
 #include <commands/PLearnCommands/RunCommand.h>
 //#include <commands/PLearnCommands/ServerCommand.h>
 //#include <commands/PLearnCommands/TestDependenciesCommand.h>
@@ -70,6 +71,9 @@
 #endif
 #include <plearn_learners/regressors/RegressionTree.h>
 #include <plearn_learners/meta/MultiClassAdaBoost.h>
+#include <plearn_learners/hyper/HyperLearner.h>
+#include <plearn_learners/hyper/HyperOptimize.h>
+#include <plearn_learners/hyper/EarlyStoppingOracle.h>
 
 /************
  * Splitter *
@@ -82,6 +86,7 @@
  ***********/
 #include <plearn/vmat/AddMissingVMatrix.h>
 #include <plearn/vmat/AutoVMatrix.h>
+#include <plearn/vmat/AutoVMatrixSaveSource.h>
 #include <plearn/vmat/BootstrapVMatrix.h>
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
 #include <plearn/vmat/DichotomizeVMatrix.h>



From nouiz at mail.berlios.de  Fri Nov 21 17:41:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 17:41:30 +0100
Subject: [Plearn-commits] r9710 - in trunk/commands: . PLearnCommands
Message-ID: <200811211641.mALGfUS9030696@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 17:41:30 +0100 (Fri, 21 Nov 2008)
New Revision: 9710

Removed:
   trunk/commands/PLearnCommands/ReadModifWriteCommand.cc
   trunk/commands/PLearnCommands/ReadModifWriteCommand.h
Modified:
   trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
   trunk/commands/plearn_desjardins.cc
Log:
removed ReadModifWriteCommand and put its functionnality in ReadAndWriteCommand.


Modified: trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2008-11-21 16:22:37 UTC (rev 9709)
+++ trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2008-11-21 16:41:30 UTC (rev 9710)
@@ -56,24 +56,26 @@
                 
                   "Used to check (debug) the serialization system",
                 
-                  "read_and_write <sourcefile> <destfile> \n"
+                  "read_and_write <sourcefile> <destfile> [modification string] ...\n"
                   "Reads an Object (in PLearn serialization format) from the <sourcefile> and writes it to the <destfile>\n"
                   "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
                   "Otherwise (ex: .plearn .vmat) it will. \n"
+                  "If their is modification string in format option=value, the modification will be made to the object before saving\n"
         )
 {}
 
 //! The actual implementation of the 'ReadAndWriteCommand' command 
 void ReadAndWriteCommand::run(const vector<string>& args)
 {
-    if(args.size()!=2)
-        PLERROR("read_and_write takes 2 arguments");
+    if(args.size()<2)
+        PLERROR("read_and_write takes 2 or more arguments: <sourcefile> <destfile> [modification string] ...");
     string source = args[0];
     string dest = args[1];
 
     string ext = extract_extension(source);
     PP<Object> o;
 
+    //read the file
     if(ext==".psave") // may be binay. Don't macro-process
     {
         PLearn::load(source,o);
@@ -86,6 +88,16 @@
         o = readObject(in);
     }
 
+    //modif the object
+    string left;
+    string right;
+    for(uint i=2; i<args.size();i++){
+        split_on_first(args[i], "=", left, right);
+        cout <<left<<endl<<right<<endl;
+        o->setOption(left, right);
+    }
+
+    //write the file
     PStream out = openFile(dest,PStream::plearn_ascii,"w");
     if(!out)
         PLERROR("Could not open file %s for writing",dest.c_str());

Deleted: trunk/commands/PLearnCommands/ReadModifWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadModifWriteCommand.cc	2008-11-21 16:22:37 UTC (rev 9709)
+++ trunk/commands/PLearnCommands/ReadModifWriteCommand.cc	2008-11-21 16:41:30 UTC (rev 9710)
@@ -1,119 +0,0 @@
-// -*- C++ -*-
-
-// ReadModifWriteCommand.cc
-//
-// Copyright (C) 2008 Frederic Bastien
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Frederic Bastien
-
-/*! \file ReadModifWriteCommand.cc */
-
-
-#include "ReadModifWriteCommand.h"
-#include <plearn/base/Object.h>
-#include <plearn/base/stringutils.h>      //!< For extract_extension.
-#include <plearn/io/fileutils.h>        //!< For readFileAndMacroProcess.
-#include <plearn/io/load_and_save.h>
-#include <plearn/io/openFile.h>
-#include <plearn/io/openString.h>
-
-namespace PLearn {
-using namespace std;
-
-//! This allows to register the 'ReadModifWriteCommand' command in the command registry
-PLearnCommandRegistry ReadModifWriteCommand::reg_(new ReadModifWriteCommand);
-
-ReadModifWriteCommand::ReadModifWriteCommand()
-    : PLearnCommand(
-        "read_modif_write",
-        "used to make modification to serialized plearn file",
-        "read_modif_write <sourcefile> <destfile> 'modification string'...\n"
-        "Reads an Object (in PLearn serialization format) from the <sourcefile> and writes it to the <destfile> after appling the modification string (in PLearn serialisation format too)\n"
-        "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
-        "Otherwise (ex: .plearn .vmat) it will. \n"
-        )
-{}
-
-//! The actual implementation of the 'ReadModifWriteCommand' command
-void ReadModifWriteCommand::run(const vector<string>& args)
-{
-    if(args.size()<3)
-        PLERROR("read_and_write takes 3 arguments: "
-                "<sourcefile> <destfile> 'modification string' ...");
-    string source = args[0];
-    string dest = args[1];
-
-    string ext = extract_extension(source);
-    PP<Object> o;
-
-    //read the file
-    if(ext==".psave") // may be binay. Don't macro-process
-    {
-        PLearn::load(source,o);
-    }
-    else
-    {
-        map<string, string> vars;
-        string script = readFileAndMacroProcess(source, vars);
-        PStream in = openString(script,PStream::plearn_ascii);
-        o = readObject(in);
-    }
-
-    //modif the object
-    string left;
-    string right;
-    for(int i=2; i<args.size();i++){
-        split_on_first(args[i], "=", left, right);
-        cout <<left<<endl<<right<<endl;
-        o->setOption(left, right);
-    }
-    //write the file
-    PStream out = openFile(dest,PStream::plearn_ascii,"w");
-    if(!out)
-        PLERROR("Could not open file %s for writing",dest.c_str());
-    out << *o;
-
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/commands/PLearnCommands/ReadModifWriteCommand.h
===================================================================
--- trunk/commands/PLearnCommands/ReadModifWriteCommand.h	2008-11-21 16:22:37 UTC (rev 9709)
+++ trunk/commands/PLearnCommands/ReadModifWriteCommand.h	2008-11-21 16:41:30 UTC (rev 9710)
@@ -1,86 +0,0 @@
-// -*- C++ -*-
-
-// ReadModifWriteCommand.h
-//
-// Copyright (C) 2008 Frederic Bastien
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Frederic Bastien
-
-/*! \file ReadModifWriteCommand.h */
-
-
-#ifndef ReadModifWriteCommand_INC
-#define ReadModifWriteCommand_INC
-
-#include <commands/PLearnCommands/PLearnCommand.h>
-#include <commands/PLearnCommands/PLearnCommandRegistry.h>
-
-namespace PLearn {
-
-/**
- * The first sentence should be a BRIEF DESCRIPTION of what the class does.
- * Place the rest of the class programmer documentation here.  Doxygen supports
- * Javadoc-style comments.  See http://www.doxygen.org/manual.html
- *
- * @todo Write class to-do's here if there are any.
- *
- * @deprecated Write deprecated stuff here if there is any.  Indicate what else
- * should be used instead.
- */
-class ReadModifWriteCommand : public PLearnCommand
-{
-    typedef PLearnCommand inherited;
-
-public:
-    ReadModifWriteCommand();
-    virtual void run(const std::vector<std::string>& args);
-
-protected:
-    static PLearnCommandRegistry reg_;
-};
-
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-11-21 16:22:37 UTC (rev 9709)
+++ trunk/commands/plearn_desjardins.cc	2008-11-21 16:41:30 UTC (rev 9710)
@@ -57,7 +57,6 @@
 #include <commands/PLearnCommands/LearnerCommand.h>
 //#include <commands/PLearnCommands/PairwiseDiffsCommand.h>
 #include <commands/PLearnCommands/ReadAndWriteCommand.h>
-#include <commands/PLearnCommands/ReadModifWriteCommand.h>
 #include <commands/PLearnCommands/RunCommand.h>
 //#include <commands/PLearnCommands/ServerCommand.h>
 //#include <commands/PLearnCommands/TestDependenciesCommand.h>



From nouiz at mail.berlios.de  Fri Nov 21 17:55:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 17:55:35 +0100
Subject: [Plearn-commits] r9711 - trunk/commands
Message-ID: <200811211655.mALGtZTP031581@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 17:55:35 +0100 (Fri, 21 Nov 2008)
New Revision: 9711

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
removed bad include.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-11-21 16:41:30 UTC (rev 9710)
+++ trunk/commands/plearn_noblas_inc.h	2008-11-21 16:55:35 UTC (rev 9711)
@@ -77,7 +77,6 @@
 #include <commands/PLearnCommands/LearnerCommand.h>
 #include <commands/PLearnCommands/PairwiseDiffsCommand.h>
 #include <commands/PLearnCommands/ReadAndWriteCommand.h>
-#include <commands/PLearnCommands/ReadModifWriteCommand.h>
 #include <commands/PLearnCommands/RunCommand.h>
 #include <commands/PLearnCommands/ServerCommand.h>
 #include <commands/PLearnCommands/TestDependenciesCommand.h>



From nouiz at mail.berlios.de  Fri Nov 21 18:07:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 18:07:47 +0100
Subject: [Plearn-commits] r9712 - trunk/commands/PLearnCommands
Message-ID: <200811211707.mALH7lhX000024@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 18:07:46 +0100 (Fri, 21 Nov 2008)
New Revision: 9712

Modified:
   trunk/commands/PLearnCommands/LearnerCommand.cc
   trunk/commands/PLearnCommands/LearnerCommand.h
Log:
added the parameter [--set_testset_as_trainingset] to plearn learner test.
This is needed for RegressionTree.


Modified: trunk/commands/PLearnCommands/LearnerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.cc	2008-11-21 16:55:35 UTC (rev 9711)
+++ trunk/commands/PLearnCommands/LearnerCommand.cc	2008-11-21 17:07:46 UTC (rev 9712)
@@ -68,7 +68,7 @@
                   "     trained_learner.psave. If the optional keyword argument 'no_forget' is provided, then the learner will\n"
                   "     not be reset by calling forget before training.\n"
                   "\n"
-                  "learner test <trained_learner.psave> <testset.vmat> <cost.stats> [<outputs.pmat>] [<costs.pmat>]\n"
+                  "learner test <trained_learner.psave> <testset.vmat> <cost.stats> [<outputs.pmat> [<costs.pmat>]] [--set_testset_as_trainingset]\n"
                   "  -  Tests the specified learner on the testset. Will produce a cost.stats file (viewable with the plearn stats\n"
                   "     command) and optionally saves individual outputs and costs\n"
                   "\n"
@@ -128,7 +128,7 @@
 //////////
 // test //
 //////////
-void LearnerCommand::test(const string& trained_learner_file, const string& testset_spec, const string& stats_file, const string& outputs_file, const string& costs_file)
+void LearnerCommand::test(const string& trained_learner_file, const string& testset_spec, const string& stats_file, const string& outputs_file, const string& costs_file, const bool set_testset_as_trainingset)
 {
     PP<PLearner> learner =
         (PLearner*) smartLoadObject(trained_learner_file);
@@ -138,6 +138,8 @@
     if(outputs_file!="")
         testoutputs = new FileVMatrix(outputs_file,l,learner->outputsize());
     VMat testcosts;
+    if(set_testset_as_trainingset)
+        learner->setTrainingSet(testset);
     if(costs_file!="")
         testcosts = new FileVMatrix(costs_file,l,learner->getTestCostNames());
 
@@ -362,12 +364,26 @@
             string testset_spec = args[2];
             string stats_basename = args[3];
             string outputs_file;
-            if(args.size()>4)
-                outputs_file = args[4];
+            bool set_testset_as_trainingset = false;
+            if(args.size()>4){
+                if(args[4]=="--set_testset_as_trainingset")
+                    set_testset_as_trainingset = true;
+                else
+                    outputs_file = args[4];
+            }
             string costs_file;
-            if(args.size()>5)
-                costs_file = args[5];
-            test(trained_learner_file, testset_spec, stats_basename, outputs_file, costs_file);
+            if(args.size()>5){
+                if(args[5]=="--set_testset_as_trainingset")
+                    set_testset_as_trainingset = true;
+                else
+                    costs_file = args[5];
+            }
+            if(args.size()>6){
+                PLCHECK(args[6]=="--set_testset_as_trainingset");
+                set_testset_as_trainingset = true;
+            }
+            test(trained_learner_file, testset_spec, stats_basename, outputs_file, costs_file,
+                 set_testset_as_trainingset);
         }
         else
             PLERROR("LearnerCommand::run you must provide at least 'plearn learner test <trained_learner.psave> <testset.vmat> <cost.stats>'");

Modified: trunk/commands/PLearnCommands/LearnerCommand.h
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.h	2008-11-21 16:55:35 UTC (rev 9711)
+++ trunk/commands/PLearnCommands/LearnerCommand.h	2008-11-21 17:07:46 UTC (rev 9712)
@@ -66,7 +66,7 @@
                       const PPath& save_learner_file,
                       bool no_forget = false);
 
-    static void test(const string& trained_learner_file, const string& testset_spec, const string& stats_file, const string& outputs_file, const string& costs_file);
+    static void test(const string& trained_learner_file, const string& testset_spec, const string& stats_file, const string& outputs_file, const string& costs_file, const bool set_testset_as_trainingset = false);
     static void compute_outputs(const string& trained_learner_file, const string& test_inputs_spec, const string& outputs_file);
 
     static void process_dataset(const string& trained_learner_file, const string& dataset_spec, const string& processed_dataset_pmat);



From nouiz at mail.berlios.de  Fri Nov 21 18:14:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 18:14:14 +0100
Subject: [Plearn-commits] r9713 - trunk/commands/PLearnCommands
Message-ID: <200811211714.mALHEE48000486@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 18:14:13 +0100 (Fri, 21 Nov 2008)
New Revision: 9713

Modified:
   trunk/commands/PLearnCommands/LearnerCommand.cc
Log:
don't call forget before the test.


Modified: trunk/commands/PLearnCommands/LearnerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.cc	2008-11-21 17:07:46 UTC (rev 9712)
+++ trunk/commands/PLearnCommands/LearnerCommand.cc	2008-11-21 17:14:13 UTC (rev 9713)
@@ -139,7 +139,7 @@
         testoutputs = new FileVMatrix(outputs_file,l,learner->outputsize());
     VMat testcosts;
     if(set_testset_as_trainingset)
-        learner->setTrainingSet(testset);
+        learner->setTrainingSet(testset, false);
     if(costs_file!="")
         testcosts = new FileVMatrix(costs_file,l,learner->getTestCostNames());
 



From nouiz at mail.berlios.de  Fri Nov 21 19:12:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 21 Nov 2008 19:12:14 +0100
Subject: [Plearn-commits] r9714 - trunk/scripts
Message-ID: <200811211812.mALICEYb019280@sheep.berlios.de>

Author: nouiz
Date: 2008-11-21 19:12:13 +0100 (Fri, 21 Nov 2008)
New Revision: 9714

Modified:
   trunk/scripts/perlgrep
Log:
list all supported extension in the help.


Modified: trunk/scripts/perlgrep
===================================================================
--- trunk/scripts/perlgrep	2008-11-21 17:14:13 UTC (rev 9713)
+++ trunk/scripts/perlgrep	2008-11-21 18:12:13 UTC (rev 9714)
@@ -129,7 +129,7 @@
 
 Will perform the specified grep operation on every file in the
 list and recursively in directories.  (only certain kinds of files are
-considered when recursing in directories, .c .cc .h .txt .plearn .pyplearn .py .vmat .pymat Makefile makefile)
+considered when recursing in directories, .c .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README)
 
 Ex: perlgrep '\\bVMatrix\\b' .
 ";



From tihocan at mail.berlios.de  Mon Nov 24 18:05:21 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 24 Nov 2008 18:05:21 +0100
Subject: [Plearn-commits] r9715 - trunk/plearn_learners/meta
Message-ID: <200811241705.mAOH5LVY022451@sheep.berlios.de>

Author: tihocan
Date: 2008-11-24 18:05:20 +0100 (Mon, 24 Nov 2008)
New Revision: 9715

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
Added missing deep copy statements

Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-11-21 18:12:13 UTC (rev 9714)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-11-24 17:05:20 UTC (rev 9715)
@@ -245,19 +245,25 @@
         setTrainingSet(getTrainingSet(),false);
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void AdaBoost::build()
 {
     inherited::build();
     build_();
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void AdaBoost::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(tmp_output2,              copies);
+    deepCopyField(weighted_costs,           copies);
+    deepCopyField(sum_weighted_costs,       copies);
     deepCopyField(learners_error,           copies);
     deepCopyField(example_weights,          copies);
     deepCopyField(weak_learner_output,      copies);
@@ -266,7 +272,9 @@
     deepCopyField(weak_learner_template,    copies);
 }
 
-
+////////////////
+// outputsize //
+////////////////
 int AdaBoost::outputsize() const
 {
     // Outputsize is always 1, since this is a 0-1 classifier



From tihocan at mail.berlios.de  Mon Nov 24 22:13:02 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 24 Nov 2008 22:13:02 +0100
Subject: [Plearn-commits] r9716 - trunk/plearn_learners/hyper
Message-ID: <200811242113.mAOLD2St032170@sheep.berlios.de>

Author: tihocan
Date: 2008-11-24 22:13:01 +0100 (Mon, 24 Nov 2008)
New Revision: 9716

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
Fixed a few indents/English typos

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-24 17:05:20 UTC (rev 9715)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-11-24 21:13:01 UTC (rev 9716)
@@ -131,9 +131,9 @@
 
     declareOption(ol, "reloaded", &HyperLearner::reloaded,
                   OptionBase::learntoption|OptionBase::nosave,
-                  "Used internally to don't reload a file many as the build function\n"
-                  " can be called many time after the expdir is set. In particular\n"
-                  " PLearn::HyperLearner::setTrainingSet.");
+        "Used internally to avoid reloading a file, since the build function\n"
+        "may be called many times after the expdir is set,in particular in\n"
+        "PLearn::HyperLearner::setTrainingSet.");
 
     inherited::declareOptions(ol);
 
@@ -199,10 +199,10 @@
     if (call_forget)
     {
         if(reloaded)
-            PLWARNING("In HyperLearner::setTrainingSet() - we where asked to"
+            PLWARNING("In HyperLearner::setTrainingSet() - we were asked to"
                       " forget after having reloaded a previous version."
-                      " To don't do this, in the PTester that include this"
-                      " HyperLearner set call_forget_in_run = 0.");
+                      " To avoid doing this, in the PTester that includes this"
+                      " HyperLearner, set call_forget_in_run = 0.");
         build();
         forget();
     }



From tihocan at mail.berlios.de  Mon Nov 24 22:40:08 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 24 Nov 2008 22:40:08 +0100
Subject: [Plearn-commits] r9717 - trunk/plearn/vmat
Message-ID: <200811242140.mAOLe85q002108@sheep.berlios.de>

Author: tihocan
Date: 2008-11-24 22:40:08 +0100 (Mon, 24 Nov 2008)
New Revision: 9717

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Fixed line length

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-11-24 21:13:01 UTC (rev 9716)
+++ trunk/plearn/vmat/VMatrix.cc	2008-11-24 21:40:08 UTC (rev 9717)
@@ -1816,7 +1816,8 @@
 ///////////////
 void VMatrix::appendRow(Vec v)
 {
-    PLERROR("This method (appendRow) not implemented by VMatrix subclass '%s'!",classname().c_str());
+    PLERROR("In VMatrix::appendRow - Not implemented by VMatrix subclass '%s'",
+            classname().c_str());
 }
 
 ///////////////



From tihocan at mail.berlios.de  Mon Nov 24 22:40:39 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 24 Nov 2008 22:40:39 +0100
Subject: [Plearn-commits] r9718 - trunk/plearn/vmat
Message-ID: <200811242140.mAOLedo0002381@sheep.berlios.de>

Author: tihocan
Date: 2008-11-24 22:40:39 +0100 (Mon, 24 Nov 2008)
New Revision: 9718

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
Log:
Option save_fields is now a PPath instead of a string

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-24 21:40:08 UTC (rev 9717)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-11-24 21:40:39 UTC (rev 9718)
@@ -155,10 +155,8 @@
 
     declareOption(ol, "save_fields", &SelectColumnsVMatrix::save_fields,
                   OptionBase::buildoption,
-                  "The filename where to save the filename of this VMatrix."
-        );
+        "Optional file where the fieldnames of this VMatrix should be saved.");
 
-
     declareOption(ol, "fields_partial_match", &SelectColumnsVMatrix::fields_partial_match, OptionBase::buildoption,
                   "If set to 1, then a field will be kept iff it contains one of the strings from 'fields'.");
 
@@ -313,8 +311,8 @@
         sinput.resize(width());
         sinput.fill(MISSING_VALUE);
 
-        if(!save_fields.empty())
-            PLearn::save(save_fields,fieldNames());
+        if(!save_fields.isEmpty())
+            PLearn::save(save_fields, fieldNames());
     }
 }
 

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-11-24 21:40:08 UTC (rev 9717)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-11-24 21:40:39 UTC (rev 9718)
@@ -70,7 +70,7 @@
     bool extend_with_missing;
     TVec<int> indices;
     TVec<string> fields;
-    string save_fields;
+    PPath save_fields;
     bool fields_partial_match;
     bool inverse_fields_selection;
     bool warn_non_selected_field;



From tihocan at mail.berlios.de  Tue Nov 25 17:47:57 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 25 Nov 2008 17:47:57 +0100
Subject: [Plearn-commits] r9719 - trunk/plearn_learners/regressors
Message-ID: <200811251647.mAPGlvIc001181@sheep.berlios.de>

Author: tihocan
Date: 2008-11-25 17:47:57 +0100 (Tue, 25 Nov 2008)
New Revision: 9719

Modified:
   trunk/plearn_learners/regressors/LinearRegressor.cc
Log:
Added missing deep copy statements

Modified: trunk/plearn_learners/regressors/LinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/LinearRegressor.cc	2008-11-24 21:40:39 UTC (rev 9718)
+++ trunk/plearn_learners/regressors/LinearRegressor.cc	2008-11-25 16:47:57 UTC (rev 9719)
@@ -179,6 +179,8 @@
     // ### that you wish to be deepCopied rather than 
     // ### shallow-copied.
     // ### ex:
+    deepCopyField(extendedinput, copies);
+    deepCopyField(input, copies);
     deepCopyField(train_costs, copies);
     deepCopyField(XtX, copies);
     deepCopyField(XtY, copies);



From tihocan at mail.berlios.de  Tue Nov 25 18:01:21 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 25 Nov 2008 18:01:21 +0100
Subject: [Plearn-commits] r9720 - trunk/plearn_learners/classifiers
Message-ID: <200811251701.mAPH1L9q002920@sheep.berlios.de>

Author: tihocan
Date: 2008-11-25 18:01:21 +0100 (Tue, 25 Nov 2008)
New Revision: 9720

Modified:
   trunk/plearn_learners/classifiers/KNNClassifier.cc
Log:
- Added missing deep copy statements
- Removed static statements when computing costnames: it is not a good idea when the result can vary depending on object options. If performance is an issue, just make it a class variable.


Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-25 16:47:57 UTC (rev 9719)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2008-11-25 17:01:21 UTC (rev 9720)
@@ -195,8 +195,11 @@
     deepCopyField(knn_output,    copies);
     deepCopyField(knn_costs,     copies);
     deepCopyField(class_weights, copies);
+    deepCopyField(multi_k_output,copies);
+    deepCopyField(multi_k_input, copies);
     deepCopyField(knn,           copies);
     deepCopyField(kernel,        copies);
+    deepCopyField(multi_k,       copies);
     inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
@@ -341,7 +344,7 @@
 TVec<string> KNNClassifier::getTestCostNames() const
 {
     int n_multi_k = multi_k.length();
-    static TVec<string> costs(2*(1+n_multi_k));
+    TVec<string> costs(2*(1+n_multi_k));
     costs[0] = "class_error";
     costs[1] = "neglogprob";
     for(int k=0; k<n_multi_k; k++)



From nouiz at mail.berlios.de  Tue Nov 25 21:53:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Nov 2008 21:53:39 +0100
Subject: [Plearn-commits] r9721 - in trunk/commands: . PLearnCommands
Message-ID: <200811252053.mAPKrd77009048@sheep.berlios.de>

Author: nouiz
Date: 2008-11-25 21:53:39 +0100 (Tue, 25 Nov 2008)
New Revision: 9721

Added:
   trunk/commands/PLearnCommands/StatsCommand.cc
   trunk/commands/PLearnCommands/StatsCommand.h
Modified:
   trunk/commands/plearn_desjardins.cc
   trunk/commands/plearn_noblas_inc.h
Log:
added a plearn command that allow to calculate some stats on stats file.


Added: trunk/commands/PLearnCommands/StatsCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/StatsCommand.cc	2008-11-25 17:01:21 UTC (rev 9720)
+++ trunk/commands/PLearnCommands/StatsCommand.cc	2008-11-25 20:53:39 UTC (rev 9721)
@@ -0,0 +1,134 @@
+// -*- C++ -*-
+
+// StatsCommand.cc
+//
+// Copyright (C) 2008 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file StatsCommand.cc */
+
+
+#include "StatsCommand.h"
+#include <plearn/math/StatsCollector.h>
+#include <plearn/math/VecStatsCollector.h>
+#include <plearn/math/TMat_sort.h>
+#include <plearn/io/load_and_save.h>
+#include <boost/regex.hpp>
+#include <plearn/base/plerror.h>
+#include <plearn/base/stringutils.h>
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'StatsCommand' command in the command registry
+PLearnCommandRegistry StatsCommand::reg_(new StatsCommand);
+
+StatsCommand::StatsCommand()
+    : PLearnCommand(
+        "stats",
+        "allow to extract some stats from stats file",
+        "plearn stats sum_sort <statsfile> --filter=boost_regex --replace=boost_regex=new_value\n"
+        "FULL DETAILED HELP HERE \n"
+        )
+{}
+
+//! The actual implementation of the 'StatsCommand' command
+void StatsCommand::run(const vector<string>& args)
+{
+    if(args[0]=="sum_sort"){
+        //plearn Stats sum_sort <statsfile> --filter=boost_regex --replace=boost_regex=new_value
+        if(args.size()<2)
+            PLERROR("plearn stats sum_sort <statsfile> --filter=boost_regex --replace=boost_regex=new_value");
+
+        //default regex that match all non new line caracter
+        boost::regex filter(".*");
+        boost::regex replace_re;
+        string replace_new = "";
+
+            
+        string statsfile = args[1];
+        string test_re;
+        try{
+            for(uint i=2;i<args.size();i++){
+                if(args[i].substr(0,9)=="--filter="){
+                    test_re=args[i].substr(9);
+                    filter.assign(test_re);
+                }else if(args[i].substr(0,10)=="--replace="){
+                    split_on_first(args[i].substr(10), "=",
+                                   test_re, replace_new);
+                    replace_re.assign(test_re);
+                }else
+                    PLERROR("In StatsCommand::run() - unknow parameter '%s'", 
+                            args[i].c_str());
+            }
+        }catch (boost::regex_error& e){
+            PLERROR("invalid regular expression: \"%s\"\n %s",
+                    test_re.c_str(),
+                    e.what());
+        }
+        TVec<StatsCollector> stats1;
+        VecStatsCollector stats;
+
+        PLearn::load(statsfile, stats);
+        Mat m(stats.size(),2);
+        for(int i = 0;i<stats.size();i++){
+            m(i,0)=i;
+            m(i,1)=stats.getStats(i).sum();
+        }
+
+        pout<<"NAME "<<"SUM"<<endl;
+        sortRows(m,1,false);
+        for(int i=0;i<m.length();i++){
+            string f = stats.getFieldNames()[int(m(i,0))];
+            if(boost::regex_match(f, filter)){
+                if(replace_re.size()>0)
+                    f = boost::regex_replace(f,replace_re,"");
+                pout<<f<<" "<<m(i,1)<<endl;
+            }
+        }
+    }else
+        PLERROR("Currently only the sub commands sum_sort is supported!");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/commands/PLearnCommands/StatsCommand.h
===================================================================
--- trunk/commands/PLearnCommands/StatsCommand.h	2008-11-25 17:01:21 UTC (rev 9720)
+++ trunk/commands/PLearnCommands/StatsCommand.h	2008-11-25 20:53:39 UTC (rev 9721)
@@ -0,0 +1,86 @@
+// -*- C++ -*-
+
+// StatsCommand.h
+//
+// Copyright (C) 2008 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file StatsCommand.h */
+
+
+#ifndef StatsCommand_INC
+#define StatsCommand_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class StatsCommand : public PLearnCommand
+{
+    typedef PLearnCommand inherited;
+
+public:
+    StatsCommand();
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-11-25 17:01:21 UTC (rev 9720)
+++ trunk/commands/plearn_desjardins.cc	2008-11-25 20:53:39 UTC (rev 9721)
@@ -61,6 +61,7 @@
 //#include <commands/PLearnCommands/ServerCommand.h>
 //#include <commands/PLearnCommands/TestDependenciesCommand.h>
 //#include <commands/PLearnCommands/TestDependencyCommand.h>
+#include <commands/PLearnCommands/StatsCommand.h>
 
 /************
  * PLearner *

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-11-25 17:01:21 UTC (rev 9720)
+++ trunk/commands/plearn_noblas_inc.h	2008-11-25 20:53:39 UTC (rev 9721)
@@ -81,6 +81,7 @@
 #include <commands/PLearnCommands/ServerCommand.h>
 #include <commands/PLearnCommands/TestDependenciesCommand.h>
 #include <commands/PLearnCommands/TestDependencyCommand.h>
+#include <commands/PLearnCommands/StatsCommand.h>
 
 // * extra stuff from Boost to generate help *
 //#include <commands/PLearnCommands/HTMLHelpCommand.h>//<! DEPRECATED (will disappear soon)



From ducharme at mail.berlios.de  Wed Nov 26 16:27:26 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 26 Nov 2008 16:27:26 +0100
Subject: [Plearn-commits] r9722 - trunk/plearn_learners/regressors
Message-ID: <200811261527.mAQFRQQ8004257@sheep.berlios.de>

Author: ducharme
Date: 2008-11-26 16:27:26 +0100 (Wed, 26 Nov 2008)
New Revision: 9722

Modified:
   trunk/plearn_learners/regressors/PruningLinearRegressor.cc
Log:
Bug fix in computing number of weights (regression coefficients) to keep.


Modified: trunk/plearn_learners/regressors/PruningLinearRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/PruningLinearRegressor.cc	2008-11-25 20:53:39 UTC (rev 9721)
+++ trunk/plearn_learners/regressors/PruningLinearRegressor.cc	2008-11-26 15:27:26 UTC (rev 9722)
@@ -211,6 +211,7 @@
 
     // Sort all t-ratios
     int nb_weights = weights.length();
+    PLASSERT(nb_weights == t_ratio.length());
     Vec t_ratio_sort = t_ratio.copy();
     sortElements(t_ratio_sort, true);
 
@@ -219,7 +220,8 @@
     if (pruning_method == "max_number")
     {
         int keep_n_weights = min(max_number, nb_weights);
-        if (include_bias)
+        // Add one coefficient if max is not yet reached
+        if (include_bias  &&  keep_n_weights<nb_weights)
             ++keep_n_weights;
         t_ratio_threshold = t_ratio_sort[keep_n_weights-1];
     }



From nouiz at mail.berlios.de  Wed Nov 26 20:07:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 26 Nov 2008 20:07:57 +0100
Subject: [Plearn-commits] r9723 - trunk/scripts
Message-ID: <200811261907.mAQJ7v1i019391@sheep.berlios.de>

Author: nouiz
Date: 2008-11-26 20:07:56 +0100 (Wed, 26 Nov 2008)
New Revision: 9723

Modified:
   trunk/scripts/dbidispatch
Log:
added option --exp_dir to dbidispatch. Joseph gived me a first version that I modified to make it more compatible with dbidispatch. From the help:

  The '--exp_dir=dir' specifies the name of the temporary directory
    relative to LOGDIR, instead of one generated automatically based
    upon the command line and the time.




Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-11-26 15:27:26 UTC (rev 9722)
+++ trunk/scripts/dbidispatch	2008-11-26 19:07:56 UTC (rev 9723)
@@ -4,7 +4,7 @@
 from socket import gethostname
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] <back-end parameter> {--file=FILEPATH | <command-template>}
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>}
 
 <back-end parameter>:
     bqtools, cluster option  :[--duree=X]
@@ -55,6 +55,9 @@
     --ssh=N is the same as --ssh --nb_proc=N
     --condor=N  is the same as --condor --nb_proc=N
     condor and bqtools default -1. Cluster default 32.  local and ssh default 1
+  The '--exp_dir=dir' specifies the name of the temporary directory
+    relative to LOGDIR, instead of one generated automatically based
+    upon the command line and the time.
   The '--[*no_]clean_up' set the DBI option clean_up to true or false
   The 'DBIDISPATCH_DEFAULT_OPTION' environnement variable can contain default
     option for dbidispatch. They can be overrided on the command line.
@@ -290,7 +293,7 @@
         testmode=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
-                                "--universe"]:
+                                "--universe", "--exp_dir"]:
         param=argv.split('=')[0][2:]
         if param in ["req", "files", "rank"]:
             #param that we happend to if defined more then one time
@@ -337,7 +340,7 @@
     print ShortHelp
     sys.exit(1)
 
-valid_dbi_param=["clean_up", "test", "dolog","nb_proc"]
+valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir"]
 if launch_cmd=="Cluster":
     valid_dbi_param +=["cwait","force","arch","interruptible",
                        "duree","cpu","mem","os"]
@@ -448,7 +451,9 @@
 else:
     (commands,choise_args)=generate_commands(command_argv)
 
-if FILE == "":
+if dbi_param.has_key("exp_dir"):
+    dbi_param["log_dir"]=os.path.join(LOGDIR,dbi_param["exp_dir"])
+elif FILE == "":
     t = [x for x in sys.argv[1:] if not x[:2]=="--"]
     t[0]=os.path.split(t[0])[1]
     tmp="_".join(t)
@@ -468,10 +473,9 @@
 
     tmp+='_'+date_str.replace(' ','_')
     dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
-    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 else:
     dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
-    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
+dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
 n="base_tasks_log_file"
 dbi_param[n]=[""]*len(commands)



From tihocan at mail.berlios.de  Wed Nov 26 21:47:23 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 26 Nov 2008 21:47:23 +0100
Subject: [Plearn-commits] r9724 - in trunk/plearn: base
	base/test/.pytest/PL_assert/expected_results
	base/test/.pytest/PL_check/expected_results io
	io/test/.pytest/test_ppath/expected_results
Message-ID: <200811262047.mAQKlNK6030633@sheep.berlios.de>

Author: tihocan
Date: 2008-11-26 21:47:22 +0100 (Wed, 26 Nov 2008)
New Revision: 9724

Modified:
   trunk/plearn/base/plerror.cc
   trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
   trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
   trunk/plearn/io/PPath.cc
   trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log
Log:
Avoid having PLERROR statements that span multiple lines when these error messages appear in the test suite, as the line number being displayed may vary depending on the compiler

Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-11-26 19:07:56 UTC (rev 9723)
+++ trunk/plearn/base/plerror.cc	2008-11-26 20:47:22 UTC (rev 9724)
@@ -192,32 +192,47 @@
     exit(1);
 }
 
-
-void pl_assert_fail(const char* expr, const char* file, unsigned line,
-                    const char* function, const string& message)
+//! Return a typical error message.
+string get_error_message(const char* type, const char* expr,
+        const char* function, const char* file, unsigned line,
+        const string& message)
 {
-    PLERROR("Assertion failed: %s\n"
+    // Allocate buffer.
+    size_t size = strlen(type) + strlen(expr) + strlen(function) + strlen(file)
+                    + message.size() + 150;
+    char msg[size];
+    // Format string.
+    snprintf(msg, size, 
+            "%s failed: %s\n"
             "Function: %s\n"
             "    File: %s\n"
-            "    Line: %d"
+            "    Line: %u"
             "%s%s",
-            expr, function, file, line,
+            type, expr, function, file, line,
             (!message.empty()? "\n Message: " : ""),
             message.c_str());
+    // Return as an STL string.
+    return string(msg);
 }
 
+void pl_assert_fail(const char* expr, const char* file, unsigned line,
+                    const char* function, const string& message)
+{
+    // Note that in this function, just like in 'pl_check_fail' below, it is
+    // important that the PLERROR statement fits on a single line. This is
+    // because otherwise some tests may fail on some compilers, as the line
+    // number of a multi-line statement may be ambiguous.
+    string msg = get_error_message("Assertion",
+            expr, function, file, line, message);
+    PLERROR(msg.c_str());
+}
 
 void pl_check_fail(const char* expr, const char* file, unsigned line,
-                   const char* function, const string& message)
+        const char* function, const string& message)
 {
-    PLERROR("Check failed: %s\n"
-            "Function: %s\n"
-            "    File: %s\n"
-            "    Line: %d"
-            "%s%s",
-            expr, function, file, line,
-            (!message.empty()? "\n Message: " : ""),
-            message.c_str());
+    string msg = get_error_message("Check",
+            expr, function, file, line, message);
+    PLERROR(msg.c_str());
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-11-26 19:07:56 UTC (rev 9723)
+++ trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-11-26 20:47:22 UTC (rev 9724)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 206
+FATAL ERROR: In file: "plerror.cc" at line 227
 Assertion failed: 3+8 == 123+46
 Function: int main()
     File: assertions.cc

Modified: trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-11-26 19:07:56 UTC (rev 9723)
+++ trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-11-26 20:47:22 UTC (rev 9724)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 220
+FATAL ERROR: In file: "plerror.cc" at line 235
 Check failed: ein == stein
 Function: void PLearn::PLCheckTest::perform()
     File: PLCheckTest.cc

Modified: trunk/plearn/io/PPath.cc
===================================================================
--- trunk/plearn/io/PPath.cc	2008-11-26 19:07:56 UTC (rev 9723)
+++ trunk/plearn/io/PPath.cc	2008-11-26 20:47:22 UTC (rev 9724)
@@ -603,10 +603,11 @@
             size_t pos_previous_slash = pos_sdd == 0 ? npos
                 : rfind(_slash_char(), pos_sdd - 1);
             if (pos_previous_slash == npos) {
-                // We need to make sure we are not trying to go up on a root directory.
+                // We need to make sure we are not trying to go up on a root
+                // directory.
                 if (PPath(substr(0, pos_sdd + 1)).isRoot())
-                    PLERROR("In PPath::resolveDots - '%s' is invalid",
-                            errorDisplay().c_str());
+                    // Long single-line error message to ensure tests pass.
+                    PLERROR("In PPath::resolveDots - '%s' is invalid", errorDisplay().c_str());
                 if (   (pos_sdd == 2 && substr(0,2) == "..")
                        || (pos_sdd == 1 && operator[](0) == '.'))
                     // We are in the case "../.." or "./.."
@@ -847,8 +848,8 @@
 {
     if ( _protocol.empty()) {
         if (!isAbsPath())
-            PLERROR("In PPath::addProtocol - A protocol can only be added to an "
-                    "absolute path, and '%s' is relative", errorDisplay().c_str());
+            // Ugly single-line error message to ensure tests pass.
+            PLERROR("In PPath::addProtocol - A protocol can only be added to an absolute path, and '%s' is relative", errorDisplay().c_str());
         return ( PPath(string(FILE_PROTOCOL) + ':' + string(*this)) );
     }
     return PPath( *this );
@@ -927,8 +928,11 @@
 PPath PPath::up() const
 {
     if (isEmpty() || isRoot())
-        PLERROR("In PPath::up - Cannot go up on directory '%s'",
-                errorDisplay().c_str());
+        // Note that the following line has more than 80 characters, but it is
+        // the simplest way to avoid issues in tests, as the line number of
+        // this error is displayed, and it may be ambiguous if it spans
+        // multiple lines.
+        PLERROR("In PPath::up - Cannot go up on directory '%s'", errorDisplay().c_str());
     return *this / "..";
 }
 

Modified: trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log
===================================================================
--- trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log	2008-11-26 19:07:56 UTC (rev 9723)
+++ trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log	2008-11-26 20:47:22 UTC (rev 9724)
@@ -32,10 +32,10 @@
 
 #####  Methods up and dirname  ############################################
 
-PPath('PL_ROOT:').up()                                      FATAL ERROR: In file: "PPath.cc" at line 931
+PPath('PL_ROOT:').up()                                      FATAL ERROR: In file: "PPath.cc" at line 935
 In PPath::up - Cannot go up on directory 'PL_ROOT:'
 
-PPath('').up()                                              FATAL ERROR: In file: "PPath.cc" at line 931
+PPath('').up()                                              FATAL ERROR: In file: "PPath.cc" at line 935
 In PPath::up - Cannot go up on directory ''
 
 PPath('PL_ROOT:foo').up() == 'PL_ROOT:'                     True
@@ -78,7 +78,7 @@
 
 PPath('/foo/bar').addProtocol()                             file:/foo/bar
 
-PPath('foo/bar').addProtocol()                              FATAL ERROR: In file: "PPath.cc" at line 851
+PPath('foo/bar').addProtocol()                              FATAL ERROR: In file: "PPath.cc" at line 852
 In PPath::addProtocol - A protocol can only be added to an absolute path, and 'foo/bar' is relative
 
 PPath('file:/foo/bar').removeProtocol()                     PL_ROOT:foo/bar
@@ -133,10 +133,10 @@
 
 foo./bar                                                    foo./bar
 
-PL_ROOT:..                                                  FATAL ERROR: In file: "PPath.cc" at line 609
+PL_ROOT:..                                                  FATAL ERROR: In file: "PPath.cc" at line 610
 In PPath::resolveDots - 'PL_ROOT:..' is invalid
 
-PL_ROOT:../foo                                              FATAL ERROR: In file: "PPath.cc" at line 609
+PL_ROOT:../foo                                              FATAL ERROR: In file: "PPath.cc" at line 610
 In PPath::resolveDots - 'PL_ROOT:../foo' is invalid
 
 ../foo                                                      ../foo



From tihocan at mail.berlios.de  Wed Nov 26 21:55:37 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 26 Nov 2008 21:55:37 +0100
Subject: [Plearn-commits] r9725 - trunk/plearn/base
Message-ID: <200811262055.mAQKtbpk031404@sheep.berlios.de>

Author: tihocan
Date: 2008-11-26 21:55:36 +0100 (Wed, 26 Nov 2008)
New Revision: 9725

Modified:
   trunk/plearn/base/plerror.cc
Log:
Fixed compilation for strict ISO C++ compilers

Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-11-26 20:47:22 UTC (rev 9724)
+++ trunk/plearn/base/plerror.cc	2008-11-26 20:55:36 UTC (rev 9725)
@@ -200,7 +200,7 @@
     // Allocate buffer.
     size_t size = strlen(type) + strlen(expr) + strlen(function) + strlen(file)
                     + message.size() + 150;
-    char msg[size];
+    char* msg = (char*) malloc(size * sizeof(char));
     // Format string.
     snprintf(msg, size, 
             "%s failed: %s\n"



From chrish at mail.berlios.de  Wed Nov 26 23:35:20 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 26 Nov 2008 23:35:20 +0100
Subject: [Plearn-commits] r9726 - trunk/plearn_learners/online
Message-ID: <200811262235.mAQMZK7J009131@sheep.berlios.de>

Author: chrish
Date: 2008-11-26 23:35:19 +0100 (Wed, 26 Nov 2008)
New Revision: 9726

Modified:
   trunk/plearn_learners/online/RBMConnection.cc
   trunk/plearn_learners/online/RBMConnection.h
   trunk/plearn_learners/online/RBMLayer.cc
Log:
Export some RBM functions as remote methods.

Modified: trunk/plearn_learners/online/RBMConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConnection.cc	2008-11-26 20:55:36 UTC (rev 9725)
+++ trunk/plearn_learners/online/RBMConnection.cc	2008-11-26 22:35:19 UTC (rev 9726)
@@ -37,11 +37,10 @@
 /*! \file RBMConnection.cc */
 
 
-
 #include "RBMConnection.h"
 #include <plearn/math/TMat_maths.h>
 #include <plearn/base/stringutils.h>
-//#include "RBMLayer.h"
+#include <plearn/base/RemoteMethodMap.h>
 
 namespace PLearn {
 using namespace std;
@@ -113,6 +112,22 @@
                     "Equals to up_size");
 }
 
+void RBMConnection::declareMethods(RemoteMethodMap& rmm)
+{
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm, "setAsDownInput", &RBMConnection::setAsDownInput,
+                  (BodyDoc("Sets 'input_vec' to 'input', and 'going_up' to true. \n"
+                           "Note that no data copy is made, so 'input' should not be modified \n"
+                           "afterwards."),
+                   ArgDoc("const Vec& input", "The input vector")));
+    declareMethod(rmm, "setAsUpInput", &RBMConnection::setAsUpInput,
+                  (BodyDoc("Sets 'input_vec' to 'input', and 'going_up' to false. \n"
+                           "Note that no data copy is made, so 'input' should not be modified \n"
+                           "afterwards."),
+                   ArgDoc("const Vec& input", "The input vector")));
+}
+
 ////////////
 // build_ //
 ////////////

Modified: trunk/plearn_learners/online/RBMConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMConnection.h	2008-11-26 20:55:36 UTC (rev 9725)
+++ trunk/plearn_learners/online/RBMConnection.h	2008-11-26 22:35:19 UTC (rev 9726)
@@ -272,6 +272,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Declares the methods that are remote-callable.
+    static void declareMethods(RemoteMethodMap& rmm);
+
 private:
     //#####  Private Member Functions  ########################################
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-11-26 20:55:36 UTC (rev 9725)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-11-26 22:35:19 UTC (rev 9726)
@@ -161,8 +161,18 @@
 
     declareMethod(rmm, "setAllBias", &RBMLayer::setAllBias,
                   (BodyDoc("Set the biases values"),
-                   ArgDoc ("bias", "the vector of biases")
-                  ));
+                   ArgDoc ("bias", "the vector of biases")));
+
+    declareMethod(rmm, "generateSample", &RBMLayer::generateSample,
+                  (BodyDoc("Generate a sample, and update the sample field")));
+    declareMethod(rmm, "getAllActivations", &RBMLayer::getAllActivations,
+                  (BodyDoc("Uses 'rbmc' to obtain the activations of all units in this layer. \n"
+                           "Unit 0 of this layer corresponds to unit 'offset' of 'rbmc'."),
+                   ArgDoc("PP<RBMConnection> rbmc", "RBM Connection"),
+                   ArgDoc("int offset", "Offset"),
+                   ArgDoc("bool minibatch", "Use minibatch")));
+    declareMethod(rmm, "computeExpectation", &RBMLayer::computeExpectation,
+                  (BodyDoc("Compute expectation.")));
 }
 
 ////////////



From tihocan at mail.berlios.de  Thu Nov 27 16:38:19 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Nov 2008 16:38:19 +0100
Subject: [Plearn-commits] r9727 - trunk/plearn/base
Message-ID: <200811271538.mARFcJLZ018819@sheep.berlios.de>

Author: tihocan
Date: 2008-11-27 16:38:18 +0100 (Thu, 27 Nov 2008)
New Revision: 9727

Modified:
   trunk/plearn/base/plerror.cc
Log:
Fixed memory leak I introduced earlier, thanks Christian!

Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-11-26 22:35:19 UTC (rev 9726)
+++ trunk/plearn/base/plerror.cc	2008-11-27 15:38:18 UTC (rev 9727)
@@ -200,7 +200,7 @@
     // Allocate buffer.
     size_t size = strlen(type) + strlen(expr) + strlen(function) + strlen(file)
                     + message.size() + 150;
-    char* msg = (char*) malloc(size * sizeof(char));
+    char* msg = new char[size];
     // Format string.
     snprintf(msg, size, 
             "%s failed: %s\n"
@@ -212,7 +212,9 @@
             (!message.empty()? "\n Message: " : ""),
             message.c_str());
     // Return as an STL string.
-    return string(msg);
+    string result(msg);
+    delete[] msg;
+    return result;
 }
 
 void pl_assert_fail(const char* expr, const char* file, unsigned line,



From tihocan at mail.berlios.de  Thu Nov 27 17:32:23 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Nov 2008 17:32:23 +0100
Subject: [Plearn-commits] r9728 - in trunk/plearn/base/test/.pytest:
	PL_assert/expected_results PL_check/expected_results
Message-ID: <200811271632.mARGWNR0025215@sheep.berlios.de>

Author: tihocan
Date: 2008-11-27 17:32:22 +0100 (Thu, 27 Nov 2008)
New Revision: 9728

Modified:
   trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
   trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
Log:
Regenerated test results to match new line numbers

Modified: trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-11-27 15:38:18 UTC (rev 9727)
+++ trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-11-27 16:32:22 UTC (rev 9728)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 227
+FATAL ERROR: In file: "plerror.cc" at line 229
 Assertion failed: 3+8 == 123+46
 Function: int main()
     File: assertions.cc

Modified: trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-11-27 15:38:18 UTC (rev 9727)
+++ trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-11-27 16:32:22 UTC (rev 9728)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 235
+FATAL ERROR: In file: "plerror.cc" at line 237
 Check failed: ein == stein
 Function: void PLearn::PLCheckTest::perform()
     File: PLCheckTest.cc



