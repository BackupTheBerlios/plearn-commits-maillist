<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6749 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-March/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6749%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703172317.l2HNHRma004626%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000197.html">
   <LINK REL="Next"  HREF="000199.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6749 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6749%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703172317.l2HNHRma004626%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6749 - trunk/plearn_learners/generic/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Sun Mar 18 00:17:27 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000197.html">[Plearn-commits] r6748 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
        <LI>Next message: <A HREF="000199.html">[Plearn-commits] r6750 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#198">[ date ]</a>
              <a href="thread.html#198">[ thread ]</a>
              <a href="subject.html#198">[ subject ]</a>
              <a href="author.html#198">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-03-18 00:17:27 +0100 (Sun, 18 Mar 2007)
New Revision: 6749

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-03-17 17:25:59 UTC (rev 6748)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-03-17 23:17:27 UTC (rev 6749)
@@ -51,15 +51,15 @@
     &quot;into covariance-corrected update directions v_t is the following:\n\n&quot;
     &quot;operator(int t, Vec g, Vec v): (reads g and writes v)\n&quot;
     &quot;    i = t%b   /* denoting b = cov_minibatch_size */\n&quot;
-    &quot;    extend X by a (k+i)-th column \gamma^{\frac{-i}{2}} g\n&quot;
+    &quot;    extend X by a (k+i)-th column gamma^{\frac{-i}{2}} g\n&quot;
     &quot;    extend G by a (k+i)-th column and row, with G_{k+i,.}=X'_{k+1,.} X\n&quot;
     &quot;      and idem for the symmetric sub-column\n&quot;
-    &quot;    extend vectors r and a by (k+i)-th element, r_{k+i-1}=0, r_{k+i}=\gamma^{\frac{-i}{2}}\n&quot;
-    &quot;    Solve linear system (G + \gamma^{-k} \lambda I) a = r in a\n&quot;
-    &quot;    v = X a\n&quot;
+    &quot;    extend vectors r and a by (k+i)-th element, r_{k+i-1}=0, r_{k+i}=gamma^{\frac{-i}{2}}\n&quot;
+    &quot;    Solve linear system (G + gamma^{-k} lambda I) a = r in a\n&quot;
+    &quot;    v = X a (1 - gamma)/(1 - gamma^t)\n&quot;
     &quot;    if i+1==b\n&quot;
     &quot;       (V,D) = leading_eigendecomposition(G,k)\n&quot;
-    &quot;       U = X V\n&quot;
+    &quot;       U = gamma^{b/2} X V\n&quot;
     &quot;\n\n&quot;
     &quot;See technical report 'A new insight on the natural gradient' for justifications\n&quot;
     );
@@ -72,6 +72,7 @@
       gamma(0.99),
       n_dim(-1),
       verbosity(0),
+      renormalize(true),
       previous_t(-1)
 {
     build();
@@ -138,6 +139,9 @@
     declareOption(ol, &quot;verbosity&quot;, &amp;NatGradEstimator::verbosity,
                   OptionBase::buildoption,
                   &quot;Verbosity level\n&quot;);
+    declareOption(ol, &quot;renormalize&quot;, &amp;NatGradEstimator::renormalize,
+                  OptionBase::buildoption,
+                  &quot;Wether to renormalize z wrt scaling that gamma produces\n&quot;);
 
     declareOption(ol, &quot;n_dim&quot;, &amp;NatGradEstimator::n_dim,
                   OptionBase::learntoption,
@@ -177,13 +181,13 @@
         Vkt = Vt.subMatRows(0,n_eigen);
         D.resize(n_eigen+1);
         G.resize(n_eigen + cov_minibatch_size, n_eigen + cov_minibatch_size);
+        A.resize(n_eigen + cov_minibatch_size, n_eigen + cov_minibatch_size);
         G.clear();
         Xt.resize(n_eigen+cov_minibatch_size, n_dim);
         Xt.clear();
         r.resize(n_eigen);
     }
 }
-
 void NatGradEstimator::operator()(int t, const Vec&amp; g, Vec v)
 {
     if (t!=0)
@@ -196,10 +200,10 @@
         init();
     }
     int i = t % cov_minibatch_size;
-    int n = i+cov_minibatch_size;
+    int n = n_eigen+i;
     Xt.resize(n+1,n_dim);
     Vec newX = Xt(n);
-    real rn = pow(gamma,-0.5*i);
+    real rn = pow(gamma,-0.5*(i+1));
     multiply(g,rn,newX);
     G.resize(n+1,n+1);
     Vec newG=G(n);
@@ -210,8 +214,10 @@
     r[n] = rn;
     // solve linear system (G + \gamma^{-k} \lambda I) a = r
     pivots.resize(n);
+    A.resize(n+1,n+1);
     A &lt;&lt; G;
-    real coef = rn*rn*lambda;
+    real rn2 = rn*rn;
+    real coef = rn2*lambda;
     for (int i=0;i&lt;=n;i++)
         A(i,i) += coef;
     Mat r_row = r.toMat(1,n+1);
@@ -220,6 +226,8 @@
         PLWARNING(&quot;NatGradEstimator: lapackSolveLinearSystem returned %d\n:&quot;,status);
     // solution is in r
     transposeProduct(v, Xt, r);
+    if (renormalize)
+        v*=(1 - pow(gamma,real(t+1)))/(1 - gamma);
 
     // recompute the eigen-decomposition
     if (i+1==cov_minibatch_size)
@@ -229,7 +237,21 @@
         
         // convert eigenvectors Vt of G into eigenvectors U of C
         product(Ut,Vkt,Xt);
-
+        Ut *= 1.0/rn;
+        if (verbosity&gt;0) // verifier Ut U = D/
+        {
+            static Mat Dmat;
+            D *= 1.0/rn2;
+            cout &lt;&lt; &quot;eigenvalues = &quot; &lt;&lt; D &lt;&lt; endl;
+            if (verbosity&gt;2)
+            {
+                Dmat.resize(n_eigen,n_eigen);
+                productTranspose(Dmat,Ut,Ut);
+                for (int j=0;j&lt;n_eigen;j++) 
+                    Dmat(j,j)-=D[j];
+                cout &lt;&lt; &quot;norm(U' U - D)/(n_eigen*n_eigen) = &quot; &lt;&lt; sumsquare(Dmat.toVec())/n_eigen &lt;&lt; endl;
+            }
+        }
         // prepare for next minibatch
         Xt.resize(n_eigen,n_dim);
         Xt &lt;&lt; Ut;

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h	2007-03-17 17:25:59 UTC (rev 6748)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h	2007-03-17 23:17:27 UTC (rev 6749)
@@ -81,6 +81,8 @@
     //! verbosity level, track improvement, spectrum, etgc.
     int verbosity;
 
+    bool renormalize;
+
     //! use the formula Ginv &lt;-- (1+eps) Ginv - eps Ginv g g' Ginv
     //! to estimate the inverse of the covariance matrix
     bool amari_version;

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-17 17:25:59 UTC (rev 6748)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-17 23:17:27 UTC (rev 6749)
@@ -213,7 +213,9 @@
     }
     example_weights.resize(minibatch_size);
     TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
-    train_costs.resize(minibatch_size,train_cost_names.length() );
+    train_costs.resize(minibatch_size,train_cost_names.length()-1 );
+
+    Profiler::activate();
 }
 
 // ### Nothing to add here, simply calls build_
@@ -291,10 +293,16 @@
     train_stats-&gt;forget();
 
     PP&lt;ProgressBar&gt; pb;
+
+    Profiler::reset(&quot;training&quot;);
+    Profiler::start(&quot;training&quot;);
     if( report_progress &amp;&amp; stage &lt; nstages )
         pb = new ProgressBar( &quot;Training &quot;+classname(),
                               nstages - stage );
 
+    Vec costs_plus_time(train_costs.width()+1);
+    costs_plus_time[train_costs.width()] = MISSING_VALUE;
+    Vec costs = costs_plus_time.subVec(0,train_costs.width());
     int nsamples = train_set-&gt;length();
 
     for( ; stage&lt;nstages; stage++)
@@ -308,12 +316,21 @@
         {
             onlineStep(stage, targets, train_costs, example_weights );
             for (int i=0;i&lt;minibatch_size;i++)
-                train_stats-&gt;update( train_costs(b) );
+            {
+                costs &lt;&lt; train_costs(b);
+                train_stats-&gt;update( costs_plus_time );
+                
+            }
         }
         if( pb )
             pb-&gt;update( stage + 1 );
     }
-
+    Profiler::end(&quot;training&quot;);
+    Profiler::report(cout);
+    const Profiler::Stats&amp; stats = Profiler::getStats(&quot;training&quot;);
+    costs.fill(MISSING_VALUE);
+    costs_plus_time[train_costs.width()] = stats.user_duration+stats.system_duration;
+    train_stats-&gt;update( costs_plus_time );
     train_stats-&gt;finalize(); // finalize statistics for this epoch
 }
 
@@ -337,6 +354,7 @@
         {
             productScaleAcc(layer_params_gradient[i-1],neuron_gradients_per_layer[i],true,
                             neuron_extended_outputs_per_layer[i-1],false,1,0);
+            layer_params_gradient[i-1] *= 1.0/minibatch_size; // use the MEAN gradient
         }
         else if (params_natgrad_template)
         {
@@ -345,7 +363,7 @@
         } else // just regular stochastic gradient
             // compute gradient on weights and update them
             productScaleAcc(layer_params[i-1],neuron_gradients_per_layer[i],true,
-                            neuron_extended_outputs_per_layer[i-1],false,-lrate,1);
+                            neuron_extended_outputs_per_layer[i-1],false,-lrate/minibatch_size,1); // mean gradient
     }
     if (full_natgrad) 
     {
@@ -459,7 +477,9 @@
 
 TVec&lt;string&gt; NatGradNNet::getTrainCostNames() const
 {
-    return getTestCostNames();
+    TVec&lt;string&gt; costs = getTestCostNames();
+    costs.append(&quot;CPU&quot;);
+    return costs;
 }
 
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-17 17:25:59 UTC (rev 6748)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-17 23:17:27 UTC (rev 6749)
@@ -42,6 +42,7 @@
 
 #include &lt;plearn_learners/generic/PLearner.h&gt;
 #include &lt;plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h&gt;
+#include &lt;plearn/sys/Profiler.h&gt;
 
 namespace PLearn {
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000197.html">[Plearn-commits] r6748 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
	<LI>Next message: <A HREF="000199.html">[Plearn-commits] r6750 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#198">[ date ]</a>
              <a href="thread.html#198">[ thread ]</a>
              <a href="subject.html#198">[ subject ]</a>
              <a href="author.html#198">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
