<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6732 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-March/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6732%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703142122.l2ELMomW029902%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000180.html">
   <LINK REL="Next"  HREF="000182.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6732 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6732%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703142122.l2ELMomW029902%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6732 - trunk/plearn_learners/generic/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Wed Mar 14 22:22:50 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000180.html">[Plearn-commits] r6731 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="000182.html">[Plearn-commits] r6733 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#181">[ date ]</a>
              <a href="thread.html#181">[ thread ]</a>
              <a href="subject.html#181">[ subject ]</a>
              <a href="author.html#181">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-03-14 22:22:48 +0100 (Wed, 14 Mar 2007)
New Revision: 6732

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:


Added: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-03-14 18:04:50 UTC (rev 6731)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.cc	2007-03-14 21:22:48 UTC (rev 6732)
@@ -0,0 +1,371 @@
+// -*- C++ -*-
+
+// NatGradEstimator.cc
+//
+// Copyright (C) 2007 yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: yoshua Bengio
+
+/*! \file NatGradEstimator.cc */
+
+
+#include &quot;NatGradEstimator.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &lt;plearn/math/plapack.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    NatGradEstimator,
+    &quot;Convert a sequence of gradients into covariance-corrected (natural gradient) directions.\n&quot;,
+    &quot;The algorithm used for converting a sequence of n-dimensional gradients g_t\n&quot;
+    &quot;into covariance-corrected update directions v_t is the following:\n\n&quot;
+    &quot;init():\n&quot;
+    &quot;  initialize U = Id(k,n)\n&quot;
+    &quot;  initialize D = lambda Id(k,k), diag matrix stored as a vector\n&quot;
+    &quot;  initialize sigma = 0\n&quot;
+    &quot;\n&quot;
+    &quot;operator(int t, Vec g, Vec v): (reads g and writes v)\n&quot;
+    &quot;    i = t%b   /* denoting b = cov_minibatch_size */\n&quot;
+    &quot;    G_{.i} = g /* = gradient for example t = i-th column of matrix G */\n&quot;
+    &quot;    if t&lt;b \n&quot;
+    &quot;        v0_i = g / (lambda + ||g||^2)\n&quot;
+    &quot;        v = v0_i\n&quot;
+    &quot;    else /* denoting k = n_eigen */ \n&quot;
+    &quot;        v0_i = (g gamma/sigma + sum_{j=1}^k (1/D_j - gamma/sigma) U_{.j} U_{.j}' g)  /* = inv(C) g */ \n&quot;
+    &quot;        u0_i = v0_i / ( gamma + v0_i' g / (i+1))\n&quot;
+    &quot;        v = u0_i  - (1/(i+1)) sum_{j=1}^{i-1} v0_j G_{.j}' u0_i / (gamma + v0_j'G_{.j}/(i+1)) \n&quot;
+    &quot;    for j = 1 to inversion_n_iterations\n&quot;
+    &quot;       v = (1 - gamma alpha) v + alpha v0_i - (alpha/i) sum_{r=0}^i v0_r G_{.r}' v\n&quot;
+    &quot;    v *= (1 - gamma^{t/b})/(1 - gamma)\n&quot;
+    &quot;    if i+1==b  /* recompute eigen-decomposition: */\n&quot;
+    &quot;       M = [gamma D    (gamma/b)^{1/2} sqrt(D) U' G;  (gamma/b)^{1/2} G' U sqrt(D)    G'G/b] /* = Gram matrix */\n&quot;
+    &quot;       (V,E) = leading_eigendecomposition(M,k)\n&quot;
+    &quot;       U = ([U sqrt(D)   G] V E^{-1/2} /* = k-principal e-vec of C */\n&quot;
+    &quot;       D = E /* = k principal e-val of C */\n&quot;
+    &quot;       sigma = {(k+1)th e-value of M}/gamma \n&quot;
+    &quot;               /* = heuristic value for lower e-values of C */\n&quot;
+    &quot;\n\n&quot;
+    &quot;This is derived from the following considerations:\n&quot;
+    &quot;  - let the covariance estimator at the beginning of minibatch t/b be C. We have its\n&quot;
+    &quot;    eigen-decomposition in principal e-vectors U, principal e-values D, and lower e-values=sigma.\n&quot;
+    &quot;  - at the end of the minibatch it is B + GG'/b\n&quot;
+    &quot;    where B is C with the upper eigenvalues reduced by a factor gamma.\n&quot;
+    &quot;  - this introduces a scaling factor (1-gamma)/(1-gamma^{t/b}) which is scaled out of\n&quot;
+    &quot;    the v's on last line of above pseudo-code\n&quot;
+    &quot;  - to obtain the eigen-decomposition efficiently, we rewrite B* + GG' in Gram matrix form\n&quot;
+    &quot;    where B* ignores the lower eigenvalues of B, i.e. B* = gamma U D U'. Hence\n&quot;
+    &quot;    B* + GG' = [sqrt(gamma) U sqrt(D)    G]' [sqrt(gamma) U sqrt(D)   G],\n&quot;
+    &quot;    but this matrix has the same eigenvalues as M = [sqrt(gamma) U sqrt(D)    G] [sqrt(gamma) U sqrt(D)    G]'\n&quot;
+    &quot;    and the eigenvectors of B*  + GG' can be recovered from above formula.\n&quot;
+    &quot;  - To regularize B* + GG', we threshold the lower eigenvalues and set them to the (k+1)-th eigenvalue.\n&quot;
+    &quot;  - on the i-th gradient g_i of the minibatch we would like to solve\n&quot;
+    &quot;           (B + (1/i)sum_{k=1}^i g_k g_k') v_i = g_i\n&quot;
+    &quot;  - we do this iteratively using as initial estimator of v_i: v_i^0 = inv(F) g_i\n&quot;
+    &quot;    where F is C with the lower eigenvalues boosted by a factor 1/gamma, and \n&quot;
+    &quot;    each iteration has the form:\n&quot;
+    &quot;            v_i &lt;-- v_i + alpha inv(F) (g_i - (B + (1/i)sum_{k=1}^i g_k g_k') v_i)\n&quot;
+    &quot;    which can be simplified into\n&quot;
+    &quot;            v_i &lt;-- (1 - alpha gamma) v_i + alpha v_i^0 - alpha/i sum_{k=1}^i v_k^0 g_k' v_i \n&quot;
+    );
+
+NatGradEstimator::NatGradEstimator()
+    /* ### Initialize all fields to their default value */
+    : cov_minibatch_size(10),
+      lambda(1),
+      n_eigen(10),
+      alpha(0.1),
+      gamma(0.9),
+      inversion_n_iterations(5),
+      n_dim(-1),
+      use_double_init(true),
+      verbosity(0),
+      sigma(0),
+      previous_t(-1)
+{
+    build();
+}
+
+// ### Nothing to add here, simply calls build_
+void NatGradEstimator::build()
+{
+    inherited::build();
+    build_();
+}
+
+void NatGradEstimator::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    deepCopyField(Ut, copies);
+    deepCopyField(E, copies);
+    deepCopyField(D, copies);
+    deepCopyField(Gt, copies);
+    deepCopyField(initial_v, copies);
+    deepCopyField(tmp_v, copies);
+    deepCopyField(M, copies);
+    deepCopyField(M11, copies);
+    deepCopyField(M12, copies);
+    deepCopyField(M21, copies);
+    deepCopyField(M22, copies);
+    deepCopyField(Vt, copies);
+    deepCopyField(Vkt, copies);
+    deepCopyField(Vbt, copies);
+    deepCopyField(newUt, copies);
+    deepCopyField(vg, copies);
+}
+
+void NatGradEstimator::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    declareOption(ol, &quot;cov_minibatch_size&quot;, &amp;NatGradEstimator::cov_minibatch_size,
+                  OptionBase::buildoption,
+                  &quot;Covariance estimator minibatch size, i.e. number of calls\n&quot;
+                  &quot;to operator() before re-estimating the principal\n&quot;
+                  &quot;eigenvectors/values. Note that each such re-computation will\n&quot;
+                  &quot;cost O(n_eigen * n)&quot;);
+    declareOption(ol, &quot;lambda&quot;, &amp;NatGradEstimator::lambda,
+                  OptionBase::buildoption,
+                  &quot;Initial variance. The first covariance is assumed to be\n&quot;
+                  &quot;lambda times the identity. Default = 1.\n&quot;);
+    declareOption(ol, &quot;n_eigen&quot;, &amp;NatGradEstimator::n_eigen,
+                  OptionBase::buildoption,
+                  &quot;Number of principal eigenvectors of the covariance matrix\n&quot;
+                  &quot;that are kept in its approximation.\n&quot;);
+    declareOption(ol, &quot;alpha&quot;, &amp;NatGradEstimator::alpha,
+                  OptionBase::buildoption,
+                  &quot;Learning rate of the inversion iterations.\n&quot;);
+    declareOption(ol, &quot;inversion_n_iterations&quot;, &amp;NatGradEstimator::inversion_n_iterations,
+                  OptionBase::buildoption,
+                  &quot;Number of iterations of numerical approximation algorithm for\n&quot;
+                  &quot;solving the system inverse(cov) v = g\n&quot;);
+    declareOption(ol, &quot;use_double_init&quot;, &amp;NatGradEstimator::use_double_init,
+                  OptionBase::buildoption,
+                  &quot;wether to use the u0 and its correction for initialization the inversion iteration\n&quot;);
+    declareOption(ol, &quot;verbosity&quot;, &amp;NatGradEstimator::verbosity,
+                  OptionBase::buildoption,
+                  &quot;Verbosity level\n&quot;);
+
+    declareOption(ol, &quot;n_dim&quot;, &amp;NatGradEstimator::n_dim,
+                  OptionBase::learntoption,
+                  &quot;Number of dimensions of the gradient vectors\n&quot;);
+    declareOption(ol, &quot;Ut&quot;, &amp;NatGradEstimator::Ut,
+                  OptionBase::learntoption,
+                  &quot;Estimated principal eigenvectors of the gradients covariance matrix\n&quot;
+                  &quot;(stored in the rows of Ut)\n&quot;);
+    declareOption(ol, &quot;E&quot;, &amp;NatGradEstimator::E,
+                  OptionBase::learntoption,
+                  &quot;Estimated principal eigenvalues of the gradients covariance matrix\n&quot;);
+    declareOption(ol, &quot;sigma&quot;, &amp;NatGradEstimator::sigma,
+                  OptionBase::learntoption,
+                  &quot;Estimated value for the minor eigenvalues of the gradients covariance matrix\n&quot;);
+    declareOption(ol, &quot;gamma&quot;, &amp;NatGradEstimator::gamma,
+                  OptionBase::learntoption,
+                  &quot;Forgetting factor in moving average estimator of covariance. 0&lt;gamma&lt;1.\n&quot;);
+    declareOption(ol, &quot;Gt&quot;, &amp;NatGradEstimator::Gt,
+                  OptionBase::learntoption,
+                  &quot;Collected gradients during a minibatch\n&quot;);
+    declareOption(ol, &quot;previous_t&quot;, &amp;NatGradEstimator::previous_t,
+                  OptionBase::learntoption,
+                  &quot;Value of t at previous call of operator()\n&quot;);
+    declareOption(ol, &quot;initial_v&quot;, &amp;NatGradEstimator::initial_v,
+                  OptionBase::learntoption,
+                  &quot;Initial v for the g's of the current minibatch\n&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void NatGradEstimator::build_()
+{
+    init();
+}
+
+void NatGradEstimator::init()
+{
+    if (n_dim&gt;=0)
+    {
+        PLASSERT_MSG(n_dim&gt;0, &quot;NatGradEstimator::init(), n_dim should be &gt; 0&quot;);
+        PLASSERT_MSG(gamma&lt;1 &amp;&amp; gamma&gt;0, &quot;NatGradEstimator::init(), gamma should be &lt; 1 and &gt;0&quot;);
+        Ut.resize(n_eigen,n_dim);
+        Vt.resize(n_eigen+1,n_eigen+cov_minibatch_size);
+        Vkt = Vt.subMat(0,n_eigen,0,n_eigen);
+        Vbt = Vt.subMat(0,n_eigen,n_eigen,cov_minibatch_size);
+        E.resize(n_eigen+1);
+        D = E.subVec(0,n_eigen);
+        M.resize(n_eigen + cov_minibatch_size, n_eigen + cov_minibatch_size);
+        M11=M.subMat(0,n_eigen,0,n_eigen);
+        M12=M.subMat(0,n_eigen,n_eigen,cov_minibatch_size);
+        M21=M.subMat(n_eigen,cov_minibatch_size,0,n_eigen);
+        M22=M.subMat(n_eigen,cov_minibatch_size,n_eigen,cov_minibatch_size);
+        Gt.resize(cov_minibatch_size, n_dim);
+        initial_v.resize(cov_minibatch_size, n_dim);
+        tmp_v.resize(n_dim);
+        newUt.resize(n_eigen,n_dim);
+        vg.resize(cov_minibatch_size);
+    }
+}
+
+void NatGradEstimator::operator()(int t, const Vec&amp; g, Vec v)
+{
+    if (t!=0)
+        PLASSERT_MSG(t==previous_t+1, &quot;NatGradEstimator() should be called sequentially!&quot;);
+    if  (n_dim&lt;0) 
+    {
+        PLASSERT_MSG(t==0, &quot;The first call to NatGradEstimator() should be with t=0\n&quot;);
+        n_dim = g.length();
+        v.resize(n_dim);
+        init();
+    }
+    int i = t % cov_minibatch_size;
+    Vec v0 = initial_v(i);
+    Gt(i) &lt;&lt; g;
+
+    // initialize v0
+    v0 &lt;&lt; g;
+    if (t&lt;cov_minibatch_size)
+    {
+        v0 *= 1.0/(lambda + pownorm(g));
+        v &lt;&lt; v0;
+    }
+    else
+    {
+        real oos = gamma/sigma;
+        real ooip1 = 1.0/(i+1.0);
+        v0 *= oos;
+        // v0 = g*gamma/sigma + sum_j (1/D_j - gamma/sigma Uj Uj' g
+        for (int j=0;j&lt;n_eigen;j++)
+        {
+            Vec Uj = Ut(j);
+            multiplyAcc(v0, Uj, (1/D[j] - oos) * dot(Uj,g));
+        }
+        if (use_double_init)
+        {
+            vg[i] = dot(v0,g);
+            multiply(v0,1.0/(gamma + vg[i]*ooip1),tmp_v); // tmp_v == u0_i here
+            v &lt;&lt; tmp_v;
+            for (int j=0;j&lt;i;j++)
+                multiplyAcc(v, initial_v(j), -ooip1*dot(Gt(j),tmp_v)/(gamma + vg[j]*ooip1));
+        }
+        else
+            v &lt;&lt; v0;
+    }
+
+    // iterate on v to solve linear system
+    for (int j=0;j&lt;inversion_n_iterations;j++)
+    {
+        multiply(v, (1 - gamma*alpha),tmp_v);
+        multiplyAcc(tmp_v, v0, alpha);
+        for (int r=0;r&lt;=i;r++)
+            multiplyAcc(tmp_v, initial_v(r), -alpha/(i+1)*dot(Gt(r),v));
+        v &lt;&lt; tmp_v;
+        // verify that we get an improvement
+        if (verbosity&gt;0)
+        {
+            // compute (B + (1/i)sum_{k=1}^i g_k g_k') v_i            
+            //        =(U (gamma D -sigma I) U' + sigma I + (1/i)sum_{k=1}^i g_k g_k') v_i            
+            multiply(v,sigma,tmp_v);
+            for (int j=0;j&lt;n_eigen;j++)
+            {
+                Vec Uj = Ut(j);
+                multiplyAcc(tmp_v,Uj,(gamma*D[j]-sigma)*dot(Uj,v));
+            }
+            for (int j=0;j&lt;=i;j++)
+            {
+                Vec Gj = Gt(j);
+                multiplyAcc(tmp_v,Gj,dot(Gj,v)/(i+1));
+            }
+            // result is in tmp_v. Compare with g_i
+            real gnorm = dot(g,g);
+            real enorm = dot(tmp_v,tmp_v);
+            real angle = acos(dot(tmp_v,g)/sqrt(gnorm*enorm))*360/(2*3.14159);
+            real err = L2distance(g,tmp_v);
+            cout &lt;&lt; &quot;linear system distance=&quot; &lt;&lt; err &lt;&lt; &quot;, angle=&quot;&lt;&lt;angle&lt;&lt;&quot;, norm ratio=&quot;&lt;&lt;enorm/gnorm&lt;&lt;endl;
+        }
+    }
+    
+    // normalize back v, to take into account scaling up of C due to gamma iteration
+    v *= (1 - pow(gamma,real(t/cov_minibatch_size)))/(1 - gamma);
+    // recompute the eigen-decomposition
+    if (i+1==cov_minibatch_size)
+    {
+        // build Gram matrix M, by blocks [M11 M12; M21 M22]
+        M11.clear();
+        for (int j=0;j&lt;n_eigen;j++)
+            M11(j,j) = gamma*D[j];
+        productTranspose(M12,Ut,Gt);
+        real gob=gamma/cov_minibatch_size;
+        for (int j=0;j&lt;n_eigen;j++)
+            M12(j) *= sqrt(D[j]*gob);
+        transpose(M12,M21);
+        productTranspose(M22,Gt,Gt);
+        M22 *= 1.0/cov_minibatch_size;
+
+        // get eigen-decomposition, with one more eigen-x than necessary to set sigma
+        eigenVecOfSymmMat(M,n_eigen+1,E,Vt);
+        
+        // convert eigenvectors Vt of M into eigenvectors U of C
+        product(newUt,Vbt,Gt);
+        Vec sqrtD = tmp_v.subVec(0,n_eigen);
+        compute_sqrt(D,sqrtD);
+        diagonalizedFactorsProduct(newUt,Vkt,sqrtD,Ut,true);
+        Ut &lt;&lt; newUt;
+    }
+    
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h	2007-03-14 18:04:50 UTC (rev 6731)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h	2007-03-14 21:22:48 UTC (rev 6732)
@@ -0,0 +1,205 @@
+// -*- C++ -*-
+
+// NatGradEstimator.h
+//
+// Copyright (C) 2007 yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: yoshua Bengio
+
+/*! \file NatGradEstimator.h */
+
+
+#ifndef NatGradEstimator_INC
+#define NatGradEstimator_INC
+
+#include &lt;plearn/base/Object.h&gt;
+#include &lt;plearn/math/TMat_impl.h&gt;
+
+namespace PLearn {
+
+/**
+ * Class used for converting a sequence of n-dimensional gradients g_t
+ * into covariance-corrected update directions v_t, approximating
+ *     v_t = inv(C_t) g_t,
+ * with C_t = gamma C_{t-1} + g_t g_t'.
+ * 
+ * There is a main method, the operator(), which takes a g_t and fills v_t.
+ * The process can be initialized by init(). 
+ */
+class NatGradEstimator : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+
+    //! mini-batch size for covariance eigen-decomposition
+    int cov_minibatch_size;
+
+    //! regularization coefficient of covariance matrix (initial values on diagonal)
+    real lambda;
+
+    //! number of eigenvectors-eigenvalues that is preserved of the covariance matrix
+    int n_eigen;
+
+    //! learning rate of the inversion iterations
+    real alpha;
+
+    //! forgetting factor in moving average estimator of covariance
+    real gamma;
+
+    //! number of iterations for approaching the solution of inv(C) v_t = g_t
+    int inversion_n_iterations;
+
+    //! number of input dimensions (size of g_t or v_t)
+    int n_dim;
+
+    //! wether to use the u0 and its correction for initialization the inversion iteration
+    bool use_double_init;
+
+    //! verbosity level, track improvement, spectrum, etgc.
+    int verbosity;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    NatGradEstimator();
+
+    // Your other public member functions go here
+
+    //! initialize the object to start collecting covariance statistics from fresh
+    void init();
+
+    //! main method of this class: reads from the gradient &quot;g&quot; field
+    //! and writes into the &quot;v&quot; field an estimator of inv(cov) g.
+    //! The argument is an index over examples, which is used to
+    //! know when cycling through a minibatch. The statistics on
+    //! the covariance are updated.
+    void operator()(int t, const Vec&amp; g, Vec v);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(NatGradEstimator);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+    //! k principal eigenvectors of the estimated covariance matrix
+    Mat Ut; // dimension = n_eigen x n_dim
+
+    //! k principal eigenvalues of the estimated covariance matrix
+    Vec D; // subvector of E
+
+    //! eigenvalues of the Gram matrix
+    Vec E;
+
+    //! eigenvalue attributed to the non-principal eigenvectors
+    real sigma;
+
+    //! gradient vectors collected during the minibatch, in each row of Gt
+    Mat Gt; // dimension = cov_minibatch_size x n_dim
+
+    //! previous value of t
+    int previous_t;
+
+    //! initial v's for the gradients in this minibatch
+    Mat initial_v;
+    //! vg[i] = initial_v[i] . Gt[i]
+    Vec vg;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+    //! temporary buffer
+    Vec tmp_v;
+    //! Gram matrix, of dimension (k + minibatch_size, k + minibatch_size)
+    //! and its sub-matrices
+    Mat M, M11, M12, M21, M22;
+    //! k+1 eigenvectors of the Gram matrix (in the rows)
+    Mat Vt;
+    Mat Vkt; //! sub-matrix of Vt with first n_eigen elements of each eigen-vector
+    Mat Vbt; //! sub-matrix of Vt with last cov_minibatch_size elements of each eigen-vector
+    //! temp for new value of Ut
+    Mat newUt;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NatGradEstimator);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-14 18:04:50 UTC (rev 6731)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-14 21:22:48 UTC (rev 6732)
@@ -0,0 +1,310 @@
+// -*- C++ -*-
+
+// NatGradNNet.cc
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file NatGradNNet.cc */
+
+
+#include &quot;NatGradNNet.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    NatGradNNet,
+    &quot;Multi-layer neural network trained with an efficient Natural Gradient optimization&quot;,
+    &quot;A separate covariance matrix is estimated for the gradients associated with the\n&quot;
+    &quot;the input weights of each neuron, and a covariance matrix between the gradients\n&quot;
+    &quot;on the neurons is also computed. These are combined to obtained an adjusted gradient\n&quot;
+    &quot;on all the parameters. The class NatGradEstimator embodies the adjustment algorithm.\n&quot;
+    &quot;Users may specify different options for the estimator that is used for correcting\n&quot;
+    &quot;the neurons gradients and for the estimator that is used for correcting the\n&quot;
+    &quot;parameters gradients (separately for each neuron).\n&quot;
+    );
+
+NatGradNNet::NatGradNNet()
+    : noutputs(-1),
+      init_lrate(0.01),
+      lrate_decay(0),
+      output_type(&quot;NLL&quot;),
+      n_layers(-1)
+{
+    random_gen = new PRandom();
+}
+
+void NatGradNNet::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;noutputs&quot;, &amp;NatGradNNet::noutputs,
+                  OptionBase::buildoption,
+                  &quot;Number of outputs of the neural network, which can be derived from  output_type and targetsize_\n&quot;);
+
+    declareOption(ol, &quot;n_layers&quot;, &amp;NatGradNNet::n_layers,
+                  OptionBase::learntoption,
+                  &quot;Number of layers of weights (ie. 2 for a neural net with one hidden layer).\n&quot;
+                  &quot;Needs not be specified explicitly (derived from hidden_layer_sizes).\n&quot;);
+
+    declareOption(ol, &quot;hidden_layer_sizes&quot;, &amp;NatGradNNet::hidden_layer_sizes,
+                  OptionBase::buildoption,
+                  &quot;Defines the architecture of the multi-layer neural network by\n&quot;
+                  &quot;specifying the number of hidden units in each hidden layer.\n&quot;);
+
+    declareOption(ol, &quot;layer_sizes&quot;, &amp;NatGradNNet::layer_sizes,
+                  OptionBase::learntoption,
+                  &quot;Derived from hidden_layer_sizes, inputsize_ and noutputs\n&quot;);
+
+    declareOption(ol, &quot;layer_params&quot;, &amp;NatGradNNet::layer_params,
+                  OptionBase::learntoption,
+                  &quot;Parameters for each layer, organized as follows: layer_params[i] \n&quot;
+                  &quot;is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)\n&quot;
+                  &quot;containing the neuron biases in its first column.\n&quot;);
+
+    declareOption(ol, &quot;init_lrate&quot;, &amp;NatGradNNet::init_lrate,
+                  OptionBase::buildoption,
+                  &quot;Initial learning rate\n&quot;);
+
+    declareOption(ol, &quot;lrate_decay&quot;, &amp;NatGradNNet::lrate_decay,
+                  OptionBase::buildoption,
+                  &quot;Learning rate decay factor\n&quot;);
+
+    declareOption(ol, &quot;neurons_natgrad&quot;, &amp;NatGradNNet::neurons_natgrad,
+                  OptionBase::buildoption,
+                  &quot;Optional NatGradEstimator for the neurons gradient.\n&quot;
+                  &quot;If not provided, then the natural gradient correction\n&quot;
+                  &quot;on the neurons gradient is not performed.\n&quot;);
+
+    declareOption(ol, &quot;params_natgrad_template&quot;, 
+                  &amp;NatGradNNet::params_natgrad_template,
+                  OptionBase::buildoption,
+                  &quot;Optional template NatGradEstimator object for the gradient of the parameters inside each neuron\n&quot;
+                  &quot;It is replicated in the params_natgrad vector, for each neuron\n&quot;
+                  &quot;If not provided, then the neuron-specific natural gradient estimator is not used.\n&quot;);
+
+    declareOption(ol, &quot;params_natgrad_per_neuron&quot;, 
+                  &amp;NatGradNNet::params_natgrad_per_neuron,
+                  OptionBase::learntoption,
+                  &quot;Vector of NatGradEstimator objects for the gradient of the parameters inside each neuron\n&quot;
+                  &quot;They are copies of the params_natgrad_template provided by the user\n&quot;);
+
+    declareOption(ol, &quot;full_natgrad&quot;, &amp;NatGradNNet::full_natgrad,
+                  OptionBase::buildoption,
+                  &quot;NatGradEstimator for all the parameter gradients simultaneously.\n&quot;
+                  &quot;This should not be set if neurons_natgrad or params_natgrad_template\n&quot;
+                  &quot;is provided. If none of the NatGradEstimators is provided, then\n&quot;
+                  &quot;regular stochastic gradient is performed.\n&quot;);
+
+
+    declareOption(ol, &quot;output_type&quot;, 
+                  &amp;NatGradNNet::output_type,
+                  OptionBase::buildoption,
+                  &quot;type of output cost: 'NLL' for classification problems,\n&quot;
+                  &quot;or 'MSE' for regression.\n&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void NatGradNNet::build_()
+{
+    if (noutputs&lt;0)
+    {
+        if (output_type==&quot;MSE&quot;)
+            noutputs = targetsize_;
+        else if (output_type==&quot;NLL&quot;)
+            PLERROR(&quot;NatGradNNet: if output_type=NLL (classification), one \n&quot;
+                    &quot;should provide noutputs = number of classes, or possibly\n&quot;
+                    &quot;1 when 2 classes\n&quot;);
+        else PLERROR(&quot;NatGradNNet: output_type should be NLL or MSE\n&quot;);
+    }
+    n_layers = hidden_layer_sizes.length()+2;
+    layer_sizes.resize(n_layers);
+    layer_sizes.subVec(1,n_layers-2) &lt;&lt; hidden_layer_sizes;
+    layer_sizes[0]=inputsize_;
+    layer_sizes[n_layers-1]=noutputs;
+    layer_params.resize(n_layers-1);
+    int n_neurons=0;
+    for (int i=0;i&lt;n_layers-1;i++)
+    {
+        layer_params[i].resize(layer_sizes[i+1],layer_sizes[i]+1);
+        n_neurons+=layer_sizes[i+1];
+    }
+    if (params_natgrad_template)
+    {
+        params_natgrad_per_neuron.resize(n_neurons);
+        for (int i=0;i&lt;n_neurons;i++)
+            params_natgrad_per_neuron[i] = PLearn::deepCopy(params_natgrad_template);
+        
+    }
+    neuron_gradients.resize(n_neurons);
+    neuron_outputs_per_layer.resize(n_layers-1);
+    neuron_gradients_per_layer.resize(n_layers-1);
+    for (int i=0,k=0;i&lt;n_layers-1;k+=layer_sizes[i+1],i++)
+    {
+        neuron_outputs_per_layer[i].resize(layer_sizes[i+1]);
+        neuron_gradients_per_layer[i] = neuron_gradients.subVec(k,layer_sizes[i+1]);
+    }
+
+}
+
+// ### Nothing to add here, simply calls build_
+void NatGradNNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void NatGradNNet::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR(&quot;NatGradNNet::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+}
+
+
+int NatGradNNet::outputsize() const
+{
+    return noutputs;
+}
+
+void NatGradNNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+}
+
+void NatGradNNet::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    while(stage&lt;nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats-&gt;forget();
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set-&gt;getExample(input, target, weight)
+        // and train_stats-&gt;update(train_costs)
+
+        ++stage;
+        train_stats-&gt;finalize(); // finalize statistics for this epoch
+    }
+    */
+}
+
+
+void NatGradNNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    // Compute the output from the input.
+    // int nout = outputsize();
+    // output.resize(nout);
+    // ...
+}
+
+void NatGradNNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+// Compute the costs from *already* computed output.
+// ...
+}
+
+TVec&lt;string&gt; NatGradNNet::getTestCostNames() const
+{
+    TVec&lt;string&gt; costs;
+    if (output_type==&quot;NLL&quot;)
+    {
+        costs.resize(2);
+        costs[0]=&quot;NLL&quot;;
+        costs[1]=&quot;class_error&quot;;
+    }
+    else if (output_type==&quot;MSE&quot;)
+    {
+        costs.resize(1);
+        costs[0]=&quot;MSE&quot;;
+    }
+    return costs;
+}
+
+TVec&lt;string&gt; NatGradNNet::getTrainCostNames() const
+{
+    return getTestCostNames();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-14 18:04:50 UTC (rev 6731)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-14 21:22:48 UTC (rev 6732)
@@ -0,0 +1,220 @@
+// -*- C++ -*-
+
+// NatGradNNet.h
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file NatGradNNet.h */
+
+
+#ifndef NatGradNNet_INC
+#define NatGradNNet_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn_learners/generic/EXPERIMENTAL/NatGradEstimator.h&gt;
+
+namespace PLearn {
+
+/**
+ * Multi-layer neural network trained with an efficient Natural Gradient optimization.
+ */
+class NatGradNNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int noutputs;
+
+    //! sizes of hidden layers, provided by the user.
+    TVec&lt;int&gt; hidden_layer_sizes;
+
+    //! layer_params[i] is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)
+    //! containing the neuron biases in its first column.
+    TVec&lt;Mat&gt; layer_params;
+
+    //! initial learning rate
+    real init_lrate;
+
+    //! learning rate decay factor
+    real lrate_decay;
+    
+    //! natural gradient estimator for neurons
+    //! (if 0 then do not correct the gradient on neurons)
+    PP&lt;NatGradEstimator&gt; neurons_natgrad;
+
+    //! natural gradient estimator for the parameters within each neuron
+    //! (if 0 then do not correct the gradient on each neuron weight)
+    PP&lt;NatGradEstimator&gt; params_natgrad_template;
+    //! the above template is used the user to specifiy all the elements of the vector below
+    TVec&lt;PP&lt;NatGradEstimator&gt; &gt; params_natgrad_per_neuron;
+
+    //! optionally, if neurons_natgrad==0 and params_natgrad_template==0, one can
+    //! have regular stochastic gradient descent, or full-covariance natural gradient
+    //! using the natural gradient estimator below
+    PP&lt;NatGradEstimator&gt; full_natgrad;
+
+    //! type of output cost: &quot;NLL&quot; for classification problems, &quot;MSE&quot; for regression
+    string output_type;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    NatGradNNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+    //                                    Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(NatGradNNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+    //! number of layers of weights (2 for a neural net with one hidden layer)
+    int n_layers;
+
+    //! layer sizes (derived from hidden_layer_sizes, inputsize_ and outputsize_)
+    TVec&lt;int&gt; layer_sizes;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+    Vec neuron_gradients;
+    TVec&lt;Vec&gt; neuron_gradients_per_layer; // pointing into neuron_gradients
+    TVec&lt;Vec&gt; neuron_outputs_per_layer; 
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NatGradNNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000180.html">[Plearn-commits] r6731 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="000182.html">[Plearn-commits] r6733 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#181">[ date ]</a>
              <a href="thread.html#181">[ thread ]</a>
              <a href="subject.html#181">[ subject ]</a>
              <a href="author.html#181">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
