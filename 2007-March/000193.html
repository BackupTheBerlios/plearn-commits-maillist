<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6744 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-March/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6744%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703161304.l2GD4uP9001848%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000192.html">
   <LINK REL="Next"  HREF="000194.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6744 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6744%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703161304.l2GD4uP9001848%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6744 - trunk/plearn_learners/generic/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Fri Mar 16 14:04:56 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000192.html">[Plearn-commits] r6743 - trunk/plearn/math
</A></li>
        <LI>Next message: <A HREF="000194.html">[Plearn-commits] r6745 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#193">[ date ]</a>
              <a href="thread.html#193">[ thread ]</a>
              <a href="subject.html#193">[ subject ]</a>
              <a href="author.html#193">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-03-16 14:04:56 +0100 (Fri, 16 Mar 2007)
New Revision: 6744

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-16 02:20:27 UTC (rev 6743)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-16 13:04:56 UTC (rev 6744)
@@ -162,15 +162,29 @@
     layer_sizes[0]=inputsize_;
     layer_sizes[n_layers-1]=noutputs;
     layer_params.resize(n_layers-1);
+    layer_params_delta.resize(n_layers-1);
+    layer_params_gradient.resize(n_layers-1);
     biases.resize(n_layers-1);
     weights.resize(n_layers-1);
     int n_neurons=0;
+    int n_params=0;
     for (int i=0;i&lt;n_layers-1;i++)
     {
-        layer_params[i].resize(layer_sizes[i+1],layer_sizes[i]+1);
+        n_neurons+=layer_sizes[i+1];
+        n_params+=layer_sizes[i+1]*(1+layer_sizes[i]);
+    }
+    all_params.resize(n_params);
+    all_params_gradient.resize(n_params);
+    all_params_delta.resize(n_params);
+    for (int i=0,p=0;i&lt;n_layers-1;i++)
+    {
+        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         biases[i]=layer_params[i].subMatColumns(0,1);
         weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
-        n_neurons+=layer_sizes[i+1];
+        layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        layer_params_delta[i]=all_params_delta.subVec(p,np);
+        p+=np;
     }
     if (params_natgrad_template)
     {
@@ -181,11 +195,16 @@
     }
     neuron_gradients.resize(minibatch_size,n_neurons);
     neuron_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_extended_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
     neuron_gradients_per_layer.resize(n_layers); // layer 0 not used
-    neuron_outputs_per_layer[0].resize(minibatch_size,layer_sizes[0]);
+    neuron_extended_outputs_per_layer[0].resize(minibatch_size,1+layer_sizes[0]);
+    neuron_outputs_per_layer[0]=neuron_extended_outputs_per_layer[0].subMatColumns(1,layer_sizes[0]);
+    neuron_extended_outputs_per_layer[0].column(0).fill(1.0); // for biases
     for (int i=1,k=0;i&lt;n_layers;k+=layer_sizes[i],i++)
     {
-        neuron_outputs_per_layer[i].resize(minibatch_size,layer_sizes[i]);
+        neuron_extended_outputs_per_layer[i].resize(minibatch_size,1+layer_sizes[i]);
+        neuron_outputs_per_layer[i]=neuron_extended_outputs_per_layer[i].subMatColumns(1,layer_sizes[i]);
+        neuron_extended_outputs_per_layer[i].column(0).fill(1.0); // for biases
         neuron_gradients_per_layer[i] = 
             neuron_gradients.subMatColumns(k,layer_sizes[i]);
     }
@@ -216,9 +235,17 @@
     deepCopyField(targets, copies);
     deepCopyField(example_weights, copies);
     deepCopyField(train_costs, copies);
+    deepCopyField(neuron_outputs_per_layer, copies);
+    deepCopyField(neuron_extended_outputs_per_layer, copies);
+    deepCopyField(all_params, copies);
+    deepCopyField(all_params_gradient, copies);
+    deepCopyField(layer_params_gradient, copies);
+    deepCopyField(neuron_gradients, copies);
+    deepCopyField(neuron_gradients_per_layer, copies);
+    deepCopyField(all_params_delta, copies);
+    deepCopyField(layer_params_delta, copies);
 /*
     deepCopyField(, copies);
-    deepCopyField(, copies);
 */
 }
 
@@ -273,8 +300,7 @@
         train_set-&gt;getExample(sample, input, target, example_weights[b]);
         if (b+1==minibatch_size) // do also special end-case || stage+1==nstages)
         {
-            onlineStep(stage, neuron_outputs_per_layer[0], targets, 
-                       train_costs, example_weights );
+            onlineStep(stage, targets, train_costs, example_weights );
             for (int i=0;i&lt;minibatch_size;i++)
                 train_stats-&gt;update( train_costs(b) );
         }
@@ -285,11 +311,12 @@
     train_stats-&gt;finalize(); // finalize statistics for this epoch
 }
 
-void NatGradNNet::onlineStep(int t, const Mat&amp; input, const Mat&amp; targets,
+void NatGradNNet::onlineStep(int t, const Mat&amp; targets,
                              Mat&amp; train_costs, Vec example_weights)
 {
     real lrate = init_lrate/(1 + t*lrate_decay);
-    fpropNet(input);
+    PLASSERT(targets.length()==minibatch_size &amp;&amp; train_costs.length()==minibatch_size &amp;&amp; example_weights.length()==minibatch_size);
+    fpropNet(minibatch_size);
     fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);
     for (int i=n_layers-1;i&gt;0;i--)
     {
@@ -299,31 +326,46 @@
         if (i&gt;1) // compute gradient on previous layer
             productScaleAcc(neuron_gradients_per_layer[i-1],neuron_gradients_per_layer[i],false,
                             weights[i-1],false,1,0);
-        // compute gradient on weights and update them
-        productScaleAcc(weights[i-1],neuron_gradients_per_layer[i],true,
-                        neuron_outputs_per_layer[i-1],false,lrate,1);
+        // compute gradient on parameters, possibly update them
+        if (full_natgrad) 
+        {
+            productScaleAcc(layer_params_gradient[i-1],neuron_gradients_per_layer[i],true,
+                            neuron_extended_outputs_per_layer[i-1],false,1,0);
+            (*full_natgrad)(t/minibatch_size,all_params_gradient,all_params_delta); // compute update direction by natural gradient
+            multiplyAcc(all_params,all_params_delta,-lrate); // update
+        }
+        else if (params_natgrad_template)
+        {
+        } else // just regular stochastic gradient
+            // compute gradient on weights and update them
+            productScaleAcc(layer_params[i-1],neuron_gradients_per_layer[i],true,
+                            neuron_extended_outputs_per_layer[i-1],false,lrate,1);
     }
 }
 
 void NatGradNNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
 {
-    fpropNet(input.toMat(1,inputsize()));
+    neuron_outputs_per_layer[0](0) &lt;&lt; input;
+    fpropNet(1);
     output &lt;&lt; neuron_outputs_per_layer[n_layers-1](0);
 }
 
 //! compute (pre-final-non-linearity) network top-layer output given input
-void NatGradNNet::fpropNet(const Mat&amp; input) const
+void NatGradNNet::fpropNet(int n_examples) const
 {
-    int n_examples = input.length();
     PLASSERT_MSG(n_examples&lt;=minibatch_size,&quot;NatGradNNet::fpropNet: nb input vectors treated should be &lt;= minibatch_size\n&quot;);
-    Mat prev_layer = (n_examples==minibatch_size)?neuron_outputs_per_layer[0]:neuron_outputs_per_layer[0].subMatRows(0,n_examples);
     for (int i=0;i&lt;n_layers-1;i++)
     {
-        Mat next_layer = (n_examples==minibatch_size)?neuron_outputs_per_layer[i+1]:neuron_outputs_per_layer[i+1].subMatRows(0,n_examples);
-        for (int k=0;k&lt;n_examples;k++)
-            next_layer(k) &lt;&lt; biases[k];
-        // try to use BLAS
-        productScaleAcc(next_layer, prev_layer, false, weights[i], true, 1, 1);
+        Mat prev_layer = neuron_extended_outputs_per_layer[i];
+        Mat next_layer = neuron_extended_outputs_per_layer[i+1];
+        if (n_examples!=minibatch_size)
+        {
+            prev_layer = prev_layer.subMatRows(0,n_examples);
+            next_layer = next_layer.subMatRows(0,n_examples);
+        }
+        // try to use BLAS for the expensive operation
+        productScaleAcc(next_layer, prev_layer, false, layer_params[i], true, 1, 1);
+        // compute layer's output non-linearity
         if (i&lt;n_layers-1)
             for (int k=0;k&lt;n_examples;k++)
             {
@@ -336,7 +378,6 @@
                 Vec L=next_layer(k);
                 log_softmax(L,L);
             }
-        prev_layer = next_layer;
     }
 }
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-16 02:20:27 UTC (rev 6743)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-16 13:04:56 UTC (rev 6744)
@@ -188,12 +188,12 @@
     // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList&amp; ol);
 
-    //! one minibatch training step for the (input,target) pair
-    void onlineStep(int t, const Mat&amp; input, const Mat&amp; target, Mat&amp; train_costs, Vec example_weights);
+    //! one minibatch training step
+    void onlineStep(int t, const Mat&amp; targets, Mat&amp; train_costs, Vec example_weights);
 
-    //! compute network top-layer output given input
+    //! compute a minibatch of size n_examples network top-layer output given layer 0 output (= network input)
     //! (note that log-probabilities are computed for classification tasks, output_type=NLL)
-    void fpropNet(const Mat&amp; input) const;
+    void fpropNet(int n_examples) const;
 
     //! compute train costs given the network top-layer output
     //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
@@ -211,9 +211,15 @@
 
     // The rest of the private stuff goes here
 
+    Vec all_params; // all the parameters in one vector
+    Vec all_params_delta; // update direction
+    Vec all_params_gradient; // all the parameter gradients in one vector
+    TVec&lt;Mat&gt; layer_params_gradient;
+    TVec&lt;Vec&gt; layer_params_delta;
     Mat neuron_gradients; // one row per example of a minibatch, has concatenation of layer 0, layer 1, ... gradients.
     TVec&lt;Mat&gt; neuron_gradients_per_layer; // pointing into neuron_gradients (one row per example of a minibatch)
     mutable TVec&lt;Mat&gt; neuron_outputs_per_layer;  // same structure
+    mutable TVec&lt;Mat&gt; neuron_extended_outputs_per_layer;  // with 1's in the first pseudo-neuron, for doing biases
     Mat targets; // one target row per example in a minibatch
     Vec example_weights; // one element per example in a minibatch
     Mat train_costs; // one row per example in a minibatch


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000192.html">[Plearn-commits] r6743 - trunk/plearn/math
</A></li>
	<LI>Next message: <A HREF="000194.html">[Plearn-commits] r6745 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#193">[ date ]</a>
              <a href="thread.html#193">[ thread ]</a>
              <a href="subject.html#193">[ subject ]</a>
              <a href="author.html#193">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
