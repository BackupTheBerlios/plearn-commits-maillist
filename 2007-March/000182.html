<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r6733 - trunk/plearn_learners/generic/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2007-March/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6733%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703150204.l2F24Qbf030542%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000181.html">
   <LINK REL="Next"  HREF="000183.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r6733 - trunk/plearn_learners/generic/EXPERIMENTAL</H1>
    <B>yoshua at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r6733%20-%20trunk/plearn_learners/generic/EXPERIMENTAL&In-Reply-To=%3C200703150204.l2F24Qbf030542%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r6733 - trunk/plearn_learners/generic/EXPERIMENTAL">yoshua at mail.berlios.de
       </A><BR>
    <I>Thu Mar 15 03:04:26 CET 2007</I>
    <P><UL>
        <LI>Previous message: <A HREF="000181.html">[Plearn-commits] r6732 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
        <LI>Next message: <A HREF="000183.html">[Plearn-commits] r6734 - in trunk: plearn/math	plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#182">[ date ]</a>
              <a href="thread.html#182">[ thread ]</a>
              <a href="subject.html#182">[ subject ]</a>
              <a href="author.html#182">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: yoshua
Date: 2007-03-15 03:04:25 +0100 (Thu, 15 Mar 2007)
New Revision: 6733

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-14 21:22:48 UTC (rev 6732)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-03-15 02:04:25 UTC (rev 6733)
@@ -137,26 +137,34 @@
 
 void NatGradNNet::build_()
 {
-    if (noutputs&lt;0)
+    if (output_type==&quot;MSE&quot;)
     {
-        if (output_type==&quot;MSE&quot;)
-            noutputs = targetsize_;
-        else if (output_type==&quot;NLL&quot;)
+        if (noutputs&lt;0) noutputs = targetsize_;
+        else PLASSERT_MSG(noutputs==targetsize_,&quot;NatGradNNet: noutputs should be -1 or match data's targetsize&quot;);
+    }
+    else if (output_type==&quot;NLL&quot;)
+    {
+        if (noutputs&lt;0)
             PLERROR(&quot;NatGradNNet: if output_type=NLL (classification), one \n&quot;
                     &quot;should provide noutputs = number of classes, or possibly\n&quot;
                     &quot;1 when 2 classes\n&quot;);
-        else PLERROR(&quot;NatGradNNet: output_type should be NLL or MSE\n&quot;);
     }
+    else PLERROR(&quot;NatGradNNet: output_type should be NLL or MSE\n&quot;);
+
     n_layers = hidden_layer_sizes.length()+2;
     layer_sizes.resize(n_layers);
     layer_sizes.subVec(1,n_layers-2) &lt;&lt; hidden_layer_sizes;
     layer_sizes[0]=inputsize_;
     layer_sizes[n_layers-1]=noutputs;
     layer_params.resize(n_layers-1);
+    biases.resize(n_layers-1);
+    weights.resize(n_layers-1);
     int n_neurons=0;
     for (int i=0;i&lt;n_layers-1;i++)
     {
         layer_params[i].resize(layer_sizes[i+1],layer_sizes[i]+1);
+        biases[i]=layer_params[i].subMatColumns(0,1);
+        weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]);
         n_neurons+=layer_sizes[i+1];
     }
     if (params_natgrad_template)
@@ -189,14 +197,19 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR(&quot;NatGradNNet::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!&quot;);
+    deepCopyField(hidden_layer_sizes, copies);
+    deepCopyField(layer_params, copies);
+    deepCopyField(neurons_natgrad, copies);
+    deepCopyField(params_natgrad_template, copies);
+    deepCopyField(params_natgrad_per_neuron, copies);
+    deepCopyField(full_natgrad, copies);
+    deepCopyField(layer_sizes, copies);
+/*    deepCopyField(, copies);
+    deepCopyField(, copies);
+    deepCopyField(, copies);
+    deepCopyField(, copies);
+    deepCopyField(, copies);
+*/
 }
 
 
@@ -210,65 +223,118 @@
     //! (Re-)initialize the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
     inherited::forget();
+    for (int i=0;i&lt;n_layers-1;i++)
+    {
+        real delta = 1/sqrt(real(layer_sizes[i]));
+        random_gen-&gt;fill_random_uniform(weights[i],-delta,delta);
+        biases[i].clear();
+    }
+    stage = 0;
 }
 
 void NatGradNNet::train()
 {
-    // The role of the train method is to bring the learner up to
-    // stage==nstages, updating train_stats with training costs measured
-    // on-line in the process.
-
-    /* TYPICAL CODE:
-
-    static Vec input;  // static so we don't reallocate memory each time...
-    static Vec target; // (but be careful that static means shared!)
+    static Vec input;  
+    static Vec target; 
     input.resize(inputsize());    // the train_set's inputsize()
     target.resize(targetsize());  // the train_set's targetsize()
-    real weight;
+    real example_weight;
 
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
+    if(!train_set)
+        PLERROR(&quot;In NNet::train, you did not setTrainingSet&quot;);
+    
+    if(!train_stats)
+        setTrainStatsCollector(new VecStatsCollector());
 
-    while(stage&lt;nstages)
-    {
-        // clear statistics of previous epoch
-        train_stats-&gt;forget();
+    TVec&lt;string&gt; train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
 
-        //... train for 1 stage, and update train_stats,
-        // using train_set-&gt;getExample(input, target, weight)
-        // and train_stats-&gt;update(train_costs)
+    train_stats-&gt;forget();
 
-        ++stage;
-        train_stats-&gt;finalize(); // finalize statistics for this epoch
+    PP&lt;ProgressBar&gt; pb;
+    if( report_progress &amp;&amp; stage &lt; nstages )
+        pb = new ProgressBar( &quot;Training &quot;+classname(),
+                              nstages - stage );
+
+    int nsamples = train_set-&gt;length();
+
+    for( ; stage&lt;nstages; stage++)
+    {
+        int sample = stage % nsamples;
+        train_set-&gt;getExample(sample, input, target, example_weight);
+        onlineStep( input, target, train_costs, example_weight );
+        train_stats-&gt;update( train_costs );
+        if( pb )
+            pb-&gt;update( stage + 1 );
     }
-    */
+
+    train_stats-&gt;finalize(); // finalize statistics for this epoch
 }
 
+void NatGradNNet::onlineStep(const Vec&amp; input, const Vec&amp; target,
+                             Vec&amp; train_costs, real example_weight)
+{
+    fpropNet(input);
+    
+}
 
 void NatGradNNet::computeOutput(const Vec&amp; input, Vec&amp; output) const
 {
-    // Compute the output from the input.
-    // int nout = outputsize();
-    // output.resize(nout);
-    // ...
+    fpropNet(input);
+    if (output_type==&quot;NLL&quot;)
+    {
+        if (outputsize()&gt;1)
+            softmax(neuron_outputs_per_layer[n_layers-1],output);
+        // keep pre-softmax output in last layer output to allow for numerically more stable gradient computation
+        else
+            compute_sigmoid(neuron_outputs_per_layer[n_layers-1],output);
+    } // else (MSE) do nothing, linear outputs
 }
 
+void NatGradNNet::fpropNet(const Vec&amp; input) const
+{
+    for (int i=0;i&lt;n_layers-1;i++)
+    {
+        Vec&amp; layer_i= neuron_outputs_per_layer[i];
+        layer_i &lt;&lt; biases[i];
+        productAcc(layer_i,weights[i],(i==0)?input:neuron_outputs_per_layer[i-1]);
+        if (i&lt;n_layers-1)
+            compute_tanh(layer_i,layer_i);
+    }
+}
+
 void NatGradNNet::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
                                            const Vec&amp; target, Vec&amp; costs) const
 {
-// Compute the costs from *already* computed output.
-// ...
+    // Compute the costs from *already* computed output.
+    if (output_type==&quot;NLL&quot;)
+    {
+        int target_class = int(round(target[0]));
+        real p=0;
+        if (outputsize()&gt;1)
+        {
+            p=output[target_class];
+            costs[1] = (target_class == argmax(output))?0:1;
+        }
+        else 
+        {
+            p = (target_class==1)?output[0]:1-output[0];
+            costs[1] = target_class&gt;0? output[0]&lt;0.5: output[0]&gt;=0.5;
+        }
+        if (p!=0)
+            costs[0] = -pl_log(p);
+        else
+        {
+            costs[0] = 1e10;
+            PLWARNING(&quot;NatGradNNet: do something better to handle near 0 probabilities...&quot;);
+        }
+    }
+    else // if (output_type==&quot;MSE&quot;)
+    {
+        costs[0] = powdistance(output,target);
+    }
 }
 
 TVec&lt;string&gt; NatGradNNet::getTestCostNames() const

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-14 21:22:48 UTC (rev 6732)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-03-15 02:04:25 UTC (rev 6733)
@@ -174,6 +174,10 @@
     //! layer sizes (derived from hidden_layer_sizes, inputsize_ and outputsize_)
     TVec&lt;int&gt; layer_sizes;
 
+    //! pointers into the layer_params
+    TVec&lt;Mat&gt; biases;
+    TVec&lt;Mat&gt; weights;
+
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -181,6 +185,10 @@
     // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList&amp; ol);
 
+    void onlineStep( const Vec&amp; input, const Vec&amp; target, Vec&amp; train_costs, real example_weight );
+
+    void fpropNet(const Vec&amp; input) const;
+
 private:
     //#####  Private Member Functions  ########################################
 
@@ -195,7 +203,7 @@
 
     Vec neuron_gradients;
     TVec&lt;Vec&gt; neuron_gradients_per_layer; // pointing into neuron_gradients
-    TVec&lt;Vec&gt; neuron_outputs_per_layer; 
+    mutable TVec&lt;Vec&gt; neuron_outputs_per_layer; 
     
 };
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000181.html">[Plearn-commits] r6732 - trunk/plearn_learners/generic/EXPERIMENTAL
</A></li>
	<LI>Next message: <A HREF="000183.html">[Plearn-commits] r6734 - in trunk: plearn/math	plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#182">[ date ]</a>
              <a href="thread.html#182">[ thread ]</a>
              <a href="subject.html#182">[ subject ]</a>
              <a href="author.html#182">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
