<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r10170 - in trunk/plearn_learners/meta: .	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2009-May/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10170%20-%20in%20trunk/plearn_learners/meta%3A%20.%0A%09test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir%0A%09test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0&In-Reply-To=%3C200905051319.n45DJr9w016862%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="003609.html">
   <LINK REL="Next"  HREF="003611.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r10170 - in trunk/plearn_learners/meta: .	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0</H1>
    <B>nouiz at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r10170%20-%20in%20trunk/plearn_learners/meta%3A%20.%0A%09test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir%0A%09test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0&In-Reply-To=%3C200905051319.n45DJr9w016862%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r10170 - in trunk/plearn_learners/meta: .	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir	test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0">nouiz at mail.berlios.de
       </A><BR>
    <I>Tue May  5 15:19:53 CEST 2009</I>
    <P><UL>
        <LI>Previous message: <A HREF="003609.html">[Plearn-commits] r10169 - trunk/plearn/vmat
</A></li>
        <LI>Next message: <A HREF="003611.html">[Plearn-commits] r10171 - trunk/plearn_learners/hyper
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3610">[ date ]</a>
              <a href="thread.html#3610">[ thread ]</a>
              <a href="subject.html#3610">[ subject ]</a>
              <a href="author.html#3610">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: nouiz
Date: 2009-05-05 15:19:52 +0200 (Tue, 05 May 2009)
New Revision: 10170

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
speed up by using a faster VMatrix for our processing.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-05-05 13:19:52 UTC (rev 10170)
@@ -38,7 +38,7 @@
 
 
 #include &quot;MultiClassAdaBoost.h&quot;
-#include &lt;plearn/vmat/ProcessingVMatrix.h&gt;
+#include &lt;plearn/vmat/OneVsAllVMatrix.h&gt;
 #include &lt;plearn/vmat/SubVMatrix.h&gt;
 #include &lt;plearn/vmat/MemoryVMatrix.h&gt;
 #include &lt;plearn_learners/regressors/RegressionTreeRegisters.h&gt;
@@ -619,28 +619,17 @@
     bool training_set_has_changed = !train_set || !(train_set-&gt;looksTheSameAs(training_set));
 
     targetname = training_set-&gt;fieldName(training_set-&gt;inputsize());
-    input_prg  = &quot;[%0:%&quot;+tostring(training_set-&gt;inputsize()-1)+&quot;]&quot;;
-    target_prg1= &quot;@&quot;+targetname+&quot; 1 0 ifelse :&quot;+targetname;
-    target_prg2= &quot;@&quot;+targetname+&quot; 2 - 0 1 ifelse :&quot;+targetname;
 
-    if(training_set-&gt;weightsize()&gt;0){
-        int index = training_set-&gt;inputsize()+training_set-&gt;targetsize();
-        weight_prg = &quot;[%&quot;+tostring(index)+&quot;]&quot;;
-    }else
-        weight_prg = &quot;1 :weights&quot;;
-    
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
     if(training_set_has_changed || !learner1-&gt;getTrainingSet()){
-        VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
-                                           target_prg1,  weight_prg);
+        VMat vmat1 = new OneVsAllVMatrix(training_set,0,true);
         if(training_set-&gt;hasMetaDataDir())
             vmat1-&gt;setMetaDataDir(training_set-&gt;getMetaDataDir()/&quot;0vsOther&quot;);
         learner1-&gt;setTrainingSet(vmat1, call_forget);
     }
     if(training_set_has_changed || !learner2-&gt;getTrainingSet()){
-        VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
-                                           target_prg2,  weight_prg);
+        VMat vmat2 = new OneVsAllVMatrix(training_set,2);
         PP&lt;RegressionTreeRegisters&gt; t1 =
             (PP&lt;RegressionTreeRegisters&gt;)learner1-&gt;getTrainingSet();
         if(t1-&gt;classname()==&quot;RegressionTreeRegisters&quot;){
@@ -733,10 +722,9 @@
                                             learner2-&gt;nTestCosts()));
     }
     if(index&lt;0){
-        testset1 = new ProcessingVMatrix(testset, input_prg,
-                                         target_prg1,  weight_prg);
-        testset2 = new ProcessingVMatrix(testset, input_prg,
-                                         target_prg2,  weight_prg);
+        testset1 = new OneVsAllVMatrix(testset,0,true);
+        testset2 = new OneVsAllVMatrix(testset,2);
+
         saved_testset.append(testset);
         saved_testset1.append(testset1);
         saved_testset2.append(testset2);
@@ -745,8 +733,8 @@
         //the same dataset to reuse their test results
         testset1=saved_testset1[index];
         testset2=saved_testset2[index];
-        PLCHECK(((PP&lt;ProcessingVMatrix&gt;)testset1)-&gt;source==testset);
-        PLCHECK(((PP&lt;ProcessingVMatrix&gt;)testset2)-&gt;source==testset);
+        PLCHECK(((PP&lt;OneVsAllVMatrix&gt;)testset1)-&gt;source==testset);
+        PLCHECK(((PP&lt;OneVsAllVMatrix&gt;)testset2)-&gt;source==testset);
     }
 
     //Profiler::pl_profile_end(&quot;MultiClassAdaBoost::test() part1&quot;);//cheap

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-05-05 13:19:52 UTC (rev 10170)
@@ -214,10 +214,6 @@
     mutable TVec&lt;Vec&gt; sub_target_tmp;
 
     string targetname;
-    string input_prg;
-    string target_prg1;
-    string target_prg2;
-    string weight_prg;
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-05-05 13:19:52 UTC (rev 10170)
@@ -250,6 +250,7 @@
 learner1 = *6 -&gt;AdaBoost(
 weak_learners = 1 [ *7 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
 compute_train_stats = 0 ;
@@ -273,11 +274,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.378311111111112819 0 0.378311111111112819 ] ;
+leave_error = 3 [ 0.378311111111107712 0 0.378311111111107712 ] ;
 split_col = 2 ;
 split_balance = 70 ;
 split_feature_value = 0.00125079586853901747 ;
-after_split_error = 0.074181818181818418 ;
+after_split_error = 0.0741818181818175992 ;
 missing_node = *0 ;
 missing_leave = *10 -&gt;RegressionTreeLeave(
 id = 2 ;
@@ -296,11 +297,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.048000000000000001 0 0.048000000000000001 ] ;
+leave_error = 3 [ 0.04799999999999989 0 0.04799999999999989 ] ;
 split_col = 2 ;
 split_balance = 24 ;
 split_feature_value = 0.000357032461916012567 ;
-after_split_error = 0.0266666666666666684 ;
+after_split_error = 0.0266666666666666094 ;
 missing_node = *0 ;
 missing_leave = *12 -&gt;RegressionTreeLeave(
 id = 5 ;
@@ -348,11 +349,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0266666666666666684 0 0.0266666666666666684 ] ;
+leave_error = 3 [ 0.0266666666666666094 0 0.0266666666666666094 ] ;
 split_col = 2 ;
 split_balance = 2 ;
 split_feature_value = 0.000981625552665510437 ;
-after_split_error = 0.0106666666666666646 ;
+after_split_error = 0.0106666666666666438 ;
 missing_node = *0 ;
 missing_leave = *16 -&gt;RegressionTreeLeave(
 id = 14 ;
@@ -371,11 +372,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
+leave_error = 3 [ 0.0106666666666666438 0 0.0106666666666666438 ] ;
 split_col = 2 ;
 split_balance = 1 ;
 split_feature_value = 0.000528285193333644099 ;
-after_split_error = 0.00666666666666666449 ;
+after_split_error = 0.00666666666666665235 ;
 missing_node = *0 ;
 missing_leave = *18 -&gt;RegressionTreeLeave(
 id = 17 ;
@@ -433,11 +434,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.026181818181818306 0 0.026181818181818306 ] ;
+leave_error = 3 [ 0.0261818181818178619 0 0.0261818181818178619 ] ;
 split_col = 4 ;
 split_balance = 88 ;
 split_feature_value = 1.54709578481515564e-13 ;
-after_split_error = 0.0218181818181818199 ;
+after_split_error = 0.0218181818181815007 ;
 missing_node = *0 ;
 missing_leave = *22 -&gt;RegressionTreeLeave(
 id = 8 ;
@@ -482,13 +483,14 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
 ] ;
-voting_weights = 1 [ 1.94591014905531323 ] ;
-sum_voting_weights = 1.94591014905531323 ;
-initial_sum_weights = 150 ;
-example_weights = 150 [ 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.0034013605442176956!
 2 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562!
  0.00340136054421769562 0.00340136054421769562 0.0034013605442!
 1769562 
0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00!
 340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 ] ;
-learners_error = 1 [ 0.0200000000000000004 ] ;
+voting_weights = 1 [ 1.94591014905531456 ] ;
+sum_voting_weights = 1.94591014905531456 ;
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.0034013605442176869!
 5 0.00340136054421768695 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695!
  0.00340136054421768695 0.00340136054421768695 0.0034013605442!
 1768695 
0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00!
 340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 ] ;
+learners_error = 1 [ 0.0199999999999999518 ] ;
 weak_learner_template = *23 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -553,6 +555,7 @@
 learner2 = *24 -&gt;AdaBoost(
 weak_learners = 1 [ *25 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
 compute_train_stats = 0 ;
@@ -564,11 +567,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.499911111111112305 0 0.499911111111112305 ] ;
+leave_error = 3 [ 0.499911111111108142 0 0.499911111111108142 ] ;
 split_col = 2 ;
 split_balance = 24 ;
 split_feature_value = 0.991025168386145405 ;
-after_split_error = 0.173253056011676648 ;
+after_split_error = 0.173253056011676232 ;
 missing_node = *0 ;
 missing_leave = *27 -&gt;RegressionTreeLeave(
 id = 2 ;
@@ -587,11 +590,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.147432950191570877 0 0.147432950191570877 ] ;
+leave_error = 3 [ 0.147432950191570544 0 0.147432950191570544 ] ;
 split_col = 1 ;
 split_balance = 31 ;
 split_feature_value = 0.482293993618237549 ;
-after_split_error = 0.104535916061339731 ;
+after_split_error = 0.104535916061339579 ;
 missing_node = *0 ;
 missing_leave = *29 -&gt;RegressionTreeLeave(
 id = 5 ;
@@ -610,11 +613,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.013107344632768362 0 0.013107344632768362 ] ;
+leave_error = 3 [ 0.0131073446327683307 0 0.0131073446327683307 ] ;
 split_col = 3 ;
 split_balance = 53 ;
 split_feature_value = 0.924226804347039965 ;
-after_split_error = 0.00888888888888889062 ;
+after_split_error = 0.00888888888888886806 ;
 missing_node = *0 ;
 missing_leave = *31 -&gt;RegressionTreeLeave(
 id = 11 ;
@@ -639,11 +642,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0914285714285713869 0 0.0914285714285713869 ] ;
+leave_error = 3 [ 0.0914285714285712481 0 0.0914285714285712481 ] ;
 split_col = 2 ;
 split_balance = 18 ;
 split_feature_value = 0.891579732096156263 ;
-after_split_error = 0.0802318840579709924 ;
+after_split_error = 0.0802318840579708398 ;
 missing_node = *0 ;
 missing_leave = *33 -&gt;RegressionTreeLeave(
 id = 14 ;
@@ -662,11 +665,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
-leave_error = 3 [ 0.0695652173913043348 0 0.0695652173913043348 ] ;
+leave_error = 3 [ 0.069565217391304196 0 0.069565217391304196 ] ;
 split_col = 2 ;
 split_balance = 15 ;
 split_feature_value = 0.808283414109232878 ;
-after_split_error = 0.0617543859649122839 ;
+after_split_error = 0.0617543859649121521 ;
 missing_node = *0 ;
 missing_leave = *35 -&gt;RegressionTreeLeave(
 id = 17 ;
@@ -691,11 +694,11 @@
 missing_is_valid = 0 ;
 leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
-leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
+leave_error = 3 [ 0.0106666666666666438 0 0.0106666666666666438 ] ;
 split_col = 2 ;
 split_balance = 1 ;
 split_feature_value = 0.982696507149771858 ;
-after_split_error = 0.00666666666666666709 ;
+after_split_error = 0.00666666666666665061 ;
 missing_node = *0 ;
 missing_leave = *37 -&gt;RegressionTreeLeave(
 id = 20 ;
@@ -728,7 +731,7 @@
 split_col = 2 ;
 split_balance = 47 ;
 split_feature_value = 0.997650553369808346 ;
-after_split_error = 0.0200000000000000039 ;
+after_split_error = 0.0199999999999999553 ;
 missing_node = *0 ;
 missing_leave = *39 -&gt;RegressionTreeLeave(
 id = 8 ;
@@ -773,13 +776,14 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 1  )
 ] ;
-voting_weights = 1 [ 1.22117351768460214 ] ;
-sum_voting_weights = 1.22117351768460214 ;
-initial_sum_weights = 150 ;
-example_weights = 150 [ 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971!
 132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579!
 71132 0.0036231884057971132 0.0036231884057971132 0.0036231884!
 05797113
2 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579711!
 32 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 ] ;
-learners_error = 1 [ 0.0800000000000000017 ] ;
+voting_weights = 1 [ 1.22117351768460325 ] ;
+sum_voting_weights = 1.22117351768460325 ;
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.!
 00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.0!
 0362318840579710496 0.00362318840579710496 0.00362318840579710!
 496 0.00
362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.003623!
 18840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 ] ;
+learners_error = 1 [ 0.0799999999999998351 ] ;
 weak_learner_template = *40 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -855,6 +859,7 @@
 ;
 weak_learner_template = *42 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -926,6 +931,7 @@
 save_stat_collectors = 1 ;
 save_split_stats = 1 ;
 save_learners = 0 ;
+save_learners_cond = &quot;&quot; ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
 save_test_outputs = 0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-05-05 13:19:52 UTC (rev 10170)
@@ -1,4 +1,4 @@
-__REVISION__ = &quot;PL10066&quot;
+__REVISION__ = &quot;PL10143&quot;
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-05-04 21:55:59 UTC (rev 10169)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-05-05 13:19:52 UTC (rev 10170)
@@ -69,6 +69,7 @@
 ;
 weak_learner_template = *8 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -155,6 +156,7 @@
 ;
 weak_learner_template = *11 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;
@@ -230,6 +232,7 @@
 ;
 weak_learner_template = *13 -&gt;RegressionTree(
 missing_is_valid = 0 ;
+will_train_again = 1 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
 compute_train_stats = 0 ;


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="003609.html">[Plearn-commits] r10169 - trunk/plearn/vmat
</A></li>
	<LI>Next message: <A HREF="003611.html">[Plearn-commits] r10171 - trunk/plearn_learners/hyper
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#3610">[ date ]</a>
              <a href="thread.html#3610">[ thread ]</a>
              <a href="subject.html#3610">[ subject ]</a>
              <a href="author.html#3610">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
