<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9104 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9104%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806041720.m54HK9ar011529%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002551.html">
   <LINK REL="Next"  HREF="002553.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9104 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9104%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806041720.m54HK9ar011529%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9104 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Jun  4 19:20:09 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002551.html">[Plearn-commits] r9103 - in trunk: commands/EXPERIMENTAL	plearn_learners/unsupervised/EXPERIMENTAL
</A></li>
        <LI>Next message: <A HREF="002553.html">[Plearn-commits] r9105 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2552">[ date ]</a>
              <a href="thread.html#2552">[ thread ]</a>
              <a href="subject.html#2552">[ subject ]</a>
              <a href="author.html#2552">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2008-06-04 19:20:08 +0200 (Wed, 04 Jun 2008)
New Revision: 9104

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Whitespaces and indentation changes


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-03 22:35:14 UTC (rev 9103)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-04 17:20:08 UTC (rev 9104)
@@ -1277,7 +1277,7 @@
             lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
         else
             lr = grad_learning_rate;
-        
+
         layers[n_layers-1]-&gt;setLearningRate( lr );
         connections[n_layers-2]-&gt;setLearningRate( lr );
 
@@ -1362,16 +1362,16 @@
                 lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
             else
                 lr = grad_learning_rate;
-            
+
             connections[ i ]-&gt;setLearningRate( lr );
             layers[ i+1 ]-&gt;setLearningRate( lr );
-            
 
+
             layers[i+1]-&gt;bpropUpdate( layers[i+1]-&gt;activation,
                                       layers[i+1]-&gt;expectation,
                                       activation_gradients[i+1],
                                       expectation_gradients[i+1] );
-            
+
             connections[i]-&gt;bpropUpdate( layers[i]-&gt;expectation,
                                          layers[i+1]-&gt;activation,
                                          expectation_gradients[i],
@@ -1389,11 +1389,11 @@
                 lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
             else
                 lr = cd_learning_rate;
-            
+
             layers[i]-&gt;setLearningRate( lr );
             layers[i+1]-&gt;setLearningRate( lr );
             connections[i]-&gt;setLearningRate( lr );
-            
+
             if( i &gt; 0 )
             {
                 save_layer_activation.resize(layers[i]-&gt;size);

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-03 22:35:14 UTC (rev 9103)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-06-04 17:20:08 UTC (rev 9104)
@@ -78,13 +78,13 @@
 
 void StackedAutoassociatorsNet::declareOptions(OptionList&amp; ol)
 {
-    declareOption(ol, &quot;greedy_learning_rate&quot;, 
+    declareOption(ol, &quot;greedy_learning_rate&quot;,
                   &amp;StackedAutoassociatorsNet::greedy_learning_rate,
                   OptionBase::buildoption,
                   &quot;The learning rate used during the autoassociator &quot;
                   &quot;gradient descent training&quot;);
 
-    declareOption(ol, &quot;greedy_decrease_ct&quot;, 
+    declareOption(ol, &quot;greedy_decrease_ct&quot;,
                   &amp;StackedAutoassociatorsNet::greedy_decrease_ct,
                   OptionBase::buildoption,
                   &quot;The decrease constant of the learning rate used during &quot;
@@ -93,19 +93,19 @@
                   &quot;its training,\n&quot;
                   &quot;the learning rate is reset to it's initial value.\n&quot;);
 
-    declareOption(ol, &quot;fine_tuning_learning_rate&quot;, 
+    declareOption(ol, &quot;fine_tuning_learning_rate&quot;,
                   &amp;StackedAutoassociatorsNet::fine_tuning_learning_rate,
                   OptionBase::buildoption,
                   &quot;The learning rate used during the fine tuning gradient descent&quot;);
 
-    declareOption(ol, &quot;fine_tuning_decrease_ct&quot;, 
+    declareOption(ol, &quot;fine_tuning_decrease_ct&quot;,
                   &amp;StackedAutoassociatorsNet::fine_tuning_decrease_ct,
                   OptionBase::buildoption,
                   &quot;The decrease constant of the learning rate used during &quot;
                   &quot;fine tuning\n&quot;
                   &quot;gradient descent.\n&quot;);
 
-    declareOption(ol, &quot;l1_neuron_decay&quot;, 
+    declareOption(ol, &quot;l1_neuron_decay&quot;,
                   &amp;StackedAutoassociatorsNet::l1_neuron_decay,
                   OptionBase::buildoption,
                   &quot; L1 penalty weight on the hidden layers, to encourage &quot;
@@ -113,14 +113,14 @@
                   &quot;the greedy unsupervised phases.\n&quot;
                   );
 
-    declareOption(ol, &quot;l1_neuron_decay_center&quot;, 
+    declareOption(ol, &quot;l1_neuron_decay_center&quot;,
                   &amp;StackedAutoassociatorsNet::l1_neuron_decay_center,
                   OptionBase::buildoption,
                   &quot;Value around which the L1 penalty should be centered, i.e.\n&quot;
                   &quot;   L1(h) = | h - l1_neuron_decay_center |\n&quot;
                   &quot;where h are the values of the neurons.\n&quot;);
 
-    declareOption(ol, &quot;training_schedule&quot;, 
+    declareOption(ol, &quot;training_schedule&quot;,
                   &amp;StackedAutoassociatorsNet::training_schedule,
                   OptionBase::buildoption,
                   &quot;Number of examples to use during each phase of greedy pre-training.\n&quot;
@@ -138,20 +138,20 @@
                   OptionBase::buildoption,
                   &quot;The weights of the connections between the layers&quot;);
 
-    declareOption(ol, &quot;reconstruction_connections&quot;, 
+    declareOption(ol, &quot;reconstruction_connections&quot;,
                   &amp;StackedAutoassociatorsNet::reconstruction_connections,
                   OptionBase::buildoption,
                   &quot;The weights of the reconstruction connections between the &quot;
                   &quot;layers&quot;);
 
-    declareOption(ol, &quot;correlation_connections&quot;, 
+    declareOption(ol, &quot;correlation_connections&quot;,
                   &amp;StackedAutoassociatorsNet::correlation_connections,
                   OptionBase::buildoption,
                   &quot;Optional weights to capture correlation and anti-correlation\n&quot;
                   &quot;in the hidden layers. They must have the same input and\n&quot;
                   &quot;output sizes, compatible with their corresponding layers.&quot;);
 
-    declareOption(ol, &quot;direct_connections&quot;, 
+    declareOption(ol, &quot;direct_connections&quot;,
                   &amp;StackedAutoassociatorsNet::direct_connections,
                   OptionBase::buildoption,
                   &quot;Optional weights from each inputs to all other inputs'\n&quot;
@@ -188,58 +188,58 @@
                   &quot;If true then all unsupervised training stages (as well as\n&quot;
                   &quot;the fine-tuning stage) are done simultaneously.\n&quot;);
 
-    declareOption(ol, &quot;partial_costs_weights&quot;, 
+    declareOption(ol, &quot;partial_costs_weights&quot;,
                   &amp;StackedAutoassociatorsNet::partial_costs_weights,
                   OptionBase::buildoption,
                   &quot;Relative weights of the partial costs. If not defined,\n&quot;
                   &quot;weights of 1 will be assumed for all partial costs.\n&quot;
         );
 
-    declareOption(ol, &quot;compute_all_test_costs&quot;, 
+    declareOption(ol, &quot;compute_all_test_costs&quot;,
                   &amp;StackedAutoassociatorsNet::compute_all_test_costs,
                   OptionBase::buildoption,
                   &quot;Indication that, at test time, all costs for all layers \n&quot;
                   &quot;(up to the currently trained layer) should be computed.\n&quot;
         );
 
-    declareOption(ol, &quot;reconstruct_hidden&quot;, 
+    declareOption(ol, &quot;reconstruct_hidden&quot;,
                   &amp;StackedAutoassociatorsNet::reconstruct_hidden,
                   OptionBase::buildoption,
                   &quot;Indication that the autoassociators are also trained to\n&quot;
                   &quot;reconstruct their hidden layers (inspired from CD1 in an RBM).\n&quot;
         );
 
-    declareOption(ol, &quot;fraction_of_masked_inputs&quot;, 
+    declareOption(ol, &quot;fraction_of_masked_inputs&quot;,
                   &amp;StackedAutoassociatorsNet::fraction_of_masked_inputs,
                   OptionBase::buildoption,
                   &quot;Random fraction of the autoassociators' input components that\n&quot;
                   &quot;masked, i.e. unsused to reconstruct the input.\n&quot;
         );
 
-    declareOption(ol, &quot;unsupervised_nstages&quot;, 
+    declareOption(ol, &quot;unsupervised_nstages&quot;,
                   &amp;StackedAutoassociatorsNet::unsupervised_nstages,
                   OptionBase::buildoption,
                   &quot;Number of samples to use for unsupervised fine-tuning.\n&quot;);
 
-    declareOption(ol, &quot;unsupervised_fine_tuning_learning_rate&quot;, 
+    declareOption(ol, &quot;unsupervised_fine_tuning_learning_rate&quot;,
                   &amp;StackedAutoassociatorsNet::unsupervised_fine_tuning_learning_rate,
                   OptionBase::buildoption,
                   &quot;The learning rate used during the unsupervised &quot;
                   &quot;fine tuning gradient descent&quot;);
 
-    declareOption(ol, &quot;unsupervised_fine_tuning_decrease_ct&quot;, 
+    declareOption(ol, &quot;unsupervised_fine_tuning_decrease_ct&quot;,
                   &amp;StackedAutoassociatorsNet::unsupervised_fine_tuning_decrease_ct,
                   OptionBase::buildoption,
                   &quot;The decrease constant of the learning rate used during\n&quot;
                   &quot;unsupervised fine tuning gradient descent.\n&quot;);
 
-    declareOption(ol, &quot;mask_input_layer_only_in_unsupervised_fine_tuning&quot;, 
+    declareOption(ol, &quot;mask_input_layer_only_in_unsupervised_fine_tuning&quot;,
                   &amp;StackedAutoassociatorsNet::mask_input_layer_only_in_unsupervised_fine_tuning,
                   OptionBase::buildoption,
                   &quot;Indication that only the input layer should be masked\n&quot;
                   &quot;during unsupervised fine-tuning.\n&quot;);
 
-    declareOption(ol, &quot;greedy_stages&quot;, 
+    declareOption(ol, &quot;greedy_stages&quot;,
                   &amp;StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
                   &quot;Number of training samples seen in the different greedy &quot;
@@ -251,13 +251,13 @@
                   &quot;Number of layers&quot;
         );
 
-    declareOption(ol, &quot;unsupervised_stage&quot;, 
+    declareOption(ol, &quot;unsupervised_stage&quot;,
                   &amp;StackedAutoassociatorsNet::unsupervised_stage,
                   OptionBase::learntoption,
                   &quot;Number of samples visited so far during unsupervised &quot;
                   &quot;fine-tuning.\n&quot;);
 
-    declareOption(ol, &quot;correlation_layers&quot;, 
+    declareOption(ol, &quot;correlation_layers&quot;,
                   &amp;StackedAutoassociatorsNet::correlation_layers,
                   OptionBase::learntoption,
                   &quot;Hidden layers for the correlation connections&quot;
@@ -286,17 +286,17 @@
     {
         // Initialize some learnt variables
         n_layers = layers.length();
-        
+
         if( weightsize_ &gt; 0 )
             PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
                     &quot;usage of weighted samples (weight size &gt; 0) is not\n&quot;
                     &quot;implemented yet.\n&quot;);
 
-        if( !online &amp;&amp; training_schedule.length() != n_layers-1 )        
+        if( !online &amp;&amp; training_schedule.length() != n_layers-1 )
             PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
                     &quot;training_schedule should have %d elements.\n&quot;,
                     n_layers-1);
-        
+
         if( partial_costs &amp;&amp; partial_costs.length() != n_layers-1 )
             PLERROR(&quot;StackedAutoassociatorsNet::build_() - \n&quot;
                     &quot;partial_costs should have %d elements.\n&quot;,
@@ -335,11 +335,11 @@
                 greedy_stages.resize(n_layers-1);
                 greedy_stages.clear();
             }
-            
+
             if(stage &gt; 0)
                 currently_trained_layer = n_layers;
             else
-            {            
+            {
                 currently_trained_layer = n_layers-1;
                 while(currently_trained_layer&gt;1
                       &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
@@ -350,7 +350,7 @@
         {
             currently_trained_layer = n_layers;
         }
-    
+
         build_layers_and_connections();
         build_costs();
     }
@@ -369,13 +369,13 @@
         PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
                 &quot;there should be %d reconstruction connections.\n&quot;,
                 n_layers-1);
-    
+
     if( correlation_connections.length() != 0 &amp;&amp;
         correlation_connections.length() != n_layers-1 )
         PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
                 &quot;there should be either %d correlation connections or none.\n&quot;,
                 n_layers-1);
-    
+
     if( direct_connections.length() != 0 &amp;&amp;
         direct_connections.length() != n_layers-1 )
         PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
@@ -387,7 +387,7 @@
                 &quot;compute_all_test_costs option is not implemented for\n&quot;
                 &quot;reconstruct_hidden option.&quot;);
 
-    
+
     if(correlation_connections.length() != 0)
     {
         if( compute_all_test_costs )
@@ -400,7 +400,7 @@
             if( greedy_stages[i] == 0 )
             {
                 CopiesMap map;
-                correlation_layers[i] = 
+                correlation_layers[i] =
                     layers[i+1]-&gt;deepCopy(map);
             }
         }
@@ -414,7 +414,7 @@
         PLERROR(&quot;StackedAutoassociatorsNet::build_layers_and_connections() - \n&quot;
                 &quot;layers[0] should have a size of %d.\n&quot;,
                 inputsize_);
-    
+
     activations.resize( n_layers );
     expectations.resize( n_layers );
     activation_gradients.resize( n_layers );
@@ -523,7 +523,7 @@
         {
             reconstruction_connections[i]-&gt;random_gen = random_gen;
             reconstruction_connections[i]-&gt;forget();
-        }        
+        }
 
         activations[i].resize( layers[i]-&gt;size );
         expectations[i].resize( layers[i]-&gt;size );
@@ -562,15 +562,15 @@
     if( !final_module )
         PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
                 &quot;final_module should be provided.\n&quot;);
-    
+
     if( layers[n_layers-1]-&gt;size != final_module-&gt;input_size )
         PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
-                &quot;final_module should have an input_size of %d.\n&quot;, 
+                &quot;final_module should have an input_size of %d.\n&quot;,
                 layers[n_layers-1]-&gt;size);
-    
+
     if( final_module-&gt;output_size != final_cost-&gt;input_size )
         PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
-                &quot;final_module should have an output_size of %d.\n&quot;, 
+                &quot;final_module should have an output_size of %d.\n&quot;,
                 final_cost-&gt;input_size);
 
     final_module-&gt;setLearningRate( fine_tuning_learning_rate );
@@ -585,14 +585,14 @@
     if(targetsize_ != 1)
         PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
                 &quot;target size of %d is not supported.\n&quot;, targetsize_);
-    
+
     if(partial_costs)
     {
 
         if( correlation_connections.length() != 0 )
             PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
                     &quot;correlation_connections cannot be used with partial costs.&quot;);
-            
+
         partial_costs_positions.resize(partial_costs.length());
         partial_costs_positions.clear();
         for(int i=0; i&lt;partial_costs.length(); i++)
@@ -602,7 +602,7 @@
                         &quot;partial_costs[%i] should be provided.\n&quot;,i);
             if( layers[i+1]-&gt;size != partial_costs[i]-&gt;input_size )
                 PLERROR(&quot;StackedAutoassociatorsNet::build_costs() - \n&quot;
-                        &quot;partial_costs[%i] should have an input_size of %d.\n&quot;, 
+                        &quot;partial_costs[%i] should have an input_size of %d.\n&quot;,
                         i,layers[i+1]-&gt;size);
             if(i==0)
                 partial_costs_positions[i] = n_layers-1;
@@ -705,13 +705,13 @@
 
     for( int i=0 ; i&lt;n_layers ; i++ )
         layers[i]-&gt;forget();
-    
+
     for( int i=0 ; i&lt;n_layers-1 ; i++ )
     {
         connections[i]-&gt;forget();
         reconstruction_connections[i]-&gt;forget();
     }
-    
+
     final_module-&gt;forget();
     final_cost-&gt;forget();
 
@@ -720,16 +720,16 @@
             partial_costs[i]-&gt;forget();
 
     if(correlation_connections.length() != 0)
-    {        
+    {
         for( int i=0 ; i&lt;n_layers-1 ; i++)
         {
             correlation_connections[i]-&gt;forget();
             correlation_layers[i]-&gt;forget();
-        }        
+        }
     }
 
     if(direct_connections.length() != 0)
-    {        
+    {
         for( int i=0 ; i&lt;n_layers-1 ; i++)
             direct_connections[i]-&gt;forget();
     }
@@ -762,7 +762,7 @@
         train_stats = new VecStatsCollector();
         train_stats-&gt;setFieldNames(getTrainCostNames());
     }
-    
+
     // clear stats of previous epoch
     train_stats-&gt;forget();
 
@@ -838,8 +838,8 @@
             {
                 if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
                 {
-                    lr = greedy_learning_rate/(1 + greedy_decrease_ct 
-                                               * (*this_stage)); 
+                    lr = greedy_learning_rate/(1 + greedy_decrease_ct
+                                               * (*this_stage));
                     layers[i]-&gt;setLearningRate( lr );
                     connections[i]-&gt;setLearningRate( lr );
                     reconstruction_connections[i]-&gt;setLearningRate( lr );
@@ -873,13 +873,13 @@
 //                PLERROR(&quot;StackedAutoassociatorsNet::train()&quot;
 //                        &quot; - \n&quot;
 //                        &quot;cannot use unsupervised fine-tuning with correlation connections.\n&quot;);
-            
+
             MODULE_LOG &lt;&lt; &quot;Unsupervised fine-tuning all parameters, &quot;;
             MODULE_LOG &lt;&lt; &quot;by gradient descent&quot; &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  unsupervised_stage = &quot; &lt;&lt; unsupervised_stage &lt;&lt; endl;
-            MODULE_LOG &lt;&lt; &quot;  unsupervised_nstages = &quot; &lt;&lt; 
+            MODULE_LOG &lt;&lt; &quot;  unsupervised_nstages = &quot; &lt;&lt;
                 unsupervised_nstages &lt;&lt; endl;
-            MODULE_LOG &lt;&lt; &quot;  unsupervised_fine_tuning_learning_rate = &quot; &lt;&lt; 
+            MODULE_LOG &lt;&lt; &quot;  unsupervised_fine_tuning_learning_rate = &quot; &lt;&lt;
                 unsupervised_fine_tuning_learning_rate &lt;&lt; endl;
 
             init_stage = unsupervised_stage;
@@ -894,13 +894,13 @@
             fine_tuning_reconstruction_expectation_gradients.resize( n_layers );
             for( int i=0 ; i&lt;n_layers ; i++ )
             {
-                fine_tuning_reconstruction_activations[i].resize( 
+                fine_tuning_reconstruction_activations[i].resize(
                     layers[i]-&gt;size );
-                fine_tuning_reconstruction_expectations[i].resize( 
+                fine_tuning_reconstruction_expectations[i].resize(
                     layers[i]-&gt;size );
-                fine_tuning_reconstruction_activation_gradients[i].resize( 
+                fine_tuning_reconstruction_activation_gradients[i].resize(
                     layers[i]-&gt;size );
-                fine_tuning_reconstruction_expectation_gradients[i].resize( 
+                fine_tuning_reconstruction_expectation_gradients[i].resize(
                     layers[i]-&gt;size );
             }
 
@@ -908,7 +908,7 @@
             {
                 masked_autoassociator_expectations.resize( n_layers-1 );
                 autoassociator_expectation_indices.resize( n_layers-1 );
-                
+
                 for( int i=0 ; i&lt;n_layers-1 ; i++ )
                 {
                     masked_autoassociator_expectations[i].resize( layers[i]-&gt;size );
@@ -924,9 +924,9 @@
             {
                 sample = unsupervised_stage % nsamples;
                 if( !fast_exact_is_equal( unsupervised_fine_tuning_decrease_ct, 0. ) )
-                    setLearningRate( 
+                    setLearningRate(
                         unsupervised_fine_tuning_learning_rate
-                        / (1. + unsupervised_fine_tuning_decrease_ct 
+                        / (1. + unsupervised_fine_tuning_decrease_ct
                            * unsupervised_stage ) );
 
                 train_set-&gt;getExample( sample, input, target, weight );
@@ -945,7 +945,7 @@
             MODULE_LOG &lt;&lt; &quot;Fine-tuning all parameters, by gradient descent&quot; &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
-            MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; &lt;&lt; 
+            MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; &lt;&lt;
                 fine_tuning_learning_rate &lt;&lt; endl;
 
             init_stage = stage;
@@ -971,7 +971,7 @@
                     pb-&gt;update( stage - init_stage + 1 );
             }
         }
-    
+
         train_stats-&gt;finalize();
         MODULE_LOG &lt;&lt; &quot;  train costs = &quot; &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
 
@@ -979,9 +979,9 @@
         if(stage &gt; 0)
             currently_trained_layer = n_layers;
         else
-        {            
+        {
             currently_trained_layer = n_layers-1;
-            while(currently_trained_layer&gt;1 
+            while(currently_trained_layer&gt;1
                   &amp;&amp; greedy_stages[currently_trained_layer-1] &lt;= 0)
                 currently_trained_layer--;
         }
@@ -993,24 +993,24 @@
             PLERROR(&quot;StackedAutoassociatorsNet::train()&quot;
                     &quot; - \n&quot;
                     &quot;unsupervised fine-tuning with online=true is not implemented.\n&quot;);
-        
+
         // Train all layers simultaneously AND fine-tuning as well!
         if( stage &lt; nstages )
         {
 
             MODULE_LOG &lt;&lt; &quot;Training all layers greedy layer-wise AND &quot;
-                       &lt;&lt; &quot;fine-tuning all parameters, by gradient descent&quot; 
+                       &lt;&lt; &quot;fine-tuning all parameters, by gradient descent&quot;
                        &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  stage = &quot; &lt;&lt; stage &lt;&lt; endl;
             MODULE_LOG &lt;&lt; &quot;  nstages = &quot; &lt;&lt; nstages &lt;&lt; endl;
-            MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot; 
+            MODULE_LOG &lt;&lt; &quot;  fine_tuning_learning_rate = &quot;
                        &lt;&lt; fine_tuning_learning_rate &lt;&lt; endl;
-            MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot; 
+            MODULE_LOG &lt;&lt; &quot;  greedy_learning_rate = &quot;
                        &lt;&lt; greedy_learning_rate &lt;&lt; endl;
 
             init_stage = stage;
             if( report_progress &amp;&amp; stage &lt; nstages )
-                pb = new ProgressBar( 
+                pb = new ProgressBar(
                     &quot;Greedy layer-wise training AND fine-tuning parameters of &quot;
                                       + classname(),
                                       nstages - init_stage );
@@ -1052,16 +1052,16 @@
             {
                 masked_autoassociator_input &lt;&lt; expectations[i];
                 for( int j=0 ; j &lt; round(fraction_of_masked_inputs*layers[index]-&gt;size) ; j++)
-                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0;
                 connections[i]-&gt;fprop( masked_autoassociator_input, correlation_activations[i] );
             }
             else
                 connections[i]-&gt;fprop( expectations[i], correlation_activations[i] );
             layers[i+1]-&gt;fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+            correlation_connections[i]-&gt;fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]-&gt;fprop( activations[i+1], 
+            correlation_layers[i]-&gt;fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
@@ -1073,7 +1073,7 @@
             {
                 masked_autoassociator_input &lt;&lt; expectations[i];
                 for( int j=0 ; j &lt; round(fraction_of_masked_inputs*layers[index]-&gt;size) ; j++)
-                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0;
                 connections[i]-&gt;fprop( masked_autoassociator_input, activations[i+1] );
             }
             else
@@ -1123,52 +1123,52 @@
     if(direct_connections.length() != 0)
     {
         if( fraction_of_masked_inputs &gt; 0 )
-            direct_connections[ index ]-&gt;fprop( masked_autoassociator_input, 
+            direct_connections[ index ]-&gt;fprop( masked_autoassociator_input,
                                                 direct_activations );
         else
-            direct_connections[ index ]-&gt;fprop( expectations[ index ], 
-                                                direct_activations );            
+            direct_connections[ index ]-&gt;fprop( expectations[ index ],
+                                                direct_activations );
         direct_and_reconstruction_activations.clear();
         direct_and_reconstruction_activations += direct_activations;
         direct_and_reconstruction_activations += reconstruction_activations;
 
         layers[ index ]-&gt;fprop( direct_and_reconstruction_activations,
                                 layers[ index ]-&gt;expectation);
-        
+
         layers[ index ]-&gt;activation &lt;&lt; direct_and_reconstruction_activations;
         //layers[ index ]-&gt;expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
         layers[ index ]-&gt;setExpectationByRef( layers[ index ]-&gt;expectation );
         train_costs[index] = layers[ index ]-&gt;fpropNLL(expectations[index]);
-        
+
         layers[ index ]-&gt;bpropNLL(expectations[index], train_costs[index],
                                   direct_and_reconstruction_activation_gradients);
 
         layers[ index ]-&gt;update(direct_and_reconstruction_activation_gradients);
 
         if( fraction_of_masked_inputs &gt; 0 )
-            direct_connections[ index ]-&gt;bpropUpdate( 
+            direct_connections[ index ]-&gt;bpropUpdate(
                 masked_autoassociator_input,
                 direct_activations,
                 reconstruction_expectation_gradients, // Will be overwritten later
                 direct_and_reconstruction_activation_gradients);
         else
-            direct_connections[ index ]-&gt;bpropUpdate( 
+            direct_connections[ index ]-&gt;bpropUpdate(
                 expectations[ index ],
                 direct_activations,
                 reconstruction_expectation_gradients, // Will be overwritten later
                 direct_and_reconstruction_activation_gradients);
-            
-        reconstruction_connections[ index ]-&gt;bpropUpdate( 
-            expectations[ index + 1], 
-            reconstruction_activations, 
-            reconstruction_expectation_gradients, 
+
+        reconstruction_connections[ index ]-&gt;bpropUpdate(
+            expectations[ index + 1],
+            reconstruction_activations,
+            reconstruction_expectation_gradients,
             direct_and_reconstruction_activation_gradients);
     }
     else
     {
         layers[ index ]-&gt;fprop( reconstruction_activations,
                                 layers[ index ]-&gt;expectation);
-        
+
         layers[ index ]-&gt;activation &lt;&lt; reconstruction_activations;
         //layers[ index ]-&gt;expectation_is_up_to_date = true;
         layers[ index ]-&gt;setExpectationByRef( layers[ index ]-&gt;expectation );
@@ -1180,7 +1180,7 @@
 
         if(reconstruct_hidden)
         {
-            connections[ index ]-&gt;fprop( layers[ index ]-&gt;expectation, 
+            connections[ index ]-&gt;fprop( layers[ index ]-&gt;expectation,
                                          hidden_reconstruction_activations );
             layers[ index+1 ]-&gt;fprop( hidden_reconstruction_activations,
                 layers[ index+1 ]-&gt;expectation );
@@ -1193,20 +1193,20 @@
             layers[ index+1 ]-&gt;bpropNLL(expectations[index+1], hid_rec_err,
                                         hidden_reconstruction_activation_gradients);
             layers[ index+1 ]-&gt;update(hidden_reconstruction_activation_gradients);
-            
-            connections[ index ]-&gt;bpropUpdate( 
-                layers[ index ]-&gt;expectation, 
+
+            connections[ index ]-&gt;bpropUpdate(
+                layers[ index ]-&gt;expectation,
                 hidden_reconstruction_activations,
                 reconstruction_expectation_gradients_from_hid_rec,
                 hidden_reconstruction_activation_gradients);
 
-            layers[ index ]-&gt;bpropUpdate( 
+            layers[ index ]-&gt;bpropUpdate(
                 reconstruction_activations,
                 layers[ index ]-&gt;expectation,
                 reconstruction_activation_gradients_from_hid_rec,
                 reconstruction_expectation_gradients_from_hid_rec);
         }
-        
+
         layers[ index ]-&gt;update(reconstruction_activation_gradients);
 
         if(reconstruct_hidden)
@@ -1214,20 +1214,20 @@
                 reconstruction_activation_gradients_from_hid_rec;
 
         // // This is a bad update! Propagates gradient through sigmoid again!
-        // layers[ index ]-&gt;bpropUpdate( reconstruction_activations, 
+        // layers[ index ]-&gt;bpropUpdate( reconstruction_activations,
         //                                   layers[ index ]-&gt;expectation,
         //                                   reconstruction_activation_gradients,
         //                                   reconstruction_expectation_gradients);
-        
-        reconstruction_connections[ index ]-&gt;bpropUpdate( 
-            expectations[ index + 1], 
-            reconstruction_activations, 
-            reconstruction_expectation_gradients, 
+
+        reconstruction_connections[ index ]-&gt;bpropUpdate(
+            expectations[ index + 1],
+            reconstruction_activations,
+            reconstruction_expectation_gradients,
             reconstruction_activation_gradients);
 
     }
 
-    
+
     if(!fast_exact_is_equal(l1_neuron_decay,0))
     {
         // Compute L1 penalty gradient on neurons
@@ -1256,26 +1256,26 @@
             reconstruction_expectation_gradients
             );
 
-        correlation_connections[ index ]-&gt;bpropUpdate( 
+        correlation_connections[ index ]-&gt;bpropUpdate(
             correlation_expectations[ index ],
             activations[ index+1 ],
-            correlation_expectation_gradients[ index ], 
+            correlation_expectation_gradients[ index ],
             reconstruction_activation_gradients);
-        
-        layers[ index+1 ]-&gt;bpropUpdate( 
+
+        layers[ index+1 ]-&gt;bpropUpdate(
             correlation_activations[ index ],
             correlation_expectations[ index ],
             correlation_activation_gradients [ index ],
-            correlation_expectation_gradients [ index ]);    
-        
+            correlation_expectation_gradients [ index ]);
+
         if( fraction_of_masked_inputs &gt; 0 )
-            connections[ index ]-&gt;bpropUpdate( 
+            connections[ index ]-&gt;bpropUpdate(
                 masked_autoassociator_input,
                 correlation_activations[ index ],
                 reconstruction_expectation_gradients, //reused
                 correlation_activation_gradients [ index ]);
         else
-            connections[ index ]-&gt;bpropUpdate( 
+            connections[ index ]-&gt;bpropUpdate(
                 expectations[ index ],
                 correlation_activations[ index ],
                 reconstruction_expectation_gradients, //reused
@@ -1286,17 +1286,17 @@
         layers[ index+1 ]-&gt;bpropUpdate( activations[ index + 1 ],
                                         expectations[ index + 1 ],
                                         // reused
-                                        reconstruction_activation_gradients, 
-                                        reconstruction_expectation_gradients);    
-        
+                                        reconstruction_activation_gradients,
+                                        reconstruction_expectation_gradients);
+
         if( fraction_of_masked_inputs &gt; 0 )
-            connections[ index ]-&gt;bpropUpdate( 
+            connections[ index ]-&gt;bpropUpdate(
                 masked_autoassociator_input,
                 activations[ index + 1 ],
                 reconstruction_expectation_gradients, //reused
                 reconstruction_activation_gradients);
         else
-            connections[ index ]-&gt;bpropUpdate( 
+            connections[ index ]-&gt;bpropUpdate(
                 expectations[ index ],
                 activations[ index + 1 ],
                 reconstruction_expectation_gradients, //reused
@@ -1305,7 +1305,7 @@
 
 }
 
-void StackedAutoassociatorsNet::unsupervisedFineTuningStep( const Vec&amp; input, 
+void StackedAutoassociatorsNet::unsupervisedFineTuningStep( const Vec&amp; input,
                                                             const Vec&amp; target,
                                                             Vec&amp; train_costs )
 {
@@ -1318,21 +1318,21 @@
         {
             for( int i=0; i&lt;autoassociator_expectation_indices.length(); i++ )
                 random_gen-&gt;shuffleElements(autoassociator_expectation_indices[i]);
-            
+
             for( int i=0 ; i&lt;n_layers-1; i++ )
             {
                 masked_autoassociator_expectations[i] &lt;&lt; expectations[i];
                 if( !(mask_input_layer_only_in_unsupervised_fine_tuning &amp;&amp; i &gt; 0) )
                     for( int j=0 ; j &lt; round(fraction_of_masked_inputs*layers[i]-&gt;size) ; j++)
-                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
-                
-                connections[i]-&gt;fprop( masked_autoassociator_expectations[i], 
+                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0;
+
+                connections[i]-&gt;fprop( masked_autoassociator_expectations[i],
                                        correlation_activations[i] );
                 layers[i+1]-&gt;fprop( correlation_activations[i],
                                     correlation_expectations[i] );
-                correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+                correlation_connections[i]-&gt;fprop( correlation_expectations[i],
                                                    activations[i+1] );
-                correlation_layers[i]-&gt;fprop( activations[i+1], 
+                correlation_layers[i]-&gt;fprop( activations[i+1],
                                               expectations[i+1] );
             }
         }
@@ -1343,9 +1343,9 @@
                 connections[i]-&gt;fprop( expectations[i], correlation_activations[i]);
                 layers[i+1]-&gt;fprop( correlation_activations[i],
                                     correlation_expectations[i] );
-                correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+                correlation_connections[i]-&gt;fprop( correlation_expectations[i],
                                                    activations[i+1] );
-                correlation_layers[i]-&gt;fprop( activations[i+1], 
+                correlation_layers[i]-&gt;fprop( activations[i+1],
                                               expectations[i+1] );
             }
         }
@@ -1356,15 +1356,15 @@
         {
             for( int i=0; i&lt;autoassociator_expectation_indices.length(); i++ )
                 random_gen-&gt;shuffleElements(autoassociator_expectation_indices[i]);
-            
+
             for( int i=0 ; i&lt;n_layers-1; i++ )
             {
                 masked_autoassociator_expectations[i] &lt;&lt; expectations[i];
                 if( !(mask_input_layer_only_in_unsupervised_fine_tuning &amp;&amp; i &gt; 0) )
                     for( int j=0 ; j &lt; round(fraction_of_masked_inputs*layers[i]-&gt;size) ; j++)
-                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0; 
-                
-                connections[i]-&gt;fprop( masked_autoassociator_expectations[i], 
+                        masked_autoassociator_expectations[i][ autoassociator_expectation_indices[i][j] ] = 0;
+
+                connections[i]-&gt;fprop( masked_autoassociator_expectations[i],
                                        activations[i+1] );
                 layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
             }
@@ -1378,23 +1378,23 @@
             }
         }
     }
-    fine_tuning_reconstruction_expectations[ n_layers-1 ] &lt;&lt; 
+    fine_tuning_reconstruction_expectations[ n_layers-1 ] &lt;&lt;
         expectations[ n_layers-1 ];
 
     for( int i=n_layers-2 ; i&gt;=0; i-- )
     {
-        reconstruction_connections[i]-&gt;fprop( 
-            fine_tuning_reconstruction_expectations[i+1], 
+        reconstruction_connections[i]-&gt;fprop(
+            fine_tuning_reconstruction_expectations[i+1],
             fine_tuning_reconstruction_activations[i] );
         layers[i]-&gt;fprop( fine_tuning_reconstruction_activations[i],
                           fine_tuning_reconstruction_expectations[i]);
     }
-    
+
     layers[ 0 ]-&gt;setExpectation( fine_tuning_reconstruction_expectations[ 0 ] );
     layers[ 0 ]-&gt;activation &lt;&lt; fine_tuning_reconstruction_activations[0];
     real rec_err = layers[ 0 ]-&gt;fpropNLL( input );
     train_costs[n_layers-1] = rec_err;
-    
+
     layers[ 0 ]-&gt;bpropNLL( input, rec_err,
                            fine_tuning_reconstruction_activation_gradients[ 0 ] );
 
@@ -1407,16 +1407,16 @@
                                     fine_tuning_reconstruction_expectations[i],
                                     fine_tuning_reconstruction_activation_gradients[i],
                                     fine_tuning_reconstruction_expectation_gradients[i]);
-        reconstruction_connections[i]-&gt;bpropUpdate( 
-            fine_tuning_reconstruction_expectations[i+1], 
+        reconstruction_connections[i]-&gt;bpropUpdate(
+            fine_tuning_reconstruction_expectations[i+1],
             fine_tuning_reconstruction_activations[i],
-            fine_tuning_reconstruction_expectation_gradients[i+1], 
+            fine_tuning_reconstruction_expectation_gradients[i+1],
             fine_tuning_reconstruction_activation_gradients[i]);
     }
 
-    expectation_gradients[ n_layers-1 ] &lt;&lt; 
+    expectation_gradients[ n_layers-1 ] &lt;&lt;
         fine_tuning_reconstruction_expectation_gradients[ n_layers-1 ];
-    
+
     for( int i=n_layers-2 ; i&gt;=0; i-- )
     {
 
@@ -1445,27 +1445,27 @@
                 activation_gradients[ i + 1 ],
                 expectation_gradients[ i + 1 ]
                 );
-            
-            correlation_connections[ i ]-&gt;bpropUpdate( 
+
+            correlation_connections[ i ]-&gt;bpropUpdate(
                 correlation_expectations[ i ],
                 activations[ i + 1 ],
-                correlation_expectation_gradients[ i ], 
+                correlation_expectation_gradients[ i ],
                 activation_gradients[ i + 1 ] );
-            
-            layers[ i + 1 ]-&gt;bpropUpdate( 
+
+            layers[ i + 1 ]-&gt;bpropUpdate(
                 correlation_activations[ i ],
                 correlation_expectations[ i ],
                 correlation_activation_gradients [ i ],
-                correlation_expectation_gradients [ i ]);    
-            
+                correlation_expectation_gradients [ i ]);
+
             if( fraction_of_masked_inputs &gt; 0 )
-                connections[ i ]-&gt;bpropUpdate( 
+                connections[ i ]-&gt;bpropUpdate(
                     masked_autoassociator_expectations[ i ],
                     correlation_activations[ i ],
-                    expectation_gradients[i], 
+                    expectation_gradients[i],
                     correlation_activation_gradients [ i ]);
             else
-                connections[ i ]-&gt;bpropUpdate( 
+                connections[ i ]-&gt;bpropUpdate(
                     expectations[ i ],
                     correlation_activations[ i ],
                     expectation_gradients[i],
@@ -1478,11 +1478,11 @@
                 activations[i+1],expectations[i+1],
                 activation_gradients[i+1],expectation_gradients[i+1]);
             if( fraction_of_masked_inputs &gt; 0 )
-                connections[i]-&gt;bpropUpdate( 
+                connections[i]-&gt;bpropUpdate(
                     masked_autoassociator_expectations[i], activations[i+1],
                     expectation_gradients[i], activation_gradients[i+1] );
             else
-                connections[i]-&gt;bpropUpdate( 
+                connections[i]-&gt;bpropUpdate(
                     expectations[i], activations[i+1],
                     expectation_gradients[i], activation_gradients[i+1] );
         }
@@ -1502,9 +1502,9 @@
             connections[i]-&gt;fprop( expectations[i], correlation_activations[i] );
             layers[i+1]-&gt;fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+            correlation_connections[i]-&gt;fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]-&gt;fprop( activations[i+1], 
+            correlation_layers[i]-&gt;fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
@@ -1537,13 +1537,13 @@
     {
         for( int i=n_layers-1 ; i&gt;0 ; i-- )
         {
-            correlation_layers[i-1]-&gt;bpropUpdate( 
+            correlation_layers[i-1]-&gt;bpropUpdate(
                 activations[i],
                 expectations[i],
                 activation_gradients[i],
                 expectation_gradients[i] );
 
-            correlation_connections[i-1]-&gt;bpropUpdate( 
+            correlation_connections[i-1]-&gt;bpropUpdate(
                 correlation_expectations[i-1],
                 activations[i],
                 correlation_expectation_gradients[i-1],
@@ -1553,7 +1553,7 @@
                                     correlation_expectations[i-1],
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
-            
+
             connections[i-1]-&gt;bpropUpdate( expectations[i-1],
                                            correlation_activations[i-1],
                                            expectation_gradients[i-1],
@@ -1568,16 +1568,16 @@
                                     expectations[i],
                                     activation_gradients[i],
                                     expectation_gradients[i] );
-            
+
             connections[i-1]-&gt;bpropUpdate( expectations[i-1],
                                            activations[i],
                                            expectation_gradients[i-1],
                                            activation_gradients[i] );
-        }        
+        }
     }
 }
 
-void StackedAutoassociatorsNet::onlineStep( const Vec&amp; input, 
+void StackedAutoassociatorsNet::onlineStep( const Vec&amp; input,
                                             const Vec&amp; target,
                                             Vec&amp; train_costs )
 {
@@ -1592,9 +1592,9 @@
             connections[i]-&gt;fprop( expectations[i], correlation_activations[i] );
             layers[i+1]-&gt;fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+            correlation_connections[i]-&gt;fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]-&gt;fprop( activations[i+1], 
+            correlation_layers[i]-&gt;fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
@@ -1604,57 +1604,61 @@
         {
             connections[i]-&gt;fprop( expectations[i], activations[i+1] );
             layers[i+1]-&gt;fprop(activations[i+1],expectations[i+1]);
-            
+
             if( partial_costs.length() != 0 &amp;&amp; partial_costs[ i ] )
             {
                 // Set learning rates
-
-                if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
-                    lr = greedy_learning_rate / 
-                        (1 + greedy_decrease_ct * stage);
+                if( !fast_exact_is_equal(fine_tuning_decrease_ct , 0 ) )
+                    lr = fine_tuning_learning_rate /
+                        (1 + fine_tuning_decrease_ct * stage);
                 else
-                    lr = greedy_learning_rate;
+                    lr = fine_tuning_learning_rate;
 
                 partial_costs[ i ]-&gt;setLearningRate( lr );
+                /* No, learning rate should already be OK in layers and
+                 * connections
                 layers[ i+1 ]-&gt;setLearningRate( lr );
                 connections[ i ]-&gt;setLearningRate( lr );
+                */
 
                 partial_costs[ i ]-&gt;fprop( expectations[ i + 1],
                                            target, partial_cost_value );
-                
+
                 // Update partial cost (might contain some weights for example)
-                partial_costs[ i ]-&gt;bpropUpdate( 
+                partial_costs[ i ]-&gt;bpropUpdate(
                     expectations[ i + 1 ],
                     target, partial_cost_value[0],
                     expectation_gradients[ i + 1 ]
                     );
-                
+
                 train_costs.subVec(partial_costs_positions[i]+1,
-                                   partial_cost_value.length()) 
+                                   partial_cost_value.length())
                     &lt;&lt; partial_cost_value;
-                
+
                 if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
                     expectation_gradients[ i + 1 ] *= partial_costs_weights[i];
-                
+
                 // Update hidden layer bias and weights
                 layers[ i+1 ]-&gt;bpropUpdate( activations[ i + 1 ],
                                             expectations[ i + 1 ],
                                             activation_gradients[ i + 1 ],
                                             expectation_gradients[ i + 1 ] );
-                
+
                 connections[ i ]-&gt;bpropUpdate( expectations[ i ],
                                                activations[ i + 1 ],
                                                expectation_gradients[ i ],
                                                activation_gradients[ i + 1 ] );
 
+                /* no need
                 if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
-                    lr = fine_tuning_learning_rate / 
+                    lr = fine_tuning_learning_rate /
                         (1 + fine_tuning_decrease_ct * stage);
                 else
                     lr = fine_tuning_learning_rate;
 
                 layers[ i+1 ]-&gt;setLearningRate( lr );
                 connections[ i ]-&gt;setLearningRate( lr );
+                */
             }
         }
     }
@@ -1699,13 +1703,13 @@
     // Backpropagate unsupervised gradient, layer-wise
     for( int i=n_layers-1 ; i&gt;0 ; i-- )
     {
-        reconstruction_connections[ i-1 ]-&gt;fprop( 
+        reconstruction_connections[ i-1 ]-&gt;fprop(
             expectations[ i ],
             reconstruction_activations);
 
         layers[ i-1 ]-&gt;fprop( reconstruction_activations,
                                 layers[ i-1 ]-&gt;expectation);
-        
+
         layers[ i-1 ]-&gt;activation &lt;&lt; reconstruction_activations;
         //layers[ i-1 ]-&gt;expectation_is_up_to_date = true;
         layers[ i-1 ]-&gt;setExpectationByRef( layers[ i-1 ]-&gt;expectation );
@@ -1717,10 +1721,10 @@
 
         layers[ i-1 ]-&gt;update(reconstruction_activation_gradients);
 
-        reconstruction_connections[ i-1 ]-&gt;bpropUpdate( 
-            expectations[ i ], 
-            reconstruction_activations, 
-            reconstruction_expectation_gradients, 
+        reconstruction_connections[ i-1 ]-&gt;bpropUpdate(
+            expectations[ i ],
+            reconstruction_activations,
+            reconstruction_expectation_gradients,
             reconstruction_activation_gradients);
 
         if(!fast_exact_is_equal(l1_neuron_decay,0))
@@ -1742,13 +1746,13 @@
 
         if( correlation_connections.length() != 0 )
         {
-            correlation_layers[i-1]-&gt;bpropUpdate( 
+            correlation_layers[i-1]-&gt;bpropUpdate(
                 activations[i],
                 expectations[i],
                 reconstruction_activation_gradients,
                 reconstruction_expectation_gradients );
-            
-            correlation_connections[i-1]-&gt;bpropUpdate( 
+
+            correlation_connections[i-1]-&gt;bpropUpdate(
                 correlation_expectations[i-1],
                 activations[i],
                 correlation_expectation_gradients[i-1],
@@ -1758,7 +1762,7 @@
                                     correlation_expectations[i-1],
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
-            
+
             connections[i-1]-&gt;bpropUpdate( expectations[i-1],
                                            correlation_activations[i-1],
                                            reconstruction_expectation_gradients,
@@ -1766,13 +1770,13 @@
         }
         else
         {
-            layers[i]-&gt;bpropUpdate( 
+            layers[i]-&gt;bpropUpdate(
                 activations[i],
                 expectations[i],
                 reconstruction_activation_gradients,
                 reconstruction_expectation_gradients );
-            
-            connections[i-1]-&gt;bpropUpdate( 
+
+            connections[i-1]-&gt;bpropUpdate(
                 expectations[i-1],
                 activations[i],
                 reconstruction_expectation_gradients,
@@ -1783,7 +1787,7 @@
     // Put back fine-tuning learning rate
     // Set learning rates
     if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
-        lr = fine_tuning_learning_rate 
+        lr = fine_tuning_learning_rate
             / (1 + fine_tuning_decrease_ct * stage) ;
     else
         lr = fine_tuning_learning_rate ;
@@ -1807,13 +1811,13 @@
     {
         for( int i=n_layers-1 ; i&gt;0 ; i-- )
         {
-            correlation_layers[i-1]-&gt;bpropUpdate( 
+            correlation_layers[i-1]-&gt;bpropUpdate(
                 activations[i],
                 expectations[i],
                 activation_gradients[i],
                 expectation_gradients[i] );
 
-            correlation_connections[i-1]-&gt;bpropUpdate( 
+            correlation_connections[i-1]-&gt;bpropUpdate(
                 correlation_expectations[i-1],
                 activations[i],
                 correlation_expectation_gradients[i-1],
@@ -1823,8 +1827,8 @@
                                     correlation_expectations[i-1],
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
-            
-            connections[i-1]-&gt;bpropUpdate( 
+
+            connections[i-1]-&gt;bpropUpdate(
                 expectations[i-1],
                 correlation_activations[i-1],
                 expectation_gradients[i-1],
@@ -1839,12 +1843,12 @@
                                     expectations[i],
                                     activation_gradients[i],
                                     expectation_gradients[i] );
-            
+
             connections[i-1]-&gt;bpropUpdate( expectations[i-1],
                                            activations[i],
                                            expectation_gradients[i-1],
                                            activation_gradients[i] );
-        }        
+        }
     }
 }
 
@@ -1861,14 +1865,14 @@
             connections[i]-&gt;fprop( expectations[i], correlation_activations[i] );
             layers[i+1]-&gt;fprop( correlation_activations[i],
                                 correlation_expectations[i] );
-            correlation_connections[i]-&gt;fprop( correlation_expectations[i], 
+            correlation_connections[i]-&gt;fprop( correlation_expectations[i],
                                                activations[i+1] );
-            correlation_layers[i]-&gt;fprop( activations[i+1], 
+            correlation_layers[i]-&gt;fprop( activations[i+1],
                                           expectations[i+1] );
         }
     }
     else
-    {   
+    {
         for(int i=0 ; i&lt;currently_trained_layer-1 ; i++ )
         {
             connections[i]-&gt;fprop( expectations[i], activations[i+1] );
@@ -1880,33 +1884,33 @@
     {
         if(correlation_connections.length() != 0)
         {
-            connections[currently_trained_layer-1]-&gt;fprop( 
-                expectations[currently_trained_layer-1], 
+            connections[currently_trained_layer-1]-&gt;fprop(
+                expectations[currently_trained_layer-1],
                 correlation_activations[currently_trained_layer-1] );
 
             layers[currently_trained_layer]-&gt;fprop(
                 correlation_activations[currently_trained_layer-1],
                 correlation_expectations[currently_trained_layer-1] );
 
-            correlation_connections[currently_trained_layer-1]-&gt;fprop( 
-                correlation_expectations[currently_trained_layer-1], 
+            correlation_connections[currently_trained_layer-1]-&gt;fprop(
+                correlation_expectations[currently_trained_layer-1],
                 activations[currently_trained_layer] );
 
-            correlation_layers[currently_trained_layer-1]-&gt;fprop( 
-                activations[currently_trained_layer], 
+            correlation_layers[currently_trained_layer-1]-&gt;fprop(
+                activations[currently_trained_layer],
                 output );
         }
         else
         {
-            connections[currently_trained_layer-1]-&gt;fprop( 
-                expectations[currently_trained_layer-1], 
+            connections[currently_trained_layer-1]-&gt;fprop(
+                expectations[currently_trained_layer-1],
                 activations[currently_trained_layer] );
             layers[currently_trained_layer]-&gt;fprop(
                 activations[currently_trained_layer],
                 output);
         }
     }
-    else        
+    else
         final_module-&gt;fprop( expectations[ currently_trained_layer - 1],
                              output );
 }
@@ -1927,15 +1931,15 @@
                                                     reconstruction_activations);
             if( direct_connections.length() != 0 )
             {
-                direct_connections[ i ]-&gt;fprop( 
-                    expectations[ i ], 
+                direct_connections[ i ]-&gt;fprop(
+                    expectations[ i ],
                     direct_activations );
                 reconstruction_activations += direct_activations;
             }
 
             layers[ i ]-&gt;fprop( reconstruction_activations,
                                 layers[ i ]-&gt;expectation);
-            
+
             layers[ i ]-&gt;activation &lt;&lt; reconstruction_activations;
             //layers[ i ]-&gt;expectation_is_up_to_date = true;
             layers[ i ]-&gt;setExpectationByRef( layers[ i ]-&gt;expectation );
@@ -1947,7 +1951,7 @@
                 partial_costs[ i ]-&gt;fprop( expectations[ i + 1],
                                            target, partial_cost_value );
                 costs.subVec(partial_costs_positions[i],
-                             partial_cost_value.length()) &lt;&lt; 
+                             partial_cost_value.length()) &lt;&lt;
                     partial_cost_value;
             }
         }
@@ -1955,50 +1959,50 @@
 
     if( currently_trained_layer&lt;n_layers )
     {
-        reconstruction_connections[ currently_trained_layer-1 ]-&gt;fprop( 
+        reconstruction_connections[ currently_trained_layer-1 ]-&gt;fprop(
             output,
             reconstruction_activations);
         if( direct_connections.length() != 0 )
         {
-            direct_connections[ currently_trained_layer-1 ]-&gt;fprop( 
-                expectations[ currently_trained_layer-1 ], 
+            direct_connections[ currently_trained_layer-1 ]-&gt;fprop(
+                expectations[ currently_trained_layer-1 ],
                 direct_activations );
             reconstruction_activations += direct_activations;
         }
-        layers[ currently_trained_layer-1 ]-&gt;fprop( 
+        layers[ currently_trained_layer-1 ]-&gt;fprop(
             reconstruction_activations,
             layers[ currently_trained_layer-1 ]-&gt;expectation);
-        
-        layers[ currently_trained_layer-1 ]-&gt;activation &lt;&lt; 
+
+        layers[ currently_trained_layer-1 ]-&gt;activation &lt;&lt;
             reconstruction_activations;
         //layers[ currently_trained_layer-1 ]-&gt;expectation_is_up_to_date = true;
-        layers[ currently_trained_layer-1 ]-&gt;setExpectationByRef( 
+        layers[ currently_trained_layer-1 ]-&gt;setExpectationByRef(
             layers[ currently_trained_layer-1 ]-&gt;expectation );
-        costs[ currently_trained_layer-1 ] = 
+        costs[ currently_trained_layer-1 ] =
             layers[ currently_trained_layer-1 ]-&gt;fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
 
         if(reconstruct_hidden)
         {
-            connections[ currently_trained_layer-1 ]-&gt;fprop( 
-                layers[ currently_trained_layer-1 ]-&gt;expectation, 
+            connections[ currently_trained_layer-1 ]-&gt;fprop(
+                layers[ currently_trained_layer-1 ]-&gt;expectation,
                 hidden_reconstruction_activations );
-            layers[ currently_trained_layer ]-&gt;fprop( 
+            layers[ currently_trained_layer ]-&gt;fprop(
                 hidden_reconstruction_activations,
                 layers[ currently_trained_layer ]-&gt;expectation );
-            layers[ currently_trained_layer ]-&gt;activation &lt;&lt; 
+            layers[ currently_trained_layer ]-&gt;activation &lt;&lt;
                 hidden_reconstruction_activations;
             //layers[ currently_trained_layer ]-&gt;expectation_is_up_to_date = true;
-            layers[ currently_trained_layer ]-&gt;setExpectationByRef( 
+            layers[ currently_trained_layer ]-&gt;setExpectationByRef(
                 layers[ currently_trained_layer ]-&gt;expectation );
-            costs[ currently_trained_layer-1 ] += 
+            costs[ currently_trained_layer-1 ] +=
                 layers[ currently_trained_layer ]-&gt;fpropNLL(
                     output);
         }
 
         if( partial_costs &amp;&amp; partial_costs[ currently_trained_layer-1 ] )
         {
-            partial_costs[ currently_trained_layer-1 ]-&gt;fprop( 
+            partial_costs[ currently_trained_layer-1 ]-&gt;fprop(
                 output,
                 target, partial_cost_value );
             costs.subVec(partial_costs_positions[currently_trained_layer-1],
@@ -2007,7 +2011,7 @@
     }
     else
     {
-        final_cost-&gt;fprop( output, target, final_cost_value );        
+        final_cost-&gt;fprop( output, target, final_cost_value );
         costs.subVec(costs.length()-final_cost_value.length(),
                      final_cost_value.length()) &lt;&lt;
             final_cost_value;
@@ -2024,12 +2028,12 @@
 
     for( int i=0; i&lt;layers.size()-1; i++)
         cost_names.push_back(&quot;reconstruction_error_&quot; + tostring(i+1));
-    
+
     for( int i=0 ; i&lt;partial_costs.size() ; i++ )
     {
         TVec&lt;string&gt; names = partial_costs[i]-&gt;costNames();
         for(int j=0; j&lt;names.length(); j++)
-            cost_names.push_back(&quot;partial&quot; + tostring(i) + &quot;.&quot; + 
+            cost_names.push_back(&quot;partial&quot; + tostring(i) + &quot;.&quot; +
                 names[j]);
     }
 
@@ -2046,12 +2050,12 @@
         cost_names.push_back(&quot;reconstruction_error_&quot; + tostring(i+1));
 
     cost_names.push_back(&quot;global_reconstruction_error&quot;);
-    
+
     for( int i=0 ; i&lt;partial_costs.size() ; i++ )
     {
         TVec&lt;string&gt; names = partial_costs[i]-&gt;costNames();
         for(int j=0; j&lt;names.length(); j++)
-            cost_names.push_back(&quot;partial&quot; + tostring(i) + &quot;.&quot; + 
+            cost_names.push_back(&quot;partial&quot; + tostring(i) + &quot;.&quot; +
                 names[j]);
     }
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002551.html">[Plearn-commits] r9103 - in trunk: commands/EXPERIMENTAL	plearn_learners/unsupervised/EXPERIMENTAL
</A></li>
	<LI>Next message: <A HREF="002553.html">[Plearn-commits] r9105 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2552">[ date ]</a>
              <a href="thread.html#2552">[ thread ]</a>
              <a href="subject.html#2552">[ subject ]</a>
              <a href="author.html#2552">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
