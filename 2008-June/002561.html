<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9113 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9113%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806110214.m5B2EBok023477%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002560.html">
   <LINK REL="Next"  HREF="002562.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9113 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9113%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806110214.m5B2EBok023477%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9113 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Jun 11 04:14:11 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002560.html">[Plearn-commits] r9112 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002562.html">[Plearn-commits] r9114 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2561">[ date ]</a>
              <a href="thread.html#2561">[ thread ]</a>
              <a href="subject.html#2561">[ subject ]</a>
              <a href="author.html#2561">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2008-06-11 04:14:10 +0200 (Wed, 11 Jun 2008)
New Revision: 9113

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Some cleanup.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-11 02:11:56 UTC (rev 9112)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-06-11 02:14:10 UTC (rev 9113)
@@ -62,9 +62,9 @@
     up_down_learning_rate( 0. ),
     up_down_decrease_ct( 0. ),
     grad_learning_rate( 0. ),
-    batch_size( 1 ),
     grad_decrease_ct( 0. ),
     // grad_weight_decay( 0. ),
+    batch_size( 1 ),
     n_classes( -1 ),
     up_down_nstages( 0 ),
     use_classification_cost( true ),
@@ -78,8 +78,6 @@
     train_stats_window( -1 ),
     minibatch_size( 0 ),
     initialize_gibbs_chain( false ),
-    final_module_has_learning_rate( false ),
-    final_cost_has_learning_rate( false ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
     final_cost_index( -1 ),
@@ -110,7 +108,7 @@
                   &quot;The decrease constant of the learning rate used during&quot;
                   &quot; contrastive divergence&quot;);
 
-    declareOption(ol, &quot;up_down_learning_rate&quot;, 
+    declareOption(ol, &quot;up_down_learning_rate&quot;,
                   &amp;DeepBeliefNet::up_down_learning_rate,
                   OptionBase::buildoption,
                   &quot;The learning rate used in the up-down algorithm during the\n&quot;
@@ -305,17 +303,21 @@
 
     declareOption(ol, &quot;minibatch_size&quot;, &amp;DeepBeliefNet::minibatch_size,
                   OptionBase::learntoption,
-                  &quot;Size of a mini-batch.&quot;);
+                  &quot;Actual size of a mini-batch (size of the training set if&quot;
+                  &quot; batch_size==1).&quot;);
 
     declareOption(ol, &quot;gibbs_down_state&quot;, &amp;DeepBeliefNet::gibbs_down_state,
                   OptionBase::learntoption,
-                  &quot;State of visible units of RBMs at each layer in background Gibbs chain.&quot;);
+                  &quot;State of visible units of RBMs at each layer in background&quot;
+                  &quot; Gibbs chain.&quot;);
 
-    declareOption(ol, &quot;cumulative_training_time&quot;, &amp;DeepBeliefNet::cumulative_training_time,
+    declareOption(ol, &quot;cumulative_training_time&quot;,
+                  &amp;DeepBeliefNet::cumulative_training_time,
                   OptionBase::learntoption | OptionBase::nosave,
                   &quot;Cumulative training time since age=0, in seconds.\n&quot;);
 
-    declareOption(ol, &quot;cumulative_testing_time&quot;, &amp;DeepBeliefNet::cumulative_testing_time,
+    declareOption(ol, &quot;cumulative_testing_time&quot;,
+                  &amp;DeepBeliefNet::cumulative_testing_time,
                   OptionBase::learntoption | OptionBase::nosave,
                   &quot;Cumulative testing time since age=0, in seconds.\n&quot;);
 
@@ -324,7 +326,7 @@
                   &quot;Number of samples visited so far during unsupervised\n&quot;
                   &quot;fine-tuning.\n&quot;);
 
-    declareOption(ol, &quot;generative_connections&quot;, 
+    declareOption(ol, &quot;generative_connections&quot;,
                   &amp;DeepBeliefNet::generative_connections,
                   OptionBase::learntoption,
                   &quot;The untied generative weights of the connections&quot;
@@ -791,7 +793,7 @@
 
     if( final_module )
         out_size += final_module-&gt;output_size;
-    
+
     if( !use_classification_cost &amp;&amp; !final_module )
         out_size += layers[i_output_layer]-&gt;size;
 
@@ -907,9 +909,10 @@
     if (online)
     {
         // Train all layers simultaneously AND fine-tuning as well!
+        int init_stage = stage;
         if( report_progress &amp;&amp; stage &lt; nstages )
             pb = new ProgressBar( &quot;Training &quot;+classname(),
-                                  nstages - stage );
+                                  nstages - init_stage );
 
         setLearningRate( grad_learning_rate );
         train_stats-&gt;forget();
@@ -926,7 +929,7 @@
                     setLearningRate( grad_learning_rate
                                      / (1. + grad_decrease_ct * stage ));
 
-                if (batch_size &gt; 1 || minibatch_hack)
+                if (minibatch_size &gt; 1 || minibatch_hack)
                 {
                     train_set-&gt;getExamples(sample_start, minibatch_size,
                                            inputs, targets, weights, NULL, true);
@@ -953,7 +956,7 @@
             }
 
             if( pb )
-                pb-&gt;update( stage + 1 );
+                pb-&gt;update( stage - init_stage + 1 );
         }
     }
     else // Greedy learning, one layer at a time.
@@ -1000,7 +1003,7 @@
                 // Do a step every 'minibatch_size' examples.
                 if (stage % minibatch_size == 0) {
                     int sample_start = stage % nsamples;
-                    if (batch_size &gt; 1 || minibatch_hack) {
+                    if (minibatch_size &gt; 1 || minibatch_hack) {
                         train_set-&gt;getExamples(sample_start, minibatch_size,
                                 inputs, targets, weights, NULL, true);
                         train_costs_m.fill(MISSING_VALUE);
@@ -1194,8 +1197,8 @@
 ////////////////
 // onlineStep //
 ////////////////
-void DeepBeliefNet::onlineStep( const Vec&amp; input, const Vec&amp; target,
-                                Vec&amp; train_costs)
+void DeepBeliefNet::onlineStep(const Vec&amp; input, const Vec&amp; target,
+                               Vec&amp; train_costs)
 {
     real lr;
     PLASSERT(batch_size == 1);
@@ -1600,7 +1603,7 @@
                 lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
             else
                 lr = grad_learning_rate;
-            
+
             connections[ i ]-&gt;setLearningRate( lr );
             layers[ i+1 ]-&gt;setLearningRate( lr );
 
@@ -1672,7 +1675,7 @@
 ////////////////
 // greedyStep //
 ////////////////
-void DeepBeliefNet::greedyStep( const Vec&amp; input, const Vec&amp; target, int index )
+void DeepBeliefNet::greedyStep(const Vec&amp; input, const Vec&amp; target, int index)
 {
     real lr;
     PLASSERT( index &lt; n_layers );
@@ -1689,8 +1692,8 @@
     {
         // put appropriate learning rate
         if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            lr = grad_learning_rate / 
-                (1. + grad_decrease_ct * 
+            lr = grad_learning_rate /
+                (1. + grad_decrease_ct *
                  (stage - cumulative_schedule[index]));
         else
             lr = grad_learning_rate;
@@ -1721,7 +1724,7 @@
 
         // put back old learning rate
         if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+            lr = cd_learning_rate / (1. + cd_decrease_ct *
                                      (stage - cumulative_schedule[index]));
         else
             lr = cd_learning_rate;
@@ -1739,7 +1742,8 @@
 /////////////////
 // greedySteps //
 /////////////////
-void DeepBeliefNet::greedyStep( const Mat&amp; inputs, const Mat&amp; targets, int index, Mat&amp; train_costs_m )
+void DeepBeliefNet::greedyStep(const Mat&amp; inputs, const Mat&amp; targets,
+                               int index, Mat&amp; train_costs_m)
 {
     real lr;
     PLASSERT( index &lt; n_layers );
@@ -1756,8 +1760,8 @@
     {
         // put appropriate learning rate
         if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            lr = grad_learning_rate / 
-                (1. + grad_decrease_ct * 
+            lr = grad_learning_rate /
+                (1. + grad_decrease_ct *
                  (stage - cumulative_schedule[index]));
         else
             lr = grad_learning_rate;
@@ -1788,7 +1792,7 @@
 
         // put back old learning rate
         if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+            lr = cd_learning_rate / (1. + cd_decrease_ct *
                                      (stage - cumulative_schedule[index]));
         else
             lr = cd_learning_rate;
@@ -1842,17 +1846,17 @@
 
         // put appropriate learning rate
         if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
-            lr = grad_learning_rate 
-                / (1. + grad_decrease_ct * 
+            lr = grad_learning_rate
+                / (1. + grad_decrease_ct *
                    (stage - cumulative_schedule[n_layers-2]));
         else
             lr = grad_learning_rate;
-        
+
         partial_costs[ n_layers-2 ]-&gt;setLearningRate( lr );
         connections[ n_layers-2 ]-&gt;setLearningRate( lr );
         layers[ n_layers-1 ]-&gt;setLearningRate( lr );
-        
 
+
         // Backward pass
         real cost;
         partial_costs[ n_layers-2 ]-&gt;fprop( layers[ n_layers-1 ]-&gt;expectation,
@@ -1876,8 +1880,8 @@
 
         // put back old learning rate
         if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
-            lr = cd_learning_rate 
-                / (1. + cd_decrease_ct * 
+            lr = cd_learning_rate
+                / (1. + cd_decrease_ct *
                    (stage - cumulative_schedule[n_layers-2]));
         else
             lr = cd_learning_rate;
@@ -1895,6 +1899,12 @@
         layers[ n_layers-1 ], n_layers-2);
 }
 
+void DeepBeliefNet::jointGreedyStep(const Mat&amp; inputs, const Mat&amp; targets)
+{
+    PLCHECK_MSG(false, &quot;Not implemented for mini-batches&quot;);
+}
+
+
 ////////////////
 // upDownStep //
 ////////////////
@@ -1970,6 +1980,12 @@
     }
 }
 
+void DeepBeliefNet::upDownStep(const Mat&amp; inputs, const Mat&amp; targets,
+                               Mat&amp; train_costs)
+{
+    PLCHECK_MSG(false, &quot;Not implemented for mini-batches&quot;);
+}
+
 ////////////////////
 // fineTuningStep //
 ////////////////////
@@ -2079,7 +2095,7 @@
 }
 
 void DeepBeliefNet::fineTuningStep(const Mat&amp; inputs, const Mat&amp; targets,
-                                   Mat&amp; train_costs )
+                                   Mat&amp; train_costs)
 {
     final_cost_values.resize(0, 0);
     // fprop
@@ -2239,7 +2255,8 @@
         }
     }
 
-    if (mbatch) {
+    if (mbatch)
+    {
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_vals.resize(minibatch_size, down_layer-&gt;size);
@@ -2366,7 +2383,7 @@
     } else {
         if( !use_mean_field_contrastive_divergence )
             up_layer-&gt;generateSample();
-        
+
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_val.resize( down_layer-&gt;size );
@@ -2572,7 +2589,7 @@
 
 //! This function is usefull when the NLL CostModule AND/OR the final_cost Module
 //! are more efficient with batch computation (or need to be computed on a bunch of examples, as LayerCostModule)
-void DeepBeliefNet::computeOutputsAndCosts(const Mat&amp; inputs, const Mat&amp; targets, 
+void DeepBeliefNet::computeOutputsAndCosts(const Mat&amp; inputs, const Mat&amp; targets,
                                       Mat&amp; outputs, Mat&amp; costs) const
 {
     int nsamples = inputs.length();
@@ -2583,7 +2600,7 @@
     for (int isample = 0; isample &lt; nsamples; isample++ )
     {
         Vec in_i = inputs(isample);
-        Vec out_i = outputs(isample); 
+        Vec out_i = outputs(isample);
         computeOutput(in_i, out_i);
         if( !partial_costs.isEmpty() )
         {
@@ -2627,7 +2644,7 @@
         costs.subMat( 0, nll_cost_index, nsamples, 1) &lt;&lt; pcosts;
 
         for (int isample = 0; isample &lt; nsamples; isample++ )
-	    costs(isample,class_cost_index) =
+            costs(isample,class_cost_index) =
                 (argmax(outputs(isample).subVec(0, n_classes)) == (int) round(targets(isample,0))) ? 0 : 1;
     }
 
@@ -2643,7 +2660,7 @@
 
     if( !partial_costs.isEmpty() )
         PLERROR(&quot;cannot compute partial costs in DeepBeliefNet::computeCostsFromOutputs(Mat&amp;, Mat&amp;, Mat&amp;, Mat&amp;)&quot;
-	        &quot;(expectations are not up to date in the batch version)&quot;);
+                &quot;(expectations are not up to date in the batch version)&quot;);
 }
 
 void DeepBeliefNet::test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats, VMat testoutputs, VMat testcosts) const
@@ -2660,7 +2677,7 @@
     //  E[testN-1.E[cumulative_test_time] will basically be the cumulative test
     //  time until (and including) the N-1th split! So it's a pretty
     //  meaningless number (more or less).
-      
+
     Profiler::reset(&quot;testing&quot;);
     Profiler::start(&quot;testing&quot;);
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2008-06-11 02:11:56 UTC (rev 9112)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2008-06-11 02:14:10 UTC (rev 9113)
@@ -68,11 +68,11 @@
     //! The learning rate used during contrastive divergence learning
     real cd_learning_rate;
 
-    //! The decrease constant of the learning rate used during 
+    //! The decrease constant of the learning rate used during
     //! contrastive divergence learning
     real cd_decrease_ct;
 
-    //! The learning rate used in the up-down algorithm during the 
+    //! The learning rate used in the up-down algorithm during the
     //! unsupervised fine tuning gradient descent
     real up_down_learning_rate;
 
@@ -83,8 +83,6 @@
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
-    int batch_size;
-
     //! The decrease constant of the learning rate used during gradient descent
     real grad_decrease_ct;
 
@@ -93,6 +91,9 @@
     real grad_weight_decay;
     */
 
+    //! Training batch size (1=stochastic learning, 0=full batch learning)
+    int batch_size;
+
     //! Number of classes in the training set
     //!   - 0 means we are doing regression,
     //!   - 1 means we have two classes, but only one output,
@@ -230,13 +231,11 @@
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void forget();
 
     //! The role of the train method is to bring the learner up to
     //! stage==nstages, updating the train_stats collector with training costs
     //! measured on-line in the process.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void train();
 
     //! Re-implementation of the PLearner test() for profiling purposes
@@ -244,18 +243,17 @@
                       VMat testoutputs=0, VMat testcosts=0) const;
 
     //! Computes the output from the input.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
 
-    virtual void computeOutputsAndCosts(const Mat&amp; inputs, const Mat&amp; targets, 
+    virtual void computeOutputsAndCosts(const Mat&amp; inputs, const Mat&amp; targets,
                                         Mat&amp; outputs, Mat&amp; costs) const;
 
     //! Computes the costs from already computed output.
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
                                          const Vec&amp; target, Vec&amp; costs) const;
-    virtual void computeClassifAndFinalCostsFromOutputs(const Mat&amp; inputs, const Mat&amp; outputs,
-                                                        const Mat&amp; targets, Mat&amp; costs) const;
+    virtual void computeClassifAndFinalCostsFromOutputs(
+            const Mat&amp; inputs, const Mat&amp; outputs,
+            const Mat&amp; targets, Mat&amp; costs) const;
 
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
@@ -268,27 +266,24 @@
     virtual TVec&lt;std::string&gt; getTrainCostNames() const;
 
 
-    void onlineStep( const Vec&amp; input, const Vec&amp; target, Vec&amp; train_costs );
+    void onlineStep(const Vec&amp; input, const Vec&amp; target, Vec&amp; train_costs);
+    void onlineStep(const Mat&amp; inputs, const Mat&amp; targets, Mat&amp; train_costs);
 
-    void onlineStep( const Mat&amp; inputs, const Mat&amp; targets, Mat&amp; train_costs );
+    void greedyStep(const Vec&amp; input, const Vec&amp; target, int index);
+    void greedyStep(const Mat&amp; inputs, const Mat&amp; targets, int index,
+                    Mat&amp; train_costs_m);
 
-    void greedyStep( const Vec&amp; input, const Vec&amp; target, int index );
 
-    //! Greedy step with mini-batches.
-    void greedyStep(const Mat&amp; inputs, const Mat&amp; targets, int index, Mat&amp; train_costs_m);
+    void jointGreedyStep(const Vec&amp; input, const Vec&amp; target);
+    void jointGreedyStep(const Mat&amp; inputs, const Mat&amp; targets);
 
-    void jointGreedyStep( const Vec&amp; input, const Vec&amp; target );
+    void upDownStep(const Vec&amp; input, const Vec&amp; target, Vec&amp; train_costs);
+    void upDownStep(const Mat&amp; inputs, const Mat&amp; targets, Mat&amp; train_costs);
 
-    void upDownStep( const Vec&amp; input, const Vec&amp; target,
-                     Vec&amp; train_costs );
+    void fineTuningStep(const Vec&amp; input, const Vec&amp; target, Vec&amp; train_costs);
+    void fineTuningStep(const Mat&amp; inputs, const Mat&amp; targets,
+                        Mat&amp; train_costs);
 
-    void fineTuningStep( const Vec&amp; input, const Vec&amp; target,
-                         Vec&amp; train_costs );
-
-    //! Fine tuning step with mini-batches.
-    void fineTuningStep( const Mat&amp; inputs, const Mat&amp; targets,
-                         Mat&amp; train_costs );
-
     //! Perform a step of contrastive divergence, assuming that
     //! down_layer-&gt;expectation(s) is set.
     void contrastiveDivergenceStep( const PP&lt;RBMLayer&gt;&amp; down_layer,
@@ -317,19 +312,16 @@
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
     PLEARN_DECLARE_OBJECT(DeepBeliefNet);
 
     // Simply calls inherited::build() then build_()
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
 
 protected:
-
+    //! Actual size of a mini-batch (size of the training set if batch_size==1)
     int minibatch_size;
 
     //#####  Not Options  #####################################################
@@ -363,22 +355,14 @@
     mutable Vec final_cost_gradient;
     mutable Mat final_cost_gradients; //!&lt; For mini-batch.
 
-    //! buffers bottom layer activation during onlineStep 
+    //! buffers bottom layer activation during onlineStep
     mutable Vec save_layer_activation;
-
     Mat save_layer_activations; //!&lt; For mini-batches.
 
-    //! buffers bottom layer expectation during onlineStep 
+    //! buffers bottom layer expectation during onlineStep
     mutable Vec save_layer_expectation;
-
     Mat save_layer_expectations; //!&lt; For mini-batches.
 
-    //! Does final_module exist and have a &quot;learning_rate&quot; option
-    bool final_module_has_learning_rate;
-
-    //! Does final_cost exist and have a &quot;learning_rate&quot; option
-    bool final_cost_has_learning_rate;
-
     //! Store a copy of the positive phase values
     mutable Vec pos_down_val;
     mutable Vec pos_up_val;
@@ -414,7 +398,7 @@
     //! Index of the cpu time cost (per each call of train())
     int training_cpu_time_cost_index;
 
-    //! The index of the cumulative training time cost 
+    //! The index of the cumulative training time cost
     int cumulative_training_time_cost_index;
 
     //! The index of the cumulative testing time cost
@@ -423,7 +407,7 @@
     //! Holds the total training (cpu)time
     real cumulative_training_time;
 
-    //! Holds the total testing (cpu)time 
+    //! Holds the total testing (cpu)time
     mutable real cumulative_testing_time;
 
     //! Cumulative training schedule


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002560.html">[Plearn-commits] r9112 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002562.html">[Plearn-commits] r9114 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2561">[ date ]</a>
              <a href="thread.html#2561">[ thread ]</a>
              <a href="subject.html#2561">[ subject ]</a>
              <a href="author.html#2561">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
