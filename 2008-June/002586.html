<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9138 - trunk/plearn_learners_experimental
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9138%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200806171947.m5HJl0Tr021634%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002585.html">
   <LINK REL="Next"  HREF="002587.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9138 - trunk/plearn_learners_experimental</H1>
    <B>larocheh at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9138%20-%20trunk/plearn_learners_experimental&In-Reply-To=%3C200806171947.m5HJl0Tr021634%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9138 - trunk/plearn_learners_experimental">larocheh at mail.berlios.de
       </A><BR>
    <I>Tue Jun 17 21:47:00 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002585.html">[Plearn-commits] r9137 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002587.html">[Plearn-commits] r9139 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2586">[ date ]</a>
              <a href="thread.html#2586">[ thread ]</a>
              <a href="subject.html#2586">[ subject ]</a>
              <a href="author.html#2586">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: larocheh
Date: 2008-06-17 21:46:59 +0200 (Tue, 17 Jun 2008)
New Revision: 9138

Added:
   trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc
   trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h
Log:
For Doomie...



Added: trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc
===================================================================
--- trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc	2008-06-17 18:21:44 UTC (rev 9137)
+++ trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.cc	2008-06-17 19:46:59 UTC (rev 9138)
@@ -0,0 +1,3143 @@
+// -*- C++ -*-
+
+// NeuralProbabilisticLanguageModel.cc
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/*! \file PLearnLibrary/PLearnAlgo/NeuralProbabilisticLanguageModel.h */
+
+
+#include &quot;NeuralProbabilisticLanguageModel.h&quot;
+#include &lt;plearn/vmat/SubVMatrix.h&gt;
+//#include &lt;plearn/sys/Profiler.h&gt;
+#include &lt;time.h&gt;
+#include &lt;stdio.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(NeuralProbabilisticLanguageModel, 
+                        &quot;Feedforward neural network for language modeling&quot;,
+                        &quot;Implementation of the Neural Probabilistic Language &quot;
+                        &quot;Model proposed by \n&quot;
+                        &quot;Bengio, Ducharme, Vincent and Jauvin (JMLR 2003), &quot;
+                        &quot;with extentensions to speedup\n&quot;
+                        &quot;the model (Bengio and S&#233;n&#233;cal, AISTATS 2003) and &quot;
+                        &quot;to include prior information\n&quot;
+                        &quot;about the distributed representation and permit &quot;
+                        &quot;generalization of these\n&quot;
+                        &quot;distributed representations to out-of-vocabulary &quot;
+                        &quot;words using features \n&quot;
+                        &quot;(Larochelle and Bengio, Tech Report 2006).\n&quot;);
+
+NeuralProbabilisticLanguageModel::NeuralProbabilisticLanguageModel() 
+// DEFAULT VALUES FOR ALL OPTIONS
+    :
+rgen(new PRandom()),
+nhidden(0),
+nhidden2(0),
+weight_decay(0),
+bias_decay(0),
+layer1_weight_decay(0),
+layer1_bias_decay(0),
+layer2_weight_decay(0),
+layer2_bias_decay(0),
+output_layer_weight_decay(0),
+output_layer_bias_decay(0),
+direct_in_to_out_weight_decay(0),
+output_layer_dist_rep_weight_decay(0),
+output_layer_dist_rep_bias_decay(0),
+fixed_output_weights(0),
+direct_in_to_out(0),
+penalty_type(&quot;L2_square&quot;),
+output_transfer_func(&quot;&quot;),
+hidden_transfer_func(&quot;tanh&quot;),
+start_learning_rate(0.01),
+decrease_constant(0),
+batch_size(1),
+stochastic_gradient_descent_speedup(true),
+initialization_method(&quot;uniform_linear&quot;),
+dist_rep_dim(-1),
+possible_targets_vary(false),
+train_proposal_distribution(true),
+sampling_block_size(50),
+minimum_effective_sample_size(100)
+{}
+
+NeuralProbabilisticLanguageModel::~NeuralProbabilisticLanguageModel()
+{
+}
+
+void NeuralProbabilisticLanguageModel::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;nhidden&quot;, &amp;NeuralProbabilisticLanguageModel::nhidden, 
+                  OptionBase::buildoption, 
+                  &quot;Number of hidden units in first hidden layer (0 means no &quot;
+                  &quot;hidden layer).\n&quot;);
+    
+    declareOption(ol, &quot;nhidden2&quot;, &amp;NeuralProbabilisticLanguageModel::nhidden2, 
+                  OptionBase::buildoption, 
+                  &quot;Number of hidden units in second hidden layer (0 means no &quot;
+                  &quot;hidden layer).\n&quot;);
+    
+    declareOption(ol, &quot;weight_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::weight_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Global weight decay for all layers.\n&quot;);
+    
+    declareOption(ol, &quot;bias_decay&quot;, &amp;NeuralProbabilisticLanguageModel::bias_decay,
+                  OptionBase::buildoption, 
+                  &quot;Global bias decay for all layers.\n&quot;);
+    
+    declareOption(ol, &quot;layer1_weight_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::layer1_weight_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional weight decay for the first hidden layer. &quot;
+                  &quot;Is added to weight_decay.\n&quot;);
+    
+    declareOption(ol, &quot;layer1_bias_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::layer1_bias_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional bias decay for the first hidden layer. &quot;
+                  &quot;Is added to bias_decay.\n&quot;);
+    
+    declareOption(ol, &quot;layer2_weight_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::layer2_weight_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional weight decay for the second hidden layer. &quot;
+                  &quot;Is added to weight_decay.\n&quot;);
+    
+    declareOption(ol, &quot;layer2_bias_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::layer2_bias_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional bias decay for the second hidden layer. &quot;
+                  &quot;Is added to bias_decay.\n&quot;);
+    
+    declareOption(ol, &quot;output_layer_weight_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::output_layer_weight_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional weight decay for the output layer. &quot;
+                  &quot;Is added to 'weight_decay'.\n&quot;);
+    
+    declareOption(ol, &quot;output_layer_bias_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::output_layer_bias_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional bias decay for the output layer. &quot;
+                  &quot;Is added to 'bias_decay'.\n&quot;);
+    
+    declareOption(ol, &quot;direct_in_to_out_weight_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::direct_in_to_out_weight_decay,
+                  OptionBase::buildoption,
+                  &quot;Additional weight decay for the weights going from the &quot;
+                  &quot;input directly to the \n output layer.  Is added to &quot;
+                  &quot;'weight_decay'.\n&quot;);
+    
+    declareOption(ol, &quot;output_layer_dist_rep_weight_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::output_layer_dist_rep_weight_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional weight decay for the output layer of distributed&quot;
+                  &quot;representation\n&quot;
+                  &quot;predictor.  Is added to 'weight_decay'.\n&quot;);
+    
+    declareOption(ol, &quot;output_layer_dist_rep_bias_decay&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::output_layer_dist_rep_bias_decay, 
+                  OptionBase::buildoption, 
+                  &quot;Additional bias decay for the output layer of distributed&quot;
+                  &quot;representation\n&quot;
+                  &quot;predictor.  Is added to 'bias_decay'.\n&quot;);
+    
+    declareOption(ol, &quot;fixed_output_weights&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::fixed_output_weights, 
+                  OptionBase::buildoption, 
+                  &quot;If true then the output weights are not learned. They are&quot;
+                  &quot;initialized to +1 or -1 randomly.\n&quot;);
+    
+    declareOption(ol, &quot;direct_in_to_out&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::direct_in_to_out, 
+                  OptionBase::buildoption, 
+                  &quot;If true then direct input to output weights will be added &quot;
+                  &quot;(if nhidden &gt; 0).\n&quot;);
+    
+    declareOption(ol, &quot;penalty_type&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::penalty_type,
+                  OptionBase::buildoption,
+                  &quot;Penalty to use on the weights (for weight and bias decay).\n&quot;
+                  &quot;Can be any of:\n&quot;
+                  &quot;  - \&quot;L1\&quot;: L1 norm,\n&quot;
+                  &quot;  - \&quot;L2_square\&quot; (default): square of the L2 norm.\n&quot;);
+    
+    declareOption(ol, &quot;output_transfer_func&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::output_transfer_func, 
+                  OptionBase::buildoption, 
+                  &quot;what transfer function to use for ouput layer? One of: \n&quot;
+                  &quot;  - \&quot;tanh\&quot; \n&quot;
+                  &quot;  - \&quot;sigmoid\&quot; \n&quot;
+                  &quot;  - \&quot;softmax\&quot; \n&quot;
+                  &quot;An empty string or \&quot;none\&quot; means no output transfer function \n&quot;);
+    
+    declareOption(ol, &quot;hidden_transfer_func&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::hidden_transfer_func, 
+                  OptionBase::buildoption, 
+                  &quot;What transfer function to use for hidden units? One of \n&quot;
+                  &quot;  - \&quot;linear\&quot; \n&quot;
+                  &quot;  - \&quot;tanh\&quot; \n&quot;
+                  &quot;  - \&quot;sigmoid\&quot; \n&quot;
+                  &quot;  - \&quot;softmax\&quot; \n&quot;);
+    
+    declareOption(ol, &quot;cost_funcs&quot;, &amp;NeuralProbabilisticLanguageModel::cost_funcs, 
+                  OptionBase::buildoption, 
+                  &quot;A list of cost functions to use\n&quot;
+                  &quot;in the form \&quot;[ cf1; cf2; cf3; ... ]\&quot; where each function &quot;
+                  &quot;is one of: \n&quot;
+                  &quot;  - \&quot;NLL\&quot; (negative log likelihood -log(p[c]) for &quot;
+                  &quot;classification) \n&quot;
+                  &quot;  - \&quot;class_error\&quot; (classification error) \n&quot;
+                  &quot;The FIRST function of the list will be used as \n&quot;
+                  &quot;the objective function to optimize \n&quot;
+                  &quot;(possibly with an added weight decay penalty) \n&quot;);
+    
+    declareOption(ol, &quot;start_learning_rate&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::start_learning_rate, 
+                  OptionBase::buildoption, 
+                  &quot;Start learning rate of gradient descent.\n&quot;);
+                  
+    declareOption(ol, &quot;decrease_constant&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::decrease_constant, 
+                  OptionBase::buildoption, 
+                  &quot;Decrease constant of gradient descent.\n&quot;);
+
+    declareOption(ol, &quot;batch_size&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::batch_size, 
+                  OptionBase::buildoption, 
+                  &quot;How many samples to use to estimate the avergage gradient before updating the weights\n&quot;
+                  &quot;0 is equivalent to specifying training_set-&gt;length() \n&quot;);
+
+    declareOption(ol, &quot;stochastic_gradient_descent_speedup&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::stochastic_gradient_descent_speedup, 
+                  OptionBase::buildoption, 
+                  &quot;Indication that a trick to speedup stochastic &quot;
+                  &quot;gradient descent\n&quot;
+                  &quot;should be used.\n&quot;);
+
+    declareOption(ol, &quot;initialization_method&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::initialization_method, 
+                  OptionBase::buildoption, 
+                  &quot;The method used to initialize the weights:\n&quot;
+                  &quot; - \&quot;normal_linear\&quot;  = a normal law with variance &quot;
+                  &quot;1/n_inputs\n&quot;
+                  &quot; - \&quot;normal_sqrt\&quot;    = a normal law with variance &quot;
+                  &quot;1/sqrt(n_inputs)\n&quot;
+                  &quot; - \&quot;uniform_linear\&quot; = a uniform law in [-1/n_inputs,&quot;
+                  &quot;1/n_inputs]\n&quot;
+                  &quot; - \&quot;uniform_sqrt\&quot;   = a uniform law in [-1/sqrt(n_inputs),&quot;
+                  &quot;1/sqrt(n_inputs)]\n&quot;
+                  &quot; - \&quot;zero\&quot;           = all weights are set to 0\n&quot;);
+    
+    declareOption(ol, &quot;dist_rep_dim&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::dist_rep_dim, 
+                  OptionBase::buildoption, 
+                  &quot; Dimensionality (number of components) of distributed &quot;
+                  &quot;representations.\n&quot;
+                  &quot;If &lt;= 0, than distributed representations will not be used.\n&quot;
+        );
+    
+    declareOption(ol, &quot;possible_targets_vary&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::possible_targets_vary, 
+                  OptionBase::buildoption, 
+                  &quot;Indication that the set of possible targets vary from\n&quot;
+                  &quot;one input vector to another.\n&quot;
+        );
+    
+    declareOption(ol, &quot;feat_sets&quot;, &amp;NeuralProbabilisticLanguageModel::feat_sets, 
+                                OptionBase::buildoption, 
+                  &quot;FeatureSets to apply on input. The number of feature\n&quot;
+                  &quot;sets should be a divisor of inputsize(). The feature\n&quot;
+                  &quot;sets applied to the ith input field is the feature\n&quot;
+                  &quot;set at position i % feat_sets.length().\n&quot;
+        );
+
+    declareOption(ol, &quot;train_proposal_distribution&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::train_proposal_distribution
+                  OptionBase::buildoption, 
+                  &quot;Indication that the proposal distribution must be trained\n&quot;
+                  &quot;(using train_set).\n&quot;
+        );
+
+    declareOption(ol, &quot;sampling_block_size&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::sampling_block_size, 
+                  OptionBase::buildoption, 
+                  &quot;Size of the sampling blocks.\n&quot;
+        );
+
+    declareOption(ol, &quot;minimum_effective_sample_size&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::minimum_effective_sample_size, 
+                  OptionBase::buildoption, 
+                  &quot;Minimum effective sample size.\n&quot;
+        );
+
+    declareOption(ol, &quot;train_set&quot;, &amp;NeuralProbabilisticLanguageModel::train_set, 
+                  OptionBase::learntoption, 
+                  &quot;VMatrix used for training, that also provides information about the data (e.g. Dictionary objects for the different fields).\n&quot;);
+
+
+                  // Networks' learnt parameters
+    declareOption(ol, &quot;w1&quot;, &amp;NeuralProbabilisticLanguageModel::w1, 
+                  OptionBase::learntoption, 
+                  &quot;Weights of first hidden layer.\n&quot;);
+    declareOption(ol, &quot;b1&quot;, &amp;NeuralProbabilisticLanguageModel::b1, 
+                  OptionBase::learntoption, 
+                  &quot;Bias of first hidden layer.\n&quot;);
+    declareOption(ol, &quot;w2&quot;, &amp;NeuralProbabilisticLanguageModel::w2, 
+                  OptionBase::learntoption, 
+                  &quot;Weights of second hidden layer.\n&quot;);
+    declareOption(ol, &quot;b2&quot;, &amp;NeuralProbabilisticLanguageModel::b2, 
+                  OptionBase::learntoption, 
+                  &quot;Bias of second hidden layer.\n&quot;);
+    declareOption(ol, &quot;wout&quot;, &amp;NeuralProbabilisticLanguageModel::wout, 
+                  OptionBase::learntoption, 
+                  &quot;Weights of output layer.\n&quot;);
+    declareOption(ol, &quot;bout&quot;, &amp;NeuralProbabilisticLanguageModel::bout, 
+                  OptionBase::learntoption, 
+                  &quot;Bias of output layer.\n&quot;);
+    declareOption(ol, &quot;direct_wout&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::direct_wout, 
+                  OptionBase::learntoption, 
+                  &quot;Direct input to output weights.\n&quot;);
+    declareOption(ol, &quot;direct_bout&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::direct_bout, 
+                  OptionBase::learntoption, 
+                  &quot;Direct input to output bias.\n&quot;);
+    declareOption(ol, &quot;wout_dist_rep&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::wout_dist_rep, 
+                  OptionBase::learntoption, 
+                  &quot;Weights of output layer for distributed representation &quot;
+                  &quot;predictor.\n&quot;);
+    declareOption(ol, &quot;bout_dist_rep&quot;, 
+                  &amp;NeuralProbabilisticLanguageModel::bout_dist_rep, 
+                  OptionBase::learntoption, 
+                  &quot;Bias of output layer for distributed representation &quot;
+                  &quot;predictor.\n&quot;);
+
+    inherited::declareOptions(ol);
+
+}
+
+///////////
+// build //
+///////////
+void NeuralProbabilisticLanguageModel::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+////////////
+// build_ //
+////////////
+void NeuralProbabilisticLanguageModel::build_()
+{
+    // Don't do anything if we don't have a train_set
+    // It's the only one who knows the inputsize, targetsize and weightsize
+
+    if(inputsize_&gt;=0 &amp;&amp; targetsize_&gt;=0 &amp;&amp; weightsize_&gt;=0)
+    {
+        if(targetsize_ != 1)
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                    &quot;targetsize_ must be 1, not %d&quot;,targetsize_);
+
+        n_feat_sets = feat_sets.length();
+
+        if(n_feat_sets == 0)
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                    &quot;at least one FeatureSet must be provided\n&quot;);
+        
+        if(inputsize_ % n_feat_sets != 0)
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                    &quot;feat_sets.length() must be a divisor of inputsize()&quot;);
+        
+        // Process penalty type option
+        string pt = lowerstring( penalty_type );
+        if( pt == &quot;l1&quot; )
+            penalty_type = &quot;L1&quot;;
+        else if( pt == &quot;l2_square&quot; || pt == &quot;l2 square&quot; || pt == &quot;l2square&quot; )
+            penalty_type = &quot;L2_square&quot;;
+        else if( pt == &quot;l2&quot; )
+        {
+            PLWARNING(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                      &quot;L2 penalty not supported, assuming you want L2 square&quot;);
+            penalty_type = &quot;L2_square&quot;;
+        }
+        else
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                    &quot;penalty_type \&quot;%s\&quot; not supported&quot;, penalty_type.c_str());
+        
+        int ncosts = cost_funcs.size();  
+        if(ncosts&lt;=0)
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                    &quot;Empty cost_funcs : must at least specify the cost &quot;
+                    &quot;function to optimize!&quot;);
+        
+        if(stage &lt;= 0 ) // Training hasn't started
+        {
+            // Initialize parameters
+            initializeParams();                        
+        }
+        
+        output_comp.resize(total_output_size);
+        row.resize(train_set-&gt;width());
+        row.fill(MISSING_VALUE);
+        feats.resize(inputsize_);
+        // Making sure that all feats[i] have non null storage...
+        for(int i=0; i&lt;feats.length(); i++)
+        {
+            feats[i].resize(1);
+            feats[i].resize(0);
+        }
+        if(fixed_output_weights &amp;&amp; stochastic_gradient_descent_speedup)
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                    &quot;cannot use stochastic gradient descent speedup with &quot;
+                    &quot;fixed output weights&quot;);
+        val_string_reference_set = train_set;
+        target_values_reference_set = train_set;
+
+        if(proposal_distribution)
+        {
+            if(batch_size != 1)
+                PLERROR(&quot;In NeuralProbabilisticLanguageModel::build_(): &quot;
+                        &quot;importance sampling speedup is not implemented for&quot;
+                        &quot;batch size != 1&quot;);
+            sample.resize(1);            
+            if(train_proposal_distribution)
+            {
+                proposal_distribution-&gt;setTrainingSet(train_set);
+                proposal_distribution-&gt;train();
+            }
+        }
+    }
+}
+
+void NeuralProbabilisticLanguageModel::fprop(const Vec&amp; inputv, Vec&amp; outputv, 
+                                             const Vec&amp; targetv, Vec&amp; costsv, 
+                                             real sampleweight) const
+{
+    
+    fpropOutput(inputv,outputv);
+    //if(is_missing(outputv[0]))
+    //    cout &lt;&lt; &quot;What the fuck&quot; &lt;&lt; endl;
+    fpropCostsFromOutput(inputv, outputv, targetv, costsv, sampleweight);
+    //if(is_missing(costsv[0]))
+    //    cout &lt;&lt; &quot;Re-What the fuck&quot; &lt;&lt; endl;
+
+}
+
+void NeuralProbabilisticLanguageModel::fpropOutput(const Vec&amp; inputv, 
+                                                   Vec&amp; outputv) const
+{
+    // Forward propagation until reaches output weights, sets last_layer
+    fpropBeforeOutputWeights(inputv);
+    
+    if(dist_rep_dim &gt; 0) // x -&gt; d(x)
+    {        
+        // d(x),h1(d(x)),h2(h1(d(x))) -&gt; o(x)
+
+        add_affine_transform(last_layer,wout,bout,outputv,false,
+                             possible_targets_vary,target_values);            
+        if(direct_in_to_out &amp;&amp; nhidden&gt;0)
+            add_affine_transform(nnet_input,direct_wout,direct_bout,
+                                 outputv,false,possible_targets_vary,
+                                 target_values);
+    }
+    else
+    {
+        // x, h1(x),h2(h1(x)) -&gt; o(x)
+        add_affine_transform(last_layer,wout,bout,outputv,nhidden&lt;=0,
+                             possible_targets_vary,target_values);            
+        if(direct_in_to_out &amp;&amp; nhidden&gt;0)
+            add_affine_transform(feat_input,direct_wout,direct_bout,
+                                 outputv,true,possible_targets_vary,
+                                 target_values);
+    }
+                               
+    if (nhidden2&gt;0 &amp;&amp; nhidden&lt;=0)
+        PLERROR(&quot;NeuralProbabilisticLanguageModel::fprop(): &quot;
+                &quot;can't have nhidden2 (=%d) &gt; 0 while nhidden=0&quot;,nhidden2);
+    
+    if(output_transfer_func!=&quot;&quot; &amp;&amp; output_transfer_func!=&quot;none&quot;)
+       add_transfer_func(outputv, output_transfer_func);
+}
+
+void NeuralProbabilisticLanguageModel::fpropBeforeOutputWeights(
+    const Vec&amp; inputv) const
+{
+    // Get possible target values
+    if(possible_targets_vary) 
+    {
+        row.subVec(0,inputsize_) &lt;&lt; inputv;
+        target_values_reference_set-&gt;getValues(row,inputsize_,target_values);
+        outputv.resize(target_values.length());
+    }
+
+    // Get features
+    ni = inputsize_;
+    nfeats = 0;
+    for(int i=0; i&lt;ni; i++)
+    {
+        str = val_string_reference_set-&gt;getValString(i,inputv[i]);
+        feat_sets[i%n_feat_sets]-&gt;getFeatures(str,feats[i]);
+        nfeats += feats[i].length();
+    }
+    
+    feat_input.resize(nfeats);
+    offset = 0;
+    id = 0;
+    for(int i=0; i&lt;ni; i++)
+    {
+        f = feats[i].data();
+        nj = feats[i].length();
+        for(int j=0; j&lt;nj; j++)
+            feat_input[id++] = offset + *f++;
+        if(dist_rep_dim &lt;= 0 || ((i+1) % n_feat_sets != 0))
+            offset += feat_sets[i % n_feat_sets]-&gt;size();
+        else
+            offset = 0;
+    }
+
+    // Fprop up to output weights
+    if(dist_rep_dim &gt; 0) // x -&gt; d(x)
+    {        
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i&lt;inputsize_;)
+        {
+            ifeats = 0;
+            for(int j=0; j&lt;n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            
+            add_affine_transform(feat_input.subVec(nfeats,ifeats),
+                                 wout_dist_rep, bout_dist_rep,
+                                 nnet_input.subVec(id*dist_rep_dim,dist_rep_dim),
+                                      true, false);
+            nfeats += ifeats;
+            id++;
+        }
+
+        if(nhidden&gt;0) // d(x) -&gt; h1(d(x))
+        {
+            add_affine_transform(nnet_input,w1,b1,hiddenv,false,false);
+            add_transfer_func(hiddenv);
+
+            if(nhidden2&gt;0) // h1(d(x)) -&gt; h2(h1(d(x)))
+            {
+                add_affine_transform(hiddenv,w2,b2,hidden2v,false,false);
+                add_transfer_func(hidden2v);
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = nnet_input;
+
+    }
+    else
+    {        
+        if(nhidden&gt;0) // x -&gt; h1(x)
+        {
+            add_affine_transform(feat_input,w1,b1,hiddenv,true,false);
+            // Transfert function
+            add_transfer_func(hiddenv);
+
+            if(nhidden2&gt;0) // h1(x) -&gt; h2(h1(x))
+            {
+                add_affine_transform(hiddenv,w2,b2,hidden2v,true,false);
+                add_transfer_func(hidden2v);
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = feat_input;
+    }
+}
+
+void NeuralProbabilisticLanguageModel::fpropCostsFromOutput(const Vec&amp; inputv, const Vec&amp; outputv, const Vec&amp; targetv, Vec&amp; costsv, real sampleweight) const
+{
+    //Compute cost
+
+    if(possible_targets_vary)
+    {
+        reind_target = target_values.find(targetv[0]);
+        if(reind_target&lt;0)
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::fprop(): target %d is not in possible targets&quot;, targetv[0]);
+    }
+    else
+        reind_target = (int)targetv[0];
+
+    // Build cost function
+
+    int ncosts = cost_funcs.size();
+    for(int k=0; k&lt;ncosts; k++)
+    {
+        if(cost_funcs[k]==&quot;NLL&quot;) 
+        {
+            costsv[k] = sampleweight*nll(outputv,reind_target);
+        }
+        else if(cost_funcs[k]==&quot;class_error&quot;)
+            costsv[k] = sampleweight*classification_loss(outputv, reind_target);
+        else 
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::fprop(): &quot;
+                    &quot;unknown cost_func option: %s&quot;,cost_funcs[k].c_str());        
+    }
+}
+
+void NeuralProbabilisticLanguageModel::bprop(Vec&amp; inputv, Vec&amp; outputv, 
+                                             Vec&amp; targetv, Vec&amp; costsv, 
+                                             real learning_rate, 
+                                             real sampleweight)
+{
+    if(possible_targets_vary) 
+    {
+        gradient_outputv.resize(target_values.length());
+        gradient_act_outputv.resize(target_values.length());
+        if(!stochastic_gradient_descent_speedup)
+            target_values_since_last_update.append(target_values);
+    }
+
+    if(!stochastic_gradient_descent_speedup)
+        feats_since_last_update.append(feat_input);
+
+    // Gradient through cost
+    if(cost_funcs[0]==&quot;NLL&quot;) 
+    {
+        // Permits to avoid numerical precision errors
+        if(output_transfer_func == &quot;softmax&quot;)
+            gradient_outputv[reind_target] = learning_rate*sampleweight;
+        else
+            gradient_outputv[reind_target] = learning_rate*sampleweight/(outputv[reind_target]);            
+    }
+    else if(cost_funcs[0]==&quot;class_error&quot;)
+    {
+        PLERROR(&quot;NeuralProbabilisticLanguageModel::bprop(): gradient &quot;
+                &quot;cannot be computed for \&quot;class_error\&quot; cost&quot;);
+    }
+
+    // Gradient through output transfer function
+    if(output_transfer_func != &quot;linear&quot;)
+    {
+        if(cost_funcs[0]==&quot;NLL&quot; &amp;&amp; output_transfer_func == &quot;softmax&quot;)
+            gradient_transfer_func(outputv,gradient_act_outputv, gradient_outputv,
+                                    output_transfer_func, reind_target);
+        else
+            gradient_transfer_func(outputv,gradient_act_outputv, gradient_outputv,
+                                    output_transfer_func);
+        gradient_last_layer = gradient_act_outputv;
+    }
+    else
+        gradient_last_layer = gradient_act_outputv;
+    
+    // Gradient through output affine transform
+
+
+    if(nhidden2 &gt; 0) {
+        gradient_affine_transform(hidden2v, wout, bout, gradient_hidden2v, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, possible_targets_vary, 
+                                  learning_rate*sampleweight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay,
+                                  target_values);
+    }
+    else if(nhidden &gt; 0) 
+    {
+        gradient_affine_transform(hiddenv, wout, bout, gradient_hiddenv,
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, possible_targets_vary, 
+                                  learning_rate*sampleweight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  target_values);
+    }
+    else
+    {
+        gradient_affine_transform(nnet_input, wout, bout, gradient_nnet_input, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  (dist_rep_dim &lt;= 0), possible_targets_vary, 
+                                  learning_rate*sampleweight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  target_values);
+    }
+
+
+    if(nhidden&gt;0 &amp;&amp; direct_in_to_out)
+    {
+        gradient_affine_transform(nnet_input, direct_wout, direct_bout,
+                                  gradient_nnet_input, 
+                                  gradient_direct_wout, gradient_direct_bout,
+                                  gradient_last_layer,
+                                  dist_rep_dim&lt;=0, possible_targets_vary,
+                                  learning_rate*sampleweight, 
+                                  weight_decay+direct_in_to_out_weight_decay,
+                                  0,
+                                  target_values);
+    }
+
+
+    if(nhidden2 &gt; 0)
+    {
+        gradient_transfer_func(hidden2v,gradient_act_hidden2v,gradient_hidden2v);
+        gradient_affine_transform(hiddenv, w2, b2, gradient_hiddenv, 
+                                  gradient_w2, gradient_b2, gradient_act_hidden2v,
+                                  false, false,learning_rate*sampleweight, 
+                                  weight_decay+layer2_weight_decay,
+                                  bias_decay+layer2_bias_decay);
+    }
+    if(nhidden &gt; 0)
+    {
+        gradient_transfer_func(hiddenv,gradient_act_hiddenv,gradient_hiddenv);  
+        gradient_affine_transform(nnet_input, w1, b1, gradient_nnet_input, 
+                                  gradient_w1, gradient_b1, gradient_act_hiddenv,
+                                  dist_rep_dim&lt;=0, false,learning_rate*sampleweight, 
+                                  weight_decay+layer1_weight_decay,
+                                  bias_decay+layer1_bias_decay);
+    }
+
+    if(dist_rep_dim &gt; 0)
+    {
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i&lt;inputsize_; )
+        {
+            ifeats = 0;
+            for(int j=0; j&lt;n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            gradient_affine_transform(feat_input.subVec(nfeats,ifeats),
+                                      wout_dist_rep, bout_dist_rep,
+                                      //gradient_feat_input.subVec(nfeats,feats[i].length()),
+                                      gradient_feat_input,// Useless anyways...
+                                      gradient_wout_dist_rep,
+                                      gradient_bout_dist_rep,
+                                      gradient_nnet_input.subVec(
+                                          id*dist_rep_dim,dist_rep_dim),
+                                      true, false, learning_rate*sampleweight, 
+                                      weight_decay+
+                                      output_layer_dist_rep_weight_decay,
+                                      bias_decay+output_layer_dist_rep_bias_decay);
+            nfeats += ifeats;
+            id++;
+        }
+    }
+
+    clearProppathGradient();
+}
+
+void NeuralProbabilisticLanguageModel::bpropBeforeOutputWeights(
+    real learning_rate, 
+    real sampleweight)
+{
+}
+
+
+void NeuralProbabilisticLanguageModel::update()
+{
+
+    if(dist_rep_dim &gt; 0)
+    {
+        update_affine_transform(feats_since_last_update, wout_dist_rep, 
+                                bout_dist_rep, gradient_wout_dist_rep,
+                                gradient_bout_dist_rep, true, false,
+                                target_values_since_last_update);
+    }
+
+    if(nhidden&gt;0) 
+    {
+        update_affine_transform(feats_since_last_update, w1, b1, 
+                                gradient_w1, gradient_b1,
+                                dist_rep_dim&lt;=0, false,
+                                target_values_since_last_update);
+        if(nhidden2&gt;0) 
+        {
+            update_affine_transform(feats_since_last_update, w2, b2, 
+                                    gradient_w2, gradient_b2,
+                                    false, false,
+                                    target_values_since_last_update);
+        }
+
+        update_affine_transform(feats_since_last_update, wout, bout, 
+                                gradient_wout, gradient_bout,
+                                false, possible_targets_vary,
+                                target_values_since_last_update);
+        if(direct_in_to_out)
+        {
+            update_affine_transform(feats_since_last_update, direct_wout, 
+                                    direct_bout, 
+                                    gradient_direct_wout, gradient_direct_bout,
+                                    false, possible_targets_vary,
+                                    target_values_since_last_update);
+        }
+    }
+    else
+    {
+        update_affine_transform(feats_since_last_update, wout, bout, 
+                                gradient_wout, gradient_bout,
+                                dist_rep_dim&lt;=0, possible_targets_vary,
+                                target_values_since_last_update);
+    }
+
+    feats_since_last_update.resize(0);
+    target_values_since_last_update.resize(0);
+}
+
+void NeuralProbabilisticLanguageModel::update_affine_transform(
+    Vec input, Mat weights, Vec bias,
+    Mat gweights, Vec gbias,
+    bool input_is_sparse, bool output_is_sparse,
+    Vec output_indices) 
+{
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output_indices.length();
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval2[(int)*pval3] += pval1[(int)*pval3];
+                pval1[(int)*pval3] = 0;
+                pval3++;
+            }
+        }
+        else
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            ni = bias.length();
+            for(int i=0; i&lt;ni; i++)
+            {
+                *pval2 += *pval1;
+                *pval1 = 0;
+                pval1++; 
+                pval2++;
+            }
+        }
+    }
+
+    // Weights
+    if(!input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        if(!gweights.isCompact() || !weights.isCompact())
+            PLERROR(&quot;In NeuralProbabilisticLanguageModel::&quot;
+                    &quot;update_affine_transform(): weights or gweights is&quot;
+                    &quot;not a compact TMat&quot;);
+        ni = weights.length();
+        nj = weights.width();
+        pval1 = gweights.data();
+        pval2 = weights.data();
+        for(int i=0; i&lt;ni; i++)
+            for(int j=0; j&lt;nj; j++)
+            {
+                *pval2 += *pval1;
+                *pval1 = 0;
+                pval1++;
+                pval2++;
+            }
+    }
+    else if(!input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        ni = output_indices.length();
+        nj = input.length();
+        pval3 = output_indices.data();
+        for(int i=0; i&lt;ni; i++)
+        {
+            for(int j=0; j&lt;nj; j++)
+            {
+                weights(j,(int)*pval3) += gweights(j,(int)*pval3);
+                gweights(j,(int)*pval3) = 0;
+            }
+            pval3++;
+        }
+    }
+    else if(input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        ni = input.length();
+        nj = weights.width();
+        pval3 = input.data();
+        for(int i=0; i&lt;ni; i++)
+        {
+            pval1 = gweights[(int)(*pval3)];
+            pval2 = weights[(int)(*pval3++)];
+            for(int j=0; j&lt;nj;j++)
+            {
+                *pval2 += *pval1;
+                *pval1 = 0;
+                pval1++;
+                pval2++;
+            }
+        }
+    }
+    else if(input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        // Weights
+        ni = input.length();
+        nj = output_indices.length();
+        pval2 = input.data();
+        for(int i=0; i&lt;ni; i++)
+        {
+            pval3 = output_indices.data();
+            for(int j=0; j&lt;nj; j++)
+            {
+                weights((int)(*pval2),(int)*pval3) += 
+                    gweights((int)(*pval2),(int)*pval3);
+                gweights((int)(*pval2),(int)*pval3) = 0;
+                pval3++;
+            }
+            pval2++;
+        }
+    }
+}
+
+//! Clear network's gradient fields
+void NeuralProbabilisticLanguageModel::clearProppathGradient()
+{
+    // Trick to make clearProppathGradient faster...
+    if(cost_funcs[0]==&quot;NLL&quot;) 
+        gradient_outputv[reind_target] = 0;
+    else
+        gradient_outputv.clear();
+    gradient_act_outputv.clear();
+    
+    if(dist_rep_dim&gt;0)
+        gradient_nnet_input.clear();
+
+    if(nhidden&gt;0) 
+    {
+        gradient_hiddenv.clear();
+        gradient_act_hiddenv.clear();
+        if(nhidden2&gt;0) 
+        {
+            gradient_hidden2v.clear();
+            gradient_act_hidden2v.clear();
+        }
+    }
+}
+
+
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
+void NeuralProbabilisticLanguageModel::computeCostsFromOutputs(const Vec&amp; inputv, 
+                                                               const Vec&amp; outputv,
+                                                               const Vec&amp; targetv,
+                                                               Vec&amp; costsv) const
+{
+    PLERROR(&quot;In NeuralProbabilisticLanguageModel::computeCostsFromOutputs():&quot;
+            &quot;output is not enough to compute costs&quot;);
+}
+
+int NeuralProbabilisticLanguageModel::my_argmax(const Vec&amp; vec, 
+                                                int default_compare) const
+{
+#ifdef BOUNDCHECK
+    if(vec.length()==0)
+        PLERROR(&quot;IN int argmax(const TVec&lt;T&gt;&amp; vec) vec has zero length&quot;);
+#endif
+    real* v = vec.data();
+    int indexmax = default_compare;
+    real maxval = v[default_compare];
+    for(int i=0; i&lt;vec.length(); i++)
+        if(v[i]&gt;maxval)
+        {
+            maxval = v[i];
+            indexmax = i;
+        }
+    return indexmax;
+}
+
+///////////////////
+// computeOutput //
+///////////////////
+void NeuralProbabilisticLanguageModel::computeOutput(const Vec&amp; inputv, 
+                                                     Vec&amp; outputv) const
+{
+    fpropOutput(inputv, output_comp);
+    if(possible_targets_vary)
+    {
+        //row.subVec(0,inputsize_) &lt;&lt; inputv;
+        //target_values_reference_set-&gt;getValues(row,inputsize_,target_values);
+        outputv[0] = target_values[
+            my_argmax(output_comp,rgen-&gt;uniform_multinomial_sample(
+                          output_comp.length()))];
+    }
+    else
+        outputv[0] = argmax(output_comp);
+}
+
+///////////////////////////
+// computeOutputAndCosts //
+///////////////////////////
+void NeuralProbabilisticLanguageModel::computeOutputAndCosts(const Vec&amp; inputv, 
+                                                             const Vec&amp; targetv, 
+                                                             Vec&amp; outputv, 
+                                                             Vec&amp; costsv) const
+{
+    fprop(inputv,output_comp,targetv,costsv);
+    if(possible_targets_vary)
+    {
+        //row.subVec(0,inputsize_) &lt;&lt; inputv;
+        //target_values_reference_set-&gt;getValues(row,inputsize_,target_values);
+        outputv[0] = 
+            target_values[
+                my_argmax(output_comp,rgen-&gt;uniform_multinomial_sample(
+                              output_comp.length()))];
+    }
+    else
+        outputv[0] = argmax(output_comp);
+}
+
+/////////////////
+// fillWeights //
+/////////////////
+void NeuralProbabilisticLanguageModel::fillWeights(const Mat&amp; weights) {
+    if (initialization_method == &quot;zero&quot;) {
+        weights.clear();
+        return;
+    }
+    real delta;
+    int is = weights.length();
+    if (initialization_method.find(&quot;linear&quot;) != string::npos)
+        delta = 1.0 / real(is);
+    else
+        delta = 1.0 / sqrt(real(is));
+    if (initialization_method.find(&quot;normal&quot;) != string::npos)
+        rgen-&gt;fill_random_normal(weights, 0, delta);
+    else
+        rgen-&gt;fill_random_uniform(weights, -delta, delta);
+}
+
+////////////
+// forget //
+////////////
+void NeuralProbabilisticLanguageModel::forget()
+{
+    if (train_set) build();
+    total_updates=0;
+    stage = 0;
+}
+
+///////////////////////
+// getTrainCostNames //
+///////////////////////
+TVec&lt;string&gt; NeuralProbabilisticLanguageModel::getTrainCostNames() const
+{
+    return cost_funcs;
+}
+
+//////////////////////
+// getTestCostNames //
+//////////////////////
+TVec&lt;string&gt; NeuralProbabilisticLanguageModel::getTestCostNames() const
+{ 
+    return cost_funcs;
+}
+
+///////////////////////
+// add_transfer_func //
+///////////////////////
+void NeuralProbabilisticLanguageModel::add_transfer_func(const Vec&amp; input, 
+                                                         string transfer_func) 
+    const
+{
+    if (transfer_func == &quot;default&quot;)
+        transfer_func = hidden_transfer_func;
+    if(transfer_func==&quot;linear&quot;)
+        return;
+    else if(transfer_func==&quot;tanh&quot;)
+    {
+        compute_tanh(input,input);
+        return;
+    }        
+    else if(transfer_func==&quot;sigmoid&quot;)
+    {
+        compute_sigmoid(input,input);
+        return;
+    }
+    else if(transfer_func==&quot;softmax&quot;)
+    {
+        compute_softmax(input,input);
+        return;
+    }
+    else PLERROR(&quot;In NeuralProbabilisticLanguageModel::add_transfer_func(): &quot;
+                 &quot;Unknown value for transfer_func: %s&quot;,transfer_func.c_str());
+}
+
+////////////////////////////
+// gradient_transfer_func //
+////////////////////////////
+void NeuralProbabilisticLanguageModel::gradient_transfer_func(
+    Vec&amp; output, 
+    Vec&amp; gradient_input,
+    Vec&amp; gradient_output,
+    string transfer_func,
+    int nll_softmax_speed_up_target) 
+{
+    if (transfer_func == &quot;default&quot;)        
+        transfer_func = hidden_transfer_func;
+    if(transfer_func==&quot;linear&quot;)
+    {
+        pval1 = gradient_output.data();
+        pval2 = gradient_input.data();
+        ni = output.length();
+        for(int i=0; i&lt;ni; i++)
+            *pval2++ += *pval1++;
+        return;
+    }
+    else if(transfer_func==&quot;tanh&quot;)
+    {
+        pval1 = gradient_output.data();
+        pval2 = output.data();
+        pval3 = gradient_input.data();
+        ni = output.length();
+        for(int i=0; i&lt;ni; i++)
+            *pval3++ += (*pval1++)*(1.0-square(*pval2++));
+        return;
+    }        
+    else if(transfer_func==&quot;sigmoid&quot;)
+    {
+        pval1 = gradient_output.data();
+        pval2 = output.data();
+        pval3 = gradient_input.data();
+        ni = output.length();
+        for(int i=0; i&lt;ni; i++)
+        {
+            *pval3++ += (*pval1++)*(*pval2)*(1.0-*pval2);
+            pval2++;
+        }   
+        return;
+    }
+    else if(transfer_func==&quot;softmax&quot;)
+    {
+        if(nll_softmax_speed_up_target&lt;0)
+        {            
+            pval3 = gradient_input.data();
+            ni = nk = output.length();
+            for(int i=0; i&lt;ni; i++)
+            {
+                val = output[i];
+                pval1 = gradient_output.data();
+                pval2 = output.data();
+                for(int k=0; k&lt;nk; k++)
+                    if(k!=i)
+                        *pval3 -= *pval1++ * val * (*pval2++);
+                    else
+                    {
+                        *pval3 += *pval1++ * val * (1.0-val);
+                        pval2++;
+                    }
+                pval3++;                
+            }   
+        }
+        else // Permits speedup and avoids numerical precision errors
+        {
+            pval2 = output.data();
+            pval3 = gradient_input.data();
+            ni = output.length();
+            grad = gradient_output[nll_softmax_speed_up_target];
+            val = output[nll_softmax_speed_up_target];
+            for(int i=0; i&lt;ni; i++)
+            {
+                if(nll_softmax_speed_up_target!=i)
+                    //*pval3++ -= grad * val * (*pval2++);
+                    *pval3++ -= grad * (*pval2++);
+                else
+                {
+                    //*pval3++ += grad * val * (1.0-val);
+                    *pval3++ += grad * (1.0-val);
+                    pval2++;
+                }
+            }   
+        }
+        return;
+    }
+    else PLERROR(&quot;In NeuralProbabilisticLanguageModel::gradient_transfer_func():&quot;
+                 &quot;Unknown value for transfer_func: %s&quot;,transfer_func.c_str());
+}
+
+void NeuralProbabilisticLanguageModel::add_affine_transform(
+    Vec input, 
+    Mat weights, 
+    Vec bias, Vec output, 
+    bool input_is_sparse, bool output_is_sparse,
+    Vec output_indices) const
+{
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = output.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output.length();
+            for(int i=0; i&lt;ni; i++)
+                *pval1++ = pval2[(int)*pval3++];
+        }
+        else
+        {
+            pval1 = output.data();
+            pval2 = bias.data();
+            ni = output.length();
+            for(int i=0; i&lt;ni; i++)
+                *pval1++ = *pval2++;
+        }
+    }
+
+    // Weights
+    if(!input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        transposeProductAcc(output,weights,input);
+    }
+    else if(!input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        ni = output.length();
+        nj = input.length();
+        pval1 = output.data();
+        pval3 = output_indices.data();
+        for(int i=0; i&lt;ni; i++)
+        {
+            pval2 = input.data();
+            for(int j=0; j&lt;nj; j++)
+                *pval1 += (*pval2++)*weights(j,(int)*pval3);
+            pval1++;
+            pval3++;
+        }
+    }
+    else if(input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        ni = input.length();
+        nj = output.length();
+        if(ni != 0)
+        {
+            pval3 = input.data();
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval1 = output.data();
+                pval2 = weights[(int)(*pval3++)];
+                for(int j=0; j&lt;nj;j++)
+                    *pval1++ += *pval2++;
+            }
+        }
+    }
+    else if(input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        // Weights
+        ni = input.length();
+        nj = output.length();
+        if(ni != 0)
+        {
+            pval2 = input.data();
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval1 = output.data();
+                pval3 = output_indices.data();
+                for(int j=0; j&lt;nj; j++)
+                    *pval1++ += weights((int)(*pval2),(int)*pval3++);
+                pval2++;
+            }
+        }
+    }
+}
+
+void NeuralProbabilisticLanguageModel::gradient_affine_transform(
+    Vec input, Mat weights, Vec bias, 
+    Vec ginput, Mat gweights, Vec gbias,
+    Vec goutput, bool input_is_sparse, 
+    bool output_is_sparse,
+    real learning_rate,
+    real weight_decay, real bias_decay,
+    Vec output_indices)
+{
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = gbias.data();
+            pval2 = goutput.data();
+            pval3 = output_indices.data();
+            ni = goutput.length();
+            
+            if(fast_exact_is_equal(bias_decay, 0))
+            {
+                // Without bias decay
+                for(int i=0; i&lt;ni; i++)
+                    pval1[(int)*pval3++] += *pval2++;
+            }
+            else
+            {
+                // With bias decay
+                if(penalty_type == &quot;L2_square&quot;)
+                {
+                    pval4 = bias.data();
+                    val = -two(learning_rate)*bias_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1[(int)*pval3] += *pval2++ + val*(pval4[(int)*pval3]);
+                        pval3++;
+                    }
+                }
+                else if(penalty_type == &quot;L1&quot;)
+                {
+                    pval4 = bias.data();
+                    val = -learning_rate*bias_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        val2 = pval4[(int)*pval3];
+                        if(val2 &gt; 0 )
+                            pval1[(int)*pval3] += *pval2 + val;
+                        else if(val2 &lt; 0)
+                            pval1[(int)*pval3] += *pval2 - val;
+                        pval2++;
+                        pval3++;
+                    }
+                }
+            }
+        }
+        else
+        {
+            pval1 = gbias.data();
+            pval2 = goutput.data();
+            ni = goutput.length();
+            if(fast_exact_is_equal(bias_decay, 0))
+            {
+                // Without bias decay
+                for(int i=0; i&lt;ni; i++)
+                    *pval1++ += *pval2++;
+            }
+            else
+            {
+                // With bias decay
+                if(penalty_type == &quot;L2_square&quot;)
+                {
+                    pval3 = bias.data();
+                    val = -two(learning_rate)*bias_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        *pval1++ += *pval2++ + val * (*pval3++);
+                    }
+                }
+                else if(penalty_type == &quot;L1&quot;)
+                {
+                    pval3 = bias.data();
+                    val = -learning_rate*bias_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        if(*pval3 &gt; 0)
+                            *pval1 += *pval2 + val;
+                        else if(*pval3 &lt; 0)
+                            *pval1 += *pval2 - val;
+                        pval1++;
+                        pval2++;
+                        pval3++;
+                    }
+                }
+            }
+        }
+    }
+
+    // Weights and input (when appropriate)
+    if(!input_is_sparse &amp;&amp; !output_is_sparse)
+    {        
+        // Input
+        //productAcc(ginput, weights, goutput);
+        // Weights
+        //externalProductAcc(gweights, input, goutput);
+
+        // Faster code to do this, which limits the accesses
+        // to memory
+
+        ni = input.length();
+        nj = goutput.length();
+        pval3 = ginput.data();
+        pval5 = input.data();
+        
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            for(int i=0; i&lt;ni; i++) {
+                
+                pval1 = goutput.data();
+                pval2 = weights[i];
+                pval4 = gweights[i];
+                for(int j=0; j&lt;nj; j++) {
+                    *pval3 += *pval2 * (*pval1);
+                    *pval4 += *pval5 * (*pval1);
+                    pval1++;
+                    pval2++;
+                    pval4++;
+                }
+                pval3++;
+                pval5++;
+            }   
+        }
+        else
+        {
+            //With weight decay            
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                val = -two(learning_rate)*weight_decay;
+                for(int i=0; i&lt;ni; i++) {   
+                    pval1 = goutput.data();
+                    pval2 = weights[i];
+                    pval4 = gweights[i];
+                    for(int j=0; j&lt;nj; j++) {
+                        *pval3 += *pval2 * (*pval1);
+                        *pval4 += *pval5 * (*pval1) + val * (*pval2);
+                        pval1++;
+                        pval2++;
+                        pval4++;
+                    }
+                    pval3++;
+                    pval5++;
+                }
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                val = -learning_rate*weight_decay;
+                for(int i=0; i&lt;ni; i++) {
+                    
+                    pval1 = goutput.data();
+                    pval2 = weights[i];
+                    pval4 = gweights[i];
+                    for(int j=0; j&lt;nj; j++) {
+                        *pval3 += *pval2 * (*pval1);
+                        if(*pval2 &gt; 0)
+                            *pval4 += *pval5 * (*pval1) + val;
+                        else if(*pval2 &lt; 0)
+                            *pval4 += *pval5 * (*pval1) - val;
+                        pval1++;
+                        pval2++;
+                        pval4++;
+                    }
+                    pval3++;
+                    pval5++;
+                }
+            }
+        }
+    }
+    else if(!input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        ni = goutput.length();
+        nj = input.length();
+        pval1 = goutput.data();
+        pval3 = output_indices.data();
+        
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval2 = input.data();
+                pval4 = ginput.data();
+                for(int j=0; j&lt;nj; j++)
+                {
+                    // Input
+                    *pval4++ += weights(j,(int)(*pval3))*(*pval1);
+                    // Weights
+                    gweights(j,(int)(*pval3)) += (*pval2++)*(*pval1);
+                }
+                pval1++;
+                pval3++;
+            }
+        }
+        else
+        {
+            // With weight decay
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                val = -two(learning_rate)*weight_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    pval2 = input.data();
+                    pval4 = ginput.data();
+                    for(int j=0; j&lt;nj; j++)
+                    {
+                        val2 = weights(j,(int)(*pval3));
+                        // Input
+                        *pval4++ += val2*(*pval1);
+                        // Weights
+                        gweights(j,(int)(*pval3)) += (*pval2++)*(*pval1) 
+                            + val*val2;
+                    }
+                    pval1++;
+                    pval3++;
+                }
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                val = -learning_rate*weight_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    pval2 = input.data();
+                    pval4 = ginput.data();
+                    for(int j=0; j&lt;nj; j++)
+                    {
+                        val2 = weights(j,(int)(*pval3));
+                        // Input
+                        *pval4++ += val2*(*pval1);
+                        // Weights
+                        if(val2 &gt; 0)
+                            gweights(j,(int)(*pval3)) += (*pval2)*(*pval1) + val;
+                        else if(val2 &lt; 0)
+                            gweights(j,(int)(*pval3)) += (*pval2)*(*pval1) - val;
+                        pval2++;
+                    }
+                    pval1++;
+                    pval3++;
+                }
+            }
+        }
+    }
+    else if(input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        ni = input.length();
+        nj = goutput.length();
+
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            if(ni != 0)
+            {
+                pval3 = input.data();
+                for(int i=0; i&lt;ni; i++)
+                {
+                    pval1 = goutput.data();
+                    pval2 = gweights[(int)(*pval3++)];
+                    for(int j=0; j&lt;nj;j++)
+                        *pval2++ += *pval1++;
+                }
+            }
+        }
+        else
+        {
+            // With weight decay
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                if(ni != 0)
+                {
+                    pval3 = input.data();                    
+                    val = -two(learning_rate)*weight_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval2 = gweights[(int)(*pval3)];
+                        pval4 = weights[(int)(*pval3++)];
+                        for(int j=0; j&lt;nj;j++)
+                        {
+                            *pval2++ += *pval1++ + val * (*pval4++);
+                        }
+                    }
+                }
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                if(ni != 0)
+                {
+                    pval3 = input.data();
+                    val = learning_rate*weight_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval2 = gweights[(int)(*pval3)];
+                        pval4 = weights[(int)(*pval3++)];
+                        for(int j=0; j&lt;nj;j++)
+                        {
+                            if(*pval4 &gt; 0)
+                                *pval2 += *pval1 + val;
+                            else if(*pval4 &lt; 0)
+                                *pval2 += *pval1 - val;
+                            pval1++;
+                            pval2++;
+                            pval4++;
+                        }
+                    }
+                }
+            }
+        }
+    }
+    else if(input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        ni = input.length();
+        nj = goutput.length();
+
+        if(fast_exact_is_equal(weight_decay, 0))
+        {
+            // Without weight decay
+            if(ni != 0)
+            {
+                pval2 = input.data();
+                for(int i=0; i&lt;ni; i++)
+                {
+                    pval1 = goutput.data();
+                    pval3 = output_indices.data();
+                    for(int j=0; j&lt;nj; j++)
+                        gweights((int)(*pval2),(int)*pval3++) += *pval1++;
+                    pval2++;
+                }
+            }
+        }
+        else
+        {
+            // With weight decay
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                if(ni != 0)
+                {
+                    pval2 = input.data();
+                    val = -two(learning_rate)*weight_decay;                    
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval3 = output_indices.data();
+                        for(int j=0; j&lt;nj; j++)
+                        {
+                            gweights((int)(*pval2),(int)*pval3) 
+                                += *pval1++ 
+                                + val * weights((int)(*pval2),(int)*pval3);
+                            pval3++;
+                        }
+                        pval2++;
+                    }
+                }
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                if(ni != 0)
+                {
+                    pval2 = input.data();
+                    val = -learning_rate*weight_decay;                    
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1 = goutput.data();
+                        pval3 = output_indices.data();
+                        for(int j=0; j&lt;nj; j++)
+                        {
+                            val2 = weights((int)(*pval2),(int)*pval3);
+                            if(val2 &gt; 0)
+                                gweights((int)(*pval2),(int)*pval3) 
+                                    += *pval1 + val;
+                            else if(val2 &lt; 0)
+                                gweights((int)(*pval2),(int)*pval3) 
+                                    += *pval1 - val;
+                            pval1++;
+                            pval3++;
+                        }
+                        pval2++;
+                    }
+                }
+            }
+        }
+    }
+
+//    gradient_penalty(input,weights,bias,gweights,gbias,input_is_sparse,output_is_sparse,
+//                     learning_rate,weight_decay,bias_decay,output_indices);
+}
+
+void NeuralProbabilisticLanguageModel::gradient_penalty(
+    Vec input, Mat weights, Vec bias, 
+    Mat gweights, Vec gbias,
+    bool input_is_sparse, bool output_is_sparse,
+    real learning_rate,
+    real weight_decay, real bias_decay,
+    Vec output_indices)
+{
+    // Bias
+    if(!fast_exact_is_equal(bias_decay, 0) &amp;&amp; !fast_exact_is_equal(bias.length(),
+                                                                   0) )
+    {
+        if(output_is_sparse)
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output_indices.length();            
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                val = -two(learning_rate)*bias_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    pval1[(int)*pval3] += val*(pval2[(int)*pval3]);
+                    pval3++;
+                }
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                val = -learning_rate*bias_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    val2 = pval2[(int)*pval3];
+                    if(val2 &gt; 0 )
+                        pval1[(int)*pval3++] += val;
+                    else if(val2 &lt; 0)
+                        pval1[(int)*pval3++] -= val;
+                }
+            }
+        }
+        else
+        {
+            pval1 = gbias.data();
+            pval2 = bias.data();
+            ni = output_indices.length();            
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                val = -two(learning_rate)*bias_decay;
+                for(int i=0; i&lt;ni; i++)
+                    *pval1++ += val*(*pval2++);
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                val = -learning_rate*bias_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    if(*pval2 &gt; 0)
+                        *pval1 += val;
+                    else if(*pval2 &lt; 0)
+                        *pval1 -= val;
+                    pval1++;
+                    pval2++;
+                }
+            }
+        }
+    }
+
+    // Weights
+    if(!fast_exact_is_equal(weight_decay, 0))
+    {
+        if(!input_is_sparse &amp;&amp; !output_is_sparse)
+        {      
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                multiplyAcc(gweights, weights,-two(learning_rate)*weight_decay);
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                val = -learning_rate*weight_decay;
+                if(gweights.isCompact() &amp;&amp; weights.isCompact())
+                {
+                    Mat::compact_iterator itm = gweights.compact_begin();
+                    Mat::compact_iterator itmend = gweights.compact_end();
+                    Mat::compact_iterator itx = weights.compact_begin();
+                    for(; itm!=itmend; ++itm, ++itx)
+                    {
+                        if(*itx &gt; 0)
+                            *itm += val;
+                        else if(*itx &lt; 0)
+                            *itm -= val;
+                    }
+                }
+                else // use non-compact iterators
+                {
+                    Mat::iterator itm = gweights.begin();
+                    Mat::iterator itmend = gweights.end();
+                    Mat::iterator itx = weights.begin();
+                    for(; itm!=itmend; ++itm, ++itx)
+                    {
+                        if(*itx &gt; 0)
+                            *itm += val;
+                        else if(*itx &lt; 0)
+                            *itm -= val;
+                    }
+                }
+            }
+        }
+        else if(!input_is_sparse &amp;&amp; output_is_sparse)
+        {
+            ni = output_indices.length();
+            nj = input.length();
+            pval1 = output_indices.data();
+
+            if(penalty_type == &quot;L2_square&quot;)
+            {
+                val = -two(learning_rate)*weight_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    for(int j=0; j&lt;nj; j++)
+                    {
+                        gweights(j,(int)(*pval1)) += val * 
+                            weights(j,(int)(*pval1));
+                    }
+                    pval1++;
+                }
+            }
+            else if(penalty_type == &quot;L1&quot;)
+            {
+                val = -learning_rate*weight_decay;
+                for(int i=0; i&lt;ni; i++)
+                {
+                    for(int j=0; j&lt;nj; j++)
+                    {
+                        val2 = weights(j,(int)(*pval1));
+                        if(val2 &gt; 0)
+                            gweights(j,(int)(*pval1)) +=  val;
+                        else if(val2 &lt; 0)
+                            gweights(j,(int)(*pval1)) -=  val;
+                    }
+                    pval1++;
+                }
+            }
+        }
+        else if(input_is_sparse &amp;&amp; !output_is_sparse)
+        {
+            ni = input.length();
+            nj = output_indices.length();
+            if(ni != 0)
+            {
+                pval3 = input.data();
+                if(penalty_type == &quot;L2_square&quot;)
+                {
+                    val = -two(learning_rate)*weight_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1 = weights[(int)(*pval3)];
+                        pval2 = gweights[(int)(*pval3++)];
+                        for(int j=0; j&lt;nj;j++)
+                            *pval2++ += val * *pval1++;
+                    }
+                }
+                else if(penalty_type == &quot;L1&quot;)
+                {
+                    val = -learning_rate*weight_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval1 = weights[(int)(*pval3)];
+                        pval2 = gweights[(int)(*pval3++)];
+                        for(int j=0; j&lt;nj;j++)
+                        {
+                            if(*pval1 &gt; 0)
+                                *pval2 += val;
+                            else if(*pval1 &lt; 0)
+                                *pval2 -= val;
+                            pval2++;
+                            pval1++;
+                        }
+                    }                
+                }
+            }
+        }
+        else if(input_is_sparse &amp;&amp; output_is_sparse)
+        {
+            ni = input.length();
+            nj = output_indices.length();
+            if(ni != 0)
+            {
+                pval1 = input.data();
+                if(penalty_type == &quot;L2_square&quot;)
+                {
+                    val = -two(learning_rate)*weight_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval2 = output_indices.data();
+                        for(int j=0; j&lt;nj; j++)
+                        {
+                            gweights((int)(*pval1),(int)*pval2) += val*
+                                weights((int)(*pval1),(int)*pval2);
+                        pval2++;
+                        }
+                        pval1++;
+                    }
+                }
+                else if(penalty_type == &quot;L1&quot;)
+                {
+                    val = -learning_rate*weight_decay;
+                    for(int i=0; i&lt;ni; i++)
+                    {
+                        pval2 = output_indices.data();
+                        for(int j=0; j&lt;nj; j++)
+                        {
+                            val2 = weights((int)(*pval1),(int)*pval2);
+                            if(val2 &gt; 0)
+                                gweights((int)(*pval1),(int)*pval2) += val;
+                            else if(val2 &lt; 0)
+                                gweights((int)(*pval1),(int)*pval2) -= val;
+                            pval2++;
+                        }
+                        pval1++;
+                    }
+                    
+                }
+            }
+        }
+    }
+}
+
+void NeuralProbabilisticLanguageModel::importance_sampling_gradient_update(
+    Vec&amp; inputv, Vec&amp; targetv, 
+    real learning_rate, int n_samples, 
+    real train_sample_weight=1)
+{
+    // TODO: implement NGramDistribution::generate()
+    //       adjust deepcopy(...)
+
+    // Do forward propagation that is common to all computations
+    fpropBeforeOutputWeights(inputv);
+
+    // Generate the n_samples samples from proposal_distribution
+    generated_samples.resize(n_samples+1);
+    densities.resize(n_samples);
+    
+    proposal_distribution-&gt;setPredictor(inputv);
+    pval1 = generated_samples.data();
+    pval2 = sample.data();
+    pval3 = densities.data();
+    for(int i=0; i&lt;n_samples; i++)
+    {
+        proposal_distribution-&gt;generate(sample);        
+        *pval1++ = *pval2;
+        *pval3++ = proposal_distribution-&gt;density(sample);        
+    }
+
+    real sum = 0;
+    generated_samples[n_samples] = targetv[0];
+    neg_energies.resize(n_samples+1);
+    getNegativeEnergyValues(generated_samples, neg_energies);
+    
+    importance_sampling_ratios.resize(
+        importance_sampling_ratios.length() + n_samples);
+    pval1 = importance_sampling_ratios.subVec(
+        importance_sampling_ratios.length() - n_samples).data();
+    pval2 = neg_energies.data();
+    pval3 = densities.data();
+    for(int i=0; i&lt;n_samples; i++)
+    {
+        *pval1 = exp(*pval2++)/ (*pval3++);
+        sum += *pval1;
+    }
+
+    // Compute importance sampling estimate of the gradient
+
+    // Training sample contribution...
+    gradient_last_layer.resize(1);
+    gradient_last_layer[0] = learning_rate*train_sample_weight;
+
+    if(nhidden2 &gt; 0) {
+        gradient_affine_transform(hidden2v, wout, bout, gradient_hidden2v, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, true, learning_rate*train_sample_weight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay,
+                                  generated_samples.subVec(n_samples,1));
+    }
+    else if(nhidden &gt; 0) 
+    {
+        gradient_affine_transform(hiddenv, wout, bout, gradient_hiddenv,
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  false, true, learning_rate*train_sample_weight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  generated_samples.subVec(n_samples,1));
+    }
+    else
+    {
+        gradient_affine_transform(nnet_input, wout, bout, gradient_nnet_input, 
+                                  gradient_wout, gradient_bout, 
+                                  gradient_last_layer,
+                                  (dist_rep_dim &lt;= 0), true, 
+                                  learning_rate*train_sample_weight, 
+                                  weight_decay+output_layer_weight_decay,
+                                  bias_decay+output_layer_bias_decay, 
+                                  generated_samples.subVec(n_samples,1));
+    }
+
+
+    if(nhidden&gt;0 &amp;&amp; direct_in_to_out)
+    {
+        gradient_affine_transform(nnet_input, direct_wout, direct_bout,
+                                  gradient_nnet_input, 
+                                  gradient_direct_wout, gradient_direct_bout,
+                                  gradient_last_layer,
+                                  dist_rep_dim&lt;=0, true,
+                                  learning_rate*train_sample_weight, 
+                                  weight_decay+direct_in_to_out_weight_decay,
+                                  0,
+                                  generated_samples.subVec(n_samples,1));
+    }
+
+    // Importance sampling contributions
+    for(int i=0; i&lt;n_samples; i++)
+    {
+        gradient_last_layer.resize(1);
+        gradient_last_layer[0] = -learning_rate*train_sample_weight*
+            importance_sampling_ratios[i]/sum;
+
+        if(nhidden2 &gt; 0) {
+            gradient_affine_transform(hidden2v, wout, bout, gradient_hidden2v, 
+                                      gradient_wout, gradient_bout, 
+                                      gradient_last_layer,
+                                      false, true, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+output_layer_weight_decay,
+                                      bias_decay+output_layer_bias_decay,
+                                      generated_samples.subVec(i,1));
+        }
+        else if(nhidden &gt; 0) 
+        {
+            gradient_affine_transform(hiddenv, wout, bout, gradient_hiddenv,
+                                      gradient_wout, gradient_bout, 
+                                      gradient_last_layer,
+                                      false, true, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+output_layer_weight_decay,
+                                      bias_decay+output_layer_bias_decay, 
+                                      generated_samples.subVec(i,1));
+        }
+        else
+        {
+            gradient_affine_transform(nnet_input, wout, bout, 
+                                      gradient_nnet_input, 
+                                      gradient_wout, gradient_bout, 
+                                      gradient_last_layer,
+                                      (dist_rep_dim &lt;= 0), true, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+output_layer_weight_decay,
+                                      bias_decay+output_layer_bias_decay, 
+                                      generated_samples.subVec(i,1));
+        }
+
+
+        if(nhidden&gt;0 &amp;&amp; direct_in_to_out)
+        {
+            gradient_affine_transform(nnet_input, direct_wout, direct_bout,
+                                      gradient_nnet_input, 
+                                      gradient_direct_wout, gradient_direct_bout,
+                                      gradient_last_layer,
+                                      dist_rep_dim&lt;=0, true,
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+direct_in_to_out_weight_decay,
+                                      0,
+                                      generated_samples.subVec(i,1));
+        }
+
+    }
+
+    // Propagate all contributions through rest of the network
+
+    if(nhidden2 &gt; 0)
+    {
+        gradient_transfer_func(hidden2v,gradient_act_hidden2v,gradient_hidden2v);
+        gradient_affine_transform(hiddenv, w2, b2, gradient_hiddenv, 
+                                  gradient_w2, gradient_b2, gradient_act_hidden2v,
+                                  false, false,learning_rate*train_sample_weight, 
+                                  weight_decay+layer2_weight_decay,
+                                  bias_decay+layer2_bias_decay);
+    }
+    if(nhidden &gt; 0)
+    {
+        gradient_transfer_func(hiddenv,gradient_act_hiddenv,gradient_hiddenv);  
+        gradient_affine_transform(nnet_input, w1, b1, gradient_nnet_input, 
+                                  gradient_w1, gradient_b1, gradient_act_hiddenv,
+                                  dist_rep_dim&lt;=0, false,learning_rate*train_sample_weight, 
+                                  weight_decay+layer1_weight_decay,
+                                  bias_decay+layer1_bias_decay);
+    }
+
+    if(dist_rep_dim &gt; 0)
+    {
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i&lt;inputsize_; )
+        {
+            ifeats = 0;
+            for(int j=0; j&lt;n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            gradient_affine_transform(feat_input.subVec(nfeats,ifeats),
+                                      wout_dist_rep, bout_dist_rep,
+                                      gradient_feat_input,// Useless anyways...
+                                      gradient_wout_dist_rep,
+                                      gradient_bout_dist_rep,
+                                      gradient_nnet_input.subVec(
+                                          id*dist_rep_dim,dist_rep_dim),
+                                      true, false, 
+                                      learning_rate*train_sample_weight, 
+                                      weight_decay+
+                                      output_layer_dist_rep_weight_decay,
+                                      bias_decay
+                                      +output_layer_dist_rep_bias_decay);
+            nfeats += ifeats;
+            id++;
+        }
+    }
+    clearProppathGradient();
+
+    // Update parameters and clear gradient
+    if(!stochastic_gradient_descent_speedup)
+        update();
+}
+
+void NeuralProbabilisticLanguageModel::getNegativeEnergyValues(
+    Vec samples, Vec neg_energies)
+{
+    if(dist_rep_dim &gt; 0) // x -&gt; d(x)
+    {        
+        // d(x),h1(d(x)),h2(h1(d(x))) -&gt; o(x)
+
+        add_affine_transform(last_layer,wout,bout,neg_energies,false,
+                             true,samples);            
+        if(direct_in_to_out &amp;&amp; nhidden&gt;0)
+            add_affine_transform(nnet_input,direct_wout,direct_bout,
+                                 neg_energies,false,true,
+                                 samples);
+    }
+    else
+    {
+        // x, h1(x),h2(h1(x)) -&gt; o(x)
+        add_affine_transform(last_layer,wout,bout,samples,nhidden&lt;=0,
+                             true,samples);            
+        if(direct_in_to_out &amp;&amp; nhidden&gt;0)
+            add_affine_transform(feat_input,direct_wout,direct_bout,
+                                 neg_energies,true,true,
+                                 samples);
+    }
+}
+
+void NeuralProbabilisticLanguageModel::compute_softmax(const Vec&amp; x, 
+                                                       const Vec&amp; y) const
+{
+    int n = x.length();
+    
+//    real* yp = y.data();
+//    real* xp = x.data();
+//    for(int i=0; i&lt;n; i++)
+//    {
+//        *yp++ = *xp &gt; 1e-5 ? *xp : 1e-5;
+//        xp++;
+//    }
+
+    if (n&gt;0)
+    {
+        real* yp = y.data();
+        real* xp = x.data();
+        real maxx = max(x);
+        real s = 0;
+        for (int i=0;i&lt;n;i++)
+            s += (*yp++ = safeexp(*xp++-maxx));
+        if (s == 0) PLERROR(&quot;trying to divide by 0 in softmax&quot;);
+        s = 1.0 / s;
+        yp = y.data();
+        for (int i=0;i&lt;n;i++)
+            *yp++ *= s;
+    }
+}
+
+real NeuralProbabilisticLanguageModel::nll(const Vec&amp; outputv, int target) const
+{
+    return -safeflog(outputv[target]);
+}
+    
+real NeuralProbabilisticLanguageModel::classification_loss(const Vec&amp; outputv, 
+                                                           int target) const
+{
+    return (argmax(outputv) == target ? 0 : 1);
+}
+
+void NeuralProbabilisticLanguageModel::initializeParams(bool set_seed)
+{
+    if (set_seed) {
+        if (seed_&gt;=0)
+            rgen-&gt;manual_seed(seed_);
+    }
+
+
+    PP&lt;Dictionary&gt; dict = train_set-&gt;getDictionary(inputsize_);
+    total_output_size = dict-&gt;size();
+
+    total_feats_per_token = 0;
+    for(int i=0; i&lt;n_feat_sets; i++)
+        total_feats_per_token += feat_sets[i]-&gt;size();
+
+    int nnet_inputsize;
+    if(dist_rep_dim &gt; 0)
+    {
+        wout_dist_rep.resize(total_feats_per_token,dist_rep_dim);
+        bout_dist_rep.resize(dist_rep_dim);
+        nnet_inputsize = dist_rep_dim*inputsize_/n_feat_sets;
+        nnet_input.resize(nnet_inputsize);
+
+        fillWeights(wout_dist_rep);
+        bout_dist_rep.clear();
+
+        gradient_wout_dist_rep.resize(total_feats_per_token,dist_rep_dim);
+        gradient_bout_dist_rep.resize(dist_rep_dim);
+        gradient_nnet_input.resize(nnet_inputsize);
+        gradient_wout_dist_rep.clear();
+        gradient_bout_dist_rep.clear();
+        gradient_nnet_input.clear();
+    }
+    else
+    {
+        nnet_inputsize = total_feats_per_token*inputsize_/n_feat_sets;
+        nnet_input = feat_input;
+    }
+
+    if(nhidden&gt;0) 
+    {
+        w1.resize(nnet_inputsize,nhidden);
+        b1.resize(nhidden);
+        hiddenv.resize(nhidden);
+
+        fillWeights(w1);
+        b1.clear();
+
+        gradient_w1.resize(nnet_inputsize,nhidden);
+        gradient_b1.resize(nhidden);
+        gradient_hiddenv.resize(nhidden);
+        gradient_act_hiddenv.resize(nhidden);
+        gradient_w1.clear();
+        gradient_b1.clear();
+        gradient_hiddenv.clear();
+        gradient_act_hiddenv.clear();
+        if(nhidden2&gt;0) 
+        {
+            w2.resize(nhidden,nhidden2);
+            b2.resize(nhidden2);
+            hidden2v.resize(nhidden2);
+            wout.resize(nhidden2,total_output_size);
+            bout.resize(total_output_size);
+
+            fillWeights(w2);
+            b2.clear();
+
+            gradient_w2.resize(nhidden,nhidden2);
+            gradient_b2.resize(nhidden2);
+            gradient_hidden2v.resize(nhidden2);
+            gradient_act_hidden2v.resize(nhidden2);
+            gradient_wout.resize(nhidden2,total_output_size);
+            gradient_bout.resize(total_output_size);
+            gradient_w2.clear();
+            gradient_b2.clear();
+            gradient_hidden2v.clear();
+            gradient_act_hidden2v.clear();
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+        else
+        {
+            wout.resize(nhidden,total_output_size);
+            bout.resize(total_output_size);
+
+            gradient_wout.resize(nhidden,total_output_size);
+            gradient_bout.resize(total_output_size);
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+            
+        if(direct_in_to_out)
+        {
+            direct_wout.resize(nnet_inputsize,total_output_size);
+            direct_bout.resize(0); // Because it is not used
+
+            fillWeights(direct_wout);
+                
+            gradient_direct_wout.resize(nnet_inputsize,total_output_size);
+            gradient_direct_wout.clear();
+            gradient_direct_bout.resize(0); // idem
+        }
+    }
+    else
+    {
+        wout.resize(nnet_inputsize,total_output_size);
+        bout.resize(total_output_size);
+
+        gradient_wout.resize(nnet_inputsize,total_output_size);
+        gradient_bout.resize(total_output_size);
+        gradient_wout.clear();
+        gradient_bout.clear();
+    }
+
+    //fillWeights(wout);
+    
+    if (fixed_output_weights) {
+        static Vec values;
+        if (values.size()==0)
+        {
+            values.resize(2);
+            values[0]=-1;
+            values[1]=1;
+        }
+        rgen-&gt;fill_random_discrete(wout.toVec(), values);
+    }
+    else 
+        fillWeights(wout);
+
+    bout.clear();
+
+    gradient_outputv.resize(total_output_size);
+    gradient_act_outputv.resize(total_output_size);
+    gradient_outputv.clear();
+    gradient_act_outputv.clear();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void NeuralProbabilisticLanguageModel::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // Private variables
+    deepCopyField(target_values,copies);
+    deepCopyField(output_comp,copies);
+    deepCopyField(row,copies);
+    deepCopyField(last_layer,copies);
+    deepCopyField(gradient_last_layer,copies);
+    deepCopyField(feats,copies);
+    deepCopyField(gradient,copies);
+    deepCopyField(neg_energies,copies);
+    deepCopyField(densities,copies);
+
+    // Protected variables
+    deepCopyField(feat_input,copies);
+    deepCopyField(gradient_feat_input,copies);
+    deepCopyField(nnet_input,copies);
+    deepCopyField(gradient_nnet_input,copies);
+    deepCopyField(hiddenv,copies);
+    deepCopyField(gradient_hiddenv,copies);
+    deepCopyField(gradient_act_hiddenv,copies);
+    deepCopyField(hidden2v,copies);
+    deepCopyField(gradient_hidden2v,copies);
+    deepCopyField(gradient_act_hidden2v,copies);
+    deepCopyField(gradient_outputv,copies);
+    deepCopyField(gradient_act_outputv,copies);
+    deepCopyField(rgen,copies);
+    deepCopyField(feats_since_last_update,copies);
+    deepCopyField(target_values_since_last_update,copies);
+    deepCopyField(val_string_reference_set,copies);
+    deepCopyField(target_values_reference_set,copies);
+    deepCopyField(importance_sampling_ratios,copies);
+    deepCopyField(sample,copies);
+    deepCopyField(generated_samples,copies);
+
+    // Public variables
+    deepCopyField(w1,copies);
+    deepCopyField(gradient_w1,copies);
+    deepCopyField(b1,copies);
+    deepCopyField(gradient_b1,copies);
+    deepCopyField(w2,copies);
+    deepCopyField(gradient_w2,copies);
+    deepCopyField(b2,copies);
+    deepCopyField(gradient_b2,copies);
+    deepCopyField(wout,copies);
+    deepCopyField(gradient_wout,copies);
+    deepCopyField(bout,copies);
+    deepCopyField(gradient_bout,copies);
+    deepCopyField(direct_wout,copies);
+    deepCopyField(gradient_direct_wout,copies);
+    deepCopyField(direct_bout,copies);
+    deepCopyField(gradient_direct_bout,copies);
+    deepCopyField(wout_dist_rep,copies);
+    deepCopyField(gradient_wout_dist_rep,copies);
+    deepCopyField(bout_dist_rep,copies);
+    deepCopyField(gradient_bout_dist_rep,copies);
+
+    // Public build options
+    deepCopyField(cost_funcs,copies);
+    deepCopyField(feat_sets,copies);
+    deepCopyField(proposal_distribution,copies);
+
+    PLERROR(&quot;not up to date&quot;);
+}
+
+////////////////
+// outputsize //
+////////////////
+int NeuralProbabilisticLanguageModel::outputsize() const {
+    return targetsize_;
+}
+
+///////////
+// train //
+///////////
+void NeuralProbabilisticLanguageModel::train()
+{
+    //Profiler::activate();
+    if(!train_set)
+        PLERROR(&quot;In NeuralProbabilisticLanguageModel::train, &quot;
+                &quot;you did not setTrainingSet&quot;);
+
+    if(!train_stats)
+        PLERROR(&quot;In NeuralProbabilisticLanguageModel::train, &quot;
+                &quot;you did not setTrainStatsCollector&quot;);
+ 
+    Vec outputv(total_output_size);
+    Vec costsv(getTrainCostNames().length());
+    Vec inputv(train_set-&gt;inputsize());
+    Vec targetv(train_set-&gt;targetsize());
+    real sample_weight = 1;
+
+    int l = train_set-&gt;length();  
+    int bs = batch_size&gt;0 ? batch_size : l;
+
+    // Importance sampling speedup variables
+    
+    // Effective sample size statistics
+    real effective_sample_size_sum = 0;
+    real effective_sample_size_square_sum = 0;
+    real importance_sampling_ratio_k = 0;
+    // Current true sample size;
+    int n_samples = 0;
+
+    real 
+
+    PP&lt;ProgressBar&gt; pb;
+    if(report_progress)
+        pb = new ProgressBar(&quot;Training &quot; + classname() + &quot; from stage &quot; 
+                             + tostring(stage) + &quot; to &quot; 
+                             + tostring(nstages), nstages-stage);
+
+    //if(stage == 0)
+    //{
+    //    for(int t=0; t&lt;l;t++)
+    //    {
+    //        cout &lt;&lt; &quot;t=&quot; &lt;&lt; t &lt;&lt; &quot; &quot;;
+    //        train_set-&gt;getExample(t,inputv,targetv,sample_weight);
+    //        row.subVec(0,inputsize_) &lt;&lt; inputv;
+    //        train_set-&gt;getValues(row,inputsize_,target_values);
+    //        if(target_values.length() != 1)
+    //            verify_gradient(inputv,targetv,1e-6);
+    //    }
+    //    return;
+    //}
+
+    Mat old_gradient_wout;
+    Vec old_gradient_bout;
+    Mat old_gradient_wout_dist_rep;
+    Vec old_gradient_bout_dist_rep;
+    Mat old_gradient_w1;
+    Vec old_gradient_b1;
+    Mat old_gradient_w2;
+    Vec old_gradient_b2;
+    Mat old_gradient_direct_wout;
+
+    if(stochastic_gradient_descent_speedup)
+    {
+        // Trick to make stochastic gradient descent faster
+
+        old_gradient_wout = gradient_wout;
+        old_gradient_bout = gradient_bout;
+        gradient_wout = wout;
+        gradient_bout = bout;
+        
+        if(dist_rep_dim &gt; 0)
+        {
+            old_gradient_wout_dist_rep = gradient_wout_dist_rep;
+            old_gradient_bout_dist_rep = gradient_bout_dist_rep;
+            gradient_wout_dist_rep = wout_dist_rep;
+            gradient_bout_dist_rep = bout_dist_rep;
+        }
+
+        if(nhidden&gt;0) 
+        {
+            old_gradient_w1 = gradient_w1;
+            old_gradient_b1 = gradient_b1;
+            gradient_w1 = w1;
+            gradient_b1 = b1;
+            if(nhidden2&gt;0) 
+            {
+                old_gradient_w2 = gradient_w2;
+                old_gradient_b2 = gradient_b2;
+                gradient_w2 = w2;
+                gradient_b2 = b2;
+            }
+            
+            if(direct_in_to_out)
+            {
+                old_gradient_direct_wout = gradient_direct_wout;
+                gradient_direct_wout = direct_wout;
+            }
+        }
+    }
+
+    int initial_stage = stage;
+    while(stage&lt;nstages)
+    {
+        for(int t=0; t&lt;l;)
+        {
+            //if(t%1000 == 0)
+            //{
+            //    cout &lt;&lt; &quot;Time: &quot; &lt;&lt; clock()/CLOCKS_PER_SEC &lt;&lt; &quot; seconds.&quot; &lt;&lt; endl;
+            //}
+            for(int i=0; i&lt;bs; i++)
+            {
+                //if(t == 71705)
+                //    cout &lt;&lt; &quot;It's going to fuck !!!&quot; &lt;&lt; endl;
+                
+                //if(t == 71704)
+                //    cout &lt;&lt; &quot;It's going to fuck !!!&quot; &lt;&lt; endl;
+                
+                train_set-&gt;getExample(t%l,inputv,targetv,sample_weight);
+
+                if(proposal_distributions)
+                {
+                    n_samples = 0;
+                    importance_sampling_ratios.resize(0);
+                    effective_sample_size_sum = 0;
+                    effective_sample_size_square_sum = 0;                    
+                    while(effective_sample_size &lt; minimum_effective_sample_size)
+                    {
+                        if(n_samples &gt;= total_output_size)
+                        {
+                            gradient_last_layer.resize(total_output_size);
+                            
+                            fprop(inputv,outputv,targetv,costsv,sample_weight);
+                            bprop(inputv,outputv,targetv,costsv,
+                                  start_learning_rate/
+                                  (bs*(1.0+decrease_constant*total_updates)),
+                                  sample_weight);
+                            train_stats-&gt;update(costsv);
+                            break;
+                        }
+                        
+                        importance_sampling_gradient_update(
+                            inputv,targetv,
+                            start_learning_rate/
+                            (bs*(1.0+decrease_constant*total_updates)),
+                            sampling_block_size,
+                            sampleweight
+                            );
+
+                        // Update effective sample size
+                        pval1 = importance_sampling_ratios.subVec(
+                            nsamples,sampling_block_size).data();
+                        for(int k=0; k&lt;sampling_block_size; k++)
+                        {                            
+                            effective_sample_size_sum += *pval1;
+                            effective_sample_size_square_sum += *pval1 * (*pval1);
+                            pval1++;
+                        }
+                        
+                        effective_sample_size = 
+                            (effective_sample_size_sum*effective_sample_size_sum)/
+                            effective_sample_size_square_sum;
+                        n_samples += sampling_block_size;
+                    }
+                }
+                else
+                {
+                    //Profiler::start(&quot;fprop()&quot;);
+                    fprop(inputv,outputv,targetv,costsv,sample_weight);
+                    //Profiler::end(&quot;fprop()&quot;);
+                    //Profiler::start(&quot;bprop()&quot;);
+                    bprop(inputv,outputv,targetv,costsv,
+                          start_learning_rate/
+                          (bs*(1.0+decrease_constant*total_updates)),
+                          sample_weight);
+                    //Profiler::end(&quot;bprop()&quot;);
+                    train_stats-&gt;update(costsv);
+                }
+                t++;
+            }
+            // Update
+            if(!stochastic_gradient_descent_speedup)
+                update();
+            total_updates++;
+        }
+        train_stats-&gt;finalize();
+        ++stage;
+        if(verbosity&gt;2)
+            cout &lt;&lt; &quot;Epoch &quot; &lt;&lt; stage &lt;&lt; &quot; train objective: &quot; 
+                 &lt;&lt; train_stats-&gt;getMean() &lt;&lt; endl;
+        if(pb) pb-&gt;update(stage-initial_stage);
+    }
+
+    if(stochastic_gradient_descent_speedup)
+    {
+        // Trick to make stochastic gradient descent faster
+
+        gradient_wout = old_gradient_wout;
+        gradient_bout = old_gradient_bout;
+        
+        if(dist_rep_dim &gt; 0)
+        {
+            gradient_wout_dist_rep = old_gradient_wout_dist_rep;
+            gradient_bout_dist_rep = old_gradient_bout_dist_rep;
+        }
+
+        if(nhidden&gt;0) 
+        {
+            gradient_w1 = old_gradient_w1;
+            gradient_b1 = old_gradient_b1;
+            if(nhidden2&gt;0) 
+            {
+                gradient_w2 = old_gradient_w2;
+                gradient_b2 = old_gradient_b2;
+            }
+            
+            if(direct_in_to_out)
+            {
+                gradient_direct_wout = old_gradient_direct_wout;
+            }
+        }
+    }
+    //Profiler::report(cout);
+}
+
+void NeuralProbabilisticLanguageModel::verify_gradient(
+    Vec&amp; input, Vec targetv, real step)
+{
+    Vec costsv(getTrainCostNames().length());
+    real sampleweight = 1;
+    real verify_step = step;
+    
+    // To avoid the interaction between fprop and this function
+    int nfeats = 0;
+    int id = 0;
+    int ifeats = 0;
+
+    Vec est_gradient_bout;
+    Mat est_gradient_wout;
+    Vec est_gradient_bout_dist_rep;
+    Mat est_gradient_wout_dist_rep;
+    Vec est_gradient_b1;
+    Mat est_gradient_w1;
+    Vec est_gradient_b2;
+    Mat est_gradient_w2;
+    Vec est_gradient_direct_bout;
+    Mat est_gradient_direct_wout;
+
+    int nnet_inputsize;
+    if(dist_rep_dim &gt; 0)
+    {
+        nnet_inputsize = dist_rep_dim*inputsize_/n_feat_sets;
+        est_gradient_wout_dist_rep.resize(total_feats_per_token,dist_rep_dim);
+        est_gradient_bout_dist_rep.resize(dist_rep_dim);
+        est_gradient_wout_dist_rep.clear();
+        est_gradient_bout_dist_rep.clear();
+        gradient_wout_dist_rep.clear();
+        gradient_bout_dist_rep.clear();
+    }
+    else
+    {
+        nnet_inputsize = total_feats_per_token*inputsize_/n_feat_sets;
+    }
+    
+    if(nhidden&gt;0) 
+    {
+        est_gradient_w1.resize(nnet_inputsize,nhidden);
+        est_gradient_b1.resize(nhidden);
+        est_gradient_w1.clear();
+        est_gradient_b1.clear();
+        gradient_w1.clear();
+        gradient_b1.clear();
+        if(nhidden2&gt;0) 
+        {
+            est_gradient_w2.resize(nhidden,nhidden2);
+            est_gradient_b2.resize(nhidden2);
+            est_gradient_wout.resize(nhidden2,total_output_size);
+            est_gradient_bout.resize(total_output_size);
+            est_gradient_w2.clear();
+            est_gradient_b2.clear();
+            est_gradient_wout.clear();
+            est_gradient_bout.clear();
+            gradient_w2.clear();
+            gradient_b2.clear();
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+        else
+        {
+            est_gradient_wout.resize(nhidden,total_output_size);
+            est_gradient_bout.resize(total_output_size);
+            est_gradient_wout.clear();
+            est_gradient_bout.clear();
+            gradient_wout.clear();
+            gradient_bout.clear();
+        }
+            
+        if(direct_in_to_out)
+        {
+            est_gradient_direct_wout.resize(nnet_inputsize,total_output_size);
+            est_gradient_direct_wout.clear();
+            est_gradient_direct_bout.resize(0); // idem
+            gradient_direct_wout.clear();                        
+        }
+    }
+    else
+    {
+        est_gradient_wout.resize(nnet_inputsize,total_output_size);
+        est_gradient_bout.resize(total_output_size);
+        est_gradient_wout.clear();
+        est_gradient_bout.clear();
+        gradient_wout.clear();
+        gradient_bout.clear();
+    }
+
+    fprop(input, output_comp, targetv, costsv);
+    bprop(input,output_comp,targetv,costsv,
+          -1, sampleweight);
+    clearProppathGradient();
+    
+    // Compute estimated gradient
+
+    if(dist_rep_dim &gt; 0) 
+    {        
+        nfeats = 0;
+        id = 0;
+        for(int i=0; i&lt;inputsize_;)
+        {
+            ifeats = 0;
+            for(int j=0; j&lt;n_feat_sets; j++,i++)
+                ifeats += feats[i].length();
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                feat_input.subVec(nfeats,ifeats),
+                wout_dist_rep, bout_dist_rep,
+                est_gradient_wout_dist_rep, est_gradient_bout_dist_rep,
+                true, false, verify_step);
+            nfeats += ifeats;
+            id++;
+        }
+
+        cout &lt;&lt; &quot;Verify wout_dist_rep&quot; &lt;&lt; endl;
+        output_gradient_verification(gradient_wout_dist_rep.toVec(), 
+                                     est_gradient_wout_dist_rep.toVec());
+        cout &lt;&lt; &quot;Verify bout_dist_rep&quot; &lt;&lt; endl;
+        output_gradient_verification(gradient_bout_dist_rep, 
+                                     est_gradient_bout_dist_rep);
+        gradient_wout_dist_rep.clear();
+        gradient_bout_dist_rep.clear();
+
+        if(nhidden&gt;0) 
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                nnet_input,w1,b1,
+                est_gradient_w1, est_gradient_b1, false,false, verify_step);
+
+            cout &lt;&lt; &quot;Verify w1&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_w1.toVec(), 
+                                         est_gradient_w1.toVec());
+            cout &lt;&lt; &quot;Verify b1&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_b1, est_gradient_b1);
+            
+            if(nhidden2&gt;0) 
+            {
+                verify_gradient_affine_transform(
+                    input,output_comp, targetv, costsv, sampleweight,    
+                    hiddenv,w2,b2,
+                    est_gradient_w2, est_gradient_b2,
+                    false,false, verify_step);
+                cout &lt;&lt; &quot;Verify w2&quot; &lt;&lt; endl;
+                output_gradient_verification(gradient_w2.toVec(), 
+                                             est_gradient_w2.toVec());
+                cout &lt;&lt; &quot;Verify b2&quot; &lt;&lt; endl;
+                output_gradient_verification(gradient_b2, est_gradient_b2);
+
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = nnet_input;
+
+        verify_gradient_affine_transform(
+            input,output_comp, targetv, costsv, sampleweight,
+            last_layer,wout,bout,
+            est_gradient_wout, est_gradient_bout, false,
+            possible_targets_vary,verify_step,target_values);
+
+        cout &lt;&lt; &quot;Verify wout&quot; &lt;&lt; endl;
+        output_gradient_verification(gradient_wout.toVec(), 
+                                     est_gradient_wout.toVec());
+        cout &lt;&lt; &quot;Verify bout&quot; &lt;&lt; endl;
+        output_gradient_verification(gradient_bout, est_gradient_bout);
+ 
+        if(direct_in_to_out &amp;&amp; nhidden&gt;0)
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                nnet_input,direct_wout,direct_bout,
+                est_gradient_direct_wout, est_gradient_direct_bout,false,
+                possible_targets_vary, verify_step, target_values);
+            cout &lt;&lt; &quot;Verify direct_wout&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_direct_wout.toVec(), 
+                                         est_gradient_direct_wout.toVec());
+            //cout &lt;&lt; &quot;Verify direct_bout&quot; &lt;&lt; endl;
+            //output_gradient_verification(gradient_direct_bout, est_gradient_direct_bout);
+        }
+    }
+    else
+    {        
+        if(nhidden&gt;0)
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                feat_input,w1,b1,
+                est_gradient_w1, est_gradient_b1,
+                true,false, verify_step);
+
+            cout &lt;&lt; &quot;Verify w1&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_w1.toVec(), 
+                                         est_gradient_w1.toVec());
+            cout &lt;&lt; &quot;Verify b1&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_b1, est_gradient_b1);
+
+            if(nhidden2&gt;0)
+            {
+                verify_gradient_affine_transform(
+                    input,output_comp, targetv, costsv, sampleweight,
+                    hiddenv,w2,b2,
+                    est_gradient_w2, est_gradient_b2,true,false,
+                    verify_step);
+
+                cout &lt;&lt; &quot;Verify w2&quot; &lt;&lt; endl;
+                output_gradient_verification(gradient_w2.toVec(), 
+                                             est_gradient_w2.toVec());
+                cout &lt;&lt; &quot;Verify b2&quot; &lt;&lt; endl;
+                output_gradient_verification(gradient_b2, est_gradient_b2);
+                
+                last_layer = hidden2v;
+            }
+            else
+                last_layer = hiddenv;
+        }
+        else
+            last_layer = feat_input;
+        
+        verify_gradient_affine_transform(
+            input,output_comp, targetv, costsv, sampleweight,
+            last_layer,wout,bout,
+            est_gradient_wout, est_gradient_bout, nhidden&lt;=0,
+            possible_targets_vary,verify_step, target_values);
+
+        cout &lt;&lt; &quot;Verify wout&quot; &lt;&lt; endl;
+        output_gradient_verification(gradient_wout.toVec(), 
+                                     est_gradient_wout.toVec());
+        cout &lt;&lt; &quot;Verify bout&quot; &lt;&lt; endl;
+        output_gradient_verification(gradient_bout, est_gradient_bout);
+        
+        if(direct_in_to_out &amp;&amp; nhidden&gt;0)
+        {
+            verify_gradient_affine_transform(
+                input,output_comp, targetv, costsv, sampleweight,
+                feat_input,direct_wout,direct_bout,
+                est_gradient_wout, est_gradient_bout,true,
+                possible_targets_vary, verify_step,target_values);
+            cout &lt;&lt; &quot;Verify direct_wout&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_direct_wout.toVec(), 
+                                         est_gradient_direct_wout.toVec());
+            cout &lt;&lt; &quot;Verify direct_bout&quot; &lt;&lt; endl;
+            output_gradient_verification(gradient_direct_bout, 
+                                         est_gradient_direct_bout);
+        }
+    }
+
+}
+
+void NeuralProbabilisticLanguageModel::verify_gradient_affine_transform(
+    Vec global_input, Vec&amp; global_output, Vec&amp; global_targetv,
+    Vec&amp; global_costs, real sampleweight,
+    Vec input, Mat weights, Vec bias,
+    Mat est_gweights, Vec est_gbias,  
+    bool input_is_sparse, bool output_is_sparse,
+    real step,
+    Vec output_indices) const
+{
+    real *pval1, *pval2, *pval3;
+    int ni,nj;
+    real out1,out2;
+    // Bias
+    if(bias.length() != 0)
+    {
+        if(output_is_sparse)
+        {
+            pval1 = est_gbias.data();
+            pval2 = bias.data();
+            pval3 = output_indices.data();
+            ni = output_indices.length();
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval2[(int)*pval3] += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                pval2[(int)*pval3] -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                pval1[(int)*pval3] = (out1-out2)/(2*step);
+                pval2[(int)*pval3] += step;
+                pval3++;
+            }
+        }
+        else
+        {
+            pval1 = est_gbias.data();
+            pval2 = bias.data();
+            ni = bias.length();
+            for(int i=0; i&lt;ni; i++)
+            {
+                *pval2 += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                *pval2 -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                *pval1 = (out1-out2)/(2*step);
+                *pval2 += step;
+                pval1++; 
+                pval2++;
+            }
+        }
+    }
+
+    // Weights
+    if(!input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        ni = weights.length();
+        nj = weights.width();
+        for(int i=0; i&lt;ni; i++)
+            for(int j=0; j&lt;nj; j++)
+            {
+                weights(i,j) += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                weights(i,j) -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                weights(i,j) += step;
+                est_gweights(i,j) = (out1-out2)/(2*step);
+            }
+    }
+    else if(!input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        ni = output_indices.length();
+        nj = input.length();
+        pval3 = output_indices.data();
+        for(int i=0; i&lt;ni; i++)
+        {
+            for(int j=0; j&lt;nj; j++)
+            {
+                weights(j,(int)*pval3) += step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out1 = global_costs[0];
+                weights(j,(int)*pval3) -= 2*step;
+                fprop(global_input, global_output, global_targetv, 
+                      global_costs, sampleweight);
+                out2 = global_costs[0];
+                weights(j,(int)*pval3) += step;
+                est_gweights(j,(int)*pval3) = (out1-out2)/(2*step);
+//                if(target_values.length() != 1 &amp;&amp; input[j] != 0 &amp;&amp; (out1-out2)/(2*step) == 0)
+//                {                    
+//                    print_what_the_fuck();
+//                    weights(j,(int)*pval3) += 1;
+//                    fprop(global_input, global_output, global_targetv, global_costs, sampleweight);
+//                    weights(j,(int)*pval3) -= 1;
+//                    cout &lt;&lt; &quot;out1 - global_costs[0] =&quot; &lt;&lt; out1-global_costs[0] &lt;&lt; endl;
+//                }
+            }
+            pval3++;
+        }
+    }
+    else if(input_is_sparse &amp;&amp; !output_is_sparse)
+    {
+        ni = input.length();
+        nj = weights.width();
+        if(ni != 0 )
+        {
+            pval3 = input.data();
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval1 = est_gweights[(int)(*pval3)];
+                pval2 = weights[(int)(*pval3++)];
+                for(int j=0; j&lt;nj;j++)
+                {
+                    *pval2 += step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out1 = global_costs[0];
+                    *pval2 -= 2*step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out2 = global_costs[0];
+                    *pval1 = (out1-out2)/(2*step);
+                    *pval2 += step;
+                    pval1++;
+                    pval2++;
+                }
+            }
+        }
+    }
+    else if(input_is_sparse &amp;&amp; output_is_sparse)
+    {
+        // Weights
+        ni = input.length();
+        nj = output_indices.length();
+        if(ni != 0)
+        {
+            pval2 = input.data();
+            for(int i=0; i&lt;ni; i++)
+            {
+                pval3 = output_indices.data();
+                for(int j=0; j&lt;nj; j++)
+                {
+                    weights((int)(*pval2),(int)*pval3) += step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out1 = global_costs[0];
+                    weights((int)(*pval2),(int)*pval3) -= 2*step;
+                    fprop(global_input, global_output, global_targetv, 
+                          global_costs, sampleweight);
+                    out2 = global_costs[0];
+                    est_gweights((int)(*pval2),(int)*pval3)  = 
+                        (out1-out2)/(2*step);
+                    weights((int)(*pval2),(int)*pval3) += step;
+                    pval3++;
+                }
+                pval2++;
+            }
+        }
+    }
+}
+
+
+void NeuralProbabilisticLanguageModel::output_gradient_verification(
+    Vec grad, Vec est_grad)
+{
+    // Inspired from Func::verifyGradient()
+
+    Vec num = apply(grad - est_grad,(tRealFunc)FABS);
+    Vec denom = real(0.5)*apply(grad + est_grad,(tRealFunc)FABS);
+    for (int i = 0; i &lt; num.length(); i++)
+    {
+        if (!fast_exact_is_equal(num[i], 0))
+            num[i] /= denom[i];
+        else
+            if(!fast_exact_is_equal(denom[i],0))
+                cout &lt;&lt; &quot;at position &quot; &lt;&lt; i &lt;&lt; &quot; num[i] == 0 but denom[i] = &quot; 
+                     &lt;&lt; denom[i] &lt;&lt; endl;
+    }
+    int pos = argmax(num);
+    cout &lt;&lt; max(num) &lt;&lt; &quot; (at position &quot; &lt;&lt; pos &lt;&lt; &quot;/&quot; &lt;&lt; num.length()
+         &lt;&lt; &quot;, computed = &quot; &lt;&lt; grad[pos] &lt;&lt; &quot; and estimated = &quot;
+         &lt;&lt; est_grad[pos] &lt;&lt; &quot;)&quot; &lt;&lt; endl;
+
+    real norm_grad = norm(grad);
+    real norm_est_grad = norm(est_grad);
+    real cos_angle = fast_exact_is_equal(norm_grad*norm_est_grad,
+                                         0)
+        ? MISSING_VALUE
+        : dot(grad,est_grad) /
+        (norm_grad*norm_est_grad);
+    if (cos_angle &gt; 1)
+        cos_angle = 1;      // Numerical imprecisions can lead to such situation.
+    cout &lt;&lt; &quot;grad.length() = &quot; &lt;&lt; grad.length() &lt;&lt; endl;
+    cout &lt;&lt; &quot;cos(angle) : &quot; &lt;&lt; cos_angle &lt;&lt; endl;
+    cout &lt;&lt; &quot;angle : &quot; &lt;&lt; ( is_missing(cos_angle) ? MISSING_VALUE
+                            : acos(cos_angle) ) &lt;&lt; endl;
+}
+
+void NeuralProbabilisticLanguageModel::batchComputeOutputAndConfidence(
+    VMat inputs, real probability,
+    VMat outputs_and_confidence) const
+{
+    val_string_reference_set = inputs;
+    inherited::batchComputeOutputAndConfidence(inputs,
+                                               probability,
+                                               outputs_and_confidence);
+    val_string_reference_set = train_set;
+}
+
+void NeuralProbabilisticLanguageModel::use(VMat testset, VMat outputs) const
+{
+    val_string_reference_set = testset;
+    if(testset-&gt;width() &gt; train_set-&gt;inputsize())
+        target_values_reference_set = testset;
+    target_values_reference_set = testset;
+    inherited::use(testset,outputs);
+    val_string_reference_set = train_set;
+    if(testset-&gt;width() &gt; train_set-&gt;inputsize())
+        target_values_reference_set = train_set;
+}
+
+void NeuralProbabilisticLanguageModel::test(VMat testset, 
+                                            PP&lt;VecStatsCollector&gt; test_stats, 
+                      VMat testoutputs, VMat testcosts) const
+{
+    val_string_reference_set = testset;
+    target_values_reference_set = testset;
+    inherited::test(testset,test_stats,testoutputs,testcosts);
+    val_string_reference_set = train_set;
+    target_values_reference_set = train_set;
+}
+
+VMat NeuralProbabilisticLanguageModel::processDataSet(VMat dataset) const
+{
+    VMat ret;
+    val_string_reference_set = dataset;
+    // Assumes it contains the target part information
+    if(dataset-&gt;width() &gt; train_set-&gt;inputsize())
+        target_values_reference_set = dataset;
+    ret = inherited::processDataSet(dataset);
+    val_string_reference_set = train_set;
+    if(dataset-&gt;width() &gt; train_set-&gt;inputsize())
+        target_values_reference_set = train_set;
+    return ret;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h
===================================================================
--- trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h	2008-06-17 18:21:44 UTC (rev 9137)
+++ trunk/plearn_learners_experimental/NeuralProbabilisticLanguageModel.h	2008-06-17 19:46:59 UTC (rev 9138)
@@ -0,0 +1,482 @@
+// -*- C++ -*-
+
+// NeuralProbabilisticLanguageModel.h
+// Copyright (c) 1998-2002 Pascal Vincent
+// Copyright (C) 1999-2002 Yoshua Bengio and University of Montreal
+// Copyright (c) 2002 Jean-Sebastien Senecal, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/*! \file PLearnLibrary/PLearnAlgo/NeuralProbabilisticLanguageModel.h */
+
+#ifndef NeuralProbabilisticLanguageModel_INC
+#define NeuralProbabilisticLanguageModel_INC
+
+#include &quot;PLearner.h&quot;
+#include &lt;plearn/math/PRandom.h&gt;
+#include &lt;plearn/feat/FeatureSet.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Feedforward neural network for language modeling
+ *
+ * Implementation of the Neural Probabilistic Language Model proposed by
+ * Bengio, Ducharme, Vincent and Jauvin (JMLR 2003), with extentensions to speedup
+ * the model (Bengio and S&#233;n&#233;cal, AISTATS 2003) and to include prior information
+ * about the distributed representation and permit generalization of these
+ * distributed representations to out-of-vocabulary words using features
+ * (Larochelle and Bengio, Tech Report 2006).
+ */
+class NeuralProbabilisticLanguageModel: public PLearner
+{
+
+private:
+
+    typedef PLearner inherited;
+    
+    //! Vector of possible target values
+    mutable Vec target_values;
+    //! Vector for output computations 
+    mutable Vec output_comp;
+    //! Row vector
+    mutable Vec row;
+    //! Last layer of network (pointer to either nnet_input,
+    //! vnhidden or vnhidden2)
+    mutable Vec last_layer;
+    //! Gradient of last layer in back propagation
+    mutable Vec gradient_last_layer;
+    //! Features for each token
+    mutable TVec&lt; TVec&lt;int&gt; &gt; feats;
+
+    //! Temporary computations variable, used in fprop() and bprop()
+    //! Care must be taken when using these variables,
+    //! since they are used by many different functions
+    mutable Vec gradient, neg_energies, densities;
+    mutable string str;
+    mutable real * pval1, * pval2, * pval3, * pval4, * pval5;
+    mutable real val, val2, grad;
+    mutable int offset;
+    mutable int ni,nj,nk,id,nfeats,ifeats;
+    mutable int* f;
+
+protected:
+
+    //! Total output size
+    int total_output_size;
+    //! Total updates so far;
+    int total_updates;
+    //! Number of feature sets
+    int n_feat_sets;
+    //! Number of features per input token
+    //! for which a distributed representation
+    //! is computed.
+    int total_feats_per_token;
+    //! Reindexed target
+    mutable int reind_target;
+    //! Feature input;
+    mutable Vec feat_input;
+    //! Gradient on feature input (useless for now)
+    Vec gradient_feat_input;
+    //! Input vector to NNet (after mapping into distributed representations)
+    Vec nnet_input;
+    //! Gradient for vector to NNet
+    Vec gradient_nnet_input;
+    //! First hidden layer value
+    Vec hiddenv;
+    //! Gradient of first hidden layer
+    Vec gradient_hiddenv;
+    //! Gradient through first hidden layer activation
+    Vec gradient_act_hiddenv;
+    //! Second hidden layer value
+    Vec hidden2v;
+    //! Gradient of second hidden layer
+    Vec gradient_hidden2v;
+    //! Gradient through second hidden layer activation
+    Vec gradient_act_hidden2v;
+    //! Gradient on output
+    Vec gradient_outputv;
+    //! Gradient throught output layer activation
+    Vec gradient_act_outputv;
+    //! Random number generator for parameters initialization
+    PP&lt;PRandom&gt; rgen;
+    //! Features seen in input since last update
+    Vec feats_since_last_update;
+    //! Possible target values seen since last update
+    Vec target_values_since_last_update;
+    //! VMatrix used to get values to string mapping for input tokens
+    mutable VMat val_string_reference_set;
+    //! Possible target values mapping.
+    mutable VMat target_values_reference_set;
+    //! Importance sampling ratios of the samples
+    Vec importance_sampling_ratios;
+    //! Generated sample from proposal distribution
+    Vec sample;
+    //! Set of generated samples from the proposal distribution
+    Vec generated_samples;
+
+public: 
+    //! Weights of first hidden layer
+    Mat w1;
+    //! Gradient on weights of first hidden layer
+    Mat gradient_w1;
+    //! Bias of first hidden layer
+    Vec b1;
+    //! Gradient on bias of first hidden layer
+    Vec gradient_b1;
+    //! Weights of second hidden layer
+    Mat w2;
+    //! gradient on weights of second hidden layer
+    Mat gradient_w2;
+    //! Bias of second hidden layer
+    Vec b2;
+    //! Gradient on bias of second hidden layer
+    Vec gradient_b2;
+    //! Weights of output layer
+    Mat wout;
+    //! Gradient on weights of output layer
+    Mat gradient_wout;
+    //! Bias of output layer
+    Vec bout;
+    //! Gradient on bias of output layer
+    Vec gradient_bout;
+    //! Direct input to output weights
+    Mat direct_wout;
+    //! Gradient on direct input to output weights
+    Mat gradient_direct_wout;
+    //! Direct input to output bias (empty, since no bias is used)
+    Vec direct_bout;
+    //! Gradient on direct input to output bias (empty, since no bias is used)
+    Vec gradient_direct_bout;
+    //! Weights of output layer for distributed
+    //! representation predictor
+    Mat wout_dist_rep;
+    //! Gradient on weights of output layer for distributed
+    //! representation predictor
+    Mat gradient_wout_dist_rep;
+    //! Bias of output layer for distributed
+    //! representation predictor
+    Vec bout_dist_rep;
+    //! Gradient on bias of output layer for distributed
+    //! representation predictor
+    Vec gradient_bout_dist_rep;
+
+public:
+
+    // Build options:
+
+    //! Number of hidden nunits in first hidden layer (default:0)
+    int nhidden;
+    //! Number of hidden units in second hidden layer (default:0)
+    int nhidden2; 
+    //! Weight decay (default:0)
+    real weight_decay; 
+    //! Bias decay (default:0)
+    real bias_decay; 
+    //! Weight decay for weights from input layer to first hidden layer 
+    //! (default:0)
+    real layer1_weight_decay; 
+    //! Bias decay for weights from input layer to first hidden layer 
+    //! (default:0)
+    real layer1_bias_decay;   
+    //! Weight decay for weights from first hidden layer to second hidden
+    //! layer (default:0)
+    real layer2_weight_decay; 
+    //! Bias decay for weights from first hidden layer to second hidden 
+    //! layer (default:0)
+    real layer2_bias_decay;   
+    //! Weight decay for weights from last hidden layer to output layer 
+    //! (default:0)
+    real output_layer_weight_decay; 
+    //! Bias decay for weights from last hidden layer to output layer 
+    //! (default:0)
+    real output_layer_bias_decay;
+    //! Weight decay for weights from input directly to output layer 
+    //! (default:0)
+    real direct_in_to_out_weight_decay;
+    //! Weight decay for weights from last hidden layer to output layer
+    //! of distributed representation predictor (default:0)
+    real output_layer_dist_rep_weight_decay; 
+    //! Bias decay for weights from last hidden layer to output layer of 
+    //! distributed representation predictor (default:0)
+    real output_layer_dist_rep_bias_decay;
+    //! Margin requirement, used only with the margin_perceptron_cost 
+    //! cost function (default:1)
+    real margin; 
+    //! If true then the output weights are not learned. 
+    //! They are initialized to +1 or -1 randomly (default:false)
+    bool fixed_output_weights;
+    //! If true then direct input to output weights will be added 
+    //! (if nhidden &gt; 0)
+    bool direct_in_to_out;
+    //! Penalty to use on the weights (for weight and bias decay) 
+    //! (default:&quot;L2_square&quot;)
+    string penalty_type; 
+    //! Transfer function to use for ouput layer (default:&quot;&quot;)
+    string output_transfer_func; 
+    //! Transfer function to use for hidden units (default:&quot;tanh&quot;)
+    //! tanh, sigmoid, softplus, softmax, etc...  
+    string hidden_transfer_func; 
+    //! Cost functions.
+    TVec&lt;string&gt; cost_funcs;  
+    //! Start learning rate of gradient descent
+    real start_learning_rate;
+    //! Decrease constant of gradietn descent
+    real decrease_constant;
+    //! Number of samples to use to estimate gradient before an update.
+    //! 0 means the whole training set (default: 1)
+    int batch_size; 
+    //! Indication that a trick to speedup stochastic gradient descent
+    //! should be used. 
+    bool stochastic_gradient_descent_speedup;
+    //! Method of initialization for neural network's weights
+    string initialization_method;
+    //! Dimensionality (number of components) of distributed representations
+    //! If &lt;= 0, than distributed representations will not be used.
+    int dist_rep_dim;
+    //! Indication that the set of possible targets vary from
+    //! one input vector to another.
+    bool possible_targets_vary;
+    //! FeatureSets to apply on input
+    TVec&lt;PP&lt;FeatureSet&gt; &gt; feat_sets;
+    //! Proposal distribution for importance sampling
+    //! speedup method (Bengio and Senecal 2006).
+    //! If NULL, then this speedup method won't be used.
+    //! This proposal distribution should use the same
+    //! symbol int/string mapping as this class
+    //! uses.
+    PP&lt;PDistribution&gt; proposal_distribution;
+    //! Indication that the proposal distribution
+    //! must be trained (using train_set).
+    bool train_proposal_distribution;
+    //! Size of the sampling blocks
+    int sampling_block_size;
+    //! Minimum effective sample size
+    int minimum_effective_sample_size;
+
+private:
+    void build_();
+
+    //! Softmax vector y obtained on x
+    //! This implementation is such that 
+    //! compute_softmax(x,x) is such that x
+    //! becomes its softmax value.
+    void compute_softmax(const Vec&amp; x, const Vec&amp; y) const;
+
+    //! Negative log-likelihood loss
+    real nll(const Vec&amp; outputv, int target) const;
+    
+    //! Classification loss
+    real classification_loss(const Vec&amp; outputv, int target) const;
+    
+    //! Argmax function that lets you define the default (first)
+    //! component used for comparisons. This is useful to avoid bias in the prediction
+    //! when the getValues() provides some information about
+    //! the prior distribution of the targets (e.g. the first target given by 
+    //! getValues() is the most likely) and the output of the model is
+    //! the same for all targets.
+    int my_argmax(const Vec&amp; vec, int default_compare=0) const;
+
+public:
+
+    NeuralProbabilisticLanguageModel();
+    virtual ~NeuralProbabilisticLanguageModel();
+    PLEARN_DECLARE_OBJECT(NeuralProbabilisticLanguageModel);
+
+    virtual void build();
+    virtual void forget(); // simply calls initializeParams()
+
+    virtual int outputsize() const;
+    virtual TVec&lt;string&gt; getTrainCostNames() const;
+    virtual TVec&lt;string&gt; getTestCostNames() const;
+
+    virtual void train();
+
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+                                       Vec&amp; output, Vec&amp; costs) const;
+
+    virtual void computeCostsFromOutputs(const Vec&amp; input, 
+                                         const Vec&amp; output, 
+                                         const Vec&amp; target, 
+                                         Vec&amp; costs) const;
+
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap &amp;copies);
+
+protected:
+    static void declareOptions(OptionList&amp; ol);
+
+    //! Forward propagation in the network
+    void fprop(const Vec&amp; inputv, Vec&amp; outputv, const Vec&amp; targetv, Vec&amp; costsv, real sampleweight=1) const;
+
+    //! Forward propagation to compute the output
+    void fpropOutput(const Vec&amp; inputv, Vec&amp; outputv) const;
+
+    //! Forward propagation until output weights are reached
+    //! (called by fpropOutput(...) and importance_sampling_gradient_update(...)
+    void fpropBeforeOutputWeights(const Vec&amp; inputv) const;
+
+    //! Forward propagation to compute the costs from the output
+    void fpropCostsFromOutput(const Vec&amp; inputv, const Vec&amp; outputv, const Vec&amp; targetv, Vec&amp; costsv, real sampleweight=1) const;
+
+    //! Backward propagation in the network,
+    //! which assumes that a forward propagation has been done
+    //! before.
+    //! A learning rate needs to be provided because it is
+    //! -learning_rate * gradient that is propagated, not just the gradient.
+    void bprop(Vec&amp; inputv, Vec&amp; outputv, Vec&amp; targetv, Vec&amp; costsv, real learning_rate, real sampleweight=1);
+
+    //! Update network's parameters
+    void update();
+
+    //! Update affine transformation's parameters
+    void update_affine_transform(Vec input, Mat weights, Vec bias,
+                                 Mat gweights, Vec gbias,
+                                 bool input_is_sparse, bool output_is_sparse,
+                                 Vec output_indices);
+    
+    //! Clear network's propagation path gradient fields
+    //! Assumes fprop and bprop have been called before.
+    void clearProppathGradient();
+
+    //! Initialize the parameters. If 'set_seed' is set to false, the seed
+    //! will not be set in this method (it will be assumed to be already
+    //! initialized according to the 'seed' option).
+    //! The index of the extra task (-1 if main task) also needs to be
+    //! provided.
+    virtual void initializeParams(bool set_seed = true);
+
+    //! Computes the result of the application of the
+    //! given transfer function on the input vector
+    void add_transfer_func(const Vec&amp; input, 
+                          string transfer_func = &quot;default&quot;) const;
+
+    //! Computes the gradient through the given activation function,
+    //! the output value and the initial gradient on that output
+    //! (i.e. before the activation function).
+    //! After calling this function, gradient_act_output corresponds
+    //! the gradient after the activation function.
+    //! nll_softmax_speed_up_target is to speed up the gradient computation
+    //! for the output layer using the softmax transfert function
+    //! and the NLL cost function is applied.
+    void gradient_transfer_func(Vec&amp; output, Vec&amp; gradient_input, 
+                                Vec&amp; gradient_output,                   
+                                string transfer_func = &quot;default&quot;,
+                                int nll_softmax_speed_up_target=-1);
+
+    //! Applies affine transform on input using provided weights
+    //! and bias. Information about the nature of the input and output
+    //! need to be provided.
+    //! If bias.length() == 0, then output initial value is used as the bias.
+    void add_affine_transform(Vec input, Mat weights, Vec bias, Vec output, 
+                              bool input_is_sparse, bool output_is_sparse,
+                              Vec output_indices = Vec(0)) const;
+
+    //! Propagate gradient through affine transform on input using provided weights
+    //! and bias. Information about the nature of the input and output
+    //! need to be provided. 
+    //! If bias.length() == 0, then no backprop is made to bias.
+    void gradient_affine_transform(Vec input, Mat weights, Vec bias, 
+                                   Vec ginput, Mat gweights, Vec gbias, Vec goutput, 
+                                   bool input_is_sparse, bool output_is_sparse,
+                                   real learning_rate,
+                                   real weight_decay, real bias_decay,
+                                   Vec output_indices = Vec(0));
+
+    //! Propagate penalty gradient through weights and bias, 
+    //! scaled by -learning rate.
+    void gradient_penalty(Vec input, Mat weights, Vec bias, 
+                          Mat gweights, Vec gbias,  
+                          bool input_is_sparse, bool output_is_sparse,
+                          real learning_rate,
+                          real weight_decay, real bias_decay,
+                          Vec output_indices = Vec(0));
+    
+    //! Update the neural network parameters using the importance sampling
+    //! estimate of the gradient, based on n_samples of the proposal distribution.
+    void importance_sampling_gradient_update(Vec&amp; inputv, Vec&amp; targetv, 
+                                             real learning_rate, int n_samples, 
+                                             real train_sample_weight=1);
+
+    //! Gives scalar negative energy values for some samples (words).
+    //! Assumes fpropBeforeOutputWeights has been called previously.
+    void getNegativeEnergyValues(Vec samples, Vec neg_energies);
+
+    //! Fill a matrix of weights according to the 'initialization_method' 
+    //! specified. 
+    void fillWeights(const Mat&amp; weights);
+
+    //! Verify gradient of propagation path
+    void verify_gradient(Vec&amp; input, Vec target, real step);
+
+    //! Verify gradient of affine_transform parameters
+    void verify_gradient_affine_transform(
+        Vec global_input, Vec&amp; global_output, Vec&amp; global_targetv, 
+        Vec&amp; global_costs, real sampleweight,
+        Vec input, Mat weights, Vec bias,
+        Mat est_gweights, Vec est_gbias, 
+        bool input_is_sparse, bool output_is_sparse,
+        real step,
+        Vec output_indices = Vec(0)) const;
+    
+    void output_gradient_verification(Vec grad, Vec est_grad);
+
+    //! Changes the reference_set and then calls the parent's class method
+    void batchComputeOutputAndConfidence(VMat inputs, real probability,
+                                         VMat outputs_and_confidence) const;
+    //! Changes the reference_set and then calls the parent's class method
+    virtual void use(VMat testset, VMat outputs) const;
+    //! Changes the reference_set and then calls the parent's class method
+    virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats, 
+                      VMat testoutputs=0, VMat testcosts=0) const;
+    //! Changes the reference_set and then calls the parent's class method
+    virtual VMat processDataSet(VMat dataset) const;
+        
+};
+
+DECLARE_OBJECT_PTR(NeuralProbabilisticLanguageModel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002585.html">[Plearn-commits] r9137 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002587.html">[Plearn-commits] r9139 - trunk/commands
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2586">[ date ]</a>
              <a href="thread.html#2586">[ thread ]</a>
              <a href="subject.html#2586">[ subject ]</a>
              <a href="author.html#2586">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
