<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9103 - in trunk: commands/EXPERIMENTAL	plearn_learners/unsupervised/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9103%20-%20in%20trunk%3A%20commands/EXPERIMENTAL%0A%09plearn_learners/unsupervised/EXPERIMENTAL&In-Reply-To=%3C200806032235.m53MZFPn025133%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002550.html">
   <LINK REL="Next"  HREF="002552.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9103 - in trunk: commands/EXPERIMENTAL	plearn_learners/unsupervised/EXPERIMENTAL</H1>
    <B>plearner at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9103%20-%20in%20trunk%3A%20commands/EXPERIMENTAL%0A%09plearn_learners/unsupervised/EXPERIMENTAL&In-Reply-To=%3C200806032235.m53MZFPn025133%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9103 - in trunk: commands/EXPERIMENTAL	plearn_learners/unsupervised/EXPERIMENTAL">plearner at mail.berlios.de
       </A><BR>
    <I>Wed Jun  4 00:35:15 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002550.html">[Plearn-commits] r9102 - trunk/commands/PLearnCommands
</A></li>
        <LI>Next message: <A HREF="002552.html">[Plearn-commits] r9104 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2551">[ date ]</a>
              <a href="thread.html#2551">[ thread ]</a>
              <a href="subject.html#2551">[ subject ]</a>
              <a href="author.html#2551">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: plearner
Date: 2008-06-04 00:35:14 +0200 (Wed, 04 Jun 2008)
New Revision: 9103

Added:
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
   trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h
Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
Log:
New experimental unsupervised learner DiverseComponentAnalysis


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-06-03 22:30:54 UTC (rev 9102)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-06-03 22:35:14 UTC (rev 9103)
@@ -65,7 +65,7 @@
 #include &lt;commands/PLearnCommands/HelpCommand.h&gt;
 // #include &lt;commands/PLearnCommands/JulianDateCommand.h&gt;
 // #include &lt;commands/PLearnCommands/KolmogorovSmirnovCommand.h&gt;
-// #include &lt;commands/PLearnCommands/LearnerCommand.h&gt;
+#include &lt;commands/PLearnCommands/LearnerCommand.h&gt;
 // #include &lt;commands/PLearnCommands/PairwiseDiffsCommand.h&gt;
 #include &lt;commands/PLearnCommands/ReadAndWriteCommand.h&gt;
 #include &lt;commands/PLearnCommands/RunCommand.h&gt;
@@ -77,8 +77,9 @@
 // #include &lt;commands/PLearnCommands/HTMLHelpCommand.h&gt;
 
 // //#include &lt;commands/PLearnCommands/TxtmatCommand.h&gt;
+#include &lt;commands/PLearnCommands/VerifyGradientCommand.h&gt;
+#include &lt;commands/PLearnCommands/ExtractOptionCommand.h&gt;
 
-
 // /**************
 //  * Dictionary *
 //  **************/
@@ -325,7 +326,7 @@
 // #include &lt;plearn_learners/unsupervised/Isomap.h&gt;
 // #include &lt;plearn_learners/unsupervised/KernelPCA.h&gt;
 // #include &lt;plearn_learners/unsupervised/LLE.h&gt;
-// #include &lt;plearn_learners/unsupervised/PCA.h&gt;
+#include &lt;plearn_learners/unsupervised/PCA.h&gt;
 // #include &lt;plearn_learners/unsupervised/SpectralClustering.h&gt;
 
 // Kernels
@@ -394,6 +395,10 @@
 #include &lt;plearn_learners/classifiers/KNNClassifier.h&gt;
 #include &lt;plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h&gt;
 
+// Stuff used for DiverseComponentAnalysis
+#include &lt;plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h&gt;
+
+
 using namespace PLearn;
 
 int main(int argc, char** argv)

Added: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2008-06-03 22:30:54 UTC (rev 9102)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.cc	2008-06-03 22:35:14 UTC (rev 9103)
@@ -0,0 +1,596 @@
+// -*- C++ -*-
+
+// DiverseComponentAnalysis.cc
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file DiverseComponentAnalysis.cc */
+
+
+#include &quot;DiverseComponentAnalysis.h&quot;
+#include &lt;plearn/vmat/VMat_basic_stats.h&gt;
+#include&lt;plearn/math/TMat_maths.h&gt;
+#include&lt;plearn/var/ProductVariable.h&gt;
+#include&lt;plearn/var/ProductTransposeVariable.h&gt;
+#include&lt;plearn/var/TransposeProductVariable.h&gt;
+#include&lt;plearn/var/SquareVariable.h&gt;
+#include&lt;plearn/var/AbsVariable.h&gt;
+#include&lt;plearn/var/SquareRootVariable.h&gt;
+#include&lt;plearn/var/ExpVariable.h&gt;
+#include&lt;plearn/var/TimesVariable.h&gt;
+#include&lt;plearn/var/SumVariable.h&gt;
+#include&lt;plearn/var/SigmoidVariable.h&gt;
+#include&lt;plearn/var/TanhVariable.h&gt;
+#include&lt;plearn/var/NegateElementsVariable.h&gt;
+#include&lt;plearn/var/TimesConstantVariable.h&gt;
+#include&lt;plearn/var/SumSquareVariable.h&gt;
+#include&lt;plearn/var/RowSumSquareVariable.h&gt;
+#include&lt;plearn/var/EXPERIMENTAL/ConstrainedSourceVariable.h&gt;
+#include&lt;plearn/var/EXPERIMENTAL/Cov2CorrVariable.h&gt;
+#include&lt;plearn/var/EXPERIMENTAL/DiagVariable.h&gt;
+#include&lt;plearn/var/EXPERIMENTAL/NonDiagVariable.h&gt;
+#include&lt;plearn/var/TransposeVariable.h&gt;
+#include&lt;plearn/var/ColumnSumVariable.h&gt;
+#include&lt;plearn/var/Var_operators.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DiverseComponentAnalysis,
+    &quot;Diverse Component Analysis&quot;,
+    &quot;This is an experimental class that finds linear\n&quot;
+    &quot;projection directions that should yield\n&quot;
+    &quot;'diverse' components, based on some diversity loss&quot;);
+
+DiverseComponentAnalysis::DiverseComponentAnalysis()
+    :ncomponents(2),
+     nonlinearity(&quot;none&quot;),
+     cov_transformation_type(&quot;cov&quot;),
+     diag_premul(1.0),
+     offdiag_premul(1.0),
+     diag_nonlinearity(&quot;square&quot;),
+     offdiag_nonlinearity(&quot;square&quot;),
+     diag_weight(-1.0),
+     offdiag_weight(1.0),
+     force_zero_mean(false),
+     epsilon(1e-8),
+     nu(0),
+     constrain_norm_type(-2),
+     normalize(false)
+/* ### Initialize all fields to their default value here */
+{
+    // ### If this learner needs to generate random numbers, uncomment the
+    // ### line below to enable the use of the inherited PRandom object.
+    random_gen = new PRandom();
+}
+
+void DiverseComponentAnalysis::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, &quot;myoption&quot;, &amp;DiverseComponentAnalysis::myoption,
+    //               OptionBase::buildoption,
+    //               &quot;Help text describing this option&quot;);
+    // ...
+
+    declareOption(
+        ol, &quot;nonlinearity&quot;, &amp;DiverseComponentAnalysis::nonlinearity, OptionBase::buildoption,
+        &quot;The nonlinearity to apply after linear transformation of the inpuits t ield the representation.&quot;);
+
+    declareOption(
+        ol, &quot;force_zero_mean&quot;, &amp;DiverseComponentAnalysis::force_zero_mean, OptionBase::buildoption,
+        &quot;If true then input mean won't be computes but forced to 0 (and a corresponding different covariance matrix will be computed)&quot;);
+
+    declareOption(
+        ol, &quot;epsilon&quot;, &amp;DiverseComponentAnalysis::epsilon, OptionBase::buildoption,
+        &quot;regularization value to add to diagonal of computed input covariance matrix.&quot;);
+
+    declareOption(
+        ol, &quot;nu&quot;, &amp;DiverseComponentAnalysis::nu, OptionBase::buildoption,
+        &quot;regularization parameter simulating destruction noise: \n&quot;
+        &quot;off-diagonal elements of covariance matrix Cx will be multiplied by 1-nu.&quot;);
+
+    declareOption(
+        ol, &quot;constrain_norm_type&quot;, &amp;DiverseComponentAnalysis::constrain_norm_type, OptionBase::buildoption,
+        &quot;How to constrain the norms of rows of W: \n&quot;
+        &quot;  -1: L1 norm constrained source \n&quot;
+        &quot;  -2: L2 norm constrained source \n&quot;
+        &quot;  -3:explicit L2 normalization \n&quot;
+        &quot;  &gt;0:add specified value times exp(sumsquare(W)) to the cost\n&quot;);
+
+    declareOption(
+        ol, &quot;ncomponents&quot;, &amp;DiverseComponentAnalysis::ncomponents, OptionBase::buildoption,
+        &quot;The number components to keep (that's also the outputsize).&quot;);
+
+    declareOption(
+        ol, &quot;cov_transformation_type&quot;, &amp;DiverseComponentAnalysis::cov_transformation_type, OptionBase::buildoption,
+        &quot;Controls the kind of transformation to apply to covariance matrix\n&quot;
+        &quot;cov: no transformation (keep covariance)\n&quot;
+        &quot;corr: transform into correlations, but keeping variances on the diagonal.\n&quot;
+        &quot;squaredist: do a 'squared distance kernel' Dij &lt;- Cii+Cjj-2Cij kind of transformation\n&quot;
+        &quot;sincov: instead of ||u|| ||v|| cos(angle(u,v)) we transform it to ||u|| ||v|| |sin(angle(u,v))|\n&quot;
+        &quot;        this is computed as sqrt((1-&lt;u.v&gt;^2) * &lt;u,u&gt;^2 * &lt;v,v&gt;^2) where &lt;u,v&gt; is given by the covariance matrix\n&quot;);
+
+    declareOption(
+        ol, &quot;diag_premul&quot;, &amp;DiverseComponentAnalysis::diag_premul, OptionBase::buildoption,
+        &quot;diagonal elements of Cy will be pre-multiplied by diag_premul (before applying non-linearity)&quot;);
+
+    declareOption(
+        ol, &quot;offdiag_premul&quot;, &amp;DiverseComponentAnalysis::offdiag_premul, OptionBase::buildoption,
+        &quot;Non-diagonal elements of Cy will be pre-multiplied by diag_premul (before applying non-linearity)&quot;);
+
+    declareOption(
+        ol, &quot;diag_nonlinearity&quot;, &amp;DiverseComponentAnalysis::diag_nonlinearity, OptionBase::buildoption,
+        &quot;The kind of nonlinearity to apply to the diagonal elements of Cy\n&quot;
+        &quot;after it's been through cov_transformation_type\n&quot;
+        &quot;Currently supported: none square abs sqrt sqrtabs exp tanh sigmoid&quot;);
+
+    declareOption(
+        ol, &quot;offdiag_nonlinearity&quot;, &amp;DiverseComponentAnalysis::offdiag_nonlinearity, OptionBase::buildoption,
+        &quot;The kind of nonlinearity to apply to the non-diagonal elements of Cy \n&quot;
+        &quot;after it's been through cov_transformation_type\n&quot;
+        &quot;Currently supported: none square abs sqrt sqrtabs exp tanh sigmoid&quot;);
+
+    declareOption(
+        ol, &quot;diag_weight&quot;, &amp;DiverseComponentAnalysis::diag_weight, OptionBase::buildoption,
+        &quot;what weight to give to the sum of transformed diagonal elements in the cost&quot;);
+
+    declareOption(
+        ol, &quot;offdiag_weight&quot;, &amp;DiverseComponentAnalysis::offdiag_weight, OptionBase::buildoption,
+        &quot;what weight to give to the sum of transformed non-diagonal elements in the cost&quot;);
+
+    declareOption(
+        ol, &quot;optimizer&quot;, &amp;DiverseComponentAnalysis::optimizer, OptionBase::buildoption,
+        &quot;The gradient-based optimizer to use&quot;);
+
+    declareOption(
+        ol, &quot;normalize&quot;, &amp;DiverseComponentAnalysis::normalize, OptionBase::buildoption,
+        &quot;If true computed outputs will be scaled so they have unit variance.\n&quot;
+        &quot;(see explanation about inv_stddev_of_projections)&quot;);
+
+
+    // learnt options
+    declareOption(
+        ol, &quot;mu&quot;, &amp;DiverseComponentAnalysis::mu, OptionBase::learntoption,
+        &quot;The (weighted) mean of the samples&quot;);
+
+    declareOption(
+        ol, &quot;Cx&quot;, &amp;DiverseComponentAnalysis::Cx, OptionBase::learntoption,
+        &quot;The (weighted) covariance of the samples&quot;);
+
+    declareOption(
+        ol, &quot;W&quot;, &amp;DiverseComponentAnalysis::W, OptionBase::learntoption,
+        &quot;A ncomponents x inputsize matrix containing the learnt projection directions&quot;);
+
+    declareOption(
+        ol, &quot;bias&quot;, &amp;DiverseComponentAnalysis::bias, OptionBase::learntoption,
+        &quot;A 1 x ncomponents matrix containing the learnt bias (for the nonlinear case only)&quot;);
+
+    declareOption(
+        ol, &quot;inv_stddev_of_projections&quot;, &amp;DiverseComponentAnalysis::inv_stddev_of_projections, OptionBase::learntoption,
+        &quot;As its name implies, this is one over the standard deviation of projected data.\n&quot;
+        &quot;when normalize=true computeOutput will multiply the projection by this,\n&quot;
+        &quot; elementwise, so that the output should have unit variance&quot; );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void DiverseComponentAnalysis::declareMethods(RemoteMethodMap&amp; rmm)
+{
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm,
+                  &quot;getVarValue&quot;,
+                  &amp;DiverseComponentAnalysis::getVarValue,
+                  (BodyDoc(&quot;Returns the matValue of the variable with the given name&quot;),
+                   ArgDoc(&quot;varname&quot;, &quot;name of the variable searched for&quot;),
+                   RetDoc(&quot;Returns the value of the var as a Mat&quot;)));
+
+    declareMethod(rmm,
+                  &quot;getVarGradient&quot;,
+                  &amp;DiverseComponentAnalysis::getVarGradient,
+                  (BodyDoc(&quot;Returns the matGradient of the variable with the given name&quot;),
+                   ArgDoc(&quot;varname&quot;, &quot;name of the variable searched for&quot;),
+                   RetDoc(&quot;Returns the gradient of the var as a Mat&quot;)));
+
+    declareMethod(rmm,
+                  &quot;listVarNames&quot;,
+                  &amp;DiverseComponentAnalysis::listVarNames,
+                  (BodyDoc(&quot;Returns a list of the names of all vars&quot;),
+                   RetDoc(&quot;Returns a list of the names of all vars&quot;)));
+
+}
+
+Var DiverseComponentAnalysis::nonlinear_transform(Var in, string nonlinearity)
+{
+    Var res; // result
+    if(nonlinearity==&quot;none&quot; || nonlinearity==&quot;linear&quot;)
+        res = in;
+    else if(nonlinearity==&quot;square&quot;)
+        res = square(in);
+    else if(nonlinearity==&quot;abs&quot;)
+        res = abs(in);
+    else if(nonlinearity==&quot;sqrt&quot;)
+        res = squareroot(in);
+    else if(nonlinearity==&quot;sqrtabs&quot;)
+        res = squareroot(abs(in));
+    else if(nonlinearity==&quot;exp&quot;)
+        res = exp(in);
+    else if(nonlinearity==&quot;tanh&quot;)
+        res = tanh(in);
+    else if(nonlinearity==&quot;sigmoid&quot;)
+        res = sigmoid(in);
+    else
+        PLERROR(&quot;Unknown nonlinearity %s&quot;,nonlinearity.c_str());
+    return res;
+}
+
+void DiverseComponentAnalysis::build_()
+{
+    perr &lt;&lt; &quot;Entering DiverseComponentAnalysis::build_()&quot; &lt;&lt; endl;
+    bool rebuild_all = inputsize_&gt;0 &amp;&amp; (W.isNull() || (W-&gt;matValue.width()!=inputsize_));
+    bool rebuild_some = inputsize_&gt;0 &amp;&amp; Cyt.isNull();
+    bool linear = (nonlinearity==&quot;none&quot; || nonlinearity==&quot;linear&quot;);
+    if(rebuild_some || rebuild_all)
+    {
+        perr &lt;&lt; &quot;Building with inputsize_ = &quot; &lt;&lt; inputsize_ &lt;&lt; endl;
+
+        Var nW;
+        
+        if(constrain_norm_type==-1) // use constrainted source to constrain L1 norms to 1
+        {
+            perr &lt;&lt; &quot;using constrainted source to constrain L1 norms to 1&quot; &lt;&lt; endl;
+            if(rebuild_all)
+                W = new ConstrainedSourceVariable(ncomponents,inputsize_,1);
+            nW = W;
+        }
+        else if(constrain_norm_type==-2) // use constrainted source to constrain L2 norms to 1
+        {
+            perr &lt;&lt; &quot;using constrainted source to constrain L2 norms to 1&quot; &lt;&lt; endl;
+            if(rebuild_all)
+                W = new ConstrainedSourceVariable(ncomponents,inputsize_,2);
+            nW = W;
+        }
+        else if(constrain_norm_type==-3) // compute L2 normalization explicitly
+        {
+            perr &lt;&lt; &quot;Normalizing explicitly&quot; &lt;&lt; endl;
+            if(rebuild_all)
+                W = Var(ncomponents,inputsize_);
+            nW = W/squareroot(rowSumSquare(W));
+        }
+        else  // using ordinary weight decay: nW is not hard-constrained to be normalized
+        {
+            perr &lt;&lt; &quot;Using ordinary weight decay &quot; &lt;&lt; constrain_norm_type &lt;&lt; endl;
+            if(rebuild_all)
+                W = Var(ncomponents,inputsize_);
+            nW = W;
+        }
+
+        if(linear) 
+        {
+            if(rebuild_all)
+                Cx = Var(inputsize_,inputsize_);
+            Cx-&gt;setName(&quot;Cx&quot;);
+            Cy = product(nW, productTranspose(Cx, nW));
+        }
+        else // nonlinear trasform
+        {
+            int l = train_set-&gt;length();
+            perr &lt;&lt; &quot;Building with nonlinear transform and l=&quot;&lt;&lt;l &lt;&lt;&quot; examples of inputsize=&quot; &lt;&lt; inputsize_ &lt;&lt; endl;
+
+            inputdata = Var(l,inputsize_);
+            if(rebuild_all)
+                bias = Var(1, ncomponents);
+            trdata = productTranspose(inputdata,nW)+bias;
+            perr &lt;&lt; &quot;USING MAIN REPRESENTATION NONLINEARITY: &quot; &lt;&lt; nonlinearity &lt;&lt; endl;
+            trdata = nonlinear_transform(trdata,nonlinearity);
+            if(force_zero_mean)
+                ctrdata = trdata;
+            else
+                ctrdata = trdata-(1.0/l)*columnSum(trdata);
+            ctrdata-&gt;setName(&quot;ctrdata&quot;);            
+            trdata-&gt;setName(&quot;trdata&quot;);            
+            Cy = (1.0/l)*transposeProduct(ctrdata,ctrdata);
+        }
+        perr &lt;&lt; &quot;Built Cy of size &quot; &lt;&lt; Cy-&gt;length() &lt;&lt; &quot;x&quot; &lt;&lt; Cy-&gt;width() &lt;&lt; endl;
+
+        if(cov_transformation_type==&quot;cov&quot;)
+            Cyt = Cy;
+        else if(cov_transformation_type==&quot;corr&quot;)
+            Cyt = cov2corr(Cy,2);
+        else if(cov_transformation_type==&quot;squaredist&quot;)
+        {
+            Var dCy = diag(Cy);
+            Cyt = Cy*(-2.0)+dCy+transpose(dCy);
+        }
+        else if(cov_transformation_type==&quot;sincov&quot;)
+        {
+            Var dCy = diag(Cy);
+            Cyt = squareroot(((1+1e-6)-square(cov2corr(Cy)))*dCy*transpose(dCy));            
+            // Cyt = ((1.0-square(cov2corr(Cy)))*dCy*transpose(dCy));            
+        }
+        else 
+            PLERROR(&quot;Invalid cov_transformation_type&quot;);
+
+        if(diag_weight!=0)
+            L += diag_weight*sum(nonlinear_transform(diag(Cyt*diag_premul),diag_nonlinearity));
+        if(offdiag_weight!=0)
+            L += offdiag_weight*sum(nonlinear_transform(nondiag(Cyt*offdiag_premul),offdiag_nonlinearity));
+            
+        if(constrain_norm_type&gt;0)
+            L += L+constrain_norm_type*exp(sumsquare(W));
+
+        if(!optimizer)
+            PLERROR(&quot;You must specify the optimizer field (ex: GradientOptimizer)&quot;);
+        if(linear)
+            optimizer-&gt;setToOptimize(W, L);
+        else
+            optimizer-&gt;setToOptimize(W&amp;bias, L);
+        
+        perr &lt;&lt; &quot;Built optimizer&quot; &lt;&lt; endl;
+        nW-&gt;setName(&quot;W&quot;);
+        Cy-&gt;setName(&quot;Cy&quot;);
+        Cyt-&gt;setName(&quot;Cyt&quot;);
+        L-&gt;setName(&quot;L&quot;);
+
+        allvars = Cx &amp; trdata&amp; ctrdata&amp; nW &amp; Cy &amp; Cyt &amp; L;
+    }
+    perr &lt;&lt; &quot;Exiting DiverseComponentAnalysis::build_()&quot; &lt;&lt; endl;
+}
+
+
+TVec&lt;string&gt; DiverseComponentAnalysis::listVarNames() const
+{
+    int n = allvars.length();
+    TVec&lt;string&gt; names;
+    for(int i=0; i&lt;n; i++)
+        if(allvars[i].isNotNull())
+            names.append(allvars[i]-&gt;getName());
+    return names;
+}
+
+Mat DiverseComponentAnalysis::getVarValue(string varname) const
+{
+    for(int i=0; i&lt;allvars.length(); i++)
+    {
+        Var v = allvars[i];        
+        if(v.isNotNull() &amp;&amp; v-&gt;getName()==varname)
+            return v-&gt;matValue;
+    }
+    PLERROR(&quot;No Var with name %s&quot;, varname.c_str());
+    return Mat();
+}
+
+Mat DiverseComponentAnalysis::getVarGradient(string varname) const
+{
+    for(int i=0; i&lt;allvars.length(); i++)
+    {
+        Var v = allvars[i];
+        if(v.isNotNull() &amp;&amp; v-&gt;getName()==varname)
+            return v-&gt;matGradient;
+    }
+    PLERROR(&quot;No Var with name %s&quot;, varname.c_str());
+    return Mat();
+}
+
+void DiverseComponentAnalysis::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void DiverseComponentAnalysis::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(mu, copies);
+    deepCopyField(Cx, copies);
+    deepCopyField(W, copies);
+}
+
+
+int DiverseComponentAnalysis::outputsize() const
+{
+    return ncomponents;
+}
+
+void DiverseComponentAnalysis::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+
+    // this will reset stage=0 and reset the random_gen to the initial seed_
+    inherited::forget();
+
+    perr &lt;&lt; &quot;Called DCS::forget() with inputsize_ = &quot; &lt;&lt; inputsize_ &lt;&lt; endl;
+    if(inputsize_&gt;0)
+    {
+        random_gen-&gt;fill_random_normal(W-&gt;value, 0., 1.);
+        perr &lt;&lt; &quot;Squared norm of first row of W after fill_random_normal: &quot; &lt;&lt; pownorm(W-&gt;matValue(0)) &lt;&lt; endl;
+        int normval = (constrain_norm_type==-1 ?1 :2);
+        for(int i=0; i&lt;ncomponents; i++)
+            PLearn::normalize(W-&gt;matValue(i), normval);
+        perr &lt;&lt; &quot;Squared norm of first row of W after L&quot; &lt;&lt; normval &lt;&lt; &quot; normalization: &quot; &lt;&lt; pownorm(W-&gt;matValue(0)) &lt;&lt; endl;
+    }
+}
+
+void DiverseComponentAnalysis::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    */
+
+    if (!initTrain())
+        return;
+
+    while(stage&lt;nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats-&gt;forget();
+
+        if(stage==0) // do stage 1
+        {
+            bool linear = (nonlinearity==&quot;none&quot; || nonlinearity==&quot;linear&quot;);
+            if(!linear)
+            {
+                perr &lt;&lt; &quot;Nonlinear training to stage 1&quot; &lt;&lt; endl;
+                Mat X = inputdata-&gt;matValue;
+                int l = train_set-&gt;length();
+                Vec target;
+                real weight;
+                for(int i=0; i&lt;l; i++)
+                {
+                    Vec Xi = X(i);
+                    train_set-&gt;getExample(i,Xi,target,weight);
+                }
+                mu.resize(inputsize_);
+                columnMean(X, mu);
+                perr &lt;&lt; &quot;Nonlinear training to stage 1. DONE.&quot; &lt;&lt; endl;
+            }
+            else // linear case
+            {
+                if(force_zero_mean)
+                {
+                    mu.resize(inputsize());
+                    mu.fill(0);
+                    computeInputCovar(train_set, mu, Cx-&gt;matValue, epsilon);
+                }
+                else
+                    computeInputMeanAndCovar(train_set, mu, Cx-&gt;matValue, epsilon);
+
+                if(nu!=0)
+                {
+                    Mat C = Cx-&gt;matValue;
+                    int l = C.length();
+                    for(int i=0; i&lt;l; i++)
+                        for(int j=0; j&lt;l; j++)
+                            if(i!=j)
+                                C(i,j) *= (1-nu);
+                }
+            }
+        }
+        else
+        {
+            optimizer-&gt;optimizeN(*train_stats);
+            Mat C = Cy-&gt;matValue;
+            int l = C.length();            
+            inv_stddev_of_projections.resize(l);
+            for(int i=0; i&lt;l; i++)
+                inv_stddev_of_projections = 1.0/sqrt(C(i,i));
+        }
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set-&gt;getExample(input, target, weight)
+        // and train_stats-&gt;update(train_costs)
+
+        ++stage;
+        train_stats-&gt;finalize(); // finalize statistics for this epoch
+    }
+}
+
+
+void DiverseComponentAnalysis::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    static Vec x;
+    x.resize(input.length());
+    x &lt;&lt; input;
+
+    // Center and project on directions
+    x -= mu;
+    output.resize(ncomponents);
+    product(output, W-&gt;matValue, x);
+    if(normalize)
+        output *= inv_stddev_of_projections;
+}
+
+void DiverseComponentAnalysis::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+    costs.resize(0);
+}
+
+TVec&lt;string&gt; DiverseComponentAnalysis::getTestCostNames() const
+{
+    return TVec&lt;string&gt;();
+}
+
+TVec&lt;string&gt; DiverseComponentAnalysis::getTrainCostNames() const
+{
+    return TVec&lt;string&gt;(1,&quot;L&quot;);
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h
===================================================================
--- trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h	2008-06-03 22:30:54 UTC (rev 9102)
+++ trunk/plearn_learners/unsupervised/EXPERIMENTAL/DiverseComponentAnalysis.h	2008-06-03 22:35:14 UTC (rev 9103)
@@ -0,0 +1,256 @@
+// -*- C++ -*-
+
+// DiverseComponentAnalysis.h
+//
+// Copyright (C) 2008 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file DiverseComponentAnalysis.h */
+
+
+#ifndef DiverseComponentAnalysis_INC
+#define DiverseComponentAnalysis_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+#include &lt;plearn/var/Var.h&gt;
+#include &lt;plearn/opt/Optimizer.h&gt;
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class DiverseComponentAnalysis : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! The number of components to keep (that's also the outputsize)    
+    int ncomponents;
+    string nonlinearity;
+
+    string cov_transformation_type;
+
+    double diag_premul;
+    double offdiag_premul;
+
+    string diag_nonlinearity;
+    string offdiag_nonlinearity;
+
+    double diag_weight;
+    double offdiag_weight;
+
+    bool force_zero_mean;
+    real epsilon; // regularization value to add to diagonal of computed covariance matrix
+    real nu; // regularization: off-diagonal elements of input covariance will be multiplied by 1-nu
+    real constrain_norm_type; // how to constrain the norms of rows of W: -1:constrained source; -2:explicit normalization; &gt;0:ordinary weight decay
+
+    bool normalize;
+
+    PP&lt;Optimizer&gt; optimizer;
+
+    // *******************
+    // * learned options *
+    // *******************
+
+    //! The (weighted) mean of the samples 
+    Vec mu;
+
+    //! The (weighted) covariance matrix of the samples
+    Var Cx;
+
+    //! A ncomponents x inputsize matrix containing the projection directions
+    Var W; 
+
+    //! The covariance of transformed data Cy = cov(W x) = W Cx W^T
+    Var Cy;
+
+    //! The &quot;transformed&quot; covariance according to cov_transformation_type
+    Var Cyt;
+
+    //! The loss variable: sum(asCy)
+    Var L;
+
+    Vec inv_stddev_of_projections;
+
+    // For the nonlinear case
+    Var inputdata; // whole input data matrix
+    Var bias;
+    Var trdata;  // transformed data
+    Var ctrdata; // centered transformed data
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    DiverseComponentAnalysis();
+
+    //! Returns the value of the var with the specified name (searched in VarArray allvars)
+    Mat getVarValue(string varname) const;
+
+    //! Returns the gradient of the var with the specified name (searched in VarArray allvars)
+    Mat getVarGradient(string varname) const;
+
+    //! Returns the names of all vars (in VarArray allvars)
+    TVec&lt;string&gt; listVarNames() const;
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+    //                                    Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(DiverseComponentAnalysis);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList&amp; ol);
+    static void declareMethods(RemoteMethodMap&amp; rmm);
+    static Var nonlinear_transform(Var in, string nonlinearity);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+    VarArray allvars;
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DiverseComponentAnalysis);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002550.html">[Plearn-commits] r9102 - trunk/commands/PLearnCommands
</A></li>
	<LI>Next message: <A HREF="002552.html">[Plearn-commits] r9104 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2551">[ date ]</a>
              <a href="thread.html#2551">[ thread ]</a>
              <a href="subject.html#2551">[ subject ]</a>
              <a href="author.html#2551">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
