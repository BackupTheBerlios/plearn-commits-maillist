<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9176 - trunk/plearn_learners/generic
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9176%20-%20trunk/plearn_learners/generic&In-Reply-To=%3C200806261749.m5QHnZ3r030227%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002623.html">
   <LINK REL="Next"  HREF="002625.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9176 - trunk/plearn_learners/generic</H1>
    <B>tihocan at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9176%20-%20trunk/plearn_learners/generic&In-Reply-To=%3C200806261749.m5QHnZ3r030227%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9176 - trunk/plearn_learners/generic">tihocan at mail.berlios.de
       </A><BR>
    <I>Thu Jun 26 19:49:35 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002623.html">[Plearn-commits] r9175 - in trunk: python_modules/plearn/parallel	scripts
</A></li>
        <LI>Next message: <A HREF="002625.html">[Plearn-commits] r9177 - trunk/plearn/var
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2624">[ date ]</a>
              <a href="thread.html#2624">[ thread ]</a>
              <a href="subject.html#2624">[ subject ]</a>
              <a href="author.html#2624">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: tihocan
Date: 2008-06-26 19:49:35 +0200 (Thu, 26 Jun 2008)
New Revision: 9176

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/NNet.h
Log:
Moved some code in a new virtual method 'getCost' that can be overridden in subclasses

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-06-26 16:50:52 UTC (rev 9175)
+++ trunk/plearn_learners/generic/NNet.cc	2008-06-26 17:49:35 UTC (rev 9176)
@@ -579,87 +579,8 @@
     costs.resize(ncosts);
 
     for (int k=0; k&lt;ncosts; k++)
-    {
-        // create costfuncs and apply individual weights if weightpart &gt; 1
-        if (cost_funcs[k]==&quot;mse&quot;)
-            costs[k]= sumsquare(the_output-the_target);
-        else if (cost_funcs[k]==&quot;mse_onehot&quot;)
-            costs[k] = onehot_squared_loss(the_output, the_target);
-        else if (cost_funcs[k]==&quot;NLL&quot;) 
-        {
-            if (the_output-&gt;width() == 1) {
-                // Assume sigmoid output here!
-                costs[k] = stable_cross_entropy(before_transfer_func, the_target);
-            } else {
-                if (output_transfer_func == &quot;log_softmax&quot;)
-                    costs[k] = -the_output[the_target];
-                else
-                    costs[k] = neg_log_pi(the_output, the_target);
-            }
-        } 
-        else if (cost_funcs[k]==&quot;class_error&quot;)
-        {
-            if (the_output-&gt;width()==1)
-                costs[k] = binary_classification_loss(the_output, the_target);
-            else
-                costs[k] = classification_loss(the_output, the_target);
-        }
-        else if (cost_funcs[k]==&quot;binary_class_error&quot;)
-            costs[k] = binary_classification_loss(the_output, the_target);
-        else if (cost_funcs[k]==&quot;multiclass_error&quot;)
-            costs[k] = multiclass_loss(the_output, the_target);
-        else if (cost_funcs[k]==&quot;cross_entropy&quot;)
-            costs[k] = cross_entropy(the_output, the_target);
-        else if(cost_funcs[k]==&quot;conf_rated_adaboost_cost&quot;)
-        {
-            if(output_transfer_func != &quot;sigmoid&quot;)
-                PLWARNING(&quot;In NNet:buildCosts(): conf_rated_adaboost_cost expects an output in (0,1)&quot;);
-            alpha_adaboost = Var(1,1); alpha_adaboost-&gt;value[0] = 1.0;
-            params.append(alpha_adaboost);
-            costs[k] = conf_rated_adaboost_cost(the_output, the_target, alpha_adaboost);
-        }
-        else if (cost_funcs[k]==&quot;gradient_adaboost_cost&quot;)
-        {
-            if(output_transfer_func != &quot;sigmoid&quot;)
-                PLWARNING(&quot;In NNet:buildCosts(): gradient_adaboost_cost expects an output in (0,1)&quot;);
-            costs[k] = gradient_adaboost_cost(the_output, the_target);
-        }
-        else if (cost_funcs[k]==&quot;stable_cross_entropy&quot;) {
-            Var c = stable_cross_entropy(before_transfer_func, the_target);
-            costs[k] = c;
-            PLASSERT( classification_regularizer &gt;= 0 );
-            if (classification_regularizer &gt; 0) {
-                // There is a regularizer to add to the cost function.
-                dynamic_cast&lt;NegCrossEntropySigmoidVariable*&gt;((Variable*) c)-&gt;
-                    setRegularizer(classification_regularizer);
-            }
-        }
-        else if (cost_funcs[k]==&quot;margin_perceptron_cost&quot;)
-            costs[k] = margin_perceptron_cost(the_output,the_target,margin);
-        else if (cost_funcs[k]==&quot;lift_output&quot;)
-            costs[k] = lift_output(the_output, the_target);
-        else if (cost_funcs[k]==&quot;poisson_nll&quot;) {
-            VarArray the_varray(the_output, the_target);
-            if (weightsize()&gt;0)
-                the_varray.push_back(sampleweight);
-            costs[k] = neglogpoissonvariable(the_varray);
-        }
-        else if (cost_funcs[k] == &quot;L1&quot;)
-            costs[k] = sumabs(the_output - the_target);
-        else {
-            // Assume we got a Variable name and its options                
-            costs[k]= dynamic_cast&lt;Variable*&gt;(newObject(cost_funcs[k]));
-            if(costs[k].isNull())
-                PLERROR(&quot;In NNet::build_()  unknown cost_func option: %s&quot;,cost_funcs[k].c_str());
-            costs[k]-&gt;setParents(the_output &amp; the_target);
-            costs[k]-&gt;build();
-        }
+        costs[k] = getCost(cost_funcs[k], the_output, the_target, before_transfer_func);
 
-        // We don't need to take into account the sampleweight, because it is
-        // taken care of in stats-&gt;update.
-    }
-
-
     /*
      * weight and bias decay penalty
      */
@@ -1049,6 +970,95 @@
     n_training_bags = -1;
 }
 
+/////////////
+// getCost //
+/////////////
+Var NNet::getCost(const string&amp; costname, const Var&amp; the_output,
+                  const Var&amp; the_target, const Var&amp; before_transfer_func)
+{
+    // We don't need to take into account the sampleweight, because it is
+    // taken care of in stats-&gt;update.
+    if (costname==&quot;mse&quot;)
+        return sumsquare(the_output-the_target);
+    else if (costname==&quot;mse_onehot&quot;)
+        return onehot_squared_loss(the_output, the_target);
+    else if (costname==&quot;NLL&quot;) 
+    {
+        if (the_output-&gt;width() == 1) {
+            // Assume sigmoid output here!
+            return stable_cross_entropy(before_transfer_func, the_target);
+        } else {
+            if (output_transfer_func == &quot;log_softmax&quot;)
+                return -the_output[the_target];
+            else
+                return neg_log_pi(the_output, the_target);
+        }
+    } 
+    else if (costname==&quot;class_error&quot;)
+    {
+        if (the_output-&gt;width()==1)
+            return binary_classification_loss(the_output, the_target);
+        else
+            return classification_loss(the_output, the_target);
+    }
+    else if (costname==&quot;binary_class_error&quot;)
+        return binary_classification_loss(the_output, the_target);
+    else if (costname==&quot;multiclass_error&quot;)
+        return multiclass_loss(the_output, the_target);
+    else if (costname==&quot;cross_entropy&quot;)
+        return cross_entropy(the_output, the_target);
+    else if(costname==&quot;conf_rated_adaboost_cost&quot;)
+    {
+        if(output_transfer_func != &quot;sigmoid&quot;)
+            PLWARNING(&quot;In NNet:buildCosts(): conf_rated_adaboost_cost expects an output in (0,1)&quot;);
+        alpha_adaboost = Var(1,1); alpha_adaboost-&gt;value[0] = 1.0;
+        params.append(alpha_adaboost);
+        return conf_rated_adaboost_cost(the_output, the_target, alpha_adaboost);
+    }
+    else if (costname==&quot;gradient_adaboost_cost&quot;)
+    {
+        if(output_transfer_func != &quot;sigmoid&quot;)
+            PLWARNING(&quot;In NNet:buildCosts(): gradient_adaboost_cost expects an output in (0,1)&quot;);
+        return gradient_adaboost_cost(the_output, the_target);
+    }
+    else if (costname==&quot;stable_cross_entropy&quot;) {
+        Var c = stable_cross_entropy(before_transfer_func, the_target);
+        PLASSERT( classification_regularizer &gt;= 0 );
+        if (classification_regularizer &gt; 0) {
+            // There is a regularizer to add to the cost function.
+            dynamic_cast&lt;NegCrossEntropySigmoidVariable*&gt;((Variable*) c)-&gt;
+                setRegularizer(classification_regularizer);
+        }
+        return c;
+    }
+    else if (costname==&quot;margin_perceptron_cost&quot;)
+        return margin_perceptron_cost(the_output,the_target,margin);
+    else if (costname==&quot;lift_output&quot;)
+        return lift_output(the_output, the_target);
+    else if (costname==&quot;poisson_nll&quot;) {
+        VarArray the_varray(the_output, the_target);
+        if (weightsize()&gt;0) {
+            PLERROR(&quot;In NNet::getCost - The weight is used, is this really &quot;
+                    &quot;intended? (see comment in code at the top of this &quot;
+                    &quot;method&quot;);
+            the_varray.push_back(sampleweight);
+        }
+        return neglogpoissonvariable(the_varray);
+    }
+    else if (costname == &quot;L1&quot;)
+        return sumabs(the_output - the_target);
+    else {
+        // Assume we got a Variable name and its options                
+        Var cost = dynamic_cast&lt;Variable*&gt;(newObject(costname));
+        if(cost.isNull())
+            PLERROR(&quot;In NNet::build_() - unknown cost name: %s&quot;,
+                    costname.c_str());
+        cost-&gt;setParents(the_output &amp; the_target);
+        cost-&gt;build();
+        return cost;
+    }
+}
+
 ///////////////////////
 // getTrainCostNames //
 ///////////////////////

Modified: trunk/plearn_learners/generic/NNet.h
===================================================================
--- trunk/plearn_learners/generic/NNet.h	2008-06-26 16:50:52 UTC (rev 9175)
+++ trunk/plearn_learners/generic/NNet.h	2008-06-26 17:49:35 UTC (rev 9176)
@@ -267,6 +267,11 @@
     //! Build the costs variable from other variables.
     void buildCosts(const Var&amp; output, const Var&amp; target, const Var&amp; hidden_layer, const Var&amp; before_transfer_func);
 
+    //! Return the cost corresponding to the given cost name. This method is
+    //! virtual so that subclasses can implement their own custom costs.
+    virtual Var getCost(const string&amp; costname, const Var&amp; output,
+                        const Var&amp; target, const Var&amp; before_transfer_func);
+
     //! Build the various functions used in the network.
     void buildFuncs(const Var&amp; the_input, const Var&amp; the_output, const Var&amp; the_target, const Var&amp; the_sampleweight, const Var&amp; the_bag_size);
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002623.html">[Plearn-commits] r9175 - in trunk: python_modules/plearn/parallel	scripts
</A></li>
	<LI>Next message: <A HREF="002625.html">[Plearn-commits] r9177 - trunk/plearn/var
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2624">[ date ]</a>
              <a href="thread.html#2624">[ thread ]</a>
              <a href="subject.html#2624">[ subject ]</a>
              <a href="author.html#2624">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
