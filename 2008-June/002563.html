<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9115 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9115%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806110237.m5B2bTO5026001%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002562.html">
   <LINK REL="Next"  HREF="002564.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9115 - trunk/plearn_learners/online</H1>
    <B>lamblin at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9115%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806110237.m5B2bTO5026001%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9115 - trunk/plearn_learners/online">lamblin at mail.berlios.de
       </A><BR>
    <I>Wed Jun 11 04:37:29 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002562.html">[Plearn-commits] r9114 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002564.html">[Plearn-commits] r9116 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2563">[ date ]</a>
              <a href="thread.html#2563">[ thread ]</a>
              <a href="subject.html#2563">[ subject ]</a>
              <a href="author.html#2563">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: lamblin
Date: 2008-06-11 04:37:22 +0200 (Wed, 11 Jun 2008)
New Revision: 9115

Modified:
   trunk/plearn_learners/online/BinarizeModule.cc
   trunk/plearn_learners/online/CombiningCostsModule.cc
   trunk/plearn_learners/online/CostModule.h
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/CrossEntropyCostModule.h
   trunk/plearn_learners/online/ForwardModule.cc
   trunk/plearn_learners/online/ForwardModule.h
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/GradNNetLayerModule.h
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
   trunk/plearn_learners/online/LinearCombinationModule.cc
   trunk/plearn_learners/online/LinearCombinationModule.h
   trunk/plearn_learners/online/LinearFilterModule.cc
   trunk/plearn_learners/online/LinearFilterModule.h
   trunk/plearn_learners/online/LogaddOnBagsModule.cc
   trunk/plearn_learners/online/LogaddOnBagsModule.h
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/ModuleLearner.h
   trunk/plearn_learners/online/ModuleStackModule.h
   trunk/plearn_learners/online/ModuleTester.cc
   trunk/plearn_learners/online/NetworkConnection.h
   trunk/plearn_learners/online/NetworkModule.cc
   trunk/plearn_learners/online/NetworkModule.h
   trunk/plearn_learners/online/OnBagsModule.cc
   trunk/plearn_learners/online/OnBagsModule.h
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMatrixConnection.cc
   trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
   trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc
   trunk/plearn_learners/online/RBMMultitaskClassificationModule.h
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/SoftmaxModule.h
   trunk/plearn_learners/online/SplitModule.cc
   trunk/plearn_learners/online/SplitModule.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
   trunk/plearn_learners/online/TanhModule.cc
   trunk/plearn_learners/online/VBoundDBN2.cc
   trunk/plearn_learners/online/VBoundDBN2.h
Log:
Whitespace and tabs fix.


Modified: trunk/plearn_learners/online/BinarizeModule.cc
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/BinarizeModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -110,7 +110,7 @@
     Mat* output = ports_value[1];
     Mat* input_gradient = ports_gradient[0];
     Mat* output_gradient = ports_gradient[1];
-    
+
     int mbs=output-&gt;length();
     if (input_gradient)
     {

Modified: trunk/plearn_learners/online/CombiningCostsModule.cc
===================================================================
--- trunk/plearn_learners/online/CombiningCostsModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CombiningCostsModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -81,7 +81,7 @@
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
-    
+
     redeclareOption(ol, &quot;input_size&quot;, &amp;CombiningCostsModule::input_size,
                     OptionBase::learntoption,
                     &quot;Is set to sub_costs[0]-&gt;input_size.&quot;);
@@ -127,13 +127,13 @@
             PLERROR( &quot;CombiningCostsModule::build_(): sub_costs[%d]-&gt;input_size&quot;
                      &quot; (%d)\n&quot;
                      &quot;should be equal to %d.\n&quot;,
-                     i,sub_costs[i]-&gt;input_size, input_size);  
+                     i,sub_costs[i]-&gt;input_size, input_size);
 
         if(sub_costs[i]-&gt;target_size != target_size)
             PLERROR( &quot;CombiningCostsModule::build_(): sub_costs[%d]-&gt;target_size&quot;
                      &quot; (%d)\n&quot;
                      &quot;should be equal to %d.\n&quot;,
-                     i,sub_costs[i]-&gt;target_size, target_size);  
+                     i,sub_costs[i]-&gt;target_size, target_size);
     }
 
     sub_costs_values.resize( n_sub_costs );

Modified: trunk/plearn_learners/online/CostModule.h
===================================================================
--- trunk/plearn_learners/online/CostModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CostModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -113,7 +113,7 @@
     // TODO Had to override to compile, this is weird.
     virtual void bpropUpdate(const Mat&amp; input, const Mat&amp; output,
                              Mat&amp; input_gradient, const Mat&amp; output_gradient,
-                             bool accumulate=false) 
+                             bool accumulate=false)
     {
         inherited::bpropUpdate(input, output, input_gradient, output_gradient,
                 accumulate);

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -125,7 +125,7 @@
 
     for (int i = 0; i &lt; inputs.length(); i++)
         fprop(inputs(i), targets(i), costs(i,0));
- 
+
 }
 
 void CrossEntropyCostModule::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
@@ -166,8 +166,8 @@
         prediction_grad-&gt;resize(batch_size, getPortSizes()(0,1));
 
         for( int i=0; i &lt; batch_size; i++ )
-            for ( int j=0; j &lt; target-&gt;width(); j++ ) 
-                (*prediction_grad)(i, j) += 
+            for ( int j=0; j &lt; target-&gt;width(); j++ )
+                (*prediction_grad)(i, j) +=
                 (*cost_grad)(i,0)*((*target)(i,j) - sigmoid(-(*prediction)(i,j) ));
     }
 

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.h
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -67,7 +67,7 @@
     virtual void fprop(const Vec&amp; input, const Vec&amp; target, real&amp; cost) const;
     virtual void fprop(const Vec&amp; input, const Vec&amp; target, Vec&amp; cost) const;
     virtual void fprop(const Mat&amp; input, const Mat&amp; target, Mat&amp; cost) const;
-   
+
     //! New version of backpropagation
     virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
                                 const TVec&lt;Mat*&gt;&amp; ports_gradient);

Modified: trunk/plearn_learners/online/ForwardModule.cc
===================================================================
--- trunk/plearn_learners/online/ForwardModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ForwardModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -87,7 +87,7 @@
         &quot;will be forgotten.&quot;);
 
     // 'nosave' options.
-    
+
     // 'current' is an option only so it can be modified in server mode for
     // instance, in order to bypass a call to build (yes, this is a hack!).
     declareOption(ol, &quot;current&quot;, &amp;ForwardModule::current,

Modified: trunk/plearn_learners/online/ForwardModule.h
===================================================================
--- trunk/plearn_learners/online/ForwardModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ForwardModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -156,7 +156,7 @@
 
     //! Index of the module currently being used in the 'modules' list.
     int current;
-    
+
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -95,7 +95,7 @@
 
     // Add bias.
     resizeOnes(n);
-    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical 
+    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical
 }
 
 /////////////////
@@ -282,8 +282,8 @@
     }
     step_number += n;
 }
-    
 
+
 //////////////////
 // bbpropUpdate //
 //////////////////

Modified: trunk/plearn_learners/online/GradNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/GradNNetLayerModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -122,7 +122,7 @@
                              Mat&amp; input_gradients,
                              const Mat&amp; output_gradients,
                              bool accumulate = false);
-    
+
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               const Vec&amp; output_gradient,
                               const Vec&amp; output_diag_hessian);
@@ -155,7 +155,7 @@
 
     //! A vector filled with all ones.
     Vec ones;
-    
+
     //#####  Protected Options  ###############################################
 
 protected:

Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -95,15 +95,15 @@
     declareOption(ol, &quot;nstages_max&quot;, &amp;LayerCostModule::nstages_max,
                   OptionBase::buildoption,
         &quot;Maximal number of updates for which the gradient of the cost function will be propagated.\n&quot;
-	&quot;-1 means: always train without limit.\n&quot;
+        &quot;-1 means: always train without limit.\n&quot;
         );
 
     declareOption(ol, &quot;optimization_strategy&quot;, &amp;LayerCostModule::optimization_strategy,
                   OptionBase::buildoption,
         &quot;Strategy to compute the gradient:\n&quot;
-	&quot;- \&quot;standard\&quot;: standard computation\n&quot;
-	&quot;- \&quot;half\&quot;: we will propagate the gradient only on units tagged as i &lt; j.\n&quot;
-	&quot;- \&quot;random_half\&quot;: idem than 'half' with the order of the indices that changes randomly during training.\n&quot;
+        &quot;- \&quot;standard\&quot;: standard computation\n&quot;
+        &quot;- \&quot;half\&quot;: we will propagate the gradient only on units tagged as i &lt; j.\n&quot;
+        &quot;- \&quot;random_half\&quot;: idem than 'half' with the order of the indices that changes randomly during training.\n&quot;
         );
 
     declareOption(ol, &quot;momentum&quot;, &amp;LayerCostModule::momentum,
@@ -202,22 +202,22 @@
             inputs_histo.resize(input_size,histo_size);
         HISTO_STEP = 1.0/(real)histo_size;
 
-	if( cost_function == &quot;kl_div&quot; )
-	{
-	    cache_differ_count_i.resize(input_size);
-	    cache_differ_count_j.resize(input_size);
-	    cache_n_differ.resize(input_size);
-	    for( int i = 0; i &lt; input_size; i ++)
-	    {
-	        cache_differ_count_i[i].resize(i);
-	        cache_differ_count_j[i].resize(i);
-	        cache_n_differ[i].resize(i);
-  	        for( int j = 0; j &lt; i; j ++)
-	        {
-	            cache_differ_count_i[i][j].resize(histo_size);
-		    cache_differ_count_j[i][j].resize(histo_size);
-		    cache_n_differ[i][j].resize(histo_size);
-	        }
+        if( cost_function == &quot;kl_div&quot; )
+        {
+            cache_differ_count_i.resize(input_size);
+            cache_differ_count_j.resize(input_size);
+            cache_n_differ.resize(input_size);
+            for( int i = 0; i &lt; input_size; i ++)
+            {
+                cache_differ_count_i[i].resize(i);
+                cache_differ_count_j[i].resize(i);
+                cache_n_differ[i].resize(i);
+                for( int j = 0; j &lt; i; j ++)
+                {
+                    cache_differ_count_i[i][j].resize(histo_size);
+                    cache_differ_count_j[i][j].resize(histo_size);
+                    cache_n_differ[i][j].resize(histo_size);
+                }
             }
         }
     }
@@ -279,7 +279,7 @@
 
     inputs_expectation.clear();
     inputs_stds.clear();
-    
+
     inputs_correlations.clear();
     inputs_cross_quadratic_mean.clear();
     if( momentum &gt; 0.0)
@@ -310,7 +310,7 @@
     deepCopyField(cache_differ_count_i, copies);
     deepCopyField(cache_differ_count_j, copies);
     deepCopyField(cache_n_differ, copies);
-    
+
     deepCopyField(ports, copies);
 }
 
@@ -355,7 +355,7 @@
     }
     else
         costs.clear();
-    
+
     if( !is_cost_function_stochastic )
     {
         PLASSERT( inputs.width() == input_size );
@@ -388,8 +388,8 @@
         //! ************************************************************
 
 
-	    Mat histo;
-	    computeHisto( inputs, histo );
+            Mat histo;
+            computeHisto( inputs, histo );
             costs(0,0) = computeKLdiv( histo );
         }
         else if( cost_function == &quot;kl_div_simple&quot; )
@@ -404,7 +404,7 @@
         //! ************************************************************
 
             Mat histo;
-	    computeSafeHisto( inputs, histo );
+            computeSafeHisto( inputs, histo );
 
             // Computing the KL divergence
             for (int i = 0; i &lt; input_size; i++)
@@ -430,8 +430,8 @@
         //! ************************************************************
 
             Vec expectation;
-	    Mat cross_quadratic_mean;
-	    computePascalStatistics( inputs, expectation, cross_quadratic_mean );
+            Mat cross_quadratic_mean;
+            computePascalStatistics( inputs, expectation, cross_quadratic_mean );
 
             // Computing the cost
             for (int i = 0; i &lt; input_size; i++)
@@ -461,9 +461,9 @@
         //! ************************************************************
 
             Vec expectation;
-	    Mat cross_quadratic_mean;
+            Mat cross_quadratic_mean;
             Vec stds;
-	    Mat correlations;
+            Mat correlations;
             computeCorrelationStatistics( inputs, expectation, cross_quadratic_mean, stds, correlations );
 
             // Computing the cost
@@ -605,18 +605,18 @@
     if( p_inputs_grad &amp;&amp; p_inputs_grad-&gt;isEmpty()
         &amp;&amp; p_cost_grad &amp;&amp; !p_cost_grad-&gt;isEmpty() )
     {
-	PLASSERT( p_inputs &amp;&amp; !p_inputs-&gt;isEmpty());
+        PLASSERT( p_inputs &amp;&amp; !p_inputs-&gt;isEmpty());
         int n_samples = p_inputs-&gt;length();
-	PLASSERT( p_cost_grad-&gt;length() == n_samples );
-	PLASSERT( p_cost_grad-&gt;width() == 1 );
+        PLASSERT( p_cost_grad-&gt;length() == n_samples );
+        PLASSERT( p_cost_grad-&gt;width() == 1 );
 
         bpropUpdate( *p_inputs, *p_inputs_grad);
 
         for( int isample = 0; isample &lt; n_samples; isample++ )
-	    for( int i = 0; i &lt; input_size; i++ )
-	        (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0);
+            for( int i = 0; i &lt; input_size; i++ )
+                (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0);
 
-	checkProp(ports_gradient);
+        checkProp(ports_gradient);
     }
     else if( !p_inputs_grad &amp;&amp; !p_cost_grad )
         return;
@@ -653,8 +653,8 @@
         for (int isample = 0; isample &lt; n_samples; isample++)
         {
             real qi, qj, comp_qi, comp_qj;
-	    Vec comp_q(input_size), log_term(input_size);
-	    for (int i = 0 ; i &lt; input_size ; i++ )
+            Vec comp_q(input_size), log_term(input_size);
+            for (int i = 0 ; i &lt; input_size ; i++ )
             {
                 qi = inputs(isample,i);
                 comp_qi = 1.0 - qi;
@@ -673,7 +673,7 @@
                     inputs(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
                     // The symetric part (loop  j=i+1...input_size)
                     if( bprop_all_terms )
-		        inputs(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
+                        inputs(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
                 }
             }
                 for (int i = 0; i &lt; input_size; i++ )
@@ -686,7 +686,7 @@
         for (int isample = 0; isample &lt; n_samples; isample++)
         {
             real qi, qj, comp_qi, comp_qj;
-	    Vec comp_q(input_size), log_term(input_size);
+            Vec comp_q(input_size), log_term(input_size);
             for (int i = 0; i &lt; input_size; i++ )
             {
                 qi = inputs(isample,i);
@@ -710,7 +710,7 @@
                     inputs_grad(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
                     // The symetric part (loop  j=i+1...input_size)
                     if( bprop_all_terms )
-		        inputs_grad(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
+                        inputs_grad(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
                 }
             }
             for (int i = 0; i &lt; input_size; i++ )
@@ -724,8 +724,8 @@
         real cost_before = computeKLdiv( true );
 
         if( !bprop_all_terms )
-	    PLERROR(&quot;kl_div with bprop_all_terms=false not implemented yet&quot;);
-    
+            PLERROR(&quot;kl_div with bprop_all_terms=false not implemented yet&quot;);
+
         for (int isample = 0; isample &lt; n_samples; isample++)
         {
             real qi, qj;
@@ -735,11 +735,11 @@
             {
                 qi=inputs(isample,i);
                 if( histo_index(qi) &lt; histo_size-1 )
-                { 
+                {
                     inputs(isample,i) += dq(qi);
                     computeHisto(inputs);
                     real cost_after = computeKLdiv( false );
-                    inputs(isample,i) -= dq(qi); 
+                    inputs(isample,i) -= dq(qi);
                     inputs_grad(isample, i) = (cost_after - cost_before)*1./dq(qi);
                 }
                 //else inputs_grad(isample, i) = 0.;
@@ -747,7 +747,7 @@
                 continue;
 
                 inputs_grad(isample, i) = 0.;
-                    
+
                 qi = inputs(isample,i);
                 int index_i = histo_index(qi);
                 if( ( index_i == histo_size-1 ) ) // we do not care about this...
@@ -755,7 +755,7 @@
                 real over_dqi=1.0/dq(qi);
                 // qi + dq(qi) ==&gt; | p_inputs_histo(i,index_i)   - one_count
                 //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-                    		    
+
                 for (int j = 0; j &lt; i; j++)
                 {
                     inputs_grad(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
@@ -767,17 +767,17 @@
                     real over_dqj=1.0/dq(qj);
                     // qj + dq(qj) ==&gt; | p_inputs_histo(j,index_j)   - one_count
                     //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
+
                     inputs_grad(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                 }
             }
-        }            
+        }
     } // END cost_function == &quot;kl_div&quot;
 
     else if( cost_function == &quot;kl_div_simple&quot; )
     {
         computeSafeHisto(inputs);
-            
+
         for (int isample = 0; isample &lt; n_samples; isample++)
         {
             // Computing the difference of KL divergence
@@ -800,7 +800,7 @@
                     inputs_grad(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
 
                     if( bprop_all_terms )
-		    {
+                    {
                         qj = inputs(isample,j);
                         int index_j = histo_index(qj);
                         if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
@@ -808,9 +808,9 @@
                         real over_dqj=1.0/dq(qj);
                         // qj + dq(qj) ==&gt; | p_inputs_histo(j,index_j)   - one_count
                         //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
-		        inputs_grad(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
-		    }
+
+                        inputs_grad(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
+                    }
                 }
             }
 
@@ -838,7 +838,7 @@
                     real d_temp = deriv_func_(inputs_cross_quadratic_mean(i,j));
                     qj = inputs(isample,j);
                     inputs_grad(isample, i) += d_temp *qj;
-	            if( bprop_all_terms )
+                    if( bprop_all_terms )
                         inputs_grad(isample, j) += d_temp *qi;
                 }
             }
@@ -885,7 +885,7 @@
                     continue;
                 }
                 //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
-                //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                //!                  = ( qi(t) - E(Qi) ) / n_samples
                 //!
                 //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
                 //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
@@ -901,14 +901,14 @@
                 for (int j = 0; j &lt; i; j++)
                 {
                     if( fast_exact_is_equal( inputs_correlations(i,j), 0.) )
-		    {
-			if (isample == 0)
-			    PLWARNING(&quot;correlation(i,j)=0 for i=%d, j=%d&quot;, i, j);
+                    {
+                        if (isample == 0)
+                            PLWARNING(&quot;correlation(i,j)=0 for i=%d, j=%d&quot;, i, j);
                         continue;
                     }
                     qj = inputs(isample,j);
                     real correlation_denum = inputs_stds[i]*inputs_stds[j];
-		    real squared_correlation_denum = correlation_denum * correlation_denum;
+                    real squared_correlation_denum = correlation_denum * correlation_denum;
                     if( fast_exact_is_equal( squared_correlation_denum, 0. ) )
                         continue;
                     real dfunc_dCorr = deriv_func_( inputs_correlations(i,j) );
@@ -916,16 +916,16 @@
                                              - inputs_expectation[i]*inputs_expectation[j] );
 
                     if( correlation_num/correlation_denum - inputs_correlations(i,j) &gt; 0.0000001 )
-			PLERROR( &quot;num/denum (%f) &lt;&gt; correlation (%f) for (i,j)=(%d,%d)&quot;,
-				 correlation_num/correlation_denum, inputs_correlations(i,j),i,j);
+                        PLERROR( &quot;num/denum (%f) &lt;&gt; correlation (%f) for (i,j)=(%d,%d)&quot;,
+                                 correlation_num/correlation_denum, inputs_correlations(i,j),i,j);
 
-                    inputs_grad(isample, i) += dfunc_dCorr * ( 
+                    inputs_grad(isample, i) += dfunc_dCorr * (
                                                  correlation_denum * dCROSSij_dqj[j]
                                                - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
                                                  ) / squared_correlation_denum;
 
                     if( bprop_all_terms )
-			inputs_grad(isample, j) += dfunc_dCorr * ( 
+                        inputs_grad(isample, j) += dfunc_dCorr * (
                                                      correlation_denum * dCROSSij_dqj[i]
                                                    - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
                                                      ) / squared_correlation_denum;
@@ -962,11 +962,11 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
+
     expectation.resize( input_size );
-    expectation.clear(); 
+    expectation.clear();
     cross_quadratic_mean.resize(input_size,input_size);
-    cross_quadratic_mean.clear(); 
+    cross_quadratic_mean.clear();
 
     inputs_expectation.clear();
     inputs_cross_quadratic_mean.clear();
@@ -1065,9 +1065,9 @@
     Vec input;
 
     expectation.resize( input_size );
-    expectation.clear(); 
+    expectation.clear();
     cross_quadratic_mean.resize(input_size,input_size);
-    cross_quadratic_mean.clear(); 
+    cross_quadratic_mean.clear();
     stds.resize( input_size );
     stds.clear();
     correlations.resize(input_size,input_size);
@@ -1092,27 +1092,27 @@
         cross_quadratic_mean(i,i) *= one_count;
 
         if( fast_is_equal(momentum, 0.)
-	||  !during_training )
+            ||  !during_training )
         {
- 	    //! Computation of the standard deviations
-	    //! requires temporary variable because of numerical imprecision
-	    real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
-	    if( tmp &gt; 0. )  //  std[i] = 0 by default
-	        stds[i] = sqrt( tmp );
+            //! Computation of the standard deviations
+            //! requires temporary variable because of numerical imprecision
+            real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
+            if( tmp &gt; 0. )  //  std[i] = 0 by default
+                stds[i] = sqrt( tmp );
         }
-	
+
         for (int j = 0; j &lt; i; j++)
         {
             //! Normalization (2/2)
             cross_quadratic_mean(i,j) *= one_count;
 
             if( fast_is_equal(momentum, 0.)
-	    ||  !during_training )
-            {	    
-	        //! Correlations
-	        real tmp = stds[i] * stds[j];
+                ||  !during_training )
+            {
+                //! Correlations
+                real tmp = stds[i] * stds[j];
                 if( !fast_is_equal(tmp, 0.) )  //  correlations(i,j) = 1 by default
-	            correlations(i,j) = ( cross_quadratic_mean(i,j)
+                    correlations(i,j) = ( cross_quadratic_mean(i,j)
                                           - expectation[i]*expectation[j] ) / tmp;
             }
         }
@@ -1131,19 +1131,19 @@
                                         +(1.0-momentum)*cross_quadratic_mean(i,i);
             inputs_cross_quadratic_mean_trainMemory(i,i) = cross_quadratic_mean(i,i);
 
-	    real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
-	    if( tmp &gt; 0. )  //  std[i] = 0 by default
-	        stds[i] = sqrt( tmp );
-	    
+            real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
+            if( tmp &gt; 0. )  //  std[i] = 0 by default
+                stds[i] = sqrt( tmp );
+
             for (int j = 0; j &lt; i; j++)
             {
-                 cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
-                                             +(1.0-momentum)*cross_quadratic_mean(i,j);
-                 inputs_cross_quadratic_mean_trainMemory(i,j) = cross_quadratic_mean(i,j);
+                cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
+                    +(1.0-momentum)*cross_quadratic_mean(i,j);
+                inputs_cross_quadratic_mean_trainMemory(i,j) = cross_quadratic_mean(i,j);
 
-	         tmp = stds[i] * stds[j];
+                tmp = stds[i] * stds[j];
                  if( !fast_is_equal(tmp, 0.) )  //  correlations(i,j) = 1 by default
-	             correlations(i,j) = ( cross_quadratic_mean(i,j)
+                     correlations(i,j) = ( cross_quadratic_mean(i,j)
                                          - expectation[i]*expectation[j] ) / tmp;
 
             }
@@ -1162,7 +1162,7 @@
     for (int i = 0; i &lt; input_size; i++)
         for (int j = 0; j &lt; i; j++)
         {
-            // These variables are used in case one bin of 
+            // These variables are used in case one bin of
             // the histogram is empty for one unit
             // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
             // In such case, we ''differ'' the count for the next bin and so on.
@@ -1197,14 +1197,14 @@
                 }
             }
 //                    if( differ_count_i &gt; 0.0 )
-//                    {   
-//                        &quot;cas ou on regroupe avec le dernier&quot;;   
+//                    {
+//                        &quot;cas ou on regroupe avec le dernier&quot;;
 //                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
 //                                  *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
-//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
 //                    }
-//                     
+//
 //                    else if ( differ_count_j &gt; 0.0 )
 //                    {
 //                        &quot;cas ou on regroupe avec le dernier&quot;;
@@ -1212,7 +1212,7 @@
 //                                 *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
 //                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
-//                    }    
+//                    }
         }
     // Normalization w.r.t. number of units
     return cost *norm_factor;
@@ -1226,12 +1226,12 @@
             for (int i = 0; i &lt; input_size; i++)
                 for (int j = 0; j &lt; i; j++)
                 {
-                    // These variables are used in case one bin of 
+                    // These variables are used in case one bin of
                     // the histogram is empty for one unit
                     // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
                     // In such case, we ''differ'' the count for the next bin and so on.
-		    cache_differ_count_i[ i ][ j ].clear();
-		    cache_differ_count_j[ i ][ j ].clear();
+                    cache_differ_count_i[ i ][ j ].clear();
+                    cache_differ_count_j[ i ][ j ].clear();
                     cache_n_differ[i][j].fill( 0. );
 //                    real last_positive_Ni_k, last_positive_Nj_k;
 //                    real last_n_differ;
@@ -1242,17 +1242,17 @@
 
                         if( fast_exact_is_equal(Ni_k, 0.0) )
                         {
-			    if( k &lt; histo_size - 1 ) // &quot;cas ou on regroupe avec le dernier&quot;;
-			    {
-			        cache_differ_count_j[i][j][ k+1 ] = Nj_k;
+                            if( k &lt; histo_size - 1 ) // &quot;cas ou on regroupe avec le dernier&quot;;
+                            {
+                                cache_differ_count_j[i][j][ k+1 ] = Nj_k;
                                 cache_n_differ[i][j][ k+1 ] = cache_n_differ[i][j][ k ] + 1;
                             }
-			}
+                        }
                         else if( fast_exact_is_equal(Nj_k, 0.0) )
                         {
-			    if( k &lt; histo_size - 1 ) // &quot;cas ou on regroupe avec le dernier&quot;;
-			    {
-			        cache_differ_count_i[i][j][ k+1 ] = Ni_k;
+                            if( k &lt; histo_size - 1 ) // &quot;cas ou on regroupe avec le dernier&quot;;
+                            {
+                                cache_differ_count_i[i][j][ k+1 ] = Ni_k;
                                 cache_n_differ[i][j][ k+1 ] = cache_n_differ[i][j][ k ] + 1;
                             }
                         }
@@ -1268,7 +1268,7 @@
 //                    else if ( cache_differ_count_j[i][j][ histo_size - 1 ] &gt; 0.0 )
 //                        &quot;cas ou on regroupe avec le dernier&quot;;
                     }
-		}
+                }
             // Normalization w.r.t. number of units
             return cost *norm_factor;
     }
@@ -1288,7 +1288,7 @@
     //   =&gt; ) the input(isample,i) has been counted
 
     real grad_update = 0.0;
-    
+
     real Ni_ki, Nj_ki, Ni_ki_shift1, Nj_ki_shift1;
     real n_differ_before_ki, n_differ_before_ki_shift1;
 
@@ -1322,42 +1322,42 @@
     {
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki,
-	                          Nj_ki )
-	               * ( 1 + n_differ_before_ki);
+                                  Nj_ki )
+                       * ( 1 + n_differ_before_ki);
 
         if( fast_exact_is_equal(Ni_ki, one_count) )
         {
             additional_differ_count_j_after = Nj_ki;
-	    n_differ_after_ki_shift1 = n_differ_after_ki + 1;
-	                          // = n_differ_before_ki + 1;
+            n_differ_after_ki_shift1 = n_differ_after_ki + 1;
+                                  // = n_differ_before_ki + 1;
         }
         else
-	{
+        {
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki - one_count,
-	                              Nj_ki )
-	                   * ( 1 + n_differ_after_ki );
-	}
+                                      Nj_ki )
+                           * ( 1 + n_differ_after_ki );
+        }
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki_shift1 + one_count,
-	                                  Nj_ki_shift1 + additional_differ_count_j_after )
-	                       * ( 1 + n_differ_after_ki_shift1 );
+                                          Nj_ki_shift1 + additional_differ_count_j_after )
+                               * ( 1 + n_differ_after_ki_shift1 );
 
             if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // &quot;cas ou on regroupe avec le dernier&quot;;
             {
                 // removing the term of the sum that will be modified
                 grad_update -= KLdivTerm( Ni_ki_shift1,
-		                          Nj_ki_shift1 )
-		               * ( 1 + n_differ_before_ki_shift1 );                
+                                          Nj_ki_shift1 )
+                               * ( 1 + n_differ_before_ki_shift1 );
             }
             else // ( Ni_ki_shift1 == 0.0 )
             {
                 // We search   ki' &gt; k(i)+1   such that   n(i,ki') &gt; 0
                 real additional_differ_count_j_before = 0.;
-		real additional_n_differ_before_ki_shift1 = 0.;
+                real additional_n_differ_before_ki_shift1 = 0.;
                 int ki;
                 for (ki = index_i+2; ki &lt; histo_size; ki++)
                 {
@@ -1369,15 +1369,15 @@
                 if( ki &lt; histo_size )
                 {
                     grad_update -= KLdivTerm( inputs_histo( i, ki ),
-		                              Nj_ki_shift1 + additional_differ_count_j_before )
-		                   * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
+                                              Nj_ki_shift1 + additional_differ_count_j_before )
+                                   * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
 
                     if( additional_differ_count_j_before &gt; 0. )
-		    // We have to report the additional count for unit j
+                    // We have to report the additional count for unit j
                     {
                         grad_update += KLdivTerm( inputs_histo( i, ki ),
-			                          additional_differ_count_j_before )
-			               * ( additional_n_differ_before_ki_shift1 );
+                                                  additional_differ_count_j_before )
+                                       * ( additional_n_differ_before_ki_shift1 );
                     }
                 }
             }
@@ -1385,7 +1385,7 @@
         else // ( Nj_ki_shift1 == 0.0 )
         {
             real additional_differ_count_i_before = 0.;
-	    // We search kj &gt; ki+1 tq inputs_histo( j, kj ) &gt; 0.
+            // We search kj &gt; ki+1 tq inputs_histo( j, kj ) &gt; 0.
             int kj;
             for( kj = index_i+2; kj &lt; histo_size; kj++)
             {
@@ -1394,64 +1394,64 @@
                 if( inputs_histo( j, kj ) &gt; 0. )
                     break;
             }
-	    if ( !fast_exact_is_equal(additional_differ_count_j_after, 0. ) )
-	        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
+            if ( !fast_exact_is_equal(additional_differ_count_j_after, 0. ) )
+                n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
             if( kj &lt; histo_size )
             {
                 if ( fast_exact_is_equal(n_differ_after_ki_shift1, n_differ_before_ki_shift1) )
-		{
-		    // ( no qj were differed after we changed count at bin ki )
-		    // OR ( some qj were differed to bin ki+1 AND the bin were not empty )
+                {
+                    // ( no qj were differed after we changed count at bin ki )
+                    // OR ( some qj were differed to bin ki+1 AND the bin were not empty )
                     grad_update += KLdivTerm( Ni_ki_shift1 + additional_differ_count_i_before + one_count,
-		                             inputs_histo( j, kj ) + additional_differ_count_j_after )
-		                   * ( 1 + n_differ_after_ki_shift1 );
-                }	   		
-		else
-		{
-		    PLASSERT( n_differ_before_ki_shift1 &gt; n_differ_after_ki_shift1 );
+                                              inputs_histo( j, kj ) + additional_differ_count_j_after )
+                                   * ( 1 + n_differ_after_ki_shift1 );
+                }
+                else
+                {
+                    PLASSERT( n_differ_before_ki_shift1 &gt; n_differ_after_ki_shift1 );
                     grad_update += KLdivTerm( Ni_ki_shift1 + one_count,
-		                              additional_differ_count_j_after )
-		                   * ( 1 + n_differ_after_ki_shift1 );
+                                              additional_differ_count_j_after )
+                                   * ( 1 + n_differ_after_ki_shift1 );
                     grad_update += KLdivTerm( additional_differ_count_i_before,
-		                              inputs_histo( j, kj ) )
-		                   * ( n_differ_before_ki_shift1 - n_differ_after_ki_shift1 );
+                                              inputs_histo( j, kj ) )
+                                   * ( n_differ_before_ki_shift1 - n_differ_after_ki_shift1 );
                 }
 
                 if( !fast_exact_is_equal(Ni_ki_shift1 + additional_differ_count_i_before,0.0) )
-		{
+                {
                     grad_update -= KLdivTerm( Ni_ki_shift1 + additional_differ_count_i_before,
-		                              inputs_histo( j, kj ) )
-		                   * ( 1 + n_differ_before_ki_shift1 );
-	        }
-		else // ( Ni_ki_shift1' == 0 == Nj_ki_shift1 ) &amp;&amp; ( pas de q[i] avant q[j']... )
-		{
-		    // We search ki' &gt; kj+1 tq inputs_histo( i, ki' ) &gt; 0.
+                                              inputs_histo( j, kj ) )
+                                   * ( 1 + n_differ_before_ki_shift1 );
+                }
+                else // ( Ni_ki_shift1' == 0 == Nj_ki_shift1 ) &amp;&amp; ( pas de q[i] avant q[j']... )
+                {
+                    // We search ki' &gt; kj+1 tq inputs_histo( i, ki' ) &gt; 0.
                     real additional_differ_count_j_before = 0.;
-		    real additional_n_differ_before_ki_shift1 = 0.;
-		    int kj2;
+                    real additional_n_differ_before_ki_shift1 = 0.;
+                    int kj2;
                     for( kj2 = kj+1; kj2 &lt; histo_size; kj2++)
                     {
-			additional_differ_count_j_before += inputs_histo( j, kj2 );
+                        additional_differ_count_j_before += inputs_histo( j, kj2 );
                         additional_n_differ_before_ki_shift1 += 1;
                         if( inputs_histo( i, kj2 ) &gt; 0. )
                             break;
-		    }
-		    if ( fast_exact_is_equal(additional_differ_count_j_before, 0. ) )
-		        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
+                    }
+                    if ( fast_exact_is_equal(additional_differ_count_j_before, 0. ) )
+                        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
                     if( kj2 &lt; histo_size )
-		    {
-		        grad_update -= KLdivTerm( inputs_histo( i, kj2 ),
-			                          Nj_ki_shift1 + additional_differ_count_j_before )
-		                       * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
+                    {
+                        grad_update -= KLdivTerm( inputs_histo( i, kj2 ),
+                                                  Nj_ki_shift1 + additional_differ_count_j_before )
+                                       * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
 
                         if( additional_differ_count_j_before &gt; 0. )
-			{
+                        {
                             grad_update += KLdivTerm( inputs_histo( i, kj2 ),
-			                              additional_differ_count_j_before )
-		                           * ( additional_n_differ_before_ki_shift1 );
+                                                      additional_differ_count_j_before )
+                                           * ( additional_n_differ_before_ki_shift1 );
                         }
                     }
-	        }
+                }
             }
         }
     }
@@ -1505,15 +1505,15 @@
 {
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
-    
+
     histo.resize(input_size,histo_size);
-    histo.clear(); 
+    histo.clear();
     for (int isample = 0; isample &lt; n_samples; isample++)
     {
         Vec input = inputs(isample);
         for (int i = 0; i &lt; input_size; i++)
-	{
-	    PLASSERT( histo_index(input[i]) &lt; histo_size);
+        {
+            PLASSERT( histo_index(input[i]) &lt; histo_size);
             histo( i, histo_index(input[i]) ) += one_count;
         }
     }

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LayerCostModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -58,7 +58,7 @@
     //! Generic name of the cost function
     string cost_function;
 
-    //! Maximum number of stages we want to propagate the gradient    
+    //! Maximum number of stages we want to propagate the gradient
     int nstages_max;
 
     //! Parameter to compute moving means in non stochastic cost functions
@@ -66,9 +66,9 @@
 
     //! Kind of optimization
     string optimization_strategy;
-    
+
     //! Parameter in pascal's cost function
-    real alpha;    
+    real alpha;
 
     //! For non stochastic KL divergence cost function
     int histo_size;
@@ -91,7 +91,7 @@
 
     //! Variables for (non stochastic) Pascal's/correlation function with momentum
     //! -------------------------------------------------------------
-    //! Statistics on outputs (estimated empiricially on the data)    
+    //! Statistics on outputs (estimated empiricially on the data)
     Vec inputs_expectation_trainMemory;
     Mat inputs_cross_quadratic_mean_trainMemory;
 
@@ -146,7 +146,7 @@
     virtual void computeCorrelationStatistics(const Mat&amp; inputs);
     virtual void computeCorrelationStatistics(const Mat&amp; inputs,
                                               Vec&amp; expectation, Mat&amp; cross_quadratic_mean,
-                                              Vec&amp; stds, Mat&amp; correlations) const;    
+                                              Vec&amp; stds, Mat&amp; correlations) const;
     //! Returns all ports in a RBMModule.
     virtual const TVec&lt;string&gt;&amp; getPorts();
 
@@ -210,7 +210,7 @@
     real HISTO_STEP;
     //! the weight of a sample within a batch (usually, 1/n_samples)
 
-    mutable real one_count; 
+    mutable real one_count;
     TVec&lt; TVec&lt; Vec &gt; &gt; cache_differ_count_i;
     TVec&lt; TVec&lt; Vec &gt; &gt; cache_differ_count_j;
     TVec&lt; TVec&lt; Vec &gt; &gt; cache_n_differ;

Modified: trunk/plearn_learners/online/LinearCombinationModule.cc
===================================================================
--- trunk/plearn_learners/online/LinearCombinationModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearCombinationModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -138,7 +138,7 @@
         // has build completed? there should be at least one input port + the output port
         PLERROR(&quot;LinearCombinationModule should have at least 2 ports (one input port and one output port)\n&quot;);
     PLASSERT( ports_value.length() == n_ports ); // is the input coherent with expected nPorts
-    
+
     const TVec&lt;Mat*&gt;&amp; inputs = ports_value;
     Mat* output = ports_value[n_ports-1];
     if (output) {

Modified: trunk/plearn_learners/online/LinearCombinationModule.h
===================================================================
--- trunk/plearn_learners/online/LinearCombinationModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearCombinationModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -57,7 +57,7 @@
     //#####  Public Build Options  ############################################
 
     //! ### declare public option fields (such as build options) here
-    
+
     //! the weights of the linear combination: one per input port
     Vec weights;
     //! whether to adapt the weights, and whether to clear them to 0 upon forget()

Modified: trunk/plearn_learners/online/LinearFilterModule.cc
===================================================================
--- trunk/plearn_learners/online/LinearFilterModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearFilterModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -99,7 +99,7 @@
 
     // Add bias.
     resizeOnes(n);
-    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical 
+    externalProductAcc(outputs, ones, bias); // could be more efficient, but not critical
 }
 
 /////////////////
@@ -148,7 +148,7 @@
                 else
                     weights[i] = 0.;
             }
-            
+
             if( between_0_and_1 )
             {
                 if( weights[i] &gt; 1. )
@@ -274,7 +274,7 @@
         resizeOnes(n);
         transposeProductScaleAcc(bias, output_gradients, ones, -avg_lr, real(1));
     }
-    
+
     // Update weights.
     for(int i_sample = 0; i_sample &lt; outputs.length() ;i_sample++)
         for(int i = 0; i &lt; output_size; i++ )
@@ -304,8 +304,8 @@
     }
     step_number += n;
 }
-    
 
+
 //////////////////
 // bbpropUpdate //
 //////////////////

Modified: trunk/plearn_learners/online/LinearFilterModule.h
===================================================================
--- trunk/plearn_learners/online/LinearFilterModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LinearFilterModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -97,7 +97,7 @@
 
     //! The bias
     Vec bias;
-    
+
     bool no_bias;
     bool between_0_and_1;
 
@@ -125,7 +125,7 @@
                              Mat&amp; input_gradients,
                              const Mat&amp; output_gradients,
                              bool accumulate = false);
-    
+
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               const Vec&amp; output_gradient,
                               const Vec&amp; output_diag_hessian);
@@ -158,7 +158,7 @@
 
     //! A vector filled with all ones.
     Vec ones;
-    
+
     //#####  Protected Options  ###############################################
 
 protected:

Modified: trunk/plearn_learners/online/LogaddOnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/LogaddOnBagsModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LogaddOnBagsModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -97,7 +97,7 @@
 {
     for( int i = 0; i &lt; input_size; i++ )
         accumulated_output[i] = logadd(accumulated_output[i],
-                                       input[i]); 
+                                       input[i]);
 }
 void LogaddOnBagsModule::fpropOutput(Vec&amp; output)
 {

Modified: trunk/plearn_learners/online/LogaddOnBagsModule.h
===================================================================
--- trunk/plearn_learners/online/LogaddOnBagsModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/LogaddOnBagsModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -87,7 +87,7 @@
 protected:
 
     Vec accumulated_output;
-    
+
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -342,17 +342,17 @@
                     bag_info = int(round(target.lastElement()));
                     isample ++;
                 }
-                isample = isample % train_set-&gt;length();                 
+                isample = isample % train_set-&gt;length();
             }
             if( stage + inputs.length() &gt; nstages )
                 break;
             // Perform a training step.
-            trainingStep(inputs, targets, weights);              
+            trainingStep(inputs, targets, weights);
             // Handle training progress.
             stage += inputs.length();
             if (report_progress)
                 pb-&gt;update(stage - stage_init);
-        }    
+        }
     else
         while (stage + mbatch_size &lt;= nstages) {
             // Obtain training samples.

Modified: trunk/plearn_learners/online/ModuleLearner.h
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleLearner.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -190,7 +190,7 @@
     //! Matrix that contains only ones (used to fill weights at test time).
     mutable Mat all_ones;
 
-    //! Matrix that stores a copy of the costs 
+    //! Matrix that stores a copy of the costs
     //! (used to update the cost statistics).
     mutable Mat tmp_costs;
 

Modified: trunk/plearn_learners/online/ModuleStackModule.h
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleStackModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -95,7 +95,7 @@
                              Mat&amp; input_gradients,
                              const Mat&amp; output_gradients,
                              bool accumulate = false);
-    
+
     //! This version does not obtain the input gradient.
     virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
                              const Vec&amp; output_gradient);

Modified: trunk/plearn_learners/online/ModuleTester.cc
===================================================================
--- trunk/plearn_learners/online/ModuleTester.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/ModuleTester.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -434,7 +434,7 @@
                             PLASSERT( out_val &amp;&amp; out_prev &amp;&amp; out_grad_ );
                             for (int oi = 0; oi &lt; out_val-&gt;length(); oi++)
                                 for (int oj = 0; oj &lt; out_val-&gt;width(); oj++) {
-                                    real diff = (*out_val)(oi, oj) - 
+                                    real diff = (*out_val)(oi, oj) -
                                         (*out_prev)(oi, oj);
                                     (*grad)(p, q) +=
                                         diff * (*out_grad_)(oi, oj) / step;

Modified: trunk/plearn_learners/online/NetworkConnection.h
===================================================================
--- trunk/plearn_learners/online/NetworkConnection.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/NetworkConnection.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -121,7 +121,7 @@
 
     //! Destination port.
     string dst_port;
-    
+
     //#####  Protected Options  ###############################################
 
 protected:

Modified: trunk/plearn_learners/online/NetworkModule.cc
===================================================================
--- trunk/plearn_learners/online/NetworkModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/NetworkModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -328,7 +328,7 @@
                                     tostring(count_null++));
                             all_modules.append(null);
                         /*} else {
-                            PLASSERT( all_modules.length() == 
+                            PLASSERT( all_modules.length() ==
                                         modules.length() + 1 );
                             null = all_modules.lastElement();
                         }*/
@@ -350,7 +350,7 @@
             }
         }
     }
-    
+
     // Construct fprop and bprop paths from the list of modules and
     // connections.
 
@@ -383,7 +383,7 @@
     map&lt;const OnlineLearningModule*, int&gt; module_to_index;
     for (int i = 0; i &lt; all_modules.length(); i++)
         module_to_index[all_modules[i]] = i;
-    
+
     // Analyze the list of ports.
     // The 'port_correspondances' lists, for each module, the correspondances
     // between the modules' ports and the ports of the NetworkModule.

Modified: trunk/plearn_learners/online/NetworkModule.h
===================================================================
--- trunk/plearn_learners/online/NetworkModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/NetworkModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -141,7 +141,7 @@
     //! p1 the index of the port in this NetworkModule, and p2 the index of the
     //! corresponding port in the i-th module during fprop.
     TVec&lt; TMat&lt;int&gt; &gt; fprop_toplug;
-    
+
     //! The i-th element is the list of Mat* pointers being provided to the
     //! i-th module in a bprop step.
     TVec&lt; TVec&lt;Mat*&gt; &gt; bprop_data;

Modified: trunk/plearn_learners/online/OnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnBagsModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -72,7 +72,7 @@
 {
     PLASSERT( bagtarget_size &gt; 0 );
     PLASSERT( input_size &gt; 0 );
-    
+
     // The port story...
     ports.resize(0);
     portname_to_index.clear();
@@ -156,7 +156,7 @@
         wait_new_bag = false;
     }
     else
-    {   
+    {
         PLASSERT( !wait_new_bag );
         fpropAcc( input );
     }
@@ -206,19 +206,19 @@
     if( p_inputs_grad
         &amp;&amp; p_output_grad &amp;&amp; !p_output_grad-&gt;isEmpty() )
     {
-	    PLASSERT( p_inputs &amp;&amp; !p_inputs-&gt;isEmpty());
-	    PLASSERT( p_bagtargets &amp;&amp; !p_bagtargets-&gt;isEmpty());
+        PLASSERT( p_inputs &amp;&amp; !p_inputs-&gt;isEmpty());
+        PLASSERT( p_bagtargets &amp;&amp; !p_bagtargets-&gt;isEmpty());
         int n_samples = p_inputs-&gt;length();
-    	PLASSERT( p_output_grad-&gt;length() == n_samples );
-    	PLASSERT( p_output_grad-&gt;width() == output_size );
+        PLASSERT( p_output_grad-&gt;length() == n_samples );
+        PLASSERT( p_output_grad-&gt;width() == output_size );
         if( p_inputs_grad-&gt;isEmpty() )
         {
             p_inputs_grad-&gt;resize( n_samples, input_size);
             p_inputs_grad-&gt;clear();
         }
-        bpropUpdate( *p_inputs, *p_bagtargets, 
+        bpropUpdate( *p_inputs, *p_bagtargets,
                      *p_inputs_grad, *p_output_grad, true );
-    	checkProp(ports_gradient);
+        checkProp(ports_gradient);
     }
     else if( !p_inputs_grad &amp;&amp; !p_output_grad )
         return;

Modified: trunk/plearn_learners/online/OnBagsModule.h
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnBagsModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -64,7 +64,7 @@
     OnBagsModule();
 
     //!### Main functions to be coded in subclasses ###
-    
+
     //! update internal statistics needed to compute the output of bag
     virtual void fpropAcc(const Vec&amp; input);
     virtual void fpropInit(const Vec&amp; input);
@@ -74,7 +74,7 @@
     virtual void bprop( const Mat&amp; baginputs,
                           const Vec&amp; bagoutput_gradient,
                           Mat&amp; baginputs_gradients);
-                          
+
    //!#################################################
 
     //! given the input and target, compute the cost
@@ -122,7 +122,7 @@
     virtual void forget();
 
 protected:
-    
+
     bool wait_new_bag;
 
     //! Map from a port name to its index in the 'ports' vector.

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -413,8 +413,8 @@
     return outputs;
 }
 
-map&lt;string,Mat&gt; OnlineLearningModule::namedBpropAccUpdate(map&lt;string,Mat&gt;&amp; values, 
-                                                          map&lt;string,Mat&gt;&amp; gradients, 
+map&lt;string,Mat&gt; OnlineLearningModule::namedBpropAccUpdate(map&lt;string,Mat&gt;&amp; values,
+                                                          map&lt;string,Mat&gt;&amp; gradients,
                                                           TVec&lt;string&gt; additional_input_gradients)
 {
     map&lt;string,Mat&gt; all_gradients;
@@ -439,7 +439,7 @@
     for (it=gradients.begin();it!=gradients.end();++it)
         all_gradients[it-&gt;first]=it-&gt;second;
     for (int i=0;i&lt;additional_input_gradients.length();i++)
-        all_gradients[additional_input_gradients[i]]= 
+        all_gradients[additional_input_gradients[i]]=
             *ports_gradient[getPortIndex(additional_input_gradients[i])];
     return all_gradients;
 }

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -133,8 +133,8 @@
     virtual void fprop(const TVec&lt;Mat*&gt;&amp; ports_value);
 
     virtual map&lt;string,Mat&gt; namedFprop(map&lt;string,Mat&gt;&amp; inputs, TVec&lt;string&gt; wanted_outputs);
-    virtual map&lt;string,Mat&gt; namedBpropAccUpdate(map&lt;string,Mat&gt;&amp; values, 
-                                                map&lt;string,Mat&gt;&amp; gradients, 
+    virtual map&lt;string,Mat&gt; namedBpropAccUpdate(map&lt;string,Mat&gt;&amp; values,
+                                                map&lt;string,Mat&gt;&amp; gradients,
                                                 TVec&lt;string&gt; additional_input_gradients);
 
     //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -133,9 +133,9 @@
     virtual void freeEnergyContributionGradient(const Vec&amp; unit_activations,
                                                 Vec&amp; unit_activations_gradient,
                                                 real output_gradient = 1,
-                                                bool accumulate = false) 
+                                                bool accumulate = false)
         const;
-    
+
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec&amp; output);

Modified: trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMDiagonalMatrixConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -60,33 +60,33 @@
                   OptionBase::learntoption,
                   &quot;Vector containing the diagonal of the weight matrix.\n&quot;);
 
-    declareOption(ol, &quot;L1_penalty_factor&quot;, 
+    declareOption(ol, &quot;L1_penalty_factor&quot;,
                   &amp;RBMDiagonalMatrixConnection::L1_penalty_factor,
                   OptionBase::buildoption,
                   &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
                   &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| &quot;
                   &quot;during training.\n&quot;);
 
-    declareOption(ol, &quot;L2_penalty_factor&quot;, 
+    declareOption(ol, &quot;L2_penalty_factor&quot;,
                   &amp;RBMDiagonalMatrixConnection::L2_penalty_factor,
                   OptionBase::buildoption,
                   &quot;Optional (default=0) factor of L2 regularization term, i.e.\n&quot;
                   &quot;minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 &quot;
                   &quot;during training.\n&quot;);
 
-    declareOption(ol, &quot;L2_decrease_constant&quot;, 
+    declareOption(ol, &quot;L2_decrease_constant&quot;,
                   &amp;RBMDiagonalMatrixConnection::L2_decrease_constant,
                   OptionBase::buildoption,
         &quot;Parameter of the L2 penalty decrease (see L2_decrease_type).&quot;,
         OptionBase::advanced_level);
 
-    declareOption(ol, &quot;L2_shift&quot;, 
+    declareOption(ol, &quot;L2_shift&quot;,
                   &amp;RBMDiagonalMatrixConnection::L2_shift,
                   OptionBase::buildoption,
         &quot;Parameter of the L2 penalty decrease (see L2_decrease_type).&quot;,
         OptionBase::advanced_level);
 
-    declareOption(ol, &quot;L2_decrease_type&quot;, 
+    declareOption(ol, &quot;L2_decrease_type&quot;,
                   &amp;RBMDiagonalMatrixConnection::L2_decrease_type,
                   OptionBase::buildoption,
         &quot;The kind of L2 decrease that is being applied. The decrease\n&quot;
@@ -97,7 +97,7 @@
         &quot; - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)&quot;,
         OptionBase::advanced_level);
 
-    declareOption(ol, &quot;L2_n_updates&quot;, 
+    declareOption(ol, &quot;L2_n_updates&quot;,
                   &amp;RBMDiagonalMatrixConnection::L2_n_updates,
                   OptionBase::learntoption,
         &quot;Number of times that weights have been changed by the L2 penalty\n&quot;
@@ -262,7 +262,7 @@
         }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 
     clearStats();
@@ -306,7 +306,7 @@
         }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -352,7 +352,7 @@
         PLERROR(&quot;RBMDiagonalMatrixConnection::update minibatch with momentum - Not implemented&quot;);
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -455,7 +455,7 @@
         }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -509,7 +509,7 @@
             w[i] -= avg_lr * in[i] * outg[i];
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -533,7 +533,7 @@
     {
         if( delta_L2 != 0. )
             w_[i] *= (1 - delta_L2);
-        
+
         if( delta_L1 != 0. )
         {
             if( w_[i] &gt; delta_L1 )
@@ -544,7 +544,7 @@
                 w_[i] = 0.;
         }
     }
-    
+
     if (delta_L2 &gt; 0)
         L2_n_updates++;
 }
@@ -565,7 +565,7 @@
     {
         if( delta_L2 != 0. )
             gw_[i] += delta_L2*w_[i];
-        
+
         if( delta_L1 != 0. )
         {
             if( w_[i] &gt; 0 )

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -404,7 +404,7 @@
 
     if ( fixed_std_deviation &gt; 0 &amp;&amp; share_quad_coeff )
     {
-        if( share_quad_coeff ) 
+        if( share_quad_coeff )
             PLERROR(&quot;In RBMGaussianLayer::build_(): fixed_std_deviation should not &quot;
                     &quot;be &gt; 0 when share_quad_coeff is true.&quot;);
         quad_coeff.fill( 1 / ( M_SQRT2 * fixed_std_deviation ) );
@@ -491,7 +491,7 @@
     {
         real pos_factor = -learning_rate / pos_count;
         real neg_factor = learning_rate / neg_count;
-        
+
         real* a = quad_coeff.data();
         real* aps = quad_coeff_pos_stats.data();
         real* ans = quad_coeff_neg_stats.data();
@@ -549,7 +549,7 @@
         // We will need to recompute sigma
         sigma_is_up_to_date = false;
     }
-    
+
     // will update the bias, and clear the statistics
     inherited::update();
 }
@@ -619,7 +619,7 @@
         // We will need to recompute sigma
         sigma_is_up_to_date = false;
     }
-    
+
     // update the bias
     inherited::update( pos_values, neg_values );
 }
@@ -740,7 +740,7 @@
             r = (target[i] - expectation[i]);
             ret += r * r;
         }
-    } 
+    }
     else
     {
         if(share_quad_coeff)
@@ -756,7 +756,7 @@
                 //      + log(sqrt(2*Pi) * sigma[i])
                 real r = (target[i] - expectation[i]) * quad_coeff[i];
                 ret += r * r + pl_log(sigma[i]);
-                
+
             }
         ret += 0.5*size*Log2Pi;
     }

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -93,8 +93,8 @@
     // Set diagonal to 0
     if( lateral_weights.length() != 0 )
     {
-        real *d = lateral_weights.data();        
-        for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+        real *d = lateral_weights.data();
+        for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
             *d = 0;
     }
 
@@ -107,7 +107,7 @@
         mean_field_output_weights(i,i) = 1;
     for( int i=0; i&lt;mean_field_output_bias.length(); i++ )
         mean_field_output_bias[i] = -0.5;
-    
+
 }
 
 ////////////////////
@@ -122,7 +122,7 @@
             &quot;before calling generateSample()&quot;);
 
     for( int i=0 ; i&lt;size ; i++ )
-        sample[i] = random_gen-&gt;binomial_sample( expectation[i] );    
+        sample[i] = random_gen-&gt;binomial_sample( expectation[i] );
 }
 
 /////////////////////
@@ -160,7 +160,7 @@
         else
             for( int i=0 ; i&lt;size ; i++ )
                 mean_field_input[i] = sigmoid( activation[i] );
-        
+
         product(pre_sigmoid_mean_field_output, mean_field_output_weights, mean_field_input);
         pre_sigmoid_mean_field_output += mean_field_output_bias;
 
@@ -182,34 +182,34 @@
         for( int i=0 ; i&lt;size ; i++ )
         {
             mean_field_i = expectation[i];
-            temp_mean_field_gradient[i] = (pre_sigmoid_mean_field_output[i] 
-                                           - temp_mean_field_gradient[i]) 
+            temp_mean_field_gradient[i] = (pre_sigmoid_mean_field_output[i]
+                                           - temp_mean_field_gradient[i])
                 * mean_field_i * (1 - mean_field_i);
         }
 
-        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient, 
+        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient,
                                  mean_field_input, -learning_rate );
         multiplyScaledAdd( temp_mean_field_gradient, 1.0, -learning_rate, mean_field_output_bias);
     }
     else
-    {        
+    {
         if( temp_output.length() != n_lateral_connections_passes+1 )
         {
             temp_output.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i&lt;n_lateral_connections_passes+1 ; i++ )
                 temp_output[i].resize(size);
-        }       
-        
+        }
+
         current_temp_output = temp_output[0];
         temp_output.last() = expectation;
-        
+
         if (use_fast_approximations)
             for( int i=0 ; i&lt;size ; i++ )
                 current_temp_output[i] = fastsigmoid( activation[i] );
         else
             for( int i=0 ; i&lt;size ; i++ )
                 current_temp_output[i] = sigmoid( activation[i] );
-        
+
         for( int t=0; t&lt;n_lateral_connections_passes; t++ )
         {
             previous_temp_output = current_temp_output;
@@ -229,8 +229,8 @@
                 else
                 {
                     for( int i=0 ; i&lt;size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -244,12 +244,12 @@
                 else
                 {
                     for( int i=0 ; i&lt;size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
-            if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) &amp;&amp; 
+            if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) &amp;&amp;
                 dist(current_temp_output, previous_temp_output,2)/size &lt; mean_field_precision_threshold )
             {
                 expectation &lt;&lt; current_temp_output;
@@ -283,17 +283,17 @@
     else
     {
         dampening_expectations.resize( batch_size, size );
-        
+
         if( temp_outputs.length() != n_lateral_connections_passes+1 )
         {
             temp_outputs.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i&lt;n_lateral_connections_passes+1 ; i++ )
                 temp_outputs[i].resize( batch_size, size);
-        }       
-        
+        }
+
         current_temp_outputs = temp_outputs[0];
         temp_outputs.last() = expectations;
-        
+
         if (use_fast_approximations)
             for (int k = 0; k &lt; batch_size; k++)
                 for (int i = 0 ; i &lt; size ; i++)
@@ -308,11 +308,11 @@
             previous_temp_outputs = current_temp_outputs;
             current_temp_outputs = temp_outputs[t+1];
             if( topographic_lateral_weights.length() == 0 )
-                productTranspose(dampening_expectations, previous_temp_outputs, 
+                productTranspose(dampening_expectations, previous_temp_outputs,
                                  lateral_weights);
             else
                 for( int b = 0; b&lt;dampening_expectations.length(); b++)
-                    productTopoLateralWeights( dampening_expectations(b), 
+                    productTopoLateralWeights( dampening_expectations(b),
                                                previous_temp_outputs(b) );
 
             dampening_expectations += activations;
@@ -322,7 +322,7 @@
                 {
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 fastsigmoid( dampening_expectations(k, i) );
                 }
                 else
@@ -330,7 +330,7 @@
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
                             current_temp_outputs(k, i) = (1-dampening_factor)
-                                * fastsigmoid( dampening_expectations(k, i) ) 
+                                * fastsigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -340,15 +340,15 @@
                 {
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 sigmoid( dampening_expectations(k, i) );
                 }
                 else
                 {
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
-                            current_temp_outputs(k, i) = (1-dampening_factor) 
-                                * sigmoid( dampening_expectations(k, i) ) 
+                            current_temp_outputs(k, i) = (1-dampening_factor)
+                                * sigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -376,7 +376,7 @@
         else
             for( int i=0 ; i&lt;size ; i++ )
                 mean_field_input[i] = sigmoid( bias_plus_input[i] );
-        
+
         product(pre_sigmoid_mean_field_output, mean_field_output_weights, mean_field_input);
         pre_sigmoid_mean_field_output += mean_field_output_bias;
 
@@ -388,14 +388,14 @@
                 output[i] = sigmoid( pre_sigmoid_mean_field_output[i] );
     }
     else
-    {        
+    {
 
         if( temp_output.length() != n_lateral_connections_passes+1 )
         {
             temp_output.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i&lt;n_lateral_connections_passes+1 ; i++ )
                 temp_output[i].resize(size);
-        }       
+        }
 
         temp_output.last() = output;
         current_temp_output = temp_output[0];
@@ -426,8 +426,8 @@
                 else
                 {
                     for( int i=0 ; i&lt;size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -441,8 +441,8 @@
                 else
                 {
                     for( int i=0 ; i&lt;size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -476,7 +476,7 @@
             temp_outputs.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i&lt;n_lateral_connections_passes+1 ; i++ )
                 temp_outputs[i].resize(mbatch_size,size);
-        }       
+        }
 
         temp_outputs.last() = outputs;
         current_temp_outputs = temp_outputs[0];
@@ -495,11 +495,11 @@
             previous_temp_outputs = current_temp_outputs;
             current_temp_outputs = temp_outputs[t+1];
             if( topographic_lateral_weights.length() == 0 )
-                productTranspose(dampening_expectations, previous_temp_outputs, 
+                productTranspose(dampening_expectations, previous_temp_outputs,
                                  lateral_weights);
             else
                 for( int b = 0; b&lt;dampening_expectations.length(); b++)
-                    productTopoLateralWeights( dampening_expectations(b), 
+                    productTopoLateralWeights( dampening_expectations(b),
                                                previous_temp_outputs(b) );
 
             dampening_expectations += bias_plus_inputs;
@@ -509,7 +509,7 @@
                 {
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 fastsigmoid( dampening_expectations(k, i) );
                 }
                 else
@@ -517,7 +517,7 @@
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
                             current_temp_outputs(k, i) = (1-dampening_factor)
-                                * fastsigmoid( dampening_expectations(k, i) ) 
+                                * fastsigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -527,7 +527,7 @@
                 {
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
-                            current_temp_outputs(k, i) = 
+                            current_temp_outputs(k, i) =
                                 sigmoid( dampening_expectations(k, i) );
                 }
                 else
@@ -535,7 +535,7 @@
                     for(int k = 0; k &lt; batch_size; k++)
                         for( int i=0 ; i&lt;size ; i++ )
                             current_temp_outputs(k, i) = (1-dampening_factor)
-                                * sigmoid( dampening_expectations(k, i) ) 
+                                * sigmoid( dampening_expectations(k, i) )
                                 + dampening_factor * previous_temp_outputs(k, i);
                 }
             }
@@ -565,7 +565,7 @@
             temp_output.resize(n_lateral_connections_passes+1);
             for( int i=0 ; i&lt;n_lateral_connections_passes+1 ; i++ )
                 temp_output[i].resize(size);
-        }       
+        }
 
         temp_output.last() = output;
         current_temp_output = temp_output[0];
@@ -596,8 +596,8 @@
                 else
                 {
                     for( int i=0 ; i&lt;size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -611,8 +611,8 @@
                 else
                 {
                     for( int i=0 ; i&lt;size ; i++ )
-                        current_temp_output[i] = 
-                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        current_temp_output[i] =
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] )
                             + dampening_factor * previous_temp_output[i];
                 }
             }
@@ -625,7 +625,7 @@
 void RBMLateralBinomialLayer::externalSymetricProductAcc(const Mat&amp; mat, const Vec&amp; v1, const Vec&amp; v2)
 {
 #ifdef BOUNDCHECK
-    if (v1.length()!=mat.length() || mat.width()!=v2.length() 
+    if (v1.length()!=mat.length() || mat.width()!=v2.length()
         || v1.length() != v2.length())
         PLERROR(&quot;externalSymetricProductAcc(Mat,Vec,Vec), incompatible &quot;
                 &quot;arguments sizes&quot;);
@@ -667,7 +667,7 @@
     }
 }
 
-void RBMLateralBinomialLayer::productTopoLateralWeights(const Vec&amp; result, 
+void RBMLateralBinomialLayer::productTopoLateralWeights(const Vec&amp; result,
                                                         const Vec&amp; input ) const
 {
     // Could be made faster, in terms of memory access
@@ -683,21 +683,21 @@
         neuron_h = i%topographic_width;
         wi = 0;
         current_weights = topographic_lateral_weights[i].data();
-        
-        vmin = neuron_v &lt; topographic_patch_vradius ? 
+
+        vmin = neuron_v &lt; topographic_patch_vradius ?
             - neuron_v : - topographic_patch_vradius;
-        vmax = topographic_length - neuron_v - 1 &lt; topographic_patch_vradius ? 
+        vmax = topographic_length - neuron_v - 1 &lt; topographic_patch_vradius ?
             topographic_length - neuron_v - 1: topographic_patch_vradius;
 
-        hmin = neuron_h &lt; topographic_patch_hradius ? 
+        hmin = neuron_h &lt; topographic_patch_hradius ?
             - neuron_h : - topographic_patch_hradius;
-        hmax = topographic_width - neuron_h - 1 &lt; topographic_patch_hradius ? 
+        hmax = topographic_width - neuron_h - 1 &lt; topographic_patch_hradius ?
             topographic_width - neuron_h - 1: topographic_patch_hradius;
 
-        for( int j = -1 * topographic_patch_vradius; 
-             j &lt;= topographic_patch_vradius ; j++ ) 
+        for( int j = -1 * topographic_patch_vradius;
+             j &lt;= topographic_patch_vradius ; j++ )
         {
-            for( int k = -1 * topographic_patch_hradius; 
+            for( int k = -1 * topographic_patch_hradius;
                  k &lt;= topographic_patch_hradius; k++ )
             {
                 connected_neuron = (i+j*topographic_width)+k;
@@ -738,23 +738,23 @@
         current_weights = topographic_lateral_weights[i].data();
         current_weights_gradient = weights_gradient[i].data();
 
-        vmin = neuron_v &lt; topographic_patch_vradius ? 
+        vmin = neuron_v &lt; topographic_patch_vradius ?
             - neuron_v : - topographic_patch_vradius;
-        vmax = topographic_length - neuron_v - 1 &lt; topographic_patch_vradius ? 
+        vmax = topographic_length - neuron_v - 1 &lt; topographic_patch_vradius ?
             topographic_length - neuron_v - 1: topographic_patch_vradius;
 
-        hmin = neuron_h &lt; topographic_patch_hradius ? 
+        hmin = neuron_h &lt; topographic_patch_hradius ?
             - neuron_h : - topographic_patch_hradius;
-        hmax = topographic_width - neuron_h - 1 &lt; topographic_patch_hradius ? 
+        hmax = topographic_width - neuron_h - 1 &lt; topographic_patch_hradius ?
             topographic_width - neuron_h - 1: topographic_patch_hradius;
 
         result_gradient_i = result_gradient[i];
         input_i = input[i];
 
-        for( int j = -1 * topographic_patch_vradius; 
+        for( int j = -1 * topographic_patch_vradius;
              j &lt;= topographic_patch_vradius ; j++ )
         {
-            for( int k = -1 * topographic_patch_hradius; 
+            for( int k = -1 * topographic_patch_hradius;
                  k &lt;= topographic_patch_hradius; k++ )
             {
                 connected_neuron = (i+j*topographic_width)+k;
@@ -763,9 +763,9 @@
                     if( j &gt;= vmin &amp;&amp; j &lt;= vmax &amp;&amp;
                         k &gt;= hmin &amp;&amp; k &lt;= hmax )
                     {
-                        input_gradient[connected_neuron] += 
+                        input_gradient[connected_neuron] +=
                             result_gradient_i * current_weights[wi];
-                        current_weights_gradient[wi] += 
+                        current_weights_gradient[wi] +=
                             //0.5 * ( result_gradient_i * input[connected_neuron] +
                             ( result_gradient_i * input[connected_neuron] +
                               input_i * result_gradient[connected_neuron] );
@@ -783,7 +783,7 @@
 {
     if( !do_not_learn_topographic_lateral_weights )
     {
-        
+
         // Could be made faster, in terms of memory access
         int connected_neuron;
         int wi;
@@ -797,25 +797,25 @@
             neuron_v = i/topographic_width;
             neuron_h = i%topographic_width;
             wi = 0;
-            
-            vmin = neuron_v &lt; topographic_patch_vradius ? 
+
+            vmin = neuron_v &lt; topographic_patch_vradius ?
                 - neuron_v : - topographic_patch_vradius;
-            vmax = topographic_length - neuron_v - 1 &lt; topographic_patch_vradius ? 
+            vmax = topographic_length - neuron_v - 1 &lt; topographic_patch_vradius ?
                 topographic_length - neuron_v - 1: topographic_patch_vradius;
-            
-            hmin = neuron_h &lt; topographic_patch_hradius ? 
+
+            hmin = neuron_h &lt; topographic_patch_hradius ?
                 - neuron_h : - topographic_patch_hradius;
-            hmax = topographic_width - neuron_h - 1 &lt; topographic_patch_hradius ? 
+            hmax = topographic_width - neuron_h - 1 &lt; topographic_patch_hradius ?
                 topographic_width - neuron_h - 1: topographic_patch_hradius;
-            
+
             current_weights = topographic_lateral_weights[i].data();
             pos_values_i = pos_values[i];
             neg_values_i = neg_values[i];
-            
-            for( int j = - topographic_patch_vradius; 
+
+            for( int j = - topographic_patch_vradius;
                  j &lt;= topographic_patch_vradius ; j++ )
             {
-                for( int k = -topographic_patch_hradius; 
+                for( int k = -topographic_patch_hradius;
                      k &lt;= topographic_patch_hradius; k++ )
                 {
                     connected_neuron = (i+j*topographic_width)+k;
@@ -824,9 +824,9 @@
                         if( j &gt;= vmin &amp;&amp; j &lt;= vmax &amp;&amp;
                             k &gt;= hmin &amp;&amp; k &lt;= hmax )
                         {
-                            current_weights[wi] += 
-                                //learning_rate * 0.5 * ( 
-                                learning_rate * ( 
+                            current_weights[wi] +=
+                                //learning_rate * 0.5 * (
+                                learning_rate * (
                                     pos_values_i * pos_values[connected_neuron] -
                                     neg_values_i * neg_values[connected_neuron] );
                         }
@@ -873,7 +873,7 @@
 
         transposeProductAcc( input_gradient, mean_field_output_weights, temp_mean_field_gradient );
 
-        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient, 
+        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient,
                                  mean_field_input, -learning_rate );
         multiplyScaledAdd( temp_mean_field_gradient, 1.0, -learning_rate, mean_field_output_bias);
 
@@ -903,7 +903,7 @@
                 // Contribution from the mean field approximation
                 temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                     output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
+
                 // Contribution from the dampening
                 temp_mean_field_gradient[i] *= dampening_factor;
             }
@@ -914,16 +914,16 @@
             // Lateral weights gradient contribution
             if( topographic_lateral_weights.length() == 0)
             {
-                externalSymetricProductAcc( lateral_weights_gradient, 
+                externalSymetricProductAcc( lateral_weights_gradient,
                                             temp_mean_field_gradient2,
                                             temp_output[t] );
-            
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                     temp_mean_field_gradient2);
             }
             else
             {
-                productTopoLateralWeightsGradients( 
+                productTopoLateralWeightsGradients(
                     temp_output[t],
                     temp_mean_field_gradient,
                     temp_mean_field_gradient2,
@@ -932,7 +932,7 @@
 
             current_temp_output = temp_output[t];
         }
-    
+
         for( int i=0 ; i&lt;size ; i++ )
         {
             output_i = current_temp_output[i];
@@ -983,10 +983,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i&lt;topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR(&quot;In RBMLateralBinomialLayer:bpropUpdate - Not implemented for &quot;
                             &quot;topographic weights&quot;);
@@ -996,8 +996,8 @@
         // Set diagonal to 0
         if( lateral_weights.length() != 0 )
         {
-            real *d = lateral_weights.data();        
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            real *d = lateral_weights.data();
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1057,32 +1057,32 @@
                 for( int i=0 ; i&lt;size ; i++ )
                 {
                     output_i = current_temp_output[i];
-                
+
                     // Contribution from the mean field approximation
                     temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                         output_i * (1-output_i) * temp_mean_field_gradient[i];
-                
+
                     // Contribution from the dampening
                     temp_mean_field_gradient[i] *= dampening_factor;
                 }
-            
+
                 // Input gradient contribution
                 temp_input_gradient += temp_mean_field_gradient2;
-            
+
                 // Lateral weights gradient contribution
                 if( topographic_lateral_weights.length() == 0)
                 {
-                
-                    externalSymetricProductAcc( lateral_weights_gradient, 
+
+                    externalSymetricProductAcc( lateral_weights_gradient,
                                                 temp_mean_field_gradient2,
                                                 temp_outputs[t](j) );
-                
-                    transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                    transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                         temp_mean_field_gradient2);
                 }
                 else
                 {
-                    productTopoLateralWeightsGradients( 
+                    productTopoLateralWeightsGradients(
                         temp_outputs[t](j),
                         temp_mean_field_gradient,
                         temp_mean_field_gradient2,
@@ -1091,7 +1091,7 @@
 
                 current_temp_output = temp_outputs[t](j);
             }
-    
+
             for( int i=0 ; i&lt;size ; i++ )
             {
                 output_i = current_temp_output[i];
@@ -1099,7 +1099,7 @@
             }
 
             temp_input_gradient += temp_mean_field_gradient;
-        
+
             input_gradients(j) += temp_input_gradient;
 
             // Update bias
@@ -1115,7 +1115,7 @@
                 else
                     PLERROR(&quot;In RBMLateralBinomialLayer:bpropUpdate - Not implemented for &quot;
                             &quot;momentum with mini-batches&quot;);
-            }        
+            }
         }
 
         if( topographic_lateral_weights.length() == 0)
@@ -1133,10 +1133,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i&lt;topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR(&quot;In RBMLateralBinomialLayer:bpropUpdate - Not implemented for &quot;
                             &quot;topographic weights&quot;);
@@ -1148,7 +1148,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1191,7 +1191,7 @@
                 // Contribution from the mean field approximation
                 temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                     output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
+
                 // Contribution from the dampening
                 temp_mean_field_gradient[i] *= dampening_factor;
             }
@@ -1203,16 +1203,16 @@
             if( topographic_lateral_weights.length() == 0)
             {
 
-                externalSymetricProductAcc( lateral_weights_gradient, 
+                externalSymetricProductAcc( lateral_weights_gradient,
                                             temp_mean_field_gradient2,
                                             temp_output[t] );
-            
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                     temp_mean_field_gradient2);
             }
             else
             {
-                productTopoLateralWeightsGradients( 
+                productTopoLateralWeightsGradients(
                     temp_output[t],
                     temp_mean_field_gradient,
                     temp_mean_field_gradient2,
@@ -1221,7 +1221,7 @@
 
             current_temp_output = temp_output[t];
         }
-    
+
         for( int i=0 ; i&lt;size ; i++ )
         {
             output_i = current_temp_output[i];
@@ -1253,21 +1253,21 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i&lt;topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR(&quot;In RBMLateralBinomialLayer:bpropUpdate - Not implemented for &quot;
                             &quot;topographic weights&quot;);
             }
         }
-        
+
         // Set diagonal to 0
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1276,7 +1276,7 @@
 real RBMLateralBinomialLayer::fpropNLL(const Vec&amp; target)
 {
     PLASSERT( target.size() == input_size );
-    computeExpectation(); 
+    computeExpectation();
 
     real ret = 0;
     real target_i, expectation_i;
@@ -1295,7 +1295,7 @@
 
 void RBMLateralBinomialLayer::fpropNLL(const Mat&amp; targets, const Mat&amp; costs_column)
 {
-    computeExpectations(); 
+    computeExpectations();
 
     PLASSERT( targets.width() == input_size );
     PLASSERT( targets.length() == batch_size );
@@ -1350,7 +1350,7 @@
                 // Contribution from the mean field approximation
                 temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                     output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
+
                 // Contribution from the dampening
                 temp_mean_field_gradient[i] *= dampening_factor;
             }
@@ -1361,16 +1361,16 @@
             // Lateral weights gradient contribution
             if( topographic_lateral_weights.length() == 0)
             {
-                externalSymetricProductAcc( lateral_weights_gradient, 
+                externalSymetricProductAcc( lateral_weights_gradient,
                                             temp_mean_field_gradient2,
                                             temp_output[t] );
-            
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                     temp_mean_field_gradient2);
             }
             else
             {
-                productTopoLateralWeightsGradients( 
+                productTopoLateralWeightsGradients(
                     temp_output[t],
                     temp_mean_field_gradient,
                     temp_mean_field_gradient2,
@@ -1379,7 +1379,7 @@
 
             current_temp_output = temp_output[t];
         }
-    
+
         for( int i=0 ; i&lt;size ; i++ )
         {
             output_i = current_temp_output[i];
@@ -1409,10 +1409,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i&lt;topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR(&quot;In RBMLateralBinomialLayer:bpropNLL - Not implemented for &quot;
                             &quot;topographic weights&quot;);
@@ -1422,7 +1422,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1465,32 +1465,32 @@
                 for( int i=0 ; i&lt;size ; i++ )
                 {
                     output_i = current_temp_output[i];
-                
+
                     // Contribution from the mean field approximation
                     temp_mean_field_gradient2[i] =  (1-dampening_factor)*
                         output_i * (1-output_i) * temp_mean_field_gradient[i];
-                
+
                     // Contribution from the dampening
                     temp_mean_field_gradient[i] *= dampening_factor;
                 }
-            
+
                 // Input gradient contribution
                 bias_gradients(j) += temp_mean_field_gradient2;
-            
+
                 // Lateral weights gradient contribution
                 if( topographic_lateral_weights.length() == 0)
                 {
 
-                    externalSymetricProductAcc( lateral_weights_gradient, 
+                    externalSymetricProductAcc( lateral_weights_gradient,
                                                 temp_mean_field_gradient2,
                                                 temp_outputs[t](j) );
-                
-                    transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+
+                    transposeProductAcc(temp_mean_field_gradient, lateral_weights,
                                         temp_mean_field_gradient2);
                 }
                 else
                 {
-                    productTopoLateralWeightsGradients( 
+                    productTopoLateralWeightsGradients(
                         temp_outputs[t](j),
                         temp_mean_field_gradient,
                         temp_mean_field_gradient2,
@@ -1498,7 +1498,7 @@
                 }
                 current_temp_output = temp_outputs[t](j);
             }
-    
+
             for( int i=0 ; i&lt;size ; i++ )
             {
                 output_i = current_temp_output[i];
@@ -1524,10 +1524,10 @@
             {
                 if( momentum == 0. )
                     for( int i=0; i&lt;topographic_lateral_weights.length(); i++ )
-                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0,
                                            -learning_rate,
                                            topographic_lateral_weights[i]);
-            
+
                 else
                     PLERROR(&quot;In RBMLateralBinomialLayer:bpropNLL - Not implemented for &quot;
                             &quot;topographic weights&quot;);
@@ -1538,7 +1538,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1570,7 +1570,7 @@
 
 
 void RBMLateralBinomialLayer::update()
-{ 
+{
     //real pos_factor = 0.5 * learning_rate / pos_count;
     //real neg_factor = - 0.5 * learning_rate / neg_count;
     real pos_factor = learning_rate / pos_count;
@@ -1585,7 +1585,7 @@
     {
         multiplyScaledAdd( lateral_weights_pos_stats, neg_factor, pos_factor,
                            lateral_weights_neg_stats);
-        lateral_weights += lateral_weights_neg_stats; 
+        lateral_weights += lateral_weights_neg_stats;
     }
     else
     {
@@ -1600,7 +1600,7 @@
     if( lateral_weights.length() != 0 )
     {
         real *d = lateral_weights.data();
-        for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+        for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
             *d = 0;
     }
 
@@ -1639,13 +1639,13 @@
                                     //- 0.5 * learning_rate);
                                     - learning_rate);
             lateral_weights += lateral_weights_inc;
-        }    
+        }
 
         // Set diagonal to 0
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1694,7 +1694,7 @@
         if( lateral_weights.length() != 0 )
         {
             real *d = lateral_weights.data();
-            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            for (int i=0; i&lt;lateral_weights.length(); i++,d+=lateral_weights.mod()+1)
                 *d = 0;
         }
     }
@@ -1704,7 +1704,7 @@
         {
             for(int b=0; b&lt;pos_values.length(); b++)
                 updateTopoLateralWeightsCD(pos_values(b), neg_values(b));
-            
+
         }
         else
             PLERROR(&quot;In RBMLateralBinomialLayer:bpropNLL - Not implemented for &quot;
@@ -1733,18 +1733,18 @@
 
 void RBMLateralBinomialLayer::declareOptions(OptionList&amp; ol)
 {
-    declareOption(ol, &quot;n_lateral_connections_passes&quot;, 
+    declareOption(ol, &quot;n_lateral_connections_passes&quot;,
                   &amp;RBMLateralBinomialLayer::n_lateral_connections_passes,
                   OptionBase::buildoption,
                   &quot;Number of passes through the lateral connections.\n&quot;);
 
-    declareOption(ol, &quot;dampening_factor&quot;, 
+    declareOption(ol, &quot;dampening_factor&quot;,
                   &amp;RBMLateralBinomialLayer::dampening_factor,
                   OptionBase::buildoption,
                   &quot;Dampening factor ( expectation_t = (1-df) * currrent mean field&quot;
                   &quot; + df * expectation_{t-1}).\n&quot;);
 
-    declareOption(ol, &quot;mean_field_precision_threshold&quot;, 
+    declareOption(ol, &quot;mean_field_precision_threshold&quot;,
                   &amp;RBMLateralBinomialLayer::mean_field_precision_threshold,
                   OptionBase::buildoption,
                   &quot;Mean-field precision threshold that, once reached, stops the mean-field\n&quot;
@@ -1752,59 +1752,59 @@
                   &quot;Precision is computed as:\n&quot;
                   &quot;  dist(last_mean_field, current_mean_field) / size\n&quot;);
 
-    declareOption(ol, &quot;topographic_length&quot;, 
+    declareOption(ol, &quot;topographic_length&quot;,
                   &amp;RBMLateralBinomialLayer::topographic_length,
                   OptionBase::buildoption,
                   &quot;Length of the topographic map.\n&quot;);
 
-    declareOption(ol, &quot;topographic_width&quot;, 
+    declareOption(ol, &quot;topographic_width&quot;,
                   &amp;RBMLateralBinomialLayer::topographic_width,
                   OptionBase::buildoption,
                   &quot;Width of the topographic map.\n&quot;);
 
-    declareOption(ol, &quot;topographic_patch_vradius&quot;, 
+    declareOption(ol, &quot;topographic_patch_vradius&quot;,
                   &amp;RBMLateralBinomialLayer::topographic_patch_vradius,
                   OptionBase::buildoption,
                   &quot;Vertical radius of the topographic local weight patches.\n&quot;);
 
-    declareOption(ol, &quot;topographic_patch_hradius&quot;, 
+    declareOption(ol, &quot;topographic_patch_hradius&quot;,
                   &amp;RBMLateralBinomialLayer::topographic_patch_hradius,
                   OptionBase::buildoption,
                   &quot;Horizontal radius of the topographic local weight patches.\n&quot;);
 
-    declareOption(ol, &quot;topographic_lateral_weights_init_value&quot;, 
+    declareOption(ol, &quot;topographic_lateral_weights_init_value&quot;,
                   &amp;RBMLateralBinomialLayer::topographic_lateral_weights_init_value,
                   OptionBase::buildoption,
                   &quot;Initial value for the topographic_lateral_weights.\n&quot;);
 
-    declareOption(ol, &quot;do_not_learn_topographic_lateral_weights&quot;, 
+    declareOption(ol, &quot;do_not_learn_topographic_lateral_weights&quot;,
                   &amp;RBMLateralBinomialLayer::do_not_learn_topographic_lateral_weights,
                   OptionBase::buildoption,
                   &quot;Indication that the topographic_lateral_weights should\n&quot;
                   &quot;be fixed at their initial value.\n&quot;);
 
-    declareOption(ol, &quot;lateral_weights&quot;, 
+    declareOption(ol, &quot;lateral_weights&quot;,
                   &amp;RBMLateralBinomialLayer::lateral_weights,
                   OptionBase::learntoption,
                   &quot;Lateral connections.\n&quot;);
 
-    declareOption(ol, &quot;topographic_lateral_weights&quot;, 
+    declareOption(ol, &quot;topographic_lateral_weights&quot;,
                   &amp;RBMLateralBinomialLayer::topographic_lateral_weights,
                   OptionBase::learntoption,
                   &quot;Local topographic lateral connections.\n&quot;);
 
-    declareOption(ol, &quot;use_parametric_mean_field&quot;, 
+    declareOption(ol, &quot;use_parametric_mean_field&quot;,
                   &amp;RBMLateralBinomialLayer::use_parametric_mean_field,
                   OptionBase::buildoption,
                   &quot;Indication that a parametric predictor of the mean-field\n&quot;
                   &quot;approximation of the hidden layer conditional distribution.\n&quot;);
 
-    declareOption(ol, &quot;mean_field_output_weights&quot;, 
+    declareOption(ol, &quot;mean_field_output_weights&quot;,
                   &amp;RBMLateralBinomialLayer::mean_field_output_weights,
                   OptionBase::learntoption,
                   &quot;Output weights of the mean field predictor.\n&quot;);
 
-    declareOption(ol, &quot;mean_field_output_bias&quot;, 
+    declareOption(ol, &quot;mean_field_output_bias&quot;,
                   &amp;RBMLateralBinomialLayer::mean_field_output_bias,
                   OptionBase::learntoption,
                   &quot;Output bias of the mean field predictor.\n&quot;);
@@ -1827,11 +1827,11 @@
     if( n_lateral_connections_passes &lt; 0 )
         PLERROR(&quot;In RBMLateralBinomialLayer::build_(): n_lateral_connections_passes\n&quot;
                 &quot; should be &gt;= 0.&quot;);
- 
+
     if( use_parametric_mean_field &amp;&amp; topographic_length &gt; 0 &amp;&amp; topographic_width &gt; 0 )
         PLERROR(&quot;RBMLateralBinomialLayer::build_(): can't use parametric mean field &quot;
             &quot;and topographic lateral connections.&quot;);
-    
+
     if( use_parametric_mean_field )
     {
         mean_field_output_weights.resize(size,size);
@@ -1851,7 +1851,7 @@
         {
             bias_inc.resize( size );
             lateral_weights_inc.resize(size,size);
-        }   
+        }
     }
     else
     {
@@ -1871,10 +1871,10 @@
         topographic_lateral_weights_gradient.resize(size);
         for( int i=0; i&lt;size; i++ )
         {
-            topographic_lateral_weights[i].resize( 
+            topographic_lateral_weights[i].resize(
                 ( 2 * topographic_patch_hradius + 1 ) *
                 ( 2 * topographic_patch_vradius + 1 ) - 1 );
-            topographic_lateral_weights_gradient[i].resize( 
+            topographic_lateral_weights_gradient[i].resize(
                 ( 2 * topographic_patch_hradius + 1 ) *
                 ( 2 * topographic_patch_vradius + 1 ) - 1 );
         }

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -56,15 +56,15 @@
 public:
     //#####  Public Build Options  ############################################
 
-    //! Number of passes through the lateral connections 
+    //! Number of passes through the lateral connections
     int n_lateral_connections_passes;
 
-    //! Dampening factor 
+    //! Dampening factor
     //! ( expectation_t = (1-df) * currrent mean field + df * expectation_{t-1})
     real dampening_factor;
 
-    //! Mean-field precision threshold that, once reached, stops the mean-field 
-    //! expectation approximation computation. Used only in computeExpectation(). 
+    //! Mean-field precision threshold that, once reached, stops the mean-field
+    //! expectation approximation computation. Used only in computeExpectation().
     //! Precision is computed as:
     //!   dist(last_mean_field, current_mean_field) / size
     real mean_field_precision_threshold;
@@ -87,7 +87,7 @@
     //! Indication that the topographic_lateral_weights should
     //! be fixed at their initial value.
     bool do_not_learn_topographic_lateral_weights;
-    
+
     //! Lateral connections
     Mat lateral_weights;
 
@@ -100,7 +100,7 @@
     //! Accumulates negative contribution to the gradient of lateral weights
     Mat lateral_weights_neg_stats;
 
-    //! Indication that a parametric predictor of the mean-field 
+    //! Indication that a parametric predictor of the mean-field
     //! approximation of the hidden layer conditional distribution.
     bool use_parametric_mean_field;
 
@@ -186,18 +186,18 @@
     virtual void accumulateNegStats( const Vec&amp; neg_values );
     virtual void accumulateNegStats( const Mat&amp; neg_values );
 
-    //! Update bias and lateral connections parameters 
+    //! Update bias and lateral connections parameters
     //! according to accumulated statistics
     virtual void update();
 
     //! Updates ONLY the bias parameters according to the given gradient
     virtual void update( const Vec&amp; grad );
 
-    //! Update bias and lateral connections 
+    //! Update bias and lateral connections
     //! parameters according to one pair of vectors
     virtual void update( const Vec&amp; pos_values, const Vec&amp; neg_values );
 
-    //! Update bias and lateral connections 
+    //! Update bias and lateral connections
     //! parameters according to one pair of matrices.
     virtual void update( const Mat&amp; pos_values, const Mat&amp; neg_values );
 
@@ -275,13 +275,13 @@
     static void declareOptions(OptionList&amp; ol);
 
     //! Computes mat[i][j] += 0.5 * (v1[i] * v2[j] +  v1[j] * v2[i])
-    void externalSymetricProductAcc(const Mat&amp; mat, const Vec&amp; v1, 
+    void externalSymetricProductAcc(const Mat&amp; mat, const Vec&amp; v1,
                                     const Vec&amp; v2);
 
     void productTopoLateralWeights( const Vec&amp; result, const Vec&amp; input ) const;
 
     void productTopoLateralWeightsGradients( const Vec&amp; input, const Vec&amp; input_gradient,
-                                             const Vec&amp; result_gradient, 
+                                             const Vec&amp; result_gradient,
                                              const TVec&lt; Vec &gt;&amp; weights_gradient );
 
     void updateTopoLateralWeightsCD( const Vec&amp; pos_values, const Vec&amp; neg_values );

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -393,7 +393,7 @@
             this-&gt;classname().c_str());
 }
 
-void RBMLayer::bpropNLL(const Mat&amp; targets,  const Mat&amp; costs_column, 
+void RBMLayer::bpropNLL(const Mat&amp; targets,  const Mat&amp; costs_column,
                         Mat&amp; bias_gradients)
 {
     PLERROR(&quot;In RBMLayer::bpropNLL(): not implemented in subclass %s&quot;,
@@ -469,7 +469,7 @@
     }
 
     applyBiasDecay();
-    
+
     clearStats();
 }
 
@@ -656,7 +656,7 @@
     columnSum(gibbs_neg_values,tmp);
     if (neg_count==0)
         multiply(tmp, normalize_factor, bias_neg_stats);
-    else // bias_neg_stats &lt;-- tmp*(1-gibbs_chain_statistics_forgetting_factor)/minibatch_size 
+    else // bias_neg_stats &lt;-- tmp*(1-gibbs_chain_statistics_forgetting_factor)/minibatch_size
         //                    +gibbs_chain_statistics_forgetting_factor*bias_neg_stats
         multiplyScaledAdd(tmp,gibbs_ma_coefficient,
                           normalize_factor*(1-gibbs_ma_coefficient),
@@ -674,7 +674,7 @@
         gibbs_ma_coefficient = sigmoid(gibbs_ma_increment + inverse_sigmoid(gibbs_ma_coefficient));
 
 
-    // delta w = lrate * ( meanoverrows(pos_values) - neg_stats ) 
+    // delta w = lrate * ( meanoverrows(pos_values) - neg_stats )
     columnSum(pos_values,tmp);
     multiplyAcc(bias, tmp, learning_rate*normalize_factor);
     multiplyAcc(bias, bias_neg_stats, -learning_rate);
@@ -761,7 +761,7 @@
 
     for( int i=0 ; i&lt;size ; i++ )
         bg[i] = -bps[i] + bns[i];
-    
+
     addBiasDecay(bias_gradient);
 
 }
@@ -832,7 +832,7 @@
         real *bg = bias_gradients[b];
         real *b = bias.data();
         bias_decay_type = lowerstring(bias_decay_type);
-        
+
         if (bias_decay_type==&quot;negative&quot;)  // Pushes the biases towards -\infty
             for( int i=0 ; i&lt;size ; i++ )
                 bg[i] += avg_lr * bias_decay_parameter;
@@ -847,7 +847,7 @@
 
 void RBMLayer::applyBiasDecay()
 {
-    
+
     PLASSERT(bias.size()==size);
 
     real* b = bias.data();
@@ -860,11 +860,11 @@
             b[i] -= learning_rate * bias_decay_parameter;
     else if (bias_decay_type==&quot;l2&quot;) // L2 penalty on the biases
         bias *= (1 - learning_rate * bias_decay_parameter);
-    else 
+    else
         PLERROR(&quot;RBMLayer::applyBiasDecay(string) bias_decay_type %s is not in&quot;
                 &quot; the list, in subclass %s\n&quot;,bias_decay_type.c_str(),classname().c_str());
 
-}   
+}
 
 } // end of namespace PLearn
 

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -297,7 +297,7 @@
     virtual void freeEnergyContributionGradient(const Vec&amp; unit_activations,
                                                 Vec&amp; unit_activations_gradient,
                                                 real output_gradient = 1,
-                                                bool accumulate = false ) 
+                                                bool accumulate = false )
         const;
 
     //! Returns a number of different configurations the layer can be in.

Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -76,7 +76,7 @@
                   &quot;its inverse sigmoid by gibbs_ma_increment). After the last\n&quot;
                   &quot;increase has been made, the moving average coefficient stays constant.\n&quot;);
 
-    declareOption(ol, &quot;gibbs_ma_increment&quot;, 
+    declareOption(ol, &quot;gibbs_ma_increment&quot;,
                   &amp;RBMMatrixConnection::gibbs_ma_increment,
                   OptionBase::buildoption,
                   &quot;The increment in the inverse sigmoid of the moving &quot;
@@ -84,39 +84,39 @@
                   &quot;to apply after the number of updates reaches an element &quot;
                   &quot;of the gibbs_ma_schedule.\n&quot;);
 
-    declareOption(ol, &quot;gibbs_initial_ma_coefficient&quot;, 
+    declareOption(ol, &quot;gibbs_initial_ma_coefficient&quot;,
                   &amp;RBMMatrixConnection::gibbs_initial_ma_coefficient,
                   OptionBase::buildoption,
                   &quot;Initial moving average coefficient for the negative phase &quot;
                   &quot;statistics in the Gibbs chain.\n&quot;);
 
-    declareOption(ol, &quot;L1_penalty_factor&quot;, 
+    declareOption(ol, &quot;L1_penalty_factor&quot;,
                   &amp;RBMMatrixConnection::L1_penalty_factor,
                   OptionBase::buildoption,
                   &quot;Optional (default=0) factor of L1 regularization term, i.e.\n&quot;
                   &quot;minimize L1_penalty_factor * sum_{ij} |weights(i,j)| &quot;
                   &quot;during training.\n&quot;);
 
-    declareOption(ol, &quot;L2_penalty_factor&quot;, 
+    declareOption(ol, &quot;L2_penalty_factor&quot;,
                   &amp;RBMMatrixConnection::L2_penalty_factor,
                   OptionBase::buildoption,
                   &quot;Optional (default=0) factor of L2 regularization term, i.e.\n&quot;
                   &quot;minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 &quot;
                   &quot;during training.\n&quot;);
 
-    declareOption(ol, &quot;L2_decrease_constant&quot;, 
+    declareOption(ol, &quot;L2_decrease_constant&quot;,
                   &amp;RBMMatrixConnection::L2_decrease_constant,
                   OptionBase::buildoption,
         &quot;Parameter of the L2 penalty decrease (see L2_decrease_type).&quot;,
         OptionBase::advanced_level);
 
-    declareOption(ol, &quot;L2_shift&quot;, 
+    declareOption(ol, &quot;L2_shift&quot;,
                   &amp;RBMMatrixConnection::L2_shift,
                   OptionBase::buildoption,
         &quot;Parameter of the L2 penalty decrease (see L2_decrease_type).&quot;,
         OptionBase::advanced_level);
 
-    declareOption(ol, &quot;L2_decrease_type&quot;, 
+    declareOption(ol, &quot;L2_decrease_type&quot;,
                   &amp;RBMMatrixConnection::L2_decrease_type,
                   OptionBase::buildoption,
         &quot;The kind of L2 decrease that is being applied. The decrease\n&quot;
@@ -127,7 +127,7 @@
         &quot; - 'sigmoid_like': sigmoid((L2_shift - t) * L2_decrease_constant)&quot;,
         OptionBase::advanced_level);
 
-    declareOption(ol, &quot;L2_n_updates&quot;, 
+    declareOption(ol, &quot;L2_n_updates&quot;,
                   &amp;RBMMatrixConnection::L2_n_updates,
                   OptionBase::learntoption,
         &quot;Number of times that weights have been changed by the L2 penalty\n&quot;
@@ -273,7 +273,7 @@
             }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 
     clearStats();
@@ -332,7 +332,7 @@
             }
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -386,7 +386,7 @@
          */
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -427,7 +427,7 @@
         -learning_rate*(1-background_gibbs_update_ratio)*normalize_factor,
         real(1));
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -472,7 +472,7 @@
                              learning_rate*normalize_factor, real(1));
     multiplyAcc(weights, weights_neg_stats, -learning_rate);
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -621,7 +621,7 @@
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, output_gradient, input, -learning_rate );
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -654,7 +654,7 @@
     transposeProductScaleAcc(weights, output_gradients, inputs,
                              -learning_rate / inputs.length(), real(1));
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -695,7 +695,7 @@
                          input);
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         addWeightPenalty(rbm_weights, rbm_weights_gradient);
 }
 
@@ -766,7 +766,7 @@
         PLCHECK_MSG( false,
                      &quot;Unknown port configuration&quot; );
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         applyWeightPenalty();
 }
 
@@ -800,7 +800,7 @@
                 w_i[j] = wns_i[j]/pos_count - wps_i[j]/neg_count;
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         addWeightPenalty(weights, weights_gradient);
 }
 
@@ -841,7 +841,7 @@
                 w_i[j] =  *nuv_i * ndv[j] - *puv_i * pdv[j] ;
     }
 
-    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0))
         addWeightPenalty(weights, weights_gradient);
 }
 
@@ -867,7 +867,7 @@
         {
             if( delta_L2 != 0. )
                 w_[j] *= (1 - delta_L2);
-        
+
             if( delta_L1 != 0. )
             {
                 if( w_[j] &gt; delta_L1 )
@@ -901,7 +901,7 @@
         {
             if( delta_L2 != 0. )
                 gw_[j] += delta_L2*w_[j];
-        
+
             if( delta_L1 != 0. )
             {
                 if( w_[j] &gt; 0 )

Modified: trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixConnectionNatGrad.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -135,7 +135,7 @@
 }
 
 
-void RBMMatrixConnectionNatGrad::bpropUpdate(const Mat&amp; inputs, 
+void RBMMatrixConnectionNatGrad::bpropUpdate(const Mat&amp; inputs,
                                              const Mat&amp; outputs,
                                              Mat&amp; input_gradients,
                                              const Mat&amp; output_gradients,
@@ -171,9 +171,9 @@
     }
     pos_count++;
 }
- 
 
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -48,7 +48,7 @@
     &quot;RBMMatrixConnection's weights&quot;,
     &quot;&quot;);
 
-RBMMatrixTransposeConnection::RBMMatrixTransposeConnection( 
+RBMMatrixTransposeConnection::RBMMatrixTransposeConnection(
     PP&lt;RBMMatrixConnection&gt; the_rbm_matrix_connection,
     real the_learning_rate,
     bool call_build_) :
@@ -63,18 +63,18 @@
 
 void RBMMatrixTransposeConnection::declareOptions(OptionList&amp; ol)
 {
-    declareOption(ol, &quot;rbm_matrix_connection&quot;, 
+    declareOption(ol, &quot;rbm_matrix_connection&quot;,
                   &amp;RBMMatrixTransposeConnection::rbm_matrix_connection,
                   OptionBase::buildoption,
                   &quot;RBMMatrixConnection from which the weights are taken&quot;);
 
-    declareOption(ol, &quot;learn_scale&quot;, 
+    declareOption(ol, &quot;learn_scale&quot;,
                   &amp;RBMMatrixTransposeConnection::learn_scale,
                   OptionBase::buildoption,
                   &quot;Indication that the scale of the weight matrix should be &quot;
                   &quot;learned.\n&quot;);
 
-    declareOption(ol, &quot;scale&quot;, 
+    declareOption(ol, &quot;scale&quot;,
                   &amp;RBMMatrixTransposeConnection::scale,
                   OptionBase::learntoption,
                   &quot;Learned scale for weight matrix.\n&quot;);
@@ -353,7 +353,7 @@
 }
 
 //! this version allows to obtain the input gradient as well
-void RBMMatrixTransposeConnection::bpropUpdate(const Vec&amp; input, 
+void RBMMatrixTransposeConnection::bpropUpdate(const Vec&amp; input,
                                                const Vec&amp; output,
                                                Vec&amp; input_gradient,
                                                const Vec&amp; output_gradient,
@@ -375,11 +375,11 @@
     else
     {
         input_gradient.resize( down_size );
-        
+
         // input_gradient = weights' * output_gradient
         product( input_gradient, weights, output_gradient );
     }
-    
+
     // weights -= learning_rate * output_gradient * input'
     externalProductScaleAcc( weights, input, output_gradient, -learning_rate );
     if( learn_scale )

Modified: trunk/plearn_learners/online/RBMMatrixTransposeConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMatrixTransposeConnection.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -86,7 +86,7 @@
     //#####  Public Member Functions  #########################################
 
     //! Default constructor.
-    RBMMatrixTransposeConnection( 
+    RBMMatrixTransposeConnection(
         PP&lt;RBMMatrixConnection&gt; the_rbm_matrix_connection = 0,
         real the_learning_rate = 0,
         bool call_build_ = false);

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -661,7 +661,7 @@
                                                               layer_size));
 
         bias.subVec(init_pos, layer_size) &lt;&lt; layer-&gt;bias;
-        layer-&gt;bias = bias.subVec(init_pos, layer_size);        
+        layer-&gt;bias = bias.subVec(init_pos, layer_size);
 
         // We changed fields of layer, so we need to rebuild it (especially
         // if it is another RBMMixedLayer)
@@ -745,7 +745,7 @@
         Vec output_i = output.subVec( begin, size_i );
         sub_layers[i]-&gt;getConfiguration(conf_i % conf_layer_i, output_i);
         conf_i /= conf_layer_i;
-    }    
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -135,7 +135,7 @@
 void RBMModule::declareOptions(OptionList&amp; ol)
 {
     // Build options.
-    
+
     declareOption(ol, &quot;visible_layer&quot;, &amp;RBMModule::visible_layer,
                   OptionBase::buildoption,
         &quot;Visible layer of the RBM.&quot;);
@@ -183,12 +183,12 @@
                   &amp;RBMModule::deterministic_reconstruction_in_cd,
                   OptionBase::buildoption,
         &quot;Whether to use the expectation of the visible (given a hidden sample)\n&quot;
-	&quot;or a sample of the visible in the contrastive divergence learning.\n&quot;
+        &quot;or a sample of the visible in the contrastive divergence learning.\n&quot;
         &quot;In other words, instead of the classical Gibbs sampling\n&quot;
         &quot;   v_0 --&gt; h_0 ~ p(h|v_0) --&gt; v_1 ~ p(v|h_0) --&gt;  p(h|v_1)\n&quot;
         &quot;we will have by setting 'deterministic_reconstruction_in_cd=1'\n&quot;
         &quot;   v_0 --&gt; h_0 ~ p(h|v_0) --&gt; v_1 = E(v|h_0) --&gt;  p(h|E(v|h_0)).&quot;);
- 
+
     declareOption(ol, &quot;standard_cd_grad&quot;,
                   &amp;RBMModule::standard_cd_grad,
                   OptionBase::buildoption,
@@ -253,7 +253,7 @@
                   &quot;of w.r.t. the contrastive divergence.\n&quot;);
 
     // Learnt options.
-    
+
     declareOption(ol, &quot;Gibbs_step&quot;,
                   &amp;RBMModule::Gibbs_step,
                   OptionBase::learntoption,
@@ -352,13 +352,13 @@
     addPortName(&quot;hidden_bias&quot;);
     addPortName(&quot;weights&quot;);
     addPortName(&quot;neg_log_likelihood&quot;);
-    // a column matrix with one element -log P(h) for each row h of &quot;hidden&quot;, 
+    // a column matrix with one element -log P(h) for each row h of &quot;hidden&quot;,
     // used as an input port, with neg_log_pvisible_given_phidden as output
-    addPortName(&quot;neg_log_phidden&quot;); 
+    addPortName(&quot;neg_log_phidden&quot;);
     // compute column matrix with one entry -log P(x) = -log( sum_h P(x|h) P(h) ) for
-    // each row x of &quot;visible&quot;, and where {P(h)}_h is provided 
+    // each row x of &quot;visible&quot;, and where {P(h)}_h is provided
     // in &quot;neg_log_phidden&quot; for the set of h's in &quot;hidden&quot;.
-    addPortName(&quot;neg_log_pvisible_given_phidden&quot;); 
+    addPortName(&quot;neg_log_pvisible_given_phidden&quot;);
     if(reconstruction_connection)
     {
         addPortName(&quot;visible_reconstruction.state&quot;);
@@ -779,7 +779,7 @@
     if (compute_contrastive_divergence)
     {
         contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)];
-/* YB: I don't agree with this error message: the behavior should be adapted to the provided ports. 
+/* YB: I don't agree with this error message: the behavior should be adapted to the provided ports.
       if (!contrastive_divergence || !contrastive_divergence-&gt;isEmpty())
             PLERROR(&quot;In RBMModule::fprop - When option &quot;
                     &quot;'compute_contrastive_divergence' is 'true', the &quot;
@@ -1117,7 +1117,7 @@
                 n_samples = hidden_sample-&gt;length();
             }
             PLCHECK( n_samples &gt; 0 );
-            
+
             // the visible_layer-&gt;expectations contain the &quot;state&quot; from which we
             // start or continue the chain
             if (visible_layer-&gt;samples.length() != n_samples)
@@ -1417,7 +1417,7 @@
     Mat* contrastive_divergence = NULL;
     if (compute_contrastive_divergence)
         contrastive_divergence = ports_value[getPortIndex(&quot;contrastive_divergence&quot;)];
-    bool computed_contrastive_divergence = compute_contrastive_divergence &amp;&amp; 
+    bool computed_contrastive_divergence = compute_contrastive_divergence &amp;&amp;
         contrastive_divergence &amp;&amp; !contrastive_divergence-&gt;isEmpty();
 
     // Ensure the gradient w.r.t. contrastive divergence is 1 (if provided).
@@ -1550,9 +1550,9 @@
         PLASSERT(hidden &amp;&amp; !hidden-&gt;isEmpty());
         setAllLearningRates(grad_learning_rate);
         visible_layer-&gt;bpropUpdate(*visible_activations,
-                                   *visible, visible_act_grad, *visible_grad, 
+                                   *visible, visible_act_grad, *visible_grad,
                                    false);
-        
+
 //        PLASSERT_MSG(!visible_bias_grad,&quot;back-prop into visible bias  not implemented for downward fprop&quot;);
 //        PLASSERT_MSG(!weights_grad,&quot;back-prop into weights  not implemented for downward fprop&quot;);
 //        hidden_grad-&gt;resize(mbs,hidden_layer-&gt;size);
@@ -1695,7 +1695,7 @@
                 vis_expect_ptr = visible_layer-&gt;getExpectations();
                 negative_phase_visible_samples = &amp;vis_expect_ptr;
             }
-	    else // classical CD learning
+            else // classical CD learning
                negative_phase_visible_samples = &amp;(visible_layer-&gt;samples);
             negative_phase_hidden_activations = &amp;(hidden_layer-&gt;activations);
             negative_phase_hidden_expectations = &amp;(hidden_layer-&gt;getExpectations());
@@ -1906,7 +1906,7 @@
 
         // UGLY HACK WHICH BREAKS THE RULE THAT RBMMODULE CAN BE CALLED IN DIFFERENT CONTEXTS AND fprop/bprop ORDERS
         // BUT NECESSARY WHEN hidden WAS AN INPUT
-        if (hidden_is_output) 
+        if (hidden_is_output)
         {
             // Hidden layer bias update
             hidden_layer-&gt;bpropUpdate(*hidden_act,
@@ -1960,7 +1960,7 @@
                  visible_layer-&gt;classname()==&quot;RBMGaussianlLayer&quot;);
         PLASSERT(connection-&gt;classname()==&quot;RBMMatrixConnection&quot;);
         PLASSERT(hidden &amp;&amp; !hidden-&gt;isEmpty());
-        // FE(x) = -b'x - sum_i softplus(hidden_layer-&gt;activation[i])        
+        // FE(x) = -b'x - sum_i softplus(hidden_layer-&gt;activation[i])
         // dFE(x)/dx = -b - sum_i sigmoid(hidden_layer-&gt;activation[i]) W_i
         // dC/dxt = -b dC/dFE - dC/dFE sum_i p_ti W_i
         int mbs=energy_grad-&gt;length();

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -284,7 +284,7 @@
 public:
     //! Forward the given learning rate to all elements of this module.
     void setAllLearningRates(real lr);
-        
+
     //! Compute activations on the hidden layer based on the provided
     //! visible input.
     //! If 'hidden_bias' is not null nor empty, then it is used as an

Modified: trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -165,7 +165,7 @@
         PLERROR(&quot;In RBMMultitaskClassificationModule::build_(): &quot;
                 &quot;n_mean_field_iterations should be &gt; 0\n&quot;);
 
-    last_to_target_gradient.resize( last_to_target-&gt;up_size, 
+    last_to_target_gradient.resize( last_to_target-&gt;up_size,
                                     last_to_target-&gt;down_size );
 
     // If we have a random_gen, share it with the ones who do not
@@ -233,27 +233,27 @@
     output.resize( output_size );
 
     previous_to_last-&gt;fprop( input, mean_field_activations_hidden[0] );
-    last_layer-&gt;fprop( mean_field_activations_hidden[0], 
+    last_layer-&gt;fprop( mean_field_activations_hidden[0],
                        mean_field_approximations_hidden[0] );
 
     Mat weights = last_to_target-&gt;weights;
     for( int t=0; t&lt;n_mean_field_iterations; t++ )
     {
-        transposeProduct( mean_field_activations_target[t], weights, 
+        transposeProduct( mean_field_activations_target[t], weights,
                           mean_field_approximations_hidden[t] );
         target_layer-&gt;fprop( mean_field_activations_target[t],
                              mean_field_approximations_target[t] );
-        
+
         if( t != n_mean_field_iterations -1 )
         {
-            product( mean_field_activations_hidden[t+1], weights, 
+            product( mean_field_activations_hidden[t+1], weights,
                      mean_field_approximations_target[t] );
             mean_field_activations_hidden[t+1] += mean_field_activations_hidden[0];
             last_layer-&gt;fprop( mean_field_activations_hidden[t+1],
                                mean_field_approximations_hidden[t+1] );
         }
     }
-    
+
     if( fprop_outputs_activation )
     {
         output &lt;&lt; mean_field_activations_target.last();
@@ -317,9 +317,9 @@
                             mean_field_approximations_hidden[t],
                             mean_field_activations_gradient_target);
 
-        product( mean_field_approximations_gradient_hidden, weights, 
+        product( mean_field_approximations_gradient_hidden, weights,
                           mean_field_activations_gradient_target);
-        
+
         if( t != 0 )
         {
             last_layer-&gt;bpropUpdate( mean_field_activations_hidden[t],
@@ -333,7 +333,7 @@
                                 mean_field_approximations_target[t-1]
                                 );
 
-            transposeProduct( mean_field_approximations_gradient_target, weights, 
+            transposeProduct( mean_field_approximations_gradient_target, weights,
                               mean_field_activations_gradient_hidden);
         }
     }
@@ -345,11 +345,11 @@
         );
 
     previous_to_last-&gt;bpropUpdate( input, mean_field_activations_hidden[0],
-                                   input_gradient, 
+                                   input_gradient,
                                    mean_field_activations_gradient_hidden,
                                    accumulate);
 
-    multiplyAcc( weights, last_to_target_gradient, 
+    multiplyAcc( weights, last_to_target_gradient,
                  - (last_to_target-&gt;learning_rate) );
 }
 

Modified: trunk/plearn_learners/online/RBMMultitaskClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMMultitaskClassificationModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMMultitaskClassificationModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -79,7 +79,7 @@
     int n_mean_field_iterations;
 
     //! Indication that fprop should output the value of the activation
-    //! before the squashing function and the application of the bias, 
+    //! before the squashing function and the application of the bias,
     //! instead of the mean-field approximation.
     bool fprop_outputs_activation;
 
@@ -194,7 +194,7 @@
     mutable Vec mean_field_approximations_gradient_target;
     mutable Vec mean_field_activations_gradient_hidden;
     mutable Vec mean_field_approximations_gradient_hidden;
-    
+
 };
 
 // Declares a few other classes and functions related to this class

Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -74,7 +74,7 @@
     else
         sample.clear();
 
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, node_sample, sub_tree_size;
     int offset = 0;
 
@@ -85,13 +85,13 @@
         sub_tree_size = node;
         while( depth &lt; tree_depth )
         {
-            node_sample = random_gen-&gt;binomial_sample( 
+            node_sample = random_gen-&gt;binomial_sample(
                 local_node_expectation[ node + offset ] );
             if( use_signed_samples )
                 sample[node + offset] = 2*node_sample-1;
             else
                 sample[node + offset] = node_sample;
-            
+
             // Descending in the tree
             sub_tree_size /= 2;
             if ( node_sample &gt; 0.5 )
@@ -144,14 +144,14 @@
     if( expectation_is_up_to_date )
         return;
 
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, sub_tree_size, grand_parent;
     int offset = 0;
     bool left_of_grand_parent;
     real grand_parent_prob;
 
     // Get local expectations at every node
-    
+
     // Divide and conquer computation of local (conditional) free energies
     for( int t=0; t&lt;n_trees; t++ )
     {
@@ -178,12 +178,12 @@
         {
             for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
             {
-                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
+                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) *
                 //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
-                //off_free_energy[ n + offset ] = 
+                //off_free_energy[ n + offset ] =
                 //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
                 // Now working in log-domain
-                on_free_energy[ n + offset ] = activation[n+offset] + 
+                on_free_energy[ n + offset ] = activation[n+offset] +
                     logadd( on_free_energy[n + offset - (sub_tree_size/2+1)],
                             off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
                 if( use_signed_samples )
@@ -191,28 +191,28 @@
                         logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
                 else
-                    off_free_energy[ n + offset ] = 
+                    off_free_energy[ n + offset ] =
                         logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
-                
+
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
         }
         offset += n_nodes_per_tree;
-    }    
-    
+    }
+
     for( int i=0 ; i&lt;size ; i++ )
         //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
         // Now working in log-domain
-        local_node_expectation[i] = safeexp(on_free_energy[i] 
+        local_node_expectation[i] = safeexp(on_free_energy[i]
                                             - logadd(on_free_energy[i], off_free_energy[i]));
 
     // Compute marginal expectations
     offset = 0;
     for( int t=0; t&lt;n_trees; t++ )
     {
-        // Initialize root        
+        // Initialize root
         node = n_nodes_per_tree / 2;
         expectation[ node + offset ] = local_node_expectation[ node + offset ];
         off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
@@ -321,14 +321,14 @@
     PLASSERT( input.size() == input_size );
     output.resize( output_size );
 
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, sub_tree_size, grand_parent;
-    int offset = 0;    
+    int offset = 0;
     bool left_of_grand_parent;
     real grand_parent_prob;
 
     // Get local expectations at every node
-    
+
     // Divide and conquer computation of local (conditional) free energies
     for( int t=0; t&lt;n_trees; t++ )
     {
@@ -355,40 +355,40 @@
         {
             for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 2*sub_tree_size + 2 )
             {
-                //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]) * 
+                //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]) *
                 //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
-                //off_free_energy[ n + offset ] = 
+                //off_free_energy[ n + offset ] =
                 //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
                 // Now working in the log-domain
                 on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset] +
-                    logadd( on_free_energy[n + offset - (sub_tree_size/2+1)], 
+                    logadd( on_free_energy[n + offset - (sub_tree_size/2+1)],
                             off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
                 if( use_signed_samples )
                     off_free_energy[ n + offset ] = -(input[n+offset] + bias[n+offset]) +
-                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
                 else
-                    off_free_energy[ n + offset ] = 
-                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                    off_free_energy[ n + offset ] =
+                        logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
                                 off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
         }
         offset += n_nodes_per_tree;
-    }    
-    
+    }
+
     for( int i=0 ; i&lt;size ; i++ )
         //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
         // Now working in log-domain
-        local_node_expectation[i] = safeexp(on_free_energy[i] 
+        local_node_expectation[i] = safeexp(on_free_energy[i]
                                             - logadd(on_free_energy[i], off_free_energy[i]));
 
     // Compute marginal expectations
-    offset = 0;    
+    offset = 0;
     for( int t=0; t&lt;n_trees; t++ )
     {
-        // Initialize root        
+        // Initialize root
         node = n_nodes_per_tree / 2;
         output[ node + offset ] = local_node_expectation[ node + offset ];
         off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
@@ -516,7 +516,7 @@
     }
 
     // Compute gradient on marginal expectations
-    int n_nodes_per_tree = size / n_trees;    
+    int n_nodes_per_tree = size / n_trees;
     int node, depth, sub_tree_size, grand_parent;
     int offset = 0;
     bool left_of_grand_parent;
@@ -537,7 +537,7 @@
             left_of_grand_parent = true;
             for( int n=sub_tree_size; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
             {
-                out_grad = output_gradient[ n + offset ] + 
+                out_grad = output_gradient[ n + offset ] +
                     on_tree_gradient[ n + offset ] ;
                 off_grad = off_tree_gradient[ n + offset ] ;
                 node_exp = local_node_expectation[ n + offset ];
@@ -548,9 +548,9 @@
                     grand_parent = n + offset + 3*sub_tree_size + 3;
                     grand_parent_prob = output[ grand_parent ];
                     // Gradient for rest of the tree
-                    on_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
-                          + off_grad * (1 - node_exp) ) 
+                    on_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
+                          + off_grad * (1 - node_exp) )
                         * parent_exp;
                     left_of_grand_parent = false;
                 }
@@ -559,20 +559,20 @@
                     grand_parent = n + offset - sub_tree_size - 1;
                     grand_parent_prob = off_expectation[ grand_parent ];
                     // Gradient for rest of the tree
-                    off_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
+                    off_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
                           + off_grad * (1 - node_exp) )
                         * parent_exp;
                     left_of_grand_parent = true;
                 }
 
                 // Gradient w/r current node
-                local_node_expectation_gradient[ n + offset ] += 
+                local_node_expectation_gradient[ n + offset ] +=
                     ( out_grad - off_grad ) * parent_exp * grand_parent_prob;
                     //* node_exp * ( 1 - node_exp );
 
                 // Gradient w/r parent node
-                local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] += 
+                local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] +=
                     ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob;
                     //* parent_exp * (1-parent_exp) ;
 
@@ -582,7 +582,7 @@
             left_of_grand_parent = true;
             for( int n=3*sub_tree_size+2; n&lt;n_nodes_per_tree; n += 4*sub_tree_size + 4 )
             {
-                out_grad = output_gradient[ n + offset ] + 
+                out_grad = output_gradient[ n + offset ] +
                     on_tree_gradient[ n + offset ] ;
                 off_grad = off_tree_gradient[ n + offset ] ;
                 node_exp = local_node_expectation[ n + offset ];
@@ -593,9 +593,9 @@
                     grand_parent = n + offset + sub_tree_size + 1;
                     grand_parent_prob = output[ grand_parent ];
                     // Gradient for rest of the tree
-                    on_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
-                          + off_grad * (1 - node_exp) ) 
+                    on_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
+                          + off_grad * (1 - node_exp) )
                         * ( 1 - parent_exp );
                     left_of_grand_parent = false;
                 }
@@ -604,20 +604,20 @@
                     grand_parent = n + offset - 3*sub_tree_size - 3;
                     grand_parent_prob = off_expectation[ grand_parent ];
                     // Gradient for rest of the tree
-                    off_tree_gradient[ grand_parent ] += 
-                        ( out_grad * node_exp 
-                          + off_grad * (1 - node_exp) ) 
+                    off_tree_gradient[ grand_parent ] +=
+                        ( out_grad * node_exp
+                          + off_grad * (1 - node_exp) )
                         * ( 1 - parent_exp );
                     left_of_grand_parent = true;
                 }
 
                 // Gradient w/r current node
-                local_node_expectation_gradient[ n + offset ] += 
+                local_node_expectation_gradient[ n + offset ] +=
                     ( out_grad - off_grad ) * ( 1 - parent_exp ) * grand_parent_prob;
                     //* node_exp * ( 1 - node_exp );
 
                 // Gradient w/r parent node
-                local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -= 
+                local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -=
                     ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob;
                     //* parent_exp * (1-parent_exp) ;
             }
@@ -630,49 +630,49 @@
 
         //// Left child
         node = sub_tree_size;
-        out_grad = output_gradient[ node + offset ] + 
+        out_grad = output_gradient[ node + offset ] +
             on_tree_gradient[ node + offset ] ;
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
         parent_exp = local_node_expectation[ node + offset + sub_tree_size + 1 ];
-        
+
         // Gradient w/r current node
-        local_node_expectation_gradient[ node + offset ] += 
+        local_node_expectation_gradient[ node + offset ] +=
             ( out_grad - off_grad ) * parent_exp;
             //* node_exp * ( 1 - node_exp );
-        
+
         // Gradient w/r parent node
-        local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] += 
+        local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] +=
             ( out_grad * node_exp  + off_grad * (1 - node_exp) );
             //* parent_exp * (1-parent_exp) ;
 
         //// Right child
         node = 3*sub_tree_size+2;
-        out_grad = output_gradient[ node + offset ] + 
+        out_grad = output_gradient[ node + offset ] +
             on_tree_gradient[ node + offset ] ;
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
         parent_exp = local_node_expectation[ node + offset - sub_tree_size - 1 ];
 
         // Gradient w/r current node
-        local_node_expectation_gradient[ node + offset ] += 
+        local_node_expectation_gradient[ node + offset ] +=
             ( out_grad - off_grad ) * ( 1 - parent_exp ) ;
             //* node_exp * ( 1 - node_exp );
-        
+
         // Gradient w/r parent node
-        local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -= 
+        local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -=
             ( out_grad * node_exp + off_grad * (1 - node_exp) ) ;
             //* parent_exp * (1-parent_exp) ;
-        
+
         ////// Root
         node = n_nodes_per_tree / 2;
         sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
 
-        out_grad = output_gradient[ node + offset ] + 
+        out_grad = output_gradient[ node + offset ] +
             on_tree_gradient[ node + offset ] ;
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
-        local_node_expectation_gradient[ node + offset ] += 
+        local_node_expectation_gradient[ node + offset ] +=
             ( out_grad - off_grad );// * node_exp * ( 1 - node_exp );
 
         offset += n_nodes_per_tree;
@@ -699,16 +699,16 @@
                 out_grad = on_free_energy_gradient[ n + offset ];
                 node_exp = local_node_expectation[n + offset - (sub_tree_size/2+1)];
                 input_gradient[n+offset] += out_grad;
-                on_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * node_exp; 
-                off_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * (1 - node_exp); 
+                on_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * node_exp;
+                off_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * (1 - node_exp);
 
                 out_grad = off_free_energy_gradient[ n + offset ];
                 node_exp = local_node_expectation[n + offset + (sub_tree_size/2+1)];
                 if( use_signed_samples )
                     input_gradient[n+offset] -= out_grad;
-                on_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += out_grad * node_exp; 
-                off_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += 
-                    out_grad * (1 - node_exp); 
+                on_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += out_grad * node_exp;
+                off_free_energy_gradient[n + offset + (sub_tree_size/2+1)] +=
+                    out_grad * (1 - node_exp);
             }
             sub_tree_size /= 2;
             depth++;
@@ -721,7 +721,7 @@
         {
             input_gradient[n+offset] += on_free_energy_gradient[ n + offset ];
             if( use_signed_samples )
-                input_gradient[n+offset] -= off_free_energy_gradient[ n + offset ];                
+                input_gradient[n+offset] -= off_free_energy_gradient[ n + offset ];
         }
 
         offset += n_nodes_per_tree;
@@ -858,7 +858,7 @@
             // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
             // but it is numerically unstable, so use instead the following identity:
             //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
-            //     = act + softplus(-act) - target*act 
+            //     = act + softplus(-act) - target*act
             //     = softplus(act) - target*act
         }
     } else {
@@ -892,7 +892,7 @@
             for( int i=0 ; i&lt;size ; i++ ) // loop over outputs
             {
                 if(!fast_exact_is_equal(target[i],0.0))
-                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // nll -= target[i] * pl_log(expectations[i]);
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
                     nll += target[i] * tabulated_softplus(-activation[i]);
@@ -908,7 +908,7 @@
             for( int i=0 ; i&lt;size ; i++ ) // loop over outputs
             {
                 if(!fast_exact_is_equal(target[i],0.0))
-                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // nll -= target[i] * pl_log(expectations[i]);
                     // but it is numerically unstable, so use instead
                     // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
                     nll += target[i] * softplus(-activation[i]);

Modified: trunk/plearn_learners/online/SoftmaxModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/SoftmaxModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -79,7 +79,7 @@
                              Mat&amp; input_gradients,
                              const Mat&amp; output_gradients,
                              bool accumulate = false);
-    
+
     //! this version allows to obtain the input gradient and diag_hessian
     virtual void bbpropUpdate(const Vec&amp; input, const Vec&amp; output,
                               Vec&amp; input_gradient,

Modified: trunk/plearn_learners/online/SplitModule.cc
===================================================================
--- trunk/plearn_learners/online/SplitModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/SplitModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -59,7 +59,7 @@
     &quot;can be seen as performed on each row of these matrices.\n&quot;
     );
 
-SplitModule::SplitModule() 
+SplitModule::SplitModule()
     : down_port_name(&quot;down_port&quot;)
 /* ### Initialize all fields to their default value here */
 {
@@ -194,7 +194,7 @@
             start += width;
         }
         return;
-    } 
+    }
     bool one_of_the_up_ports_wants_a_gradient = false;
     for (int i=0;i&lt;up_port_sizes.length();i++)
         if (ports_gradient[i+1] &amp;&amp; ports_gradient[i+1]-&gt;isEmpty())
@@ -211,7 +211,7 @@
         for (int i=0;i&lt;up_port_sizes.length();i++)
         {
             int width = up_port_sizes[i];
-            if (ports_gradient[i+1] &amp;&amp; ports_gradient[i+1]-&gt;isEmpty()) 
+            if (ports_gradient[i+1] &amp;&amp; ports_gradient[i+1]-&gt;isEmpty())
             {
                 ports_gradient[i+1]-&gt;resize(output_gradient.length(),width);
                 (*ports_gradient[i+1]) += output_gradient.subMatColumns(start,width);
@@ -242,7 +242,7 @@
 //////////////////
 const TMat&lt;int&gt;&amp; SplitModule::getPortSizes() {
     if (sizes.isEmpty())
-    { 
+    {
         sizes.resize(nPorts(),2);
         sizes.column(0).fill(-1);
         sizes(0,1) = input_size;

Modified: trunk/plearn_learners/online/SplitModule.h
===================================================================
--- trunk/plearn_learners/online/SplitModule.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/SplitModule.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -67,7 +67,7 @@
     string down_port_name;
     TVec&lt;int&gt; up_port_sizes;
     TVec&lt;string&gt; up_port_names;
-     
+
 public:
     //#####  Public Member Functions  #########################################
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -203,10 +203,10 @@
     virtual void train();
 
     //! Computes the output from the input.
-    virtual void computeOutputWithoutCorrelationConnections(const Vec&amp; input, 
+    virtual void computeOutputWithoutCorrelationConnections(const Vec&amp; input,
                                                         Vec&amp; output) const;
     //! Computes the output from the input
-    virtual void computeOutputsWithoutCorrelationConnections(const Mat&amp; input, 
+    virtual void computeOutputsWithoutCorrelationConnections(const Mat&amp; input,
                                                         Mat&amp; output) const;
 
     //! Computes the output from the input.
@@ -416,9 +416,9 @@
 
     void setLearningRate( real the_learning_rate );
 
-    // List of remote methods    
+    // List of remote methods
     Vec remote_computeOutputWithoutCorrelationConnections(const Vec&amp; input) const;
-    
+
     Mat remote_computeOutputsWithoutCorrelationConnections(const Mat&amp; inputs) const;
 
     //! Global storage to save memory allocations.

Modified: trunk/plearn_learners/online/TanhModule.cc
===================================================================
--- trunk/plearn_learners/online/TanhModule.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/TanhModule.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -98,7 +98,7 @@
                              bool accumulate)
 {
     int mbs=inputs.length();
-    PLASSERT(mbs==outputs.length() &amp;&amp; 
+    PLASSERT(mbs==outputs.length() &amp;&amp;
              mbs==output_gradients.length());
     input_gradients.resize(mbs,input_size);
     for (int i=0;i&lt;mbs;i++)

Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2008-06-11 02:37:22 UTC (rev 9115)
@@ -183,7 +183,7 @@
     substract(delta_h, *sampled_h_, delta_h);
     columnSum(delta_h,delta_hb);
     multiplyAcc(rbm1-&gt;hidden_layer-&gt;bias,delta_hb,rbm1-&gt;cd_learning_rate);
-    
+
     //  dlogbound/dji sampling approx = v[j] - reconstructed_v[j]
     columnSum(reconstructed_v,delta_vb1);
     columnSum(*input,delta_vb2);
@@ -247,7 +247,7 @@
     //
 
     // for learning or testing
-    if (input &amp;&amp; !input-&gt;isEmpty()) 
+    if (input &amp;&amp; !input-&gt;isEmpty())
     {
         int mbs=input-&gt;length();
         FE1v.resize(mbs,1);
@@ -261,8 +261,8 @@
         global_improvement-&gt;resize(mbs,1);
         ph_given_v-&gt;resize(mbs,rbm1-&gt;hidden_layer-&gt;size);
 
-        // compute things needed for everything else 
-    
+        // compute things needed for everything else
+
         rbm1-&gt;sampleHiddenGivenVisible(*input);
         *ph_given_v &lt;&lt; rbm1-&gt;hidden_layer-&gt;getExpectations();
         *sampled_h &lt;&lt; rbm1-&gt;hidden_layer-&gt;samples;

Modified: trunk/plearn_learners/online/VBoundDBN2.h
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.h	2008-06-11 02:17:58 UTC (rev 9114)
+++ trunk/plearn_learners/online/VBoundDBN2.h	2008-06-11 02:37:22 UTC (rev 9115)
@@ -47,7 +47,7 @@
 
 /**
  * 2-RBM DBN trained using Hinton's new variational bound of global likelihood:
- * 
+ *
  * log P(x) &gt;= -FE1(x) + E_{P1(h|x)}[ FE1(h) - FE2(h) ] - log Z2
  *
  * where P1 and P2 are RBMs with Pi(x) = exp(-FEi(x))/Zi.


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002562.html">[Plearn-commits] r9114 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002564.html">[Plearn-commits] r9116 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2563">[ date ]</a>
              <a href="thread.html#2563">[ thread ]</a>
              <a href="subject.html#2563">[ subject ]</a>
              <a href="author.html#2563">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
