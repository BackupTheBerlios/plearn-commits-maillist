<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9121 - in trunk: commands plearn/ker	plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9121%20-%20in%20trunk%3A%20commands%20plearn/ker%0A%09plearn_learners/online&In-Reply-To=%3C200806112155.m5BLtNUr018071%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002568.html">
   <LINK REL="Next"  HREF="002570.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9121 - in trunk: commands plearn/ker	plearn_learners/online</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9121%20-%20in%20trunk%3A%20commands%20plearn/ker%0A%09plearn_learners/online&In-Reply-To=%3C200806112155.m5BLtNUr018071%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9121 - in trunk: commands plearn/ker	plearn_learners/online">louradou at mail.berlios.de
       </A><BR>
    <I>Wed Jun 11 23:55:23 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002568.html">[Plearn-commits] r9120 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002570.html">[Plearn-commits] r9122 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2569">[ date ]</a>
              <a href="thread.html#2569">[ thread ]</a>
              <a href="subject.html#2569">[ subject ]</a>
              <a href="author.html#2569">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2008-06-11 23:55:21 +0200 (Wed, 11 Jun 2008)
New Revision: 9121

Added:
   trunk/plearn/ker/CosKernel.cc
   trunk/plearn/ker/CosKernel.h
   trunk/plearn_learners/online/ShuntingNNetLayerModule.cc
   trunk/plearn_learners/online/ShuntingNNetLayerModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
- added Cosinus kernel (distance kernel, good with kNN)
- addded Shunting Layer in online



Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/commands/plearn_noblas_inc.h	2008-06-11 21:55:21 UTC (rev 9121)
@@ -107,6 +107,7 @@
  **********/
 #include &lt;plearn/ker/AdditiveNormalizationKernel.h&gt;
 #include &lt;plearn/ker/BetaKernel.h&gt;
+#include &lt;plearn/ker/CosKernel.h&gt;
 #include &lt;plearn/ker/DistanceKernel.h&gt;
 #include &lt;plearn/ker/DotProductKernel.h&gt;
 #include &lt;plearn/ker/EpanechnikovKernel.h&gt;
@@ -242,9 +243,11 @@
 #include &lt;plearn_learners/online/RBMMixedLayer.h&gt;
 #include &lt;plearn_learners/online/RBMModule.h&gt;
 #include &lt;plearn_learners/online/RBMMultinomialLayer.h&gt;
+#include &lt;plearn_learners/online/RBMSparse1DMatrixConnection.h&gt;
 #include &lt;plearn_learners/online/RBMTrainer.h&gt;
 #include &lt;plearn_learners/online/RBMTruncExpLayer.h&gt;
 #include &lt;plearn_learners/online/ScaleGradientModule.h&gt;
+#include &lt;plearn_learners/online/ShuntingNNetLayerModule.h&gt;
 #include &lt;plearn_learners/online/SoftmaxModule.h&gt;
 #include &lt;plearn_learners/online/SplitModule.h&gt;
 #include &lt;plearn_learners/online/SquaredErrorCostModule.h&gt;

Added: trunk/plearn/ker/CosKernel.cc
===================================================================
--- trunk/plearn/ker/CosKernel.cc	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn/ker/CosKernel.cc	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,120 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 2007 Jerome Louradour
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: CosKernel.cc 7675 2007-06-29 19:50:49Z tihocan $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#include &quot;CosKernel.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    CosKernel,
+    &quot;Implements a distance based on a cosinus.&quot;,
+    &quot;Output k(x,y)=(0.5 - 0.5*cos(x,y)) (between 0 and 1)\n&quot;);
+
+////////////////////
+// CosKernel //
+////////////////////
+CosKernel::CosKernel()
+{
+    n=2.0;
+    optimized=false;
+    pow_distance=false;
+    ignore_missing=false;
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void CosKernel::declareOptions(OptionList&amp; ol)
+{
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, &quot;n&quot;, &amp;CosKernel::n, OptionBase::nosave, 
+                  &quot;Obsolete option for cosinus kernel.&quot;);
+
+    redeclareOption(ol, &quot;pow_distance&quot;, &amp;CosKernel::pow_distance, OptionBase::nosave, 
+                  &quot;Obsolete option for cosinus kernel.&quot;);
+
+    redeclareOption(ol, &quot;optimized&quot;, &amp;CosKernel::optimized, OptionBase::nosave, 
+                  &quot;Obsolete option for cosinus kernel.&quot;);
+
+    redeclareOption(ol, &quot;ignore_missing&quot;, &amp;CosKernel::ignore_missing, OptionBase::nosave, 
+                  &quot;Obsolete option for cosinus kernel.&quot;);
+}
+
+//////////////
+// evaluate //
+//////////////
+real CosKernel::evaluate(const Vec&amp; x1, const Vec&amp; x2) const {
+    if (ignore_missing &amp;&amp; !pow_distance)
+        PLERROR(&quot;In CosKernel::evaluate(int i, int j) - 'ignore_missing' &quot;
+                &quot;implemented only if pow_distance is set&quot;);
+    static real cosinus;
+    cosinus = dot(x1, x2) / ( norm(x1) * norm(x2) );
+    return (1.-cosinus)*.5;
+}
+
+//////////////////
+// evaluate_i_j //
+//////////////////
+real CosKernel::evaluate_i_j(int i, int j) const {
+    static real cosinus;
+    if (i == j)
+        // The case 'i == j' can cause precision issues because of the optimized
+        // formula below. Thus we make sure we always return 0.
+        return 0;
+    cosinus = data-&gt;dot(i, j, data_inputsize) / sqrt(squarednorms[i] * squarednorms[j]);
+    return (1.-cosinus)*.5;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/CosKernel.h
===================================================================
--- trunk/plearn/ker/CosKernel.h	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn/ker/CosKernel.h	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,89 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 2007 Jerome Louradour
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: CosKernel.h 7664 2007-06-28 19:47:30Z nouiz $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#ifndef CosKernel_INC
+#define CosKernel_INC
+
+#include &quot;DistanceKernel.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+//! This class implements an Ln distance (defaults to L2 i.e. euclidean distance).
+class CosKernel: public DistanceKernel
+{
+
+private:
+
+    typedef DistanceKernel inherited;
+
+protected:
+
+public:
+
+    CosKernel();
+    
+    PLEARN_DECLARE_OBJECT(CosKernel);
+
+    virtual real evaluate(const Vec&amp; x1, const Vec&amp; x2) const;
+    virtual real evaluate_i_j(int i, int j) const;
+
+
+protected:
+    static void declareOptions(OptionList&amp; ol);
+};
+
+DECLARE_OBJECT_PTR(CosKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ShuntingNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/ShuntingNNetLayerModule.cc	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn_learners/online/ShuntingNNetLayerModule.cc	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,542 @@
+// -*- C++ -*-
+
+// ShuntingNNetLayerModule.cc
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************
+   * $Id: ShuntingNNetLayerModule.cc,v 1.3 2006/01/18 04:04:06 lamblinp Exp $
+   ******************************************************* */
+
+// Authors: Jerome Louradour
+
+/*! \file ShuntingNNetLayerModule.cc */
+
+
+#include &quot;ShuntingNNetLayerModule.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ShuntingNNetLayerModule,
+    &quot;Affine transformation module, with stochastic gradient descent updates&quot;,
+    &quot;Neural Network layer, using stochastic gradient to update neuron weights\n&quot;
+    &quot;       Output = weights * Input + bias\n&quot;
+    &quot;Weights and bias are updated by online gradient descent, with learning\n&quot;
+    &quot;rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).\n&quot;
+    &quot;An L1 and L2 regularization penalty can be added to push weights to 0.\n&quot;
+    &quot;Weights can be initialized to 0, to a given initial matrix, or randomly\n&quot;
+    &quot;from a uniform distribution.\n&quot;
+    );
+
+/////////////////////////
+// ShuntingNNetLayerModule //
+/////////////////////////
+ShuntingNNetLayerModule::ShuntingNNetLayerModule():
+    start_learning_rate( .001 ),
+    decrease_constant( 0. ),
+    init_weights_random_scale( 1. ),
+    init_quad_weights_random_scale( 1. ),
+    n_filters( 1 ),
+    n_filters_inhib( -1 ),
+    step_number( 0 )
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+
+void ShuntingNNetLayerModule::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;start_learning_rate&quot;,
+                  &amp;ShuntingNNetLayerModule::start_learning_rate,
+                  OptionBase::buildoption,
+                  &quot;Learning-rate of stochastic gradient optimization&quot;);
+
+    declareOption(ol, &quot;decrease_constant&quot;,
+                  &amp;ShuntingNNetLayerModule::decrease_constant,
+                  OptionBase::buildoption,
+                  &quot;Decrease constant of stochastic gradient optimization&quot;);
+
+    declareOption(ol, &quot;init_weights_random_scale&quot;,
+                  &amp;ShuntingNNetLayerModule::init_weights_random_scale,
+                  OptionBase::buildoption,
+                  &quot;Weights of the excitation (softplus part) are initialized randomly\n&quot;
+                  &quot;from a uniform in [-r,r], with r = init_weights_random_scale/input_size.\n&quot;
+                  &quot;To clear the weights initially, just set this option to 0.&quot;);
+                  
+    declareOption(ol, &quot;init_quad_weights_random_scale&quot;,
+                  &amp;ShuntingNNetLayerModule::init_quad_weights_random_scale,
+                  OptionBase::buildoption,
+                  &quot;Weights of the quadratic part (of excitation, as well as inhibition) are initialized randomly\n&quot;
+                  &quot;from a uniform in [-r,r], with r = init_weights_random_scale/input_size.\n&quot;
+                  &quot;To clear the weights initially, just set this option to 0.&quot;);
+                  
+    declareOption(ol, &quot;n_filters&quot;,
+                  &amp;ShuntingNNetLayerModule::n_filters,
+                  OptionBase::buildoption,
+                  &quot;Number of synapses per neuron for excitation.\n&quot;);
+
+    declareOption(ol, &quot;n_filters_inhib&quot;,
+                  &amp;ShuntingNNetLayerModule::n_filters_inhib,
+                  OptionBase::buildoption,
+                  &quot;Number of synapses per neuron for inhibition.\n&quot;
+                  &quot;Must be lower or equal to n_filters in the current implementation (!).\n&quot;
+                  &quot;If -1, then it is taken equal to n_filters.&quot;);
+
+    declareOption(ol, &quot;excit_quad_weights&quot;, &amp;ShuntingNNetLayerModule::excit_quad_weights,
+                  OptionBase::learntoption,
+                  &quot;List of weights vectors of the neurons&quot;
+                  &quot;contributing to the excitation -- quadratic part)&quot;);
+
+    declareOption(ol, &quot;inhib_quad_weights&quot;, &amp;ShuntingNNetLayerModule::inhib_quad_weights,
+                  OptionBase::learntoption,
+                  &quot;List of weights vectors of the neurons (inhibation -- quadratic part)\n&quot;);
+
+    declareOption(ol, &quot;excit_weights&quot;, &amp;ShuntingNNetLayerModule::excit_weights,
+                  OptionBase::learntoption,
+                  &quot;Input weights vectors of the neurons (excitation -- softplus part)\n&quot;);
+
+    declareOption(ol, &quot;bias&quot;, &amp;ShuntingNNetLayerModule::bias,
+                  OptionBase::learntoption,
+                  &quot;Bias of the neurons (in the softplus of the excitations)\n&quot;);
+
+    declareOption(ol, &quot;excit_num_coeff&quot;, &amp;ShuntingNNetLayerModule::excit_num_coeff,
+                  OptionBase::learntoption,
+                  &quot;Multiplicative Coefficient applied on the excitation\n&quot;
+                  &quot;in the numerator of the activation closed form.\n&quot;);
+
+    declareOption(ol, &quot;inhib_num_coeff&quot;, &amp;ShuntingNNetLayerModule::inhib_num_coeff,
+                  OptionBase::learntoption,
+                  &quot;Multiplicative Coefficient applied on the inhibition\n&quot;
+                  &quot;in the numerator of the activation closed form.\n&quot;);
+
+    inherited::declareOptions(ol);
+}
+///////////
+// build //
+///////////
+
+void ShuntingNNetLayerModule::build_()
+{
+    if( input_size &lt; 0 ) // has not been initialized
+        return;
+
+    if( output_size &lt; 0 )
+        PLERROR(&quot;ShuntingNNetLayerModule::build_: 'output_size' is &lt; 0 (%i),\n&quot;
+                &quot; you should set it to a positive integer (the number of&quot;
+                &quot; neurons).\n&quot;, output_size);
+
+    if (n_filters_inhib &lt; 0)
+        n_filters_inhib= n_filters;
+    PLASSERT( n_filters&gt;0 );
+    
+    if(    excit_quad_weights.length() != n_filters
+        || inhib_quad_weights.length() != n_filters_inhib
+        || excit_weights.length() != output_size
+        || excit_weights.width() != input_size
+        || bias.size() != output_size )
+    {
+        forget();
+    }
+}
+void ShuntingNNetLayerModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// forget //
+////////////
+
+void ShuntingNNetLayerModule::forget()
+{
+    learning_rate = start_learning_rate;
+    step_number = 0;
+
+    bias.resize( output_size );
+    bias.clear();
+    
+    excit_num_coeff.resize( output_size );
+    inhib_num_coeff.resize( output_size );
+    excit_num_coeff.fill(1.);
+    inhib_num_coeff.fill(1.);
+
+    excit_weights.resize( output_size, input_size );
+    excit_quad_weights.resize( n_filters );
+    PLASSERT( n_filters_inhib &gt;= 0 &amp;&amp; n_filters_inhib &lt;= n_filters );
+    inhib_quad_weights.resize( n_filters_inhib );
+    
+    if( !random_gen )
+    {
+        PLWARNING( &quot;ShuntingNNetLayerModule: cannot forget() without random_gen&quot; );
+        return;
+    }
+    
+    real r = init_weights_random_scale / (real)input_size;
+    if( r &gt; 0. )
+        random_gen-&gt;fill_random_uniform(excit_weights, -r, r);
+    else
+        excit_weights.clear();
+      
+    r = init_quad_weights_random_scale / (real)input_size;    
+    if( r &gt; 0. )
+        for( int k = 0; k &lt; n_filters; k++ )
+        {
+            excit_quad_weights[k].resize( output_size, input_size );
+            random_gen-&gt;fill_random_uniform(excit_quad_weights[k], -r, r);
+            if ( k &lt; n_filters_inhib ) {
+                inhib_quad_weights[k].resize( output_size, input_size );
+                random_gen-&gt;fill_random_uniform(inhib_quad_weights[k], -r, r);
+            }
+        }
+    else
+        for( int k = 0; k &lt; n_filters; k++ )
+        {
+            excit_quad_weights[k].resize(output_size, input_size );
+            excit_quad_weights[k].clear();
+            if ( k &lt; n_filters_inhib ) {
+                inhib_quad_weights[k].resize(output_size, input_size );
+                inhib_quad_weights[k].clear();
+            }
+        }
+}
+
+///////////
+// fprop //
+///////////
+
+void ShuntingNNetLayerModule::fprop(const Vec&amp; input, Vec&amp; output) const
+{
+    PLASSERT_MSG( input.size() == input_size,
+                  &quot;input.size() should be equal to this-&gt;input_size&quot; );
+
+    output.resize( output_size );
+
+    if( during_training )
+    {
+        batch_excitations.resize(1, output_size);
+        batch_inhibitions.resize(1, output_size);
+    }
+//    if( use_fast_approximations )
+
+        for( int i = 0; i &lt; output_size; i++ )
+        {
+            real excitation = 0.;
+            real inhibition = 0.;
+            for ( int k=0; k &lt; n_filters; k++ )
+            {
+                excitation += square( dot( excit_quad_weights[k](i), input ) );
+                if ( k &lt; n_filters_inhib )
+                    inhibition += square( dot( inhib_quad_weights[k](i), input ) );
+            }
+            excitation = sqrt( excitation + tabulated_softplus( dot( excit_weights(i), input ) + bias[i] ) );
+            inhibition = sqrt( inhibition );
+            if( during_training )
+            {
+                    batch_excitations(0,i) = excitation;
+                    batch_inhibitions(0,i) = inhibition;
+            }
+
+            output[i] = ( excit_num_coeff[i]* excitation - inhib_num_coeff[i]* inhibition ) /
+                        (1. + excitation + inhibition );
+        }
+//    else
+}
+
+void ShuntingNNetLayerModule::fprop(const Mat&amp; inputs, Mat&amp; outputs)
+{
+    PLASSERT( inputs.width() == input_size );
+    int n = inputs.length();
+    outputs.resize(n, output_size);
+    
+
+    Mat excitations_part2(n, output_size);
+    excitations_part2.clear();
+    productTranspose(excitations_part2, inputs, excit_weights);
+    resizeOnes(n);
+    externalProductAcc(excitations_part2, ones, bias);
+
+    Mat excitations(n, output_size), inhibitions(n, output_size);
+    excitations.clear();
+    inhibitions.clear();
+
+        for ( int k=0; k &lt; n_filters; k++ )
+        {
+            Mat tmp_sample_output(n, output_size);
+
+            tmp_sample_output.clear();
+            productTranspose(tmp_sample_output, inputs, excit_quad_weights[k]);
+            squareElements(tmp_sample_output);
+            multiplyAcc(excitations, tmp_sample_output, 1.);
+
+            if ( k &lt; n_filters_inhib ) {
+                tmp_sample_output.clear();
+                productTranspose(tmp_sample_output, inputs, inhib_quad_weights[k]);
+                squareElements(tmp_sample_output);
+                multiplyAcc(inhibitions, tmp_sample_output, 1.);
+            }
+        }
+        for( int i_sample = 0; i_sample &lt; n; i_sample ++)
+        {
+            for( int i = 0; i &lt; output_size; i++ )
+            {
+                excitations(i_sample,i) = sqrt( excitations(i_sample,i) + tabulated_softplus( excitations_part2(i_sample,i) ) );
+                inhibitions(i_sample,i) = sqrt( inhibitions(i_sample,i) );
+
+                real E = excitations(i_sample,i);
+                real S = inhibitions(i_sample,i);
+                    
+                outputs(i_sample,i) = ( excit_num_coeff[i]* E - inhib_num_coeff[i]* S ) /
+                                       (1. + E + S );
+            }
+        }
+
+    if( during_training )
+    {
+        batch_excitations.resize(n, output_size);
+        batch_inhibitions.resize(n, output_size);
+        batch_excitations &lt;&lt; excitations;
+        batch_inhibitions &lt;&lt; inhibitions;
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+
+void ShuntingNNetLayerModule::bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                                      const Vec&amp; output_gradient)
+{
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+
+    for( int i=0; i&lt;output_size; i++ )
+    {
+        real tmp = square(1 + batch_excitations(0,i) + batch_inhibitions(0,i) );
+        
+        real Dactivation_Dexcit =   ( excit_num_coeff[i]  +  batch_inhibitions(0,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp;
+        real Dactivation_Dinhib = - ( inhib_num_coeff[i]  +  batch_excitations(0,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp;
+
+        real lr_og_excit = learning_rate * output_gradient[i];
+        PLASSERT( batch_excitations(0,i)&gt;0. );
+        PLASSERT( batch_inhibitions(0,i)&gt;0. );
+        real lr_og_inhib = lr_og_excit * Dactivation_Dinhib / batch_inhibitions(0,i);
+        lr_og_excit *= Dactivation_Dexcit / batch_excitations(0,i);
+        
+        tmp = lr_og_excit * sigmoid( dot( excit_weights(i), input ) + bias[i] ) * .5;
+
+        bias[i] -= tmp;
+        multiplyAcc( excit_weights(i), input, -tmp);
+
+        for( int k = 0; k &lt; n_filters; k++ )
+        {
+            real tmp_excit2 = lr_og_excit * dot( excit_quad_weights[k](i), input );
+            real tmp_inhib2 = 0;
+            if (k &lt; n_filters_inhib)
+                tmp_inhib2 = lr_og_inhib * dot( inhib_quad_weights[k](i), input );
+            for( int j=0; j&lt;input_size; j++ )
+            {
+                excit_quad_weights[k](i,j) -= tmp_excit2 * input[j];
+                if (k &lt; n_filters_inhib)
+                    inhib_quad_weights[k](i,j) -= tmp_inhib2 * input[j];
+            }   
+        }
+    }
+
+    step_number++;
+}
+
+void ShuntingNNetLayerModule::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+        Mat&amp; input_gradients,
+        const Mat&amp; output_gradients,
+        bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( outputs.width() == output_size );
+    PLASSERT( output_gradients.width() == output_size );
+
+    //fprop(inputs);
+
+    int n = inputs.length();
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &amp;&amp;
+                input_gradients.length() == n,
+                &quot;Cannot resize input_gradients and accumulate into it&quot; );
+    }
+    else
+    {
+        input_gradients.resize(n, input_size);
+        input_gradients.fill(0);
+    }
+
+    learning_rate = start_learning_rate / (1+decrease_constant*step_number);
+    real avg_lr = learning_rate / n; // To obtain an average on a mini-batch.
+
+    if ( avg_lr == 0. )
+        return ; 
+
+        Mat tmp(n, output_size);
+        // tmp = (1 + E + S ).^2;
+        tmp.fill(1.);
+        multiplyAcc(tmp, batch_excitations, 1);
+        multiplyAcc(tmp, batch_inhibitions, 1);
+        squareElements(tmp);
+        
+        Vec bias_updates(output_size);
+        Mat excit_weights_updates( output_size, input_size);
+        TVec&lt;Mat&gt; excit_quad_weights_updates(n_filters);
+        TVec&lt;Mat&gt; inhib_quad_weights_updates(n_filters_inhib);
+        // Initialisation 
+        bias_updates.clear();
+        excit_weights_updates.clear();
+        for( int k=0; k &lt; n_filters; k++ )
+        {
+            excit_quad_weights_updates[k].resize( output_size, input_size);
+            excit_quad_weights_updates[k].clear();
+            if (k &lt; n_filters_inhib) {
+                inhib_quad_weights_updates[k].resize( output_size, input_size);
+                inhib_quad_weights_updates[k].clear();
+            }
+        }
+
+        for( int i_sample = 0; i_sample &lt; n; i_sample++ )
+        for( int i=0; i&lt;output_size; i++ )
+        {
+            real Dactivation_Dexcit =   ( excit_num_coeff[i]  +  batch_inhibitions(i_sample,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp(i_sample,i);
+            real Dactivation_Dinhib = - ( inhib_num_coeff[i]  +  batch_excitations(i_sample,i)*(excit_num_coeff[i] + inhib_num_coeff[i]) ) / tmp(i_sample,i);
+            
+            real lr_og_excit = avg_lr * output_gradients(i_sample,i);
+            PLASSERT( batch_excitations(i_sample,i)&gt;0. );
+            PLASSERT( n_filters_inhib==0 || batch_inhibitions(i_sample,i)&gt;0. );
+            real lr_og_inhib = lr_og_excit * Dactivation_Dinhib / batch_inhibitions(i_sample,i);
+            lr_og_excit *= Dactivation_Dexcit / batch_excitations(i_sample,i);
+                
+            real tmp2 = lr_og_excit * sigmoid( dot( excit_weights(i), inputs(i_sample) ) + bias[i] ) * .5;
+
+            bias_updates[i] -= tmp2;
+            multiplyAcc( excit_weights_updates(i), inputs(i_sample), -tmp2);
+
+            for( int k = 0; k &lt; n_filters; k++ )
+            {
+                real tmp_excit2 = lr_og_excit   * dot( excit_quad_weights[k](i), inputs(i_sample) );
+                real tmp_inhib2 = 0;
+                if (k &lt; n_filters_inhib)
+                    tmp_inhib2 = lr_og_inhib   * dot( inhib_quad_weights[k](i), inputs(i_sample) );
+                //for( int j=0; j&lt;input_size; j++ )
+                //{
+                //    excit_quad_weights_updates[k](i,j) -= tmp_excit2 * inputs(i_sample,j);
+                //    if (k &lt; n_filters_inhib)
+                //        inhib_quad_weights_updates[k](i,j) -= tmp_inhib2 * inputs(i_sample,j);
+                //}
+                multiplyAcc( excit_quad_weights_updates[k](i), inputs(i_sample), -tmp_excit2);
+                if (k &lt; n_filters_inhib)
+                    multiplyAcc( inhib_quad_weights_updates[k](i), inputs(i_sample), -tmp_inhib2);
+            }
+        }
+
+        multiplyAcc( bias, bias_updates, 1.);
+        multiplyAcc( excit_weights, excit_weights_updates, 1.);
+        for( int k = 0; k &lt; n_filters; k++ )
+        {
+            multiplyAcc( excit_quad_weights[k], excit_quad_weights_updates[k], 1.);
+            if (k &lt; n_filters_inhib)
+                multiplyAcc( inhib_quad_weights[k], inhib_quad_weights_updates[k], 1.);
+        }
+        batch_excitations.clear();
+        batch_inhibitions.clear();
+
+    step_number += n;
+}
+
+
+
+
+void ShuntingNNetLayerModule::setLearningRate( real dynamic_learning_rate )
+{
+    start_learning_rate = dynamic_learning_rate;
+    step_number = 0;
+    // learning_rate will automatically be set in bpropUpdate()
+}
+
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+
+void ShuntingNNetLayerModule::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(excit_weights,            copies);
+    deepCopyField(excit_quad_weights,       copies);
+    deepCopyField(inhib_quad_weights,       copies);
+    deepCopyField(bias,                     copies);
+    deepCopyField(excit_num_coeff,          copies);
+    deepCopyField(inhib_num_coeff,          copies);
+    deepCopyField(ones,                     copies);
+}
+
+
+
+
+////////////////
+// resizeOnes //
+////////////////
+void ShuntingNNetLayerModule::resizeOnes(int n) const
+{
+    if (ones.length() &lt; n) {
+        ones.resize(n);
+        ones.fill(1);
+    } else if (ones.length() &gt; n)
+        ones.resize(n);
+}
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ShuntingNNetLayerModule.h
===================================================================
--- trunk/plearn_learners/online/ShuntingNNetLayerModule.h	2008-06-11 20:56:31 UTC (rev 9120)
+++ trunk/plearn_learners/online/ShuntingNNetLayerModule.h	2008-06-11 21:55:21 UTC (rev 9121)
@@ -0,0 +1,189 @@
+// -*- C++ -*-
+
+// ShuntingNNetLayerModule.h
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************
+   * $Id: ShuntingNNetLayerModule.h,v 1.3 2006/01/08 00:14:53 lamblinp Exp $
+   ******************************************************* */
+
+// Authors: Jerome Louradour
+
+/*! \file ShuntingNNetLayerModule.h */
+
+
+#ifndef ShuntingNNetLayerModule_INC
+#define ShuntingNNetLayerModule_INC
+
+#include &lt;plearn/base/Object.h&gt;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &quot;OnlineLearningModule.h&quot;
+
+namespace PLearn {
+
+/**
+ * Affine transformation module, with stochastic gradient descent updates.
+ *
+ * Neural Network layer, using stochastic gradient to update neuron weights,
+ *      Output = weights * Input + bias
+ * Weights and bias are updated by online gradient descent, with learning
+ * rate possibly decreasing in 1/(1 + n_updates_done * decrease_constant).
+ * An L1 and L2 regularization penalty can be added to push weights to 0.
+ * Weights can be initialized to 0, to a given initial matrix, or randomly
+ * from a uniform distribution.
+ *
+ */
+class ShuntingNNetLayerModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Starting learning-rate, by which we multiply the gradient step
+    real start_learning_rate;
+
+    //! learning_rate = start_learning_rate / (1 + decrease_constant*t),
+    //! where t is the number of updates since the beginning
+    real decrease_constant;
+
+    //! If init_weights is not provided, the weights are initialized randomly
+    //! from a uniform in [-r,r], with r = init_weights_random_scale/input_size
+    real init_weights_random_scale;
+    real init_quad_weights_random_scale;
+
+    //! Number of excitation/inhibition quadratic weights
+    int n_filters;
+    int n_filters_inhib;
+
+    //! The weights, one neuron per line
+    TVec&lt;Mat&gt; excit_quad_weights;
+    TVec&lt;Mat&gt; inhib_quad_weights;
+    Mat excit_weights;
+    
+    //! The bias
+    Vec bias;
+
+    //! The multiplicative coefficients of excitation and inhibition
+    //! (in the numerator of the output activation)
+    Vec excit_num_coeff;
+    Vec inhib_num_coeff;
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ShuntingNNetLayerModule();
+
+    // Your other public member functions go here
+
+    virtual void fprop(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Overridden.
+    virtual void fprop(const Mat&amp; inputs, Mat&amp; outputs);
+
+    virtual void bpropUpdate(const Vec&amp; input, const Vec&amp; output,
+                             const Vec&amp; output_gradient);
+
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate = false);
+    
+    virtual void forget();
+
+    virtual void setLearningRate(real dynamic_learning_rate);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(ShuntingNNetLayerModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+
+    //! A vector filled with all ones.
+    mutable Vec ones;
+    
+    mutable Mat batch_excitations;
+    mutable Mat batch_inhibitions;
+    
+    //#####  Protected Options  ###############################################
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    //! Resize vector 'ones'.
+    void resizeOnes(int n) const;
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    real learning_rate;
+    int step_number;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ShuntingNNetLayerModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002568.html">[Plearn-commits] r9120 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002570.html">[Plearn-commits] r9122 - trunk/plearn_learners/online
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2569">[ date ]</a>
              <a href="thread.html#2569">[ thread ]</a>
              <a href="subject.html#2569">[ subject ]</a>
              <a href="author.html#2569">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
