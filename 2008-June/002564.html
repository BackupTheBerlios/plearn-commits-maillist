<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r9116 - trunk/plearn_learners/online
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-June/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9116%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806111500.m5BF0A89018030%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="002563.html">
   <LINK REL="Next"  HREF="002565.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r9116 - trunk/plearn_learners/online</H1>
    <B>louradou at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r9116%20-%20trunk/plearn_learners/online&In-Reply-To=%3C200806111500.m5BF0A89018030%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r9116 - trunk/plearn_learners/online">louradou at mail.berlios.de
       </A><BR>
    <I>Wed Jun 11 17:00:10 CEST 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="002563.html">[Plearn-commits] r9115 - trunk/plearn_learners/online
</A></li>
        <LI>Next message: <A HREF="002565.html">[Plearn-commits] r9117 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2564">[ date ]</a>
              <a href="thread.html#2564">[ thread ]</a>
              <a href="subject.html#2564">[ subject ]</a>
              <a href="author.html#2564">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: louradou
Date: 2008-06-11 17:00:09 +0200 (Wed, 11 Jun 2008)
New Revision: 9116

Added:
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
   trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
Log:
Added: sparse RBM connections



Added: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc	2008-06-11 02:37:22 UTC (rev 9115)
+++ trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc	2008-06-11 15:00:09 UTC (rev 9116)
@@ -0,0 +1,440 @@
+// -*- C++ -*-
+
+// RBMSparse1DMatrixConnection.cc
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file RBMSparse1DMatrixConnection.cc */
+
+
+
+#include &quot;RBMSparse1DMatrixConnection.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMSparse1DMatrixConnection,
+    &quot;RBM connections with sparses weights, designed for 1D inputs.&quot;,
+    &quot;&quot;);
+
+RBMSparse1DMatrixConnection::RBMSparse1DMatrixConnection( real the_learning_rate ) :
+    filter_size(-1),
+    enforce_positive_weights(false)
+{
+}
+
+void RBMSparse1DMatrixConnection::declareOptions(OptionList&amp; ol)
+{
+    declareOption(ol, &quot;filter_size&quot;, &amp;RBMSparse1DMatrixConnection::filter_size,
+                  OptionBase::buildoption,
+                  &quot;Length of each filter. If -1 then input_size is taken (RBMMatrixConnection).&quot;);
+
+    declareOption(ol, &quot;enforce_positive_weights&quot;, &amp;RBMSparse1DMatrixConnection::enforce_positive_weights,
+                  OptionBase::buildoption,
+                  &quot;Whether or not to enforce having positive weights.&quot;);
+
+    declareOption(ol, &quot;step_size&quot;, &amp;RBMSparse1DMatrixConnection::step_size,
+                  OptionBase::learntoption,
+                  &quot;Step between each filter.&quot;);
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////////////
+// declareMethods //
+////////////////////
+void RBMSparse1DMatrixConnection::declareMethods(RemoteMethodMap&amp; rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+    declareMethod(
+        rmm, &quot;getWeights&quot;, &amp;RBMSparse1DMatrixConnection::getWeights,
+        (BodyDoc(&quot;Returns the full weights (including 0s).\n&quot;),
+         RetDoc (&quot;Matrix of weights (n_hidden x input_size)&quot;)));
+}
+
+void RBMSparse1DMatrixConnection::build_()
+{
+    if( up_size &lt;= 0 || down_size &lt;= 0 )
+        return;
+
+    if( filter_size &lt; 0 )
+        filter_size = down_size;
+        
+    step_size = (int)((real)(down_size-filter_size)/(real)(up_size-1));
+        
+    PLASSERT( filter_size &lt;= down_size );
+
+    bool needs_forget = false; // do we need to reinitialize the parameters?
+
+    if( weights.length() != up_size ||
+        weights.width() != filter_size )
+    {
+        weights.resize( up_size, filter_size );
+        needs_forget = true;
+    }
+
+    weights_pos_stats.resize( up_size, filter_size );
+    weights_neg_stats.resize( up_size, filter_size );
+
+    if( momentum != 0. )
+        weights_inc.resize( up_size, filter_size );
+
+    if( needs_forget ) {
+        forget();
+    }
+    
+    clearStats();
+}
+
+void RBMSparse1DMatrixConnection::build()
+{
+    RBMConnection::build();
+    build_();
+}
+
+int RBMSparse1DMatrixConnection::filterStart(int idx) const
+{
+    return step_size*idx;
+}
+
+int RBMSparse1DMatrixConnection::filterSize(int idx) const
+{
+    return filter_size;
+}
+
+Mat RBMSparse1DMatrixConnection::getWeights() const
+{
+    Mat w( up_size, down_size);
+    w.clear();
+    for ( int i=0; i&lt;up_size; i++)
+        w(i).subVec( filterStart(i), filterSize(i) ) &lt;&lt; weights(i);
+    return w;
+}
+
+////////////////////////
+// accumulateStats //
+////////////////////////
+void RBMSparse1DMatrixConnection::accumulatePosStats( const Mat&amp; down_values,
+                                              const Mat&amp; up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+    // weights_pos_stats += up_values * down_values'
+    for ( int i=0; i&lt;up_size; i++)
+        transposeProductAcc( weights_pos_stats(i),
+                             down_values.subMatColumns( filterStart(i), filterSize(i) ),
+                             up_values(i));
+    pos_count+=mbs;
+}
+
+void RBMSparse1DMatrixConnection::accumulateNegStats( const Mat&amp; down_values,
+                                              const Mat&amp; up_values )
+{
+    int mbs=down_values.length();
+    PLASSERT(up_values.length()==mbs);
+    // weights_neg_stats += up_values * down_values'
+    for ( int i=0; i&lt;up_size; i++)
+        transposeProductAcc( weights_neg_stats(i),
+                             down_values.subMatColumns( filterStart(i), filterSize(i) ),
+                             up_values(i));
+    neg_count+=mbs;
+}
+
+////////////////////
+// computeProduct //
+////////////////////
+void RBMSparse1DMatrixConnection::computeProducts(int start, int length,
+                                          Mat&amp; activations,
+                                          bool accumulate ) const
+{
+    PLASSERT( activations.width() == length );
+    activations.resize(inputs_mat.length(), length);
+    if( going_up )
+    {
+        PLASSERT( start+length &lt;= up_size );
+        // activations(k, i-start) += sum_j weights(i,j) inputs_mat(k, j)
+        if( accumulate )
+            for (int i=start; i&lt;start+length; i++)
+                productAcc( activations.column(i-start).toVec(),
+                            inputs_mat.subMatColumns( filterStart(i), filterSize(i) ),
+                            weights(i) );
+        else
+            for (int i=start; i&lt;start+length; i++)
+                product( activations.column(i-start).toVec(),
+                         inputs_mat.subMatColumns( filterStart(i), filterSize(i) ),
+                         weights(i) );
+    }
+    else
+    {
+        PLASSERT( start+length &lt;= down_size );
+        if( !accumulate )
+            activations.clear();
+        // activations(k, i-start) += sum_j weights(j,i) inputs_mat(k, j)
+        Mat all_activations(inputs_mat.length(), down_size);
+        all_activations.subMatColumns( start, length ) &lt;&lt; activations;
+        for (int i=0; i&lt;up_size; i++)
+        {
+            externalProductAcc( all_activations.subMatColumns( filterStart(i), filterSize(i) ),
+                                inputs_mat.column(i).toVec(),
+                                weights(i) );
+        }
+        activations &lt;&lt; all_activations.subMatColumns( start, length );
+    }
+}
+
+///////////
+// fprop //
+///////////
+void RBMSparse1DMatrixConnection::fprop(const Vec&amp; input, const Mat&amp; rbm_weights,
+                          Vec&amp; output) const
+{
+    PLERROR(&quot;RBMSparse1DMatrixConnection::fprop not implemented.&quot;);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMSparse1DMatrixConnection::bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                                      Mat&amp; input_gradients,
+                                      const Mat&amp; output_gradients,
+                                      bool accumulate)
+{
+    PLASSERT( inputs.width() == down_size );
+    PLASSERT( outputs.width() == up_size );
+    PLASSERT( output_gradients.width() == up_size );
+
+    if( accumulate )
+        PLASSERT_MSG( input_gradients.width() == down_size &amp;&amp;
+                      input_gradients.length() == inputs.length(),
+                      &quot;Cannot resize input_gradients and accumulate into it&quot; );
+    else {
+        input_gradients.resize(inputs.length(), down_size);
+        input_gradients.clear();
+    }  
+        
+    for (int i=0; i&lt;up_size; i++) {
+        int filter_start= filterStart(i), length= filterSize(i);
+        
+        // input_gradients = output_gradient * weights
+        externalProductAcc( input_gradients.subMatColumns( filter_start, length ),
+                            output_gradients.column(i).toVec(),
+                            weights(i));
+
+        // weights -= learning_rate/n * output_gradients' * inputs
+        transposeProductScaleAcc( weights(i),
+                                  inputs.subMatColumns( filter_start, length ),
+                                  output_gradients.column(i).toVec(),
+                                  -learning_rate / inputs.length(), real(1));
+
+        if( enforce_positive_weights )
+            for (int j=0; j&lt;filter_size; j++)
+                weights(i,j)= max( real(0), weights(i,j) );
+   }
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void RBMSparse1DMatrixConnection::bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                         const TVec&lt;Mat*&gt;&amp; ports_gradient)
+{
+    //TODO: add weights as port?
+    PLASSERT( ports_value.length() == nPorts()
+              &amp;&amp; ports_gradient.length() == nPorts() );
+
+    Mat* down = ports_value[0];
+    //Mat* up = ports_value[1];
+    Mat* down_grad = ports_gradient[0];
+    Mat* up_grad = ports_gradient[1];
+
+    PLASSERT( down &amp;&amp; !down-&gt;isEmpty() );
+    PLASSERT( up &amp;&amp; !up-&gt;isEmpty() );
+
+    int batch_size = down-&gt;length();
+    PLASSERT( up-&gt;length() == batch_size );
+
+    // If we have up_grad
+    if( up_grad &amp;&amp; !up_grad-&gt;isEmpty() )
+    {
+        // down_grad should not be provided
+        PLASSERT( !down_grad || down_grad-&gt;isEmpty() );
+        PLASSERT( up_grad-&gt;length() == batch_size );
+        PLASSERT( up_grad-&gt;width() == up_size );
+
+        bool compute_down_grad = false;
+        if( down_grad &amp;&amp; down_grad-&gt;isEmpty() )
+        {
+            compute_down_grad = true;
+            PLASSERT( down_grad-&gt;width() == down_size );
+            down_grad-&gt;resize(batch_size, down_size);
+        }
+        
+        for (int i=0; i&lt;up_size; i++) {
+            int filter_start= filterStart(i), length= filterSize(i);
+            
+            // propagate gradient
+            // input_gradients = output_gradient * weights
+            if( compute_down_grad )
+                externalProductAcc( down_grad-&gt;subMatColumns( filter_start, length ),
+                                    up_grad-&gt;column(i).toVec(),
+                                    weights(i));
+
+            // update weights
+            // weights -= learning_rate/n * output_gradients' * inputs
+            transposeProductScaleAcc( weights(i),
+                                      down-&gt;subMatColumns( filter_start, length ),
+                                      up_grad-&gt;column(i).toVec(),
+                                      -learning_rate / batch_size, real(1));
+
+            if( enforce_positive_weights )
+                for (int j=0; j&lt;filter_size; j++)
+                    weights(i,j)= max( real(0), weights(i,j) );
+       }
+    }
+    else if( down_grad &amp;&amp; !down_grad-&gt;isEmpty() )
+    {
+        PLERROR(&quot;down-up gradient not implemented in RBMSparse1DMatrixConnection::bpropAccUpdate.&quot;);
+
+        PLASSERT( down_grad-&gt;length() == batch_size );
+        PLASSERT( down_grad-&gt;width() == down_size );
+    }
+    else
+        PLCHECK_MSG( false,
+                     &quot;Unknown port configuration&quot; );
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+void RBMSparse1DMatrixConnection::update( const Mat&amp; pos_down_values, // v_0
+                                  const Mat&amp; pos_up_values,   // h_0
+                                  const Mat&amp; neg_down_values, // v_1
+                                  const Mat&amp; neg_up_values )  // h_1
+{
+    // weights += learning_rate * ( h_0 v_0' - h_1 v_1' );
+    // or:
+    // weights[i][j] += learning_rate * (h_0[i] v_0[j] - h_1[i] v_1[j]);
+
+    PLASSERT( pos_up_values.width() == weights.length() );
+    PLASSERT( neg_up_values.width() == weights.length() );
+    PLASSERT( pos_down_values.width() == down_size );
+    PLASSERT( neg_down_values.width() == down_size );
+
+    if( momentum == 0. )
+    {
+        // We use the average gradient over a mini-batch.
+        real avg_lr = learning_rate / pos_down_values.length();
+
+        for (int i=0; i&lt;up_size; i++) {
+            int filter_start= filterStart(i), length= filterSize(i);
+
+            transposeProductScaleAcc( weights(i),
+                                      pos_down_values.subMatColumns( filter_start, length ),
+                                      pos_up_values.column(i).toVec(), 
+                                      avg_lr, real(1));
+
+            transposeProductScaleAcc( weights(i),
+                                      neg_down_values.subMatColumns( filter_start, length ),
+                                      neg_up_values.column(i).toVec(),
+                                      -avg_lr, real(1));
+
+            if( enforce_positive_weights )
+                for (int j=0; j&lt;filter_size; j++)
+                    weights(i,j)= max( real(0), weights(i,j) );
+        }
+    }
+    else
+        PLERROR(&quot;RBMSparse1DMatrixConnection::update minibatch with momentum - Not implemented&quot;);
+
+    if(!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) 
+        applyWeightPenalty();
+}
+
+////////////
+// forget //
+////////////
+// Reset the parameters to the state they would be BEFORE starting training.
+void RBMSparse1DMatrixConnection::forget()
+{
+    clearStats();
+    if( initialization_method == &quot;zero&quot; )
+        weights.clear();
+    else
+    {
+        if( !random_gen ) {
+            PLWARNING( &quot;RBMSparse1DMatrixConnection: cannot forget() without&quot;
+                       &quot; random_gen&quot; );
+            return;
+        }
+        real d = 1. / max( filter_size, up_size );
+        if( initialization_method == &quot;uniform_sqrt&quot; )
+            d = sqrt( d );
+
+        if( enforce_positive_weights )
+            random_gen-&gt;fill_random_uniform( weights, real(0), d );
+        else
+            random_gen-&gt;fill_random_uniform( weights, -d, d );
+    }
+    L2_n_updates = 0;
+}
+
+//! return the number of parameters
+int RBMSparse1DMatrixConnection::nParameters() const
+{
+    return weights.size();
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.cc
___________________________________________________________________
Name: svn:executable
   + *

Added: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
===================================================================
--- trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h	2008-06-11 02:37:22 UTC (rev 9115)
+++ trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h	2008-06-11 15:00:09 UTC (rev 9116)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+
+// RBMSparse1DMatrixConnection.h
+//
+// Copyright (C) 2008 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file RBMSparse1DMatrixConnection.h */
+
+
+#ifndef RBMSparse1DMatrixConnection_INC
+#define RBMSparse1DMatrixConnection_INC
+
+#include &quot;RBMMatrixConnection.h&quot;
+
+namespace PLearn {
+using namespace std;
+
+
+/**
+ * Stores and learns the parameters between two linear layers of an RBM.
+ *
+ */
+class RBMSparse1DMatrixConnection: public RBMMatrixConnection
+{
+    typedef RBMMatrixConnection inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int filter_size;
+
+    bool enforce_positive_weights;
+
+    //#####  Learned Options  #################################################
+
+    int step_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMSparse1DMatrixConnection( real the_learning_rate=0 );
+
+    // Your other public member functions go here
+    virtual void accumulatePosStats( const Mat&amp; down_values,
+                                     const Mat&amp; up_values );
+
+    virtual void accumulateNegStats( const Mat&amp; down_values,
+                                     const Mat&amp; up_values );
+
+    //! Computes the vectors of activation of &quot;length&quot; units,
+    //! starting from &quot;start&quot;, and stores (or add) them into &quot;activations&quot;.
+    //! &quot;start&quot; indexes an up unit if &quot;going_up&quot;, else a down unit.
+    virtual void computeProducts(int start, int length,
+                                 Mat&amp; activations,
+                                 bool accumulate=false ) const;
+
+
+    //! given the input and the connection weights,
+    //! compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec&amp; input, const Mat&amp; rbm_weights,
+                       Vec&amp; output) const;
+
+    virtual void bpropUpdate(const Mat&amp; inputs, const Mat&amp; outputs,
+                             Mat&amp; input_gradients,
+                             const Mat&amp; output_gradients,
+                             bool accumulate = false);
+
+    virtual void bpropAccUpdate(const TVec&lt;Mat*&gt;&amp; ports_value,
+                                const TVec&lt;Mat*&gt;&amp; ports_gradient);
+
+    virtual void update( const Mat&amp; pos_down_values,
+                         const Mat&amp; pos_up_values,
+                         const Mat&amp; neg_down_values,
+                         const Mat&amp; neg_up_values);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //! return the number of parameters
+    virtual int nParameters() const;
+
+    virtual Mat getWeights() const;
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMSparse1DMatrixConnection);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+protected:
+    //#####  Protected Member Functions  ######################################
+    //! Declares the class options.
+    static void declareOptions(OptionList&amp; ol);
+
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap&amp; rmm);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    int filterStart(int idx) const;
+    int filterSize(int idx) const;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMSparse1DMatrixConnection);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/plearn_learners/online/RBMSparse1DMatrixConnection.h
___________________________________________________________________
Name: svn:executable
   + *


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="002563.html">[Plearn-commits] r9115 - trunk/plearn_learners/online
</A></li>
	<LI>Next message: <A HREF="002565.html">[Plearn-commits] r9117 - trunk/plearn/ker
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#2564">[ date ]</a>
              <a href="thread.html#2564">[ thread ]</a>
              <a href="subject.html#2564">[ subject ]</a>
              <a href="author.html#2564">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
