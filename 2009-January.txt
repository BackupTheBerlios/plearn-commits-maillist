From laulysta at mail.berlios.de  Sat Jan  3 23:52:57 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Sat, 3 Jan 2009 23:52:57 +0100
Subject: [Plearn-commits] r9805 - trunk/plearn_learners_experimental
Message-ID: <200901032252.n03MqvRB005711@sheep.berlios.de>

Author: laulysta
Date: 2009-01-03 23:52:57 +0100 (Sat, 03 Jan 2009)
New Revision: 9805

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
hidden reconstuction not tied


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-31 04:11:52 UTC (rev 9804)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-01-03 22:52:57 UTC (rev 9805)
@@ -138,6 +138,12 @@
                   "The RBMConnection between the first hidden layers, "
                   "through time (optional).\n");
 
+    declareOption(ol, "dynamic_reconstruction_connections", 
+                  &DenoisingRecurrentNet::dynamic_reconstruction_connections,
+                  OptionBase::buildoption,
+                  "The RBMConnection for the reconstruction between the hidden layers, "
+                  "through time (optional).\n");
+
     declareOption(ol, "hidden_connections", 
                   &DenoisingRecurrentNet::hidden_connections,
                   OptionBase::buildoption,
@@ -402,6 +408,18 @@
             dynamic_connections->build();
         }
 
+        if( dynamic_reconstruction_connections )
+        {
+            dynamic_reconstruction_connections->down_size = hidden_layer->size;
+            dynamic_reconstruction_connections->up_size = hidden_layer->size;
+            if( !dynamic_reconstruction_connections->random_gen )
+            {
+                dynamic_reconstruction_connections->random_gen = random_gen;
+                dynamic_reconstruction_connections->forget();
+            }
+            dynamic_reconstruction_connections->build();
+        }
+
         if( hidden_layer2 )
         {
             if( !hidden_layer2->random_gen )
@@ -467,6 +485,7 @@
     deepCopyField( hidden_layer, copies);
     deepCopyField( hidden_layer2 , copies);
     deepCopyField( dynamic_connections , copies);
+    deepCopyField( dynamic_reconstruction_connections , copies);
     deepCopyField( hidden_connections , copies);
     deepCopyField( input_connections , copies);
     deepCopyField( target_connections , copies);
@@ -516,6 +535,8 @@
     input_connections->forget();
     if( dynamic_connections )
         dynamic_connections->forget();
+    if( dynamic_reconstruction_connections )
+        dynamic_reconstruction_connections->forget();
     if( hidden_layer2 )
     {
         hidden_layer2->forget();
@@ -693,7 +714,7 @@
                 if(hidden_reconstruction_lr!=0){
                     setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0, 0 );
+                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0 );
                 }
 
                 // recurrent noisy phase
@@ -995,6 +1016,14 @@
     return conn->weights;
 }
 
+Mat DenoisingRecurrentNet::getDynamicReconstructionConnectionsWeightMatrix()
+{
+    RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)dynamic_reconstruction_connections);
+    if(conn==0)
+        PLERROR("Expecting input connection to be a RBMMatrixConnection. Je sais c'est sale, mais au point ou on est rendu..");
+    return conn->weights;
+}
+
 double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
                                                                        Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
@@ -1087,7 +1116,8 @@
     reconstruction_prob.resize(fullhiddenlength);
 
     // predict (denoised) input_reconstruction 
-    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
+    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden);
+    product(reconstruction_activation, reconstruction_weights, hidden);
     reconstruction_activation += reconstruction_bias;
 
     for( int j=0 ; j<fullhiddenlength ; j++ )
@@ -1103,7 +1133,14 @@
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
 
 
-    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    //productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    
+    //update bias
+    multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
+    // update weight
+    externalProductScaleAcc(reconstruction_weights, hidden_reconstruction_activation_grad, hidden, -lr);
+                
 
     // update weight
     //externalProductScaleAcc(reconstruction_weights, hidden, hidden_reconstruction_activation_grad, -lr);
@@ -1355,7 +1392,7 @@
             // Add contribution of hidden reconstruction cost in hidden_gradient
             Vec hidden_reconstruction_activation_grad;
             hidden_reconstruction_activation_grad.resize(hidden_layer->size);
-            Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+            Mat reconstruction_weights = getDynamicReconstructionConnectionsWeightMatrix();
             if(hidden_reconstruction_weight!=0)
             {
                 //Vec hidden_reconstruction_activation_grad;
@@ -1364,7 +1401,7 @@
                 //truc stan
                 //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
                 fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-            
+                
             }
             
 
@@ -1373,6 +1410,7 @@
             { // add weighted contribution of hidden_temporal gradient to hidden_gradient
                 // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
                 multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+                
             }
             hidden_layer->bpropUpdate(
                 hidden_act_no_bias_list(i), hidden_list(i),
@@ -1383,14 +1421,14 @@
                 hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
                 hidden_gradient, hidden_temporal_gradient);
 
-            if(hidden_reconstruction_weight!=0)
+            /*if(hidden_reconstruction_weight!=0)
             {
                 // update bias
                 multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -current_learning_rate);
                 // update weight
                 externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -current_learning_rate);
                 
-            }
+                }*/
 
             input_connections->bpropUpdate(
                 input_list[i],
@@ -1751,6 +1789,10 @@
         //dynamic_connections->setLearningRate( dynamic_gradient_scale_factor*the_learning_rate ); 
         dynamic_connections->setLearningRate( the_learning_rate ); 
     }
+    if( dynamic_reconstruction_connections ){
+        //dynamic_reconstruction_connections->setLearningRate( dynamic_gradient_scale_factor*the_learning_rate ); 
+        dynamic_reconstruction_connections->setLearningRate( the_learning_rate ); 
+    }
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-31 04:11:52 UTC (rev 9804)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-01-03 22:52:57 UTC (rev 9805)
@@ -97,6 +97,9 @@
     //! The RBMConnection between the first hidden layers, through time
     PP<RBMConnection> dynamic_connections;
 
+    //! The RBMConnection for the reconstruction between the hidden layers, through time
+    PP<RBMConnection> dynamic_reconstruction_connections;
+
     //! The RBMConnection between the first and second hidden layers (optional)
     PP<RBMConnection> hidden_connections;
 
@@ -408,6 +411,8 @@
 
     Mat getDynamicConnectionsWeightMatrix();
 
+    Mat getDynamicReconstructionConnectionsWeightMatrix();
+
     //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
     //! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias



From nouiz at mail.berlios.de  Mon Jan  5 18:57:34 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 5 Jan 2009 18:57:34 +0100
Subject: [Plearn-commits] r9806 - trunk/plearn/base
Message-ID: <200901051757.n05HvYS7009409@sheep.berlios.de>

Author: nouiz
Date: 2009-01-05 18:57:32 +0100 (Mon, 05 Jan 2009)
New Revision: 9806

Modified:
   trunk/plearn/base/CopiesMap.h
Log:
added a NODEEPCOPY for int8_t that caused compilation error. I don't understand why we don't need a NODEEPCOPY for int16_t but we need one for int8_t.


Modified: trunk/plearn/base/CopiesMap.h
===================================================================
--- trunk/plearn/base/CopiesMap.h	2009-01-03 22:52:57 UTC (rev 9805)
+++ trunk/plearn/base/CopiesMap.h	2009-01-05 17:57:32 UTC (rev 9806)
@@ -108,6 +108,7 @@
 NODEEPCOPY(float)
 NODEEPCOPY(const float)
 NODEEPCOPY(int)
+NODEEPCOPY(int8_t)
 NODEEPCOPY(const int)
 NODEEPCOPY(unsigned int)
 NODEEPCOPY(const unsigned int)



From nouiz at mail.berlios.de  Mon Jan  5 19:10:27 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 5 Jan 2009 19:10:27 +0100
Subject: [Plearn-commits] r9807 - trunk/plearn_learners/regressors
Message-ID: <200901051810.n05IARQS025676@sheep.berlios.de>

Author: nouiz
Date: 2009-01-05 19:10:26 +0100 (Mon, 05 Jan 2009)
New Revision: 9807

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
parallel matrix sort and better comment.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-05 17:57:32 UTC (rev 9806)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-05 18:10:26 UTC (rev 9807)
@@ -212,8 +212,7 @@
     }
 }
 
-//! reg must already have a size >= the number of row that we will put in it.
-//! Will resize reg to the number of registered leave==leave_id
+//! reg.size() == the number of row that we will put in it.
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg) const
 {
@@ -252,6 +251,7 @@
 //     {
 //         sorted_row(each_train_sample_index).fill(each_train_sample_index);
 //     }
+#pragma omp parallel for default(none) shared(pb)
     for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
     {
         sortEachDim(sample_dim);



From nouiz at mail.berlios.de  Tue Jan  6 15:42:42 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Jan 2009 15:42:42 +0100
Subject: [Plearn-commits] r9808 - trunk/plearn_learners/meta
Message-ID: <200901061442.n06Egghm012894@sheep.berlios.de>

Author: nouiz
Date: 2009-01-06 15:42:41 +0100 (Tue, 06 Jan 2009)
New Revision: 9808

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
added LOG that warn about non optimal code execution.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-05 18:10:26 UTC (rev 9807)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-06 14:42:41 UTC (rev 9808)
@@ -49,6 +49,8 @@
 #include <plearn/io/load_and_save.h>
 #include <plearn/base/stringutils.h>
 #include <plearn_learners/regressors/RegressionTreeRegisters.h>
+#define PL_LOG_MODULE_NAME "AdaBoost"
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;
@@ -962,7 +964,12 @@
         if(!modif_train_set_weights)
             if(training_set->weightsize()==1)
                 modif_train_set_weights=1;
-
+            else
+                NORMAL_LOG<<"In AdaBoost::setTrainingSet() -"
+                          <<" We have RegressionTree as weak_learner, but the"
+                          <<" training_set don't have a weigth. This will cause"
+                          <<" the creation of a RegressionTreeRegisters at"
+                          <<" each stage of AdaBoost!";
         //we do this as RegressionTreeNode need a train_set for getTestCostNames
         if(!weak_learner_template->getTrainingSet())
             weak_learner_template->setTrainingSet(training_set,call_forget);



From nouiz at mail.berlios.de  Tue Jan  6 17:06:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Jan 2009 17:06:39 +0100
Subject: [Plearn-commits] r9809 - trunk/plearn_learners/regressors
Message-ID: <200901061606.n06G6dmO021578@sheep.berlios.de>

Author: nouiz
Date: 2009-01-06 17:06:39 +0100 (Tue, 06 Jan 2009)
New Revision: 9809

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
handle correctly RTR_type_id with unsigned type.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-01-06 14:42:41 UTC (rev 9808)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-01-06 16:06:39 UTC (rev 9809)
@@ -125,8 +125,7 @@
 void RegressionTreeLeave::initLeave(PP<RegressionTreeRegisters> the_train_set, RTR_type_id the_id, bool the_missing_leave)
 {
     train_set = the_train_set;
-    if(the_id>=0)
-        id = the_id;
+    id = the_id;
     missing_leave = the_missing_leave;
 }
 

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-01-06 14:42:41 UTC (rev 9808)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-01-06 16:06:39 UTC (rev 9809)
@@ -86,7 +86,7 @@
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
-    void         initLeave(PP<RegressionTreeRegisters> the_train_set, RTR_type_id the_id=-1, bool the_missing_leave = false);
+    void         initLeave(PP<RegressionTreeRegisters> the_train_set, RTR_type_id the_id, bool the_missing_leave = false);
     virtual void         initStats();
     virtual void         addRow(int row);
     virtual void         addRow(int row, real target, real weight);



From nouiz at mail.berlios.de  Tue Jan  6 18:11:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Jan 2009 18:11:45 +0100
Subject: [Plearn-commits] r9810 - trunk/plearn_learners/regressors
Message-ID: <200901061711.n06HBjHc028117@sheep.berlios.de>

Author: nouiz
Date: 2009-01-06 18:11:45 +0100 (Tue, 06 Jan 2009)
New Revision: 9810

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
added an error.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-01-06 16:06:39 UTC (rev 9809)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-01-06 17:11:45 UTC (rev 9810)
@@ -217,6 +217,9 @@
 {
     Profiler::pl_profile_start("RegressionTree::train");
 
+    if(std::numeric_limits<RTR_type_id>::max() < nstages*(missing_is_valid?9:6))
+        PLERROR("The type of RTR_type_id(%s) doesn't have enought capacity","RTR_type_id");
+
     if (stage == 0) initialiseTree();
     PP<ProgressBar> pb;
     if (report_progress)



From saintmlx at mail.berlios.de  Tue Jan  6 20:37:53 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 6 Jan 2009 20:37:53 +0100
Subject: [Plearn-commits] r9811 - trunk/plearn/python
Message-ID: <200901061937.n06Jbr5h029193@sheep.berlios.de>

Author: saintmlx
Date: 2009-01-06 20:37:53 +0100 (Tue, 06 Jan 2009)
New Revision: 9811

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
- more debug output
- allow printing of python-wrapped objects from PLearn
- make python aware of PLearn option flags



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2009-01-06 17:11:45 UTC (rev 9810)
+++ trunk/plearn/python/PythonExtension.cc	2009-01-06 19:37:53 UTC (rev 9811)
@@ -275,7 +275,8 @@
             "  \"\"\" \n" + class_help_text + "\n \"\"\"\n"
             "  def __new__(cls,*args,**kwargs):\n"
             "    #get_plearn_module().loggingControl(500, ['__ALL__'])"
-            "    #print '** "+pyclassname+".__new__',kwargs\n"
+            "    #print '** "+pyclassname+".__new__',args,kwargs\n"
+            "    #import sys; sys.stdout.flush()\n"
             "    obj= object.__new__(cls,*args,**kwargs)\n"
             "    if '_cptr' not in kwargs:\n"
             "      obj._cptr= cls._newCPPObj('"+classname+"')\n"
@@ -300,9 +301,11 @@
         //set option names
         OptionList& options= tit->second.getoptionlist_method();
         unsigned int nopts= options.size();
-        set<string> optionnames;
+        //set<string> optionnames;
+        map<string, vector<string> > optionnames;
         for(unsigned int i= 0; i < nopts; ++i)
-            optionnames.insert(options[i]->optionname());
+            //optionnames.insert(options[i]->optionname());
+            optionnames[options[i]->optionname()]= options[i]->flagStrings();
         the_pyclass= clit->second;
         if(-1==PyObject_SetAttrString(the_pyclass, const_cast<char*>("_optionnames"), 
                                       PythonObjectWrapper(optionnames).getPyObject()))

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2009-01-06 17:11:45 UTC (rev 9810)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2009-01-06 19:37:53 UTC (rev 9811)
@@ -41,6 +41,8 @@
 
 /*! \file PythonObjectWrapper.cc */
 
+#define PL_LOG_MODULE_NAME "PythonObjectWrapper"
+
 // Must include Python first...
 #include "PythonObjectWrapper.h"
 #include "PythonEmbedder.h"
@@ -58,6 +60,7 @@
 #include <plearn/var/VarArray.h>
 #include <plearn/base/RealMapping.h> // for RealRange
 #include <plearn/vmat/VMField.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;
@@ -193,6 +196,9 @@
 Object* ConvertFromPyObject<Object*>::convert(PyObject* pyobj,
                                               bool print_traceback)
 {
+    DBG_MODULE_LOG << "ConvertFromPyObject<Object*>::convert("
+                   << (void*)pyobj << ' ' << PythonObjectWrapper(pyobj)
+                   << ')' << endl;
     PLASSERT(pyobj);
     if(pyobj == Py_None)
         return 0;
@@ -211,6 +217,9 @@
     Object* obj= static_cast<Object*>(PyCObject_AsVoidPtr(cptr));
 
     Py_DECREF(cptr);
+    DBG_MODULE_LOG << "EXITING ConvertFromPyObject<Object*>::convert("
+                   << (void*)pyobj << ' ' << PythonObjectWrapper(pyobj)
+                   << ')' << " => " << (void*)obj << ' ' << obj->asString() << endl;
     return obj;
 }
 
@@ -502,6 +511,8 @@
 //##### Trampoline ############################################################
 PyObject* PythonObjectWrapper::trampoline(PyObject* self, PyObject* args)
 {
+    DBG_MODULE_LOG << "PythonObjectWrapper::trampoline(" << PythonObjectWrapper(self)
+                   << ", " << PythonObjectWrapper(args) << ')' << endl;
     PythonGlobalInterpreterLock gil;         // For thread-safety
 
     //get object and trampoline from self
@@ -557,6 +568,26 @@
     }
 }
 
+
+void checkWrappedObjects(const string& msg)
+{
+    DBG_MODULE_LOG << msg << endl;
+    map<PyObject*, const Object*> rev_map;
+    for(PythonObjectWrapper::wrapped_objects_t::iterator it= PythonObjectWrapper::m_wrapped_objects.begin();
+        it != PythonObjectWrapper::m_wrapped_objects.end(); ++it)
+    {
+        DBG_MODULE_LOG << "checking:" << (void*)it->first << " -> " << (void*)it->second << endl;
+        map<PyObject*, const Object*>::iterator jt= rev_map.find(it->second);
+        DBG_MODULE_LOG.clearInOutMaps();
+        if(jt != rev_map.end())
+            DBG_MODULE_LOG << "*** ALREADY IN MAP:" << it->second << "w/" << it->first << " ; now " << jt->second << endl;
+        //else
+        rev_map[it->second]= it->first;
+    }
+    DBG_MODULE_LOG << "FINISHED checking wrapped objects:\t" << rev_map.size() << '\t' << msg << endl;
+}
+
+
 PyObject* PythonObjectWrapper::python_del(PyObject* self, PyObject* args)
 {
     TVec<PyObject*> args_tvec=
@@ -593,6 +624,7 @@
         PythonObjectWrapper(args).as<TVec<PyObject*> >();
     Object* o= newObjectFromClassname(PyString_AsString(args_tvec[1]));
 
+    DBG_MODULE_LOG << "In PythonObjectWrapper::newCPPObj() " << PyString_AsString(args_tvec[1]) << " <-> " << (void*)o << '\t' << o << endl;
     //perr << "new o->usage()= " << o->usage() << endl;
 
     return PyCObject_FromVoidPtr(o, 0);
@@ -609,16 +641,21 @@
 
     //perr << "ref o->usage()= " << o->usage() << endl;
     PythonObjectWrapper::m_wrapped_objects[o]= pyo;
+    //checkWrappedObjects(">>>>>>>>>> in refcppobj -> checkWrappedObjects");// debug only
     //perr << "refCPPObj: " << (void*)o << " : " << (void*)pyo << endl;
 
+    DBG_MODULE_LOG << "In PythonObjectWrapper::refCPPObj() " << PythonObjectWrapper(pyo) << " <-> " << (void*)o << '\t' << o << endl;
     addToWrappedObjectsSet(pyo);//Py_INCREF(pyo);
-    //printWrappedObjects();
+    
+    //printWrappedObjects();///***///***
 
     return newPyObject();//None
 }
 
 void PythonObjectWrapper::gc_collect1()
 {
+    DBG_MODULE_LOG << "entering PythonObjectWrapper::gc_collect1()" << endl;
+
     if(m_gc_next_object == m_wrapped_objects.end())
         m_gc_next_object= m_wrapped_objects.begin();
     if(m_gc_next_object != m_wrapped_objects.end())
@@ -628,10 +665,24 @@
         if(it->first->usage() == 1 && it->second->ob_refcnt == 1)
         {
             //Py_DECREF(it->second);
+            DBG_MODULE_LOG.clearInOutMaps();
+            PyObject* cptr= PyObject_GetAttrString(it->second, const_cast<char*>("_cptr"));
+            DBG_MODULE_LOG << "In PythonObjectWrapper::gc_collect1(), removing object " 
+                           << PythonObjectWrapper(it->second)
+                           << " ; python version of: " << it->first << " (" << (void*)it->first 
+                           << ", " << PyCObject_AsVoidPtr(cptr) << ')'
+                           << endl;
+            if(!cptr)
+            {
+                if(PyErr_Occurred()) PyErr_Print();
+                PLERROR("In PythonObjectWrapper::gc_collect1 : cannot get attribute '_cptr' from Python object ");
+            }
+            Py_DECREF(cptr);
             removeFromWrappedObjectsSet(it->second);
             gc_collect1();
         }
     }
+    DBG_MODULE_LOG << "exiting PythonObjectWrapper::gc_collect1()" << endl;
 }
 
 
@@ -660,6 +711,8 @@
 
 PyObject* ConvertToPyObject<Object*>::newPyObject(const Object* x)
 {
+    DBG_MODULE_LOG << "ENTER ConvertToPyObject<Object*>::newPyObject " 
+                   << (void*)x << ' ' << (x?x->asString():"") << endl;
     // void ptr becomes None
     if(!x) return PythonObjectWrapper::newPyObject();
 
@@ -707,11 +760,15 @@
     x->ref();
 
     PythonObjectWrapper::m_wrapped_objects[x]= the_obj;
+    //checkWrappedObjects(">>>>>>>>>> in newpyobj -> checkWrappedObjects"); // debug only
 
 //    perr << "newPyObject: " << (void*)x << " : " << (void*)the_obj << endl;
 
     addToWrappedObjectsSet(the_obj);//Py_INCREF(the_obj);
     //printWrappedObjects();
+    DBG_MODULE_LOG << "EXIT ConvertToPyObject<Object*>::newPyObject " 
+                   << (void*)x << ' ' << x->asString() << " => "
+                   << (void*) the_obj << ' ' << PythonObjectWrapper(the_obj) << endl;
 
     return the_obj;
 }
@@ -948,6 +1005,9 @@
 
 PStream& operator<<(PStream& out, const PythonObjectWrapper& v)
 {
+    out << v.getPyObject();
+    return out;
+
     PLERROR("operator<<(PStream&, const PythonObjectWrapper&) : "
             "not supported (yet).");
 /*
@@ -970,7 +1030,7 @@
     Py_DECREF(res);
     string pickle= 
         PythonObjectWrapper(env).as<std::map<string, PythonObjectWrapper> >()["result"];
-    Py_DECREF(env);
+        Py_DECREF(env);
     string toout= string("PythonObjectWrapper(ownership=") + tostring(v.m_ownership) + ", object=\"" + pickle + "\")";
     out << toout;
 */
@@ -1002,6 +1062,10 @@
 //! debug
 void printWrappedObjects()
 {
+    //checkWrappedObjects(">>>>>>>>>> in printwrappedobjs -> checkWrappedObjects"); // debug only
+
+    DBG_MODULE_LOG << "the_PLearn_python_module= " << (void*)the_PLearn_python_module << endl;
+
     perr << "wrapped_objects= " << endl;
     for(PythonObjectWrapper::wrapped_objects_t::iterator it= 
             PythonObjectWrapper::m_wrapped_objects.begin();
@@ -1013,6 +1077,7 @@
 
 void ramassePoubelles()
 {
+    DBG_MODULE_LOG << "entering ramassePoubelles" << endl;
     size_t sz= 0;
     while(sz != PythonObjectWrapper::m_wrapped_objects.size())
     {
@@ -1024,9 +1089,13 @@
             PythonObjectWrapper::wrapped_objects_t::iterator jt= it;
             ++it;
             if(jt->second->ob_refcnt == 1 && jt->first->usage() == 1)
+            {
+                DBG_MODULE_LOG << "In ramassePoubelles, removing object" << PythonObjectWrapper(jt->second) << endl;
                 removeFromWrappedObjectsSet(jt->second);
+            }
         }
     }
+    DBG_MODULE_LOG << "exiting ramassePoubelles" << endl;
 }
 
 



From saintmlx at mail.berlios.de  Tue Jan  6 20:39:01 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 6 Jan 2009 20:39:01 +0100
Subject: [Plearn-commits] r9812 - trunk/python_modules/plearn/pybridge
Message-ID: <200901061939.n06Jd1Se029261@sheep.berlios.de>

Author: saintmlx
Date: 2009-01-06 20:39:01 +0100 (Tue, 06 Jan 2009)
New Revision: 9812

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- use flags for PLearn options when pickling (e.g. nosave, remotetransmit)
- patch unpickle (again...)



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2009-01-06 19:37:53 UTC (rev 9811)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2009-01-06 19:39:01 UTC (rev 9812)
@@ -131,14 +131,17 @@
 
     def __getstate__(self):
         """
-        Returns self's dict, except that the value associated with the '_cptr'
-        key is replaced by 
+        Returns self's dict, except that the value associated with the
+        '_cptr' key is replaced by a PLearn class name with a dict of
+        options and values.
         """
         PLEARN_PICKLE_PROTOCOL_VERSION= 2
         d= self.__dict__.copy()
         d['_cptr']= (PLEARN_PICKLE_PROTOCOL_VERSION,self.classname(),{})
         for o in self._optionnames:
-            d['_cptr'][2][o]= self.getOption(o)
+            if 'nosave' not in self._optionnames[o] \
+                    or get_remote_pickle() and 'remotetransmit' in self._optionnames[o]:
+                d['_cptr'][2][o]= self.getOption(o)
         return d
     ##### old, deprecated version follows: (for reference only)
     def old_deprecated___getstate__(self):
@@ -171,6 +174,13 @@
         PLEARN_PICKLE_PROTOCOL_VERSION= 2
         if d[0] != PLEARN_PICKLE_PROTOCOL_VERSION:
             raise RuntimeError, "PLearn pickle protocol version should be 2"
+
+        if not hasattr(self, '_cptr'):
+            # TODO: check that this works in all cases...
+            newone= plearn_module.newObjectFromClassname(d[1])
+            self._cptr= newone._cptr
+            self._refCPPObj(self, False)
+
         # empty PLearn object already exists (from __new__)
         for k in dict:
             if k != '_cptr':



From nouiz at mail.berlios.de  Tue Jan  6 21:03:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Jan 2009 21:03:12 +0100
Subject: [Plearn-commits] r9813 - trunk/plearn/math
Message-ID: <200901062003.n06K3COH031663@sheep.berlios.de>

Author: nouiz
Date: 2009-01-06 21:03:11 +0100 (Tue, 06 Jan 2009)
New Revision: 9813

Modified:
   trunk/plearn/math/TVec_decl.h
Log:
an optimization probably caused by gcc4.1 not inlining TVec.resize(). It don't inline it probably as it is too big for its default value. Maybe playing with the parameter "-finline-limit=n" with n>600(default is 600) would help. But their is chance that other compiler do it too... So this fix for append.


Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2009-01-06 19:39:01 UTC (rev 9812)
+++ trunk/plearn/math/TVec_decl.h	2009-01-06 20:03:11 UTC (rev 9813)
@@ -566,7 +566,13 @@
 
     inline void append(const T& newval)
     {
-        resize(length()+1, length());
+        //we do this as an speed optimization. I see a 3.5% speed up...
+        //g++4.1 don't seam to inline resize event in heavy loop of append.
+        //maybe this is the cause of the speed up?
+        if (storage.isNotNull() && (length() < capacity())){
+            length_++;
+        }else
+            resize(length()+1, length());
         lastElement() = newval;
     }
 



From nouiz at mail.berlios.de  Tue Jan  6 22:26:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Jan 2009 22:26:57 +0100
Subject: [Plearn-commits] r9814 - trunk/plearn_learners/meta
Message-ID: <200901062126.n06LQvQY006532@sheep.berlios.de>

Author: nouiz
Date: 2009-01-06 22:26:57 +0100 (Tue, 06 Jan 2009)
New Revision: 9814

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
print the progress bar only when usefull.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-06 20:03:11 UTC (rev 9813)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-06 21:26:57 UTC (rev 9814)
@@ -408,14 +408,14 @@
     {
         VMat weak_learner_training_set;
         { 
-            PP<ProgressBar> pb;
-            if(report_progress) pb = new ProgressBar(
-                "AdaBoost round " + tostring(stage) +
-                ": making training set for weak learner", n);
-
             // We shall now construct a training set for the new weak learner:
             if (weight_by_resampling)
             {
+                PP<ProgressBar> pb;
+                if(report_progress) pb = new ProgressBar(
+                    "AdaBoost round " + tostring(stage) +
+                    ": making training set for weak learner", n);
+
                 // use a "smart" resampling that approximated sampling 
                 // with replacement with the probabilities given by 
                 // example_weights.



From nouiz at mail.berlios.de  Tue Jan  6 23:01:56 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 6 Jan 2009 23:01:56 +0100
Subject: [Plearn-commits] r9815 - trunk/plearn_learners/regressors
Message-ID: <200901062201.n06M1uBQ011059@sheep.berlios.de>

Author: nouiz
Date: 2009-01-06 23:01:56 +0100 (Tue, 06 Jan 2009)
New Revision: 9815

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
optimization by implementing by using pointeur. This give ~15% speed up.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-06 21:26:57 UTC (rev 9814)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-06 22:01:56 UTC (rev 9815)
@@ -220,12 +220,20 @@
     int idx=0;
     int n=reg.length();
     int i;
+    RTR_type* preg = reg.data();
+    RTR_type* ptsorted_row = tsorted_row[col];
+    RTR_type_id* pleave_register = leave_register.data();
     for( i=0;i<length() && n> idx;i++)
     {
-        int srow = tsorted_row(col, i);
-        if ( leave_register[srow] == leave_id)
-            reg[idx++]=srow;
+        PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
+        int srow = ptsorted_row[i];
+        if ( pleave_register[srow] == leave_id){
+            PLASSERT(leave_register[srow] == leave_id);
+            PLASSERT(preg[idx]==reg[idx]);
+            preg[idx++]=srow;
+        }
     }
+    PLASSERT(idx==reg->size());
 }
 
 void RegressionTreeRegisters::sortRows()



From larocheh at mail.berlios.de  Thu Jan  8 17:59:47 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 8 Jan 2009 17:59:47 +0100
Subject: [Plearn-commits] r9816 - in trunk: . plearn/vmat
Message-ID: <200901081659.n08GxlQx017025@sheep.berlios.de>

Author: larocheh
Date: 2009-01-08 17:59:46 +0100 (Thu, 08 Jan 2009)
New Revision: 9816

Added:
   trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
   trunk/plearn/vmat/InfiniteMNISTVMatrix.h
Modified:
   trunk/pymake.config.model
Log:
This commit adds a VMatrix that contains samples of the MNIST dataset as well as transformed versions of these samples according to class-invariant transformations.

The pymake.config.model file was updated to include the necessary library flag for compilation. This currently works only at the LISA lab...


Added: trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
===================================================================
--- trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-06 22:01:56 UTC (rev 9815)
+++ trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-08 16:59:46 UTC (rev 9816)
@@ -0,0 +1,251 @@
+// -*- C++ -*-
+
+// InfiniteMNISTVMatrix.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file InfiniteMNISTVMatrix.cc */
+
+
+#include "InfiniteMNISTVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+
+// Initialize static variables
+mnistproblem_t* InfiniteMNISTVMatrix::dataset = 0;
+int InfiniteMNISTVMatrix::n_pointers_to_dataset = 0;
+
+PLEARN_IMPLEMENT_OBJECT(
+    InfiniteMNISTVMatrix,
+    "VMatrix containing an \"infinite\" stream of MNIST samples.",
+    "VMatrix that uses the code from \"Training Invariant Support Vector Machines\n"
+    "using Selective Sampling\" by Loosli, Canu and Bottou (JMLR 2007), to generate\n"
+    "\"infinite\" stream (i.e. INT_MAX sized set) of samples from the MNIST dataset. The samples\n"
+    "are obtained by applying some class-invariante transformations on the original MNIST\n"
+    "dataset.\n"
+    );
+
+InfiniteMNISTVMatrix::InfiniteMNISTVMatrix():
+    include_test_examples(false),
+    include_validation_examples(false),
+    input_divisor(1.),
+    test_images(TEST_IMAGES_PATH),
+    test_labels(TEST_LABELS_PATH),
+    train_images(TRAIN_IMAGES_PATH),
+    train_labels(TRAIN_LABELS_PATH),
+    fields(FIELDS_PATH),
+    tangent_vectors(TANGVEC_PATH)
+/* ### Initialize all fields to their default value */
+{
+    InfiniteMNISTVMatrix::n_pointers_to_dataset++;
+}
+
+InfiniteMNISTVMatrix::~InfiniteMNISTVMatrix()
+{
+    InfiniteMNISTVMatrix::n_pointers_to_dataset--;
+    if( InfiniteMNISTVMatrix::dataset && InfiniteMNISTVMatrix::n_pointers_to_dataset == 0 )
+    {
+            destroy_mnistproblem(dataset);
+            InfiniteMNISTVMatrix::dataset = 0;
+    }
+}
+
+void InfiniteMNISTVMatrix::getNewRow(int i, const Vec& v) const
+{
+    int i_dataset;
+    if( include_test_examples )
+        if( include_validation_examples )
+            i_dataset = i;
+        else
+            if( i < 10000)
+                i_dataset = i;
+            else
+                i_dataset = i + ((i-10000)/50000)*10000;
+    else
+        if( include_validation_examples )
+            i_dataset = i+10000;
+        else
+            i_dataset = i + (i/50000)*10000 + 10000;
+
+    unsigned char *image = cache_transformed_vector(InfiniteMNISTVMatrix::dataset, i_dataset);
+
+    unsigned char* xj=image;
+    real* vj=v.data();
+    for( int j=0; j<inputsize_; j++, xj++, vj++ )
+        *vj = *xj/input_divisor;
+    
+    v.last() = InfiniteMNISTVMatrix::dataset->y[ (i_dataset<10000) ? i_dataset : 10000 + ((i_dataset - 10000) % 60000) ];
+}
+
+void InfiniteMNISTVMatrix::declareOptions(OptionList& ol)
+{
+     declareOption(ol, "include_test_examples", &InfiniteMNISTVMatrix::include_test_examples,
+                   OptionBase::buildoption,
+                   "Indication that the test examples from the MNIST dataset should be included.\n"
+                   "This option is false by default. If true, these examples will be the first"
+                   "10000\n"
+                   "of this VMatrix.\n");
+
+     declareOption(ol, "include_validation_examples", &InfiniteMNISTVMatrix::include_validation_examples,
+                   OptionBase::buildoption,
+                   "Indication that the validation set examples (the last 10000 examples from the\n"
+                   "training set) should be included in this VMatrix.\n");     
+
+     declareOption(ol, "input_divisor", &InfiniteMNISTVMatrix::input_divisor,
+                   OptionBase::buildoption,
+                   "Value that the inputs should be divided by.\n");     
+
+     declareOption(ol, "test_images", &InfiniteMNISTVMatrix::test_images,
+                   OptionBase::buildoption,
+                   "File path of MNIST test images.\n");     
+
+     declareOption(ol, "test_labels", &InfiniteMNISTVMatrix::test_labels,
+                   OptionBase::buildoption,
+                   "File path of MNIST test labels.\n");     
+
+     declareOption(ol, "train_images", &InfiniteMNISTVMatrix::train_images,
+                   OptionBase::buildoption,
+                   "File path of MNIST train images.\n");     
+
+     declareOption(ol, "train_labels", &InfiniteMNISTVMatrix::train_labels,
+                   OptionBase::buildoption,
+                   "File path of MNIST train labels.\n");     
+
+     declareOption(ol, "fields", &InfiniteMNISTVMatrix::fields,
+                   OptionBase::buildoption,
+                   "File path of MNIST fields information.\n");     
+
+     declareOption(ol, "tangent_vectors", &InfiniteMNISTVMatrix::tangent_vectors,
+                   OptionBase::buildoption,
+                   "File paht of MNIST transformation tangent vectors.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void InfiniteMNISTVMatrix::build_()
+{
+
+    if( !InfiniteMNISTVMatrix::dataset )
+    {
+        char* test_images_char = new char[test_images.size()+1];
+        char* test_labels_char = new char[test_labels.size()+1];
+        char* train_images_char = new char[train_images.size()+1];
+        char* train_labels_char = new char[train_labels.size()+1];
+        char* fields_char = new char[fields.size()+1];
+        char* tangent_vectors_char = new char[tangent_vectors.size()+1];
+        
+        strcpy(test_images_char,test_images.c_str());
+        strcpy(test_labels_char,test_labels.c_str());
+        strcpy(train_images_char,train_images.c_str());
+        strcpy(train_labels_char,train_labels.c_str());
+        strcpy(fields_char,fields.c_str());
+        strcpy(tangent_vectors_char,tangent_vectors.c_str());
+        
+        InfiniteMNISTVMatrix::dataset = create_mnistproblem(
+            test_images_char,
+            test_labels_char,
+            train_images_char,
+            train_labels_char,
+            fields_char,
+            tangent_vectors_char);
+        if( !InfiniteMNISTVMatrix::dataset )
+            PLERROR("In InfiniteMNISTVMatrix(): could not load MNIST dataset");
+    }
+
+
+    if( include_test_examples )
+        if( include_validation_examples )
+            length_ = INT_MAX;
+        else
+            // Might be removing more samples than need, but we have so many anyways...
+            length_ = INT_MAX - ((INT_MAX-10000)/50000)*10000 + 1;
+    
+    else
+        if( include_validation_examples )
+            length_ = INT_MAX - 10000+1;
+        else
+            // Might be removing more samples than need, but we have so many anyways...
+            length_ = INT_MAX - ((INT_MAX-10000)/50000)*10000 - 10000 + 1;
+
+    inputsize_ = 784;
+    targetsize_ = 1;
+    weightsize_ = 0;
+    extrasize_ = 0;
+    width_ = 785;
+
+    // ### You should keep the line 'updateMtime(0);' if you don't implement the 
+    // ### update of the mtime. Otherwise you can have an mtime != 0 that is not valid.
+    updateMtime(0);
+    //updateMtime(filename);
+    //updateMtime(VMat);
+}
+
+// ### Nothing to add here, simply calls build_
+void InfiniteMNISTVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+void InfiniteMNISTVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    PLWARNING("InfiniteMNISTVMatrix::makeDeepCopyFromShallowCopy is not totally implemented. Need "
+              "to figure out how to deep copy the \"dataset\" variable (mnistproblem_t*).\n");
+    InfiniteMNISTVMatrix::n_pointers_to_dataset++;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/InfiniteMNISTVMatrix.h
===================================================================
--- trunk/plearn/vmat/InfiniteMNISTVMatrix.h	2009-01-06 22:01:56 UTC (rev 9815)
+++ trunk/plearn/vmat/InfiniteMNISTVMatrix.h	2009-01-08 16:59:46 UTC (rev 9816)
@@ -0,0 +1,168 @@
+// -*- C++ -*-
+
+// InfiniteMNISTVMatrix.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file InfiniteMNISTVMatrix.h */
+
+
+#ifndef InfiniteMNISTVMatrix_INC
+#define InfiniteMNISTVMatrix_INC
+
+#define TEST_IMAGES_PATH "/home/fringant2/lisa/data/mnist/t10k-images-idx3-ubyte"
+#define TEST_LABELS_PATH "/home/fringant2/lisa/data/mnist/t10k-labels-idx1-ubyte"
+#define TRAIN_IMAGES_PATH "/home/fringant2/lisa/data/mnist/train-images-idx3-ubyte"
+#define TRAIN_LABELS_PATH "/home/fringant2/lisa/data/mnist/train-labels-idx1-ubyte"
+#define FIELDS_PATH "/home/fringant2/lisa/data/mnist/fields_float_1522x28x28.bin"
+#define TANGVEC_PATH "/home/fringant2/lisa/data/mnist/tangVec_float_60000x28x28.bin"
+
+#include <plearn/vmat/RowBufferedVMatrix.h>
+#include <kernel-invariant.h>
+
+namespace PLearn {
+
+/**
+ * VMatrix that uses the code from "Training Invariant Support Vector Machines 
+ * using Selective Sampling" by Loosli, Canu and Bottou (JMLR 2007), to generate 
+ * "infinite" stream (i.e. INT_MAX sized set) of samples from the MNIST dataset. The samples
+ * are obtained by applying some class-invariante transformations on the original MNIST
+ * dataset.
+ */
+class InfiniteMNISTVMatrix : public RowBufferedVMatrix
+{
+    typedef RowBufferedVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Indication that the test examples from the MNIST dataset should be included.
+    //! This option is false by default. If true, these examples will be the first 10000
+    //! of this VMatrix.
+    bool include_test_examples;
+
+    //! Indication that the validation set examples (the last 10000 examples from the
+    //! training set) should be included in this VMatrix.
+    bool include_validation_examples;
+    
+    //! Value that the inputs should be divided by.
+    real input_divisor;
+
+    // Files required for loading infinite MNIST dataset
+    //! File path of MNIST test images.
+    string test_images;
+    //! File path of MNIST test labels.
+    string test_labels;
+    //! File path of MNIST train images.
+    string train_images;
+    //! File path of MNIST train labels.
+    string train_labels;
+    //! File path of MNIST fields information.
+    string fields;
+    //! File path of MNIST transformation tangent vectors.
+    string tangent_vectors;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    InfiniteMNISTVMatrix();
+
+    ~InfiniteMNISTVMatrix();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(InfiniteMNISTVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    static mnistproblem_t *dataset;
+    static int n_pointers_to_dataset;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! 'v' is assumed to be the right size.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(InfiniteMNISTVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-01-06 22:01:56 UTC (rev 9815)
+++ trunk/pymake.config.model	2009-01-08 16:59:46 UTC (rev 9816)
@@ -156,6 +156,11 @@
                                                   "pl_repository_revision.cc" ))
 
 from plearn.pytest.core import pytest_defines
+
+optionalLibrary( name = 'infinite_mnist',
+                 triggers = 'kernel-invariant.h',
+                 linkeroptions = '-lkernel-invariant' )
+
 optionalLibrary( name = 'pytest_defines',
                  triggers = 'pytest/defines.h',
                  includedirs = [ os.path.join(ppath('HOME'), '.plearn') ], 



From nouiz at mail.berlios.de  Fri Jan  9 20:56:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 Jan 2009 20:56:58 +0100
Subject: [Plearn-commits] r9817 - trunk/scripts
Message-ID: <200901091956.n09Juwlm022558@sheep.berlios.de>

Author: nouiz
Date: 2009-01-09 20:56:57 +0100 (Fri, 09 Jan 2009)
New Revision: 9817

Modified:
   trunk/scripts/perlgrep
Log:
now check in .tex file.


Modified: trunk/scripts/perlgrep
===================================================================
--- trunk/scripts/perlgrep	2009-01-08 16:59:46 UTC (rev 9816)
+++ trunk/scripts/perlgrep	2009-01-09 19:56:57 UTC (rev 9817)
@@ -103,7 +103,7 @@
                 my @flist = lsdir($fname);
                 
                 @flist = grep { -d $_ or /Makefile|makefile|pytest\.config|
-                                    \.c$|\.C$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$
+                                    \.c$|\.C$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$|\.tex$
                                    |\.plearn$|\.pyplearn$|\.psave$|\.vmat$|\.py$|\.pymat$
                                    |\.txt$|^readme|^Readme|^README/x } @flist;
                 process_list(@flist); 
@@ -129,7 +129,7 @@
 
 Will perform the specified grep operation on every file in the
 list and recursively in directories.  (only certain kinds of files are
-considered when recursing in directories, .c .C .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README)
+considered when recursing in directories, .c .C .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README .tex)
 
 Ex: perlgrep '\\bVMatrix\\b' .
 ";



From nouiz at mail.berlios.de  Fri Jan  9 21:24:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 9 Jan 2009 21:24:57 +0100
Subject: [Plearn-commits] r9818 - trunk/plearn_learners/meta
Message-ID: <200901092024.n09KOvlS025758@sheep.berlios.de>

Author: nouiz
Date: 2009-01-09 21:24:56 +0100 (Fri, 09 Jan 2009)
New Revision: 9818

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
first implementation of MultiClassAdaBoost::test that forward the call to the sublearner to use some futur optimisation.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-09 19:56:57 UTC (rev 9817)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-09 20:24:56 UTC (rev 9818)
@@ -62,7 +62,8 @@
     total_train_time(0),
     test_time(0),
     total_test_time(0),
-    forward_sub_learner_test_costs(false)
+    forward_sub_learner_test_costs(false),
+    forward_test(false)
 /* ### Initialize all fields to their default value here */
 {
     // ...
@@ -108,6 +109,10 @@
                   &MultiClassAdaBoost::learner_template,
                   OptionBase::buildoption,
                   "The template to use for learner1 and learner2.\n");
+    declareOption(ol, "forward_test", 
+                  &MultiClassAdaBoost::forward_test,
+                  OptionBase::buildoption,
+                  "Did we add forward the test fct to the sub learner.\n");
 
     declareOption(ol, "train_time",
                   &MultiClassAdaBoost::train_time, OptionBase::learntoption,
@@ -139,6 +144,8 @@
         if(!learner2)
             learner2 = ::PLearn::deepCopy(learner_template);
     }
+    tmp_target.resize(1);
+    tmp_output.resize(outputsize());
     if(learner1)
         output1.resize(learner1->outputsize());
     if(learner2)
@@ -171,6 +178,10 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
+    deepCopyField(tmp_input,             copies);
+    deepCopyField(tmp_target,            copies);
+    deepCopyField(tmp_output,            copies);
+    deepCopyField(tmp_costs,             copies);
     deepCopyField(output1,           copies);
     deepCopyField(output2,           copies);
     deepCopyField(subcosts1,         copies);
@@ -249,7 +260,6 @@
     learner1->train();
     learner2->train();
 #endif
-
     stage=max(learner1->stage,learner2->stage);
 
     train_stats->stats.resize(0);
@@ -272,7 +282,7 @@
     const Profiler::Stats& stats_test = Profiler::getStats("MultiClassAdaBoost::test");
     tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
     test_time=tmp-total_test_time;
-    total_test_time=tmp; 
+    total_test_time=tmp;
     EXTREME_MODULE_LOG<<"train() end"<<endl;
 }
 
@@ -310,6 +320,7 @@
     output[1]=output1[0];
     output[2]=output2[0];
 }
+
 void MultiClassAdaBoost::computeOutputAndCosts(const Vec& input,
                                                const Vec& target,
                                                Vec& output, Vec& costs) const
@@ -385,6 +396,15 @@
 void MultiClassAdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
+  subcosts1.resize(0);
+  subcosts2.resize(0);
+  computeCostsFromOutputs_(input, output, target, subcosts1, subcosts2, costs);
+}
+
+void MultiClassAdaBoost::computeCostsFromOutputs_(const Vec& input, const Vec& output,
+						  const Vec& target, Vec& sub_costs1,
+						  Vec& sub_costs2, Vec& costs) const
+{
     PLASSERT(costs.size()==nTestCosts());
 
     int out = int(round(output[0]));
@@ -406,20 +426,24 @@
     costs[8]=total_train_time;
     costs[9]=test_time;
     costs[10]=total_test_time;
-
     if(forward_sub_learner_test_costs){
         costs.resize(7+4);
-        subcosts1.resize(learner1->nTestCosts());
-        subcosts2.resize(learner1->nTestCosts());
+	PLASSERT(sub_costs1.size()==learner1->nTestCosts() || sub_costs1.size()==0);
+	PLASSERT(sub_costs2.size()==learner2->nTestCosts() || sub_costs2.size()==0);
+
         getSubLearnerTarget(target, sub_target_tmp);
-
-//not paralized as this to add more overhead then the time saved.
-//meaby not true for all weak_learner.
-        learner1->computeCostsOnly(input,sub_target_tmp[0],subcosts1);
-        learner2->computeCostsOnly(input,sub_target_tmp[1],subcosts2);
-
-        subcosts1+=subcosts2;
-        costs.append(subcosts1);
+	if(sub_costs1.size()==0){
+            PLASSERT(input.size()>0);
+            sub_costs1.resize(learner1->nTestCosts());
+            learner1->computeCostsOnly(input,sub_target_tmp[0],sub_costs1);
+	}
+	if(sub_costs2.size()==0){
+            PLASSERT(input.size()>0);
+            sub_costs2.resize(learner2->nTestCosts());
+            learner2->computeCostsOnly(input,sub_target_tmp[1],sub_costs2);
+	}
+        sub_costs1+=sub_costs2;
+        costs.append(sub_costs1);
     }
 
     PLASSERT(costs.size()==nTestCosts());
@@ -516,7 +540,7 @@
         weight_prg = "[%"+tostring(index)+"]";
     }else
         weight_prg = "1 :weights";
-        
+    
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
     if(training_set_has_changed || !learner1->getTrainingSet()){
@@ -527,14 +551,14 @@
     if(training_set_has_changed || !learner2->getTrainingSet()){
         VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
                                            target_prg2,  weight_prg);
-        PP<RegressionTreeRegisters> t1 = 
+        PP<RegressionTreeRegisters> t1 =
             (PP<RegressionTreeRegisters>)learner1->getTrainingSet();
         if(t1->classname()=="RegressionTreeRegisters"){
             vmat2 = new RegressionTreeRegisters(vmat2,
-                                               t1->getTSortedRow(), 
-                                               t1->getTSource(),
-                                               learner1->report_progress,
-                                               learner1->verbosity);
+                                                t1->getTSortedRow(),
+                                                t1->getTSource(),
+                                                learner1->report_progress,
+                                                learner1->verbosity);
         }
         learner2->setTrainingSet(vmat2, call_forget);
     }
@@ -551,12 +575,147 @@
                               VMat testoutputs, VMat testcosts) const
 {
     Profiler::pl_profile_start("MultiClassAdaBoost::test");
+    if(!forward_test){
+        inherited::test(testset,test_stats,testoutputs,testcosts);
+        Profiler::end("MultiClassAdaBoost::test");
+	return;
+    }
+    Profiler::pl_profile_start("MultiClassAdaBoost::test() part1");
+    int index=-1;
+    for(int i=0;i<saved_testset.length();i++){
+        if(saved_testset[i]==testset){
+            index=i;break;
+        }
+    }
+    PP<VecStatsCollector> test_stats1 = 0;
+    PP<VecStatsCollector> test_stats2 = 0;
+    VMat testoutputs1 = VMat(new MemoryVMatrix(testset->length(),
+                                               learner1->outputsize()));
+    VMat testoutputs2 = VMat(new MemoryVMatrix(testset->length(),
+                                               learner2->outputsize()));
+    VMat testcosts1 = 0;
+    VMat testcosts2 = 0;
+    VMat testset1 = 0;
+    VMat testset2 = 0;
+    if ((testcosts || test_stats )){
+        //comment
+        testcosts1 = VMat(new MemoryVMatrix(testset->length(),
+                                            learner1->nTestCosts()));
+        testcosts2 = VMat(new MemoryVMatrix(testset->length(),
+                                            learner2->nTestCosts()));
+    }
+    if(index<0){
+        testset1 = new ProcessingVMatrix(testset, input_prg,
+                                         target_prg1,  weight_prg);
+        testset2 = new ProcessingVMatrix(testset, input_prg,
+                                         target_prg2,  weight_prg);
+        saved_testset.append(testset);
+        saved_testset1.append(testset1);
+        saved_testset2.append(testset2);
+    }else{
+        //we need to do that as AdaBoost need 
+        //the same dataset to reuse their test results
+        testset1=saved_testset1[index];
+        testset2=saved_testset2[index];
+        PLCHECK(((PP<ProcessingVMatrix>)testset1)->source==testset);
+        PLCHECK(((PP<ProcessingVMatrix>)testset2)->source==testset);
+    }
 
-    //we need this in case we reload the learner without training it.
-    subcosts1.resize(learner1->nTestCosts());
-    subcosts2.resize(learner2->nTestCosts());
+    if (test_stats){
+        
+    }
+    Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");
+    Profiler::start("MultiClassAdaBoost::subtest");
+    learner1->test(testset1,test_stats1,testoutputs1,testcosts1);
+    learner2->test(testset2,test_stats2,testoutputs2,testcosts2);
+    Profiler::end("MultiClassAdaBoost::subtest");
 
-    inherited::test(testset,test_stats,testoutputs,testcosts);
+    VMat my_outputs = 0;
+    VMat my_costs = 0;
+    if(testoutputs){
+        my_outputs=testoutputs;
+    }else if(bool(testcosts) | bool(test_stats)){
+        my_outputs=VMat(new MemoryVMatrix(testset->length(),
+                                          outputsize()));
+    }
+    if(testcosts){
+        my_costs=testcosts;
+    }else if(test_stats){
+        my_costs=VMat(new MemoryVMatrix(testset->length(),
+					nTestCosts()));
+    }
+    Profiler::pl_profile_start("MultiClassAdaBoost::test() my_outputs");
+    if(my_outputs){
+        for(int row=0;row<testset.length();row++){
+            real out1=testoutputs1->get(row,0);
+            real out2=testoutputs2->get(row,0);
+            int ind1=int(round(out1));
+            int ind2=int(round(out2));
+            int ind=-1;
+            if(ind1==0 && ind2==0)
+                ind=0;
+            else if(ind1==1 && ind2==0)
+                ind=1;
+            else if(ind1==1 && ind2==1)
+                ind=2;
+            else
+                ind=1;//TODOself.confusion_target;
+            tmp_output[0]=ind;
+            tmp_output[1]=out1;
+            tmp_output[2]=out2;
+	    my_outputs->putOrAppendRow(row,tmp_output);
+	}
+    }
+    Profiler::pl_profile_end("MultiClassAdaBoost::test() my_outputs");
+    Profiler::pl_profile_start("MultiClassAdaBoost::test() my_costs");
+
+    if (my_costs){
+        tmp_costs.resize(nTestCosts());
+//        if (forward_sub_learner_test_costs)
+	    //TODO optimize by reusing testoutputs1 and testoutputs2
+            //            PLWARNING("will be long");
+	int target_index = testset->inputsize();
+	PLASSERT(testset->targetsize()==1);
+        Vec costs1(learner1->nTestCosts());
+        Vec costs2(learner1->nTestCosts());
+        for(int row=0;row<testset.length();row++){
+            //default version
+            //testset.getExample(row, input, target, weight);
+	    //computeCostsFromOutputs(input,my_outputs(row),target,costs);
+            
+            //the input is not needed for the cost of this class if the subcost are know.
+            testset->getSubRow(row,target_index,tmp_target);
+//	    Vec costs1=testcosts1(row);
+//	    Vec costs2=testcosts2(row);
+            testcosts1->getRow(row,costs1);
+            testcosts2->getRow(row,costs2);
+            //TODO??? tmp_input is empty!!!
+	    computeCostsFromOutputs_(tmp_input, my_outputs(row), tmp_target, costs1,
+                                     costs2, tmp_costs);
+	    my_costs->putOrAppendRow(row,tmp_costs);
+        }
+    }
+    Profiler::pl_profile_end("MultiClassAdaBoost::test() my_costs");
+    Profiler::pl_profile_start("MultiClassAdaBoost::test() test_stats");
+
+    if (test_stats){
+	if(testset->weightsize()==0){
+            for(int row=0;row<testset.length();row++){
+                Vec costs = my_costs(row);
+                test_stats->update(costs, 1);
+            }
+	}else{
+            int weight_index=inputsize()+targetsize();
+            Vec costs(my_costs.width());
+            for(int row=0;row<testset.length();row++){
+//                Vec costs = my_costs(row);
+                my_costs->getRow(row, costs);
+                test_stats->update(costs, testset->get(row, weight_index));
+            }
+	}
+    }
+    Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
+
     Profiler::pl_profile_end("MultiClassAdaBoost::test");
 }
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-01-09 19:56:57 UTC (rev 9817)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-01-09 20:24:56 UTC (rev 9818)
@@ -59,11 +59,20 @@
 {
     typedef PLearner inherited;
 
+    mutable Vec tmp_input;
+    mutable Vec tmp_target;
+    mutable Vec tmp_output;
+    mutable Vec tmp_costs;
+
     mutable Vec output1;
     mutable Vec output2;
     mutable Vec subcosts1;
     mutable Vec subcosts2;
 
+    mutable TVec<VMat> saved_testset;
+    mutable TVec<VMat> saved_testset1;
+    mutable TVec<VMat> saved_testset2;
+
     //! The time it took for the last execution of the train() function
     real train_time;
     //! The total time passed in training
@@ -82,6 +91,9 @@
     //! Did we add the learner1 and learner2 costs to our costs
     bool forward_sub_learner_test_costs;
 
+    //! Did we forward the test function to the sub learner?
+    bool forward_test;
+
     //! The learner1 and learner2 must be trained!
     PP<AdaBoost> learner1;
     PP<AdaBoost> learner2;
@@ -126,6 +138,9 @@
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
 
+    void computeCostsFromOutputs_(const Vec& input, const Vec& output,
+                                  const Vec& target, Vec& sub_costs1,
+                                  Vec& sub_costs2, Vec& costs) const;
     virtual TVec<string> getOutputNames() const;
 
     //! Returns the names of the costs computed by computeCostsFromOutpus (and



From plearner at mail.berlios.de  Sat Jan 10 18:47:07 2009
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 10 Jan 2009 18:47:07 +0100
Subject: [Plearn-commits] r9819 - trunk/python_modules/plearn/plotting
Message-ID: <200901101747.n0AHl7w9026014@sheep.berlios.de>

Author: plearner
Date: 2009-01-10 18:47:06 +0100 (Sat, 10 Jan 2009)
New Revision: 9819

Modified:
   trunk/python_modules/plearn/plotting/numpy_utils.py
Log:
added a couple of utility functions

Modified: trunk/python_modules/plearn/plotting/numpy_utils.py
===================================================================
--- trunk/python_modules/plearn/plotting/numpy_utils.py	2009-01-09 20:24:56 UTC (rev 9818)
+++ trunk/python_modules/plearn/plotting/numpy_utils.py	2009-01-10 17:47:06 UTC (rev 9819)
@@ -35,11 +35,109 @@
 # Author: Pascal Vincent
 
 # from array import *
+import numpy
+import MA
 import numpy.numarray as numarray
 from numpy.numarray import *
 
 threshold = 0
 
+def default_is_missing(x):
+    return x is None or x=='-' or (type(x) is str and x.strip()=='')
+    
+def to_numpy_float_array(values_list,
+                         missing_value = ValueError("Found value interpreted as missing value and no missing_value was specified"),
+                         mapping = ValueError("Could not convert value to float and no mapping was specified"),
+                         mapping_start = 1.0,
+                         is_missing = default_is_missing ):
+    """
+    Transforms a list of values into a numpy array of floats.
+    Values that are not floats are handled automatically, according to the following policies:    
+
+      * missing_value specifies what to do if we encounter a value that we consider a missing value:
+        If missing_value is a float, then its value is used
+        If missing_value is None, then we'll return an apporpriately masked array (MA.array)
+        If missing_value is not specified as a float nor None, it will get raised as an exception.        
+      Note that a value x is considered a missing value if it satisfies is_missing(x)
+      (defaults to None or blank string or single dash '-').
+      
+      * If the value x is not missing, an attempt will be made to convert it to float using float(x).      
+        This has the effect of converting strings representing float to that float,
+        and of converting a bool to 1. or 0.
+
+      * If the value is the string 'True' or 'False' it will similarly be changed to 1. or 0.      
+
+      * If all the above fails, mapping can be used to specify how to automatically handle the case:
+          If mapping is a float, then its value will be used.
+          If mapping is a dictionary, then it will be looked up to find the corresponding value to use
+          and if it is not found, then new corresponding mapping will automatically be added
+          starting at mapping_start (which will get incremented ny 1).
+          If you don't want automatical adding of mappings you can specify mapping_start = None
+          (in this case an exception will be raised if not present in the mapping)
+          If mapping is not specified as a float or a dict it will get raised as an exception.B 
+      """
+    if type(missing_value) is int:
+        missing_value = float(missing_value)
+    if type(mapping) is int:
+        mapping = float(mapping)
+
+    vec = []
+    missing_pos = []
+    for i in xrange(len(values_list)):
+        val = values_list[i]
+        if is_missing(val):
+            if type(missing_value) is float:
+                val = missing_value
+            elif missing_value is None:
+                val = 0.
+                missing_pos.append(1)
+            else:
+                raise missing_value
+        else:
+            try:
+                val = float(val)
+            except ValueError:
+                if val=='True':
+                    val = 1.
+                elif val=='False':
+                    val = 0.
+                elif type(mapping) is float:
+                    val = mapping
+                elif type(mapping) is dict:
+                    if val in mapping:
+                        val = mapping[val]
+                    elif mapping_start is None:
+                        raise ValueError("At position "+str(i)+" value "+str(val)+" not present in specified mapping.")
+                    else:
+                        mapping[val] = mapping_start
+                        val = mapping_start
+                        mapping_start += 1.
+                else: # mapping is neither float nor dict:
+                    raise mapping
+        # Now we have a val that should be a valid float
+        vec.append(val)
+            
+    # now return a proper numpy.array or MA.array (if we wanted masking).
+    if len(missing_pos)==0: # return numpy.array
+        return numpy.array(vec)
+    else: # return masked array
+        n = len(vec)
+        mask = [0]*n
+        for pos in missing_pos:
+            mask[pos] = 1
+        return MA.array(vec, mask=mask)
+
+def mapping_of_values_to_pos(array_or_list):
+    """Returns a dictionary mapping values present in array_or_list to a vector of their positions in the array_or_list."""
+    d = {}    
+    for i in xrange(len(array_or_list)):
+        pos = d.setdefault(array_or_list[i], [])
+        pos.append(i)
+    # convert the positions lists into numpy arrays
+    for key in d:
+        d[key] = numpy.array(d[key])
+    return d
+
 def margin(scorevec):
     if len(scorevec)==1:
         return abs(scorevec[0]-threshold)



From nouiz at mail.berlios.de  Mon Jan 12 16:09:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 12 Jan 2009 16:09:12 +0100
Subject: [Plearn-commits] r9820 - trunk/plearn_learners/meta
Message-ID: <200901121509.n0CF9CaJ030606@sheep.berlios.de>

Author: nouiz
Date: 2009-01-12 16:09:12 +0100 (Mon, 12 Jan 2009)
New Revision: 9820

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
small fix.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-10 17:47:06 UTC (rev 9819)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-12 15:09:12 UTC (rev 9820)
@@ -574,7 +574,7 @@
 void MultiClassAdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,
                               VMat testoutputs, VMat testcosts) const
 {
-    Profiler::pl_profile_start("MultiClassAdaBoost::test");
+    Profiler::start("MultiClassAdaBoost::test");
     if(!forward_test){
         inherited::test(testset,test_stats,testoutputs,testcosts);
         Profiler::end("MultiClassAdaBoost::test");
@@ -716,7 +716,7 @@
     }
     Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
 
-    Profiler::pl_profile_end("MultiClassAdaBoost::test");
+    Profiler::end("MultiClassAdaBoost::test");
 }
 
 } // end of namespace PLearn



From tihocan at mail.berlios.de  Mon Jan 12 21:01:20 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 12 Jan 2009 21:01:20 +0100
Subject: [Plearn-commits] r9821 -
	trunk/plearn_learners/classifiers/EXPERIMENTAL
Message-ID: <200901122001.n0CK1KGn023542@sheep.berlios.de>

Author: tihocan
Date: 2009-01-12 21:01:20 +0100 (Mon, 12 Jan 2009)
New Revision: 9821

Modified:
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
Log:
Forwarding report_progress option to underlying logistic classifiers

Modified: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2009-01-12 15:09:12 UTC (rev 9820)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2009-01-12 20:01:20 UTC (rev 9821)
@@ -214,6 +214,7 @@
         PP<NNet> nnet = new NNet();
         nnet->optimizer = opt;
         nnet->seed_ = this->seed_;
+        nnet->report_progress = this->report_progress;
         if (n_classes == 2) {
             cost_func = "stable_cross_entropy";
             nnet->output_transfer_func = "sigmoid";
@@ -263,6 +264,7 @@
         hyper->save_final_learner = false;
         hyper->strategy = TVec< PP<HyperCommand> >(1, get_pointer(strategy));
         hyper->tester = htester;
+        hyper->verbosity = 0; // Get rid of useless output.
         hyper->build();
         // Perform training.
         hyper->train();



From nouiz at mail.berlios.de  Mon Jan 12 21:21:43 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 12 Jan 2009 21:21:43 +0100
Subject: [Plearn-commits] r9822 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200901122021.n0CKLhRJ025180@sheep.berlios.de>

Author: nouiz
Date: 2009-01-12 21:21:43 +0100 (Mon, 12 Jan 2009)
New Revision: 9822

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
make that --machine and --machines add to a pool of machine. We can use any machine in that pool.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-12 20:01:20 UTC (rev 9821)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-12 20:21:43 UTC (rev 9822)
@@ -729,6 +729,8 @@
         self.pkdilly = False
         self.launch_file = None
         self.universe = "vanilla"
+        self.machine = []
+        self.machines = []
 
         DBIBase.__init__(self, commands, **args)
 
@@ -1283,7 +1285,16 @@
             self.req=reduce(lambda x,y:x+' || (OpSys == "'+str(y)+'")',
                             self.os.split(','),
                             self.req+'&&(False ')+")"
-
+        machine_choice=[]
+        for m in self.machine:
+            machine_choice.append('(Machine=="'+m+'")')
+        for m in self.machines:
+            machine_choice.append('(regexp("'+m+'", target.Machine))')
+        if machine_choice:
+            self.req+="&&(False "
+            for m in machine_choice:
+                self.req+="||"+m
+            self.req+=")"
         #if no mem requirement added, use the executable size.
         #todo: if they are not the same executable, take the biggest
         if self.mem<=0:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-12 20:01:20 UTC (rev 9821)
+++ trunk/scripts/dbidispatch	2009-01-12 20:21:43 UTC (rev 9822)
@@ -4,7 +4,7 @@
 from socket import gethostname
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>}
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--machine=HOSTNAME] [--machines=regex] <back-end parameter> {--file=FILEPATH | <command-template>}
 
 <back-end parameter>:
     bqtools, cluster option  :[--duree=X]
@@ -22,6 +22,7 @@
                               [*--[no_]abs_path] [--[*no_]pkdilly]
                               [*--[no_]set_special_env]
                               [--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}]
+                              [--machine=HOSTNAME] [--machines=regex]
     cluster option           :[*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible]
 An * after '[', '{' or ',' signals the default value.
@@ -140,10 +141,12 @@
     first. This is equivalent to dbidispatch '--rank=SERVER=?=True'.
   The '--rank=STRING' option add rank=STRING in the submit file.
   The '--machine=full_host_name' option add the requirement that the executing
-     host is full_host_name. Is equivalent to
+     host is full_host_name. If multiple --machine or --machines options,
+    take anyone of them. Is equivalent to
      dbidispatch '--req=Machine=="full_host_name"'
   The '--machines=regexp' option add the requirement that the executing host 
-    name must be match the regexp
+    name must be match the regexp. If multiple --machine or --machines options,
+    take anyone of them.
      dbidispatch '--machines=computer00*'
         witch is equivalent to
      dbidispatch '--req=regexp("computer0*", target.Machine)'
@@ -293,28 +296,26 @@
         testmode=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
-                                "--universe", "--exp_dir"]:
-        param=argv.split('=')[0][2:]
+                                "--universe", "--exp_dir", "--machine", "--machines"]:
+        sp = argv.split('=',1)
+        param=sp[0][2:]
+        val = sp[1]
         if param in ["req", "files", "rank"]:
             #param that we happend to if defined more then one time
             dbi_param.setdefault(param,'True')
-            dbi_param[param]+='&&('+argv.split('=',1)[1]+')'
+            dbi_param[param]+='&&('+val+')'
         elif param == "raw":
             dbi_param.setdefault(param,'')
-            dbi_param[param]+='\n'+argv.split('=',1)[1]
+            dbi_param[param]+='\n'+val
         elif param=="env":
             dbi_param.setdefault(param,"")
-            dbi_param[param]+='"'+argv.split('=',1)[1]+'"'
+            dbi_param[param]+='"'+val+'"'
+        elif param=="machines" or param=="machine":
+            dbi_param.setdefault(param,[])
+            dbi_param[param]+=val.split(",")
         else:
             #otherwise we erase the old value
-            dbi_param[param]=argv.split('=',1)[1]
-    elif argv.startswith('--machine=') or argv.startswith('--machines='):
-        if argv.split('=')[0] == "--machine":
-            new='&&(Machine=="'+argv.split('=')[1]+'")'
-        elif argv.split('=')[0] == "--machines":
-            new='&&(regexp("'+argv.split('=')[1]+'", target.Machine))'
-        dbi_param.setdefault('req','True')
-        dbi_param["req"]+=new
+            dbi_param[param]=val
     elif argv=="--server" or argv=="--no_server" \
             or argv=='--prefserver' or argv=="--no_prefserver":
         if argv.find('prefserver')!=-1:
@@ -347,7 +348,7 @@
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
-                       "universe"]
+                       "universe", "machine", "machines"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long", "duree"]
 



From nouiz at mail.berlios.de  Wed Jan 14 15:28:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 15:28:29 +0100
Subject: [Plearn-commits] r9823 - trunk/python_modules/plearn/pymake
Message-ID: <200901141428.n0EESTh0004136@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 15:28:29 +0100 (Wed, 14 Jan 2009)
New Revision: 9823

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
print some stuff at specific verbosity level to make pymake less verbose.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-01-12 20:21:43 UTC (rev 9822)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 14:28:29 UTC (rev 9823)
@@ -2954,12 +2954,14 @@
             pyopt = pymake_options_defs[opt]
             if pyopt.in_output_dirname:
                 objsdir = objsdir + '_' + opt
-
-        print '*** Running pymake on '+os.path.basename(target)+' using configuration file: ' + configpath
-        print '*** Running pymake on '+os.path.basename(target)+' using options: ' + string.join(map(lambda o: '-'+o if o else '', options))
-        print '++++ Computing dependencies of '+target
+        if verbose>2:
+            print '*** Running pymake on '+os.path.basename(target)+' using configuration file: ' + configpath
+        if verbose>1:
+            print '*** Running pymake on '+os.path.basename(target)+' using options: ' + string.join(map(lambda o: '-'+o if o else '', options))
+            print '++++ Computing dependencies of '+target
         get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, ccfiles_to_link, executables_to_link, linkname)
-        print '++++ Dependencies computed'
+        if verbose>1:
+            print '++++ Dependencies computed'
 
         if distribute:
             # We dont want to compile. We will extract the necessary file to compile



From nouiz at mail.berlios.de  Wed Jan 14 15:38:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 15:38:26 +0100
Subject: [Plearn-commits] r9824 - trunk/scripts
Message-ID: <200901141438.n0EEcQIj004666@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 15:38:26 +0100 (Wed, 14 Jan 2009)
New Revision: 9824

Modified:
   trunk/scripts/multipymake
Log:
now return the same exit code as pymake. If pymake fail, finish right away event if their is more to compile as they should also fail.


Modified: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2009-01-14 14:28:29 UTC (rev 9823)
+++ trunk/scripts/multipymake	2009-01-14 14:38:26 UTC (rev 9824)
@@ -67,8 +67,14 @@
   iname=${i//\ /_}
   echo -e "\nCompiling \"${BASEPROG}${iname}.cc\" with command:"
   echo " pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc"
-  pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc || ( echo "Build failed for $i"; exit)
-  echo -e "Ended with status: $?\n"
+  pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc
+  status=$?
+  if [ "$status" != "0" ]; then
+      last_status=$status
+      echo "Build failed for $i"
+      exit $status
+  fi
+#  echo -e "Ended with status: $status\n"
 done
 
 echo "Time finished:" `date`



From nouiz at mail.berlios.de  Wed Jan 14 15:43:05 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 15:43:05 +0100
Subject: [Plearn-commits] r9825 - trunk/python_modules/plearn/pymake
Message-ID: <200901141443.n0EEh5e1004858@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 15:43:05 +0100 (Wed, 14 Jan 2009)
New Revision: 9825

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
print stuff only if needed.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 14:38:26 UTC (rev 9824)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 14:43:05 UTC (rev 9825)
@@ -2981,9 +2981,10 @@
                 print "Files to compile: "
                 for i in ccfiles_to_compile:
                     print i.filebase
-            print '++++ Compiling',
-            print str(len(ccfiles_to_compile))+'/'+str(len(ccfiles_to_link))
-            print 'files...'
+            if len(ccfiles_to_compile)>0:
+                print '++++ Compiling',
+                print str(len(ccfiles_to_compile))+'/'+str(len(ccfiles_to_link)),
+                print 'files...'
 
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())



From larocheh at mail.berlios.de  Wed Jan 14 16:06:12 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 14 Jan 2009 16:06:12 +0100
Subject: [Plearn-commits] r9826 - trunk/plearn/vmat
Message-ID: <200901141506.n0EF6CAF006510@sheep.berlios.de>

Author: larocheh
Date: 2009-01-14 16:06:10 +0100 (Wed, 14 Jan 2009)
New Revision: 9826

Modified:
   trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
Log:
Corrected a memory leak...


Modified: trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
===================================================================
--- trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-14 14:43:05 UTC (rev 9825)
+++ trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-14 15:06:10 UTC (rev 9826)
@@ -107,6 +107,8 @@
         *vj = *xj/input_divisor;
     
     v.last() = InfiniteMNISTVMatrix::dataset->y[ (i_dataset<10000) ? i_dataset : 10000 + ((i_dataset - 10000) % 60000) ];
+
+    free(image);
 }
 
 void InfiniteMNISTVMatrix::declareOptions(OptionList& ol)



From nouiz at mail.berlios.de  Wed Jan 14 16:14:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 16:14:29 +0100
Subject: [Plearn-commits] r9827 - trunk/scripts
Message-ID: <200901141514.n0EFETiK007334@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 16:14:29 +0100 (Wed, 14 Jan 2009)
New Revision: 9827

Modified:
   trunk/scripts/multipymake
Log:
removed useless line.


Modified: trunk/scripts/multipymake
===================================================================
--- trunk/scripts/multipymake	2009-01-14 15:06:10 UTC (rev 9826)
+++ trunk/scripts/multipymake	2009-01-14 15:14:29 UTC (rev 9827)
@@ -70,7 +70,6 @@
   pymake $i ${PYMAKEFLAGS[@]} ${BASEPROG}${iname}.cc
   status=$?
   if [ "$status" != "0" ]; then
-      last_status=$status
       echo "Build failed for $i"
       exit $status
   fi



From nouiz at mail.berlios.de  Wed Jan 14 18:14:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 18:14:32 +0100
Subject: [Plearn-commits] r9828 - trunk/python_modules/plearn/pymake
Message-ID: <200901141714.n0EHEWvD018143@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 18:14:31 +0100 (Wed, 14 Jan 2009)
New Revision: 9828

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
now print the number of code and headers file that was modified. This way we clearly see the impact size of modifying a header file...


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 15:14:29 UTC (rev 9827)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 17:14:31 UTC (rev 9828)
@@ -916,7 +916,7 @@
 
 def get_ccfiles_to_compile_and_link(target, ccfiles_to_compile, ccfiles_to_link, executables_to_link,linkname):
     """A target can be a .cc file, a binary target, or a directory.
-    The function updates (by appending to them) ccfiles_to_compile and ofiles_to_link
+    The function updates (by appending to them) ccfiles_to_compile and ccfiles_to_link
     ccfiles_to_compile is a dictionary containing FileInfo of all .cc files to compile
     ccfiles_to_link is a dictionary containing FileInfo of all .cc files on
     which target depends
@@ -1715,7 +1715,7 @@
     def analyseFile(self):
         global useqt
 
-        self.mtime = os.path.getmtime(self.filepath)
+        self.mtime = self.get_mtime()
         self.filedir, fname = os.path.split(self.filepath)
         self.filebase, self.fileext = os.path.splitext(fname)
 
@@ -1907,13 +1907,17 @@
                 linkeroptions = linkeroptions + '-L'+qtdir + 'lib/ -lqt-mt'
         return linkeroptions
 
+    def get_mtime(self):
+        if not hasattr(self,"mtime"):
+            self.mtime = os.path.getmtime(self.filepath)
+        return self.mtime
 
     def get_depmtime(self):
         "returns the single latest last modification time of"
         "this file and all its .h dependencies through includes"
         if not hasattr(self,"depmtime"):
             # YB DEBUGGING
-            self.mtime = os.path.getmtime(self.filepath)
+            self.mtime = self.get_mtime()
             #
             self.depmtime = self.mtime
             for includefile in self.includes_from_sourcedirs:
@@ -1923,7 +1927,34 @@
                     #print "time of ",includefile.filepath," = ",depmtime
         return self.depmtime
 
-
+    def file_is_modified(self):
+        """return false if the corresponding .o file is up to date,
+        without taking dependencies into accout"""
+        try:
+            if self.fileext in cpp_exts:
+                if not os.path.exists(self.corresponding_ofile):
+                    return False
+                else:
+                    t1 = self.get_mtime()
+                    t2 = os.path.getmtime(self.corresponding_ofile)
+                    return t1 > t2
+            elif self.fileext in h_exts:
+                f=self.corresponding_ccfile
+                if not f or not os.path.exists(f.filepath):
+                    return False
+                else:
+                    t1 = self.get_mtime()
+                    t2 = os.path.getmtime(f.corresponding_ofile)
+#                    print self.filename(), f.filename(), t1,t2,os.path.getmtime(self.filepath), t1<=t2, t1>t2
+                    return t1 > t2
+                pass
+            else:
+                print self.filepath, "not handled here."
+                return False
+        except OSError: # OSError: [Errno 116] Stale NFS file handle
+            print "OSError... (probably NFS latency); Will be retrying"
+            return False
+        
     def corresponding_ofile_is_up_to_date(self):
         """returns true if the corresponding .o file is up to date,
         false if it needs recompiling."""
@@ -2982,9 +3013,13 @@
                 for i in ccfiles_to_compile:
                     print i.filebase
             if len(ccfiles_to_compile)>0:
+                ccf=reduce(lambda x,y: x+y.file_is_modified(),ccfiles_to_compile,0)
+                l=[x for x in file_info_map.values() if x.fileext in h_exts]
+                hf=reduce(lambda x,y: x+y.file_is_modified(),l,0)
                 print '++++ Compiling',
                 print str(len(ccfiles_to_compile))+'/'+str(len(ccfiles_to_link)),
-                print 'files...'
+                print 'files. '+str(ccf)+' code and '+str(hf),
+                print 'headers file were modified.'
 
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())



From nouiz at mail.berlios.de  Wed Jan 14 19:59:28 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 19:59:28 +0100
Subject: [Plearn-commits] r9829 - trunk/python_modules/plearn/pymake
Message-ID: <200901141859.n0EIxSNl012122@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 19:59:27 +0100 (Wed, 14 Jan 2009)
New Revision: 9829

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
-don't print the version if verbose==1. Moved it earlier to make it the first stuff to print.
-also don't print the current platform if verbose==1


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 17:14:31 UTC (rev 9828)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-01-14 18:59:27 UTC (rev 9829)
@@ -50,6 +50,9 @@
 #####  Helper function definitions  ###########################################
 # (plus some global variables)
 
+def printversion():
+    print "pymake 2.0 [ (C) 2001, Pascal Vincent. This is free software distributed under a BSD type license. Report problems to vincentp at iro.umontreal.ca ]"
+
 ###  Usage
 
 def printshortusage():
@@ -2546,7 +2549,6 @@
 #####  Main function  #########################################################
 
 def main( args ):
-    print "pymake 2.0 [ (C) 2001, Pascal Vincent. This is free software distributed under a BSD type license. Report problems to vincentp at iro.umontreal.ca ]"
 
     ######## Initialization of variables
     #
@@ -2588,9 +2590,6 @@
     homedir = get_homedir()
     myhostname = get_hostname()
 
-    print '*** Current platform is: ' + platform
-    print
-
     # Filetype extensions
     cpp_exts = ['.cc','.c','.C','.cpp','.CC', '.cxx']
     h_exts = ['.h','.H','.hpp']
@@ -2676,7 +2675,30 @@
     
 
     ####  Checking optionargs to know which task to perform
+    # I do multiple for on optionarfs, as their is a bug that make that not all
+    # elements of optionargs are computed
+    for option in optionargs:
+        if option[0] == 'v':
+            remove_verbosity_option = True
+            if option == 'v' or option == 'v1' or option == 'v0':
+                verbose = 1
+            elif option == 'v'*2 or option == 'v2':
+                verbose = 2
+            elif option == 'v'*3 or option == 'v3':
+                verbose = 3
+            elif option[:4] == 'v'*4 or option == 'v4':
+                verbose = 4
+            else:
+                # Not a verbosity option, just an option starting with a 'v'.
+                remove_verbosity_option = False
+            if remove_verbosity_option:
+                optionargs.remove(option)
+    if verbose > 1:
+        printversion()
+        print '*** Current platform is: ' + platform
+        print
 
+
     ##  Options specifying the type of compiled file to produce
     # do we want to create a dll instead of an executable file
     if 'dll' in optionargs:
@@ -2832,25 +2854,6 @@
                 optionargs.remove(option)
                 optionargs.append('tmp')
 
-    # I do multiple for on optionarfs, as their is a bug that make that not all
-    # elements of optionargs are computed
-    for option in optionargs:
-        if option[0] == 'v':
-            remove_verbosity_option = True
-            if option == 'v' or option == 'v1' or option == 'v0':
-                verbose = 1
-            elif option == 'v'*2 or option == 'v2':
-                verbose = 2
-            elif option == 'v'*3 or option == 'v3':
-                verbose = 3
-            elif option[:4] == 'v'*4 or option == 'v4':
-                verbose = 4
-            else:
-                # Not a verbosity option, just an option starting with a 'v'.
-                remove_verbosity_option = False
-            if remove_verbosity_option:
-                optionargs.remove(option)
-
     if 'tmp' in optionargs:
         objspolicy = 2
         temp_objs=1
@@ -2917,6 +2920,7 @@
         distribute = 0
 
     if 'help' in optionargs:
+        printversion()
         printusage()
         sys.exit()
 



From nouiz at mail.berlios.de  Wed Jan 14 20:00:28 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 20:00:28 +0100
Subject: [Plearn-commits] r9830 - trunk/plearn_learners/hyper
Message-ID: <200901141900.n0EJ0SHt012491@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 20:00:27 +0100 (Wed, 14 Jan 2009)
New Revision: 9830

Modified:
   trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
Log:
print the reason of the stopping in the debug module.


Modified: trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2009-01-14 18:59:27 UTC (rev 9829)
+++ trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2009-01-14 19:00:27 UTC (rev 9830)
@@ -42,7 +42,9 @@
 /*! \file EarlyStoppingOracle.cc */
 #include "EarlyStoppingOracle.h"
 #include <plearn/base/stringutils.h>
-
+#define PL_LOG_MODULE_NAME "EarlyStoppingOracle"
+#include <plearn/io/pl_log.h>
+#
 namespace PLearn {
 using namespace std;
 
@@ -190,9 +192,60 @@
               (relative_max_degradation>=0 && degradation > relative_max_degradation * abs(best_objective)) ||
               (improvement < min_improvement) ||
               (relative_min_improvement>=0 && improvement < relative_min_improvement * abs(previous_objective))
-                ) && current_step >= min_n_steps)
+                ) && current_step >= min_n_steps){
             met_early_stopping = true;
 
+            //print debug info
+            if(current_objective < min_value){
+                DBG_MODULE_LOG 
+                    <<"stopping the learner as: current_objective "
+                    <<current_objective
+                    <<" < min_value "<<min_value
+                    <<endl;
+            }
+            if(current_objective > max_value){
+                DBG_MODULE_LOG
+                    <<"stopping the learner as: current_objective ("<<current_objective
+                    <<") < max_value ("<<max_value<<")"<<endl;
+            }
+            if(n_degraded_steps >= max_degraded_steps){
+                DBG_MODULE_LOG
+                    <<"stopping the learner as:n_degraded_steps("<<
+                    n_degraded_steps<<") >= max_degraded_steps("<<
+                    max_degraded_steps<<") "
+                    <<endl;
+            }
+            if(degradation > max_degradation){
+                DBG_MODULE_LOG
+                    <<"stopping the learner as: degradation("<<degradation
+                    <<") > max_degradation("<<max_degradation<<")"
+                    <<endl;
+            }
+            if(relative_max_degradation>=0 
+               && degradation > relative_max_degradation * abs(best_objective)){
+                DBG_MODULE_LOG
+                    <<"stopping the learner as: relative_max_degradation>=0 "
+                    <<"&& degradation("<<degradation
+                    <<") > relative_max_degradation("<<relative_max_degradation
+                    <<") * abs(best_objective)("<<best_objective<<")"
+                    <<endl;
+            }
+            if(improvement < min_improvement){
+                DBG_MODULE_LOG
+                    <<"stopping the learner as: improvement("<<improvement<<") < min_improvement("
+                    <<min_improvement<<")"
+                    <<endl;
+            }
+            if(relative_min_improvement>=0 
+               && improvement < relative_min_improvement * abs(previous_objective)){
+                DBG_MODULE_LOG
+                    <<"stopping the learner as: relative_min_improvement("<<relative_min_improvement<<")>=0 "
+                    <<endl
+                    <<"&& improvement("<<improvement<<") < relative_min_improvement("<<relative_min_improvement<<") * abs(previous_objective"<<previous_objective<<")"
+                    <<endl;
+            }
+        }
+
         previous_objective = current_objective;
     }
 



From nouiz at mail.berlios.de  Wed Jan 14 20:09:00 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 20:09:00 +0100
Subject: [Plearn-commits] r9831 - trunk
Message-ID: <200901141909.n0EJ90pq013453@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 20:08:59 +0100 (Wed, 14 Jan 2009)
New Revision: 9831

Modified:
   trunk/pymake.config.model
Log:
added the -pass=X option in the pymake.config.model. This will transparently pass X to the compiler pass, not the linker.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-01-14 19:00:27 UTC (rev 9830)
+++ trunk/pymake.config.model	2009-01-14 19:08:59 UTC (rev 9831)
@@ -284,8 +284,29 @@
   [ '', 'cygwin-fgets-bugfix'],
   [ '', 'openmpgcc'],
   
+  [ '', 'pass']
 ]
 
+optionargs_new=set()
+for option in optionargs:
+    if option.startswith('pass='):
+        option=option[5:]
+        optionargs_new.add(option)
+        pymakeOption( name = option,
+                      description = option,
+                      compileroptions = option)
+        options_choices+=[['',option]]
+    else:
+        optionargs_new.add(option)
+optionargs=list(optionargs_new)
+
+pymakeOption( name = 'pass',
+              description = 'The command line parameter -pass=X will make add the parameter X to the compiler command. Usefull to test new option without modifying the pymake.config.model file.',
+              )
+if "openmpgcc" in optionargs:
+    print "WARNING: plearn is not fully thread safe, use it at your own risk!"
+
+#optionargs.remove('myoption')
 ### Using Python code snippets in C++ code
 ### TODO Using /u/lisa/... is quite ugly. But the PLEARN_LIBDIR ppath may also
 ### be problematic (it should be different depending on the platform) => must



From nouiz at mail.berlios.de  Wed Jan 14 21:04:14 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 21:04:14 +0100
Subject: [Plearn-commits] r9832 - trunk/plearn/vmat
Message-ID: <200901142004.n0EK4EtH020876@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 21:04:12 +0100 (Wed, 14 Jan 2009)
New Revision: 9832

Modified:
   trunk/plearn/vmat/BinaryOpVMatrix.cc
   trunk/plearn/vmat/BinaryOpVMatrix.h
Log:
added a constructor.


Modified: trunk/plearn/vmat/BinaryOpVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryOpVMatrix.cc	2009-01-14 19:08:59 UTC (rev 9831)
+++ trunk/plearn/vmat/BinaryOpVMatrix.cc	2009-01-14 20:04:12 UTC (rev 9832)
@@ -51,6 +51,13 @@
 {
 }
 
+BinaryOpVMatrix::BinaryOpVMatrix(VMat source1, VMat source2, string op):
+    source1(source1),
+    source2(source2),
+    op(op)
+{
+    build();
+}
 PLEARN_IMPLEMENT_OBJECT(BinaryOpVMatrix,
                         "This VMat allows simple binary operations on two VMatrix.",
                         "It is assumed that the two source matrices are the same size"

Modified: trunk/plearn/vmat/BinaryOpVMatrix.h
===================================================================
--- trunk/plearn/vmat/BinaryOpVMatrix.h	2009-01-14 19:08:59 UTC (rev 9831)
+++ trunk/plearn/vmat/BinaryOpVMatrix.h	2009-01-14 20:04:12 UTC (rev 9832)
@@ -82,6 +82,7 @@
     //! Default constructor.
     BinaryOpVMatrix();
 
+    BinaryOpVMatrix(VMat source1, VMat source2, string op);
     // ******************
     // * Object methods *
     // ******************



From nouiz at mail.berlios.de  Wed Jan 14 21:13:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 21:13:45 +0100
Subject: [Plearn-commits] r9833 - trunk/commands/PLearnCommands
Message-ID: <200901142013.n0EKDjhl021705@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 21:13:44 +0100 (Wed, 14 Jan 2009)
New Revision: 9833

Modified:
   trunk/commands/PLearnCommands/VMatViewCommand.cc
Log:
added plearn vmat view [--add,--sub,--diff,--mult,--div] that to the operation between two matrix before printing it.


Modified: trunk/commands/PLearnCommands/VMatViewCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatViewCommand.cc	2009-01-14 20:04:12 UTC (rev 9832)
+++ trunk/commands/PLearnCommands/VMatViewCommand.cc	2009-01-14 20:13:44 UTC (rev 9833)
@@ -40,6 +40,7 @@
 #include "VMatViewCommand.h"
 #include <plearn/db/getDataSet.h>
 #include <plearn/misc/viewVMat.h>
+#include <plearn/vmat/BinaryOpVMatrix.h>
 
 namespace PLearn {
 using namespace std;
@@ -51,7 +52,7 @@
     : PLearnCommand(
         "vmat_view",
         "interactive display of a vmatrix (curses-based)",
-        "vmat view <vmat_specification> \n"
+        "vmat view [--add,--sub,--diff,--mult,--div] <vmat> ...\n"
         "will interactively display contents of the \n"
         "specified vmatrix (any recognized file format)\n"
         )
@@ -60,6 +61,24 @@
 //! The actual implementation of the 'VMatViewCommand' command
 void VMatViewCommand::run(const vector<string>& args)
 {
+    string op;
+    if(args[0]=="--add")
+        op="add";
+    else if(args[0]=="--sub" || args[0]=="--diff")
+        op="sub";
+    else if(args[0]=="--mult")
+        op="mult";
+    else if(args[0]=="--div")
+        op="div";
+    if(!op.empty()){
+        if(args.size()!=3)
+            PLERROR("Usage: vmat_view [--add,--sub,--diff,--mult,--div] <source1> ... \n"
+                    "If an option is used their must be two sources matrix.");
+        VMat vm = new BinaryOpVMatrix(getDataSet(args[1]),getDataSet(args[2]),op);
+        viewVMat(vm, op);
+        return;
+    }
+
     for(uint i=0;i<args.size();i++){
         PPath vmat_view_dataset(args[i]);
         if(args.size()>1)



From nouiz at mail.berlios.de  Wed Jan 14 21:33:48 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 21:33:48 +0100
Subject: [Plearn-commits] r9834 - trunk/plearn_learners/meta
Message-ID: <200901142033.n0EKXmlX023861@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 21:33:48 +0100 (Wed, 14 Jan 2009)
New Revision: 9834

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
added the AdaBoost::reuse_test_results option. If true, we will reuse the past partial test results. Default to false. Tested to give the same results as before, but I must verify that this take a constant time. But this constant can be higher then the default version for low number of weak_learners.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-14 20:13:44 UTC (rev 9833)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-14 20:33:48 UTC (rev 9834)
@@ -69,7 +69,8 @@
       early_stopping(1),
       save_often(0),
       forward_sub_learner_test_costs(false),
-      modif_train_set_weights(false)
+      modif_train_set_weights(false),
+      reuse_test_results(false)
 { }
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -218,7 +219,29 @@
                   OptionBase::nosave,
                   "A temp vector that contain the weak learner output\n");
 
-    // Now call the parent class' declareOptions
+    declareOption(ol, "reuse_test_results",
+                  &AdaBoost::reuse_test_results,
+                  OptionBase::buildoption,
+                  "If true we save and reuse previous call to test(). This is"
+                  " usefull to have a test time that is independent of the"
+                  " number of adaboost itaration.\n");
+
+     declareOption(ol, "saved_testset",
+                  &AdaBoost::saved_testset,
+                  OptionBase::nosave,
+                  "Used with reuse_test_results\n");
+
+     declareOption(ol, "saved_testoutputs",
+                  &AdaBoost::saved_testoutputs,
+                  OptionBase::nosave,
+                  "Used with reuse_test_results\n");
+
+     declareOption(ol, "saved_last_test_stages",
+                  &AdaBoost::saved_last_test_stages,
+                  OptionBase::nosave,
+                  "Used with reuse_test_results\n");
+
+   // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 
     declareOption(ol, "train_set",
@@ -263,9 +286,12 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(tmp_output2,              copies);
     deepCopyField(weighted_costs,           copies);
     deepCopyField(sum_weighted_costs,       copies);
+    deepCopyField(saved_testset,            copies);
+    deepCopyField(saved_testoutputs,        copies);
+    deepCopyField(saved_last_test_stages,   copies);
+
     deepCopyField(learners_error,           copies);
     deepCopyField(example_weights,          copies);
     deepCopyField(weak_learner_output,      copies);
@@ -279,8 +305,9 @@
 ////////////////
 int AdaBoost::outputsize() const
 {
-    // Outputsize is always 1, since this is a 0-1 classifier
-    return 1;
+    // Outputsize is always 2, since this is a 0-1 classifier
+    // and we append the weighted sum to allow the reuse of previous test
+    return 2;
 }
 
 void AdaBoost::finalize()
@@ -701,30 +728,103 @@
 
 
     }
+    PLCHECK(stage==weak_learners.length());
     Profiler::pl_profile_end("AdaBoost::train");
 
 }
 
+void AdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,
+                    VMat testoutputs, VMat testcosts) const
+{
+    if(!reuse_test_results){
+        inherited::test(testset, test_stats, testoutputs, testcosts);
+        return;
+    }
+    Profiler::pl_profile_start("AdaBoost::test()");
+    int index=-1;
+    for(int i=0;i<saved_testset.size();i++){
+        if(saved_testset[i]==testset){
+            index=i;
+            break;
+        }
+    }
+    if(index<0){
+        //first time the testset is seen
+        Profiler::pl_profile_start("AdaBoost::test() first" );
+        inherited::test(testset, test_stats, testoutputs, testcosts);
+        saved_testset.append(testset);
+        saved_testoutputs.append(PLearn::deepCopy(testoutputs));
+        PLCHECK(weak_learners.length()==stage);
+        saved_last_test_stages.append(stage);
+        Profiler::pl_profile_end("AdaBoost::test() first" );
+    }else{
+        Profiler::pl_profile_start("AdaBoost::test() seconds" );
+        PLCHECK(weak_learners.size()>1);
+        PLCHECK(stage>1);
+        PLCHECK(weak_learner_output.size()==weak_learner_template->outputsize());
 
-void AdaBoost::computeOutput(const Vec& input, Vec& output) const
+        PLCHECK(saved_testset.length()>index);
+        PLCHECK(saved_testoutputs.length()>index);
+        PLCHECK(saved_last_test_stages.length()>index);
+
+        int stages_done = saved_last_test_stages[index];
+        PLCHECK(weak_learners.size()>=stages_done);
+         
+        Vec input;
+        Vec output(outputsize());
+        Vec target;
+        Vec costs(nTestCosts());
+        real weight;
+        VMat old_outputs=saved_testoutputs[index];
+        PLCHECK(old_outputs->width()==testoutputs->width());
+        PLCHECK(old_outputs->length()==testset->length());
+#ifndef NDEBUG
+        Vec output2(outputsize());
+        Vec costs2(nTestCosts());
+#endif
+        for(int row=0;row<testset.length();row++){
+            output=old_outputs(row);
+            //compute the new testoutputs
+            testset.getExample(row, input, target, weight);
+            computeOutput_(input, output, stages_done, output[1]);
+            computeCostsFromOutputs(input,output,target,costs);
+#ifndef NDEBUG
+            computeOutputAndCosts(input,target, output2, costs2);
+            PLCHECK(output==output2);
+            PLCHECK(costs.isEqual(costs2,true));
+#endif
+            if(testoutputs)testoutputs->putOrAppendRow(row,output);
+            if(testcosts)testcosts->putOrAppendRow(row,costs);
+            if(test_stats)test_stats->update(costs,weight);
+        }
+        saved_testoutputs[index]=PLearn::deepCopy(testoutputs);
+        saved_last_test_stages[index]=stage;
+        Profiler::pl_profile_end("AdaBoost::test() seconds" );
+    }
+    Profiler::pl_profile_end("AdaBoost::test()");
+}
+
+void AdaBoost::computeOutput_(const Vec& input, Vec& output,
+                             int start, real sum) const
 {
     PLASSERT(weak_learners.size()>0);
     PLASSERT(weak_learner_output.size()==weak_learner_template->outputsize());
     PLASSERT(output.size()==outputsize());
-    real sum_out=0;
+    real sum_out=sum;
     if(!pseudo_loss_adaboost && !conf_rated_adaboost)
-        for (int i=0;i<weak_learners.size();i++){
+        for (int i=start;i<weak_learners.size();i++){
             weak_learners[i]->computeOutput(input,weak_learner_output);
             sum_out += (weak_learner_output[0] < output_threshold ? 0 : 1) 
                 *voting_weights[i];
         }
     else
-        for (int i=0;i<weak_learners.size();i++){
+        for (int i=start;i<weak_learners.size();i++){
             weak_learners[i]->computeOutput(input,weak_learner_output);
             sum_out += weak_learner_output[0]*voting_weights[i];
         }
 
     output[0] = sum_out/sum_voting_weights;
+    output[1] = sum_out;
 }
 
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 
@@ -737,7 +837,6 @@
     else
         PLASSERT(costs.size()==nTrainCosts()||costs.size()==nTestCosts());
     costs.resize(5);
-    costs.clear();
 
     // First cost is negative log-likelihood...  output[0] is the likelihood
     // of the first class
@@ -828,6 +927,7 @@
     }
 
     output[0] = sum_out/sum_voting_weights;
+    output[1] = sum_out;
 
     //when computing train stats, costs==nTrainCosts() 
     //  and forward_sub_learner_test_costs==false

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2009-01-14 20:13:44 UTC (rev 9833)
+++ trunk/plearn_learners/meta/AdaBoost.h	2009-01-14 20:33:48 UTC (rev 9834)
@@ -53,18 +53,21 @@
     typedef PLearner inherited;
 
     //! Global storage to save memory allocations.
-    mutable Vec tmp_output2;
     mutable Vec weighted_costs;
     mutable Vec sum_weighted_costs;
+    mutable Vec weak_learner_output;
+
+    //! Used with reuse_test_results
+    mutable TVec<VMat> saved_testset;
+    mutable TVec<VMat> saved_testoutputs;
+    mutable TVec<int>  saved_last_test_stages;
+
 protected:
     // average weighted error of each learner
     Vec learners_error;
     // weighing scheme over examples
     Vec example_weights;
 
-    //! Used to store outputs from the weak learners.
-    mutable Vec weak_learner_output;
-
     // *********************
     // * protected options *
     // *********************
@@ -125,6 +128,11 @@
 
     // Did we modif directly the train_set weights?
     bool modif_train_set_weights;
+
+    // Did we save and reuse previous test result?
+    // This is usefull to have a test time that is 
+    // independent of the number of adaboost itaration
+    bool reuse_test_results;
     // ****************
     // * Constructors *
     // ****************
@@ -144,6 +152,9 @@
 
     void computeTrainingError(Vec input, Vec target);
 
+    void computeOutput_(const Vec& input, Vec& output,
+                       int start=0, real sum=0.) const;
+
 protected: 
     //! Declares this class' options
     // (Please implement in .cc)
@@ -183,14 +194,15 @@
     //! And sets 'stage' back to 0   (this is the stage of a fresh learner!)
     virtual void forget();
 
-    
     //! The role of the train method is to bring the learner up to stage==nstages,
     //! updating the train_stats collector with training costs measured on-line in the process.
     virtual void train();
+    virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+                      VMat testoutputs, VMat testcosts) const;
 
-
     //! Computes the output from the input
-    virtual void computeOutput(const Vec& input, Vec& output) const;
+    virtual void computeOutput(const Vec& input, Vec& output) const{
+        computeOutput_(input,output,0,0);}
     virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
                                        Vec& output, Vec& costs) const;
 



From nouiz at mail.berlios.de  Wed Jan 14 22:10:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 14 Jan 2009 22:10:45 +0100
Subject: [Plearn-commits] r9835 - trunk/plearn_learners/meta
Message-ID: <200901142110.n0ELAj84027108@sheep.berlios.de>

Author: nouiz
Date: 2009-01-14 22:10:44 +0100 (Wed, 14 Jan 2009)
New Revision: 9835

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
fixed test.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-14 20:33:48 UTC (rev 9834)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-14 21:10:44 UTC (rev 9835)
@@ -307,7 +307,10 @@
 {
     // Outputsize is always 2, since this is a 0-1 classifier
     // and we append the weighted sum to allow the reuse of previous test
-    return 2;
+    if(reuse_test_results)
+        return 2;
+    else 
+        return 1;
 }
 
 void AdaBoost::finalize()
@@ -824,7 +827,8 @@
         }
 
     output[0] = sum_out/sum_voting_weights;
-    output[1] = sum_out;
+    if(reuse_test_results)
+        output[1] = sum_out;
 }
 
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 
@@ -927,7 +931,8 @@
     }
 
     output[0] = sum_out/sum_voting_weights;
-    output[1] = sum_out;
+    if(reuse_test_results)
+        output[1] = sum_out;
 
     //when computing train stats, costs==nTrainCosts() 
     //  and forward_sub_learner_test_costs==false



From nouiz at mail.berlios.de  Thu Jan 15 20:51:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 15 Jan 2009 20:51:57 +0100
Subject: [Plearn-commits] r9836 - trunk/plearn_learners/meta
Message-ID: <200901151951.n0FJpvfX010097@sheep.berlios.de>

Author: nouiz
Date: 2009-01-15 20:51:57 +0100 (Thu, 15 Jan 2009)
New Revision: 9836

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
In MultiClassAdaBoost, if forward_test==2 we automatically decide if we are better to forward it or not.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-14 21:10:44 UTC (rev 9835)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-15 19:51:57 UTC (rev 9836)
@@ -62,8 +62,13 @@
     total_train_time(0),
     test_time(0),
     total_test_time(0),
+    time_sum(0),
+    time_sum_rtr(0),
+    time_last_stage(0),
+    time_last_stage_rtr(0),
+    last_stage(0),
     forward_sub_learner_test_costs(false),
-    forward_test(false)
+    forward_test(0)
 /* ### Initialize all fields to their default value here */
 {
     // ...
@@ -112,7 +117,9 @@
     declareOption(ol, "forward_test", 
                   &MultiClassAdaBoost::forward_test,
                   OptionBase::buildoption,
-                  "Did we add forward the test fct to the sub learner.\n");
+                  "if 0, default test. If 1 forward the test fct to the sub"
+                  " learner. If 2, determine at each stage what is the faster"
+                  " based on past  test time.\n");
 
     declareOption(ol, "train_time",
                   &MultiClassAdaBoost::train_time, OptionBase::learntoption,
@@ -130,6 +137,26 @@
                   &MultiClassAdaBoost::total_test_time, OptionBase::learntoption,
                   "The total time spent in the test() function in second.");
 
+    declareOption(ol, "time_sum",
+                  &MultiClassAdaBoost::time_sum, 
+                  OptionBase::learntoption|OptionBase::nosave,
+                  "The time spend in test() during the last stage. If test() is called multiple time for the same stage this is the sum of the time.");
+    declareOption(ol, "time_sum_rtr",
+                  &MultiClassAdaBoost::time_sum_rtr, 
+                  OptionBase::learntoption|OptionBase::nosave,
+                  "The time spend in test() during the last stage. If test() is called multiple time for the same stage this is the sum of the time.");
+    declareOption(ol, "time_last_stage",
+                  &MultiClassAdaBoost::time_last_stage, 
+                  OptionBase::learntoption|OptionBase::nosave,
+                  "");
+    declareOption(ol, "time_last_stage_rtr",
+                  &MultiClassAdaBoost::time_last_stage_rtr, 
+                  OptionBase::learntoption|OptionBase::nosave,
+                  "");
+    declareOption(ol, "last_stage",
+                  &MultiClassAdaBoost::last_stage, 
+                  OptionBase::learntoption |OptionBase::nosave,
+                  "The stage at witch time_sum was used");
  }
 
 void MultiClassAdaBoost::build_()
@@ -163,7 +190,7 @@
     }
 
     Profiler::activate();
-    Profiler::reset("MultiClassAdaBoost::test");
+    Profiler::reset("MultiClassAdaBoost::test()");
 }
 
 // ### Nothing to add here, simply calls build_
@@ -279,7 +306,7 @@
 
     //we get the test_time here as we want the test time for all dataset.
     //if we put it in the test function, we would have it for one dataset.
-    const Profiler::Stats& stats_test = Profiler::getStats("MultiClassAdaBoost::test");
+    const Profiler::Stats& stats_test = Profiler::getStats("MultiClassAdaBoost::test()");
     tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
     test_time=tmp-total_test_time;
     total_test_time=tmp;
@@ -574,13 +601,39 @@
 void MultiClassAdaBoost::test(VMat testset, PP<VecStatsCollector> test_stats,
                               VMat testoutputs, VMat testcosts) const
 {
-    Profiler::start("MultiClassAdaBoost::test");
+    Profiler::start("MultiClassAdaBoost::test()");
     if(!forward_test){
+         inherited::test(testset,test_stats,testoutputs,testcosts);
+         Profiler::end("MultiClassAdaBoost::test()");
+         return;
+    }
+
+    if(last_stage<stage && time_sum>0){
+        time_last_stage=time_sum;
+        time_sum=0;
+    }
+    if(last_stage<stage && time_sum_rtr>0){
+        time_last_stage_rtr=time_sum_rtr;
+        time_sum_rtr=0;
+    }
+
+    if(forward_test==2 && time_last_stage<time_last_stage_rtr){
+        DBG_MODULE_LOG<<"inherited start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+        Profiler::reset("MultiClassAdaBoost::test() current");
+        Profiler::start("MultiClassAdaBoost::test() current");
+        PLCHECK(last_stage<=stage);
         inherited::test(testset,test_stats,testoutputs,testcosts);
-        Profiler::end("MultiClassAdaBoost::test");
-	return;
+        Profiler::end("MultiClassAdaBoost::test() current");
+        Profiler::end("MultiClassAdaBoost::test()");
+        time_sum += Profiler::getStats("MultiClassAdaBoost::test() current").wall_duration;
+        last_stage=stage;
+        DBG_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+        return;
     }
-    Profiler::pl_profile_start("MultiClassAdaBoost::test() part1");
+    DBG_MODULE_LOG<<"start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+    Profiler::reset("MultiClassAdaBoost::test() current");
+    Profiler::start("MultiClassAdaBoost::test() current");
+    //Profiler::pl_profile_start("MultiClassAdaBoost::test() part1");//cheap
     int index=-1;
     for(int i=0;i<saved_testset.length();i++){
         if(saved_testset[i]==testset){
@@ -597,7 +650,7 @@
     VMat testcosts2 = 0;
     VMat testset1 = 0;
     VMat testset2 = 0;
-    if ((testcosts || test_stats )){
+    if ((testcosts || test_stats )&& forward_sub_learner_test_costs){
         //comment
         testcosts1 = VMat(new MemoryVMatrix(testset->length(),
                                             learner1->nTestCosts()));
@@ -624,11 +677,11 @@
     if (test_stats){
         
     }
-    Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");
-    Profiler::start("MultiClassAdaBoost::subtest");
+    //Profiler::pl_profile_end("MultiClassAdaBoost::test() part1");//cheap
+    Profiler::start("MultiClassAdaBoost::test() subtest");
     learner1->test(testset1,test_stats1,testoutputs1,testcosts1);
     learner2->test(testset2,test_stats2,testoutputs2,testcosts2);
-    Profiler::end("MultiClassAdaBoost::subtest");
+    Profiler::end("MultiClassAdaBoost::test() subtest");
 
     VMat my_outputs = 0;
     VMat my_costs = 0;
@@ -676,8 +729,11 @@
             //            PLWARNING("will be long");
 	int target_index = testset->inputsize();
 	PLASSERT(testset->targetsize()==1);
-        Vec costs1(learner1->nTestCosts());
-        Vec costs2(learner1->nTestCosts());
+        Vec costs1,costs2;
+        if(forward_sub_learner_test_costs){
+            costs1.resize(learner1->nTestCosts());
+            costs2.resize(learner2->nTestCosts());
+        }
         for(int row=0;row<testset.length();row++){
             //default version
             //testset.getExample(row, input, target, weight);
@@ -687,8 +743,10 @@
             testset->getSubRow(row,target_index,tmp_target);
 //	    Vec costs1=testcosts1(row);
 //	    Vec costs2=testcosts2(row);
-            testcosts1->getRow(row,costs1);
-            testcosts2->getRow(row,costs2);
+            if(forward_sub_learner_test_costs){
+                testcosts1->getRow(row,costs1);
+                testcosts2->getRow(row,costs2);
+            }
             //TODO??? tmp_input is empty!!!
 	    computeCostsFromOutputs_(tmp_input, my_outputs(row), tmp_target, costs1,
                                      costs2, tmp_costs);
@@ -715,8 +773,14 @@
 	}
     }
     Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
+    Profiler::end("MultiClassAdaBoost::test() current");
+    Profiler::end("MultiClassAdaBoost::test()");
+    
+    time_sum_rtr += Profiler::getStats("MultiClassAdaBoost::test() current").wall_duration;
 
-    Profiler::end("MultiClassAdaBoost::test");
+    last_stage=stage;
+    DBG_MODULE_LOG<<"end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-01-14 21:10:44 UTC (rev 9835)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2009-01-15 19:51:57 UTC (rev 9836)
@@ -82,6 +82,13 @@
     real test_time;
     //! The total time passed in test()
     real total_test_time;
+
+    mutable long time_sum;
+    mutable long time_sum_rtr;
+    mutable long time_last_stage;
+    mutable long time_last_stage_rtr;
+    mutable int last_stage;
+
 public:
     //#####  Public Build Options  ############################################
 
@@ -92,7 +99,7 @@
     bool forward_sub_learner_test_costs;
 
     //! Did we forward the test function to the sub learner?
-    bool forward_test;
+    uint16_t forward_test;
 
     //! The learner1 and learner2 must be trained!
     PP<AdaBoost> learner1;



From chrish at mail.berlios.de  Thu Jan 15 23:08:49 2009
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 15 Jan 2009 23:08:49 +0100
Subject: [Plearn-commits] r9837 - trunk/python_modules/plearn/gui_tools
Message-ID: <200901152208.n0FM8nRj023885@sheep.berlios.de>

Author: chrish
Date: 2009-01-15 23:08:49 +0100 (Thu, 15 Jan 2009)
New Revision: 9837

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Add ability to load additional arguments from a file, with @filename syntax.

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2009-01-15 19:51:57 UTC (rev 9836)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2009-01-15 22:08:49 UTC (rev 9837)
@@ -386,10 +386,30 @@
         'params' (and the classes contained therin) inherits from
         HasStrictTraits so that any assignment to inexistant options raises
         an exception.
+
+        If an argument starts with an '@'-sign, it is intrepreted as a filename
+        containing extra arguments to insert in the list of arguments. The file is
+        opened, and each line that doesn't start with a '#'-sign is taken as a new 
+        argument. These argument are inserted in order, where the @filename directive
+        was found, with said @filename directive being removed from the list of 
+        arguments. It is possible to use this multiple times: @filename2 @filename2
+
         """
+
+        # First replace all @filename by their contents
+        expanded_argv = []
         for arg in argv:
+            if arg.startswith('@'):
+                f = open(arg[1:], 'rU')
+                expanded_argv.extend(line.strip() for line in f if not line.startswith('#'))
+                f.close()
+            else:
+                expanded_argv.append(arg)
+
+        for arg in expanded_argv:
             if arg.startswith('-'):
                 continue
+            
             if '=' in arg:
                 (k,v) = arg.split('=', 1)
                 v = v.replace("'", "\\'")



From nouiz at mail.berlios.de  Fri Jan 16 15:39:33 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 Jan 2009 15:39:33 +0100
Subject: [Plearn-commits] r9838 - trunk/scripts
Message-ID: <200901161439.n0GEdXYH002576@sheep.berlios.de>

Author: nouiz
Date: 2009-01-16 15:39:33 +0100 (Fri, 16 Jan 2009)
New Revision: 9838

Modified:
   trunk/scripts/dbidispatch
Log:
added the option --queue=X for dbidispatch


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-15 22:08:49 UTC (rev 9837)
+++ trunk/scripts/dbidispatch	2009-01-16 14:39:33 UTC (rev 9838)
@@ -4,11 +4,12 @@
 from socket import gethostname
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--machine=HOSTNAME] [--machines=regex] <back-end parameter> {--file=FILEPATH | <command-template>}
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>}
 
 <back-end parameter>:
     bqtools, cluster option  :[--duree=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
+                              [--queue=X]
     cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
                               [--cpu=nb_cpu_per_node]
     bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only)}+]
@@ -22,11 +23,11 @@
                               [*--[no_]abs_path] [--[*no_]pkdilly]
                               [*--[no_]set_special_env]
                               [--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}]
-                              [--machine=HOSTNAME] [--machines=regex]
+                              [--machine=HOSTNAME+] [--machines=regex+]
     cluster option           :[*--[no_]cwait]  [--[*no_]force]
                               [--[*no_]interruptible]
 An * after '[', '{' or ',' signals the default value.
-An + after } tell that we can put one or more of the choise separeted by a comma
+An + tell that we can put one or more separeted by a comma
 '''
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster,
 local and ssh. If no system is selected on the command line, we try them in the
@@ -84,7 +85,8 @@
     set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
     different queue with few nodes, please make sure you are not using too many
     nodes at once with the --nb_proc option.
-
+  The '--queue=X' tell on witch queue the jobs will be launched
+  
 cluster and condor options:
   The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
     Default the same as the submit host.
@@ -296,7 +298,8 @@
         testmode=False
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
-                                "--universe", "--exp_dir", "--machine", "--machines"]:
+                                "--universe", "--exp_dir", "--machine", "--machines",
+                                "--queue"]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]



From nouiz at mail.berlios.de  Fri Jan 16 15:57:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 Jan 2009 15:57:58 +0100
Subject: [Plearn-commits] r9839 - trunk/python_modules/plearn/parallel
Message-ID: <200901161457.n0GEvwik004990@sheep.berlios.de>

Author: nouiz
Date: 2009-01-16 15:57:58 +0100 (Fri, 16 Jan 2009)
New Revision: 9839

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
changed the default queue to don't specify the @ms so that it will work on mammouth parallel and serie automatically. This work as if it is not specified it still work, at least in case their is only one @ that work.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-16 14:39:33 UTC (rev 9838)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-16 14:57:58 UTC (rev 9839)
@@ -547,7 +547,7 @@
         self.nb_proc = -1
         self.clean_up = True
         self.micro = 1
-        self.queue = "qwork at ms"
+        self.queue = "qwork"
         self.long = False
         self.duree = "120:00:00"
         self.mem = None



From nouiz at mail.berlios.de  Fri Jan 16 18:01:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 Jan 2009 18:01:29 +0100
Subject: [Plearn-commits] r9840 - trunk/plearn_learners/meta
Message-ID: <200901161701.n0GH1T3Z021409@sheep.berlios.de>

Author: nouiz
Date: 2009-01-16 18:01:21 +0100 (Fri, 16 Jan 2009)
New Revision: 9840

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
-changed cout for NORMAL_LOG
-a fast progress bar now need the verbosity > 2 to be displayed.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-16 14:57:58 UTC (rev 9839)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-16 17:01:21 UTC (rev 9840)
@@ -343,8 +343,8 @@
         return;
     else if (nstages < stage){        //!< Asking to revert to previous stage
         PLCHECK(nstages>0); // should use forget
-        cout<<"In AdaBoost::train() - reverting from stage "<<stage
-            <<" to stage "<<nstages<<endl;
+        NORMAL_LOG<<"In AdaBoost::train() - reverting from stage "<<stage
+                  <<" to stage "<<nstages<<endl;
         stage = nstages;
         PLCHECK(learners_error.size()>=stage);
         PLCHECK(weak_learners.size()>=stage);
@@ -513,10 +513,10 @@
         // calculate its weighted training error 
         {
             PP<ProgressBar> pb;
-            if(report_progress) pb = new ProgressBar("computing weighted training error of weak learner",n);
+            if(report_progress && verbosity >1) pb = new ProgressBar("computing weighted training error of weak learner",n);
             learners_error[stage] = 0;
             for (int i=0; i<n; ++i) {
-                if(report_progress) pb->update(i);
+                if(pb) pb->update(i);
                 train_set->getExample(i, input, target, weight);
 #ifdef BOUNDCHECK
                 if(!(is_equal(target[0],0)||is_equal(target[0],1)))
@@ -568,8 +568,8 @@
         }
 
         if (verbosity>1)
-            cout << "weak learner at stage " << stage 
-                 << " has average loss = " << learners_error[stage] << endl;
+            NORMAL_LOG << "weak learner at stage " << stage 
+                       << " has average loss = " << learners_error[stage] << endl;
 
         weak_learners.push_back(new_weak_learner);
 
@@ -612,15 +612,15 @@
             for(iter=1;iter<=itmax;iter++)
             {
                 if(verbosity>4)
-                    cout << "iteration " << iter << ": fx = " << fb << endl;
+                    NORMAL_LOG << "iteration " << iter << ": fx = " << fb << endl;
                 if (abs(cx-ax) <= tolerance)
                 {
                     xmin=bx;
                     if(verbosity>3)
                     {
-                        cout << "nIters for minimum: " << iter << endl;
-                        cout << "xmin = " << xmin << endl;
-                        cout << "fx = " << fb << endl;
+                        NORMAL_LOG << "nIters for minimum: " << iter << endl;
+                        NORMAL_LOG << "xmin = " << xmin << endl;
+                        NORMAL_LOG << "fx = " << fb << endl;
                     }
                     break;
                 }
@@ -678,7 +678,7 @@
             }
             if(verbosity>3)
             {
-                cout << "Too many iterations in Brent" << endl;
+                NORMAL_LOG << "Too many iterations in Brent" << endl;
             }
             xmin=bx;
             voting_weights.push_back(xmin);
@@ -705,9 +705,9 @@
 
         if(fast_exact_is_equal(learners_error[stage], 0))
         {
-            cout << "AdaBoost::train found weak learner with 0 training "
-                 << "error at stage " 
-                 << stage << " is " << learners_error[stage] << endl;  
+            NORMAL_LOG << "AdaBoost::train found weak learner with 0 training "
+                       << "error at stage " 
+                       << stage << " is " << learners_error[stage] << endl;  
 
             // Simulate infinite weight on new_weak_learner
             weak_learners.resize(0);
@@ -723,7 +723,7 @@
         if (early_stopping && learners_error[stage] >= target_error)
         {
             nstages = stage;
-            cout << 
+            NORMAL_LOG << 
                 "AdaBoost::train early stopping because learner's loss at stage " 
                  << stage << " is " << learners_error[stage] << endl;       
             break;
@@ -1046,9 +1046,9 @@
             save_forward_sub_learner_test_costs;
 
         if (verbosity>2)
-            cout << "At stage " << stage << 
+            NORMAL_LOG << "At stage " << stage << 
                 " boosted (weighted) classification error on training set = " 
-                 << train_stats->getMean() << endl;
+                       << train_stats->getMean() << endl;
      
     }
 }



From nouiz at mail.berlios.de  Fri Jan 16 18:01:49 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 Jan 2009 18:01:49 +0100
Subject: [Plearn-commits] r9841 - trunk/plearn_learners/meta
Message-ID: <200901161701.n0GH1ng9021447@sheep.berlios.de>

Author: nouiz
Date: 2009-01-16 18:01:49 +0100 (Fri, 16 Jan 2009)
New Revision: 9841

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
bugfix a PLECHECK.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-16 17:01:21 UTC (rev 9840)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-16 17:01:49 UTC (rev 9841)
@@ -731,7 +731,7 @@
 
 
     }
-    PLCHECK(stage==weak_learners.length());
+    PLCHECK(stage==weak_learners.length() || found_zero_error_weak_learner);
     Profiler::pl_profile_end("AdaBoost::train");
 
 }



From nouiz at mail.berlios.de  Fri Jan 16 22:32:15 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 16 Jan 2009 22:32:15 +0100
Subject: [Plearn-commits] r9842 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200901162132.n0GLWFiB002264@sheep.berlios.de>

Author: nouiz
Date: 2009-01-16 22:32:14 +0100 (Fri, 16 Jan 2009)
New Revision: 9842

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-Aadded the option --nano for bqtools.
-Added some debug stuff.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-16 17:01:49 UTC (rev 9841)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-16 21:32:14 UTC (rev 9842)
@@ -32,6 +32,9 @@
 STATUS_WAITING = 2
 STATUS_INIT = 3
 
+class DBIError(Exception):
+    """Base class for exceptions in this module."""
+    pass
 
 #original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
 class LockedIterator:
@@ -547,6 +550,7 @@
         self.nb_proc = -1
         self.clean_up = True
         self.micro = 1
+        self.nano = 1
         self.queue = "qwork"
         self.long = False
         self.duree = "120:00:00"
@@ -555,6 +559,7 @@
 
         self.nb_proc = int(self.nb_proc)
         self.micro = int(self.micro)
+        self.nano = int(self.nano)
 
 ### We can't accept the symbols "," as this cause trouble with bqtools
         if self.log_dir.find(',')!=-1 or self.log_file.find(',')!=-1:
@@ -651,7 +656,8 @@
                 linkFiles = launcher
                 preBatch = rm -f _*.BQ
                 microJobs = %d
-                '''%(self.unique_id[1:12],self.queue,self.duree,self.micro)) )
+                nanoJobs = %d
+                '''%(self.unique_id[1:12],self.queue,self.duree,self.micro,self.nano)) )
         if self.nb_proc>0:
             bqsubmit_dat.write('''\nconcurrentJobs = %d\n'''%(self.nb_proc))
 
@@ -670,6 +676,28 @@
                 t.set_scheduled_time()
             self.p = Popen( 'bqsubmit', shell=True)
             self.p.wait()
+            
+            #check for error string as bqsubmit don't already return an errorcode !=0 when their was an error.
+            error_str=False
+#            print self.p.stderr
+#            print dir(self.p.stderr)
+#            dir(self.p.stderr)
+#            print self.p.stderr.closed
+#            print self.p.stderr.peek
+#            help(self.p.stderr)
+#            self.p.stderr.flush()
+#            self.p.stderr.write('dd')
+#            print self.p.stderr.read()
+#            lines = self.p.stderr.readline()
+#            print len(lines)
+#            for line in lines:
+#                if line in ["qsub: Job exceeds queue resource limits MSG=cannot satisfy queue max walltime requirement\n"]:
+#                    error_str=True
+#                print line,
+            if self.p.returncode!=0:
+                raise DBIError("[DBI] ERROR: the bqsubmit returned an error code of"+str(self.p.returncode))
+            if error_str:
+                raise DBIError("[DBI] ERROR: the bqsubmit returned an error string. It was probably not launched correctly.")
         else:
             print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
             if self.dolog:
@@ -1728,6 +1756,9 @@
     """
     try:
         jobs = eval('DBI'+launch_system+'(commands,**args)')
+    except DBIError, e:
+        print e
+        sys.exit(1)
     except NameError:
         print 'The launch system ',launch_system, ' does not exists. Available systems are: Cluster, Ssh, Bqtools and Condor'
         traceback.print_exc()

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-16 17:01:49 UTC (rev 9841)
+++ trunk/scripts/dbidispatch	2009-01-16 21:32:14 UTC (rev 9842)
@@ -10,6 +10,7 @@
     bqtools, cluster option  :[--duree=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X]
+                              [--nano=X]
     cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
                               [--cpu=nb_cpu_per_node]
     bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only)}+]
@@ -86,6 +87,7 @@
     different queue with few nodes, please make sure you are not using too many
     nodes at once with the --nb_proc option.
   The '--queue=X' tell on witch queue the jobs will be launched
+  The '--nano=X'
   
 cluster and condor options:
   The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
@@ -299,7 +301,7 @@
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
                                 "--universe", "--exp_dir", "--machine", "--machines",
-                                "--queue"]:
+                                "--queue", "--nano"]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -353,7 +355,7 @@
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["micro", "long", "duree"]
+    valid_dbi_param +=["micro", "long", "duree", "queue", "nano"]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.
@@ -576,8 +578,14 @@
     t1=time.time()
     jobs = DBI(commands,launch_cmd,**dbi_param)
     t2=time.time()
+    error=False
     print "it took %f s to create the DBI objects"%(t2-t1)
-    jobs.run()
+    try:
+        jobs.run()
+    except DBIError, e:
+        error=True
+        print e
+        sys.exit(1)
     t3=time.time()
     jobs.wait()
     if "test" in dbi_param:



From larocheh at mail.berlios.de  Fri Jan 16 22:42:20 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 Jan 2009 22:42:20 +0100
Subject: [Plearn-commits] r9843 - trunk/plearn_learners/online
Message-ID: <200901162142.n0GLgKWC003406@sheep.berlios.de>

Author: larocheh
Date: 2009-01-16 22:42:19 +0100 (Fri, 16 Jan 2009)
New Revision: 9843

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added an option to have connections between the target and 
the hidden layer, during greedy training. This allows to do 
joint (denoising) autoencoding on the input (or hidden layer 
representation) and the target.

This should work in the non-minibatch setting, either in greedy 2-phase or online learning.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-16 21:32:14 UTC (rev 9842)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-16 21:42:19 UTC (rev 9843)
@@ -68,6 +68,7 @@
     noise_type( "masking_noise" ),
     fraction_of_masked_inputs( 0 ),
     probability_of_masked_inputs( 0 ),
+    probability_of_masked_target( 0 ),
     mask_with_mean( false ),
     gaussian_std( 1. ),
     binary_sampling_noise_parameter( 1. ),
@@ -211,6 +212,16 @@
                   "weights of 1 will be assumed for all partial costs.\n"
         );
 
+    declareOption(ol, "greedy_target_connections",
+                  &StackedAutoassociatorsNet::greedy_target_connections,
+                  OptionBase::buildoption,
+                  "Optional target connections during greedy training..\n"
+                  "They connect the target with the hidden layer from which\n"
+                  "the autoassociator's cost (including partial cost) is computed\n"
+                  "(only during training).\n"
+                  "Currently works only if target is a class index.\n"
+        );
+
     declareOption(ol, "compute_all_test_costs",
                   &StackedAutoassociatorsNet::compute_all_test_costs,
                   OptionBase::buildoption,
@@ -251,6 +262,12 @@
                   "or fraction_of_masked_inputs should be > 0.\n"
         );
 
+    declareOption(ol, "probability_of_masked_target",
+                  &StackedAutoassociatorsNet::probability_of_masked_target,
+                  OptionBase::buildoption,
+                  "Probability of masking the target, when using greedy_target_connections.\n"
+        );
+
     declareOption(ol, "mask_with_mean",
                   &StackedAutoassociatorsNet::mask_with_mean,
                   OptionBase::buildoption,
@@ -413,6 +430,11 @@
                     " - \n"
                     "probability_of_masked_inputs should be > or equal to 0.\n");
 
+        if( probability_of_masked_target < 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "probability_of_masked_target should be > or equal to 0.\n");
+
         if( online && fraction_of_masked_inputs > 0)
             PLERROR("StackedAutoassociatorsNet::build_()"
                     " - \n"
@@ -605,6 +627,25 @@
             }
         }
 
+        if(greedy_target_connections.length() != 0)
+        {
+            if(reconstruct_hidden)
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "greedy_target_connections not implemented with reconstruct_hidden=true.\n");
+
+            if( greedy_target_connections[i]->up_size != layers[i+1]->size )
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "greedy_target_connections[%i] should have a up_size of %d.\n",
+                        i, layers[i+1]->size);
+            if( !(greedy_target_connections[i]->random_gen) )
+            {
+                greedy_target_connections[i]->random_gen = random_gen;
+                greedy_target_connections[i]->forget();
+            }
+        }
+
         if( !(layers[i]->random_gen) )
         {
             layers[i]->random_gen = random_gen;
@@ -640,19 +681,32 @@
 
     // For denoising autoencoders
     corrupted_autoassociator_expectations.resize( n_layers-1 );
-    if( noise_type == "masking_noise" && fraction_of_masked_inputs > 0 )
+    if( noise_type == "masking_noise" )
         autoassociator_expectation_indices.resize( n_layers-1 );
     
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         corrupted_autoassociator_expectations[i].resize( layers[i]->size );
-        if( noise_type == "masking_noise" && fraction_of_masked_inputs > 0 )
+        if( noise_type == "masking_noise" )
         {
             autoassociator_expectation_indices[i].resize( layers[i]->size );
             for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
                 autoassociator_expectation_indices[i][j] = j;
         }
     }
+
+    if(greedy_target_connections.length() != 0)
+    {
+        target_vec.resize(greedy_target_connections[0]->down_size);
+        target_vec_gradient.resize(greedy_target_connections[0]->down_size);
+        targets_vec.resize(n_layers-1);
+        targets_vec_gradient.resize(n_layers-1);
+        for( int i=0; i<n_layers-1; i++ )
+        {
+            targets_vec[i].resize(greedy_target_connections[0]->down_size);
+            targets_vec_gradient[i].resize(greedy_target_connections[0]->down_size);
+        }
+    }
 }
 
 void StackedAutoassociatorsNet::build_costs()
@@ -757,6 +811,7 @@
     deepCopyField(final_cost, copies);
     deepCopyField(partial_costs, copies);
     deepCopyField(partial_costs_weights, copies);
+    deepCopyField(greedy_target_connections, copies);
 
     // Protected options
     deepCopyField(activations, copies);
@@ -807,6 +862,10 @@
     deepCopyField(corrupted_autoassociator_expectations, copies);
     deepCopyField(autoassociator_expectation_indices, copies);
     deepCopyField(expectation_means, copies);
+    deepCopyField(target_vec, copies);
+    deepCopyField(target_vec_gradient, copies);
+    deepCopyField(targets_vec, copies);
+    deepCopyField(targets_vec_gradient, copies);
     deepCopyField(greedy_stages, copies);
 }
 
@@ -1299,10 +1358,24 @@
             }
             else
                 connections[i]->fprop( expectations[i], correlation_activations[i] );
+
+            if( i == index && greedy_target_connections.length() && greedy_target_connections[i] )
+            {
+                target_vec.clear();
+                if( probability_of_masked_target == 0 ||
+                    random_gen->uniform_sample() >= probability_of_masked_target )
+                    target_vec[(int)target[0]] = 1;
+
+                greedy_target_connections[i]->setAsDownInput(target_vec);
+                greedy_target_connections[i]->computeProduct(0, correlation_activations[i].length(),
+                                                             correlation_activations[i], true);
+            }
+
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
             correlation_connections[i]->fprop( correlation_expectations[i],
                                                activations[i+1] );
+
             correlation_layers[i]->fprop( activations[i+1],
                                           expectations[i+1] );
         }
@@ -1318,6 +1391,19 @@
             }
             else
                 connections[i]->fprop( expectations[i], activations[i+1] );
+            
+            if( i == index && greedy_target_connections.length() && greedy_target_connections[i] )
+            {
+                target_vec.clear();
+                if( probability_of_masked_target == 0 ||
+                    random_gen->uniform_sample() >= probability_of_masked_target )
+                    target_vec[(int)target[0]] = 1;
+
+                greedy_target_connections[i]->setAsDownInput(target_vec);
+                greedy_target_connections[i]->computeProduct(0, activations[i+1].length(),
+                                                             activations[i+1], true);
+            }
+
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
     }
@@ -1496,6 +1582,15 @@
             correlation_activations[ index ],
             reconstruction_expectation_gradients, //reused
             correlation_activation_gradients [ index ]);
+
+        if( greedy_target_connections.length() && greedy_target_connections[index] )
+        {
+            greedy_target_connections[index]->bpropUpdate(
+                target_vec, 
+                correlation_activations[index],
+                target_vec_gradient,
+                correlation_activation_gradients [ index ]);
+        }
     }
     else
     {
@@ -1510,6 +1605,14 @@
             activations[ index + 1 ],
             reconstruction_expectation_gradients, //reused
             reconstruction_activation_gradients);
+        if( greedy_target_connections.length() && greedy_target_connections[index] )
+        {
+            greedy_target_connections[index]->bpropUpdate(
+                target_vec, 
+                activations[ index + 1 ],
+                target_vec_gradient,
+                reconstruction_activation_gradients);
+        }
     }
 
 }
@@ -1776,12 +1879,26 @@
         for( int i=0 ; i<n_layers-1; i++ )
         {
             connections[i]->fprop( expectations[i], correlation_activations[i] );
+
+            if( greedy_target_connections.length() && greedy_target_connections[i] )
+            {
+                targets_vec[i].clear();
+                if( probability_of_masked_target == 0 ||
+                    random_gen->uniform_sample() >= probability_of_masked_target )
+                    targets_vec[i][(int)target[0]] = 1;
+
+                greedy_target_connections[i]->setAsDownInput(targets_vec[i]);
+                greedy_target_connections[i]->computeProduct(0, correlation_activations[i].length(),
+                                                             correlation_activations[i], true);
+            }
+
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
             correlation_connections[i]->fprop( correlation_expectations[i],
                                                activations[i+1] );
             correlation_layers[i]->fprop( activations[i+1],
                                           expectations[i+1] );
+
         }
     }
     else
@@ -1789,63 +1906,20 @@
         for( int i=0 ; i<n_layers-1; i++ )
         {
             connections[i]->fprop( expectations[i], activations[i+1] );
-            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
 
-            if( partial_costs.length() != 0 && partial_costs[ i ] )
+            if( greedy_target_connections.length() && greedy_target_connections[i] )
             {
-                // Set learning rates
-                if( !fast_exact_is_equal(fine_tuning_decrease_ct , 0 ) )
-                    lr = fine_tuning_learning_rate /
-                        (1 + fine_tuning_decrease_ct * stage);
-                else
-                    lr = fine_tuning_learning_rate;
+                targets_vec[i].clear();
+                if( probability_of_masked_target == 0 ||
+                    random_gen->uniform_sample() >= probability_of_masked_target )
+                    targets_vec[i][(int)target[0]] = 1;
 
-                partial_costs[ i ]->setLearningRate( lr );
-                /* No, learning rate should already be OK in layers and
-                 * connections
-                layers[ i+1 ]->setLearningRate( lr );
-                connections[ i ]->setLearningRate( lr );
-                */
+                greedy_target_connections[i]->setAsDownInput(targets_vec[i]);
+                greedy_target_connections[i]->computeProduct(0, activations[i+1].length(),
+                                                             activations[i+1], true);
+            }
 
-                partial_costs[ i ]->fprop( expectations[ i + 1],
-                                           target, partial_cost_value );
-
-                // Update partial cost (might contain some weights for example)
-                partial_costs[ i ]->bpropUpdate(
-                    expectations[ i + 1 ],
-                    target, partial_cost_value[0],
-                    expectation_gradients[ i + 1 ]
-                    );
-
-                train_costs.subVec(partial_costs_positions[i]+1,
-                                   partial_cost_value.length())
-                    << partial_cost_value;
-
-                if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
-                    expectation_gradients[ i + 1 ] *= partial_costs_weights[i];
-
-                // Update hidden layer bias and weights
-                layers[ i+1 ]->bpropUpdate( activations[ i + 1 ],
-                                            expectations[ i + 1 ],
-                                            activation_gradients[ i + 1 ],
-                                            expectation_gradients[ i + 1 ] );
-
-                connections[ i ]->bpropUpdate( expectations[ i ],
-                                               activations[ i + 1 ],
-                                               expectation_gradients[ i ],
-                                               activation_gradients[ i + 1 ] );
-
-                /* no need
-                if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
-                    lr = fine_tuning_learning_rate /
-                        (1 + fine_tuning_decrease_ct * stage);
-                else
-                    lr = fine_tuning_learning_rate;
-
-                layers[ i+1 ]->setLearningRate( lr );
-                connections[ i ]->setLearningRate( lr );
-                */
-            }
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
     }
 
@@ -1883,6 +1957,10 @@
             correlation_layers[i]->setLearningRate( lr );
             correlation_connections[i]->setLearningRate( lr );
         }
+        if( partial_costs.length() != 0 && partial_costs[ i ] )
+        {
+            partial_costs[ i ]->setLearningRate( lr );
+        }
     }
     layers[n_layers-1]->setLearningRate( lr );
 
@@ -1913,6 +1991,28 @@
             reconstruction_expectation_gradients,
             reconstruction_activation_gradients);
 
+        if( partial_costs.length() != 0 && partial_costs[ i-1 ] )
+        {
+            
+            partial_costs[ i-1 ]->fprop( expectations[ i],
+                                       target, partial_cost_value );
+            
+            // Update partial cost (might contain some weights for example)
+            partial_costs[ i-1 ]->bpropUpdate(
+                expectations[ i ],
+                target, partial_cost_value[0],
+                expectation_gradients[ i ]
+                );
+
+            train_costs.subVec(partial_costs_positions[i-1]+1,
+                               partial_cost_value.length())
+                << partial_cost_value;
+            
+            if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
+                expectation_gradients[ i ] *= partial_costs_weights[i-1];
+            reconstruction_expectation_gradients += expectation_gradients[ i ];
+        }
+
         if(!fast_exact_is_equal(l1_neuron_decay,0))
         {
             // Compute L1 penalty gradient on neurons
@@ -1953,6 +2053,15 @@
                                            correlation_activations[i-1],
                                            reconstruction_expectation_gradients,
                                            correlation_activation_gradients[i-1] );
+
+            if( greedy_target_connections.length() && greedy_target_connections[i-1] )
+            {
+                greedy_target_connections[i-1]->bpropUpdate(
+                    targets_vec[i-1], 
+                    correlation_activations[i-1],
+                    targets_vec_gradient[i-1],
+                    correlation_activation_gradients [ i-1 ]);
+            }
         }
         else
         {
@@ -1967,6 +2076,15 @@
                 activations[i],
                 reconstruction_expectation_gradients,
                 reconstruction_activation_gradients);
+
+            if( greedy_target_connections.length() && greedy_target_connections[i-1] )
+            {
+                greedy_target_connections[i-1]->bpropUpdate(
+                    targets_vec[i-1], 
+                    activations[ i ],
+                    targets_vec_gradient[i-1],
+                    reconstruction_activation_gradients);
+            }
         }
     }
 
@@ -2051,10 +2169,19 @@
     expectations_m[0].resize(mbatch_size, inputsize());
     expectations_m[0] << inputs;
 
+    if( greedy_target_connections.length() != 0 )
+        PLERROR("In StackedAutoassociatorsNet::onlineStep(): greedy_target_connections not "
+                "implemented yet in mini-batch online setting.\n");
+    
     if(correlation_connections.length() != 0)
     {
         for( int i=0 ; i<n_layers-1; i++ )
         {
+            if( partial_costs.length() != 0 && partial_costs[ i ] )
+                PLERROR("In StackedAutoassociatorsNet::onlineStep(): partial costs not "
+                        "implemented yet for correlation_connections, in mini-batch online "
+                        "setting.\n");
+
             connections[i]->fprop(expectations_m[i],
                                   correlation_activations_m[i]);
             layers[i+1]->fprop(correlation_activations_m[i],

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-01-16 21:32:14 UTC (rev 9842)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-01-16 21:42:19 UTC (rev 9843)
@@ -65,7 +65,7 @@
     //#####  Public Build Options  ############################################
 
     //! The learning rate used during the autoassociator gradient descent
-    //! training
+    //! training. It is also used for the partial costs.
     real greedy_learning_rate;
 
     //! The decrease constant of the learning rate used during the
@@ -139,6 +139,13 @@
     //! weights of 1 will be assumed for all partial costs.
     Vec partial_costs_weights;
 
+    //! Optional target connections during greedy training.
+    //! They connect the target with the hidden layer from which
+    //! the autoassociator's cost (including partial cost) is computed
+    //! (only during training).
+    //! Currently works only if target is a class index.
+    TVec< PP<RBMConnection> > greedy_target_connections;
+
     //! Indication that, at test time, all costs for all
     //! layers (up to the currently trained layer) should be computed.
     bool compute_all_test_costs;
@@ -158,6 +165,9 @@
     //! or fraction_of_masked_inputs should be > 0.
     real probability_of_masked_inputs;
 
+    //! Probability of masking the target, when using greedy_target_connections
+    real probability_of_masked_target;
+
     //! Indication that inputs should be masked with the 
     //! training set mean of that component
     bool mask_with_mean;
@@ -404,6 +414,13 @@
     //! Mean of layers on the training set for each layer
     TVec<Vec> expectation_means;
 
+    //! Vectorial representation of the target
+    Vec target_vec;
+    Vec target_vec_gradient;
+    //! For online case
+    TVec< Vec > targets_vec;
+    TVec< Vec > targets_vec_gradient;
+
     //! Stages of the different greedy phases
     TVec<int> greedy_stages;
 



From larocheh at mail.berlios.de  Fri Jan 16 22:45:31 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 16 Jan 2009 22:45:31 +0100
Subject: [Plearn-commits] r9844 - trunk/plearn/vmat
Message-ID: <200901162145.n0GLjVvp003804@sheep.berlios.de>

Author: larocheh
Date: 2009-01-16 22:45:30 +0100 (Fri, 16 Jan 2009)
New Revision: 9844

Modified:
   trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
Log:
Turns out there wasn't a memory leak. The problem was 
that kernel-invariant.{h,c} was keeping a cache that was 
too big. I set that cache to be of 1 example (not in this 
commit, but in the lab installation).


Modified: trunk/plearn/vmat/InfiniteMNISTVMatrix.cc
===================================================================
--- trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-16 21:42:19 UTC (rev 9843)
+++ trunk/plearn/vmat/InfiniteMNISTVMatrix.cc	2009-01-16 21:45:30 UTC (rev 9844)
@@ -107,8 +107,6 @@
         *vj = *xj/input_divisor;
     
     v.last() = InfiniteMNISTVMatrix::dataset->y[ (i_dataset<10000) ? i_dataset : 10000 + ((i_dataset - 10000) % 60000) ];
-
-    free(image);
 }
 
 void InfiniteMNISTVMatrix::declareOptions(OptionList& ol)



From lamblin at mail.berlios.de  Fri Jan 16 22:47:31 2009
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 16 Jan 2009 22:47:31 +0100
Subject: [Plearn-commits] r9845 - in trunk: commands plearn_learners/online
Message-ID: <200901162147.n0GLlVHj003883@sheep.berlios.de>

Author: lamblin
Date: 2009-01-16 22:47:31 +0100 (Fri, 16 Jan 2009)
New Revision: 9845

Added:
   trunk/plearn_learners/online/SoftmaxNLLCostModule.cc
   trunk/plearn_learners/online/SoftmaxNLLCostModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Softmax and NLL in the same module, to avoid precision loss in the gradient
computation.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2009-01-16 21:45:30 UTC (rev 9844)
+++ trunk/commands/plearn_noblas_inc.h	2009-01-16 21:47:31 UTC (rev 9845)
@@ -252,6 +252,7 @@
 #include <plearn_learners/online/ScaleGradientModule.h>
 #include <plearn_learners/online/ShuntingNNetLayerModule.h>
 #include <plearn_learners/online/SoftmaxModule.h>
+#include <plearn_learners/online/SoftmaxNLLCostModule.h>
 #include <plearn_learners/online/SplitModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
 #include <plearn_learners/online/BinarizeModule.h>

Added: trunk/plearn_learners/online/SoftmaxNLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/SoftmaxNLLCostModule.cc	2009-01-16 21:45:30 UTC (rev 9844)
+++ trunk/plearn_learners/online/SoftmaxNLLCostModule.cc	2009-01-16 21:47:31 UTC (rev 9845)
@@ -0,0 +1,325 @@
+// -*- C++ -*-
+
+// SoftmaxNLLCostModule.cc
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file SoftmaxNLLCostModule.cc */
+
+
+
+#include "SoftmaxNLLCostModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    SoftmaxNLLCostModule,
+    "Like SoftmaxModule and NLLCostModule, with more precision.",
+    "If target is the index of the true class, this module computes\n"
+    "    cost = -log( softmax(input)[target] ),\n"
+    "and back-propagates the gradient and diagonal of Hessian.\n");
+
+SoftmaxNLLCostModule::SoftmaxNLLCostModule()
+{
+    output_size = 1;
+    target_size = 1;
+}
+
+void SoftmaxNLLCostModule::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "myoption", &SoftmaxNLLCostModule::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void SoftmaxNLLCostModule::build_()
+{
+}
+
+// ### Nothing to add here, simply calls build_
+void SoftmaxNLLCostModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void SoftmaxNLLCostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+
+///////////
+// fprop //
+///////////
+void SoftmaxNLLCostModule::fprop(const Vec& input, const Vec& target, Vec& cost) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
+
+    tmp_vec.resize(input_size);
+    cost.resize(output_size);
+
+    if (input.hasMissing())
+        cost[0] = MISSING_VALUE;
+    else
+    {
+        int the_target = (int) round( target[0] );
+        log_softmax(input, tmp_vec);
+        cost[0] = - tmp_vec[the_target];
+    }
+}
+
+void SoftmaxNLLCostModule::fprop(const Mat& inputs, const Mat& targets, Mat& costs)
+    const
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+
+    int batch_size = inputs.length();
+    PLASSERT( inputs.length() == batch_size );
+    PLASSERT( targets.length() == batch_size );
+
+    tmp_vec.resize(input_size);
+    costs.resize(batch_size, output_size);
+
+    for( int k=0; k<batch_size; k++ )
+    {
+        if (inputs(k).hasMissing())
+            costs(k, 0) = MISSING_VALUE;
+        else
+        {
+            int target_k = (int) round( targets(k, 0) );
+            log_softmax(inputs(k), tmp_vec);
+            costs(k, 0) = - tmp_vec[target_k];
+        }
+    }
+}
+
+void SoftmaxNLLCostModule::fprop(const TVec<Mat*>& ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+
+    Mat* prediction = ports_value[0];
+    Mat* target = ports_value[1];
+    Mat* cost = ports_value[2];
+
+    // If we have prediction and target, and we want cost
+    if( prediction && !prediction->isEmpty()
+        && target && !target->isEmpty()
+        && cost && cost->isEmpty() )
+
+    {
+        PLASSERT( prediction->width() == port_sizes(0, 1) );
+        PLASSERT( target->width() == port_sizes(1, 1) );
+
+        int batch_size = prediction->length();
+        PLASSERT( target->length() == batch_size );
+
+        cost->resize(batch_size, port_sizes(2, 1));
+
+
+        for( int i=0; i<batch_size; i++ )
+        {
+            if( (*prediction)(i).hasMissing() || is_missing((*target)(i,0)) )
+                (*cost)(i,0) = MISSING_VALUE;
+            else
+            {
+                int target_i = (int) round( (*target)(i,0) );
+                PLASSERT( is_equal( (*target)(i, 0), target_i ) );
+                log_softmax( (*prediction)(i), tmp_vec );
+                (*cost)(i,0) = - tmp_vec[target_i];
+            }
+        }
+    }
+    else if( !prediction && !target && !cost )
+        return;
+    else
+        PLCHECK_MSG( false, "Unknown port configuration" );
+
+    checkProp(ports_value);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void SoftmaxNLLCostModule::bpropUpdate(
+        const Vec& input, const Vec& target, real cost,
+        Vec& input_gradient, bool accumulate)
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( input_size );
+        input_gradient.clear();
+    }
+
+    int the_target = (int) round( target[0] );
+
+    // input_gradient[ i ] = softmax(x)[i] if i != t,
+    // input_gradient[ t ] = softmax(x)[t] - 1.
+    softmax(input, input_gradient);
+    input_gradient[ the_target ] -= 1.;
+}
+
+void SoftmaxNLLCostModule::bpropUpdate(
+        const Mat& inputs, const Mat& targets, const Vec& costs,
+        Mat& input_gradients, bool accumulate)
+{
+    PLASSERT( inputs.width() == input_size );
+    PLASSERT( targets.width() == target_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == input_size &&
+                input_gradients.length() == inputs.length(),
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), input_size );
+        input_gradients.clear();
+    }
+
+    // input_gradient[ i ] = softmax(x)[i] if i != t,
+    // input_gradient[ t ] = softmax(x)[t] - 1.
+    for (int i = 0; i < inputs.length(); i++) {
+        int the_target = (int) round( targets(i, 0) );
+        softmax(inputs(i), input_gradients(i));
+        input_gradients(i, the_target) -= 1.;
+    }
+}
+
+void SoftmaxNLLCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                          const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+
+    Mat* prediction = ports_value[0];
+    Mat* target = ports_value[1];
+#ifdef BOUNDCHECK
+    Mat* cost = ports_value[2];
+#endif
+    Mat* prediction_grad = ports_gradient[0];
+    Mat* target_grad = ports_gradient[1];
+    Mat* cost_grad = ports_gradient[2];
+
+    // If we have cost_grad and we want prediction_grad
+    if( prediction_grad && prediction_grad->isEmpty()
+        && cost_grad && !cost_grad->isEmpty() )
+    {
+        PLASSERT( prediction );
+        PLASSERT( target );
+        PLASSERT( cost );
+        PLASSERT( !target_grad );
+
+        PLASSERT( prediction->width() == port_sizes(0,1) );
+        PLASSERT( target->width() == port_sizes(1,1) );
+        PLASSERT( cost->width() == port_sizes(2,1) );
+        PLASSERT( prediction_grad->width() == port_sizes(0,1) );
+        PLASSERT( cost_grad->width() == port_sizes(2,1) );
+        PLASSERT( cost_grad->width() == 1 );
+
+        int batch_size = prediction->length();
+        PLASSERT( target->length() == batch_size );
+        PLASSERT( cost->length() == batch_size );
+        PLASSERT( cost_grad->length() == batch_size );
+
+        prediction_grad->resize(batch_size, port_sizes(0,1));
+
+        for( int k=0; k<batch_size; k++ )
+        {
+            // input_gradient[ i ] = softmax(x)[i] if i != t,
+            // input_gradient[ t ] = softmax(x)[t] - 1.
+            int target_k = (int) round((*target)(k, 0));
+            softmax((*prediction)(k), (*prediction_grad)(k));
+            (*prediction_grad)(k, target_k) -= 1.;
+        }
+    }
+    else if( !prediction_grad && !target_grad && !cost_grad )
+        return;
+    else if( !cost_grad && prediction_grad && prediction_grad->isEmpty() )
+        PLERROR("In SoftmaxNLLCostModule::bpropAccUpdate - cost gradient is NULL,\n"
+                "cannot compute prediction gradient. Maybe you should set\n"
+                "\"propagate_gradient = 0\" on the incoming connection.\n");
+    else
+        PLERROR("In OnlineLearningModule::bpropAccUpdate - Port configuration "
+                "not implemented for class '%s'", classname().c_str());
+
+    checkProp(ports_value);
+    checkProp(ports_gradient);
+}
+
+void SoftmaxNLLCostModule::bbpropUpdate(const Vec& input, const Vec& target,
+                                 real cost,
+                                 Vec& input_gradient, Vec& input_diag_hessian,
+                                 bool accumulate)
+{
+    PLCHECK(false);
+}
+
+TVec<string> SoftmaxNLLCostModule::costNames()
+{
+    if (name == "" || name == classname())
+        return TVec<string>(1, "NLL");
+    else
+        return TVec<string>(1, name + ".NLL");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/SoftmaxNLLCostModule.h
===================================================================
--- trunk/plearn_learners/online/SoftmaxNLLCostModule.h	2009-01-16 21:45:30 UTC (rev 9844)
+++ trunk/plearn_learners/online/SoftmaxNLLCostModule.h	2009-01-16 21:47:31 UTC (rev 9845)
@@ -0,0 +1,164 @@
+// -*- C++ -*-
+
+// SoftmaxNLLCostModule.h
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file SoftmaxNLLCostModule.h */
+
+
+#ifndef SoftmaxNLLCostModule_INC
+#define SoftmaxNLLCostModule_INC
+
+#include <plearn_learners/online/CostModule.h>
+
+namespace PLearn {
+
+/**
+ * Computes the NLL, given a probability vector and the true class.
+ * If input is the probability vector, and target the index of the true class,
+ * this module computes cost = -log( input[target] ), and back-propagates the
+ * gradient and diagonal of Hessian.
+ */
+class SoftmaxNLLCostModule : public CostModule
+{
+    typedef CostModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    SoftmaxNLLCostModule();
+
+    // Your other public member functions go here
+
+    //! given the input and target, compute the cost
+    virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
+
+    //! batch version
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs)
+        const;
+
+    //! new version
+    virtual void fprop(const TVec<Mat*>& ports_value);
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop.
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
+                             Vec& input_gradient, bool accumulate=false);
+
+    //! Overridden.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+            const Vec& costs, Mat& input_gradients, bool accumulate = false);
+
+    //! Does nothing
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost)
+    {}
+
+    //! New version of backpropagation
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this back.
+    virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
+                              Vec& input_gradient, Vec& input_diag_hessian,
+                              bool accumulate=false);
+
+    //! Does nothing
+    virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost)
+    {}
+
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Indicates the name of the computed costs
+    virtual TVec<string> costNames();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(SoftmaxNLLCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    mutable Vec tmp_vec;
+    mutable Mat tmp_mat;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SoftmaxNLLCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Sat Jan 17 17:19:00 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sat, 17 Jan 2009 17:19:00 +0100
Subject: [Plearn-commits] r9846 - trunk/plearn_learners/online
Message-ID: <200901171619.n0HGJ0sd011633@sheep.berlios.de>

Author: larocheh
Date: 2009-01-17 17:19:00 +0100 (Sat, 17 Jan 2009)
New Revision: 9846

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Implemented online learning with denoising autoencoders, in the non-minibatch case.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-16 21:47:31 UTC (rev 9845)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-17 16:19:00 UTC (rev 9846)
@@ -435,10 +435,11 @@
                     " - \n"
                     "probability_of_masked_target should be > or equal to 0.\n");
 
-        if( online && fraction_of_masked_inputs > 0)
+        if( online && noise_type != "masking_noise" && batch_size != 1)
             PLERROR("StackedAutoassociatorsNet::build_()"
                     " - \n"
-                    "masked inputs has not been implemented for online option.\n");
+                    "corrupted inputs only works with masking noise in online setting,"
+                    "in the non-minibatch case.\n");
 
         if( !online )
         {
@@ -681,12 +682,14 @@
 
     // For denoising autoencoders
     corrupted_autoassociator_expectations.resize( n_layers-1 );
+    binary_masks.resize( n_layers-1 );
     if( noise_type == "masking_noise" )
         autoassociator_expectation_indices.resize( n_layers-1 );
     
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         corrupted_autoassociator_expectations[i].resize( layers[i]->size );
+        binary_masks[i].resize( layers[i]->size ); // For online learning
         if( noise_type == "masking_noise" )
         {
             autoassociator_expectation_indices[i].resize( layers[i]->size );
@@ -860,6 +863,8 @@
     deepCopyField(final_cost_gradient, copies);
     deepCopyField(final_cost_gradients, copies);
     deepCopyField(corrupted_autoassociator_expectations, copies);
+    deepCopyField(binary_masks, copies);
+    deepCopyField(tmp_mask, copies);
     deepCopyField(autoassociator_expectation_indices, copies);
     deepCopyField(expectation_means, copies);
     deepCopyField(target_vec, copies);
@@ -1284,6 +1289,13 @@
 
 void StackedAutoassociatorsNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer)
 {
+    tmp_mask.resize(input.length());
+    corrupt_input(input,corrupted_input,layer,tmp_mask);
+}
+
+void StackedAutoassociatorsNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer, Vec& binary_mask)
+{
+    binary_mask.fill(1);
     corrupted_input.resize(input.length());
     if( mask_input_layer_only && layer != 0 )
     {
@@ -1300,13 +1312,19 @@
             if( mask_with_mean )
                 for( int j=0 ; j <input.length() ; j++)
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
+                    {
                         corrupted_input[ j ] = expectation_means[layer][ j ];
+                        binary_mask[ j ] = 0;
+                    }
                     else
                         corrupted_input[ j ] = input[ j ];
             else
                 for( int j=0 ; j <input.length() ; j++)
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
+                    {
                         corrupted_input[ j ] = 0;
+                        binary_mask[ j ] = 0;
+                    }
                     else
                         corrupted_input[ j ] = input[ j ];
                 
@@ -1317,10 +1335,16 @@
             corrupted_input << input;
             if( mask_with_mean )
                 for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                {
                     corrupted_input[ autoassociator_expectation_indices[layer][j] ] = expectation_means[layer][autoassociator_expectation_indices[layer][j]];
+                    binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
+                }
             else
                 for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                {
                     corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
+                    binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
+                }
         }
 
     }
@@ -1354,7 +1378,8 @@
             if( i == index )
             {
                 corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i );
-                connections[i]->fprop( corrupted_autoassociator_expectations[i], correlation_activations[i] );
+                connections[i]->fprop( corrupted_autoassociator_expectations[i], 
+                                       correlation_activations[i] );
             }
             else
                 connections[i]->fprop( expectations[i], correlation_activations[i] );
@@ -1878,7 +1903,10 @@
     {
         for( int i=0 ; i<n_layers-1; i++ )
         {
-            connections[i]->fprop( expectations[i], correlation_activations[i] );
+            corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], 
+                           i, binary_masks[i] );
+            connections[i]->fprop( corrupted_autoassociator_expectations[i], 
+                                   correlation_activations[i] );
 
             if( greedy_target_connections.length() && greedy_target_connections[i] )
             {
@@ -1905,8 +1933,11 @@
     {
         for( int i=0 ; i<n_layers-1; i++ )
         {
-            connections[i]->fprop( expectations[i], activations[i+1] );
-
+            corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], 
+                           i, binary_masks[i] );
+            connections[i]->fprop( corrupted_autoassociator_expectations[i], 
+                                   activations[i+1] );
+            
             if( greedy_target_connections.length() && greedy_target_connections[i] )
             {
                 targets_vec[i].clear();
@@ -1923,22 +1954,6 @@
         }
     }
 
-    final_module->fprop( expectations[ n_layers-1 ],
-                         final_cost_input );
-    final_cost->fprop( final_cost_input, target, final_cost_value );
-
-    train_costs.subVec(train_costs.length()-final_cost_value.length(),
-                       final_cost_value.length()) <<
-        final_cost_value;
-
-    final_cost->bpropUpdate( final_cost_input, target,
-                             final_cost_value[0],
-                             final_cost_gradient );
-    final_module->bpropUpdate( expectations[ n_layers-1 ],
-                               final_cost_input,
-                               expectation_gradients[ n_layers-1 ],
-                               final_cost_gradient );
-
     // Unsupervised greedy layer-wise cost
 
     // Set learning rates
@@ -2049,7 +2064,7 @@
                                     correlation_activation_gradients[i-1],
                                     correlation_expectation_gradients[i-1] );
 
-            connections[i-1]->bpropUpdate( expectations[i-1],
+            connections[i-1]->bpropUpdate( corrupted_autoassociator_expectations[i-1],
                                            correlation_activations[i-1],
                                            reconstruction_expectation_gradients,
                                            correlation_activation_gradients[i-1] );
@@ -2072,7 +2087,7 @@
                 reconstruction_expectation_gradients );
 
             connections[i-1]->bpropUpdate(
-                expectations[i-1],
+                corrupted_autoassociator_expectations[i-1],
                 activations[i],
                 reconstruction_expectation_gradients,
                 reconstruction_activation_gradients);
@@ -2110,6 +2125,23 @@
     }
     layers[n_layers-1]->setLearningRate( lr );
 
+
+    final_module->fprop( expectations[ n_layers-1 ],
+                         final_cost_input );
+    final_cost->fprop( final_cost_input, target, final_cost_value );
+
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) <<
+        final_cost_value;
+
+    final_cost->bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module->bpropUpdate( expectations[ n_layers-1 ],
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
     // Fine-tuning backpropagation
     if( correlation_connections.length() != 0 )
     {
@@ -2133,10 +2165,11 @@
                                     correlation_expectation_gradients[i-1] );
 
             connections[i-1]->bpropUpdate(
-                expectations[i-1],
+                corrupted_autoassociator_expectations[i-1],
                 correlation_activations[i-1],
                 expectation_gradients[i-1],
                 correlation_activation_gradients[i-1] );
+            expectation_gradients[i-1] *= binary_masks[ i-1 ];
         }
     }
     else
@@ -2148,10 +2181,11 @@
                                     activation_gradients[i],
                                     expectation_gradients[i] );
 
-            connections[i-1]->bpropUpdate( expectations[i-1],
+            connections[i-1]->bpropUpdate( corrupted_autoassociator_expectations[i-1],
                                            activations[i],
                                            expectation_gradients[i-1],
                                            activation_gradients[i] );
+            expectation_gradients[i-1] *= binary_masks[ i-1 ];
         }
     }
 }

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-01-16 21:47:31 UTC (rev 9845)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-01-17 16:19:00 UTC (rev 9846)
@@ -408,6 +408,12 @@
     //! Layers randomly masked, for unsupervised fine-tuning.
     TVec< Vec > corrupted_autoassociator_expectations;
 
+    //! Layers random binary maske, for online learning.
+    TVec< Vec > binary_masks;
+
+    //! For when corrupt_input() with binary_mask parameter is called
+    Vec tmp_mask;
+
     //! Indices of the expectation components
     TVec< TVec<int> > autoassociator_expectation_indices;
 
@@ -458,6 +464,8 @@
 
     void corrupt_input(const Vec& input, Vec& corrupted_input, int layer);
 
+    void corrupt_input(const Vec& input, Vec& corrupted_input, int layer, Vec& binary_mask);
+
     //! Global storage to save memory allocations.
     mutable Vec tmp_output;
     mutable Mat tmp_output_mat;



From larocheh at mail.berlios.de  Mon Jan 19 22:20:46 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 19 Jan 2009 22:20:46 +0100
Subject: [Plearn-commits] r9847 - trunk/plearn_learners/online
Message-ID: <200901192120.n0JLKkBe007789@sheep.berlios.de>

Author: larocheh
Date: 2009-01-19 22:20:45 +0100 (Mon, 19 Jan 2009)
New Revision: 9847

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Had forgotten to call forget on the greedy_target_connections...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-17 16:19:00 UTC (rev 9846)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-19 21:20:45 UTC (rev 9847)
@@ -927,6 +927,9 @@
             direct_connections[i]->forget();
     }
 
+    for( int i=0; i<greedy_target_connections.length(); i++ )
+        greedy_target_connections[i]->forget();
+
     stage = 0;
     unsupervised_stage = 0;
     greedy_stages.clear();



From nouiz at mail.berlios.de  Tue Jan 20 17:34:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 17:34:58 +0100
Subject: [Plearn-commits] r9848 - trunk/scripts
Message-ID: <200901201634.n0KGYwFp017902@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 17:34:57 +0100 (Tue, 20 Jan 2009)
New Revision: 9848

Modified:
   trunk/scripts/dbidispatch
Log:
removed duplicate entry in help.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-19 21:20:45 UTC (rev 9847)
+++ trunk/scripts/dbidispatch	2009-01-20 16:34:57 UTC (rev 9848)
@@ -8,19 +8,18 @@
 
 <back-end parameter>:
     bqtools, cluster option  :[--duree=X]
+    cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
+                              [--cpu=nb_cpu_per_node]
+    bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only,clusterid(condor only),processid(condor only))}+]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X]
                               [--nano=X]
-    cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
-                              [--cpu=nb_cpu_per_node]
-    bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only)}+]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                               [--[*no_]getenv] [*--[no_]prefserver] 
                               [--rank=RANK_EXPRESSION] 
                               [--files=file1[,file2...]]
                               [--env=VAR=VALUE[;VAR2=VALUE2]]
                               [--raw=CONDOR_EXPRESSION]
-                              [--tasks_filename={compact,explicit,*nb0,nb1,sh}+]
                               [*--[no_]abs_path] [--[*no_]pkdilly]
                               [*--[no_]set_special_env]
                               [--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}]
@@ -113,6 +112,9 @@
       - sh      : (condor only)parse the command for > and 2> redirection command.
                   If one or both of them are missing, they are redirected
                   to /dev/null
+      - clusterid: (condor only)put the cluster id of the jobs.
+      - processid: (condor only)put the process id of the jobs. Idem as nb0
+
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
     This must be enabled if there is not nb_proc available nodes. Otherwise 
@@ -273,7 +275,7 @@
             dbi_param["micro"]=argv[8:]
     elif argv.startswith("--tasks_filename="):
         part = argv.split('=',1)
-        accepted_value=["compact","explicit","nb0","nb1","sh"]
+        accepted_value=["compact","explicit","nb0","nb1","sh","clusterid","processid"]
         val=part[1].split(",") 
         for v in val:
             if v not in accepted_value:
@@ -282,7 +284,7 @@
         tasks_filename = val
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
-                   "--set_special_env", "--abs_path", "--pkdilly"]:
+                   "--set_special_env", "--abs_path", "--pkdilly", "--to_all"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
@@ -353,7 +355,7 @@
 elif launch_cmd=="Condor":
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
-                       "universe", "machine", "machines"]
+                       "universe", "machine", "machines", "to_all"]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["micro", "long", "duree", "queue", "nano"]
 
@@ -457,6 +459,12 @@
 else:
     (commands,choise_args)=generate_commands(command_argv)
 
+#we duplicate the command so that everything else work correctly.
+if dbi_param.has_key("to_all"):
+    assert(len(commands)==1)
+    assert(dbi_param.has_key("machine"))
+    commands=commands*len(dbi_param["machine"])
+
 if dbi_param.has_key("exp_dir"):
     dbi_param["log_dir"]=os.path.join(LOGDIR,dbi_param["exp_dir"])
 elif FILE == "":
@@ -501,6 +509,10 @@
         dbi_param[n]=merge_pattern(map(str,range(1,len(commands)+1)))
     elif pattern == "":
         pass
+    elif pattern == "clusterid":#$(Cluster)
+        dbi_param[n]=merge_pattern(["$(Cluster)"]*len(dbi_param[n]))
+    elif pattern == "processid":#$(Process)
+        dbi_param[n]=merge_pattern(["$(Process)"]*len(dbi_param[n]))
     elif pattern == "sh":
         stdouts=[]
         stderrs=[]



From nouiz at mail.berlios.de  Tue Jan 20 17:36:50 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 17:36:50 +0100
Subject: [Plearn-commits] r9849 - trunk/python_modules/plearn/parallel
Message-ID: <200901201636.n0KGaodS017962@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 17:36:49 +0100 (Tue, 20 Jan 2009)
New Revision: 9849

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-added in the previous commit the option --tasks_filename={...,clusterid, processid}
-added in the previous commit and this one a hidden option --to_all that I need for some test.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-20 16:34:57 UTC (rev 9848)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-20 16:36:49 UTC (rev 9849)
@@ -759,6 +759,7 @@
         self.universe = "vanilla"
         self.machine = []
         self.machines = []
+        self.to_all = False
 
         DBIBase.__init__(self, commands, **args)
 
@@ -1236,20 +1237,28 @@
             for i in condor_datas:
                 condor_submit_fd.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
         else:
-            def print_task(task, stdout_file, stderr_file):
+            def print_task(task, stdout_file, stderr_file,req=""):
                 argstring = condor_escape_argument(' ; '.join(task.commands))
                 condor_submit_fd.write("arguments    = %s \n" %argstring)
-                condor_submit_fd.write("output       = %s \n" %stdout_file)
-                condor_submit_fd.write("error        = %s \nqueue\n" %stderr_file)
-
+                if stdout_file:
+                    condor_submit_fd.write("output       = %s \n" %stdout_file)
+                if stderr_file:
+                    condor_submit_fd.write("error        = %s \nqueue\n" %stderr_file)
+                if req:
+                    condor_submit_fd.write("requirements   = %s\n"%(req))
+            reqs=[""]*len(self.tasks)
+            if self.to_all:
+                reqs=[]
+                for m in self.machine:
+                    reqs.append(self.req+'&&(Machine=="'+m+'")')
             if self.base_tasks_log_file:
-                for (task,task_log) in zip(self.tasks,self.base_tasks_log_file):
+                for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,reqs):
                     stdout_file=self.log_dir+"/condor"+task_log+".out"
                     stderr_file=self.log_dir+"/condor"+task_log+".err"
-                    print_task(task,stdout_file,stderr_file)
+                    print_task(task,stdout_file,stderr_file,req)
             elif self.stdouts and self.stderrs:
                 assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
-                for (task,stdout_file,stderr_file) in zip(self.tasks,self.stdouts,self.stderrs):
+                for (task,stdout_file,stderr_file,req) in zip(self.tasks,self.stdouts,self.stderrs,reqs):
                     if stdout_file==stderr_file:
                         print "Condor can't redirect the stdout and stderr to the same file!"
                         sys.exit(1)
@@ -1258,9 +1267,8 @@
                 print "DBICondor should have stdouts and stderrs or none of them"
                 sys.exit(1)
             else:
-                for task in self.tasks:
-                    argstring =condor_escape_argument(' ; '.join(task.commands))
-                    condor_submit_fd.write("arguments      = %s \nqueue\n" %argstring)
+                for (task,req) in zip(self.tasks,reqs):
+                    print_task(task, "", "", req)
         condor_submit_fd.close()
 
         self.make_launch_script('sh -c "$@"')
@@ -1314,8 +1322,9 @@
                             self.os.split(','),
                             self.req+'&&(False ')+")"
         machine_choice=[]
-        for m in self.machine:
-            machine_choice.append('(Machine=="'+m+'")')
+        if not self.to_all:
+            for m in self.machine:
+                machine_choice.append('(Machine=="'+m+'")')
         for m in self.machines:
             machine_choice.append('(regexp("'+m+'", target.Machine))')
         if machine_choice:



From nouiz at mail.berlios.de  Tue Jan 20 17:39:13 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 17:39:13 +0100
Subject: [Plearn-commits] r9850 - trunk/python_modules/plearn/parallel
Message-ID: <200901201639.n0KGdDKs018080@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 17:39:13 +0100 (Tue, 20 Jan 2009)
New Revision: 9850

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
better variable name and comment.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-20 16:36:49 UTC (rev 9849)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-20 16:39:13 UTC (rev 9850)
@@ -1246,19 +1246,21 @@
                     condor_submit_fd.write("error        = %s \nqueue\n" %stderr_file)
                 if req:
                     condor_submit_fd.write("requirements   = %s\n"%(req))
-            reqs=[""]*len(self.tasks)
+            local_req=[""]*len(self.tasks)
             if self.to_all:
-                reqs=[]
+                local_req=[]
                 for m in self.machine:
-                    reqs.append(self.req+'&&(Machine=="'+m+'")')
+                    local_req.append(self.req+'&&(Machine=="'+m+'")')
             if self.base_tasks_log_file:
-                for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,reqs):
+                for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,
+                                               local_req):
                     stdout_file=self.log_dir+"/condor"+task_log+".out"
                     stderr_file=self.log_dir+"/condor"+task_log+".err"
                     print_task(task,stdout_file,stderr_file,req)
             elif self.stdouts and self.stderrs:
                 assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
-                for (task,stdout_file,stderr_file,req) in zip(self.tasks,self.stdouts,self.stderrs,reqs):
+                for (task,stdout_file,stderr_file,req) in zip(self.tasks,self.stdouts,
+                                                              self.stderrs,local_req):
                     if stdout_file==stderr_file:
                         print "Condor can't redirect the stdout and stderr to the same file!"
                         sys.exit(1)
@@ -1267,7 +1269,7 @@
                 print "DBICondor should have stdouts and stderrs or none of them"
                 sys.exit(1)
             else:
-                for (task,req) in zip(self.tasks,reqs):
+                for (task,req) in zip(self.tasks,local_req):
                     print_task(task, "", "", req)
         condor_submit_fd.close()
 
@@ -1323,6 +1325,8 @@
                             self.req+'&&(False ')+")"
         machine_choice=[]
         if not self.to_all:
+            #we don't put them in the requirement here
+            #as they will be "local" requirement to each jobs.
             for m in self.machine:
                 machine_choice.append('(Machine=="'+m+'")')
         for m in self.machines:



From nouiz at mail.berlios.de  Tue Jan 20 18:14:59 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 18:14:59 +0100
Subject: [Plearn-commits] r9851 - trunk/python_modules/plearn/parallel
Message-ID: <200901201714.n0KHExwW021457@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 18:14:59 +0100 (Tue, 20 Jan 2009)
New Revision: 9851

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
small refactoring:
-all error use DBIError
-moved some error to be detected earlier
-local_req are not self.tasks_req in DBICondor so that both run can use it.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-20 16:39:13 UTC (rev 9850)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-20 17:14:59 UTC (rev 9851)
@@ -103,8 +103,7 @@
         if maxThreads==-1:
             nb_thread=len(argsVector)
         elif maxThreads<=0:
-            print "[DBI] you set %d concurrent jobs. Must be higher then 0!!"%(maxThreads)
-            sys.exit(1)
+            raise DBIError("[DBI] ERROR: you set %d concurrent jobs. Must be higher then 0!!"%(maxThreads))
         else:
             nb_thread=maxThreads
         if nb_thread>len(argsVector):
@@ -563,10 +562,7 @@
 
 ### We can't accept the symbols "," as this cause trouble with bqtools
         if self.log_dir.find(',')!=-1 or self.log_file.find(',')!=-1:
-            print "[DBI] ERROR: The log file and the log dir should not have the symbol ','"
-            print "[DBI] log file=",self.log_file
-            print "[DBI] log dir=",self.log_dir
-            sys.exit(1)
+            raise DBIError("[DBI] ERROR: The log file(%s) and the log dir(%s) should not have the symbol ','"%(self.log_file,self.log_dir))
 
         # create directory in which all the temp files will be created
         if not os.path.exists(self.tmp_dir):
@@ -721,8 +717,9 @@
     # escape the double quote so that dagman handle it corretly
     # DAGMAN don't handle single quote!!!
     if "'" in argstring:
-        print "[DBI] ERROR: the condor back-end with dagman don't support using the ' symbol in the command line"
-        sys.exit(1)
+        raise DBIError("[DBI] ERROR: the condor back-end with dagman don't support using the ' symbol in the command to execute")
+    if ";" in argstring:
+        raise DBIError("[DBI] ERROR: the condor back-end with dagman don't support the symbol ';' in the command to execute!")
     return argstring.replace('"',r'\\\"')
 
 class DBICondor(DBIBase):
@@ -765,13 +762,11 @@
 
         valid_universe = ["standard", "vanilla", "grid", "java", "scheduler", "local", "parallel", "vm"]
         if not self.universe in valid_universe:
-            print "[DBI] ERROR: the universe option have an invalid value",self.universe,". Valid values are:",valid_universe
-            sys.exit(1)
+            raise DBIError("[DBI] ERROR: the universe option have an invalid value",self.universe,". Valid values are:",valid_universe)
         if self.universe=="local":
             n=subprocess.Popen("cat /proc/cpuinfo |grep processor|wc -l", shell = True, stdout=PIPE).stdout.readline()
             if len(commands)>int(n):
-                print "[DBI] ERROR we refuse to start more jobs on the local universe then the total number of core. Start less jobs or use another universe."
-                sys.exit(1)
+                raise DBIError("[DBI] ERROR we refuse to start more jobs on the local universe then the total number of core. Start less jobs or use another universe.")
 
         #transform from meg to kilo
         self.mem=int(self.mem)*1024
@@ -858,7 +853,7 @@
                 newcommand+='; else '
                 newcommand+=c+".32"+c2+'; fi'
                 if not os.access(c+".64", os.X_OK):
-                    raise Exception("The command '"+c+".64' does not have execution permission!")
+                    raise DBIError("[DBI] ERROR: The command '"+c+".64' does not have execution permission!")
 #                newcommand=command
                 c+=".32"
             elif self.cplat=="INTEL" and os.path.exists(c+".32"):
@@ -877,9 +872,9 @@
                 pass
             elif not os.path.exists(c):
                 if not os.path.abspath(c):
-                    raise Exception("The command '"+c+"' does not exist!")
+                    raise DBIError("[DBI] ERROR: The command '"+c+"' does not exist!")
             elif not os.access(c, os.X_OK):
-                raise Exception("The command '"+c+"' does not have execution permission!")
+                raise DBIError("[DBI] ERROR: The command '"+c+"' does not have execution permission!")
 
             self.tasks.append(Task(newcommand, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
@@ -906,8 +901,7 @@
             if err.startswith('La tache a soumettre est dans: '):
                 pkdilly_file = err.split()[-1]
         if not pkdilly_file:
-            print "[DBI] ERROR: pkdilly didn't returned a good string"
-            sys.exit(1)
+            raise DBIError("[DBI] ERROR: pkdilly didn't returned a good string")
 
         pkdilly_fd = open( pkdilly_file, 'r' )
         lines = pkdilly_fd.readlines()
@@ -1087,6 +1081,8 @@
             os.rename(launch_tmp_file, self.launch_file)
 
     def run_dag(self):
+        if self.to_all:
+            raise DBIError("[DBI] ERROR: condor backend don't support the option --to_all and a maximum number of process")
         condor_submit_fd = open( self.condor_submit_file, 'w' )
 
         self.log_file = os.path.join("/tmp/bastienf/dbidispatch",self.log_dir)
@@ -1129,11 +1125,6 @@
 
         condor_dag_file = self.condor_submit_file+".dag"
         condor_dag_fd = open( condor_dag_file, 'w' )
-        for task in self.tasks:
-            for c in task.commands:
-                if ";" in c:
-                    print "[DBI] ERROR the option --condor=N don't support the symbol ';' in the command to execute!"
-                    sys.exit(1)
 
         if self.base_tasks_log_file:
             for i in range(len(self.tasks)):
@@ -1148,19 +1139,11 @@
         elif self.stdouts and self.stderrs:
             assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
             for (task,stdout_file,stderr_file) in zip(self.tasks,self.stdouts,self.stderrs):
-                if stdout_file==stderr_file:
-                    print "Condor can't redirect the stdout and stderr to the same file!"
-                    sys.exit(1)
                 argstring =condor_dag_escape_argument(' ; '.join(task.commands))
                 condor_dag_fd.write("JOB %d %s\n"%(i,self.condor_submit_file))
                 condor_dag_fd.write('VARS %d args="%s"\n'%(i,argstring))
                 condor_dag_fd.write('VARS %d stdout="%s"\n'%(i,stdout_file))
                 condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(i,stderr_file))
-
-
-        elif self.stdouts or self.stderrs:
-            print "DBICondor should have stdouts and stderrs or none of them"
-            sys.exit(1)
         else:
             #should not happen
             raise NotImplementedError()
@@ -1246,30 +1229,19 @@
                     condor_submit_fd.write("error        = %s \nqueue\n" %stderr_file)
                 if req:
                     condor_submit_fd.write("requirements   = %s\n"%(req))
-            local_req=[""]*len(self.tasks)
-            if self.to_all:
-                local_req=[]
-                for m in self.machine:
-                    local_req.append(self.req+'&&(Machine=="'+m+'")')
             if self.base_tasks_log_file:
                 for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,
-                                               local_req):
+                                               self.tasks_req):
                     stdout_file=self.log_dir+"/condor"+task_log+".out"
                     stderr_file=self.log_dir+"/condor"+task_log+".err"
                     print_task(task,stdout_file,stderr_file,req)
             elif self.stdouts and self.stderrs:
                 assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
                 for (task,stdout_file,stderr_file,req) in zip(self.tasks,self.stdouts,
-                                                              self.stderrs,local_req):
-                    if stdout_file==stderr_file:
-                        print "Condor can't redirect the stdout and stderr to the same file!"
-                        sys.exit(1)
+                                                              self.stderrs,self.tasks_req):
                     print_task(task,stdout_file,stderr_file)
-            elif self.stdouts or self.stderrs:
-                print "DBICondor should have stdouts and stderrs or none of them"
-                sys.exit(1)
             else:
-                for (task,req) in zip(self.tasks,local_req):
+                for (task,req) in zip(self.tasks,self.tasks_req):
                     print_task(task, "", "", req)
         condor_submit_fd.close()
 
@@ -1288,10 +1260,16 @@
                     pass
                 pass
 
-
     def run(self):
         if (self.pkdilly and self.nb_proc > 0):
-            print "curently pkdilly with nb_proc >0 is not supported!"
+            raise DBIError("[DBI] ERROR: curently pkdilly with nb_proc >0 is not supported!")
+        if (self.stdouts and not self.stderrs) or (self.stderrs and not self.stdouts)
+            raise DBIError("[DBI] ERROR: the condor back-end should have both stdouts and stderrs or none of them")
+        if self.stdouts and self.stderrs:
+            assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
+            for (stdout_file,stderr_file) in zip(self.stdouts, self.stderrs):
+                if stdout_file==stderr_file:
+                    raise DBIError("[DBI] ERROR: the condor back-end can't redirect the stdout and stderr to the same file!")
 
         print "[DBI] The Log file are under %s"%self.log_dir
         if self.source_file and self.source_file.endswith(".cshrc"):
@@ -1324,11 +1302,17 @@
                             self.os.split(','),
                             self.req+'&&(False ')+")"
         machine_choice=[]
+        self.tasks_req=[""]*len(self.tasks)
         if not self.to_all:
             #we don't put them in the requirement here
             #as they will be "local" requirement to each jobs.
             for m in self.machine:
                 machine_choice.append('(Machine=="'+m+'")')
+        else:
+            assert(len(self.machines)==0)
+            for m in self.machine:
+                self.tasks_req.append(self.req+'&&(Machine=="'+m+'")')
+
         for m in self.machines:
             machine_choice.append('(regexp("'+m+'", target.Machine))')
         if machine_choice:
@@ -1446,7 +1430,7 @@
             # same architecture as the architecture of the launch computer
 
             if not os.access(c, os.X_OK):
-                raise Exception("The command '"+c+"' does not exist or does not have execution permission!")
+                raise DBIError("[DBI] ERROR: The command '"+c+"' does not exist or does not have execution permission!")
             self.tasks.append(Task(command, tmp_dir, log_dir,
                                    time_format, pre_tasks,
                                    post_tasks,dolog,id,False,self.args))
@@ -1609,8 +1593,7 @@
 def find_all_ssh_hosts():
     hostspath_list = [os.path.join(os.getenv("HOME"),".pymake",get_platform()+'.hosts')]
     if os.path.exists(hostspath_list[0])==0:
-        print "[DBI] no host file %s for the ssh backend"%(hostspath_list[0])
-        sys.exit(1)
+        raise DBIError("[DBI] ERROR: no host file %s for the ssh backend"%(hostspath_list[0]))
     print "[DBI] using file %s for the list of host"%(hostspath_list[0])
 #    from plearn.pymake.pymake import process_hostspath_list
 #    (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list,19,get_hostname())



From nouiz at mail.berlios.de  Tue Jan 20 18:59:59 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 18:59:59 +0100
Subject: [Plearn-commits] r9852 - trunk/plearn_learners/meta
Message-ID: <200901201759.n0KHxxdW021033@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 18:59:49 +0100 (Tue, 20 Jan 2009)
New Revision: 9852

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
use the PLearner::will_train flags to do some optimization.(don't allocate memory that won't be used.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-20 17:14:59 UTC (rev 9851)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-20 17:59:49 UTC (rev 9852)
@@ -1063,10 +1063,11 @@
         if(training_set->classname()!="RegressionTreeRegisters")
             training_set = new RegressionTreeRegisters(training_set,
                                                        report_progress,
-                                                       verbosity);
+                                                       verbosity,
+                                                       will_train, will_train);
 
         //we need to change the weight of the trainning set to reuse the RegressionTreeRegister
-        if(!modif_train_set_weights)
+        if(!modif_train_set_weights){
             if(training_set->weightsize()==1)
                 modif_train_set_weights=1;
             else
@@ -1075,6 +1076,7 @@
                           <<" training_set don't have a weigth. This will cause"
                           <<" the creation of a RegressionTreeRegisters at"
                           <<" each stage of AdaBoost!";
+        }
         //we do this as RegressionTreeNode need a train_set for getTestCostNames
         if(!weak_learner_template->getTrainingSet())
             weak_learner_template->setTrainingSet(training_set,call_forget);

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-20 17:14:59 UTC (rev 9851)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-20 17:59:49 UTC (rev 9852)
@@ -585,7 +585,7 @@
                                                 t1->getTSortedRow(),
                                                 t1->getTSource(),
                                                 learner1->report_progress,
-                                                learner1->verbosity);
+                                                learner1->verbosity,false);
         }
         learner2->setTrainingSet(vmat2, call_forget);
     }



From larocheh at mail.berlios.de  Tue Jan 20 19:31:07 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 20 Jan 2009 19:31:07 +0100
Subject: [Plearn-commits] r9853 - trunk/plearn_learners/online
Message-ID: <200901201831.n0KIV766024615@sheep.berlios.de>

Author: larocheh
Date: 2009-01-20 19:31:07 +0100 (Tue, 20 Jan 2009)
New Revision: 9853

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Had forgotten to set learning rate of greedy_target_connections in setLearningRate method...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-20 17:59:49 UTC (rev 9852)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-20 18:31:07 UTC (rev 9853)
@@ -2752,6 +2752,10 @@
         }
         reconstruction_connections[i]->setLearningRate( the_learning_rate );
     }
+
+    for( int i=0; i<greedy_target_connections.length(); i++ )
+        greedy_target_connections[i]->setLearningRate( the_learning_rate );
+
     layers[n_layers-1]->setLearningRate( the_learning_rate );
 
     final_cost->setLearningRate( the_learning_rate );



From nouiz at mail.berlios.de  Tue Jan 20 19:37:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 19:37:58 +0100
Subject: [Plearn-commits] r9854 - trunk/plearn_learners/meta
Message-ID: <200901201837.n0KIbwqi014116@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 19:37:58 +0100 (Tue, 20 Jan 2009)
New Revision: 9854

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
in MultiClassAdaBoost::finalize(),
call inherited::finalize();


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-20 18:31:07 UTC (rev 9853)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-20 18:37:58 UTC (rev 9854)
@@ -230,22 +230,13 @@
 
 void MultiClassAdaBoost::finalize()
 {
+    inherited::finalize();
     learner1->finalize();
     learner2->finalize();
 }
 
 void MultiClassAdaBoost::forget()
 {
-    //! (Re-)initialize the PLearner in its fresh state (that state may depend
-    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
-    //! a fresh learner!)
-    /*!
-      A typical forget() method should do the following:
-      - call inherited::forget() to initialize its random number generator
-        with the 'seed' option
-      - initialize the learner's parameters, using this random generator
-      - stage = 0
-    */
     inherited::forget();
 
     stage = 0;



From nouiz at mail.berlios.de  Tue Jan 20 19:44:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 19:44:06 +0100
Subject: [Plearn-commits] r9855 - trunk/plearn_learners/regressors
Message-ID: <200901201844.n0KIi6Xg016325@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 19:44:05 +0100 (Tue, 20 Jan 2009)
New Revision: 9855

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
added two option that we can use to lower the memory used if the learner won't be trained.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-20 18:37:58 UTC (rev 9854)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-20 18:44:05 UTC (rev 9855)
@@ -60,7 +60,9 @@
 RegressionTreeRegisters::RegressionTreeRegisters():
     report_progress(0),
     verbosity(0),
-    next_id(0)
+    next_id(0),
+    do_sort_rows(true),
+    mem_tsource(true)
 {
     build();
 }
@@ -69,10 +71,14 @@
                                                  TMat<RTR_type> tsorted_row_,
                                                  VMat tsource_,
                                                  bool report_progress_,
-                                                 bool verbosity_):
+                                                 bool verbosity_,
+                                                 bool do_sort_rows_,
+                                                 bool mem_tsource_):
     report_progress(report_progress_),
     verbosity(verbosity_),
-    next_id(0)
+    next_id(0),
+    do_sort_rows(do_sort_rows_),
+    mem_tsource(mem_tsource_)
 {
     source = source_;
     tsource = tsource_;
@@ -82,10 +88,14 @@
 
 RegressionTreeRegisters::RegressionTreeRegisters(VMat source_,
                                                  bool report_progress_,
-                                                 bool verbosity_):
+                                                 bool verbosity_,
+                                                 bool do_sort_rows_,
+                                                 bool mem_tsource_):
     report_progress(report_progress_),
     verbosity(verbosity_),
-    next_id(0)
+    next_id(0),
+    do_sort_rows(do_sort_rows_),
+    mem_tsource(mem_tsource_)
 {
     source = source_;
     build();
@@ -114,6 +124,15 @@
     declareOption(ol, "leave_register", &RegressionTreeRegisters::leave_register, OptionBase::learntoption,
                   "The vector identifying the leave to which, each row belongs\n");
 
+    declareOption(ol, "do_sort_rows", &RegressionTreeRegisters::do_sort_rows,
+                  OptionBase::buildoption,
+                  "Do we generate the sorted rows? Not usefull if used only to test.\n");
+
+    declareOption(ol, "mem_tsource", &RegressionTreeRegisters::mem_tsource,
+                  OptionBase::buildoption,
+                  "Do we put the tsource in memory? default to true as this"
+                  " give an great speed up for the trainning of RegressionTree.\n");
+
     //too big to save
     declareOption(ol, "tsorted_row", &RegressionTreeRegisters::tsorted_row, OptionBase::nosave,
                   "The matrix holding the sequence of samples in ascending value order for each dimension\n");
@@ -154,11 +173,13 @@
     PLCHECK(source->inputsize()>0);
 
     if(!tsource){
-        VMat tmp = VMat(new TransposeVMatrix(new SubVMatrix(
-                                                 source, 0,0,source->length(),
-                                                 source->inputsize())));
-        PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
-        tsource = VMat(tmp2 );
+        tsource = VMat(new TransposeVMatrix(new SubVMatrix(
+                                                source, 0,0,source->length(),
+                                                source->inputsize())));
+        if(do_sort_rows){
+            PP<MemoryVMatrixNoSave> tmp = new MemoryVMatrixNoSave(tsource);
+            tsource = VMat(tmp);
+        }
     }
     setMetaInfoFrom(source);
     weightsize_=1;
@@ -194,7 +215,7 @@
     getAllRegisteredRow(leave_id,col,reg);
     target.resize(reg.length());
     weight.resize(reg.length());
-    value.resize(reg.length());
+    value.resize(reg.length());    
     if(weightsize() <= 0){
         weight.fill(1.0 / length());
         for(int i=0;i<reg.length();i++){            
@@ -239,6 +260,8 @@
 void RegressionTreeRegisters::sortRows()
 {
     next_id = 0;
+    if(!do_sort_rows)
+        return;
     if (tsorted_row.length() == inputsize() && tsorted_row.width() == length())
     {
         verbose("RegressionTreeRegisters: Sorted train set indices are present, no sort required", 3);

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-01-20 18:37:58 UTC (rev 9854)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-01-20 18:44:05 UTC (rev 9855)
@@ -71,14 +71,12 @@
 
     int  report_progress;
     int  verbosity;
-//    VMat train_set;
   
 /*
   Learnt options: they are sized and initialized if need be, at build() or reinitRegisters()
 */
 
     int       next_id;
-    //TMat<int> sorted_row;
 
     TMat<RTR_type> tsorted_row;
     TVec<RTR_type_id> leave_register;
@@ -88,14 +86,19 @@
     TVec<pair<real,real> > target_weight;
     VMat source;
 
+    bool do_sort_rows;
+    bool mem_tsource;
+
 public:
 
     RegressionTreeRegisters();
     RegressionTreeRegisters(VMat source_, bool report_progress_ = false,
-                            bool vebosity_ = false);
+                            bool vebosity_ = false, bool do_sort_rows = true,
+                            bool mem_tsource_ = true);
     RegressionTreeRegisters(VMat source_, TMat<RTR_type> tsorted_row_,
                             VMat tsource_, bool report_progress_ = false,
-                            bool vebosity_ = false);
+                            bool vebosity_ = false, bool do_sort_rows = true,
+                            bool mem_tsource_ = true);
     virtual              ~RegressionTreeRegisters();
     
     PLEARN_DECLARE_OBJECT(RegressionTreeRegisters);



From nouiz at mail.berlios.de  Tue Jan 20 19:48:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 19:48:29 +0100
Subject: [Plearn-commits] r9856 - trunk/plearn_learners/meta
Message-ID: <200901201848.n0KImTWY017130@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 19:48:29 +0100 (Tue, 20 Jan 2009)
New Revision: 9856

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
revert commit that use fonctionnality not commited.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-20 18:44:05 UTC (rev 9855)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-20 18:48:29 UTC (rev 9856)
@@ -1063,8 +1063,7 @@
         if(training_set->classname()!="RegressionTreeRegisters")
             training_set = new RegressionTreeRegisters(training_set,
                                                        report_progress,
-                                                       verbosity,
-                                                       will_train, will_train);
+                                                       verbosity);
 
         //we need to change the weight of the trainning set to reuse the RegressionTreeRegister
         if(!modif_train_set_weights){



From nouiz at mail.berlios.de  Tue Jan 20 20:04:34 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 20:04:34 +0100
Subject: [Plearn-commits] r9857 - in trunk/plearn_learners:
	meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir
	meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0
	meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir
	meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0
	meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir
	meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0
	regressors
	regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
	regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
	regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir
Message-ID: <200901201904.n0KJ4Y3e019071@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 20:04:33 +0100 (Tue, 20 Jan 2009)
New Revision: 9857

Modified:
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave
Log:
changed the default value for RegressionTreeLeave::id to be 0 instead of -1. This allow using an unsigned type for it. This is safe as we must provide one id when we init a leave.


Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1571,7 +1571,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
 leave_template = *102 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -1617,6 +1617,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9752"
+__REVISION__ = "PL9855"
 conf                                          = False
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -59,7 +59,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
 leave_template = *8 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -105,6 +105,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1571,7 +1571,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
 leave_template = *102 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -1617,6 +1617,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9752"
+__REVISION__ = "PL9855"
 conf                                          = True
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -59,7 +59,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
 leave_template = *8 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -105,6 +105,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1571,7 +1571,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
 leave_template = *102 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -1617,6 +1617,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9752"
+__REVISION__ = "PL9855"
 conf                                          = False
 pseudo                                        = True

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -59,7 +59,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
 leave_template = *8 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -105,6 +105,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-01-20 19:04:33 UTC (rev 9857)
@@ -57,7 +57,7 @@
 RegressionTreeLeave::RegressionTreeLeave():
     missing_leave(false),
     loss_function_weight(0),
-    id(-1),
+    id(0),
     length_(0),
     weights_sum(0),
     targets_sum(0),

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL9485"
+__REVISION__ = "PL9855"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -66,7 +66,7 @@
 multiclass_outputs = []
 ;
 leave_template = *8 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -104,6 +104,7 @@
 report_stats = 1 ;
 save_initial_tester = 0 ;
 save_stat_collectors = 1 ;
+save_split_stats = 1 ;
 save_learners = 0 ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
@@ -119,7 +120,9 @@
 final_commands = []
 ;
 save_test_confidence = 0 ;
-enforce_clean_expdir = 1  )
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
@@ -188,6 +191,7 @@
 report_stats = 1 ;
 save_initial_tester = 1 ;
 save_stat_collectors = 1 ;
+save_split_stats = 1 ;
 save_learners = 0 ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
@@ -203,4 +207,6 @@
 final_commands = []
 ;
 save_test_confidence = 0 ;
-enforce_clean_expdir = 1  )
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9485"
+__REVISION__ = "PL9855"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -81,7 +81,7 @@
 ;
 l1_loss_function_factor = 0 ;
 l2_loss_function_factor = 0 ;
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -119,6 +119,7 @@
 report_stats = 1 ;
 save_initial_tester = 0 ;
 save_stat_collectors = 1 ;
+save_split_stats = 1 ;
 save_learners = 0 ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
@@ -134,7 +135,9 @@
 final_commands = []
 ;
 save_test_confidence = 0 ;
-enforce_clean_expdir = 1  )
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
@@ -203,6 +206,7 @@
 report_stats = 1 ;
 save_initial_tester = 1 ;
 save_stat_collectors = 1 ;
+save_split_stats = 1 ;
 save_learners = 0 ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
@@ -218,4 +222,6 @@
 final_commands = []
 ;
 save_test_confidence = 0 ;
-enforce_clean_expdir = 1  )
+enforce_clean_expdir = 1 ;
+redirect_stdout = 0 ;
+redirect_stderr = 0  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/metainfos.txt	2009-01-20 19:04:33 UTC (rev 9857)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL9601"
+__REVISION__ = "PL9855"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave	2009-01-20 18:48:29 UTC (rev 9856)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClassOutput/expected_results/expdir/tester.psave	2009-01-20 19:04:33 UTC (rev 9857)
@@ -48,7 +48,7 @@
 output_confidence_target = 1 ;
 multiclass_outputs = 4 [ 0 1 2 3 ] ;
 leave_template = *7 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;



From larocheh at mail.berlios.de  Tue Jan 20 20:07:14 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 20 Jan 2009 20:07:14 +0100
Subject: [Plearn-commits] r9858 - trunk/plearn_learners/online
Message-ID: <200901201907.n0KJ7ESn019335@sheep.berlios.de>

Author: larocheh
Date: 2009-01-20 20:07:13 +0100 (Tue, 20 Jan 2009)
New Revision: 9858

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Replaced the option 'use_mean_field_contrastive_divergence' to 'mean_field_contrastive_divergence_ratio', which lets you balance ordinary CD and MF-CD.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2009-01-20 19:04:33 UTC (rev 9857)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2009-01-20 19:07:13 UTC (rev 9858)
@@ -74,7 +74,7 @@
     online ( false ),
     background_gibbs_update_ratio(0),
     gibbs_chain_reinit_freq( INT_MAX ),
-    use_mean_field_contrastive_divergence( false ),
+    mean_field_contrastive_divergence_ratio( 0 ),
     train_stats_window( -1 ),
     minibatch_size( 0 ),
     initialize_gibbs_chain( false ),
@@ -279,11 +279,12 @@
                   "If == INT_MAX, the default value of this option, then NEVER\n"
                   "re-initialize except at the beginning, when stage==0.\n");
 
-    declareOption(ol, "use_mean_field_contrastive_divergence",
-                  &DeepBeliefNet::use_mean_field_contrastive_divergence,
+    declareOption(ol, "mean_field_contrastive_divergence_ratio",
+                  &DeepBeliefNet::mean_field_contrastive_divergence_ratio,
                   OptionBase::buildoption,
-                  "Indication that mean-field contrastive divergence\n"
-                  "should be used instead of standard contrastive divergence.\n");
+                  "Coefficient between 0 and 1. 0 means CD-1 update only and\n"
+                  "1 means MF-CD only. Values in between means a weighted\n" 
+                  "combination of both.\n");
 
     declareOption(ol, "train_stats_window",
                   &DeepBeliefNet::train_stats_window,
@@ -364,20 +365,21 @@
         PLERROR("In DeepBeliefNet::build_ - up-down algorithm not implemented "
             "for minibatch setting.");
 
-    if( use_mean_field_contrastive_divergence && up_down_nstages > 0 )
-        PLERROR("In DeepBeliefNet::build_ - up-down algorithm not implemented "
-            "for mean-field CD.");
-
-    if( use_mean_field_contrastive_divergence &&
+    if( mean_field_contrastive_divergence_ratio > 0 &&
         background_gibbs_update_ratio != 0 )
         PLERROR("In DeepBeliefNet::build_ - mean-field CD cannot be used "
                 "with background_gibbs_update_ratio != 0.");
 
-    if( use_mean_field_contrastive_divergence &&
+    if( mean_field_contrastive_divergence_ratio > 0 &&
         use_sample_for_up_layer )
         PLERROR("In DeepBeliefNet::build_ - mean-field CD cannot be used "
-                "with use_sample_for_top_layer");
+                "with use_sample_for_up_layer.");
 
+    if( mean_field_contrastive_divergence_ratio < 0 ||
+        mean_field_contrastive_divergence_ratio > 1 )
+        PLERROR("In DeepBeliefNet::build_ - mean_field_contrastive_divergence_ratio should "
+            "be in [0,1].");
+
     if( !online )
     {
         if( training_schedule.length() != n_layers )
@@ -526,12 +528,6 @@
     expectations_gradients.resize( n_layers );
     gibbs_down_state.resize( n_layers-1 );
 
-    if( up_down_nstages > 0 )
-    {
-        up_sample.resize(n_layers);
-        down_sample.resize(n_layers);
-    }
-
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         if( layers[i]->size != connections[i]->down_size )
@@ -560,21 +556,11 @@
         activation_gradients[i].resize( layers[i]->size );
         expectation_gradients[i].resize( layers[i]->size );
 
-        if( up_down_nstages > 0 )
-        {
-            up_sample[i].resize(layers[i]->size);
-            down_sample[i].resize(layers[i]->size);
-        }
     }
     if( !(layers[n_layers-1]->random_gen) )
     {
         layers[n_layers-1]->random_gen = random_gen;
         layers[n_layers-1]->forget();
-        if( up_down_nstages > 0 )
-        {
-            up_sample[n_layers-1].resize(layers[n_layers-1]->size);
-            down_sample[n_layers-1].resize(layers[n_layers-1]->size);
-        }
     }
     int last_layer_size = layers[n_layers-1]->size;
     PLASSERT_MSG(last_layer_size >= 0,
@@ -769,6 +755,10 @@
     deepCopyField(pos_up_val,               copies);
     deepCopyField(cd_neg_up_vals,           copies);
     deepCopyField(cd_neg_down_vals,         copies);
+    deepCopyField(mf_cd_neg_up_vals,        copies);
+    deepCopyField(mf_cd_neg_down_vals,      copies);
+    deepCopyField(mf_cd_neg_up_val,         copies);
+    deepCopyField(mf_cd_neg_down_val,       copies);
     deepCopyField(gibbs_down_state,         copies);
     deepCopyField(optimized_costs,          copies);
     deepCopyField(reconstruction_costs,     copies);
@@ -1084,6 +1074,15 @@
                     wt->build();
                     generative_connections[c] = wt;
                 }
+
+                up_sample.resize(n_layers);
+                down_sample.resize(n_layers);
+                
+                for( int i=0 ; i<n_layers ; i++ )
+                {
+                    up_sample[i].resize(layers[i]->size);
+                    down_sample[i].resize(layers[i]->size);
+                }
             }
             /***** up-down algorithm *****/
             MODULE_LOG << "Up-down gradient descent algorithm" << endl;
@@ -2264,8 +2263,8 @@
         pos_up_vals.resize(minibatch_size, up_layer->size);
 
         pos_down_vals << down_layer->getExpectations();
-        if( !use_sample_for_up_layer )
-            pos_up_vals << up_layer->getExpectations();
+        pos_up_vals << up_layer->getExpectations();
+        up_layer->generateSamples();
 
         // down propagation, starting from a sample of up_layer
         if (background_gibbs_update_ratio<1)
@@ -2273,8 +2272,11 @@
         {
             Mat neg_down_vals;
             Mat neg_up_vals;
-            if( use_mean_field_contrastive_divergence )
+            if( mean_field_contrastive_divergence_ratio > 0 )
             {
+                mf_cd_neg_down_vals.resize(minibatch_size, down_layer->size);
+                mf_cd_neg_up_vals.resize(minibatch_size, up_layer->size);
+
                 connection->setAsUpInputs( up_layer->getExpectations() );
                 down_layer->getAllActivations( connection, 0, true );
                 down_layer->computeExpectations();
@@ -2283,12 +2285,12 @@
                 up_layer->getAllActivations( connection, 0, mbatch );
                 up_layer->computeExpectations();
 
-                neg_down_vals = down_layer->getExpectations();
-                neg_up_vals = up_layer->getExpectations();
+                mf_cd_neg_down_vals << down_layer->getExpectations();
+                mf_cd_neg_up_vals << up_layer->getExpectations();
             }
-            else
+            
+            if( mean_field_contrastive_divergence_ratio <  1 )
             {
-                up_layer->generateSamples();
                 if( use_sample_for_up_layer )
                     pos_up_vals << up_layer->samples;
                 connection->setAsUpInputs( up_layer->samples );
@@ -2313,10 +2315,45 @@
             if (background_gibbs_update_ratio==0)
             // update here only if there is ONLY contrastive divergence
             {
-                down_layer->update( pos_down_vals, neg_down_vals );
-                connection->update( pos_down_vals, pos_up_vals,
-                                    neg_down_vals, neg_up_vals );
-                up_layer->update( pos_up_vals, neg_up_vals );
+                if( mean_field_contrastive_divergence_ratio < 1 )
+                {
+                    real lr_dl = down_layer->learning_rate;
+                    real lr_ul = up_layer->learning_rate;
+                    real lr_c = connection->learning_rate;
+
+                    down_layer->setLearningRate(lr_dl * (1-mean_field_contrastive_divergence_ratio));
+                    up_layer->setLearningRate(lr_ul * (1-mean_field_contrastive_divergence_ratio));
+                    connection->setLearningRate(lr_c * (1-mean_field_contrastive_divergence_ratio));
+
+                    down_layer->update( pos_down_vals, neg_down_vals );
+                    connection->update( pos_down_vals, pos_up_vals,
+                                        neg_down_vals, neg_up_vals );
+                    up_layer->update( pos_up_vals, neg_up_vals );
+
+                    down_layer->setLearningRate(lr_dl);
+                    up_layer->setLearningRate(lr_ul);
+                    connection->setLearningRate(lr_c);
+                }
+
+                if( mean_field_contrastive_divergence_ratio > 0 )
+                {
+                    real lr_dl = down_layer->learning_rate;
+                    real lr_ul = up_layer->learning_rate;
+                    real lr_c = connection->learning_rate;
+
+                    down_layer->setLearningRate(lr_dl * mean_field_contrastive_divergence_ratio);
+                    up_layer->setLearningRate(lr_ul * mean_field_contrastive_divergence_ratio);
+                    connection->setLearningRate(lr_c * mean_field_contrastive_divergence_ratio);
+
+                    down_layer->update( pos_down_vals, mf_cd_neg_down_vals );
+                    connection->update( pos_down_vals, pos_up_vals,
+                                        mf_cd_neg_down_vals, mf_cd_neg_up_vals );
+                    up_layer->update( pos_up_vals, mf_cd_neg_up_vals );
+
+                    down_layer->setLearningRate(lr_dl);
+                    up_layer->setLearningRate(lr_ul);
+                    connection->setLearningRate(lr_c);
+                }
             }
             else
             {
@@ -2382,59 +2419,96 @@
             down_state << down_layer->samples;
         }
     } else {
-        if( !use_mean_field_contrastive_divergence )
-            up_layer->generateSample();
-
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_val.resize( down_layer->size );
         pos_up_val.resize( up_layer->size );
 
+        Vec neg_down_val;
+        Vec neg_up_val;
+
         pos_down_val << down_layer->expectation;
-        if( use_sample_for_up_layer )
-            pos_up_val << up_layer->sample;
-        else
-            pos_up_val << up_layer->expectation;
-
+        pos_up_val << up_layer->expectation;
+        up_layer->generateSample();
+            
+        // negative phase
         // down propagation, starting from a sample of up_layer
-        if( use_mean_field_contrastive_divergence )
+        if( mean_field_contrastive_divergence_ratio > 0 )
+        {
             connection->setAsUpInput( up_layer->expectation );
-        else
+            down_layer->getAllActivations( connection );
+            down_layer->computeExpectation();
+            connection->setAsDownInput( down_layer->expectation );
+            up_layer->getAllActivations( connection, 0, mbatch );
+            up_layer->computeExpectation();
+            mf_cd_neg_down_val.resize( down_layer->size );
+            mf_cd_neg_up_val.resize( up_layer->size );
+            mf_cd_neg_down_val << down_layer->expectation;
+            mf_cd_neg_up_val << up_layer->expectation;
+        }
+
+        if( mean_field_contrastive_divergence_ratio < 1 )
+        {
+            if( use_sample_for_up_layer )
+                pos_up_val << up_layer->sample;
             connection->setAsUpInput( up_layer->sample );
-
-        down_layer->getAllActivations( connection );
-        down_layer->computeExpectation();
-        if( !use_mean_field_contrastive_divergence )
+            down_layer->getAllActivations( connection );
+            down_layer->computeExpectation();
             down_layer->generateSample();
+            connection->setAsDownInput( down_layer->sample );
+            up_layer->getAllActivations( connection, 0, mbatch );
+            up_layer->computeExpectation();
 
-        // negative phase
-        if( use_mean_field_contrastive_divergence )
-            connection->setAsDownInput( down_layer->expectation );
-        else
-            connection->setAsDownInput( down_layer->sample );
-        up_layer->getAllActivations( connection, 0, mbatch );
-        up_layer->computeExpectation();
-        // accumulate negative stats
-        // no need to deep-copy because the values won't change before update
-        Vec neg_down_val;
-        if( use_mean_field_contrastive_divergence )
-            neg_down_val = down_layer->expectation;
-        else
             neg_down_val = down_layer->sample;
-        Vec neg_up_val;
-        if( use_sample_for_up_layer )
+            if( use_sample_for_up_layer )
+            {
+                up_layer->generateSample();
+                neg_up_val = up_layer->sample;
+            }
+            else
+                neg_up_val = up_layer->expectation;
+        }
+
+        // update
+        if( mean_field_contrastive_divergence_ratio < 1 )
         {
-            up_layer->generateSample();
-            neg_up_val = up_layer->sample;
+            real lr_dl = down_layer->learning_rate;
+            real lr_ul = up_layer->learning_rate;
+            real lr_c = connection->learning_rate;
+            
+            down_layer->setLearningRate(lr_dl * (1-mean_field_contrastive_divergence_ratio));
+            up_layer->setLearningRate(lr_ul * (1-mean_field_contrastive_divergence_ratio));
+            connection->setLearningRate(lr_c * (1-mean_field_contrastive_divergence_ratio));
+            
+            down_layer->update( pos_down_val, neg_down_val );
+            connection->update( pos_down_val, pos_up_val,
+                                neg_down_val, neg_up_val );
+            up_layer->update( pos_up_val, neg_up_val );
+            
+            down_layer->setLearningRate(lr_dl);
+            up_layer->setLearningRate(lr_ul);
+            connection->setLearningRate(lr_c);
         }
-        else
-            neg_up_val = up_layer->expectation;
 
-        // update
-        down_layer->update( pos_down_val, neg_down_val );
-        connection->update( pos_down_val, pos_up_val,
-                neg_down_val, neg_up_val );
-        up_layer->update( pos_up_val, neg_up_val );
+        if( mean_field_contrastive_divergence_ratio > 0 )
+        {
+            real lr_dl = down_layer->learning_rate;
+            real lr_ul = up_layer->learning_rate;
+            real lr_c = connection->learning_rate;
+            
+            down_layer->setLearningRate(lr_dl * mean_field_contrastive_divergence_ratio);
+            up_layer->setLearningRate(lr_ul * mean_field_contrastive_divergence_ratio);
+            connection->setLearningRate(lr_c * mean_field_contrastive_divergence_ratio);
+            
+            down_layer->update( pos_down_val, mf_cd_neg_down_val );
+            connection->update( pos_down_val, pos_up_val,
+                                mf_cd_neg_down_val, mf_cd_neg_up_val );
+            up_layer->update( pos_up_val, mf_cd_neg_up_val );
+            
+            down_layer->setLearningRate(lr_dl);
+            up_layer->setLearningRate(lr_ul);
+            connection->setLearningRate(lr_c);
+        }
     }
 }
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2009-01-20 19:04:33 UTC (rev 9857)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2009-01-20 19:07:13 UTC (rev 9858)
@@ -194,9 +194,10 @@
     //! stage==0)
     int gibbs_chain_reinit_freq;
 
-    //! Indication that mean-field contrastive divergence
-    //! should be used instead of standard contrastive divergence.
-    bool use_mean_field_contrastive_divergence;
+    //! Coefficient between 0 and 1. 0 means CD-1 update only and
+    //! 1 means MF-CD only. Values in between means a weighted 
+    //! combination of both.
+    real mean_field_contrastive_divergence_ratio;
 
     //! The number of samples to use to compute training stats.
     //! -1 (default) means the number of training samples.
@@ -370,6 +371,10 @@
     mutable Mat pos_up_vals;
     mutable Mat cd_neg_down_vals;
     mutable Mat cd_neg_up_vals;
+    mutable Mat mf_cd_neg_down_vals;
+    mutable Mat mf_cd_neg_up_vals;
+    mutable Vec mf_cd_neg_down_val;
+    mutable Vec mf_cd_neg_up_val;
 
     //! Store the state of the Gibbs chain for each RBM
     mutable TVec<Mat> gibbs_down_state;



From nouiz at mail.berlios.de  Tue Jan 20 20:25:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 20:25:58 +0100
Subject: [Plearn-commits] r9859 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results:
	. expdir expdir/Split0
Message-ID: <200901201925.n0KJPwBE020838@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 20:25:57 +0100 (Tue, 20 Jan 2009)
New Revision: 9859

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
updated test following commit about RegressionTreeLeave::id default value of 0.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/RUN.log	2009-01-20 19:25:57 UTC (rev 9859)
@@ -1,121 +1,121 @@
 HyperLearner: starting the optimization
 split_cols: 2 2 2 
 split_values: 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 
-weak learner at stage 0 has average loss = 0.02
+weak learner at stage 0 has average loss = 0.0200000000000000004
 split_cols: 2 1 2 
 split_values: 0.991025168386145405 0.482293993618237549 0.891579732096156263 
-weak learner at stage 0 has average loss = 0.08
+weak learner at stage 0 has average loss = 0.0800000000000000017
 split_cols: 4 3 2 
 split_values: 3.23307269844974599e-10 0.698651129400676418 0.000537488498421501149 
-weak learner at stage 1 has average loss = 0.0306122
+weak learner at stage 1 has average loss = 0.0306122448979592558
 split_cols: 4 1 1 
 split_values: 1.54709578481515564e-13 0.588552410435003615 0.568316236782665074 
-weak learner at stage 1 has average loss = 0.13587
+weak learner at stage 1 has average loss = 0.135869565217391769
 split_cols: 2 0 3 
 split_values: 0.000528285193333644099 0.624507340564582236 0.406995257960652612 
-weak learner at stage 2 has average loss = 0.136257
+weak learner at stage 2 has average loss = 0.136257309941520577
 split_cols: 1 3 2 
 split_values: 0.526575453102100632 0.924226804347039965 0.995245802370935517 
-weak learner at stage 2 has average loss = 0.204444
+weak learner at stage 2 has average loss = 0.204444444444444567
 split_cols: 2 3 1 
 split_values: 0.000528285193333644099 0.406995257960652612 0.399391565505198165 
-weak learner at stage 3 has average loss = 0.119855
+weak learner at stage 3 has average loss = 0.119854943177360632
 split_cols: 2 0 3 
 split_values: 0.997650553369808346 0.441781515973124872 0.310957185430505323 
-weak learner at stage 3 has average loss = 0.179198
+weak learner at stage 3 has average loss = 0.179197735115788404
 split_cols: 2 1 2 
 split_values: 0.196634593877310471 0.36967671457248541 0.00125079586853901747 
-weak learner at stage 4 has average loss = 0.103659
+weak learner at stage 4 has average loss = 0.103659200908568977
 split_cols: 3 3 2 
 split_values: 0.141930657011306749 0.0564509465831030677 0.997650553369808346 
-weak learner at stage 4 has average loss = 0.27129
+weak learner at stage 4 has average loss = 0.271289700162368308
 split_cols: 0 3 1 
 split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
-weak learner at stage 5 has average loss = 0.0730531
+weak learner at stage 5 has average loss = 0.0730530888838650527
 split_cols: 2 4 4 
 split_values: 0.00363231682035125569 0.999999964757892545 0.999999999538717876 
-weak learner at stage 5 has average loss = 0.217388
+weak learner at stage 5 has average loss = 0.217387633059202834
 split_cols: 2 0 3 
 split_values: 0.00125079586853901747 0.624507340564582236 0.763338630405507312 
-weak learner at stage 6 has average loss = 0.140932
+weak learner at stage 6 has average loss = 0.140931903580067019
 split_cols: 1 1 1 
 split_values: 0.564858493389006289 0.479480044756096346 0.379258691801199865 
-weak learner at stage 6 has average loss = 0.262423
+weak learner at stage 6 has average loss = 0.262422978648221006
 split_cols: 2 3 1 
 split_values: 0.000357032461916012567 0.130712305658957473 0.399391565505198165 
-weak learner at stage 7 has average loss = 0.214419
+weak learner at stage 7 has average loss = 0.214419375868369705
 split_cols: 4 1 0 
 split_values: 0.999999999999999334 0.569140400436275673 0.318872618050356049 
-weak learner at stage 7 has average loss = 0.26615
+weak learner at stage 7 has average loss = 0.266150190899056949
 split_cols: 1 3 1 
 split_values: 0.401581050193795197 0.360834998492078562 0.370088477642079638 
-weak learner at stage 8 has average loss = 0.135994
+weak learner at stage 8 has average loss = 0.135994152283819725
 split_cols: 2 1 1 
 split_values: 0.0696715340410815898 0.502718698860307178 0.526575453102100632 
-weak learner at stage 8 has average loss = 0.270747
+weak learner at stage 8 has average loss = 0.270746640671640226
 split_cols: 2 0 1 
 split_values: 0.00125079586853901747 0.624507340564582236 0.370088477642079638 
-weak learner at stage 9 has average loss = 0.154551
+weak learner at stage 9 has average loss = 0.154550829757939212
 split_cols: 1 1 1 
 split_values: 0.502718698860307178 0.479480044756096346 0.6805672568090122 
-weak learner at stage 9 has average loss = 0.210479
+weak learner at stage 9 has average loss = 0.210479215655181712
 split_cols: 2 2 2 
 split_values: 0.000357032461916012567 0.000981625552665510437 0.00125079586853901747 
-weak learner at stage 10 has average loss = 0.108164
+weak learner at stage 10 has average loss = 0.10816358087998762
 split_cols: 1 1 2 
 split_values: 0.662011169718969006 0.6805672568090122 0.122353510242232788 
-weak learner at stage 10 has average loss = 0.272979
+weak learner at stage 10 has average loss = 0.27297941302980544
 split_cols: 1 1 1 
 split_values: 0.401581050193795197 0.332158208341527428 0.272305619535323673 
-weak learner at stage 11 has average loss = 0.132455
+weak learner at stage 11 has average loss = 0.132455305997067535
 split_cols: 3 0 1 
 split_values: 0.141930657011306749 0.40739065223433224 0.574273867344056166 
-weak learner at stage 11 has average loss = 0.315862
+weak learner at stage 11 has average loss = 0.315861838715847165
 split_cols: 4 1 3 
 split_values: 7.04158953368505536e-14 0.384853138944362905 0.662373884583906003 
-weak learner at stage 12 has average loss = 0.110589
+weak learner at stage 12 has average loss = 0.110588516596623374
 split_cols: 1 1 2 
 split_values: 0.662011169718969006 0.6805672568090122 0.00363231682035125569 
-weak learner at stage 12 has average loss = 0.327692
+weak learner at stage 12 has average loss = 0.327691610360699437
 split_cols: 3 2 0 
 split_values: 0.357489402445920146 0.0015353418386078177 0.624507340564582236 
-weak learner at stage 13 has average loss = 0.181377
+weak learner at stage 13 has average loss = 0.181377011933805232
 split_cols: 1 4 1 
 split_values: 0.379258691801199865 8.80684414283905426e-14 0.479480044756096346 
-weak learner at stage 13 has average loss = 0.350098
+weak learner at stage 13 has average loss = 0.350097661251061343
 split_cols: 2 2 2 
 split_values: 0.000981625552665510437 0.000357032461916012567 0.196634593877310471 
-weak learner at stage 14 has average loss = 0.12825
+weak learner at stage 14 has average loss = 0.128250465944868525
 split_cols: 1 4 4 
 split_values: 0.564858493389006289 0.999999999999999334 0.999999991749624728 
-weak learner at stage 14 has average loss = 0.30729
+weak learner at stage 14 has average loss = 0.307290463636907374
 split_cols: 2 3 0 
 split_values: 0.000528285193333644099 0.406995257960652612 0.624507340564582236 
-weak learner at stage 15 has average loss = 0.207133
+weak learner at stage 15 has average loss = 0.207132868740156156
 split_cols: 1 1 1 
 split_values: 0.526575453102100632 0.544178240629241028 0.502718698860307178 
-weak learner at stage 15 has average loss = 0.329174
+weak learner at stage 15 has average loss = 0.329173823067465954
 split_cols: 2 2 2 
 split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
-weak learner at stage 16 has average loss = 0.151516
+weak learner at stage 16 has average loss = 0.151516393229420981
 split_cols: 2 2 2 
 split_values: 0.995010648391392527 0.991025168386145405 0.977100614750671781 
-weak learner at stage 16 has average loss = 0.306159
+weak learner at stage 16 has average loss = 0.306159378549761108
 split_cols: 1 3 2 
 split_values: 0.36967671457248541 0.406995257960652612 0.000254178900377460826 
-weak learner at stage 17 has average loss = 0.179798
+weak learner at stage 17 has average loss = 0.179797569635924248
 split_cols: 2 2 1 
 split_values: 0.885239426681956321 0.627448174543832948 0.482293993618237549 
-weak learner at stage 17 has average loss = 0.348588
+weak learner at stage 17 has average loss = 0.348588375382964111
 split_cols: 0 3 1 
 split_values: 0.624507340564582236 0.698651129400676418 0.370088477642079638 
-weak learner at stage 18 has average loss = 0.126962
+weak learner at stage 18 has average loss = 0.12696171488450525
 split_cols: 2 4 3 
 split_values: 0.997650553369808346 0.999999999999998335 0.782913532932723921 
-weak learner at stage 18 has average loss = 0.283838
+weak learner at stage 18 has average loss = 0.283838119869411709
 split_cols: 2 2 2 
 split_values: 0.00125079586853901747 0.000981625552665510437 0.000528285193333644099 
-weak learner at stage 19 has average loss = 0.174564
+weak learner at stage 19 has average loss = 0.174564396795390542
 split_cols: 1 1 1 
 split_values: 0.662011169718969006 0.6805672568090122 0.502718698860307178 
-weak learner at stage 19 has average loss = 0.34564
+weak learner at stage 19 has average loss = 0.345640057938873146

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:25:57 UTC (rev 9859)
@@ -475,7 +475,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *23 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -521,6 +521,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 1 ;
@@ -766,7 +767,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *41 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -812,6 +813,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 1 ;
@@ -849,7 +851,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *44 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 2 ;
@@ -895,6 +897,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -911,6 +914,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
+forward_test = 0 ;
 train_time = 0 ;
 total_train_time = 0 ;
 test_time = 0 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-01-20 19:25:57 UTC (rev 9859)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9749"
+__REVISION__ = "PL9855"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-01-20 19:07:13 UTC (rev 9858)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-01-20 19:25:57 UTC (rev 9859)
@@ -75,7 +75,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *9 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -121,6 +121,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -157,7 +158,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *12 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -203,6 +204,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -240,7 +242,7 @@
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
 leave_template = *15 ->RegressionTreeLeave(
-id = -1 ;
+id = 0 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
 verbosity = 0 ;
@@ -286,6 +288,7 @@
 forward_sub_learner_test_costs = 1 ;
 modif_train_set_weights = 0 ;
 found_zero_error_weak_learner = 0 ;
+reuse_test_results = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 0 ;
@@ -302,6 +305,7 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
+forward_test = 0 ;
 train_time = 0 ;
 total_train_time = 0 ;
 test_time = 0 ;



From nouiz at mail.berlios.de  Tue Jan 20 20:38:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 20 Jan 2009 20:38:35 +0100
Subject: [Plearn-commits] r9860 - in
	trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir:
	. Split0 Split0/LearnerExpdir
Message-ID: <200901201938.n0KJcZnR021837@sheep.berlios.de>

Author: nouiz
Date: 2009-01-20 20:38:34 +0100 (Tue, 20 Jan 2009)
New Revision: 9860

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
Log:
fix test broken by Hugo:)


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2009-01-20 19:25:57 UTC (rev 9859)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2009-01-20 19:38:34 UTC (rev 9860)
@@ -45,7 +45,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
+bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -59,7 +59,7 @@
 connections = 1 [ *4 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072545 	-0.316272804129796303 	
+0.516883916989072434 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -91,8 +91,8 @@
 last_layer = *3  ;
 last_to_target = *6 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359426 	0.370787384786479657 	
-0.09708625652197031 	-0.668099599496062679 	
+-0.603392333131359537 	0.370787384786479657 	
+0.0970862565219703239 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -129,7 +129,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
+bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
@@ -179,7 +179,7 @@
 online = 0 ;
 background_gibbs_update_ratio = 0 ;
 gibbs_chain_reinit_freq = 2147483647 ;
-use_mean_field_contrastive_divergence = 0 ;
+mean_field_contrastive_divergence_ratio = 0 ;
 train_stats_window = -1 ;
 top_layer_joint_cd = 0 ;
 n_layers = 2 ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:25:57 UTC (rev 9859)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/final_learner.psave	2009-01-20 19:38:34 UTC (rev 9860)
@@ -79,7 +79,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.0041988074813960885 0.00291521845148070419 ] ;
+bias = 2 [ -0.00419880748139610759 0.0029152184514807198 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMBinomialLayer" ;
@@ -93,7 +93,7 @@
 connections = 1 [ *9 ->RBMMatrixConnection(
 weights = 2  2  [ 
 -0.222066209664342068 	0.582925561708350082 	
-0.516883916989072545 	-0.316272804129796303 	
+0.516883916989072434 	-0.316272804129796303 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -125,8 +125,8 @@
 last_layer = *8  ;
 last_to_target = *11 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.603392333131359426 	0.370787384786479657 	
-0.09708625652197031 	-0.668099599496062679 	
+-0.603392333131359537 	0.370787384786479657 	
+0.0970862565219703239 	-0.668099599496062679 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -163,7 +163,7 @@
 ;
 gibbs_ma_increment = 0.100000000000000006 ;
 gibbs_initial_ma_coefficient = 0.100000000000000006 ;
-bias = 2 [ -0.00487910443291659449 0.00487910443291660317 ] ;
+bias = 2 [ -0.00487910443291655199 0.00487910443291657281 ] ;
 input_size = 2 ;
 output_size = 2 ;
 name = "RBMMultinomialLayer" ;
@@ -213,7 +213,7 @@
 online = 0 ;
 background_gibbs_update_ratio = 0 ;
 gibbs_chain_reinit_freq = 2147483647 ;
-use_mean_field_contrastive_divergence = 0 ;
+mean_field_contrastive_divergence_ratio = 0 ;
 train_stats_window = -1 ;
 top_layer_joint_cd = 0 ;
 n_layers = 2 ;
@@ -283,7 +283,7 @@
 max_degraded_steps = 31 ;
 min_n_steps = 0 ;
 nreturned = 16 ;
-best_objective = 0.614096318051678525 ;
+best_objective = 0.614096318051678636 ;
 best_step = 16 ;
 met_early_stopping = 0  )
 ;
@@ -297,8 +297,8 @@
 auto_save = 0 ;
 auto_save_diff_time = 10800 ;
 auto_save_test = 0 ;
-best_objective = 0.614096318051678525 ;
-best_results = 4 [ 0.651261219307483818 0.5 0.614096318051678525 0 ] ;
+best_objective = 0.614096318051678636 ;
+best_results = 4 [ 0.651261219307483818 0.5 0.614096318051678636 0 ] ;
 best_learner = *5  ;
 trialnum = 16 ;
 option_vals = []

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2009-01-20 19:25:57 UTC (rev 9859)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2009-01-20 19:38:34 UTC (rev 9860)
@@ -60,12 +60,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.614096318051678525 ;
-max_ = 0.614096318051678525 ;
+min_ = 0.614096318051678636 ;
+max_ = 0.614096318051678636 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.614096318051678525 ;
-last_ = 0.614096318051678525 ;
+first_ = 0.614096318051678636 ;
+last_ = 0.614096318051678636 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2009-01-20 19:25:57 UTC (rev 9859)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/metainfos.txt	2009-01-20 19:38:34 UTC (rev 9860)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL9550"
+__REVISION__ = "PL9855"
 

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2009-01-20 19:25:57 UTC (rev 9859)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/tester.psave	2009-01-20 19:38:34 UTC (rev 9860)
@@ -101,7 +101,7 @@
 i_output_layer = 1 ;
 connections = 1 [ *10 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.560455637071321955 	0.444593612352269352 	
+-0.560455637071321955 	0.444593612352269463 	
 0.660992724240852536 	-0.608576095398922656 	
 ]
 ;
@@ -134,8 +134,8 @@
 last_layer = *9  ;
 last_to_target = *12 ->RBMMatrixConnection(
 weights = 2  2  [ 
--0.585660771216213738 	0.00133253715303369722 	
--0.342792729210354141 	-0.0547933521911874305 	
+-0.585660771216213738 	0.00133253715303371933 	
+-0.342792729210354141 	-0.0547933521911874166 	
 ]
 ;
 gibbs_ma_schedule = []
@@ -222,7 +222,7 @@
 online = 0 ;
 background_gibbs_update_ratio = 0 ;
 gibbs_chain_reinit_freq = 2147483647 ;
-use_mean_field_contrastive_divergence = 0 ;
+mean_field_contrastive_divergence_ratio = 0 ;
 train_stats_window = -1 ;
 top_layer_joint_cd = 0 ;
 n_layers = 2 ;



From nouiz at mail.berlios.de  Wed Jan 21 19:01:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 19:01:35 +0100
Subject: [Plearn-commits] r9861 - trunk/plearn_learners/meta
Message-ID: <200901211801.n0LI1Z2t004971@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 19:01:25 +0100 (Wed, 21 Jan 2009)
New Revision: 9861

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
put some log to extreme level.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-20 19:38:34 UTC (rev 9860)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-01-21 18:01:25 UTC (rev 9861)
@@ -609,7 +609,7 @@
     }
 
     if(forward_test==2 && time_last_stage<time_last_stage_rtr){
-        DBG_MODULE_LOG<<"inherited start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+        EXTREME_MODULE_LOG<<"inherited start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
         Profiler::reset("MultiClassAdaBoost::test() current");
         Profiler::start("MultiClassAdaBoost::test() current");
         PLCHECK(last_stage<=stage);
@@ -618,10 +618,10 @@
         Profiler::end("MultiClassAdaBoost::test()");
         time_sum += Profiler::getStats("MultiClassAdaBoost::test() current").wall_duration;
         last_stage=stage;
-        DBG_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+        EXTREME_MODULE_LOG<<"inherited end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
         return;
     }
-    DBG_MODULE_LOG<<"start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+    EXTREME_MODULE_LOG<<"start time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
     Profiler::reset("MultiClassAdaBoost::test() current");
     Profiler::start("MultiClassAdaBoost::test() current");
     //Profiler::pl_profile_start("MultiClassAdaBoost::test() part1");//cheap
@@ -770,7 +770,7 @@
     time_sum_rtr += Profiler::getStats("MultiClassAdaBoost::test() current").wall_duration;
 
     last_stage=stage;
-    DBG_MODULE_LOG<<"end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
+    EXTREME_MODULE_LOG<<"end time_sum="<<time_sum<<" time_sum_rtr="<<time_sum_rtr<<" last_stage="<<last_stage <<" stage=" <<stage <<" time_last_stage=" <<time_last_stage<<" time_last_stage_rtr=" <<time_last_stage_rtr<<endl;
 
 }
 



From nouiz at mail.berlios.de  Wed Jan 21 19:08:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 19:08:06 +0100
Subject: [Plearn-commits] r9862 - in trunk/plearn_learners: generic testers
Message-ID: <200901211808.n0LI864T011689@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 19:08:05 +0100 (Wed, 21 Jan 2009)
New Revision: 9862

Modified:
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/PLearner.h
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
-By default this don't change the current comportment.
-added the flag PLearner::finalized.
-This flag default to false, when the fct PLearner::finilise() is called, it set the flag to true.
-When PLearner::forget() is called the flag is set to false.
This is to remember when a learner is supposed to be finised. This allow some optimisation in the memory usage at load time. I need those optimization.

-Add the flag PTester::finalise_learner that default to false. If true, will call learner->finalize() after the train.
-Is is safe that I change de fault to be true for PTester::finalise_learner?


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2009-01-21 18:01:25 UTC (rev 9861)
+++ trunk/plearn_learners/generic/PLearner.cc	2009-01-21 18:08:05 UTC (rev 9862)
@@ -73,6 +73,7 @@
       parallelize_here(true),
       master_sends_testset_rows(false),
       use_a_separate_random_generator_for_testing(1827),
+      finalized(false),
       inputsize_(-1),
       targetsize_(-1),
       weightsize_(-1),
@@ -262,6 +263,14 @@
         "Note that this option might not be taken into account in some\n"
         "sub-classes that override the PLearner's test method.");
 
+    declareOption(
+        ol, "finalized", &PLearner::finalized,
+        OptionBase::learntoption,
+        "(default false)"
+        " After training(when finalized() is called) it will be set to true.\n"
+        " When true, it mean the learner it won't be trained again and this\n"
+        " allow some optimization.\n");
+
     inherited::declareOptions(ol);
 }
 
@@ -651,8 +660,17 @@
     if (random_gen && seed_ != 0)
         random_gen->manual_seed(seed_);
     stage = 0;
+    finalized=false;
 }
 
+//////////////
+// finalize //
+//////////////
+void PLearner::finalize()
+{
+    finalized=true;
+}
+
 ////////////////
 // nTestCosts //
 ////////////////

Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2009-01-21 18:01:25 UTC (rev 9861)
+++ trunk/plearn_learners/generic/PLearner.h	2009-01-21 18:08:05 UTC (rev 9862)
@@ -195,6 +195,15 @@
      */
     int use_a_separate_random_generator_for_testing;
 
+
+    /**
+     * (default false)
+     * After training(when finalized() is called) it will be set to true.
+     * When true, it mean the learner it won't be trained again and this
+     * allow some optimization.
+     */
+    bool finalized;
+
 protected:
 
     /**
@@ -408,7 +417,7 @@
      * So it can free resources that are needed only during the training.
      * The functions test()/computeOutputs()/... should continue to work.
      */
-    virtual void finalize(){};
+    virtual void finalize();
 
     /**
      *  *** SUBCLASS WRITING: ***

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2009-01-21 18:01:25 UTC (rev 9861)
+++ trunk/plearn_learners/testers/PTester.cc	2009-01-21 18:08:05 UTC (rev 9862)
@@ -101,6 +101,7 @@
        save_test_confidence(false),
        should_train(true),
        should_test(true),
+       finalize_learner(false),
        enforce_clean_expdir(true),
        redirect_stdout(false),
        redirect_stderr(false),
@@ -246,8 +247,13 @@
         "to train only (without testing) and save the learners, and test later. \n"
         "Any test statistics that are required to be computed if 'should_test'\n"
         "is false yield MISSING_VALUE.\n");
-    
+
     declareOption(
+        ol, "finalize_learner", &PTester::finalize_learner,
+        OptionBase::buildoption,
+        "Default false. If true, will finalize the learner after the training.");
+
+    declareOption(
         ol, "template_stats_collector", &PTester::template_stats_collector, OptionBase::buildoption,
         "If provided, this instance of a subclass of VecStatsCollector will be used as a template\n"
         "to build all the stats collector used during training and testing of the learner");
@@ -544,6 +550,8 @@
 
         train_stats->forget();
         learner->train();
+        if(finalize_learner)
+            learner->finalize();
         train_stats->finalize();
 
         if (is_splitdir)

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2009-01-21 18:01:25 UTC (rev 9861)
+++ trunk/plearn_learners/testers/PTester.h	2009-01-21 18:08:05 UTC (rev 9862)
@@ -120,6 +120,9 @@
     /// is false yield MISSING_VALUE.
     bool should_test;
 
+    //! if true, we finalize the learner after training.
+    bool finalize_learner;
+
     /**
      *  If this option is true, the PTester ensures that the expdir does not
      *  already exist when the experiment is started, and gives a PLerror



From nouiz at mail.berlios.de  Wed Jan 21 19:17:05 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 19:17:05 +0100
Subject: [Plearn-commits] r9863 - in trunk/plearn_learners: meta regressors
Message-ID: <200901211817.n0LIH5cZ021552@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 19:17:04 +0100 (Wed, 21 Jan 2009)
New Revision: 9863

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
use the new "state" finalized of a learner to apply some optimization.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-01-21 18:08:05 UTC (rev 9862)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-01-21 18:17:04 UTC (rev 9863)
@@ -315,6 +315,7 @@
 
 void AdaBoost::finalize()
 {
+    inherited::finalize();
     for(int i=0;i<weak_learners.size();i++){
         weak_learners[i]->finalize();
     }
@@ -1063,7 +1064,8 @@
         if(training_set->classname()!="RegressionTreeRegisters")
             training_set = new RegressionTreeRegisters(training_set,
                                                        report_progress,
-                                                       verbosity);
+                                                       verbosity,
+                                                       !finalized, !finalized);
 
         //we need to change the weight of the trainning set to reuse the RegressionTreeRegister
         if(!modif_train_set_weights){

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2009-01-21 18:08:05 UTC (rev 9862)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2009-01-21 18:17:04 UTC (rev 9863)
@@ -277,6 +277,7 @@
 
 void RegressionTree::finalize()
 {
+    inherited::finalize();
     root->finalize();
     priority_queue = 0;
     split_cols = TVec<int>();



From nouiz at mail.berlios.de  Wed Jan 21 19:37:55 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 19:37:55 +0100
Subject: [Plearn-commits] r9864 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200901211837.n0LIbtEJ024588@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 19:37:54 +0100 (Wed, 21 Jan 2009)
New Revision: 9864

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
fixed test following the addition of PLearner::finalized


Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-01-21 18:17:04 UTC (rev 9863)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-01-21 18:37:54 UTC (rev 9864)
@@ -5,10 +5,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 14.8530952695687528 ;
-isp_signal_sigma = 29.6219285855842109 ;
+isp_alpha = 14.8530952695504261 ;
+isp_signal_sigma = 29.6219285856411894 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 22.2544311479547581 ] ;
+isp_input_sigma = 1 [ 22.2544311482102088 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -19,7 +19,7 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.86446658049196934 ;
+isp_noise_sigma = -1.86446658047284264 ;
 isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
@@ -63,19 +63,19 @@
 ;
 save_gram_matrix = 0 ;
 alpha = 5  1  [ 
--1.11770047929104055 	
--1.44398600685698719 	
-2.34477475120374246 	
-0.35916370282251725 	
--0.273169390814979818 	
+-1.11770047926741944 	
+-1.44398600683733602 	
+2.34477475115816247 	
+0.359163702825210596 	
+-0.273169390815579172 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.17384303484850605 	-0.0428570226279049171 	-2.24278913026098703 	0.211187944941121059 	-0.0160008498417494489 	
--0.0428570226279050698 	2.63229682419555111 	-2.36514023156742859 	-0.310986674738180424 	0.0223156036848867825 	
--2.24278913026098703 	-2.36514023156742859 	4.57667695969712351 	0.0265284039081269407 	-8.51433619161135006e-05 	
-0.211187944941121086 	-0.31098667473818048 	0.0265284039081269442 	0.113894354037727866 	-0.0105928862091330695 	
--0.0160008498417494489 	0.0223156036848867825 	-8.51433619161134599e-05 	-0.0105928862091330678 	0.0346336314417550195 	
+2.1738430348361808 	-0.0428570226562168741 	-2.24278913022280779 	0.21118794494333884 	-0.016000849842130186 	
+-0.0428570226562171239 	2.63229682417954214 	-2.36514023152167674 	-0.310986674739582525 	0.0223156036853270311 	
+-2.24278913022280779 	-2.3651402315216763 	4.57667695961490306 	0.0265284039064695096 	-8.51433617983321869e-05 	
+0.211187944943338812 	-0.310986674739582469 	0.0265284039064695096 	0.113894354038512086 	-0.0105928862093947491 	
+-0.0160008498421301826 	0.0223156036853270276 	-8.51433617983319294e-05 	-0.0105928862093947491 	0.0346336314417334534 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -102,20 +102,21 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 
-!R 1 1 [ 14.9482509617864245 ] 
-!R 1 1 [ 14.4446958356311814 ] 
+!R 1 1 [ 14.9482509617851225 ] 
+!R 1 1 [ 14.4446958356332527 ] 
 !R 2 4  1  [ 
-13.4992090127012432 	
-14.4141233331850245 	
-14.9482509617864245 	
-14.4446958356311814 	
+13.4992090127014244 	
+14.4141233331841647 	
+14.9482509617851225 	
+14.4446958356332527 	
 ]
 1 [ 4  4  [ 
-0.510839430938520356 	0.283483493894024718 	0.0642765858965077541 	-0.555619592268588036 	
-0.283483493894031824 	0.391896855152693624 	0.103882973846950222 	-0.369897017895095104 	
-0.064276585896521965 	0.103882973846953774 	0.285799680578093407 	0.185897571267990003 	
--0.555619592268573825 	-0.369897017895084446 	0.185897571267993555 	2.23845483386523281 	
+0.510839430940516981 	0.283483493893925242 	0.0642765858976659388 	-0.555619592261329842 	
+0.283483493893680105 	0.391896855155454082 	0.103882973848694604 	-0.369897017886295032 	
+0.0642765858979466032 	0.10388297384912093 	0.285799680582960625 	0.185897571271347317 	
+-0.555619592261404449 	-0.369897017886252399 	0.185897571271155471 	2.23845483384018262 	
 ]
 ] 



From nouiz at mail.berlios.de  Wed Jan 21 20:36:51 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 20:36:51 +0100
Subject: [Plearn-commits] r9865 - trunk/python_modules/plearn/parallel
Message-ID: <200901211936.n0LJap48030483@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 20:36:51 +0100 (Wed, 21 Jan 2009)
New Revision: 9865

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
by default don't put the nanoJobs and microJobs parameter to bqtools. This make it work correctly on the new mammouth serie with parallel jobs on the same node.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 18:37:54 UTC (rev 9864)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 19:36:51 UTC (rev 9865)
@@ -548,8 +548,8 @@
     def __init__( self, commands, **args ):
         self.nb_proc = -1
         self.clean_up = True
-        self.micro = 1
-        self.nano = 1
+        self.micro = 0
+        self.nano = 0
         self.queue = "qwork"
         self.long = False
         self.duree = "120:00:00"
@@ -651,11 +651,13 @@
                 param1 = (task, logfile) = load tasks, logfiles
                 linkFiles = launcher
                 preBatch = rm -f _*.BQ
-                microJobs = %d
-                nanoJobs = %d
-                '''%(self.unique_id[1:12],self.queue,self.duree,self.micro,self.nano)) )
+                '''%(self.unique_id[1:12],self.queue,self.duree)) )
+        if self.micro>0:
+            bqsubmit_dat.write('''microJobs = %d\n'''%(self.micro))
+        if self.nano>0:
+            bqsubmit_dat.write('''nanoJobs = %d\n'''%(self.nano))
         if self.nb_proc>0:
-            bqsubmit_dat.write('''\nconcurrentJobs = %d\n'''%(self.nb_proc))
+            bqsubmit_dat.write('''concurrentJobs = %d\n'''%(self.nb_proc))
 
         print self.unique_id
         if self.clean_up:



From nouiz at mail.berlios.de  Wed Jan 21 20:38:20 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 20:38:20 +0100
Subject: [Plearn-commits] r9866 - trunk/python_modules/plearn/parallel
Message-ID: <200901211938.n0LJcKHn030608@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 20:38:20 +0100 (Wed, 21 Jan 2009)
New Revision: 9866

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
fixed typo.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 19:36:51 UTC (rev 9865)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 19:38:20 UTC (rev 9866)
@@ -1265,7 +1265,7 @@
     def run(self):
         if (self.pkdilly and self.nb_proc > 0):
             raise DBIError("[DBI] ERROR: curently pkdilly with nb_proc >0 is not supported!")
-        if (self.stdouts and not self.stderrs) or (self.stderrs and not self.stdouts)
+        if (self.stdouts and not self.stderrs) or (self.stderrs and not self.stdouts):
             raise DBIError("[DBI] ERROR: the condor back-end should have both stdouts and stderrs or none of them")
         if self.stdouts and self.stderrs:
             assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)



From nouiz at mail.berlios.de  Wed Jan 21 22:02:16 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 22:02:16 +0100
Subject: [Plearn-commits] r9867 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir:
	. Split0
Message-ID: <200901212102.n0LL2GY1008793@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 22:02:16 +0100 (Wed, 21 Jan 2009)
New Revision: 9867

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
fixed test.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-01-21 19:38:20 UTC (rev 9866)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:02:16 UTC (rev 9867)
@@ -246,6 +246,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0 ;
 learner1 = *6 ->AdaBoost(
 weak_learners = 1 [ *7 ->RegressionTree(
 missing_is_valid = 0 ;
@@ -459,7 +460,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 ] ;
 voting_weights = 1 [ 1.94591014905531323 ] ;
 sum_voting_weights = 1.94591014905531323 ;
@@ -507,7 +509,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -536,7 +539,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 learner2 = *24 ->AdaBoost(
 weak_learners = 1 [ *25 ->RegressionTree(
@@ -751,7 +755,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 ] ;
 voting_weights = 1 [ 1.22117351768460214 ] ;
 sum_voting_weights = 1.22117351768460214 ;
@@ -799,7 +804,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -828,7 +834,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 forward_sub_learner_test_costs = 1 ;
 learner_template = *42 ->AdaBoost(
@@ -883,7 +890,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -912,7 +920,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 forward_test = 0 ;
 train_time = 0 ;
@@ -935,6 +944,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -1004,4 +1014,5 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-01-21 19:38:20 UTC (rev 9866)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2009-01-21 21:02:16 UTC (rev 9867)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL9866"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-01-21 19:38:20 UTC (rev 9866)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2009-01-21 21:02:16 UTC (rev 9867)
@@ -55,6 +55,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0 ;
 learner1 = *7 ->AdaBoost(
 weak_learners = []
 ;
@@ -107,7 +108,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -136,7 +138,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 learner2 = *10 ->AdaBoost(
 weak_learners = []
@@ -190,7 +193,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -219,7 +223,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 forward_sub_learner_test_costs = 1 ;
 learner_template = *13 ->AdaBoost(
@@ -274,7 +279,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -303,7 +309,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 forward_test = 0 ;
 train_time = 0 ;
@@ -326,6 +333,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -396,7 +404,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -413,6 +422,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []



From nouiz at mail.berlios.de  Wed Jan 21 22:04:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 22:04:35 +0100
Subject: [Plearn-commits] r9868 - in
	trunk/plearn_learners/meta/test/AdaBoost/.pytest:
	PL_AdaBoost_base/expected_results/expdir
	PL_AdaBoost_base/expected_results/expdir/Split0
	PL_AdaBoost_conf_rated_adaboost/expected_results/expdir
	PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0
	PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir
	PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0
Message-ID: <200901212104.n0LL4ZGW009054@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 22:04:34 +0100 (Wed, 21 Jan 2009)
New Revision: 9868

Modified:
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave
Log:
fixed test.


Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:04:34 UTC (rev 9868)
@@ -495,7 +495,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -760,7 +761,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *44 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1025,7 +1027,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *63 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1290,7 +1293,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *82 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1555,7 +1559,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 ] ;
 voting_weights = 5 [ 1.13679877806039786 1.15158148033921659 0.711694555952663621 0.92485300798233383 0.415019255032632539 ] ;
 sum_voting_weights = 4.33994707736724461 ;
@@ -1603,7 +1608,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 0 ;
@@ -1632,7 +1638,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -1649,6 +1656,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -1718,4 +1726,5 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2009-01-21 21:04:34 UTC (rev 9868)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL9866"
 conf                                          = False
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/tester.psave	2009-01-21 21:04:34 UTC (rev 9868)
@@ -91,7 +91,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 0 ;
@@ -120,7 +121,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -137,6 +139,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -207,7 +210,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -224,6 +228,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:04:34 UTC (rev 9868)
@@ -495,7 +495,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -760,7 +761,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *44 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1025,7 +1027,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *63 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1290,7 +1293,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *82 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1555,7 +1559,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 ] ;
 voting_weights = 5 [ 1.13671112060546875 1.15143966674804688 0.711639404296875 0.9248046875 0.414886474609375 ] ;
 sum_voting_weights = 4.33948135375976562 ;
@@ -1603,7 +1608,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 0 ;
@@ -1632,7 +1638,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -1649,6 +1656,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -1718,4 +1726,5 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2009-01-21 21:04:34 UTC (rev 9868)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL9866"
 conf                                          = True
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/tester.psave	2009-01-21 21:04:34 UTC (rev 9868)
@@ -91,7 +91,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 0 ;
@@ -120,7 +121,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -137,6 +139,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -207,7 +210,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -224,6 +228,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2009-01-21 21:04:34 UTC (rev 9868)
@@ -495,7 +495,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -760,7 +761,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *44 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1025,7 +1027,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *63 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1290,7 +1293,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 *82 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
@@ -1555,7 +1559,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 1  )
 ] ;
 voting_weights = 5 [ 1.13679877806039786 1.15158148033921659 0.711694555952663621 0.92485300798233383 0.415019255032632539 ] ;
 sum_voting_weights = 4.33994707736724461 ;
@@ -1603,7 +1608,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -1632,7 +1638,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -1649,6 +1656,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -1718,4 +1726,5 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2009-01-21 21:04:34 UTC (rev 9868)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9855"
+__REVISION__ = "PL9866"
 conf                                          = False
 pseudo                                        = True

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave	2009-01-21 21:02:16 UTC (rev 9867)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/tester.psave	2009-01-21 21:04:34 UTC (rev 9868)
@@ -91,7 +91,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 target_error = 0.5 ;
 pseudo_loss_adaboost = 1 ;
@@ -120,7 +121,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -137,6 +139,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []
@@ -207,7 +210,8 @@
 nservers = 0 ;
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
 ;
 perf_evaluators = {};
 report_stats = 1 ;
@@ -224,6 +228,7 @@
 provide_learner_expdir = 1 ;
 should_train = 1 ;
 should_test = 1 ;
+finalize_learner = 0 ;
 template_stats_collector = *0 ;
 global_template_stats_collector = *0 ;
 final_commands = []



From nouiz at mail.berlios.de  Wed Jan 21 22:17:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 22:17:26 +0100
Subject: [Plearn-commits] r9869 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200901212117.n0LLHQGX010586@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 22:17:25 +0100 (Wed, 21 Jan 2009)
New Revision: 9869

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
implemented the --raw option to bqtools back-end


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 21:04:34 UTC (rev 9868)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 21:17:25 UTC (rev 9869)
@@ -179,6 +179,7 @@
         self.temp_files = []
         self.arch = 0 # TODO, we should put the local arch: 32,64 or 3264 bits
         self.base_tasks_log_file = []
+        self.raw = ''
 
         for key in args.keys():
             self.__dict__[key] = args[key]
@@ -658,7 +659,9 @@
             bqsubmit_dat.write('''nanoJobs = %d\n'''%(self.nano))
         if self.nb_proc>0:
             bqsubmit_dat.write('''concurrentJobs = %d\n'''%(self.nb_proc))
-
+        if self.raw:
+            bqsubmit_dat.write(self.raw+"\n")
+            
         print self.unique_id
         if self.clean_up:
             bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;')
@@ -734,7 +737,6 @@
         self.mem = 0
         self.cpu = 0
         self.req = ''
-        self.raw = ''
         self.rank = ''
         self.copy_local_source_file = False
         self.files = ''
@@ -1158,7 +1160,6 @@
         return condor_cmd
 
     def run_non_dag(self):
-        # create the bqsubmit.dat, with
         condor_datas = []
 
         #we supose that each task in tasks have the same number of commands

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-21 21:04:34 UTC (rev 9868)
+++ trunk/scripts/dbidispatch	2009-01-21 21:17:25 UTC (rev 9869)
@@ -11,6 +11,7 @@
     cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
                               [--cpu=nb_cpu_per_node]
     bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only,clusterid(condor only),processid(condor only))}+]
+                              [--raw=STRING[\nSTRING]]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X]
                               [--nano=X]
@@ -19,7 +20,6 @@
                               [--rank=RANK_EXPRESSION] 
                               [--files=file1[,file2...]]
                               [--env=VAR=VALUE[;VAR2=VALUE2]]
-                              [--raw=CONDOR_EXPRESSION]
                               [*--[no_]abs_path] [--[*no_]pkdilly]
                               [*--[no_]set_special_env]
                               [--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}]
@@ -114,6 +114,8 @@
                   to /dev/null
       - clusterid: (condor only)put the cluster id of the jobs.
       - processid: (condor only)put the process id of the jobs. Idem as nb0
+  The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the submit file.
+      if this option appread many time, they will be concatanated with a new line.
 
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
@@ -162,7 +164,6 @@
     jobs the variable VAR with value VALUE. To pass many variable you can:
       1) use one --env option and separ the pair by ';'(don't forget to quote)
       2) you can pass many time the --env parameter.
-  The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the condor submit file.
   The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, 
     MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
   The '--[no_]abs_path' option will tell condor to change the path to the 
@@ -357,7 +358,7 @@
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "to_all"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["micro", "long", "duree", "queue", "nano"]
+    valid_dbi_param +=["micro", "long", "duree", "queue", "nano", "raw"]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.



From nouiz at mail.berlios.de  Wed Jan 21 22:32:25 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 22:32:25 +0100
Subject: [Plearn-commits] r9870 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200901212132.n0LLWPde011861@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 22:32:25 +0100 (Wed, 21 Jan 2009)
New Revision: 9870

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added the --submit_options to the bqtools back-end.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 21:17:25 UTC (rev 9869)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 21:32:25 UTC (rev 9870)
@@ -555,6 +555,7 @@
         self.long = False
         self.duree = "120:00:00"
         self.mem = None
+        self.submit_options = ""
         DBIBase.__init__(self, commands, **args)
 
         self.nb_proc = int(self.nb_proc)
@@ -648,11 +649,11 @@
                 batchName = dbi_%s
                 command = sh launcher
                 templateFiles = launcher
-                submitOptions = -q %s -l walltime=%s
+                submitOptions = -q %s -l walltime=%s %s
                 param1 = (task, logfile) = load tasks, logfiles
                 linkFiles = launcher
                 preBatch = rm -f _*.BQ
-                '''%(self.unique_id[1:12],self.queue,self.duree)) )
+                '''%(self.unique_id[1:12],self.queue,self.duree,self.submit_options)) )
         if self.micro>0:
             bqsubmit_dat.write('''microJobs = %d\n'''%(self.micro))
         if self.nano>0:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-21 21:17:25 UTC (rev 9869)
+++ trunk/scripts/dbidispatch	2009-01-21 21:32:25 UTC (rev 9870)
@@ -13,8 +13,7 @@
     bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only,clusterid(condor only),processid(condor only))}+]
                               [--raw=STRING[\nSTRING]]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
-                              [--queue=X]
-                              [--nano=X]
+                              [--queue=X] [--nano=X] [--submit_options=X]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                               [--[*no_]getenv] [*--[no_]prefserver] 
                               [--rank=RANK_EXPRESSION] 
@@ -85,8 +84,9 @@
     set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
     different queue with few nodes, please make sure you are not using too many
     nodes at once with the --nb_proc option.
-  The '--queue=X' tell on witch queue the jobs will be launched
-  The '--nano=X'
+  The '--queue=X' tell on witch queue the jobs will be launched.
+  The '--nano=X' add nanoJobs=X in the submit file.
+  The '--submit_options=X' X is appended to the submitOptions in the submit file.
   
 cluster and condor options:
   The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
@@ -304,7 +304,7 @@
     elif argv.split('=')[0] in ["--duree","--cpu","--mem","--os","--nb_proc",
                                 "--req", "--files", "--raw", "--rank", "--env",
                                 "--universe", "--exp_dir", "--machine", "--machines",
-                                "--queue", "--nano"]:
+                                "--queue", "--nano", "--submit_options"]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -358,7 +358,7 @@
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "to_all"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["micro", "long", "duree", "queue", "nano", "raw"]
+    valid_dbi_param +=["micro", "long", "duree", "queue", "nano", "raw", "submit_options"]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.



From nouiz at mail.berlios.de  Wed Jan 21 23:02:00 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 21 Jan 2009 23:02:00 +0100
Subject: [Plearn-commits] r9871 - trunk/python_modules/plearn/parallel
Message-ID: <200901212202.n0LM20R9014660@sheep.berlios.de>

Author: nouiz
Date: 2009-01-21 23:02:00 +0100 (Wed, 21 Jan 2009)
New Revision: 9871

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
on bqtools, by default set to all jobs to be concurrent.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 21:32:25 UTC (rev 9870)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 22:02:00 UTC (rev 9871)
@@ -658,8 +658,11 @@
             bqsubmit_dat.write('''microJobs = %d\n'''%(self.micro))
         if self.nano>0:
             bqsubmit_dat.write('''nanoJobs = %d\n'''%(self.nano))
-        if self.nb_proc>0:
-            bqsubmit_dat.write('''concurrentJobs = %d\n'''%(self.nb_proc))
+        p=self.nb_proc
+        if p==-1:
+            p=len(self.tasks)
+        if p>0:
+            bqsubmit_dat.write('''concurrentJobs = %d\n'''%(p))
         if self.raw:
             bqsubmit_dat.write(self.raw+"\n")
             



From nouiz at mail.berlios.de  Thu Jan 22 16:41:44 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 22 Jan 2009 16:41:44 +0100
Subject: [Plearn-commits] r9872 - trunk/python_modules/plearn/parallel
Message-ID: <200901221541.n0MFfiLb027361@sheep.berlios.de>

Author: nouiz
Date: 2009-01-22 16:41:42 +0100 (Thu, 22 Jan 2009)
New Revision: 9872

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
now bqtools return error code, so we can clean up the try to parse the stderr.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-21 22:02:00 UTC (rev 9871)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-22 15:41:42 UTC (rev 9872)
@@ -682,27 +682,8 @@
             self.p = Popen( 'bqsubmit', shell=True)
             self.p.wait()
             
-            #check for error string as bqsubmit don't already return an errorcode !=0 when their was an error.
-            error_str=False
-#            print self.p.stderr
-#            print dir(self.p.stderr)
-#            dir(self.p.stderr)
-#            print self.p.stderr.closed
-#            print self.p.stderr.peek
-#            help(self.p.stderr)
-#            self.p.stderr.flush()
-#            self.p.stderr.write('dd')
-#            print self.p.stderr.read()
-#            lines = self.p.stderr.readline()
-#            print len(lines)
-#            for line in lines:
-#                if line in ["qsub: Job exceeds queue resource limits MSG=cannot satisfy queue max walltime requirement\n"]:
-#                    error_str=True
-#                print line,
             if self.p.returncode!=0:
                 raise DBIError("[DBI] ERROR: the bqsubmit returned an error code of"+str(self.p.returncode))
-            if error_str:
-                raise DBIError("[DBI] ERROR: the bqsubmit returned an error string. It was probably not launched correctly.")
         else:
             print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
             if self.dolog:



From nouiz at mail.berlios.de  Fri Jan 23 19:44:47 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 Jan 2009 19:44:47 +0100
Subject: [Plearn-commits] r9873 - trunk
Message-ID: <200901231844.n0NIilfx029095@sheep.berlios.de>

Author: nouiz
Date: 2009-01-23 19:44:47 +0100 (Fri, 23 Jan 2009)
New Revision: 9873

Modified:
   trunk/pymake.config.model
Log:
added g++-4 to use g++ version 4 on cygwin.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-01-22 15:41:42 UTC (rev 9872)
+++ trunk/pymake.config.model	2009-01-23 18:44:47 UTC (rev 9873)
@@ -258,7 +258,8 @@
 # First option that appears in each group is the default, and is assumed if you
 # do not specify any option from that group.
 options_choices = [
-  [ 'g++', 'g++3', 'g++no-cygwin', 'icc', 'icc8', 'icc9', 'icc10', 'mpi',
+  [ 'g++', 'g++3', 'g++no-cygwin', 'g++-4', 'icc',
+    'icc8', 'icc9', 'icc10', 'mpi',
     'purify', 'quantify', 'vc++', 'condor' ],
   
   [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
@@ -604,6 +605,15 @@
               cpp_definitions = ['USING_MPI=0', '_MINGW_'],
               linker = 'g++ -mno-cygwin' )
 
+pymakeOption( name = 'g++-4',
+              description = 'compiling with g++, with no MPI support, ' \
+                          + 'without the need of the Cygwin DLL',
+              compiler = 'g++-4',
+              compileroptions = '-Wno-deprecated '+pedantic_mode \
+                              + '-Wno-long-long -ftemplate-depth-100 -Wno-variadic-macros',
+              cpp_definitions = ['USING_MPI=0'],
+              linker = 'g++-4' )
+
 pymakeOption( name = 'icc',   # For C++, it is actually icpc
               description = '(DEPRECATED, use icc8 or icc9 instead) compiling with Intel Compiler (version 8.x), with no MPI support',
               # Disable some warnings:



From nouiz at mail.berlios.de  Fri Jan 23 19:49:33 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 23 Jan 2009 19:49:33 +0100
Subject: [Plearn-commits] r9874 - trunk/plearn_learners/regressors
Message-ID: <200901231849.n0NInXfc029658@sheep.berlios.de>

Author: nouiz
Date: 2009-01-23 19:49:32 +0100 (Fri, 23 Jan 2009)
New Revision: 9874

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
put the cast to int as on cygwin with g++ 3.4.4 we got an error with the cast.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-23 18:44:47 UTC (rev 9873)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-01-23 18:49:32 UTC (rev 9874)
@@ -219,14 +219,14 @@
     if(weightsize() <= 0){
         weight.fill(1.0 / length());
         for(int i=0;i<reg.length();i++){            
-            target[i] = target_weight[reg[i]].first;
+            target[i] = target_weight[int(reg[i])].first;
             value[i]  = tsource->get(col, reg[i]);
         }
     } else {
         //It is better to do multiple pass for memory access.
         for(int i=0;i<reg.length();i++){
-            target[i] = target_weight[reg[i]].first;
-            weight[i] = target_weight[reg[i]].second;
+            target[i] = target_weight[int(reg[i])].first;
+            weight[i] = target_weight[int(reg[i])].second;
         }
         for(int i=0;i<reg.length();i++)
             value[i]  = tsource->get(col, reg[i]);



From nouiz at mail.berlios.de  Mon Jan 26 21:17:33 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 26 Jan 2009 21:17:33 +0100
Subject: [Plearn-commits] r9875 - trunk/plearn_learners/regressors
Message-ID: <200901262017.n0QKHX2B025003@sheep.berlios.de>

Author: nouiz
Date: 2009-01-26 21:17:32 +0100 (Mon, 26 Jan 2009)
New Revision: 9875

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
speed up, if variable with few values, we don't look past the last split point for other split point


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-01-23 18:49:32 UTC (rev 9874)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-01-26 20:17:32 UTC (rev 9875)
@@ -318,7 +318,8 @@
 #ifndef BY_ROW
         //in case of missing value
         if(candidate.size()==0){
-            PLASSERT(missing_leave->length()>0);
+            PLCHECK(missing_leave->length()+right_leave->length()
+                    ==leave->length());
             continue;
         }
         int row = candidate.pop();
@@ -388,18 +389,24 @@
     Vec tmp(3);
 
     real missing_errors = missing_error[0] + missing_error[1];
-    for(int i=candidates.size()-2;i>=0;i--)
+    real first_value=values.first();
+    real next_feature=values.last();
+    for(int i=candidates.size()-2;i>=0&&next_feature!=first_value;i--)
     {
         int next_row = candidates[i];
-        real next_feature=values[i];
-        real row_feature=values[i+1];
+        real row_feature=next_feature;
+        PLASSERT(row_feature==values[i+1]);
+        next_feature=values[i];
+
+        real target=targets[i+1];
+        real weight=weights[i+1];
         PLASSERT(train_set->get(next_row, col)==values[i]);
         PLASSERT(train_set->get(row, col)==values[i+1]);
         PLASSERT(next_feature<=row_feature);
 
 
-        left_leave->removeRow(row, targets[i+1], weights[i+1]);
-        right_leave->addRow(row, targets[i+1], weights[i+1]);
+        left_leave->removeRow(row, target, weight);
+        right_leave->addRow(row, target, weight);
         row = next_row;
         if (next_feature < row_feature){
             left_leave->getOutputAndError(tmp, left_error);



From nouiz at mail.berlios.de  Mon Jan 26 21:45:44 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 26 Jan 2009 21:45:44 +0100
Subject: [Plearn-commits] r9876 - trunk/plearn_learners/regressors
Message-ID: <200901262045.n0QKji4R028396@sheep.berlios.de>

Author: nouiz
Date: 2009-01-26 21:45:43 +0100 (Mon, 26 Jan 2009)
New Revision: 9876

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
added comment that explain one optimization.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-01-26 20:17:32 UTC (rev 9875)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-01-26 20:45:43 UTC (rev 9876)
@@ -391,6 +391,10 @@
     real missing_errors = missing_error[0] + missing_error[1];
     real first_value=values.first();
     real next_feature=values.last();
+
+    //next_feature!=first_value is to check if their is more split point
+    // in case of binary variable or variable with few different value,
+    // this give a great speed up.
     for(int i=candidates.size()-2;i>=0&&next_feature!=first_value;i--)
     {
         int next_row = candidates[i];



From nouiz at mail.berlios.de  Mon Jan 26 22:08:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 26 Jan 2009 22:08:38 +0100
Subject: [Plearn-commits] r9877 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200901262108.n0QL8cgf032130@sheep.berlios.de>

Author: nouiz
Date: 2009-01-26 22:08:37 +0100 (Mon, 26 Jan 2009)
New Revision: 9877

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-implemented the option --cpu=X and --mem=X for bqtools.
-reordered the options to be in alphabetical order of their category name.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-01-26 20:45:43 UTC (rev 9876)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-01-26 21:08:37 UTC (rev 9877)
@@ -180,6 +180,8 @@
         self.arch = 0 # TODO, we should put the local arch: 32,64 or 3264 bits
         self.base_tasks_log_file = []
         self.raw = ''
+        self.cpu = 0
+        self.mem = 0
 
         for key in args.keys():
             self.__dict__[key] = args[key]
@@ -189,6 +191,8 @@
 #            if self.dolog or self.file_redirect_stdout or self.file_redirect_stderr:
             os.mkdir(self.log_dir)
 
+        self.mem = int(self.mem)
+        self.cpu = int(self.cpu)
         # If some arguments aren't lists, put them in a list
         if not isinstance(commands, list):
             commands = [commands]
@@ -394,13 +398,11 @@
         self.cwait=True
         self.force=False
         self.interruptible=False
-        self.cpu=1
         self.threads=[]
         self.started=0
         self.nb_proc=32
         self.mt=None
         self.args=args
-        self.mem=None
         self.os=None
         DBIBase.__init__(self, commands, **args)
 
@@ -460,13 +462,13 @@
             command += " --duree "+self.duree
         if self.cwait:
             command += " --wait"
-        if self.mem:
+        if self.mem > 0:
             command += " --memoire "+self.mem
         if self.force:
             command += " --force"
         if self.interruptible:
             command += " --interruptible"
-        if self.cpu!=1:
+        if self.cpu>0:
             command += " --cpu "+self.cpu
         if self.os:
             command += " --os "+self.os
@@ -551,10 +553,9 @@
         self.clean_up = True
         self.micro = 0
         self.nano = 0
-        self.queue = "qwork"
+        self.queue = "qwork at ms"
         self.long = False
         self.duree = "120:00:00"
-        self.mem = None
         self.submit_options = ""
         DBIBase.__init__(self, commands, **args)
 
@@ -643,17 +644,31 @@
         tasks_file.close()
         logfiles_file.close()
 
+        tmp_options = self.submit_options
+        if self.queue:
+            tmp_options+=" -q "+self.queue
+        l=""
+        if self.cpu >0:
+            l+="ncpus="+str(self.cpu)
+        if self.mem >0:
+            if l: l+=","
+            l="mem=%dM"%(self.mem)
+        if self.duree:
+            if l: l+=","
+            l+="walltime="+self.duree
+        if l:
+            tmp_options+=" -l "+l
         # Create the bqsubmit.dat, with
         bqsubmit_dat = open( 'bqsubmit.dat', 'w' )
         bqsubmit_dat.write( dedent('''\
                 batchName = dbi_%s
                 command = sh launcher
                 templateFiles = launcher
-                submitOptions = -q %s -l walltime=%s %s
+                submitOptions = %s
                 param1 = (task, logfile) = load tasks, logfiles
                 linkFiles = launcher
                 preBatch = rm -f _*.BQ
-                '''%(self.unique_id[1:12],self.queue,self.duree,self.submit_options)) )
+                '''%(self.unique_id[1:12],tmp_options)) )
         if self.micro>0:
             bqsubmit_dat.write('''microJobs = %d\n'''%(self.micro))
         if self.nano>0:
@@ -668,7 +683,7 @@
             
         print self.unique_id
         if self.clean_up:
-            bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;')
+            bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;\n')
         bqsubmit_dat.close()
 
         # Execute pre-batch
@@ -719,8 +734,6 @@
         self.nice = False
         # in Meg for initialization for consistency with cluster
         # then in kilo as that is what is needed by condor
-        self.mem = 0
-        self.cpu = 0
         self.req = ''
         self.rank = ''
         self.copy_local_source_file = False
@@ -758,7 +771,7 @@
                 raise DBIError("[DBI] ERROR we refuse to start more jobs on the local universe then the total number of core. Start less jobs or use another universe.")
 
         #transform from meg to kilo
-        self.mem=int(self.mem)*1024
+        self.mem=self.mem*1024
 
         self.os = self.os.upper()
         if not os.path.exists(self.log_dir):

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-01-26 20:45:43 UTC (rev 9876)
+++ trunk/scripts/dbidispatch	2009-01-26 21:08:37 UTC (rev 9877)
@@ -7,13 +7,20 @@
 ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] <back-end parameter> {--file=FILEPATH | <command-template>}
 
 <back-end parameter>:
+    bqtools, cluster, condor option  : [--mem=N]
+                              [--cpu=nb_cpu_per_node]
     bqtools, cluster option  :[--duree=X]
-    cluster, condor options  :[--32|--64|--3264] [--os=X] [--mem=N]
-                              [--cpu=nb_cpu_per_node]
-    bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,sh(condor only,clusterid(condor only),processid(condor only))}+]
+    bqtools, condor options  :[--tasks_filename={compact,explicit,*nb0,nb1,
+                                                 sh(condor only),
+                                                 clusterid(condor only),
+                                                 processid(condor only))}+]
                               [--raw=STRING[\nSTRING]]
+    cluster, condor options  :[--32|--64|--3264] [--os=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X] [--nano=X] [--submit_options=X]
+                              [*--[no_]clean_up]
+    cluster option           :[*--[no_]cwait]  [--[*no_]force]
+                              [--[*no_]interruptible]
     condor option            :[--req="CONDOR_REQUIREMENT"] [--[*no_]nice]
                               [--[*no_]getenv] [*--[no_]prefserver] 
                               [--rank=RANK_EXPRESSION] 
@@ -21,10 +28,9 @@
                               [--env=VAR=VALUE[;VAR2=VALUE2]]
                               [*--[no_]abs_path] [--[*no_]pkdilly]
                               [*--[no_]set_special_env]
-                              [--universe={vanilla*, standard, grid, java, scheduler, local, parallel, vm}]
+                              [--universe={vanilla*, standard, grid, java,
+                                           scheduler, local, parallel, vm}]
                               [--machine=HOSTNAME+] [--machines=regex+]
-    cluster option           :[*--[no_]cwait]  [--[*no_]force]
-                              [--[*no_]interruptible]
 An * after '[', '{' or ',' signals the default value.
 An + tell that we can put one or more separeted by a comma
 '''
@@ -59,46 +65,23 @@
   The '--exp_dir=dir' specifies the name of the temporary directory
     relative to LOGDIR, instead of one generated automatically based
     upon the command line and the time.
-  The '--[*no_]clean_up' set the DBI option clean_up to true or false
+  The '*--[no_]clean_up' set the DBI option clean_up to true or false
   The 'DBIDISPATCH_DEFAULT_OPTION' environnement variable can contain default
     option for dbidispatch. They can be overrided on the command line.
   The 'DBIDISPATCH_LOGDIR' environnement variable set the name of the directory
     where all the individual logs directory will be put. Default to LOGS.
 
+bqtools, cluster and condor option:
+  The '--mem=X' speficify the number of ram in meg the program need to execute.
+  The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
+    will be reserved for each job.
+
 bqtools and cluster option:
   The '--duree' option specifies the maximum duration of the jobs. The syntax 
     depends on the back-end. For the cluster syntax, see 'cluster --help'. 
     For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 
     13 minutes and 15 seconds.
 
-bqtools only options:
-  If the --long option is not set, the maximum duration of each job will be 
-    120 hours (5 days).
-  The '--micro[=nb_batch]' option can be used with BqTools when launching many 
-    jobs that have a very short duration. This may prevent some queue crashes. 
-    The nb_batch value is the number of experience to group together in a batch.
-    (by default not used, --micro is equivalent to --micro=20)
-  The '--long' option must be used with BqTools to launch jobs whose duration
-    is more than 5 days. The maximum duration of a job will be either the
-    BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
-    set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
-    different queue with few nodes, please make sure you are not using too many
-    nodes at once with the --nb_proc option.
-  The '--queue=X' tell on witch queue the jobs will be launched.
-  The '--nano=X' add nanoJobs=X in the submit file.
-  The '--submit_options=X' X is appended to the submitOptions in the submit file.
-  
-cluster and condor options:
-  The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
-    Default the same as the submit host.
-  The '--mem=X' speficify the number of ram in meg the program need to execute.
-  The '--os=X' speficify the os of the server. 
-    Cluster default: fc7. Cluster accepted value fc4, fc7 and fc9.
-    Condor default to the same as the submit host and --os=FC7,FC9 
-    tell to use FC7 or FC9 hosts.
-  The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
-    will be reserved for each job.
-
 bqtools and condor options:
   The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the
     filename where the stdout, stderr are redirected. We can put many option 
@@ -117,6 +100,31 @@
   The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the submit file.
       if this option appread many time, they will be concatanated with a new line.
 
+cluster and condor options:
+  The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
+    Default the same as the submit host.
+  The '--os=X' speficify the os of the server. 
+    Cluster default: fc7. Cluster accepted value fc4, fc7 and fc9.
+    Condor default to the same as the submit host and --os=FC7,FC9 
+    tell to use FC7 or FC9 hosts.
+
+bqtools only options:
+  If the --long option is not set, the maximum duration of each job will be 
+    120 hours (5 days).
+  The '--micro[=nb_batch]' option can be used with BqTools when launching many 
+    jobs that have a very short duration. This may prevent some queue crashes. 
+    The nb_batch value is the number of experience to group together in a batch.
+    (by default not used, --micro is equivalent to --micro=20)
+  The '--long' option must be used with BqTools to launch jobs whose duration
+    is more than 5 days. The maximum duration of a job will be either the
+    BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
+    set, and 1200:00:00 (50 days) otherwise. Since long jobs are launched on a
+    different queue with few nodes, please make sure you are not using too many
+    nodes at once with the --nb_proc option.
+  The '--queue=X' tell on witch queue the jobs will be launched.
+  The '--nano=X' add nanoJobs=X in the submit file.
+  The '--submit_options=X' X is appended to the submitOptions in the submit file.
+  
 cluster only options:
   The '--[no_]cwait' is transfered to cluster. 
     This must be enabled if there is not nb_proc available nodes. Otherwise 
@@ -358,7 +366,8 @@
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "to_all"]
 elif launch_cmd=="Bqtools":
-    valid_dbi_param +=["micro", "long", "duree", "queue", "nano", "raw", "submit_options"]
+    valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
+                       "nano", "queue", "raw", "submit_options"]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.



From larocheh at mail.berlios.de  Tue Jan 27 00:10:58 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 27 Jan 2009 00:10:58 +0100
Subject: [Plearn-commits] r9878 - trunk/plearn_learners/online
Message-ID: <200901262310.n0QNAwiB017533@sheep.berlios.de>

Author: larocheh
Date: 2009-01-27 00:10:57 +0100 (Tue, 27 Jan 2009)
New Revision: 9878

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Forgot to set the learning rate of the greedy_target_connections


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-26 21:08:37 UTC (rev 9877)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-01-26 23:10:57 UTC (rev 9878)
@@ -1050,6 +1050,8 @@
             {
                 direct_connections[i]->setLearningRate( lr );
             }
+            if( greedy_target_connections.length() && greedy_target_connections[i] )
+                greedy_target_connections[i]->setLearningRate( lr );
             layers[i+1]->setLearningRate( lr );
             if(partial_costs.length() != 0 && partial_costs[i])
                         partial_costs[i]->setLearningRate( lr );
@@ -1102,6 +1104,8 @@
                     }
                     if(partial_costs.length() != 0 && partial_costs[i])
                         partial_costs[i]->setLearningRate( lr );
+                    if( greedy_target_connections.length() && greedy_target_connections[i] )
+                        greedy_target_connections[i]->setLearningRate( lr );
                 }
                 sample = *this_stage % nsamples;
                 train_set->getExample(sample, input, target, weight);
@@ -1979,6 +1983,8 @@
         {
             partial_costs[ i ]->setLearningRate( lr );
         }
+        if( greedy_target_connections.length() && greedy_target_connections[i] )
+            greedy_target_connections[i]->setLearningRate( lr );
     }
     layers[n_layers-1]->setLearningRate( lr );
 
@@ -2125,6 +2131,8 @@
             correlation_layers[i]->setLearningRate( lr );
             correlation_connections[i]->setLearningRate( lr );
         }
+        if( greedy_target_connections.length() && greedy_target_connections[i] )
+            greedy_target_connections[i]->setLearningRate( lr );
     }
     layers[n_layers-1]->setLearningRate( lr );
 



From nouiz at mail.berlios.de  Wed Jan 28 18:32:39 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 28 Jan 2009 18:32:39 +0100
Subject: [Plearn-commits] r9879 - trunk
Message-ID: <200901281732.n0SHWdB1007398@sheep.berlios.de>

Author: nouiz
Date: 2009-01-28 18:32:39 +0100 (Wed, 28 Jan 2009)
New Revision: 9879

Modified:
   trunk/pymake.config.model
Log:
update for the new mammouth serie


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-01-26 23:10:57 UTC (rev 9878)
+++ trunk/pymake.config.model	2009-01-28 17:32:39 UTC (rev 9879)
@@ -372,7 +372,7 @@
         #numpy_site_packages = join(homedir, '../delallea/local/lib/python2.5/site-packages/numarray -lutil')
         python_version = '2.5'
         optionargs += [ 'python%s' % python_version.replace('.', '') ]
-        python_lib_root = '/home/delallea/local/lib'
+        python_lib_root = '/opt/python64/2.5.1/lib'
     elif domain_name.endswith('.rqchp.qc.ca'):
         numpy_includedirs   = [ '/usr/network.ALTIX/python-2.4.1/include' ]
         numpy_site_packages = join(homedir, '../delallea/local/lib/python2.4/site-packages/numarray -lutil')
@@ -867,7 +867,7 @@
 
 pymakeLinkOption( name = 'mammouthblas',
               description = 'linking BLAS for P4 Mammouth-Serie cluster',
-              linkeroptions = '-L/opt/intel/mkl/10.0.011/lib/32 -lmkl -openmp' ) #-lmkl_lapack -lmkl_p4 -lmkl_vml_p4 -lpthread ' )
+              linkeroptions = '-L/opt/intel/mkl/10.0.011/lib/em64t -lmkl -openmp' ) #-lmkl_lapack -lmkl_p4 -lmkl_vml_p4 -lpthread ' )
 
 pymakeLinkOption( name = 'apintelblas',
               description = 'Intel BLAS+LAPACK for generic install in /usr/local/lib (incl. ApSTAT)',



From nouiz at mail.berlios.de  Wed Jan 28 18:48:55 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 28 Jan 2009 18:48:55 +0100
Subject: [Plearn-commits] r9880 - trunk
Message-ID: <200901281748.n0SHmtFF025097@sheep.berlios.de>

Author: nouiz
Date: 2009-01-28 18:48:54 +0100 (Wed, 28 Jan 2009)
New Revision: 9880

Modified:
   trunk/pymake.config.model
Log:
simpler implementation to allow multiple option that start with -pass=


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-01-28 17:32:39 UTC (rev 9879)
+++ trunk/pymake.config.model	2009-01-28 17:48:54 UTC (rev 9880)
@@ -288,18 +288,18 @@
   [ '', 'pass']
 ]
 
-optionargs_new=set()
-for option in optionargs:
+#we must use a copy of the list as modifiing
+# a list with iterating throught it is not safe.
+for option in optionargs[:]:
     if option.startswith('pass='):
+        optionargs.remove(option)
         option=option[5:]
-        optionargs_new.add(option)
+        
+        optionargs.append(option)
         pymakeOption( name = option,
                       description = option,
                       compileroptions = option)
         options_choices+=[['',option]]
-    else:
-        optionargs_new.add(option)
-optionargs=list(optionargs_new)
 
 pymakeOption( name = 'pass',
               description = 'The command line parameter -pass=X will make add the parameter X to the compiler command. Usefull to test new option without modifying the pymake.config.model file.',



From nouiz at mail.berlios.de  Wed Jan 28 18:51:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 28 Jan 2009 18:51:32 +0100
Subject: [Plearn-commits] r9881 - trunk/python_modules/plearn/pymake
Message-ID: <200901281751.n0SHpWi5028543@sheep.berlios.de>

Author: nouiz
Date: 2009-01-28 18:51:31 +0100 (Wed, 28 Jan 2009)
New Revision: 9881

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
if multiple -local[=X] parameter, we keep the last one.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2009-01-28 17:48:54 UTC (rev 9880)
+++ trunk/python_modules/plearn/pymake/pymake.py	2009-01-28 17:51:31 UTC (rev 9881)
@@ -2815,7 +2815,9 @@
         while 'local_ofiles' in optionargs: optionargs.remove('local_ofiles')
     else:
         local_ofiles = 0
-    for option in optionargs:
+    #we must use a copy of optionargs as we should not modify a list that we iterate over.
+    #this cause bug if multiple local are present. In that case, we should keep the last one.
+    for option in optionargs[:]:
         if option.count('local', 0, 5)==1:
             local_compilation = 1
             optionargs.remove(option)
@@ -2827,7 +2829,8 @@
                     # nprocesses_on_localhost=1
                 else:
                     nprocesses_on_localhost=int(option[6:])
-
+            else:
+                    nprocesses_on_localhost=1
     local_ofiles_base_path= '/tmp/.pymake/local_ofiles/' # could add an option for that...
 
     if 'ssh' in optionargs:



From larocheh at mail.berlios.de  Wed Jan 28 22:55:41 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 28 Jan 2009 22:55:41 +0100
Subject: [Plearn-commits] r9882 - trunk/plearn_learners/online
Message-ID: <200901282155.n0SLtfM2019892@sheep.berlios.de>

Author: larocheh
Date: 2009-01-28 22:55:40 +0100 (Wed, 28 Jan 2009)
New Revision: 9882

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Added an option to have target connections during greedy training.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2009-01-28 17:51:31 UTC (rev 9881)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2009-01-28 21:55:40 UTC (rev 9882)
@@ -201,6 +201,14 @@
                   OptionBase::buildoption,
                   "The weights of the connections between the layers");
 
+    declareOption(ol, "greedy_target_layers", &DeepBeliefNet::greedy_target_layers,
+                  OptionBase::buildoption,
+                  "Optional target layers for greedy layer-wise pretraining");
+
+    declareOption(ol, "greedy_target_connections", &DeepBeliefNet::greedy_target_connections,
+                  OptionBase::buildoption,
+                  "Optional target matrix connections for greedy layer-wise pretraining");
+
     declareOption(ol, "classification_module",
                   &DeepBeliefNet::classification_module,
                   OptionBase::learntoption,
@@ -492,7 +500,18 @@
     else
         reconstruction_costs.resize(0);
 
+    if( !greedy_target_layers.isEmpty() )
+    {
+        greedy_target_layer_nlls_index = current_index;
+        target_one_hot.resize(n_classes);
+        for( int i=0; i<n_layers-1; i++ )
+        {
+            cost_names.append("layer"+tostring(i)+".nll");
+            current_index++;
+        }
+    }
 
+
     cost_names.append("cpu_time");
     cost_names.append("cumulative_train_time");
     cost_names.append("cumulative_test_time");
@@ -556,6 +575,78 @@
         activation_gradients[i].resize( layers[i]->size );
         expectation_gradients[i].resize( layers[i]->size );
 
+
+        if( greedy_target_layers.length()>i && greedy_target_layers[i] )
+        {
+            if( use_classification_cost )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_classification_cost not implemented for greedy_target_layers.");
+
+            if( greedy_target_connections.length()>i && !greedy_target_connections[i] )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "some greedy_target_connections are missing.");
+
+            if( greedy_target_layers[i]->size != n_classes)
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "greedy_target_layers[%d] should be of size %d.",i,n_classes);
+
+            if( greedy_target_connections[i]->down_size != n_classes ||
+                greedy_target_connections[i]->up_size != layers[i+1]->size )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "greedy_target_connections[%d] should be of size (%d,%d).",
+                        i,layers[i+1]->size,n_classes);
+                
+            if( partial_costs.length() != 0 )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "greedy_target_layers can't be used with partial_costs.");
+                
+            greedy_target_expectations.resize(n_layers-1);
+            greedy_target_activations.resize(n_layers-1);
+            greedy_target_expectation_gradients.resize(n_layers-1);
+            greedy_target_activation_gradients.resize(n_layers-1);
+            greedy_target_probability_gradients.resize(n_layers-1);
+
+            greedy_target_expectations[i].resize(n_classes);
+            greedy_target_activations[i].resize(n_classes);
+            greedy_target_expectation_gradients[i].resize(n_classes);
+            greedy_target_activation_gradients[i].resize(n_classes);
+            greedy_target_probability_gradients[i].resize(n_classes);
+            for( int c=0; c<n_classes; c++) 
+            {
+                greedy_target_expectations[i][c].resize(layers[i+1]->size);
+                greedy_target_activations[i][c].resize(layers[i+1]->size);
+                greedy_target_expectation_gradients[i][c].resize(layers[i+1]->size);
+                greedy_target_activation_gradients[i][c].resize(layers[i+1]->size);
+            }
+
+            greedy_joint_layers.resize(n_layers-1);
+            PP<RBMMixedLayer> ml = new RBMMixedLayer();
+            ml->sub_layers.resize(2);
+            ml->sub_layers[0] = layers[ i ];
+            ml->sub_layers[1] = greedy_target_layers[ i ];
+            ml->random_gen = random_gen;
+            ml->build();
+            greedy_joint_layers[i] = (RBMMixedLayer *)ml;
+
+            greedy_joint_connections.resize(n_layers-1);
+            PP<RBMMixedConnection> mc = new RBMMixedConnection();
+            mc->sub_connections.resize(1,2);
+            mc->sub_connections(0,0) = connections[i];
+            mc->sub_connections(0,1) = greedy_target_connections[i];
+            mc->build();
+            greedy_joint_connections[i] = (RBMMixedConnection *)mc;
+
+            if( !(greedy_target_connections[i]->random_gen) )
+            {
+                greedy_target_connections[i]->random_gen = random_gen;
+                greedy_target_connections[i]->forget();
+            }
+            if( !(greedy_target_layers[i]->random_gen) )
+            {
+                greedy_target_layers[i]->random_gen = random_gen;
+                greedy_target_layers[i]->forget();
+            }
+        }
     }
     if( !(layers[n_layers-1]->random_gen) )
     {
@@ -726,6 +817,8 @@
     deepCopyField(training_schedule,        copies);
     deepCopyField(layers,                   copies);
     deepCopyField(connections,              copies);
+    deepCopyField(greedy_target_layers,     copies);
+    deepCopyField(greedy_target_connections,copies);
     deepCopyField(final_module,             copies);
     deepCopyField(final_cost,               copies);
     deepCopyField(partial_costs,            copies);
@@ -738,6 +831,13 @@
     deepCopyField(activations_gradients,    copies);
     deepCopyField(expectation_gradients,    copies);
     deepCopyField(expectations_gradients,   copies);
+    deepCopyField(greedy_target_expectations,copies);
+    deepCopyField(greedy_target_activations, copies);
+    deepCopyField(greedy_target_expectation_gradients,copies);
+    deepCopyField(greedy_target_activation_gradients,copies);
+    deepCopyField(greedy_target_probability_gradients,copies);
+    deepCopyField(greedy_joint_layers   ,   copies);
+    deepCopyField(greedy_joint_connections, copies);
     deepCopyField(final_cost_input,         copies);
     deepCopyField(final_cost_inputs,        copies);
     deepCopyField(final_cost_value,         copies);
@@ -753,14 +853,17 @@
     deepCopyField(save_layer_expectations,  copies);
     deepCopyField(pos_down_val,             copies);
     deepCopyField(pos_up_val,               copies);
+    deepCopyField(pos_down_vals,            copies);
+    deepCopyField(pos_up_vals,              copies);
+    deepCopyField(cd_neg_down_vals,         copies);
     deepCopyField(cd_neg_up_vals,           copies);
-    deepCopyField(cd_neg_down_vals,         copies);
+    deepCopyField(mf_cd_neg_down_vals,      copies);
     deepCopyField(mf_cd_neg_up_vals,        copies);
-    deepCopyField(mf_cd_neg_down_vals,      copies);
+    deepCopyField(mf_cd_neg_down_val,       copies);
     deepCopyField(mf_cd_neg_up_val,         copies);
-    deepCopyField(mf_cd_neg_down_val,       copies);
     deepCopyField(gibbs_down_state,         copies);
     deepCopyField(optimized_costs,          copies);
+    deepCopyField(target_one_hot,           copies);
     deepCopyField(reconstruction_costs,     copies);
     deepCopyField(partial_costs_indices,    copies);
     deepCopyField(cumulative_schedule,      copies);
@@ -820,6 +923,15 @@
             if( partial_costs[i] )
                 partial_costs[i]->forget();
 
+    for( int i=0 ; i<generative_connections.length() ; i++ )
+        generative_connections[i]->forget();
+
+    for( int i=0; i<greedy_target_connections.length(); i++ )
+        greedy_target_connections[i]->forget();
+
+    for( int i=0; i<greedy_target_layers.length(); i++ )
+        greedy_target_layers[i]->forget();
+
     cumulative_training_time = 0;
     cumulative_testing_time = 0;
     up_down_stage = 0;
@@ -976,6 +1088,15 @@
             connections[i]->setLearningRate( cd_learning_rate );
             layers[i+1]->setLearningRate( cd_learning_rate );
 
+            if( greedy_target_layers.length() && greedy_target_layers[i] )
+                greedy_target_layers[i]->setLearningRate( cd_learning_rate );
+            if( greedy_target_connections.length() && greedy_target_connections[i] )
+                greedy_target_connections[i]->setLearningRate( cd_learning_rate );
+            if( greedy_joint_layers.length() && greedy_joint_layers[i] )
+                greedy_joint_layers[i]->setLearningRate( cd_learning_rate );
+            if( greedy_joint_connections.length() && greedy_joint_connections[i] )
+                greedy_joint_connections[i]->setLearningRate( cd_learning_rate );
+
             for( ; stage<end_stage ; stage++ )
             {
                 if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
@@ -987,6 +1108,14 @@
                     layers[i]->setLearningRate( lr );
                     connections[i]->setLearningRate( lr );
                     layers[i+1]->setLearningRate( lr );
+                    if( greedy_target_layers.length() && greedy_target_layers[i] )
+                        greedy_target_layers[i]->setLearningRate( lr );
+                    if( greedy_target_connections.length() && greedy_target_connections[i] )
+                        greedy_target_connections[i]->setLearningRate( lr );
+                    if( greedy_joint_layers.length() && greedy_joint_layers[i] )
+                        greedy_joint_layers[i]->setLearningRate( lr );
+                    if( greedy_joint_connections.length() && greedy_joint_connections[i] )
+                        greedy_joint_connections[i]->setLearningRate( lr );
                 }
 
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
@@ -1203,6 +1332,10 @@
     real lr;
     PLASSERT(batch_size == 1);
 
+    if( greedy_target_layers.length() )
+        PLERROR("In DeepBeliefNet::onlineStep(): greedy_target_layers not implemented\n"
+                "for online setting");
+
     TVec<Vec> cost;
     if (!partial_costs.isEmpty())
         cost.resize(n_layers-1);
@@ -1428,6 +1561,10 @@
         cost.resize(n_layers-1);
     }
 
+    if( greedy_target_layers.length() )
+        PLERROR("In DeepBeliefNet::onlineStep(): greedy_target_layers not implemented\n"
+                "for online setting");
+
     layers[0]->setExpectations(inputs);
     // FORWARD PHASE
     //Vec layer_input;
@@ -1683,9 +1820,56 @@
     layers[0]->expectation << input;
     for( int i=0 ; i<=index ; i++ )
     {
-        connections[i]->setAsDownInput( layers[i]->expectation );
-        layers[i+1]->getAllActivations( connections[i] );
-        layers[i+1]->computeExpectation();
+        if( greedy_target_layers.length() && greedy_target_layers[i] )
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+
+            if( i != index )
+            {
+                greedy_target_layers[i]->activation.clear();
+                greedy_target_layers[i]->activation += greedy_target_layers[i]->bias;
+                for( int c=0; c<n_classes; c++ )
+                {
+                    // Compute class free-energy
+                    layers[i+1]->activation.toMat(layers[i+1]->size,1) += 
+                        greedy_target_connections[i]->weights.column(c);
+                    greedy_target_layers[i]->activation[c] -= 
+                        layers[i+1]->freeEnergyContribution(layers[i+1]->activation);
+                    
+                    // Compute class dependent expectation and store it
+                    layers[i+1]->expectation_is_not_up_to_date();
+                    layers[i+1]->computeExpectation();
+                    greedy_target_expectations[i][c] << layers[i+1]->expectation;
+                    
+                    // Remove class-dependent energy for next free-energy computations
+                    layers[i+1]->activation.toMat(layers[i+1]->size,1) -= greedy_target_connections[i]->weights.column(c);
+                }
+                greedy_target_layers[i]->expectation_is_not_up_to_date();
+                greedy_target_layers[i]->computeExpectation();
+            
+                // Computing next layer representation
+                layers[i+1]->expectation.clear();
+                Vec expectation = layers[i+1]->expectation;
+                for( int c=0; c<n_classes; c++ )
+                {
+                    Vec expectation_c = greedy_target_expectations[i][c];
+                    real p_c = greedy_target_layers[i]->expectation[c];
+                    multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+                }
+            }
+            else
+            {
+                fill_one_hot( greedy_target_layers[i]->expectation, 
+                              (int) round(target[0]), real(0.), real(1.) );
+            }
+        }
+        else
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+            layers[i+1]->computeExpectation();
+        }
     }
 
     if( !partial_costs.isEmpty() && partial_costs[ index ] )
@@ -1733,10 +1917,20 @@
         layers[ index+1 ]->setLearningRate( lr );
     }
 
-    contrastiveDivergenceStep( layers[ index ],
-                               connections[ index ],
-                               layers[ index+1 ],
-                               index, true);
+    if( greedy_target_layers.length() && greedy_target_layers[index] )
+    {
+        contrastiveDivergenceStep( greedy_joint_layers[ index ],
+                                   greedy_joint_connections[ index ],
+                                   layers[ index+1 ],
+                                   index, false);
+    }
+    else
+    {
+        contrastiveDivergenceStep( layers[ index ],
+                                   connections[ index ],
+                                   layers[ index+1 ],
+                                   index, true);
+    }
 }
 
 /////////////////
@@ -1749,6 +1943,11 @@
     PLASSERT( index < n_layers );
 
     layers[0]->setExpectations(inputs);
+
+    if( greedy_target_layers.length() && greedy_target_layers[0] )
+        PLERROR("In DeepBeliefNet::greedyStep(): greedy_target_layers not implemented\n"
+                "for minibatch setting");
+
     for( int i=0 ; i<=index ; i++ )
     {
         connections[i]->setAsDownInputs( layers[i]->getExpectations() );
@@ -1911,6 +2110,11 @@
 void DeepBeliefNet::upDownStep( const Vec& input, const Vec& target,
                                 Vec& train_costs )
 {
+
+    if( greedy_target_layers.length() )
+        PLERROR("In DeepBeliefNet::onlineStep(): greedy_target_layers not implemented\n"
+                "for up-down setting");
+
     // Up pass
     up_sample[0] << input;
     for( int i=0 ; i<n_layers-2 ; i++ )
@@ -1997,18 +2201,96 @@
     layers[0]->expectation << input;
     for( int i=0 ; i<n_layers-2 ; i++ )
     {
-        connections[i]->setAsDownInput( layers[i]->expectation );
-        layers[i+1]->getAllActivations( connections[i] );
-        layers[i+1]->computeExpectation();
+        if( greedy_target_layers.length() && greedy_target_layers[i] )
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+            
+            greedy_target_layers[i]->activation.clear();
+            greedy_target_layers[i]->activation += greedy_target_layers[i]->bias;
+            for( int c=0; c<n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[i+1]->activation.toMat(layers[i+1]->size,1) += greedy_target_connections[i]->weights.column(c);
+                greedy_target_layers[i]->activation[c] -= layers[i+1]->freeEnergyContribution(layers[i+1]->activation);
+                
+                // Compute class dependent expectation and store it
+                layers[i+1]->expectation_is_not_up_to_date();
+                layers[i+1]->computeExpectation();
+                greedy_target_expectations[i][c] << layers[i+1]->expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[i+1]->activation.toMat(layers[i+1]->size,1) -= greedy_target_connections[i]->weights.column(c);
+            }
+            greedy_target_layers[i]->expectation_is_not_up_to_date();
+            greedy_target_layers[i]->computeExpectation();
+            
+            // Computing next layer representation
+            layers[i+1]->expectation.clear();
+            Vec expectation = layers[i+1]->expectation;
+            for( int c=0; c<n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[i][c];
+                real p_c = greedy_target_layers[i]->expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+            layers[i+1]->computeExpectation();
+        }
     }
 
     if( final_cost )
     {
-        connections[ n_layers-2 ]->setAsDownInput(
-            layers[ n_layers-2 ]->expectation );
-        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
-        layers[ n_layers-1 ]->computeExpectation();
-
+        if( greedy_target_layers.length() && greedy_target_layers[n_layers-2] )
+        {
+            connections[n_layers-2]->setAsDownInput( layers[n_layers-2]->expectation );
+            layers[n_layers-1]->getAllActivations( connections[n_layers-2] );
+            
+            greedy_target_layers[n_layers-2]->activation.clear();
+            greedy_target_layers[n_layers-2]->activation += 
+                greedy_target_layers[n_layers-2]->bias;
+            for( int c=0; c<n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[n_layers-1]->activation.toMat(layers[n_layers-1]->size,1) += 
+                    greedy_target_connections[n_layers-2]->weights.column(c);
+                greedy_target_layers[n_layers-2]->activation[c] -= 
+                    layers[n_layers-1]->freeEnergyContribution(layers[n_layers-1]->activation);
+                
+                // Compute class dependent expectation and store it
+                layers[n_layers-1]->expectation_is_not_up_to_date();
+                layers[n_layers-1]->computeExpectation();
+                greedy_target_expectations[n_layers-2][c] << layers[n_layers-1]->expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[n_layers-1]->activation.toMat(layers[n_layers-1]->size,1) -= 
+                    greedy_target_connections[n_layers-2]->weights.column(c);
+            }
+            greedy_target_layers[n_layers-2]->expectation_is_not_up_to_date();
+            greedy_target_layers[n_layers-2]->computeExpectation();
+            
+            // Computing next layer representation
+            layers[n_layers-1]->expectation.clear();
+            Vec expectation = layers[n_layers-1]->expectation;
+            for( int c=0; c<n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[n_layers-2][c];
+                real p_c = greedy_target_layers[n_layers-2]->expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[ n_layers-2 ]->setAsDownInput(
+                layers[ n_layers-2 ]->expectation );
+            layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
+            layers[ n_layers-1 ]->computeExpectation();
+        }
+        
         if( final_module )
         {
             final_module->fprop( layers[ n_layers-1 ]->expectation,
@@ -2036,17 +2318,78 @@
         train_costs.subVec(final_cost_index, final_cost_value.length())
             << final_cost_value;
 
-        layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
-                                           layers[ n_layers-1 ]->expectation,
-                                           activation_gradients[ n_layers-1 ],
-                                           expectation_gradients[ n_layers-1 ]
-                                         );
+        if( greedy_target_layers.length() && greedy_target_layers[n_layers-2] )
+        {
+            activation_gradients[n_layers-1].clear();
+            for( int c=0; c<n_classes; c++ )
+            {
+                greedy_target_expectation_gradients[n_layers-2][c] << 
+                    expectation_gradients[ n_layers-1 ];
+                greedy_target_expectation_gradients[n_layers-2][c] *= 
+                    greedy_target_layers[n_layers-2]->expectation[c];
+                layers[ n_layers-1 ]->bpropUpdate( 
+                    greedy_target_activations[n_layers-2][c],
+                    greedy_target_expectations[n_layers-2][c],
+                    greedy_target_activation_gradients[n_layers-2][c],
+                    greedy_target_expectation_gradients[n_layers-2][c] );
 
-        connections[ n_layers-2 ]->bpropUpdate(
-            layers[ n_layers-2 ]->expectation,
-            layers[ n_layers-1 ]->activation,
-            expectation_gradients[ n_layers-2 ],
-            activation_gradients[ n_layers-1 ] );
+                activation_gradients[n_layers-1] += 
+                    greedy_target_activation_gradients[n_layers-2][c];
+
+                // Update target connections, with gradient from p(h_l | h_l-1, y)
+                multiplyScaledAdd( greedy_target_activation_gradients[n_layers-2][c].toMat(layers[n_layers-1]->size,1),
+                                   1., -greedy_target_connections[n_layers-2]->learning_rate,
+                                   greedy_target_connections[n_layers-2]->weights.column(c));
+                
+                greedy_target_probability_gradients[n_layers-2][c] = 
+                    dot( expectation_gradients[ n_layers-1 ], 
+                         greedy_target_expectations[ n_layers-2 ][c] );
+            }
+
+            // Update bias
+            greedy_target_layers[n_layers-2]->bpropUpdate(
+                greedy_target_layers[n_layers-2]->expectation, // Isn't used
+                greedy_target_layers[n_layers-2]->expectation,
+                greedy_target_probability_gradients[n_layers-2], 
+                greedy_target_probability_gradients[n_layers-2] );
+
+            for( int c=0; c<n_classes; c++ )
+            {
+                layers[n_layers-1]->freeEnergyContributionGradient(
+                    greedy_target_activations[n_layers-2][c],
+                    greedy_target_activation_gradients[n_layers-2][c], // Overwrite previous activation gradient
+                    -greedy_target_probability_gradients[n_layers-2][c] );
+
+                activation_gradients[n_layers-1] += 
+                    greedy_target_activation_gradients[n_layers-2][c];
+
+                // Update target connections, with gradient from p(y | h_l-1 )
+                multiplyScaledAdd( greedy_target_activation_gradients[n_layers-2][c].toMat(layers[n_layers-1]->size,1),
+                                   1., -greedy_target_connections[n_layers-2]->learning_rate,
+                                   greedy_target_connections[n_layers-2]->weights.column(c));
+            }
+
+            connections[ n_layers-2 ]->bpropUpdate(
+                layers[ n_layers-2 ]->expectation,
+                layers[ n_layers-1 ]->activation, //Not really, but this isn't used for matrix connections
+                expectation_gradients[ n_layers-2 ],
+                activation_gradients[ n_layers-1 ] );
+            
+        }
+        else
+        {
+            layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
+                                               layers[ n_layers-1 ]->expectation,
+                                               activation_gradients[ n_layers-1 ],
+                                               expectation_gradients[ n_layers-1 ]
+                );
+            
+            connections[ n_layers-2 ]->bpropUpdate(
+                layers[ n_layers-2 ]->expectation,
+                layers[ n_layers-1 ]->activation,
+                expectation_gradients[ n_layers-2 ],
+                activation_gradients[ n_layers-1 ] );
+        }
     }
     else  {
         expectation_gradients[ n_layers-2 ].clear();
@@ -2082,21 +2425,85 @@
 
     for( int i=n_layers-2 ; i>0 ; i-- )
     {
-        layers[i]->bpropUpdate( layers[i]->activation,
-                                layers[i]->expectation,
-                                activation_gradients[i],
-                                expectation_gradients[i] );
+        if( greedy_target_layers.length() && greedy_target_layers[i] )
+        {
+            activation_gradients[i-1].clear();
+            for( int c=0; c<n_classes; c++ )
+            {
+                greedy_target_expectation_gradients[i-1][c] << 
+                    expectation_gradients[ i ];
+                greedy_target_expectation_gradients[i-1][c] *= 
+                    greedy_target_layers[i-1]->expectation[c];
+                layers[ i ]->bpropUpdate( 
+                    greedy_target_activations[i-1][c],
+                    greedy_target_expectations[i-1][c],
+                    greedy_target_activation_gradients[i-1][c],
+                    greedy_target_expectation_gradients[i-1][c] );
 
-        connections[i-1]->bpropUpdate( layers[i-1]->expectation,
-                                       layers[i]->activation,
-                                       expectation_gradients[i-1],
-                                       activation_gradients[i] );
+                activation_gradients[i ] += 
+                    greedy_target_activation_gradients[i-1][c];
+
+                // Update target connections, with gradient from p(h_l | h_l-1, y)
+                multiplyScaledAdd( greedy_target_activation_gradients[i-1][c].toMat(layers[i]->size,1),
+                                   1., -greedy_target_connections[i-1]->learning_rate,
+                                   greedy_target_connections[i-1]->weights.column(c));
+                
+                greedy_target_probability_gradients[i-1][c] = 
+                    dot( expectation_gradients[ i ], 
+                         greedy_target_expectations[ i-1 ][c] );
+            }
+
+            // Update bias
+            greedy_target_layers[i-1]->bpropUpdate(
+                greedy_target_layers[i-1]->expectation, // Isn't used
+                greedy_target_layers[i-1]->expectation,
+                greedy_target_probability_gradients[i-1], 
+                greedy_target_probability_gradients[i-1] );
+
+            for( int c=0; c<n_classes; c++ )
+            {
+                layers[i]->freeEnergyContributionGradient(
+                    greedy_target_activations[i-1][c],
+                    greedy_target_activation_gradients[i-1][c], // Overwrite previous activation gradient
+                    -greedy_target_probability_gradients[i-1][c] );
+
+                activation_gradients[i] += 
+                    greedy_target_activation_gradients[i-1][c];
+
+                // Update target connections, with gradient from p(y | h_l-1 )
+                multiplyScaledAdd( greedy_target_activation_gradients[i-1][c].toMat(layers[i]->size,1),
+                                   1., -greedy_target_connections[i-1]->learning_rate,
+                                   greedy_target_connections[i-1]->weights.column(c));
+            }
+
+            connections[ i-1 ]->bpropUpdate(
+                layers[ i-1 ]->expectation,
+                layers[ i ]->activation, //Not really, but this isn't used for matrix connections
+                expectation_gradients[ i-1 ],
+                activation_gradients[ i ] );
+        }
+        else
+        {
+            layers[i]->bpropUpdate( layers[i]->activation,
+                                    layers[i]->expectation,
+                                    activation_gradients[i],
+                                    expectation_gradients[i] );
+            
+            connections[i-1]->bpropUpdate( layers[i-1]->expectation,
+                                           layers[i]->activation,
+                                           expectation_gradients[i-1],
+                                           activation_gradients[i] );
+        }
     }
 }
 
 void DeepBeliefNet::fineTuningStep(const Mat& inputs, const Mat& targets,
                                    Mat& train_costs)
 {
+    if( greedy_target_layers.length() )
+        PLERROR("In DeepBeliefNet::fineTuningStep(): greedy_target_layers not implemented\n"
+                "for minibatch setting");
+
     final_cost_values.resize(0, 0);
     // fprop
     layers[0]->getExpectations() << inputs;
@@ -2530,10 +2937,46 @@
 
     for( int i=0 ; i<n_layers-2 ; i++ )
     {
-        connections[i]->setAsDownInput( layers[i]->expectation );
-        layers[i+1]->getAllActivations( connections[i] );
-        layers[i+1]->computeExpectation();
-
+        if( greedy_target_layers.length() && greedy_target_layers[i] )
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+            
+            greedy_target_layers[i]->activation.clear();
+            greedy_target_layers[i]->activation += greedy_target_layers[i]->bias;
+            for( int c=0; c<n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[i+1]->activation.toMat(layers[i+1]->size,1) += greedy_target_connections[i]->weights.column(c);
+                greedy_target_layers[i]->activation[c] -= layers[i+1]->freeEnergyContribution(layers[i+1]->activation);
+                
+                // Compute class dependent expectation and store it
+                layers[i+1]->expectation_is_not_up_to_date();
+                layers[i+1]->computeExpectation();
+                greedy_target_expectations[i][c] << layers[i+1]->expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[i+1]->activation.toMat(layers[i+1]->size,1) -= greedy_target_connections[i]->weights.column(c);
+            }
+            greedy_target_layers[i]->expectation_is_not_up_to_date();
+            greedy_target_layers[i]->computeExpectation();
+            
+            // Computing next layer representation
+            layers[i+1]->expectation.clear();
+            Vec expectation = layers[i+1]->expectation;
+            for( int c=0; c<n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[i][c];
+                real p_c = greedy_target_layers[i]->expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i] );
+            layers[i+1]->computeExpectation();
+        }
         if( i_output_layer==i && (!use_classification_cost && !final_module))
         {
             output.resize(outputsize());
@@ -2569,10 +3012,51 @@
 
     if( final_cost || (!partial_costs.isEmpty() && partial_costs[n_layers-2] ))
     {
-        connections[ n_layers-2 ]->setAsDownInput(
-            layers[ n_layers-2 ]->expectation );
-        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
-        layers[ n_layers-1 ]->computeExpectation();
+        if( greedy_target_layers.length() && greedy_target_layers[n_layers-2] )
+        {
+            connections[n_layers-2]->setAsDownInput( layers[n_layers-2]->expectation );
+            layers[n_layers-1]->getAllActivations( connections[n_layers-2] );
+            
+            greedy_target_layers[n_layers-2]->activation.clear();
+            greedy_target_layers[n_layers-2]->activation += 
+                greedy_target_layers[n_layers-2]->bias;
+            for( int c=0; c<n_classes; c++ )
+            {
+                // Compute class free-energy
+                layers[n_layers-1]->activation.toMat(layers[n_layers-1]->size,1) += 
+                    greedy_target_connections[n_layers-2]->weights.column(c);
+                greedy_target_layers[n_layers-2]->activation[c] -= 
+                    layers[n_layers-1]->freeEnergyContribution(layers[n_layers-1]->activation);
+                
+                // Compute class dependent expectation and store it
+                layers[n_layers-1]->expectation_is_not_up_to_date();
+                layers[n_layers-1]->computeExpectation();
+                greedy_target_expectations[n_layers-2][c] << layers[n_layers-1]->expectation;
+                
+                // Remove class-dependent energy for next free-energy computations
+                layers[n_layers-1]->activation.toMat(layers[n_layers-1]->size,1) -= 
+                    greedy_target_connections[n_layers-2]->weights.column(c);
+            }
+            greedy_target_layers[n_layers-2]->expectation_is_not_up_to_date();
+            greedy_target_layers[n_layers-2]->computeExpectation();
+            
+            // Computing next layer representation
+            layers[n_layers-1]->expectation.clear();
+            Vec expectation = layers[n_layers-1]->expectation;
+            for( int c=0; c<n_classes; c++ )
+            {
+                Vec expectation_c = greedy_target_expectations[n_layers-2][c];
+                real p_c = greedy_target_layers[n_layers-2]->expectation[c];
+                multiplyScaledAdd(expectation_c, 1., p_c, expectation);
+            }
+        }
+        else
+        {
+            connections[ n_layers-2 ]->setAsDownInput(
+                layers[ n_layers-2 ]->expectation );
+            layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
+            layers[ n_layers-1 ]->computeExpectation();
+        }
 
         if( final_module )
         {
@@ -2656,6 +3140,19 @@
             }
     }
 
+    if( !greedy_target_layers.isEmpty() )
+    {
+        target_one_hot.clear();
+        fill_one_hot( target_one_hot, 
+                      (int) round(target[0]), real(0.), real(1.) );
+        for( int i=0 ; i<n_layers-1 ; i++ )
+            if( greedy_target_layers[i] )
+                costs[greedy_target_layer_nlls_index+i] = 
+                    greedy_target_layers[i]->fpropNLL(target_one_hot);
+            else
+                costs[greedy_target_layer_nlls_index+i] = MISSING_VALUE;
+    }
+
     if (reconstruct_layerwise)
         costs.subVec(reconstruction_cost_index, reconstruction_costs.length())
             << reconstruction_costs;
@@ -2831,6 +3328,12 @@
 
     for( int i=0 ; i<generative_connections.length() ; i++ )
         generative_connections[i]->setLearningRate( the_learning_rate );
+
+    for( int i=0; i<greedy_target_connections.length(); i++ )
+        greedy_target_connections[i]->setLearningRate( the_learning_rate );
+
+    for( int i=0; i<greedy_target_layers.length(); i++ )
+        greedy_target_layers[i]->setLearningRate( the_learning_rate );
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2009-01-28 17:51:31 UTC (rev 9881)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2009-01-28 21:55:40 UTC (rev 9882)
@@ -140,6 +140,12 @@
     //! The weights of the connections between the layers
     TVec< PP<RBMConnection> > connections;
 
+    //! Optional target layers for greedy layer-wise pretraining
+    TVec< PP<RBMMultinomialLayer> > greedy_target_layers;
+
+    //! Optional target matrix connections for greedy layer-wise pretraining
+    TVec< PP<RBMMatrixConnection> > greedy_target_connections;
+
     //! Optional module that takes as input the output of the last layer
     //! (layers[n_layers-1), and its output is fed to final_cost, and
     //! concatenated with the one of classification_cost (if present) as output
@@ -339,6 +345,15 @@
     //! (at the output of the layers)
     mutable TVec<Vec> expectation_gradients;
     mutable TVec<Mat> expectations_gradients; //!< For mini-batch.
+    
+    //! For the fprop with greedy_target_layers
+    mutable TVec< TVec<Vec> > greedy_target_expectations;
+    mutable TVec< TVec<Vec> > greedy_target_activations;
+    mutable TVec< TVec<Vec> > greedy_target_expectation_gradients;
+    mutable TVec< TVec<Vec> > greedy_target_activation_gradients;
+    mutable TVec< Vec > greedy_target_probability_gradients;
+    mutable TVec< PP<RBMLayer> > greedy_joint_layers;
+    mutable TVec< PP<RBMConnection> > greedy_joint_connections;
 
     mutable Vec final_cost_input;
     mutable Mat final_cost_inputs; //!< For mini-batch.
@@ -382,6 +397,9 @@
     //! Used to store the costs optimized by the final cost module.
     Vec optimized_costs;
 
+    //! One-hot representation of the target
+    mutable Vec target_one_hot;
+
     //! Stores reconstruction costs
     mutable Vec reconstruction_costs;
 
@@ -400,6 +418,9 @@
     //! Keeps the beginning index of the reconstruction costs in train_costs
     int reconstruction_cost_index;
 
+    //! Keeps the beginning index of the greedy target layer NLLs
+    int greedy_target_layer_nlls_index;
+
     //! Index of the cpu time cost (per each call of train())
     int training_cpu_time_cost_index;
 



From laulysta at mail.berlios.de  Fri Jan 30 04:30:16 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 30 Jan 2009 04:30:16 +0100
Subject: [Plearn-commits] r9883 - trunk/plearn_learners_experimental
Message-ID: <200901300330.n0U3UG40000920@sheep.berlios.de>

Author: laulysta
Date: 2009-01-30 04:30:15 +0100 (Fri, 30 Jan 2009)
New Revision: 9883

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
accumulate gradient for matrice connections


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-01-28 21:55:40 UTC (rev 9882)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-01-30 03:30:15 UTC (rev 9883)
@@ -999,6 +999,13 @@
     }    
 }
 
+Mat DenoisingRecurrentNet::getTargetConnectionsWeightMatrix(int tar)
+{
+    RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)target_connections[tar]);
+    if(conn==0)
+        PLERROR("Expecting input connection to be a RBMMatrixConnection. Je sais c'est sale, mais au point ou on est rendu..");
+    return conn->weights;
+}
 
 Mat DenoisingRecurrentNet::getInputConnectionsWeightMatrix()
 {
@@ -1024,6 +1031,95 @@
     return conn->weights;
 }
 
+void DenoisingRecurrentNet::updateTargetLayer( Vec& grad, Vec& bias, real& lr )
+{
+    real* b = bias.data();
+    real* gb = grad.data();
+    int size = bias.length();
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        
+        b[i] -= lr * gb[i];
+        
+    }
+
+   
+}
+
+void DenoisingRecurrentNet::bpropUpdateConnection(const Vec& input, 
+                                                  const Vec& output,
+                                                  Vec& input_gradient,
+                                                  const Vec& output_gradient,
+                                                  Mat& weights,
+                                                  Mat& acc_weights_gr,
+                                                  int& down_size,
+                                                  int& up_size,
+                                                  real& lr,
+                                                  bool accumulate)
+{
+    PLASSERT( input.size() == down_size );
+    PLASSERT( output.size() == up_size );
+    PLASSERT( output_gradient.size() == up_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == down_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+
+        // input_gradient += weights' * output_gradient
+        transposeProductAcc( input_gradient, weights, output_gradient );
+    }
+    else
+    {
+        input_gradient.resize( down_size );
+
+        // input_gradient = weights' * output_gradient
+        transposeProduct( input_gradient, weights, output_gradient );
+    }
+
+    // weights -= learning_rate * output_gradient * input'
+    //externalProductScaleAcc( weights, output_gradient, input, -lr );
+    externalProductScaleAcc( acc_weights_gr, output_gradient, input, -lr );
+    
+   
+}
+
+void DenoisingRecurrentNet::bpropUpdateHiddenLayer(const Vec& input, 
+                                                   const Vec& output,
+                                                   Vec& input_gradient,
+                                                   const Vec& output_gradient,                                                
+                                                   Vec& bias,
+                                                   real& lr)
+{
+
+    int size = bias.length();
+
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    
+    input_gradient.resize( size );
+    input_gradient.clear();
+    
+    
+    for( int i=0 ; i<size ; i++ )
+    {
+        real output_i = output[i];
+        real in_grad_i;
+        in_grad_i = output_i * (1-output_i) * output_gradient[i];
+        input_gradient[i] += in_grad_i;
+        
+       
+        // update the bias: bias -= learning_rate * input_gradient
+        bias[i] -= lr * in_grad_i;
+        
+    }
+    
+    //applyBiasDecay();
+}
+
 double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
                                                                        Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
@@ -1101,7 +1197,7 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                  Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
 {
     // set appropriate sizes
@@ -1116,8 +1212,8 @@
     reconstruction_prob.resize(fullhiddenlength);
 
     // predict (denoised) input_reconstruction 
-    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden);
-    product(reconstruction_activation, reconstruction_weights, hidden);
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden);
+    //product(reconstruction_activation, reconstruction_weights, hidden);
     reconstruction_activation += reconstruction_bias;
 
     for( int j=0 ; j<fullhiddenlength ; j++ )
@@ -1133,17 +1229,17 @@
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
 
 
-    //productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
-    transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    //transposeProductAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
     
     //update bias
     multiplyAcc(reconstruction_bias, hidden_reconstruction_activation_grad, -lr);
     // update weight
-    externalProductScaleAcc(reconstruction_weights, hidden_reconstruction_activation_grad, hidden, -lr);
+    //externalProductScaleAcc(reconstruction_weights, hidden_reconstruction_activation_grad, hidden, -lr);
                 
 
     // update weight
-    //externalProductScaleAcc(reconstruction_weights, hidden, hidden_reconstruction_activation_grad, -lr);
+    externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr);
     /********************************************************************************/
 
     double result_cost = 0;
@@ -1330,6 +1426,36 @@
                                             real temporal_gradient_contribution,
                                             real predic_cost_weight)
 {
+    TVec < Mat> targetWeights ;
+    Mat inputWeights;
+    Mat dynamicWeights;
+    Mat reconsWeights;
+    targetWeights.resize(target_connections.length());
+    for( int tar=0; tar<target_layers.length(); tar++)
+    {
+       targetWeights[tar] = getTargetConnectionsWeightMatrix(tar);
+    }
+    inputWeights = getInputConnectionsWeightMatrix();
+    if(dynamic_connections )
+    { 
+        dynamicWeights = getDynamicConnectionsWeightMatrix();
+        reconsWeights = getDynamicReconstructionConnectionsWeightMatrix();
+    }
+    acc_target_connections_gr.resize(target_connections.length());
+    for( int tar=0; tar<target_layers.length(); tar++)
+    {
+        acc_target_connections_gr[tar].resize(target_connections[tar]->up_size, target_connections[tar]->down_size);
+        acc_target_connections_gr[tar].clear();
+    }
+    acc_input_connections_gr.resize(input_connections->up_size, input_connections->down_size);
+    acc_input_connections_gr.clear();
+    if(dynamic_connections )
+    { 
+        acc_dynamic_connections_gr.resize(dynamic_connections->up_size, dynamic_connections->down_size);
+        acc_dynamic_connections_gr.clear();
+    }
+
+
     hidden_temporal_gradient.resize(hidden_layer->size);
     hidden_temporal_gradient.clear();
     for(int i=hidden_list.length()-1; i>=0; i--){   
@@ -1352,13 +1478,37 @@
                     bias_gradient *= predic_cost_weight;
                     if(use_target_layers_masks)
                         bias_gradient *= masks_list[tar](i);
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                             hidden_gradient, bias_gradient,true);
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                             hidden_gradient, bias_gradient,true);
+                    //target_layers[tar]->update(bias_gradient);
+                    updateTargetLayer( bias_gradient, 
+                                       target_layers[tar]->bias, 
+                                       target_layers[tar]->learning_rate );
+                    //Mat targetWeights = getTargetConnectionsWeightMatrix(tar);
+                    if( hidden_layer2 ){
+                        //target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),hidden_gradient, bias_gradient,true);
+                        bpropUpdateConnection(hidden2_list(i),
+                                              target_prediction_act_no_bias_list[tar](i),
+                                              hidden_gradient, 
+                                              bias_gradient,
+                                              targetWeights[tar],
+                                              acc_target_connections_gr[tar],
+                                              target_connections[tar]->down_size,
+                                              target_connections[tar]->up_size,
+                                              target_connections[tar]->learning_rate,
+                                              true);
+                    }
+                    else{
+                        //target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),hidden_gradient, bias_gradient,true);
+                        bpropUpdateConnection(hidden_list(i),
+                                              target_prediction_act_no_bias_list[tar](i),
+                                              hidden_gradient, 
+                                              bias_gradient,
+                                              targetWeights[tar],
+                                              acc_target_connections_gr[tar],
+                                              target_connections[tar]->down_size,
+                                              target_connections[tar]->up_size,
+                                              target_connections[tar]->learning_rate,
+                                              true);
+                    }
                 }
             }
 
@@ -1392,7 +1542,7 @@
             // Add contribution of hidden reconstruction cost in hidden_gradient
             Vec hidden_reconstruction_activation_grad;
             hidden_reconstruction_activation_grad.resize(hidden_layer->size);
-            Mat reconstruction_weights = getDynamicReconstructionConnectionsWeightMatrix();
+            //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
             if(hidden_reconstruction_weight!=0)
             {
                 //Vec hidden_reconstruction_activation_grad;
@@ -1400,7 +1550,7 @@
 
                 //truc stan
                 //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
                 
             }
             
@@ -1412,15 +1562,25 @@
                 multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
                 
             }
-            hidden_layer->bpropUpdate(
-                hidden_act_no_bias_list(i), hidden_list(i),
-                hidden_temporal_gradient, hidden_gradient);
-                
-            dynamic_connections->bpropUpdate(
-                hidden_list(i-1),
-                hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                hidden_gradient, hidden_temporal_gradient);
 
+            bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                   hidden_list(i),
+                                   hidden_temporal_gradient, 
+                                   hidden_gradient,
+                                   hidden_layer->bias, 
+                                   hidden_layer->learning_rate );
+            //Dynamic
+            bpropUpdateConnection(hidden_list(i-1),
+                                  hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                                  hidden_gradient, 
+                                  hidden_temporal_gradient, 
+                                  dynamicWeights,
+                                  acc_dynamic_connections_gr,
+                                  dynamic_connections->down_size,
+                                  dynamic_connections->up_size,
+                                  dynamic_connections->learning_rate,
+                                  false);
+
             /*if(hidden_reconstruction_weight!=0)
             {
                 // update bias
@@ -1430,26 +1590,50 @@
                 
                 }*/
 
-            input_connections->bpropUpdate(
-                input_list[i],
-                hidden_act_no_bias_list(i), 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+            //input
+            bpropUpdateConnection(input_list[i],
+                                  hidden_act_no_bias_list(i), 
+                                  visi_bias_gradient, 
+                                  hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                  inputWeights,
+                                  acc_input_connections_gr,
+                                  input_connections->down_size,
+                                  input_connections->up_size,
+                                  input_connections->learning_rate,
+                                  false);
                 
             hidden_temporal_gradient << hidden_gradient;                
         }
         else
         {
-            hidden_layer->bpropUpdate(
-                hidden_act_no_bias_list(i), hidden_list(i),
-                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-            input_connections->bpropUpdate(
-                input_list[i],
-                hidden_act_no_bias_list(i), 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-
+            bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                   hidden_list(i),
+                                   hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
+                                   hidden_gradient,
+                                   hidden_layer->bias, 
+                                   hidden_layer->learning_rate );
+            
+            //input
+            bpropUpdateConnection(input_list[i],
+                                  hidden_act_no_bias_list(i), 
+                                  visi_bias_gradient, 
+                                  hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                  inputWeights,
+                                  acc_input_connections_gr,
+                                  input_connections->down_size,
+                                  input_connections->up_size,
+                                  input_connections->learning_rate,
+                                  false);
         }
     }
-    
+    //update matrice's connections
+    for( int tar=0; tar<target_layers.length(); tar++)
+    {
+        multiplyAcc(targetWeights[tar], acc_target_connections_gr[tar], 1);
+    }
+    multiplyAcc(inputWeights, acc_input_connections_gr, 1);
+    if(dynamic_connections )
+        multiplyAcc(dynamicWeights, acc_dynamic_connections_gr, 1);
 }
 
 

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-01-28 21:55:40 UTC (rev 9882)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-01-30 03:30:15 UTC (rev 9883)
@@ -144,7 +144,6 @@
     // Phase recurrent no noise (supervised fine tuning)
     double recurrent_lr;
 
-
     // When training with trainUnconditionalPredictor, this is simply used to store the avg encoded frame
     Vec mean_encoded_vec;
 
@@ -315,6 +314,21 @@
     //! Store external data;
     AutoVMatrix*  data;
    
+    mutable TVec< Mat > acc_target_connections_gr;
+
+    mutable Mat acc_input_connections_gr;
+
+    mutable Mat acc_dynamic_connections_gr;
+
+    //! Stores accumulate target bias gradient
+    mutable Vec acc_target_bias_gr;
+
+    //! Stores accumulate hidden bias gradient
+    mutable Vec acc_hidden_bias_gr;
+
+    //! Stores accumulate reconstruction bias gradient
+    mutable Vec acc_recons_bias_gr;
+
     //! Stores bias gradient
     mutable Vec bias_gradient;
     
@@ -407,12 +421,36 @@
     void trainUnconditionalPredictor();
     void unconditionalFprop(Vec train_costs, Vec train_n_items) const;
 
+    Mat getTargetConnectionsWeightMatrix(int tar);
+
     Mat getInputConnectionsWeightMatrix();
 
     Mat getDynamicConnectionsWeightMatrix();
 
     Mat getDynamicReconstructionConnectionsWeightMatrix();
 
+    void updateTargetLayer( Vec& grad, 
+                            Vec& bias , 
+                            real& lr );
+    
+    void bpropUpdateConnection(const Vec& input, 
+                               const Vec& output,
+                               Vec& input_gradient,
+                               const Vec& output_gradient,
+                               Mat& weights,
+                               Mat& acc_weights_gr,
+                               int& down_size,
+                               int& up_size,
+                               real& lr,
+                               bool accumulate);
+
+    void bpropUpdateHiddenLayer(const Vec& input, 
+                                const Vec& output,
+                                Vec& input_gradient,
+                                const Vec& output_gradient,
+                                Vec& bias,
+                                real& lr);
+
     //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
     //! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
@@ -432,7 +470,7 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                           Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
     
     double fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 



