From islaja at mail.berlios.de  Thu Jan 21 23:30:07 2010
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Thu, 21 Jan 2010 23:30:07 +0100
Subject: [Plearn-commits] r10314 - trunk/plearn_learners/online
Message-ID: <201001212230.o0LMU7mh002289@sheep.berlios.de>

Author: islaja
Date: 2010-01-21 23:30:06 +0100 (Thu, 21 Jan 2010)
New Revision: 10314

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
-New option for pepper salt noise:
    -'pep_salt_zero_centered' indicates the pepper and salt value : if==0 then pepper value=0 and salt value=1. if==x pepper then pepper value=-x and salt value=x.
   
-Train basic auto-encoders (AE) with noisy example:
    -Use 'noisy'. = 0 : no noisy example
                  = 1 : noisy example for unsup. pre-training
                  = 2 : noisy example for unsup. pre-training and supervised fine-tuning.

-Option to save the learner just after the pre-train
     -Use 'learnerExpdir' and 'save_learner_before_fine_tuning'.

-Option keep_online_representations",
    -Use 'keep_online_representations' to keep trace of the representations obtained during the pre-train.

- Possibility to use separated encoding and decoding layers. Until now, decoding layers were the same as the encoding layers, so that when the bias of a decoding layer was updated, the bias of the associated encoding layer was also updated.
        -Use 'reconstruction_layers'

-Added the possibility to use the 'computeOutputs' function even during the pre-training to enhance efficiency. 

-Modifications about fantasizeKTime.
    -New parameter: 'alwaysFromSrcImg', indicates if each encodage-decodage phase (fantasize phase) is always from the source image or if we use the output of the preceding fantasize phase.
    -New function 'fantasizeKTimeOnMultiSrcImg', will call fantasizeKTime for each images found in the matrix 'srcImg' received as a parameter.

-New pre-training variante: Stack Renoising Auto-Encoder (SRAE)
     -Use 'renoising'. Same as Stacked Denoising Auto-Encoder (SDAE) except that the RAE compare its output vector with a _second corrupted version of the input_.

-Addition to a pre-training variante: deMissing stacked auto-encoder (SMAE)
    - Added a second way to include missing data state: 'one_if_missing', a binary vector is used to specify if (1) the associated input is missing or (0) not.




Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-12-10 20:02:50 UTC (rev 10313)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2010-01-21 22:30:06 UTC (rev 10314)
@@ -42,6 +42,7 @@
 #include "StackedAutoassociatorsNet.h"
 #include <plearn/io/pl_log.h>
 #include <plearn/sys/Profiler.h>
+#include <plearn/io/load_and_save.h>
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
 
@@ -70,11 +71,14 @@
     missing_data_method( "binomial_complementary"),
     corrupted_data_weight( 1 ),
     data_weight( 1 ),
-    fraction_of_masked_inputs( 0 ),
-    probability_of_masked_inputs( 0 ),
-    probability_of_masked_target( 0 ),
+    fraction_of_masked_inputs( 0. ),
+    probability_of_masked_inputs( 0. ),
+    probability_of_masked_target( 0. ),
     mask_with_mean( false ),
     mask_with_pepper_salt( false ),
+    pep_salt_zero_centered( 0. ),
+    renoising( false ),
+    noisy( 0 ),
     prob_salt_noise( 0.5 ),
     gaussian_std( 1. ),
     binary_sampling_noise_parameter( 1. ),
@@ -85,6 +89,9 @@
     mask_input_layer_only( false ),
     mask_input_layer_only_in_unsupervised_fine_tuning( false ),
     train_stats_window( -1 ),
+    learnerExpdir(""),
+    save_learner_before_fine_tuning( false ),
+    keep_online_representations( false ),
     n_layers( 0 ),
     unsupervised_stage( 0 ),
     minibatch_size( 0 ),
@@ -155,6 +162,12 @@
                   "subsequent elements should be the hidden layers. The\n"
                   "output layer should not be included in layers.\n");
 
+    declareOption(ol, "reconstruction_layers", &StackedAutoassociatorsNet::reconstruction_layers,
+                  OptionBase::buildoption,
+                  "The reconstruction layers in the network (if different than encodage layers.\n"
+                  "The first element of this vector should be the layer for the input layer reconstruction and the\n"
+                  "subsequent elements should be the layer for the reconstruction of hidden layers.\n");
+
     declareOption(ol, "connections", &StackedAutoassociatorsNet::connections,
                   OptionBase::buildoption,
                   "The weights of the connections between the layers");
@@ -263,7 +276,7 @@
                   "Method used to fill the double_input vector for missing_data noise type."
                   "Choose among:\n"
                   " - \"binomial_complementary\"\n"
-                  " - \"none\"\n"
+                  " - \"one_if_missing\""
         );
 
     declareOption(ol, "corrupted_data_weight",
@@ -315,6 +328,38 @@
                   "0 or 1 according to prob_salt_noise.\n"
         );
 
+    declareOption(ol, "pep_salt_zero_centered",
+                  &StackedAutoassociatorsNet::pep_salt_zero_centered,
+                  OptionBase::buildoption,
+                  " Indicate if the mask is zero centered (>0) or not (==0). "
+                  " If equal 0 (not centered)"
+                  " then pepVal is 0 and saltVal is 1."
+                  " If is greater than 0 (centered),"
+                  " then pepVal is -pep_salt_zero_centered and "
+                  " saltVal is pep_salt_zero_centered.\n"
+        );
+
+    declareOption(ol, "renoising",
+                  &StackedAutoassociatorsNet::renoising,
+                  OptionBase::buildoption,
+                  "Indication that the autoassociator will try to"
+                  "'reconstruct' _another_ corrupted version of the input"
+                  "(instead of the input itself),"
+                  "from an initial encoded corrupted version of the input.\n"
+        );
+
+    declareOption(ol, "noisy",
+                  &StackedAutoassociatorsNet::noisy, 
+                  OptionBase::buildoption,
+                  "Indication that example are corrupted before using them for a particular training."
+                  "Note that the original example are used for any test."
+                  "Choose among:\n"
+                  "0 : no example noisy\n"
+                  "1 : noisy applied before unsup. pre-training (basic autoassociator will be used (no denoising).\n"
+                  "2 : noisy applied before unsup. pre-training and before supervised fine-tuning.\n" 
+        );
+
+
     declareOption(ol, "prob_salt_noise",
                   &StackedAutoassociatorsNet::prob_salt_noise,
                   OptionBase::buildoption,
@@ -376,6 +421,27 @@
                   "The number of samples to use to compute training stats.\n"
                   "-1 (default) means the number of training samples.\n");
 
+
+    declareOption(ol, "learnerExpdir",
+                  &StackedAutoassociatorsNet::learnerExpdir,
+                  OptionBase::buildoption,
+                  "Experiment directory where the learner will be save\n"
+                  "if save_learner_before_fine_tuning is true."
+        );
+
+    declareOption(ol, "save_learner_before_fine_tuning",
+                  &StackedAutoassociatorsNet::save_learner_before_fine_tuning,
+                  OptionBase::buildoption,
+                  "Saves the learner before the supervised fine-tuning."
+        );
+
+    declareOption(ol, "keep_online_representations",
+                  &StackedAutoassociatorsNet::keep_online_representations,
+                  OptionBase::buildoption,
+                  "Keep trace of the representations obtained during an "
+                  "unsupervised training phase.\n"
+        );
+    
     declareOption(ol, "greedy_stages",
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -419,8 +485,10 @@
     declareMethod(
         rmm, "fantasizeKTime",
         &StackedAutoassociatorsNet::fantasizeKTime,
-        (BodyDoc("On a trained learner, computes a codage-decodage (fantasize) through a specified number of hidden layer."),
-         ArgDoc ("kTime", "Number of time we want to fantasize (next source image will be the last fantasize Image, and so on for kTime.)"),
+        (BodyDoc("On a trained learner, computes a codage-decodage phase (fantasize phase) through a specified number of hidden layer. From one specified source image."),
+         ArgDoc ("kTime", "Number of time we want to fantasize. \n" 
+                 "Next input image will again be the source Image (if alwaysFromSrcImg is True) \n"
+                 "or next input image will be the last fantasize image (if alwaysFromSrcImg is False), and so on for kTime.)"),
          ArgDoc ("srcImg", "Source image vector (should have same width as raws layer)"),
          ArgDoc ("sampling", "Vector of bool indicating whether or not a sampling will be done for each hidden layer\n"
                 "during decodage. Its width indicates how many hidden layer will be used.)\n"
@@ -430,7 +498,43 @@
                 "(according to the one used during the learning stage)\n"
                 "for each layer. (should have same width as sampling or be empty if unuseful.\n"
                 "Smaller element of the vector correspond to lower layer"),
-         RetDoc ("Corresponding fantasize image (will have same width as srcImg)")));
+         ArgDoc ("alwaysFromSrcImg", "Booleen indicating whether each encode-decode \n"
+                "steps are done from the source image (sets to True) or \n"
+                "if the next input image is the preceding fantasize image obtained (sets to False). "),
+         RetDoc ("Fantasize images obtained for each kTime.")));
+    
+    declareMethod(
+        rmm, "fantasizeKTimeOnMultiSrcImg",
+        &StackedAutoassociatorsNet::fantasizeKTimeOnMultiSrcImg,
+        (BodyDoc("Call the 'fantasizeKTime' function for each source images found in the matrix 'srcImg'."),
+         ArgDoc ("kTime", "Number of time we want to fantasize for each source images. \n"
+                 "Next input image will again be the source Image (if alwaysFromSrcImg is True) \n"
+                 "or next input image will be the last fantasize image (if alwaysFromSrcImg is False), and so on for kTime.)"),
+         ArgDoc ("srcImg", "Source images matrix (should have same width as raws layer)"),
+         ArgDoc ("sampling", "Vector of bool indicating whether or not a sampling will be done for each hidden layer\n"
+                "during decodage. Its width indicates how many hidden layer will be used.)\n"
+                " (should have same width as maskNoiseFractOrProb)\n"
+                "smaller element of the vector correspond to lower layer"),
+         ArgDoc ("maskNoiseFractOrProb", "Vector of noise fraction or probability\n"
+                "(according to the one used during the learning stage)\n"
+                "for each layer. (should have same width as sampling or be empty if unuseful.\n"
+                "Smaller element of the vector correspond to lower layer"),
+         ArgDoc ("alwaysFromSrcImg", "Booleen indicating whether each encode-decode \n"
+                "steps are done from the source image (sets to True) or \n"
+                "if the next input image is the preceding fantasize image obtained (sets to False). "),
+         RetDoc ("For each source images, fantasize images obtained for each kTime.")));
+    
+    declareMethod(
+        rmm, "getTrainRepresentations", &StackedAutoassociatorsNet::getTrainRepresentations,
+        (BodyDoc("Returns the representations obtained during last pre-training of the current layer.\n"),
+         RetDoc ("Current train representations")));
+
+    declareMethod(
+        rmm, "remote_setCurrentlyTrainedLayer", &StackedAutoassociatorsNet::remote_setCurrentlyTrainedLayer,
+        (BodyDoc("Modify current_trained_layer.\n"),
+        ArgDoc ("input", "Matrix of inputs."),
+        RetDoc ("Outputs from each hidden layers.")));
+   
 }
 
 void StackedAutoassociatorsNet::build_()
@@ -517,6 +621,21 @@
                     "corrupted inputs only works with masking noise in online setting,"
                     "in the non-minibatch case.\n");
 
+        if( renoising && noisy > 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "cannot use renoising and noisy at the same time.\n");
+
+        if( renoising && noise_type == "missing_data" )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "cannot use renoising with missing data.\n");
+
+        if( noisy > 0 && noise_type == "missing_data")
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "cannot use noisy with missing data.\n");
+
         if( !online )
         {
             if( greedy_stages.length() == 0)
@@ -576,23 +695,23 @@
                 "compute_all_test_costs option is not implemented for\n"
                 "reconstruct_hidden option.");
 
-    if( noise_type == "missing_data")
+    if( noise_type == "missing_data" || renoising || noisy > 0 )
     {
         if( correlation_connections.length() !=0 )
             PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
-                    "Missing data is not implemented with correlation_connections.\n");
+                    "Missing data, renoising and noisy are not implemented with correlation_connections.\n");
     
         if( direct_connections.length() !=0 )
             PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
-                    "Missing data is not implemented with direct_connections.\n");
+                    "Missing data, renoising and noisy are not implemented with direct_connections.\n");
         
         if( reconstruct_hidden )
             PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
-                    "Missing data is not implemented with reconstruct_hidden.\n");
+                    "Missing data, renoising and noisy are not implemented with reconstruct_hidden.\n");
 
         if( online ) 
             PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
-                    "Missing data is not implemented in the online setting.\n");
+                    "Missing data, renoising and noisy are not implemented in the online setting.\n");
     }
 
     if(correlation_connections.length() != 0)
@@ -635,9 +754,26 @@
     expectation_gradients.resize( n_layers );
     expectation_gradients_m.resize( n_layers );
 
+    // If not defined, reconstruction_layers will  
+    // simply point to the layers vector. 
+    if( reconstruction_layers.length() == 0 )
+        reconstruction_layers = layers;
+    else
+        if( reconstruction_layers.length() != layers.length()-1 && 
+            reconstruction_layers.length() != layers.length() )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                     "- \n"
+                    "reconstruction_layers should have a length of layers.length-1 or layers.length, i.e: %d\n.",
+                     layers.length()-1);
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
+        if( layers[i]->size != reconstruction_layers[i]->size )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                     "- \n"
+                    "layers[%i] should have same size of reconstruction_layers[%i], i.e: %d.\n",
+                    i, i, layers[i]->size);
+
         if( noise_type == "missing_data")
         {
             if( layers[i]->size * 2 != connections[i]->down_size )
@@ -765,6 +901,12 @@
             layers[i]->random_gen = random_gen;
             layers[i]->forget();
         }
+        
+        if( !(reconstruction_layers[i]->random_gen) )
+        {
+            reconstruction_layers[i]->random_gen = random_gen;
+            reconstruction_layers[i]->forget();
+        }
 
         if( !(connections[i]->random_gen) )
         {
@@ -794,6 +936,8 @@
     expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
 
     reconstruction_weights.resize( layers[0]->size );
+    // Will be correctly resized if keep_online_representations == True
+    train_representations.resize( 1 );
 
     // For denoising autoencoders
     doubled_expectations.resize( n_layers-1 );
@@ -803,6 +947,9 @@
     
     if( (noise_type == "masking_noise" || noise_type == "missing_data") && fraction_of_masked_inputs > 0 )
         autoassociator_expectation_indices.resize( n_layers-1 );
+    
+    if( renoising || noisy > 0 )
+       second_corrupted_autoassociator_expectations.resize( n_layers-1 );
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
@@ -826,6 +973,9 @@
             for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
                 autoassociator_expectation_indices[i][j] = j;
         }
+
+        if( renoising || noisy > 0 )
+            second_corrupted_autoassociator_expectations[i].resize( layers[i]->size );
     }
 
     if(greedy_target_connections.length() != 0)
@@ -935,6 +1085,7 @@
     // Public options
     deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
+    deepCopyField(reconstruction_layers, copies);
     deepCopyField(connections, copies);
     deepCopyField(reconstruction_connections, copies);
     deepCopyField(correlation_connections, copies);
@@ -994,6 +1145,7 @@
     deepCopyField(final_cost_gradient, copies);
     deepCopyField(final_cost_gradients, copies);
     deepCopyField(corrupted_autoassociator_expectations, copies);
+    deepCopyField(second_corrupted_autoassociator_expectations, copies);
     deepCopyField(reconstruction_weights, copies);
     deepCopyField(binary_masks, copies);
     deepCopyField(tmp_mask, copies);
@@ -1033,6 +1185,7 @@
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
+        reconstruction_layers[i]->forget();
         connections[i]->forget();
         reconstruction_connections[i]->forget();
     }
@@ -1174,6 +1327,7 @@
             train_costs.fill(MISSING_VALUE);
             lr = greedy_learning_rate;
             layers[i]->setLearningRate( lr );
+            reconstruction_layers[i]->setLearningRate( lr );
             connections[i]->setLearningRate( lr );
             reconstruction_connections[i]->setLearningRate( lr );
             if(correlation_connections.length() != 0)
@@ -1218,6 +1372,14 @@
                 direct_and_reconstruction_activations.resize(layers[i]->size);
                 direct_and_reconstruction_activation_gradients.resize(layers[i]->size);
             }
+
+            if( keep_online_representations )
+            {
+                train_representations.resize(end_stage-(*this_stage));
+                train_representations.clear();
+            }
+            int greedyBatchSize = end_stage - (*this_stage);
+            string old_noise_type = noise_type;
             for( ; *this_stage<end_stage ; (*this_stage)++ )
             {
                 if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
@@ -1225,6 +1387,7 @@
                     lr = greedy_learning_rate/(1 + greedy_decrease_ct
                                                * (*this_stage));
                     layers[i]->setLearningRate( lr );
+                    reconstruction_layers[i]->setLearningRate( lr );
                     connections[i]->setLearningRate( lr );
                     reconstruction_connections[i]->setLearningRate( lr );
                     layers[i+1]->setLearningRate( lr );
@@ -1242,9 +1405,24 @@
                     if( greedy_target_connections.length() && greedy_target_connections[i] )
                         greedy_target_connections[i]->setLearningRate( lr );
                 }
+                int train_representations_i = 0;
                 sample = *this_stage % nsamples;
                 train_set->getExample(sample, input, target, weight);
-                greedyStep( input, target, i, train_costs );
+                if( keep_online_representations )
+                {
+                    train_representations_i = greedyBatchSize - (end_stage-(*this_stage));
+                    train_representations[train_representations_i].resize(layers[i+1]->size);
+                }
+                if( noisy >= 1 )
+                {
+                    corrupt_input( input, second_corrupted_autoassociator_expectations[0], 0 );
+                    noise_type = "none";
+                    greedyStep( second_corrupted_autoassociator_expectations[0], target, i, train_costs, train_representations[train_representations_i]);
+                    noise_type = old_noise_type;
+                }
+                else
+                    greedyStep( input, target, i, train_costs, train_representations[train_representations_i]);
+                
                 train_stats->update( train_costs );
 
                 if( pb )
@@ -1295,6 +1473,7 @@
 
             setLearningRate( unsupervised_fine_tuning_learning_rate );
             train_costs.fill(MISSING_VALUE);
+            string old_noise_type = noise_type;
             for( ; unsupervised_stage<unsupervised_nstages ; unsupervised_stage++ )
             {
                 sample = unsupervised_stage % nsamples;
@@ -1305,7 +1484,15 @@
                            * unsupervised_stage ) );
 
                 train_set->getExample( sample, input, target, weight );
-                unsupervisedFineTuningStep( input, target, train_costs );
+                if( noisy >= 1)
+                {
+                    corrupt_input( input, second_corrupted_autoassociator_expectations[0], 0 );
+                    noise_type = "none";
+                    unsupervisedFineTuningStep(second_corrupted_autoassociator_expectations[0], target, train_costs );
+                    noise_type = old_noise_type;
+                }
+                else
+                    unsupervisedFineTuningStep( input, target, train_costs );
                 train_stats->update( train_costs );
 
                 if( pb )
@@ -1314,6 +1501,16 @@
             Profiler::pl_profile_end("StackedAutoassociatorsNet::train unsupervised");
         }
 
+        if( save_learner_before_fine_tuning )
+        {
+            if( learnerExpdir == "" )
+                PLWARNING("StackedAutoassociatorsNet::train() - \n"
+                    "cannot save model before fine-tuning because\n"
+                    "no experiment directory has been set.");
+            else
+                PLearn::save(learnerExpdir + "/learner_before_finetuning.psave",*this);
+        }
+
         /***** fine-tuning by gradient descent *****/
         if( stage < nstages )
         {
@@ -1341,7 +1538,13 @@
                                      / (1. + fine_tuning_decrease_ct * stage ) );
 
                 train_set->getExample( sample, input, target, weight );
-                fineTuningStep( input, target, train_costs );
+                if( noisy >= 2)
+                {
+                    corrupt_input( input, second_corrupted_autoassociator_expectations[0], 0 );
+                    fineTuningStep( second_corrupted_autoassociator_expectations[0], target, train_costs );
+                }
+                else
+                    fineTuningStep( input, target, train_costs );
                 train_stats->update( train_costs );
 
                 if( pb )
@@ -1468,10 +1671,19 @@
                 PLERROR("In StackedAutoassociatorsNet::corrupt_input():" 
                         " fraction_of_masked_inputs and probability_of_masked_inputs can't be both > 0");
             if( mask_with_pepper_salt )
+            {
+                real pepVal = 0;
+                real saltVal = 1;
+                if( pep_salt_zero_centered>0. )
+                {
+                    pepVal = -pep_salt_zero_centered;
+                    saltVal = pep_salt_zero_centered;
+                }
                 for( int j=0 ; j <input.length() ; j++)
+                {
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
-                        corrupted_input[ j ] = random_gen->binomial_sample(prob_salt_noise);
+                        corrupted_input[ j ] = random_gen->bounded_sample(prob_salt_noise,pepVal,saltVal);
                         reconstruction_weights[j] = corrupted_data_weight;
                     }
                     else
@@ -1479,8 +1691,12 @@
                         corrupted_input[ j ] = input[ j ];  
                         reconstruction_weights[j] = data_weight;
                     }
+                }
+            }       
             else if( mask_with_mean )
+            {
                 for( int j=0 ; j <input.length() ; j++)
+                {
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {                    
                         corrupted_input[ j ] = expectation_means[layer][ j ];
@@ -1492,8 +1708,12 @@
                         corrupted_input[ j ] = input[ j ];
                         reconstruction_weights[j] = data_weight;
                     }
+                }
+            }
             else
+            {
                 for( int j=0 ; j <input.length() ; j++)
+                {
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
                         corrupted_input[ j ] = 0;   
@@ -1505,56 +1725,73 @@
                         corrupted_input[ j ] = input[ j ];
                         reconstruction_weights[j] = data_weight;
                     }
+                }
+            }
         }
         else
         {
             corrupted_input << input;
             reconstruction_weights.fill(data_weight);
-            if( fraction_of_masked_inputs != 0 ) 
+            if( fraction_of_masked_inputs > 0. ) 
             {
                 random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
                 if( mask_with_pepper_salt )
+                {
+                    real pepVal = 0;
+                    real saltVal = 1;
+                    if( pep_salt_zero_centered>0. )
+                    {
+                        pepVal = -pep_salt_zero_centered;
+                        saltVal = pep_salt_zero_centered;
+                    }
                     for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
-                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = random_gen->binomial_sample(prob_salt_noise);
+                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = random_gen->bounded_sample(prob_salt_noise,pepVal,saltVal);
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                     }
+                }   
                 else if( mask_with_mean )
+                {
                     for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
                         corrupted_input[ autoassociator_expectation_indices[layer][j] ] = expectation_means[layer][autoassociator_expectation_indices[layer][j]];
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                         binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
                     }
+                }
                 else
+                {
                     for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
                     {
                         corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                         binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
                     }
+                }
             }
         }
     }
     else if( noise_type == "binary_sampling" )
-    {
         for( int i=0; i<corrupted_input.length(); i++ )
             corrupted_input[i] = random_gen->binomial_sample((input[i]-0.5)*binary_sampling_noise_parameter+0.5);
-    }
     else if( noise_type == "gaussian" )
-    {
         for( int i=0; i<corrupted_input.length(); i++ )
-            corrupted_input[i] = input[i] +
-                random_gen->gaussian_01() * gaussian_std;
-    }
+            corrupted_input[i] = input[i] + random_gen->gaussian_01() * gaussian_std;
     else if( noise_type == "missing_data")
     {
         // The entry input is the doubled one according to missing_data_method
         int original_input_length = input.length() / 2;
         reconstruction_weights.resize(original_input_length);
    
-        if(missing_data_method == "binomial_complementary")
+        if(missing_data_method == "binomial_complementary" || 
+           missing_data_method == "one_if_missing")
         {
+            int down_missing_value = 0;
+            int up_missing_value = 0;
+        
+            if(missing_data_method == "one_if_missing")
+                up_missing_value = 1;
+
             if( probability_of_masked_inputs > 0 )
             {
                 if( fraction_of_masked_inputs > 0 )
@@ -1563,8 +1800,8 @@
                 for( int j=0 ; j<original_input_length ; j++ )
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
-                        corrupted_input[ j*2 ] = 0;
-                        corrupted_input[ j*2+1 ] = 0;
+                        corrupted_input[ j*2 ] = down_missing_value;
+                        corrupted_input[ j*2+1 ] = up_missing_value;
                         reconstruction_weights[j] = corrupted_data_weight;
                     }
                     else
@@ -1578,13 +1815,13 @@
             {
                 corrupted_input << input;
                 reconstruction_weights.fill(data_weight);
-                if( fraction_of_masked_inputs != 0 )
+                if( fraction_of_masked_inputs > 0. )
                 {
                     random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
                     for( int j=0 ; j < round(fraction_of_masked_inputs*original_input_length) ; j++)
                     {
-                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 ] = 0;
-                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 + 1 ] = 0;
+                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 ] = down_missing_value;
+                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 + 1 ] = up_missing_value;
                         reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
                     }
                 }
@@ -1602,34 +1839,37 @@
 }
 
 
+// ***** binomial_complementary ******
+// doubled_input[2*i] = input[i] and
+// doubled input[2*i+1] = 1-input[i]
+// If input is gradient that we have to double for backpropagation
+// (double_grad==true), then:
+// doubled_input[2*i] = input[i] and
+// oubled input[2*i+1] = -input[i]
+// ********** one_if_missing *********
+// doubled_input[2*i] = input[i] and
+// doubled input[2*i+1] = 0 (gradian or not)
 void StackedAutoassociatorsNet::double_input(const Vec& input, Vec& doubled_input, bool double_grad) const
 {
     if( noise_type == "missing_data" )
     {
-        if( missing_data_method == "binomial_complementary" )
+        doubled_input.resize(input.length()*2);
+        for( int i=0; i<input.size(); i++ )
         {
-            // doubled_input[2*i] = input[i] and
-            // doubled input[2*i+1] = 1-input[i]
-            // If input is gradient that we have to double for backpropagation,
-            // (double_grad==true), then:
-            // doubled_input[2*i] = input[i] and
-            // doubled input[2*i+1] = -input[i] 
-            doubled_input.resize(input.length()*2);
-            for( int i=0; i<input.size(); i++ )
+            doubled_input[i*2] = input[i];
+            if( missing_data_method == "binomial_complementary")
             {
-                doubled_input[i*2] = input[i];
-                // double gradient before backpropagate 
-                // during the pre-training
                 if( double_grad )
                     doubled_input[i*2+1] = - input[i];
-                // double input of a layer
                 else
                     doubled_input[i*2+1] = 1 - input[i];
             }
+            else if( missing_data_method == "one_if_missing" )
+                doubled_input[i*2+1] = 0;
+            else
+                PLERROR("In StackedAutoassociatorsNet::double_input(): "
+                "missing_data_method %s not valid",missing_data_method.c_str());
         }
-        else
-            PLERROR("In StackedAutoassociatorsNet::double_input(): "
-                    "missing_data_method %s not valid",missing_data_method.c_str());
     }
     else
     {
@@ -1638,21 +1878,26 @@
     }
 }
 
+// ***** binomial_complementary *****
+// divided_input[i] = input[2*i] - input[2*i+1]
+// even if input is the doubled_gradient
+// ********** one_if_missing *********
+// divided_input[i] = input[2*i]
 void StackedAutoassociatorsNet::divide_input(const Vec& input, Vec& divided_input) const
 {
     if( noise_type == "missing_data" )
     {
-        if( missing_data_method == "binomial_complementary" )
+        divided_input.resize(input.length()/2);
+        for( int i=0; i<divided_input.size(); i++ )  
         {
-            // divided_input[i] = input[2*i] - input[2*i+1]
-            // even if input is the doubled_gradient
-            divided_input.resize(input.length()/2);
-            for( int i=0; i<divided_input.size(); i++)  
+            if( missing_data_method == "binomial_complementary" )
                 divided_input[i] = input[i*2] - input[i*2+1];
+            else if( missing_data_method == "one_if_missing" )
+                divided_input[i] = input[i*2];
+            else
+                PLERROR("In StackedAutoassociatorsNet::divide_input(): "
+                        "missing_data_method %s not valid", missing_data_method.c_str());
         }
-        else
-            PLERROR("In StackedAutoassociatorsNet::divide_input(): "
-                    "missing_data_method %s not valid", missing_data_method.c_str());
     }
     else
     {
@@ -1663,12 +1908,13 @@
 
 
 void StackedAutoassociatorsNet::greedyStep(const Vec& input, const Vec& target,
-                                           int index, Vec train_costs)
+                                           int index, Vec train_costs, Vec& representation)
 {
     Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep");
     PLASSERT( index < n_layers );
 
     expectations[0] << input;
+
     if(correlation_connections.length() != 0)
     {
         for( int i=0 ; i<index + 1; i++ )
@@ -1685,7 +1931,7 @@
             if( i == index && greedy_target_connections.length() && greedy_target_connections[i] )
             {
                 target_vec.clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen->uniform_sample() >= probability_of_masked_target )
                     target_vec[(int)target[0]] = 1;
 
@@ -1720,7 +1966,7 @@
             if( i == index && greedy_target_connections.length() && greedy_target_connections[i] )
             {
                 target_vec.clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen->uniform_sample() >= probability_of_masked_target )
                     target_vec[(int)target[0]] = 1;
 
@@ -1730,6 +1976,8 @@
             }
 
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+            if( keep_online_representations )
+                representation << expectations[i+1];
         }
     }
 
@@ -1775,19 +2023,19 @@
         direct_and_reconstruction_activations += direct_activations;
         direct_and_reconstruction_activations += reconstruction_activations;
 
-        layers[ index ]->fprop( direct_and_reconstruction_activations,
-                                layers[ index ]->expectation);
+        reconstruction_layers[ index ]->fprop( direct_and_reconstruction_activations,
+                                reconstruction_layers[ index ]->expectation);
 
-        layers[ index ]->activation << direct_and_reconstruction_activations;
-        layers[ index ]->activation += layers[ index ]->bias;
-        //layers[ index ]->expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
-        layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
-        train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
+        reconstruction_layers[ index ]->activation << direct_and_reconstruction_activations;
+        reconstruction_layers[ index ]->activation += reconstruction_layers[ index ]->bias;
+        //reconstruction_layers[ index ]->expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
+        reconstruction_layers[ index ]->setExpectationByRef( reconstruction_layers[ index ]->expectation );
+        train_costs[index] = reconstruction_layers[ index ]->fpropNLL(expectations[index]);
 
-        layers[ index ]->bpropNLL(expectations[index], train_costs[index],
+        reconstruction_layers[ index ]->bpropNLL(expectations[index], train_costs[index],
                                   direct_and_reconstruction_activation_gradients);
 
-        layers[ index ]->update(direct_and_reconstruction_activation_gradients);
+        reconstruction_layers[ index ]->update(direct_and_reconstruction_activation_gradients);
 
         direct_connections[ index ]->bpropUpdate(
             corrupted_autoassociator_expectations[index],
@@ -1804,25 +2052,33 @@
     else
     {
         Vec divided_reconstruction_activations(reconstruction_activations.size());
-        Vec divided_reconstruction_activation_gradients(layers[ index ]->size);
+        Vec divided_reconstruction_activation_gradients(reconstruction_layers[ index ]->size);
         
         divide_input(reconstruction_activations, divided_reconstruction_activations);
 
-        layers[ index ]->fprop( divided_reconstruction_activations,
-                                layers[ index ]->expectation);
-        layers[ index ]->activation << divided_reconstruction_activations;
-        layers[ index ]->activation += layers[ index ]->bias;
-        //layers[ index ]->expectation_is_up_to_date = true;
-        layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
+        reconstruction_layers[ index ]->fprop( divided_reconstruction_activations,
+                               reconstruction_layers[ index ]->expectation);
+        reconstruction_layers[ index ]->activation << divided_reconstruction_activations;
+        reconstruction_layers[ index ]->activation += reconstruction_layers[ index ]->bias;
+        //reconstruction_layers[ index ]->expectation_is_up_to_date = true;
+        reconstruction_layers[ index ]->setExpectationByRef( reconstruction_layers[ index ]->expectation );
         real rec_err;
 
         // If we want to compute reconstruction error according to reconstruction weights.
-        //   rec_err = layers[ index ]->fpropNLL(expectations[index], reconstruction_weights);
-        
-        rec_err = layers[ index ]->fpropNLL(expectations[index]);
-
+        //   rec_err = reconstruction_layers[ index ]->fpropNLL(expectations[index], reconstruction_weights);
+       
+        if( renoising )
+        {
+            corrupt_input( expectations[index], second_corrupted_autoassociator_expectations[index], index );
+            rec_err = reconstruction_layers[ index ]->fpropNLL(second_corrupted_autoassociator_expectations[index]);
+            reconstruction_layers[ index ]->bpropNLL(second_corrupted_autoassociator_expectations[index], rec_err, divided_reconstruction_activation_gradients);
+        }
+        else
+        {
+            rec_err = reconstruction_layers[ index ]->fpropNLL(expectations[index]);
+            reconstruction_layers[ index ]->bpropNLL(expectations[index], rec_err, divided_reconstruction_activation_gradients);
+        }
         train_costs[index] = rec_err;
-        layers[ index ]->bpropNLL(expectations[index], rec_err, divided_reconstruction_activation_gradients);
 
         // apply reconstruction weights which can be different for corrupted
         // (or missing) and non corrupted data. 
@@ -1836,7 +2092,7 @@
         if(reconstruct_hidden)
         {
             Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep reconstruct_hidden");
-            connections[ index ]->fprop( layers[ index ]->expectation,
+            connections[ index ]->fprop( reconstruction_layers[ index ]->expectation,
                                          hidden_reconstruction_activations );
             layers[ index+1 ]->fprop( hidden_reconstruction_activations,
                 layers[ index+1 ]->expectation );
@@ -1853,29 +2109,29 @@
 
             Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep reconstruct_hidden connection bprop");
             connections[ index ]->bpropUpdate(
-                layers[ index ]->expectation,
+                reconstruction_layers[ index ]->expectation,
                 hidden_reconstruction_activations,
                 reconstruction_expectation_gradients_from_hid_rec,
                 hidden_reconstruction_activation_gradients);
             Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep reconstruct_hidden connection bprop");
 
-            layers[ index ]->bpropUpdate(
+            reconstruction_layers[ index ]->bpropUpdate(
                 reconstruction_activations,
-                layers[ index ]->expectation,
+                reconstruction_layers[ index ]->expectation,
                 reconstruction_activation_gradients_from_hid_rec,
                 reconstruction_expectation_gradients_from_hid_rec);
             Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep reconstruct_hidden");
         }
 
-        layers[ index ]->update(divided_reconstruction_activation_gradients);
+        reconstruction_layers[ index ]->update(divided_reconstruction_activation_gradients);
         
         if(reconstruct_hidden)
             reconstruction_activation_gradients +=
                 reconstruction_activation_gradients_from_hid_rec;
 
         // // This is a bad update! Propagates gradient through sigmoid again!
-        // layers[ index ]->bpropUpdate( reconstruction_activations,
-        //                                   layers[ index ]->expectation,
+        // reconstruction_layers[ index ]->bpropUpdate( reconstruction_activations,
+        //                                   reconstruction_layers[ index ]->expectation,
         //                                   reconstruction_activation_gradients,
         //                                   reconstruction_expectation_gradients);
         reconstruction_connections[ index ]->bpropUpdate(
@@ -2250,7 +2506,7 @@
             if( greedy_target_connections.length() && greedy_target_connections[i] )
             {
                 targets_vec[i].clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen->uniform_sample() >= probability_of_masked_target )
                     targets_vec[i][(int)target[0]] = 1;
 
@@ -2280,7 +2536,7 @@
             if( greedy_target_connections.length() && greedy_target_connections[i] )
             {
                 targets_vec[i].clear();
-                if( probability_of_masked_target == 0 ||
+                if( probability_of_masked_target == 0. ||
                     random_gen->uniform_sample() >= probability_of_masked_target )
                     targets_vec[i][(int)target[0]] = 1;
 
@@ -2304,6 +2560,7 @@
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         layers[i]->setLearningRate( lr );
+        reconstruction_layers[i]->setLearningRate( lr );
         connections[i]->setLearningRate( lr );
         reconstruction_connections[i]->setLearningRate( lr );
         if(correlation_layers.length() != 0)
@@ -2327,20 +2584,20 @@
             expectations[ i ],
             reconstruction_activations);
 
-        layers[ i-1 ]->fprop( reconstruction_activations,
-                              layers[ i-1 ]->expectation);
+        reconstruction_layers[ i-1 ]->fprop( reconstruction_activations,
+                              reconstruction_layers[ i-1 ]->expectation);
 
-        layers[ i-1 ]->activation << reconstruction_activations;
-        layers[ i-1 ]->activation += layers[ i-1 ]->bias;
-        //layers[ i-1 ]->expectation_is_up_to_date = true;
-        layers[ i-1 ]->setExpectationByRef( layers[ i-1 ]->expectation );
-        real rec_err = layers[ i-1 ]->fpropNLL( expectations[i-1] );
+        reconstruction_layers[ i-1 ]->activation << reconstruction_activations;
+        reconstruction_layers[ i-1 ]->activation += reconstruction_layers[ i-1 ]->bias;
+        //reconstruction_layers[ i-1 ]->expectation_is_up_to_date = true;
+        reconstruction_layers[ i-1 ]->setExpectationByRef( reconstruction_layers[ i-1 ]->expectation );
+        real rec_err = reconstruction_layers[ i-1 ]->fpropNLL( expectations[i-1] );
         train_costs[i-1] = rec_err;
 
-        layers[ i-1 ]->bpropNLL(expectations[i-1], rec_err,
+        reconstruction_layers[ i-1 ]->bpropNLL(expectations[i-1], rec_err,
                                   reconstruction_activation_gradients);
 
-        layers[ i-1 ]->update(reconstruction_activation_gradients);
+        reconstruction_layers[ i-1 ]->update(reconstruction_activation_gradients);
 
         reconstruction_connections[ i-1 ]->bpropUpdate(
             expectations[ i ],
@@ -2651,6 +2908,7 @@
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         layers[i]->setLearningRate( lr );
+        reconstruction_layers[i]->setLearningRate( lr );
         connections[i]->setLearningRate( lr );
         reconstruction_connections[i]->setLearningRate( lr );
         if(correlation_layers.length() != 0)
@@ -2668,22 +2926,22 @@
             expectations_m[ i ],
             reconstruction_activations_m);
 
-        layers[ i-1 ]->activations.resize(mbatch_size, layers[i-1]->size);
-        layers[ i-1 ]->activations << reconstruction_activations_m;
-        layers[ i-1 ]->activations += layers[ i-1 ]->bias;
+        reconstruction_layers[ i-1 ]->activations.resize(mbatch_size,reconstruction_layers[i-1]->size);
+        reconstruction_layers[ i-1 ]->activations << reconstruction_activations_m;
+        reconstruction_layers[ i-1 ]->activations += reconstruction_layers[ i-1 ]->bias;
 
-        Mat layer_exp = layers[i-1]->getExpectations();
-        layers[ i-1 ]->fprop(reconstruction_activations_m,
+        Mat layer_exp = reconstruction_layers[i-1]->getExpectations();
+        reconstruction_layers[ i-1 ]->fprop(reconstruction_activations_m,
                              layer_exp);
-        layers[ i-1 ]->setExpectationsByRef(layer_exp);
+        reconstruction_layers[ i-1 ]->setExpectationsByRef(layer_exp);
 
-        layers[ i-1 ]->fpropNLL(expectations_m[i-1],
+        reconstruction_layers[ i-1 ]->fpropNLL(expectations_m[i-1],
                                 train_costs.column(i-1));
 
-        layers[ i-1 ]->bpropNLL(expectations_m[i-1], train_costs.column(i-1),
+        reconstruction_layers[ i-1 ]->bpropNLL(expectations_m[i-1], train_costs.column(i-1),
                                 reconstruction_activation_gradients_m);
 
-        layers[ i-1 ]->update(reconstruction_activation_gradients_m);
+        reconstruction_layers[ i-1 ]->update(reconstruction_activation_gradients_m);
 
         reconstruction_connections[ i-1 ]->bpropUpdate(
             expectations_m[ i ],
@@ -2892,26 +3150,33 @@
 void StackedAutoassociatorsNet::computeOutputs(const Mat& input, Mat& output) const
 {
     if(correlation_connections.length() != 0
-       || currently_trained_layer!=n_layers
        || compute_all_test_costs
        || noise_type == "missing_data"){
         inherited::computeOutputs(input, output);
     }else{
         Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutputs");
-        PLCHECK(correlation_connections.length() == 0);
-        PLCHECK(currently_trained_layer == n_layers);
-        PLCHECK(!compute_all_test_costs);
 
         expectations_m[0].resize(input.length(), inputsize());
         Mat m = expectations_m[0];
         m<<input;
+
         for(int i=0 ; i<currently_trained_layer-1 ; i++ )
         {
             connections[i]->fprop( expectations_m[i], activations_m[i+1] );
             layers[i+1]->fprop(activations_m[i+1],expectations_m[i+1]);
         }
-        final_module->fprop( expectations_m[ currently_trained_layer - 1],
+        if(currently_trained_layer < n_layers)
+        {
+            connections[currently_trained_layer-1]->fprop( expectations_m[currently_trained_layer-1],
+                 activations_m[currently_trained_layer] );
+            layers[currently_trained_layer]->fprop(activations_m[currently_trained_layer],
+                 output);
+        }
+        else
+        {
+            final_module->fprop( expectations_m[ currently_trained_layer - 1],
                              output );
+        }
         Profiler::pl_profile_end("StackedAutoassociatorsNet::computeOutputs");
     }
 }
@@ -2920,15 +3185,11 @@
                                                        Mat& output, Mat& costs) const
 {
     if(correlation_connections.length() != 0 
-       || currently_trained_layer!=n_layers
        || compute_all_test_costs
        || noise_type == "missing_data"){
         inherited::computeOutputsAndCosts(input, target, output, costs);
     }else{
         Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutputsAndCosts");
-        PLCHECK(correlation_connections.length() == 0);
-        PLCHECK(currently_trained_layer == n_layers);
-        PLCHECK(!compute_all_test_costs);
 
         int n=input.length();
         PLASSERT(target.length()==n);
@@ -2970,15 +3231,15 @@
                 reconstruction_activations += direct_activations;
             }
 
-            layers[ i ]->fprop( reconstruction_activations,
-                                layers[ i ]->expectation);
+            reconstruction_layers[ i ]->fprop( reconstruction_activations,
+                                reconstruction_layers[ i ]->expectation);
 
-            layers[ i ]->activation << reconstruction_activations;
-            layers[ i ]->activation += layers[ i ]->bias;
-            //layers[ i ]->expectation_is_up_to_date = true;
-            layers[ i ]->setExpectationByRef( layers[ i ]->expectation );
+            reconstruction_layers[ i ]->activation << reconstruction_activations;
+            reconstruction_layers[ i ]->activation += reconstruction_layers[ i ]->bias;
+            //reconstruction_layers[ i ]->expectation_is_up_to_date = true;
+            reconstruction_layers[ i ]->setExpectationByRef( reconstruction_layers[ i ]->expectation );
 
-            costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
+            costs[i] = reconstruction_layers[ i ]->fpropNLL(expectations[ i ]);
 
             if( partial_costs && partial_costs[i])
             {
@@ -3007,27 +3268,26 @@
         Vec divided_reconstruction_activations(reconstruction_activations.size());
         divide_input(reconstruction_activations, divided_reconstruction_activations);
 
-        layers[ currently_trained_layer-1 ]->fprop(
+        reconstruction_layers[ currently_trained_layer-1 ]->fprop(
           divided_reconstruction_activations,
-          layers[ currently_trained_layer-1 ]->expectation);
+          reconstruction_layers[ currently_trained_layer-1 ]->expectation);
         
-        layers[ currently_trained_layer-1 ]->activation <<
+        reconstruction_layers[ currently_trained_layer-1 ]->activation <<
             divided_reconstruction_activations;
+        reconstruction_layers[ currently_trained_layer-1 ]->activation += 
+            reconstruction_layers[ currently_trained_layer-1 ]->bias;
+        //reconstruction_layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        reconstruction_layers[ currently_trained_layer-1 ]->setExpectationByRef(
+            reconstruction_layers[ currently_trained_layer-1 ]->expectation );
 
-        layers[ currently_trained_layer-1 ]->activation += 
-            layers[ currently_trained_layer-1 ]->bias;
-        //layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
-        layers[ currently_trained_layer-1 ]->setExpectationByRef(
-            layers[ currently_trained_layer-1 ]->expectation );
-
         costs[ currently_trained_layer-1 ] =
-            layers[ currently_trained_layer-1 ]->fpropNLL(
+            reconstruction_layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
 
         if(reconstruct_hidden)
         {
             connections[ currently_trained_layer-1 ]->fprop(
-                layers[ currently_trained_layer-1 ]->expectation,
+                reconstruction_layers[ currently_trained_layer-1 ]->expectation,
                 hidden_reconstruction_activations );
             layers[ currently_trained_layer ]->fprop(
                 hidden_reconstruction_activations,
@@ -3117,6 +3377,7 @@
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         layers[i]->setLearningRate( the_learning_rate );
+        reconstruction_layers[i]->setLearningRate( the_learning_rate );
         connections[i]->setLearningRate( the_learning_rate );
         if(correlation_layers.length() != 0)
         {
@@ -3139,8 +3400,24 @@
     final_module->setLearningRate( the_learning_rate );
 }
 
-Vec StackedAutoassociatorsNet::fantasizeKTime(int KTime, const Vec& srcImg, const Vec& sample, const Vec& maskNoiseFractOrProb)
+TVec<Vec> StackedAutoassociatorsNet::fantasizeKTimeOnMultiSrcImg(const int KTime, const Mat& srcImg, const Vec& sample, const Vec& maskNoiseFractOrProb, bool alwaysFromSrcImg)
 {
+    int n=srcImg.length();
+    TVec<Vec> output(0);
+
+    for( int i=0; i<n; i++ )
+    {
+        const Vec img_i = srcImg(i);
+        TVec<Vec> outputTmp;  
+        outputTmp = fantasizeKTime(KTime, img_i, sample, maskNoiseFractOrProb, alwaysFromSrcImg);
+        output = concat(output, outputTmp);        
+    }
+
+    return output;
+}
+
+TVec<Vec> StackedAutoassociatorsNet::fantasizeKTime(const int KTime, const Vec& srcImg, const Vec& sample, const Vec& maskNoiseFractOrProb, bool alwaysFromSrcImg)
+{
     bool bFractOrProbUseful=false;
 
     // Noise type that needs fraction_of_masked_inputs or prob_masked_inputs
@@ -3179,7 +3456,7 @@
 
     if(bFractOrProbUseful)
     {
-        if(old_prob_masked_inputs != 0)
+        if(old_prob_masked_inputs > 0.)
             bFraction_masked_input = false;
         else
             if(autoassociator_expectation_indices.size() == 0)
@@ -3188,12 +3465,17 @@
                 autoassociator_expectation_indices_temp_initialized = true;
             }
     }
+    
+    TVec<Vec> fantaImagesObtained(KTime+1);
 
+    fantaImagesObtained[0].resize(srcImg.size());
+    fantaImagesObtained[0] << srcImg;
     expectations[0] << srcImg;
-
+    
     // Do fantasize k time.
     for( int k=0 ; k<KTime ; k++ )
     {
+        fantaImagesObtained[k+1].resize(srcImg.size());
         for( int i=0 ; i<n_hlayers_used; i++ )
         {
             // Initialisation made only at the first loop.
@@ -3229,7 +3511,7 @@
         for( int i=n_hlayers_used-1 ; i>=0; i-- )
         {
             // Binomial sample
-            if(sample[i])
+            if( sample[i] == 1 )
                 for( int j=0; j<expectations[i+1].size(); j++ )
                     expectations[i+1][j] =
                     random_gen->binomial_sample(expectations[i+1][j]);
@@ -3241,8 +3523,11 @@
             Vec divided_reconstruction_activations(reconstruction_activations.size());
             divide_input(reconstruction_activations, divided_reconstruction_activations);
 
-            layers[i]->fprop(divided_reconstruction_activations, expectations[i]);
+            reconstruction_layers[i]->fprop(divided_reconstruction_activations, expectations[i]);
         }
+        fantaImagesObtained[k+1] << expectations[0];
+        if( alwaysFromSrcImg )
+            expectations[0] << srcImg;
     }
 
     if(bFractOrProbUseful)
@@ -3254,10 +3539,9 @@
     mask_input_layer_only = old_mask_input_layer_only;
     nb_corrupted_layer = old_nb_corrupted_layer;
 
-    return expectations[0];
+    return fantaImagesObtained;
 }
 
-
 } // end of namespace PLearn
 
 



From islaja at mail.berlios.de  Thu Jan 21 23:31:34 2010
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Thu, 21 Jan 2010 23:31:34 +0100
Subject: [Plearn-commits] r10315 - trunk/plearn_learners/online
Message-ID: <201001212231.o0LMVY7n002353@sheep.berlios.de>

Author: islaja
Date: 2010-01-21 23:31:33 +0100 (Thu, 21 Jan 2010)
New Revision: 10315

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2010-01-21 22:30:06 UTC (rev 10314)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2010-01-21 22:31:33 UTC (rev 10315)
@@ -103,6 +103,12 @@
     //! The layers of units in the network
     TVec< PP<RBMLayer> > layers;
 
+    //! The reconstruction layers in the network.
+    //! If not defined, will be the same layers as in the encodage phase.
+    //! Useful for example if we want binomial input components 
+    //! and real reconstruction components.
+    TVec< PP<RBMLayer> > reconstruction_layers;
+
     //! The weights of the connections between the layers
     TVec< PP<RBMConnection> > connections;
 
@@ -158,7 +164,7 @@
     string noise_type;
 
     //! Method used to fill the double_input vector when using missing_data
-    //| noise type.
+    //! noise type.
     string missing_data_method;
 
     //! Weight given to a corrupted (or missing) data when
@@ -188,6 +194,25 @@
     //! 0 or 1 according to prop_salt_noise. 
     bool mask_with_pepper_salt;
 
+    //! Indicate if the pepper salt is zero centered (>0) or not (0).
+    //! If pep_salt_zero_centered equal 0
+    //!   then pepper value is 0 and salt value is 1.
+    //! If pep_salt_zero_centered is greater than 0, 
+    //!   then the pepper value is -pep_salt_zero_centered and 
+    //!   the salt value is pep_salt_zero_centered.
+    real pep_salt_zero_centered;
+
+    //! Indication that the autoassociator will try to
+    //! "reconstruct" another _corrupted version_ of the input
+    //! (instead of the input itself).
+    bool renoising;
+
+    //! Usage of noisy example before doing a particular training.
+    //! Could be before unsup. pre-training (=1) or 
+    //! before unsup pre-training AND before supervised fine-tuning (=2).
+    //! Original example are used for any test.
+    int noisy;   
+
     //! Probability that we mask the input by 1 instead of 0.
     real prob_salt_noise;
 
@@ -225,7 +250,17 @@
     //! -1 (default) means the number of training samples.
     int train_stats_window;
 
+    //! Experiment directory where the learner will be save
+    //! if save_learner_before_fine_tuning is true.
+    string learnerExpdir;
 
+    //! Saves the learner before the supervised fine_tuning.
+    bool save_learner_before_fine_tuning;
+    
+    //! Keep trace of the representations obtained during an 
+    //! unsupervised training phase.
+    bool keep_online_representations;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -277,9 +312,20 @@
     //! and  for which it updates the VecStatsCollector train_stats.
     virtual TVec<std::string> getTrainCostNames() const;
 
+    //! Returns the representations obtained during last pre-training of the current layer.
+    inline TVec< Vec > getTrainRepresentations() const
+    {
+        return train_representations;
+    }
 
+    inline void remote_setCurrentlyTrainedLayer(int new_currently_trained_layer)
+    {
+        currently_trained_layer = new_currently_trained_layer;
+    }
+        
+
     void greedyStep(const Vec& input, const Vec& target, int index,
-                    Vec train_costs);
+                    Vec train_costs, Vec& representation);
     void greedyStep(const Mat& inputs, const Mat& targets, int index,
                     Mat& train_costs);
 
@@ -435,6 +481,14 @@
     //! Layers randomly masked, for unsupervised fine-tuning.
     TVec< Vec > corrupted_autoassociator_expectations;
 
+    //! Corrupted version of the autoassociator input not used for denoising.
+    //! Useful when renoising or noisy is true. 
+    //! In the first case: autoassociator will try to have its decoded version
+    //! getting closer to this corrupted version during the non-supervised training.
+    //! In the second case: example will be corrupted before any non-supervised
+    //! (without denoising) and supervised training. 
+    TVec< Vec > second_corrupted_autoassociator_expectations;
+
     //! Stores the weight of each data used when
     //! backpropagating the gradient of reconstruction cost.
     //! The weight is either corrupted_data_weight or data_weight
@@ -443,6 +497,9 @@
     //! during the reconstruction.
     mutable Vec reconstruction_weights;
 
+    //! Representations computed for the current trained layer.
+    mutable TVec< Vec > train_representations;
+
     //! Layers random binary maske, for online learning.
     TVec< Vec > binary_masks;
 
@@ -503,7 +560,9 @@
     void divide_input(const Vec& input, Vec& divided_input) const ;
 
     //! Supposes the learner is already trained.
-    //! Allows a codage-decodage ktime from a source image. Returns the 'fantasize' image. 
+    //! Allows a codage-decodage ktime, each time from the same source image (alwaysFromSrcImg==True)
+    //!   or each time from the last fantasize image (alwaysFromSrcImg==False). 
+    //!   Returns the source image followed by the kTime obtained 'fantasize' images. 
     //! You can choose how many layers to use (including raws layer) by defining the size of sample. 
     //! You can corrupt layers differently during the codage phase by defining maskNoiseFractOrProb 
     //! You can apply a binary sampling (1) or not (0) differently for each layer during the decode phase
@@ -513,9 +572,13 @@
     //!                                          // and first hidden layer.
     //!     sample = [1,0,0] // sampling only before reconstruction of the
     //!                      // raws input.
-    Vec fantasizeKTime(int KTime, const Vec& srcImg, const Vec& sample,
-                        const Vec& maskNoiseFractOrProb);
+    TVec<Vec> fantasizeKTime(const int KTime, const Vec& srcImg, const Vec& sample,
+                        const Vec& maskNoiseFractOrProb, bool alwaysFromSrcImg);
 
+    //! Same as fantasizeKTime, but does it on different source images.
+    TVec<Vec> fantasizeKTimeOnMultiSrcImg(const int KTime, const Mat& srcImg, const Vec& sample,
+                        const Vec& maskNoiseFractOrProb, bool alwaysFromSrcImg);
+
     void corrupt_input(const Vec& input, Vec& corrupted_input, int layer, Vec& binary_mask);
 
     //! Global storage to save memory allocations.



From islaja at mail.berlios.de  Fri Jan 22 00:02:19 2010
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Fri, 22 Jan 2010 00:02:19 +0100
Subject: [Plearn-commits] r10316 - trunk/plearn_learners/online
Message-ID: <201001212302.o0LN2JBV006439@sheep.berlios.de>

Author: islaja
Date: 2010-01-22 00:02:18 +0100 (Fri, 22 Jan 2010)
New Revision: 10316

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
-Option to save the learner just after the pre-train
    -Use 'learnerExpdir' and 'save_learner_before_fine_tuning'.

-Variante DBN with corruption during pre-training
    -Use 'use_corrupted_posDownVal': 
	 = 'none' , no corruption (basic RBM)
         = 'for_cd_fprop': a corrupted version of the positive input vector is used during the fprop phase of CD
         = 'for_cd_update': a corrupted version of the positive input vector is used during the update phase of CD.
    -Implemented for noises of type 'mask to zero' or 'pepper salt'

-Added the functions 'fantasizeKTime'  and 'fantasizeKTimeOnMultiSrcImg' for a generative scenario of encoding-decoding with or without sampling during the decoding phase.



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2010-01-21 22:31:33 UTC (rev 10315)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2010-01-21 23:02:18 UTC (rev 10316)
@@ -41,6 +41,7 @@
 #include "DeepBeliefNet.h"
 #include "RBMMatrixTransposeConnection.h"
 #include <plearn/io/pl_log.h>
+#include <plearn/io/load_and_save.h>
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
 
@@ -70,7 +71,14 @@
     use_classification_cost( true ),
     reconstruct_layerwise( false ),
     i_output_layer( -1 ),
+    learnerExpdir(""),
+    save_learner_before_fine_tuning( false ),
     use_sample_for_up_layer( false ),
+    use_corrupted_posDownVal( "none" ),
+    noise_type( "masking_noise" ),
+    fraction_of_masked_inputs( 0 ),
+    mask_with_pepper_salt( false ),
+    prob_salt_noise( 0.5 ),
     online ( false ),
     background_gibbs_update_ratio(0),
     gibbs_chain_reinit_freq( INT_MAX ),
@@ -93,6 +101,49 @@
     n_layers = 0;
 }
 
+
+void DeepBeliefNet::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+    declareMethod(
+        rmm, "fantasizeKTime",
+        &DeepBeliefNet::fantasizeKTime,
+        (BodyDoc("On a trained learner, computes a codage-decodage (fantasize) through a specified number of hidden layer."),
+         ArgDoc ("kTime", "Number of time we want to fantasize. \n"
+                 "Next input image will again be the source Image (if alwaysFromSrcImg is True) \n"
+                 "or next input image will be the last fantasize image (if alwaysFromSrcImg is False), and so on for kTime.)"),
+         ArgDoc ("srcImg", "Source image vector (should have same width as raws layer)"),
+         ArgDoc ("sampling", "Vector of bool indicating whether or not a sampling will be done for each hidden layer\n"
+                "during decodage. Its width indicates how many hidden layer will be used.)\n"
+                " (should have same width as maskNoiseFractOrProb)\n"
+                "smaller element of the vector correspond to lower layer"),
+         ArgDoc ("alwaysFromSrcImg", "Booleen indicating whether each encode-decode \n"
+                "steps are done from the source image (sets to True) or \n"
+                "if the next input image is the last fantasize image (sets to False). "),
+         RetDoc ("Fantasize images obtained for each kTime.")));
+
+
+    declareMethod(
+        rmm, "fantasizeKTimeOnMultiSrcImg",
+        &DeepBeliefNet::fantasizeKTimeOnMultiSrcImg,
+        (BodyDoc("Call the 'fantasizeKTime' function for each source images found in the matrix 'srcImg'."),
+         ArgDoc ("kTime", "Number of time we want to fantasize for each source images. \n"
+                 "Next input image will again be the source Image (if alwaysFromSrcImg is True) \n"
+                 "or next input image will be the last fantasize image (if alwaysFromSrcImg is False), and so on for kTime.)"),
+         ArgDoc ("srcImg", "Source images matrix (should have same width as raws layer)"),
+         ArgDoc ("sampling", "Vector of bool indicating whether or not a sampling will be done for each hidden layer\n"
+                "during decodage. Its width indicates how many hidden layer will be used.)\n"
+                " (should have same width as maskNoiseFractOrProb)\n"
+                "smaller element of the vector correspond to lower layer"),
+         ArgDoc ("alwaysFromSrcImg", "Booleen indicating whether each encode-decode \n"
+                "steps are done from the source image (sets to True) or \n"
+                "if the next input image is the preceding fantasize image obtained (sets to False). "),
+         RetDoc ("For each source images, fantasize images obtained for each kTime.")));
+}
+
+
+
 ////////////////////
 // declareOptions //
 ////////////////////
@@ -209,6 +260,19 @@
                   OptionBase::buildoption,
                   "Optional target matrix connections for greedy layer-wise pretraining");
 
+    declareOption(ol, "learnerExpdir",
+                  &DeepBeliefNet::learnerExpdir,
+                  OptionBase::buildoption,
+                  "Experiment directory where the learner will be save\n"
+                  "if save_learner_before_fine_tuning is true."
+        );
+
+    declareOption(ol, "save_learner_before_fine_tuning",
+                  &DeepBeliefNet::save_learner_before_fine_tuning,
+                  OptionBase::buildoption,
+                  "Saves the learner before the supervised fine-tuning."
+        );
+
     declareOption(ol, "classification_module",
                   &DeepBeliefNet::classification_module,
                   OptionBase::learntoption,
@@ -267,6 +331,41 @@
                   "Indication that the update of the top layer during CD uses\n"
                   "a sample, not the expectation.\n");
 
+    declareOption(ol, "use_corrupted_posDownVal",
+                  &DeepBeliefNet::use_corrupted_posDownVal,
+                  OptionBase::buildoption,
+                  "Indicates whether we will use a corrupted version of the\n"
+                  "positive down value during the CD step.\n"
+                  "Choose among:\n"
+                  " - \"for_cd_fprop\"\n"
+                  " - \"for_cd_update\"\n"
+                  " - \"none\"\n");
+
+    declareOption(ol, "noise_type",
+                  &DeepBeliefNet::noise_type,
+                  OptionBase::buildoption,
+                  "Type of noise that corrupts the pos_down_val. "
+                  "Choose among:\n"
+                  " - \"masking_noise\"\n"
+                  " - \"none\"\n");
+
+    declareOption(ol, "fraction_of_masked_inputs",
+                  &DeepBeliefNet::fraction_of_masked_inputs,
+                  OptionBase::buildoption,
+                  "Fraction of the pos_down_val components which\n"
+                  "will be masked.\n");
+
+    declareOption(ol, "mask_with_pepper_salt",
+                  &DeepBeliefNet::mask_with_pepper_salt,
+                  OptionBase::buildoption,
+                  "Indication that inputs should be masked with "
+                  "0 or 1 according to prob_salt_noise.\n");
+
+    declareOption(ol, "prob_salt_noise",
+                  &DeepBeliefNet::prob_salt_noise,
+                  OptionBase::buildoption,
+                  "Probability that we mask the input by 1 instead of 0.\n");
+
     declareOption(ol, "online", &DeepBeliefNet::online,
                   OptionBase::buildoption,
                   "If true then all unsupervised training stages (as well as\n"
@@ -388,6 +487,12 @@
         PLERROR("In DeepBeliefNet::build_ - mean_field_contrastive_divergence_ratio should "
             "be in [0,1].");
 
+    if( use_corrupted_posDownVal != "for_cd_fprop" &&
+        use_corrupted_posDownVal != "for_cd_update" &&
+        use_corrupted_posDownVal != "none" )
+        PLERROR("In DeepBeliefNet::build_ - use_corrupted_posDownVal should "
+            "be chosen among {\"for_cd_fprop\",\"for_cd_update\",\"none\"}.");
+
     if( !online )
     {
         if( training_schedule.length() != n_layers )
@@ -546,6 +651,7 @@
     expectation_gradients.resize( n_layers );
     expectations_gradients.resize( n_layers );
     gibbs_down_state.resize( n_layers-1 );
+    expectation_indices.resize( n_layers-1 );
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
@@ -647,6 +753,39 @@
                 greedy_target_layers[i]->forget();
             }
         }
+        if( use_corrupted_posDownVal != "none" )
+        {
+            if( greedy_target_layers.length() != 0 )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_corrupted_posDownVal not implemented for greedy_target_layers.");
+
+            if( online )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_corrupted_posDownVal not implemented for online.");
+
+            if( use_classification_cost )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_classification_cost not implemented for use_corrupted_posDownVal.");
+
+            if( background_gibbs_update_ratio != 0 )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_corrupted_posDownVal not implemented with background_gibbs_update_ratio!=0.");
+
+            if( batch_size != 1 || minibatch_hack )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_corrupted_posDownVal not implemented for batch_size != 1 or minibatch_hack.");
+    
+            if( !partial_costs.isEmpty() )
+                PLERROR("DeepBeliefNet::build_layers_and_connections() - \n"
+                        "use_corrupted_posDownVal not implemented for partial_costs.");
+
+            if( noise_type == "masking_noise" && fraction_of_masked_inputs > 0 )
+            {
+                expectation_indices[i].resize( layers[i]->size );
+                for( int j=0 ; j < expectation_indices[i].length() ; j++ )
+                    expectation_indices[i][j] = j;
+            }
+        }
     }
     if( !(layers[n_layers-1]->random_gen) )
     {
@@ -852,6 +991,7 @@
     deepCopyField(save_layer_activations,   copies);
     deepCopyField(save_layer_expectations,  copies);
     deepCopyField(pos_down_val,             copies);
+    deepCopyField(corrupted_pos_down_val,   copies);
     deepCopyField(pos_up_val,               copies);
     deepCopyField(pos_down_vals,            copies);
     deepCopyField(pos_up_vals,              copies);
@@ -872,6 +1012,7 @@
     deepCopyField(generative_connections,   copies);
     deepCopyField(up_sample,                copies);
     deepCopyField(down_sample,              copies);
+    deepCopyField(expectation_indices,      copies);
 }
 
 
@@ -1246,6 +1387,16 @@
             }
         }
 
+        if( save_learner_before_fine_tuning )
+        {
+            if( learnerExpdir == "" )
+                PLWARNING("DeepBeliefNet::train() - \n"
+                    "cannot save model before fine-tuning because\n"
+                    "no experiment directory has been set.");
+            else
+                PLearn::save(learnerExpdir + "/learner_before_finetuning.psave",*this);
+        }
+
         /***** fine-tuning by gradient descent *****/
         end_stage = min(cumulative_schedule[n_layers], nstages);
         if( stage >= end_stage )
@@ -1866,7 +2017,14 @@
         }
         else
         {
-            connections[i]->setAsDownInput( layers[i]->expectation );
+            if( i == index && use_corrupted_posDownVal == "for_cd_fprop" )
+            {
+                corrupted_pos_down_val.resize( layers[i]->size );
+                corrupt_input( layers[i]->expectation, corrupted_pos_down_val, index );
+                connections[i]->setAsDownInput( corrupted_pos_down_val );
+            }
+            else
+                connections[i]->setAsDownInput( layers[i]->expectation );
             layers[i+1]->getAllActivations( connections[i] );
             layers[i+1]->computeExpectation();
         }
@@ -1950,6 +2108,7 @@
 
     for( int i=0 ; i<=index ; i++ )
     {
+        
         connections[i]->setAsDownInputs( layers[i]->getExpectations() );
         layers[i+1]->getAllActivations( connections[i], 0, true );
         layers[i+1]->computeExpectations();
@@ -2656,7 +2815,14 @@
             up_layer->getAllActivations( connection, 0, true );
             up_layer->computeExpectations();
         } else {
-            connection->setAsDownInput( down_layer->expectation );
+            if( use_corrupted_posDownVal == "for_cd_fprop" )
+            {
+                corrupted_pos_down_val.resize( down_layer->size );
+                corrupt_input( down_layer->expectation, corrupted_pos_down_val, layer_index );
+                connection->setAsDownInput( corrupted_pos_down_val );
+            }
+            else
+                connection->setAsDownInput( down_layer->expectation );
             up_layer->getAllActivations( connection );
             up_layer->computeExpectation();
         }
@@ -2835,6 +3001,7 @@
         Vec neg_up_val;
 
         pos_down_val << down_layer->expectation;
+
         pos_up_val << up_layer->expectation;
         up_layer->generateSample();
             
@@ -2886,10 +3053,21 @@
             down_layer->setLearningRate(lr_dl * (1-mean_field_contrastive_divergence_ratio));
             up_layer->setLearningRate(lr_ul * (1-mean_field_contrastive_divergence_ratio));
             connection->setLearningRate(lr_c * (1-mean_field_contrastive_divergence_ratio));
-            
-            down_layer->update( pos_down_val, neg_down_val );
-            connection->update( pos_down_val, pos_up_val,
+           
+            if( use_corrupted_posDownVal == "for_cd_update" )
+            {
+                corrupted_pos_down_val.resize( down_layer->size );
+                corrupt_input( pos_down_val, corrupted_pos_down_val, layer_index );
+                down_layer->update( corrupted_pos_down_val, neg_down_val );
+                connection->update( corrupted_pos_down_val, pos_up_val,
                                 neg_down_val, neg_up_val );
+            }
+            else
+            {
+                down_layer->update( pos_down_val, neg_down_val );
+                connection->update( pos_down_val, pos_up_val,
+                                neg_down_val, neg_up_val );
+            }
             up_layer->update( pos_up_val, neg_up_val );
             
             down_layer->setLearningRate(lr_dl);
@@ -2907,9 +3085,20 @@
             up_layer->setLearningRate(lr_ul * mean_field_contrastive_divergence_ratio);
             connection->setLearningRate(lr_c * mean_field_contrastive_divergence_ratio);
             
-            down_layer->update( pos_down_val, mf_cd_neg_down_val );
-            connection->update( pos_down_val, pos_up_val,
+            if( use_corrupted_posDownVal == "for_cd_update" )
+            {
+                corrupted_pos_down_val.resize( down_layer->size );
+                corrupt_input( pos_down_val, corrupted_pos_down_val, layer_index );
+                down_layer->update( corrupted_pos_down_val, mf_cd_neg_down_val );
+                connection->update( corrupted_pos_down_val, pos_up_val,
                                 mf_cd_neg_down_val, mf_cd_neg_up_val );
+            }
+            else
+            {
+                down_layer->update( pos_down_val, mf_cd_neg_down_val );
+                connection->update( pos_down_val, pos_up_val,
+                                mf_cd_neg_down_val, mf_cd_neg_up_val );
+            }
             up_layer->update( pos_up_val, mf_cd_neg_up_val );
             
             down_layer->setLearningRate(lr_dl);
@@ -3235,6 +3424,50 @@
                 "(expectations are not up to date in the batch version)");
 }
 
+/////////////////////
+//  corrupt_input  //
+/////////////////////
+void DeepBeliefNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer)
+{
+    corrupted_input.resize(input.length());
+
+    if( noise_type == "masking_noise" )
+    {
+        corrupted_input << input;
+        if( fraction_of_masked_inputs != 0 )
+        {
+            random_gen->shuffleElements(expectation_indices[layer]);
+            if( mask_with_pepper_salt )
+                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    corrupted_input[ expectation_indices[layer][j] ] = random_gen->binomial_sample(prob_salt_noise);
+            else
+                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    corrupted_input[ expectation_indices[layer][j] ] = 0;
+        }
+    }
+ /*   else if( noise_type == "binary_sampling" )
+    {
+        for( int i=0; i<corrupted_input.length(); i++ )
+            corrupted_input[i] = random_gen->binomial_sample((input[i]-0.5)*binary_sampling_noise_parameter+0.5);
+    }
+    else if( noise_type == "gaussian" )
+    {
+        for( int i=0; i<corrupted_input.length(); i++ )
+            corrupted_input[i] = input[i] +
+                random_gen->gaussian_01() * gaussian_std;
+    }
+    else
+            PLERROR("In StackedAutoassociatorsNet::corrupt_input(): "
+                    "missing_data_method %s not valid with noise_type %s",
+                     missing_data_method.c_str(), noise_type.c_str());
+    }*/
+    else if( noise_type == "none" )
+        corrupted_input << input;
+    else
+        PLERROR("In DeepBeliefNet::corrupt_input(): noise_type %s not valid", noise_type.c_str());
+}
+
+
 void DeepBeliefNet::test(VMat testset, PP<VecStatsCollector> test_stats, VMat testoutputs, VMat testcosts) const
 {
 
@@ -3336,6 +3569,71 @@
         greedy_target_layers[i]->setLearningRate( the_learning_rate );
 }
 
+
+
+
+TVec<Vec> DeepBeliefNet::fantasizeKTimeOnMultiSrcImg(const int KTime, const Mat& srcImg, const Vec& sample, bool alwaysFromSrcImg)
+{
+    int n=srcImg.length();
+    TVec<Vec> output(0);
+
+    for( int i=0; i<n; i++ )
+    {
+        const Vec img_i = srcImg(i);
+        TVec<Vec> outputTmp;
+        outputTmp = fantasizeKTime(KTime, img_i, sample, alwaysFromSrcImg);
+        output = concat(output, outputTmp);
+    }
+
+    return output;
+}
+
+
+TVec<Vec> DeepBeliefNet::fantasizeKTime(const int KTime, const Vec& srcImg, const Vec& sample, bool alwaysFromSrcImg)
+{
+    if(sample.size() > n_layers-1)
+        PLERROR("In DeepBeliefNet::fantasize():"
+        " Size of sample (%i) should be <= "
+        "number of hidden layer (%i).",sample.size(), n_layers-1);
+
+    int n_hlayers_used = sample.size();
+
+    TVec<Vec> fantaImagesObtained(KTime+1);
+    fantaImagesObtained[0].resize(srcImg.size());
+    fantaImagesObtained[0] << srcImg;
+    layers[0]->setExpectation(srcImg);
+
+    for( int k=0 ; k<KTime ; k++ )
+    {
+        fantaImagesObtained[k+1].resize(srcImg.size());
+        for( int i=0 ; i<n_hlayers_used; i++ )
+        {
+            connections[i]->setAsDownInput( layers[i]->expectation );
+            layers[i+1]->getAllActivations( connections[i], 0, false );
+            layers[i+1]->computeExpectation();
+        }
+
+        for( int i=n_hlayers_used-1 ; i>=0; i-- )
+        {
+            if( sample[i] == 1 )
+            {
+                Vec expectDecode(layers[i+1]->size);
+                expectDecode << layers[i+1]->expectation;
+                for( int j=0; j<expectDecode.size(); j++ )
+                    expectDecode[j] = random_gen->binomial_sample(expectDecode[j]);
+                layers[i+1]->setExpectation(expectDecode);
+            }
+            connections[i]->setAsUpInput( layers[i+1]->expectation );
+                layers[i]->getAllActivations( connections[i], 0, false );
+                layers[i]->computeExpectation();
+        }
+        fantaImagesObtained[k+1] << layers[0]->expectation;
+        if( alwaysFromSrcImg )
+            layers[0]->setExpectation(srcImg);
+    }
+    return fantaImagesObtained;
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2010-01-21 22:31:33 UTC (rev 10315)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2010-01-21 23:02:18 UTC (rev 10316)
@@ -137,6 +137,13 @@
     //! (when the final module is external)
     int i_output_layer;
 
+    //! Experiment directory where the learner will be save
+    //! if save_learner_before_fine_tuning is true.
+    string learnerExpdir;
+ 
+    //! Saves the learner before the supervised fine_tuning.
+    bool save_learner_before_fine_tuning;
+  
     //! The weights of the connections between the layers
     TVec< PP<RBMConnection> > connections;
 
@@ -169,6 +176,30 @@
     //! a sample, not the expectation.
     bool use_sample_for_up_layer;
 
+    //! Indicates whether we will use a corrupted version of the
+    //! positive down value during the CD step. 
+    //! "Choose among:
+    //! - \"for_cd_fprop\"
+    //! - \"for_cd_update\"
+    //!   \"none\" 
+    string use_corrupted_posDownVal;
+    
+    //! Type of noise that corrupts the pos_down_val
+    string noise_type;
+    
+    //! Fraction of components that will be corrupted in
+    //! function corrupt_input.
+    real fraction_of_masked_inputs;
+    
+    //! Indication that inputs should be
+    //! masked with 0 or 1 according to prop_salt_noise.
+    bool mask_with_pepper_salt;
+    
+    //! Probability that we mask by 1 instead of 0 when
+    //! using mask_with_pepper_salt
+    real prob_salt_noise;
+
+
     //#####  Public Learnt Options  ###########################################
     //! The module computing the probabilities of the different classes.
     PP<RBMClassificationModule> classification_module;
@@ -272,7 +303,6 @@
     // (PLEASE IMPLEMENT IN .cc)
     virtual TVec<std::string> getTrainCostNames() const;
 
-
     void onlineStep(const Vec& input, const Vec& target, Vec& train_costs);
     void onlineStep(const Mat& inputs, const Mat& targets, Mat& train_costs);
 
@@ -381,6 +411,7 @@
 
     //! Store a copy of the positive phase values
     mutable Vec pos_down_val;
+    mutable Vec corrupted_pos_down_val;
     mutable Vec pos_up_val;
     mutable Mat pos_down_vals;
     mutable Mat pos_up_vals;
@@ -452,11 +483,17 @@
     mutable TVec<Vec> up_sample;
     mutable TVec<Vec> down_sample;
 
+    //! Indices of the expectation components
+    TVec< TVec<int> > expectation_indices;
+
 protected:
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
+   
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
 
 private:
     //#####  Private Member Functions  ########################################
@@ -472,8 +509,15 @@
 
     void build_final_cost();
 
+    void corrupt_input(const Vec& input, Vec& corrupted_input, int layer);
+
     void setLearningRate( real the_learning_rate );
 
+    TVec<Vec> fantasizeKTime(const int KTime, const Vec& srcImg, const Vec& sample,
+                         bool alwaysFromSrcImg);
+    TVec<Vec> fantasizeKTimeOnMultiSrcImg(const int KTime, const Mat& srcImg, const Vec& sample,
+                         bool alwaysFromSrcImg);
+
 private:
     //#####  Private Data Members  ############################################
 



From islaja at mail.berlios.de  Fri Jan 22 00:06:33 2010
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Fri, 22 Jan 2010 00:06:33 +0100
Subject: [Plearn-commits] r10317 - trunk/plearn_learners/online
Message-ID: <201001212306.o0LN6Xb1006790@sheep.berlios.de>

Author: islaja
Date: 2010-01-22 00:06:32 +0100 (Fri, 22 Jan 2010)
New Revision: 10317

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2010-01-21 23:02:18 UTC (rev 10316)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2010-01-21 23:06:32 UTC (rev 10317)
@@ -3493,11 +3493,12 @@
             }
 
             if(bFractOrProbUseful)
+            {
                 if(bFraction_masked_input)
                     fraction_of_masked_inputs = maskNoiseFractOrProb[i];
                 else
                     probability_of_masked_inputs = maskNoiseFractOrProb[i];
-
+            }
             double_input(expectations[i], doubled_expectations[i]);
             corrupt_input(
                 doubled_expectations[i],
@@ -3513,8 +3514,7 @@
             // Binomial sample
             if( sample[i] == 1 )
                 for( int j=0; j<expectations[i+1].size(); j++ )
-                    expectations[i+1][j] =
-                    random_gen->binomial_sample(expectations[i+1][j]);
+                    expectations[i+1][j] = random_gen->binomial_sample(expectations[i+1][j]);
     
             reconstruction_connections[i]->fprop(
                 expectations[i+1],



From islaja at mail.berlios.de  Fri Jan 22 00:13:25 2010
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Fri, 22 Jan 2010 00:13:25 +0100
Subject: [Plearn-commits] r10318 - trunk/plearn_learners/online
Message-ID: <201001212313.o0LNDP3U007267@sheep.berlios.de>

Author: islaja
Date: 2010-01-22 00:13:24 +0100 (Fri, 22 Jan 2010)
New Revision: 10318

Modified:
   trunk/plearn_learners/online/RBMMatrixConnection.cc
Log:


Modified: trunk/plearn_learners/online/RBMMatrixConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMMatrixConnection.cc	2010-01-21 23:06:32 UTC (rev 10317)
+++ trunk/plearn_learners/online/RBMMatrixConnection.cc	2010-01-21 23:13:24 UTC (rev 10318)
@@ -866,7 +866,7 @@
         for( int j=0; j<down_size; j++ )
         {
             if( delta_L2 != 0. )
-                w_[j] *= (1 - 2*delta_L2);
+                w_[j] *= (1 - delta_L2);
 
             if( delta_L1 != 0. )
             {
@@ -900,7 +900,7 @@
         for( int j=0; j<weights.width(); j++ )
         {
             if( delta_L2 != 0. )
-                gw_[j] += 2*delta_L2*w_[j];
+                gw_[j] += delta_L2*w_[j];
 
             if( delta_L1 != 0. )
             {



