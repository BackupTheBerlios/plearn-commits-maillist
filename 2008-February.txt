From larocheh at mail.berlios.de  Fri Feb  1 05:46:57 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Feb 2008 05:46:57 +0100
Subject: [Plearn-commits] r8442 - trunk/plearn_learners_experimental
Message-ID: <200802010446.m114kvrr003142@sheep.berlios.de>

Author: larocheh
Date: 2008-02-01 05:46:56 +0100 (Fri, 01 Feb 2008)
New Revision: 8442

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
   trunk/plearn_learners_experimental/DiscriminativeRBM.h
Log:
Added an option to do multitask learning.


Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-01-31 18:46:06 UTC (rev 8441)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-02-01 04:46:56 UTC (rev 8442)
@@ -69,7 +69,8 @@
     target_weights_L2_penalty_factor( 0. ),
     do_not_use_discriminative_learning( false ),
     unlabeled_class_index_begin( 0 ),
-    n_classes_at_test_time( -1 )
+    n_classes_at_test_time( -1 ),
+    n_mean_field_iterations( 1 )
 
 {
     random_gen = new PRandom();
@@ -155,12 +156,24 @@
                   "The classes that will be discriminated are indexed\n"
                   "from 0 to n_classes_at_test_time.\n");
 
+    declareOption(ol, "n_mean_field_iterations", 
+                  &DiscriminativeRBM::n_mean_field_iterations,
+                  OptionBase::buildoption,
+                  "Number of mean field iterations for the approximate computation of p(y|x)\n"
+                  "for multitask learning.\n");
+
     declareOption(ol, "classification_module",
                   &DiscriminativeRBM::classification_module,
                   OptionBase::learntoption,
                   "The module computing the class probabilities.\n"
                   );
 
+    declareOption(ol, "multitask_classification_module",
+                  &DiscriminativeRBM::multitask_classification_module,
+                  OptionBase::learntoption,
+                  "The module approximating the multitask class probabilities.\n"
+                  );
+
     declareOption(ol, "classification_cost",
                   &DiscriminativeRBM::classification_cost,
                   OptionBase::nosave,
@@ -208,14 +221,30 @@
     build_classification_cost();
 
     int current_index = 0;
-    //cost_names.append("NLL");
-    //nll_cost_index = current_index;
-    //current_index++;
+    if( targetsize() > 1 )
+    {
+        cost_names.append("NLL");
+        nll_cost_index = current_index;
+        current_index++;
+    }
     
     cost_names.append("class_error");
     class_cost_index = current_index;
     current_index++;
 
+    if( targetsize() > 1 )
+    {
+        cost_names.append("hamming_loss");
+        hamming_loss_index = current_index;
+        current_index++;
+    }
+    
+    for( int i=0; i<targetsize(); i++ )
+    {
+        cost_names.append("class_error_" + tostring(i));
+        current_index++;
+    }
+
     PLASSERT( current_index == cost_names.length() );
 }
 
@@ -249,6 +278,7 @@
 
     input_gradient.resize( inputsize() );
     class_output.resize( n_classes );
+    before_class_output.resize( n_classes );
     class_gradient.resize( n_classes );
 
     target_one_hot.resize( n_classes );
@@ -296,110 +326,177 @@
 {
     MODULE_LOG << "build_classification_cost() called" << endl;
 
-    if (!classification_module ||
-        classification_module->target_layer->size != n_classes ||
-        classification_module->last_layer != hidden_layer || 
-        classification_module->previous_to_last != connection )
+    if( targetsize() == 1 )
     {
-        // We need to (re-)create 'last_to_target', and thus the classification
-        // module too.
-        // This is not systematically done so that the learner can be
-        // saved and loaded without losing learned parameters.
-        last_to_target = new RBMMatrixConnection();
-        last_to_target->up_size = hidden_layer->size;
-        last_to_target->down_size = n_classes;
-        last_to_target->L1_penalty_factor = target_weights_L1_penalty_factor;
-        last_to_target->L2_penalty_factor = target_weights_L2_penalty_factor;
-        last_to_target->random_gen = random_gen;
-        last_to_target->build();
+        if (!classification_module ||
+            classification_module->target_layer->size != n_classes ||
+            classification_module->last_layer != hidden_layer || 
+            classification_module->previous_to_last != connection )
+        {
+            // We need to (re-)create 'last_to_target', and thus the classification
+            // module too.
+            // This is not systematically done so that the learner can be
+            // saved and loaded without losing learned parameters.
+            last_to_target = new RBMMatrixConnection();
+            last_to_target->up_size = hidden_layer->size;
+            last_to_target->down_size = n_classes;
+            last_to_target->L1_penalty_factor = target_weights_L1_penalty_factor;
+            last_to_target->L2_penalty_factor = target_weights_L2_penalty_factor;
+            last_to_target->random_gen = random_gen;
+            last_to_target->build();
+            
+            target_layer = new RBMMultinomialLayer();
+            target_layer->size = n_classes;
+            target_layer->random_gen = random_gen;
+            target_layer->build();
+            
+            classification_module = new RBMClassificationModule();
+            classification_module->previous_to_last = connection;
+            classification_module->last_layer = hidden_layer;
+            classification_module->last_to_target = last_to_target;
+            classification_module->target_layer = 
+                dynamic_cast<RBMMultinomialLayer*>((RBMLayer*) target_layer);
+            classification_module->random_gen = random_gen;
+            classification_module->build();
+        }
 
-        target_layer = new RBMMultinomialLayer();
-        target_layer->size = n_classes;
-        target_layer->random_gen = random_gen;
-        target_layer->build();
+        classification_cost = new NLLCostModule();
+        classification_cost->input_size = n_classes;
+        classification_cost->target_size = 1;
+        classification_cost->build();
+        
+        last_to_target = classification_module->last_to_target;
+        last_to_target_connection = 
+            (RBMMatrixConnection*) classification_module->last_to_target;
+        target_layer = classification_module->target_layer;
+        joint_connection = classification_module->joint_connection;
+        
+        joint_layer = new RBMMixedLayer();
+        joint_layer->sub_layers.resize( 2 );
+        joint_layer->sub_layers[0] = input_layer;
+        joint_layer->sub_layers[1] = target_layer;
+        joint_layer->random_gen = random_gen;
+        joint_layer->build();
+        
+        if( unlabeled_class_index_begin != 0 )
+        {
+            unlabeled_class_output.resize( n_classes - unlabeled_class_index_begin );
+            PP<RBMMultinomialLayer> sub_layer = new RBMMultinomialLayer();
+            sub_layer->bias = target_layer->bias.subVec(
+                unlabeled_class_index_begin,
+                n_classes - unlabeled_class_index_begin);
+            sub_layer->size = n_classes - unlabeled_class_index_begin;
+            sub_layer->random_gen = random_gen;
+            sub_layer->build();
+            
+            PP<RBMMatrixConnection> sub_connection = new RBMMatrixConnection();
+            sub_connection->weights = last_to_target->weights.subMatColumns(
+                unlabeled_class_index_begin,
+                n_classes - unlabeled_class_index_begin);
+            sub_connection->up_size = hidden_layer->size;
+            sub_connection->down_size = n_classes - unlabeled_class_index_begin;
+            sub_connection->random_gen = random_gen;
+            sub_connection->build();
+            
+            unlabeled_classification_module = new RBMClassificationModule();
+            unlabeled_classification_module->previous_to_last = connection;
+            unlabeled_classification_module->last_layer = hidden_layer;
+            unlabeled_classification_module->last_to_target = sub_connection;
+            unlabeled_classification_module->target_layer = sub_layer;
+            unlabeled_classification_module->random_gen = random_gen;
+            unlabeled_classification_module->build();
+        }
+        
+        if( n_classes_at_test_time > 0 && n_classes_at_test_time != n_classes )
+        {
+            test_time_class_output.resize( n_classes_at_test_time ); 
+            PP<RBMMultinomialLayer> sub_layer = new RBMMultinomialLayer();
+            sub_layer->bias = target_layer->bias.subVec(
+                0, n_classes_at_test_time );
+            sub_layer->size = n_classes_at_test_time;
+            sub_layer->random_gen = random_gen;
+            sub_layer->build();
+            
+            PP<RBMMatrixConnection> sub_connection = new RBMMatrixConnection();
+            sub_connection->weights = last_to_target->weights.subMatColumns(
+                0, n_classes_at_test_time );
+            sub_connection->up_size = hidden_layer->size;
+            sub_connection->down_size = n_classes_at_test_time;
+            sub_connection->random_gen = random_gen;
+            sub_connection->build();
 
-        classification_module = new RBMClassificationModule();
-        classification_module->previous_to_last = connection;
-        classification_module->last_layer = hidden_layer;
-        classification_module->last_to_target = last_to_target;
-        classification_module->target_layer = target_layer;
-        classification_module->random_gen = random_gen;
-        classification_module->build();
+            test_time_classification_module = new RBMClassificationModule();
+            test_time_classification_module->previous_to_last = connection;
+            test_time_classification_module->last_layer = hidden_layer;
+            test_time_classification_module->last_to_target = sub_connection;
+            test_time_classification_module->target_layer = sub_layer;
+            test_time_classification_module->random_gen = random_gen;
+            test_time_classification_module->build();
+        }
     }
-
-    classification_cost = new NLLCostModule();
-    classification_cost->input_size = n_classes;
-    classification_cost->target_size = 1;
-    classification_cost->build();
-
-    last_to_target = classification_module->last_to_target;
-    last_to_target_connection = 
-        (RBMMatrixConnection*) classification_module->last_to_target;
-    target_layer = classification_module->target_layer;
-    joint_connection = classification_module->joint_connection;
-
-    joint_layer = new RBMMixedLayer();
-    joint_layer->sub_layers.resize( 2 );
-    joint_layer->sub_layers[0] = input_layer;
-    joint_layer->sub_layers[1] = target_layer;
-    joint_layer->random_gen = random_gen;
-    joint_layer->build();
-
-    if( unlabeled_class_index_begin != 0 )
+    else
     {
-        unlabeled_class_output.resize( n_classes - unlabeled_class_index_begin );
-        PP<RBMMultinomialLayer> sub_layer = new RBMMultinomialLayer();
-        sub_layer->bias = target_layer->bias.subVec(
-            unlabeled_class_index_begin,
-            n_classes - unlabeled_class_index_begin);
-        sub_layer->size = n_classes - unlabeled_class_index_begin;
-        sub_layer->random_gen = random_gen;
-        sub_layer->build();
+        if( n_classes != targetsize() )
+            PLERROR("In DiscriminativeRBM::build_classification_cost(): "
+                    "n_classes should be equal to targetsize()");
+        
+        // Multitask setting
+        if (!multitask_classification_module ||
+            multitask_classification_module->target_layer->size != n_classes ||
+            multitask_classification_module->last_layer != hidden_layer || 
+            multitask_classification_module->previous_to_last != connection )
+        {
+            // We need to (re-)create 'last_to_target', and thus the 
+            // multitask_classification module too.
+            // This is not systematically done so that the learner can be
+            // saved and loaded without losing learned parameters.
+            last_to_target = new RBMMatrixConnection();
+            last_to_target->up_size = hidden_layer->size;
+            last_to_target->down_size = n_classes;
+            last_to_target->L1_penalty_factor = target_weights_L1_penalty_factor;
+            last_to_target->L2_penalty_factor = target_weights_L2_penalty_factor;
+            last_to_target->random_gen = random_gen;
+            last_to_target->build();
+            
+            target_layer = new RBMBinomialLayer();
+            target_layer->size = n_classes;
+            target_layer->random_gen = random_gen;
+            target_layer->build();
+            
+            multitask_classification_module = 
+                new RBMMultitaskClassificationModule();
+            multitask_classification_module->previous_to_last = connection;
+            multitask_classification_module->last_layer = hidden_layer;
+            multitask_classification_module->last_to_target = last_to_target;
+            multitask_classification_module->target_layer = 
+                dynamic_cast<RBMBinomialLayer*>((RBMLayer*) target_layer);
+            multitask_classification_module->fprop_outputs_activation = true;
+            multitask_classification_module->n_mean_field_iterations = n_mean_field_iterations;
+            multitask_classification_module->random_gen = random_gen;
+            multitask_classification_module->build();
+        }
 
-        PP<RBMMatrixConnection> sub_connection = new RBMMatrixConnection();
-        sub_connection->weights = last_to_target->weights.subMatColumns(
-            unlabeled_class_index_begin,
-            n_classes - unlabeled_class_index_begin);
-        sub_connection->up_size = hidden_layer->size;
-        sub_connection->down_size = n_classes - unlabeled_class_index_begin;
-        sub_connection->random_gen = random_gen;
-        sub_connection->build();
-
-        unlabeled_classification_module = new RBMClassificationModule();
-        unlabeled_classification_module->previous_to_last = connection;
-        unlabeled_classification_module->last_layer = hidden_layer;
-        unlabeled_classification_module->last_to_target = sub_connection;
-        unlabeled_classification_module->target_layer = sub_layer;
-        unlabeled_classification_module->random_gen = random_gen;
-        unlabeled_classification_module->build();
+        last_to_target = multitask_classification_module->last_to_target;
+        last_to_target_connection = 
+            (RBMMatrixConnection*) multitask_classification_module->last_to_target;
+        target_layer = multitask_classification_module->target_layer;
+        joint_connection = multitask_classification_module->joint_connection;
+        
+        joint_layer = new RBMMixedLayer();
+        joint_layer->sub_layers.resize( 2 );
+        joint_layer->sub_layers[0] = input_layer;
+        joint_layer->sub_layers[1] = target_layer;
+        joint_layer->random_gen = random_gen;
+        joint_layer->build();
+        
+        if( unlabeled_class_index_begin != 0 )
+            PLERROR("In DiscriminativeRBM::build_classification_cost(): "
+                "can't use unlabeled_class_index_begin != 0 in multitask setting");
+        
+        if( n_classes_at_test_time > 0 && n_classes_at_test_time != n_classes )
+            PLERROR("In DiscriminativeRBM::build_classification_cost(): "
+                "can't use n_classes_at_test_time in multitask setting");
     }
-
-    if( n_classes_at_test_time > 0 && n_classes_at_test_time != n_classes )
-    {
-        test_time_class_output.resize( n_classes_at_test_time ); 
-        PP<RBMMultinomialLayer> sub_layer = new RBMMultinomialLayer();
-        sub_layer->bias = target_layer->bias.subVec(
-            0, n_classes_at_test_time );
-        sub_layer->size = n_classes_at_test_time;
-        sub_layer->random_gen = random_gen;
-        sub_layer->build();
-
-        PP<RBMMatrixConnection> sub_connection = new RBMMatrixConnection();
-        sub_connection->weights = last_to_target->weights.subMatColumns(
-            0, n_classes_at_test_time );
-        sub_connection->up_size = hidden_layer->size;
-        sub_connection->down_size = n_classes_at_test_time;
-        sub_connection->random_gen = random_gen;
-        sub_connection->build();
-
-        test_time_classification_module = new RBMClassificationModule();
-        test_time_classification_module->previous_to_last = connection;
-        test_time_classification_module->last_layer = hidden_layer;
-        test_time_classification_module->last_to_target = sub_connection;
-        test_time_classification_module->target_layer = sub_layer;
-        test_time_classification_module->random_gen = random_gen;
-        test_time_classification_module->build();
-    }
 }
 
 ///////////
@@ -422,6 +519,7 @@
     deepCopyField(hidden_layer, copies);
     deepCopyField(connection, copies);
     deepCopyField(classification_module, copies);
+    deepCopyField(multitask_classification_module, copies);
     deepCopyField(cost_names, copies);
     deepCopyField(classification_cost, copies);
     deepCopyField(joint_layer, copies);
@@ -446,6 +544,7 @@
     deepCopyField(semi_sup_neg_up_val, copies);
     deepCopyField(input_gradient, copies);
     deepCopyField(class_output, copies);
+    deepCopyField(before_class_output, copies);
     deepCopyField(unlabeled_class_output, copies);
     deepCopyField(test_time_class_output, copies);
     deepCopyField(class_gradient, copies);
@@ -470,8 +569,15 @@
     input_layer->forget();
     hidden_layer->forget();
     connection->forget();
-    classification_cost->forget();
-    classification_module->forget();
+    if( targetsize() > 1 )
+    {
+        multitask_classification_module->forget();
+    }
+    else
+    {
+        classification_cost->forget();
+        classification_module->forget();
+    }
 }
 
 ///////////
@@ -523,16 +629,26 @@
         if( pb )
             pb->update( stage - init_stage + 1 );
 
-        // Get CD stats...
-        target_one_hot.clear();
-        if( !is_missing(target[0]) )
+        if( targetsize() == 1 )
         {
-            target_index = (int)round( target[0] );
-            target_one_hot[ target_index ] = 1;
+            target_one_hot.clear();
+            if( !is_missing(target[0]) )
+            {
+                target_index = (int)round( target[0] );
+                target_one_hot[ target_index ] = 1;
+            }
         }
+        else
+        {
+            target_one_hot << target;
+        }
+
+        // Get CD stats...
+
         // ... for discriminative learning
         if( !do_not_use_discriminative_learning && 
-            !use_exact_disc_gradient && !is_missing(target[0]) )
+            !use_exact_disc_gradient && 
+            ( !is_missing(target[0]) || targetsize() > 1 ) )
         {
             // Positive phase
 
@@ -567,7 +683,8 @@
         }
 
         // ... for generative learning        
-        if( !is_missing(target[0]) && gen_learning_weight > 0 )
+        if( ( !is_missing(target[0]) || targetsize() > 1 ) && 
+            gen_learning_weight > 0 )
         {
             // Positive phase
             if( !use_exact_disc_gradient && !do_not_use_discriminative_learning )
@@ -631,7 +748,11 @@
 
         }
 
-        // ... and for semi-supervised learning        
+        // ... and for semi-supervised learning
+        if( targetsize() > 1 && semi_sup_learning_weight > 0 )
+            PLERROR("DiscriminativeRBM::train(): semi-supervised learning "
+                "is not implemented yet for multi-task learning.");
+
         if( is_missing(target[0]) && semi_sup_learning_weight > 0 )
         {
             // Positive phase
@@ -686,25 +807,82 @@
         // Get gradient and update
 
         if( !do_not_use_discriminative_learning && 
-            use_exact_disc_gradient && !is_missing(target[0]) )
+            use_exact_disc_gradient && 
+            ( !is_missing(target[0]) || targetsize() > 1 ) )
         {
-            classification_module->fprop( input, class_output );
-            // This doesn't work. gcc bug?
-            //classification_cost->fprop( class_output, target, nll_cost );
-            classification_cost->CostModule::fprop( class_output, target,
-                                                    nll_cost );
+            if( targetsize() == 1)
+            {
+                classification_module->fprop( input, class_output );
+                // This doesn't work. gcc bug?
+                //classification_cost->fprop( class_output, target, nll_cost );
+                classification_cost->CostModule::fprop( class_output, target,
+                                                        nll_cost );
+                
+                class_error =  ( argmax(class_output) == target_index ) ? 0: 1;  
+                //train_costs[nll_cost_index] = nll_cost;
+                train_costs[class_cost_index] = class_error;
+                
+                classification_cost->bpropUpdate( class_output, target, nll_cost,
+                                                  class_gradient );
+                
+                classification_module->bpropUpdate( input,  class_output,
+                                                    input_gradient, class_gradient );
+                
+                train_stats->update( train_costs );
+            }
+            else
+            {
+                multitask_classification_module->fprop( input, before_class_output );
+                // This doesn't work. gcc bug?
+                //multitask_classification_cost->fprop( class_output, target, 
+                //                                      nll_cost );
+                //multitask_classification_cost->CostModule::fprop( class_output, 
+                //                                                  target,
+                //                                                  nll_cost );
+                
+                target_layer->fprop( before_class_output, class_output );
+                target_layer->activation << before_class_output;
+                target_layer->activation += target_layer->bias;
+                target_layer->setExpectation( class_output );
+                nll_cost = target_layer->fpropNLL( target );
+                
+                train_costs.clear();
+                train_costs[nll_cost_index] = nll_cost;
 
-            class_error =  ( argmax(class_output) == target_index ) ? 0: 1;  
-            //train_costs[nll_cost_index] = nll_cost;
-            train_costs[class_cost_index] = class_error;
+                for( int task=0; task<targetsize(); task++)
+                {
+                    if( class_output[task] > 0.5 && target[task] != 1)
+                    {
+                        train_costs[ hamming_loss_index ]++;
+                        train_costs[ hamming_loss_index + task + 1 ] = 1;
+                    }
+                    
+                    if( class_output[task] <= 0.5 && target[task] != 0)
+                    {
+                        train_costs[ hamming_loss_index ]++;
+                        train_costs[ hamming_loss_index + task + 1 ] = 1;
+                    }
+                }
 
-            classification_cost->bpropUpdate( class_output, target, nll_cost,
-                                              class_gradient );
+                if( train_costs[ hamming_loss_index ] > 0 )
+                    train_costs[ class_cost_index ] = 1;
 
-            classification_module->bpropUpdate( input,  class_output,
-                                                input_gradient, class_gradient );
+                train_costs[ hamming_loss_index ] /= targetsize();
+                
+                //multitask_classification_cost->bpropUpdate( 
+                //    class_output, target, nll_cost,
+                //    class_gradient );
+                
+                class_gradient.clear();
+                target_layer->bpropNLL( target, nll_cost, class_gradient );
+                target_layer->update( class_gradient );
 
-            train_stats->update( train_costs );
+                multitask_classification_module->bpropUpdate( 
+                    input,  before_class_output,
+                    input_gradient, class_gradient );
+                
+                train_stats->update( train_costs );
+            }
         }
 
         // CD Updates
@@ -751,15 +929,23 @@
 {
     // Compute the output from the input.
     output.resize(0);
-    if( test_time_classification_module )
+    if( targetsize() == 1 )
     {
-        test_time_classification_module->fprop( input,
-                                                output );
+        if( test_time_classification_module )
+        {
+            test_time_classification_module->fprop( input,
+                                                    output );
+        }
+        else
+        {
+            classification_module->fprop( input,
+                                          output );
+        }
     }
     else
     {
-        classification_module->fprop( input,
-                                      output );
+        multitask_classification_module->fprop( input,
+                                                output );
     }
 }
 
@@ -771,14 +957,55 @@
     // Compute the costs from *already* computed output.
     costs.resize( cost_names.length() );
     costs.fill( MISSING_VALUE );
-    
-    if( !is_missing(target[0]) )
+
+    if( targetsize() == 1 )
     {
-        //classification_cost->fprop( output, target, costs[nll_cost_index] );
-        //classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
-        costs[class_cost_index] =
-            (argmax(output) == (int) round(target[0]))? 0 : 1;
+        if( !is_missing(target[0]) )
+        {
+            //classification_cost->fprop( output, target, costs[nll_cost_index] );
+            //classification_cost->CostModule::fprop( output, target, costs[nll_cost_index] );
+            costs[class_cost_index] =
+                (argmax(output) == (int) round(target[0]))? 0 : 1;
+        }
     }
+    else
+    {
+        costs.clear();
+
+        // This doesn't work. gcc bug?
+        //multitask_classification_cost->fprop( output, target, 
+        //                                      costs[nll_cost_index] );
+        //multitask_classification_cost->CostModule::fprop( output, 
+        //                                                  target,
+        //                                                  nll_cost );
+
+        target_layer->fprop( output, class_output );
+        target_layer->activation << output;
+        target_layer->activation += target_layer->bias;
+        target_layer->setExpectation( class_output );
+        costs[ nll_cost_index ] = target_layer->fpropNLL( target );
+
+
+        for( int task=0; task<targetsize(); task++)
+        {
+            if( class_output[task] > 0.5 && target[task] != 1)
+            {
+                costs[ hamming_loss_index ]++;
+                costs[ hamming_loss_index + task + 1 ] = 1;
+            }
+            
+            if( class_output[task] <= 0.5 && target[task] != 0)
+            {
+                costs[ hamming_loss_index ]++;
+                costs[ hamming_loss_index + task + 1 ] = 1;
+            }
+        }
+        
+        if( costs[ hamming_loss_index ] > 0 )
+            costs[ class_cost_index ] = 1;
+        
+        costs[ hamming_loss_index ] /= targetsize();
+    }
 }
 
 TVec<string> DiscriminativeRBM::getTestCostNames() const
@@ -805,7 +1032,10 @@
     connection->setLearningRate( the_learning_rate );
     target_layer->setLearningRate( the_learning_rate );
     last_to_target->setLearningRate( the_learning_rate );
-    classification_cost->setLearningRate( the_learning_rate );
+    if( targetsize() == 1)
+        classification_cost->setLearningRate( the_learning_rate );
+    //else
+    //    multitask_classification_cost->setLearningRate( the_learning_rate );
     //classification_module->setLearningRate( the_learning_rate );
 }
 

Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.h
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-01-31 18:46:06 UTC (rev 8441)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-02-01 04:46:56 UTC (rev 8442)
@@ -42,8 +42,10 @@
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/CrossEntropyCostModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMMultitaskClassificationModule.h>
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMConnection.h>
@@ -114,10 +116,18 @@
     //! from 0 to n_classes_at_test_time.
     int n_classes_at_test_time;
 
+    //! Number of mean field iterations for the approximate computation of p(y|x)
+    //! for multitask learning.
+    int n_mean_field_iterations;
+
     //#####  Public Learnt Options  ###########################################
     //! The module computing the probabilities of the different classes.
     PP<RBMClassificationModule> classification_module;
 
+    //! The module for approximate computation of the probabilities 
+    //! of the different classes.
+    PP<RBMMultitaskClassificationModule> multitask_classification_module;
+
     //! The computed cost names
     TVec<string> cost_names;
 
@@ -222,7 +232,7 @@
 
     //! Part of the RBM visible layer corresponding to the target
     //! (pointer to classification_module->target_layer)
-    PP<RBMMultinomialLayer> target_layer;
+    PP<RBMLayer> target_layer;
 
     //! Classification module for when unlabeled_class_index_begin != 0
     PP<RBMClassificationModule> unlabeled_classification_module;
@@ -251,6 +261,7 @@
     //! Temporary variables for gradient descent
     mutable Vec input_gradient;
     mutable Vec class_output;
+    mutable Vec before_class_output;
     mutable Vec unlabeled_class_output;
     mutable Vec test_time_class_output;
     mutable Vec class_gradient;
@@ -261,6 +272,9 @@
     //! Keeps the index of the class_error cost in train_costs
     int class_cost_index;
 
+    //! Keeps the index of the hamming_loss cost in train_costs
+    int hamming_loss_index;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From larocheh at mail.berlios.de  Fri Feb  1 16:59:56 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Feb 2008 16:59:56 +0100
Subject: [Plearn-commits] r8443 - trunk/plearn_learners/online
Message-ID: <200802011559.m11FxuOK017887@sheep.berlios.de>

Author: larocheh
Date: 2008-02-01 16:59:56 +0100 (Fri, 01 Feb 2008)
New Revision: 8443

Added:
   trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.h
Log:
RBM layer with lateral connection


Added: trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-02-01 04:46:56 UTC (rev 8442)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-02-01 15:59:56 UTC (rev 8443)
@@ -0,0 +1,1784 @@
+// -*- C++ -*-
+
+// RBMLateralBinomialLayer.cc
+//
+// Copyright (C) 2006 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMLateralBinomialLayer.cc */
+
+
+
+#include "RBMLateralBinomialLayer.h"
+#include <plearn/math/TMat_maths.h>
+#include "RBMConnection.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMLateralBinomialLayer,
+    "Layer in an RBM formed with binomial units and lateral connections.",
+    "");
+
+RBMLateralBinomialLayer::RBMLateralBinomialLayer( real the_learning_rate ) :
+    inherited( the_learning_rate ),
+    n_lateral_connections_passes( 1 ),
+    dampening_factor( 0. ),
+    mean_field_precision_threshold( 0. ),
+    topographic_length( -1 ),
+    topographic_width( -1 ),
+    topographic_patch_vradius( 5 ),
+    topographic_patch_hradius( 5 ),
+    topographic_lateral_weights_init_value( 0. ),
+    do_not_learn_topographic_lateral_weights( false )
+{
+}
+
+void RBMLateralBinomialLayer::reset()
+{
+    inherited::reset();
+    lateral_weights_inc.clear();
+}
+
+void RBMLateralBinomialLayer::clearStats()
+{
+    inherited::clearStats();
+    lateral_weights_pos_stats.clear();
+    lateral_weights_neg_stats.clear();
+}
+
+void RBMLateralBinomialLayer::forget()
+{
+    inherited::forget();
+    //real bu;
+    //for( int i=0; i<lateral_weights.length(); i++)
+    //    for( int j=0; j<lateral_weights.width(); j++)
+    //    {
+    //        bu = random_gen->bounded_uniform(-1.0/size,1.0/size);
+    //        lateral_weights(i,j) = bu;
+    //        lateral_weights(j,i) = bu;
+    //    }
+    lateral_weights.clear();
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();        
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+
+    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+        //topographic_lateral_weights[i].clear();
+        topographic_lateral_weights[i].fill( topographic_lateral_weights_init_value );
+}
+
+////////////////////
+// generateSample //
+////////////////////
+void RBMLateralBinomialLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectation_is_up_to_date, "Expectation should be computed "
+            "before calling generateSample()");
+
+    for( int i=0 ; i<size ; i++ )
+        sample[i] = random_gen->binomial_sample( expectation[i] );
+    
+}
+
+/////////////////////
+// generateSamples //
+/////////////////////
+void RBMLateralBinomialLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectations_are_up_to_date, "Expectations should be computed "
+            "before calling generateSamples()");
+
+    PLASSERT( samples.width() == size && samples.length() == batch_size );
+
+    for (int k = 0; k < batch_size; k++) {
+        for (int i=0 ; i<size ; i++)
+            samples(k, i) = random_gen->binomial_sample( expectations(k, i) );
+    }
+}
+
+////////////////////////
+// computeExpectation //
+////////////////////////
+void RBMLateralBinomialLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    if( temp_output.length() != n_lateral_connections_passes+1 )
+    {
+        temp_output.resize(n_lateral_connections_passes+1);
+        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+            temp_output[i].resize(size);
+    }       
+
+    current_temp_output = temp_output[0];
+    temp_output.last() = expectation;
+
+    if (use_fast_approximations)
+        for( int i=0 ; i<size ; i++ )
+            current_temp_output[i] = fastsigmoid( activation[i] );
+    else
+        for( int i=0 ; i<size ; i++ )
+            current_temp_output[i] = sigmoid( activation[i] );
+
+    for( int t=0; t<n_lateral_connections_passes; t++ )
+    {
+        previous_temp_output = current_temp_output;
+        current_temp_output = temp_output[t+1];
+        if( topographic_lateral_weights.length() == 0 )
+            product(dampening_expectation, lateral_weights, previous_temp_output);
+        else
+            productTopoLateralWeights( dampening_expectation, previous_temp_output );
+        dampening_expectation += activation;
+        if (use_fast_approximations)
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+            }
+            else
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = 
+                        (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        + dampening_factor * previous_temp_output[i];
+            }
+        }
+        else
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = sigmoid( dampening_expectation[i] );
+            }
+            else
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = 
+                        (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        + dampening_factor * previous_temp_output[i];
+            }
+        }
+        if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) && 
+            dist(current_temp_output, previous_temp_output,2)/size < mean_field_precision_threshold )
+        {
+            expectation << current_temp_output;
+            break;
+        }
+        //cout << sqrt(max(square(current_temp_output-previous_temp_output))) << " ";
+        //cout << dist(current_temp_output, previous_temp_output,2)/current_temp_output.length() << " ";
+    }
+    //cout << endl;
+    //expectation << current_temp_output;
+    expectation_is_up_to_date = true;
+}
+
+/////////////////////////
+// computeExpectations //
+/////////////////////////
+void RBMLateralBinomialLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLASSERT( expectations.width() == size
+              && expectations.length() == batch_size );
+    dampening_expectations.resize( batch_size, size );
+
+    if( temp_outputs.length() != n_lateral_connections_passes+1 )
+    {
+        temp_outputs.resize(n_lateral_connections_passes+1);
+        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+            temp_outputs[i].resize( batch_size, size);
+    }       
+
+    current_temp_outputs = temp_outputs[0];
+    temp_outputs.last() = expectations;
+
+    if (use_fast_approximations)
+        for (int k = 0; k < batch_size; k++)
+            for (int i = 0 ; i < size ; i++)
+                current_temp_outputs(k, i) = fastsigmoid(activations(k, i));
+    else
+        for (int k = 0; k < batch_size; k++)
+            for (int i = 0 ; i < size ; i++)
+                current_temp_outputs(k, i) = sigmoid(activations(k, i));
+
+    for( int t=0; t<n_lateral_connections_passes; t++ )
+    {
+        previous_temp_outputs = current_temp_outputs;
+        current_temp_outputs = temp_outputs[t+1];
+        if( topographic_lateral_weights.length() == 0 )
+            productTranspose(dampening_expectations, previous_temp_outputs, 
+                             lateral_weights);
+        else
+            for( int b = 0; b<dampening_expectations.length(); b++)
+                productTopoLateralWeights( dampening_expectations(b), 
+                                           previous_temp_outputs(b) );
+
+        dampening_expectations += activations;
+        if (use_fast_approximations)
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = 
+                            fastsigmoid( dampening_expectations(k, i) );
+            }
+            else
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = (1-dampening_factor)
+                            * fastsigmoid( dampening_expectations(k, i) ) 
+                            + dampening_factor * previous_temp_outputs(k, i);
+            }
+        }
+        else
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = 
+                            sigmoid( dampening_expectations(k, i) );
+            }
+            else
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = (1-dampening_factor) 
+                            * sigmoid( dampening_expectations(k, i) ) 
+                            + dampening_factor * previous_temp_outputs(k, i);
+            }
+        }
+    }
+    //expectations << current_temp_outputs;
+    expectations_are_up_to_date = true;
+}
+
+///////////
+// fprop //
+///////////
+void RBMLateralBinomialLayer::fprop( const Vec& input, Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+
+    add(bias, input, bias_plus_input);
+
+    if( temp_output.length() != n_lateral_connections_passes+1 )
+    {
+        temp_output.resize(n_lateral_connections_passes+1);
+        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+            temp_output[i].resize(size);
+    }       
+
+    temp_output.last() = output;
+    current_temp_output = temp_output[0];
+
+    if (use_fast_approximations)
+        for( int i=0 ; i<size ; i++ )
+            current_temp_output[i] = fastsigmoid( bias_plus_input[i] );
+    else
+        for( int i=0 ; i<size ; i++ )
+            current_temp_output[i] = sigmoid( bias_plus_input[i] );
+
+    for( int t=0; t<n_lateral_connections_passes; t++ )
+    {
+        previous_temp_output = current_temp_output;
+        current_temp_output = temp_output[t+1];
+        if( topographic_lateral_weights.length() == 0 )
+            product(dampening_expectation, lateral_weights, previous_temp_output);
+        else
+            productTopoLateralWeights( dampening_expectation, previous_temp_output );
+        dampening_expectation += bias_plus_input;
+        if (use_fast_approximations)
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+            }
+            else
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = 
+                        (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        + dampening_factor * previous_temp_output[i];
+            }
+        }
+        else
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = sigmoid( dampening_expectation[i] );
+            }
+            else
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = 
+                        (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        + dampening_factor * previous_temp_output[i];
+            }
+        }
+    }
+}
+
+void RBMLateralBinomialLayer::fprop( const Mat& inputs, Mat& outputs ) const
+{
+    int mbatch_size = inputs.length();
+    PLASSERT( inputs.width() == size );
+    outputs.resize( mbatch_size, size );
+    dampening_expectations.resize( mbatch_size, size );
+
+    if(bias_plus_inputs.length() != inputs.length() ||
+       bias_plus_inputs.width() != inputs.width())
+        bias_plus_inputs.resize(inputs.length(), inputs.width());
+    bias_plus_inputs << inputs;
+    bias_plus_inputs += bias;
+
+    if( temp_outputs.length() != n_lateral_connections_passes+1 )
+    {
+        temp_outputs.resize(n_lateral_connections_passes+1);
+        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+            temp_outputs[i].resize(mbatch_size,size);
+    }       
+
+    temp_outputs.last() = outputs;
+    current_temp_outputs = temp_outputs[0];
+
+    if (use_fast_approximations)
+        for( int k = 0; k < mbatch_size; k++ )
+            for( int i = 0; i < size; i++ )
+                current_temp_outputs(k,i) = fastsigmoid( bias_plus_inputs(k,i) );
+    else
+        for( int k = 0; k < mbatch_size; k++ )
+            for( int i = 0; i < size; i++ )
+                current_temp_outputs(k,i) = sigmoid( bias_plus_inputs(k,i) );
+
+    for( int t=0; t<n_lateral_connections_passes; t++ )
+    {
+        previous_temp_outputs = current_temp_outputs;
+        current_temp_outputs = temp_outputs[t+1];
+        if( topographic_lateral_weights.length() == 0 )
+            productTranspose(dampening_expectations, previous_temp_outputs, 
+                             lateral_weights);
+        else
+            for( int b = 0; b<dampening_expectations.length(); b++)
+                productTopoLateralWeights( dampening_expectations(b), 
+                                           previous_temp_outputs(b) );
+
+        dampening_expectations += bias_plus_inputs;
+        if (use_fast_approximations)
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = 
+                            fastsigmoid( dampening_expectations(k, i) );
+            }
+            else
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = (1-dampening_factor)
+                            * fastsigmoid( dampening_expectations(k, i) ) 
+                            + dampening_factor * previous_temp_outputs(k, i);
+            }
+        }
+        else
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = 
+                            sigmoid( dampening_expectations(k, i) );
+            }
+            else
+            {
+                for(int k = 0; k < batch_size; k++)
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_outputs(k, i) = (1-dampening_factor)
+                            * sigmoid( dampening_expectations(k, i) ) 
+                            + dampening_factor * previous_temp_outputs(k, i);
+            }
+        }
+    }
+}
+
+void RBMLateralBinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
+                              Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( rbm_bias.size() == input_size );
+    output.resize( output_size );
+
+    add(rbm_bias, input, bias_plus_input);
+
+        if( temp_output.length() != n_lateral_connections_passes+1 )
+    {
+        temp_output.resize(n_lateral_connections_passes+1);
+        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+            temp_output[i].resize(size);
+    }       
+
+    temp_output.last() = output;
+    current_temp_output = temp_output[0];
+
+    if (use_fast_approximations)
+        for( int i=0 ; i<size ; i++ )
+            current_temp_output[i] = fastsigmoid( bias_plus_input[i] );
+    else
+        for( int i=0 ; i<size ; i++ )
+            current_temp_output[i] = sigmoid( bias_plus_input[i] );
+
+    for( int t=0; t<n_lateral_connections_passes; t++ )
+    {
+        previous_temp_output = current_temp_output;
+        current_temp_output = temp_output[t+1];
+        if( topographic_lateral_weights.length() == 0 )
+            product(dampening_expectation, lateral_weights, previous_temp_output);
+        else
+            productTopoLateralWeights( dampening_expectation, previous_temp_output );
+        dampening_expectation += bias_plus_input;
+        if (use_fast_approximations)
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+            }
+            else
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = 
+                        (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                        + dampening_factor * previous_temp_output[i];
+            }
+        }
+        else
+        {
+            if( fast_exact_is_equal( dampening_factor, 0) )
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = sigmoid( dampening_expectation[i] );
+            }
+            else
+            {
+                for( int i=0 ; i<size ; i++ )
+                    current_temp_output[i] = 
+                        (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                        + dampening_factor * previous_temp_output[i];
+            }
+        }
+    }
+}
+
+// HUGO: NO 0.5! Computes mat[i][j] += 0.5 * (v1[i] * v2[j] + v1[j] * v2[i])
+// Computes mat[i][j] += (v1[i] * v2[j] + v1[j] * v2[i])
+void RBMLateralBinomialLayer::externalSymetricProductAcc(const Mat& mat, const Vec& v1, const Vec& v2)
+{
+#ifdef BOUNDCHECK
+    if (v1.length()!=mat.length() || mat.width()!=v2.length() 
+        || v1.length() != v2.length())
+        PLERROR("externalSymetricProductAcc(Mat,Vec,Vec), incompatible "
+                "arguments sizes");
+#endif
+
+    real* v_1=v1.data();
+    real* v_2=v2.data();
+    real* mp = mat.data();
+    int l = mat.length();
+    int w = mat.width();
+
+    if(mat.isCompact())
+    {
+        real* pv11 = v_1;
+        real* pv21 = v_2;
+        for(int i=0; i<l; i++)
+        {
+            real* pv22 = v_2;
+            real* pv12 = v_1;
+            real val1 = *pv11++;
+            real val2 = *pv21++;
+            for(int j=0; j<w; j++)
+                //*mp++ += 0.5 * (val1 * *pv22++ + val2 * *pv12++) ;
+                *mp++ += (val1 * *pv22++ + val2 * *pv12++) ;
+        }
+    }
+    else
+    {
+        cerr << "!";
+        for (int i=0;i<l;i++)
+        {
+            real* mi = mat[i];
+            real v1i = v_1[i];
+            real v2i = v_2[i];
+            for (int j=0;j<w;j++)
+                //mi[j] += 0.5 * ( v1i * v_2[j] + v2i * v_1[j]);
+                mi[j] += ( v1i * v_2[j] + v2i * v_1[j]);
+        }
+    }
+}
+
+void RBMLateralBinomialLayer::productTopoLateralWeights(const Vec& result, 
+                                                        const Vec& input ) const
+{
+    // Could be made faster, in terms of memory access
+    result.clear();
+    int connected_neuron;
+    int wi;
+    real* current_weights;
+    int neuron_v, neuron_h;
+    int vmin, vmax, hmin, hmax;
+    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+    {
+        neuron_v = i/topographic_width;
+        neuron_h = i%topographic_width;
+        wi = 0;
+        current_weights = topographic_lateral_weights[i].data();
+        
+        vmin = neuron_v < topographic_patch_vradius ? 
+            - neuron_v : - topographic_patch_vradius;
+        vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ? 
+            topographic_length - neuron_v - 1: topographic_patch_vradius;
+
+        hmin = neuron_h < topographic_patch_hradius ? 
+            - neuron_h : - topographic_patch_hradius;
+        hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ? 
+            topographic_width - neuron_h - 1: topographic_patch_hradius;
+
+        for( int j = -1 * topographic_patch_vradius; 
+             j <= topographic_patch_vradius ; j++ ) 
+        {
+            for( int k = -1 * topographic_patch_hradius; 
+                 k <= topographic_patch_hradius; k++ )
+            {
+                connected_neuron = (i+j*topographic_width)+k;
+                if( connected_neuron != i )
+                {
+                    if( j >= vmin && j <= vmax &&
+                        k >= hmin && k <= hmax )
+                        result[i] += input[connected_neuron]
+                            * current_weights[wi];
+                    wi++;
+                }
+            }
+        }
+    }
+}
+
+void RBMLateralBinomialLayer::productTopoLateralWeightsGradients(
+    const Vec& input,
+    const Vec& input_gradient,
+    const Vec& result_gradient,
+    const TVec< Vec >& weights_gradient
+    )
+{
+    // Could be made faster, in terms of memory access
+    int connected_neuron;
+    int wi;
+    real* current_weights;
+    real* current_weights_gradient;
+    int neuron_v, neuron_h;
+    int vmin, vmax, hmin, hmax;
+    real result_gradient_i;
+    real input_i;
+    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+    {
+        neuron_v = i/topographic_width;
+        neuron_h = i%topographic_width;
+        wi = 0;
+        current_weights = topographic_lateral_weights[i].data();
+        current_weights_gradient = weights_gradient[i].data();
+
+        vmin = neuron_v < topographic_patch_vradius ? 
+            - neuron_v : - topographic_patch_vradius;
+        vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ? 
+            topographic_length - neuron_v - 1: topographic_patch_vradius;
+
+        hmin = neuron_h < topographic_patch_hradius ? 
+            - neuron_h : - topographic_patch_hradius;
+        hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ? 
+            topographic_width - neuron_h - 1: topographic_patch_hradius;
+
+        result_gradient_i = result_gradient[i];
+        input_i = input[i];
+
+        for( int j = -1 * topographic_patch_vradius; 
+             j <= topographic_patch_vradius ; j++ )
+        {
+            for( int k = -1 * topographic_patch_hradius; 
+                 k <= topographic_patch_hradius; k++ )
+            {
+                connected_neuron = (i+j*topographic_width)+k;
+                if( connected_neuron != i )
+                {
+                    if( j >= vmin && j <= vmax &&
+                        k >= hmin && k <= hmax )
+                    {
+                        input_gradient[connected_neuron] += 
+                            result_gradient_i * current_weights[wi];
+                        current_weights_gradient[wi] += 
+                            //0.5 * ( result_gradient_i * input[connected_neuron] +
+                            ( result_gradient_i * input[connected_neuron] +
+                              input_i * result_gradient[connected_neuron] );
+                    }
+                    wi++;
+                }
+            }
+        }
+    }
+}
+
+void RBMLateralBinomialLayer::updateTopoLateralWeightsCD(
+    const Vec& pos_values,
+    const Vec& neg_values  )
+{
+    if( !do_not_learn_topographic_lateral_weights )
+    {
+        
+        // Could be made faster, in terms of memory access
+        int connected_neuron;
+        int wi;
+        int neuron_v, neuron_h;
+        int vmin, vmax, hmin, hmax;
+        real* current_weights;
+        real pos_values_i;
+        real neg_values_i;
+        for( int i=0; i<topographic_lateral_weights.length(); i++ )
+        {
+            neuron_v = i/topographic_width;
+            neuron_h = i%topographic_width;
+            wi = 0;
+            
+            vmin = neuron_v < topographic_patch_vradius ? 
+                - neuron_v : - topographic_patch_vradius;
+            vmax = topographic_length - neuron_v - 1 < topographic_patch_vradius ? 
+                topographic_length - neuron_v - 1: topographic_patch_vradius;
+            
+            hmin = neuron_h < topographic_patch_hradius ? 
+                - neuron_h : - topographic_patch_hradius;
+            hmax = topographic_width - neuron_h - 1 < topographic_patch_hradius ? 
+                topographic_width - neuron_h - 1: topographic_patch_hradius;
+            
+            current_weights = topographic_lateral_weights[i].data();
+            pos_values_i = pos_values[i];
+            neg_values_i = neg_values[i];
+            
+            for( int j = - topographic_patch_vradius; 
+                 j <= topographic_patch_vradius ; j++ )
+            {
+                for( int k = -topographic_patch_hradius; 
+                     k <= topographic_patch_hradius; k++ )
+                {
+                    connected_neuron = (i+j*topographic_width)+k;
+                    if( connected_neuron != i )
+                    {
+                        if( j >= vmin && j <= vmax &&
+                            k >= hmin && k <= hmax )
+                        {
+                            current_weights[wi] += 
+                                //learning_rate * 0.5 * ( 
+                                learning_rate * ( 
+                                    pos_values_i * pos_values[connected_neuron] -
+                                    neg_values_i * neg_values[connected_neuron] );
+                        }
+                        wi++;
+                    }
+                }
+            }
+        }
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMLateralBinomialLayer::bpropUpdate(const Vec& input, const Vec& output,
+                                   Vec& input_gradient,
+                                   const Vec& output_gradient,
+                                   bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    //if( momentum != 0. )
+    //    bias_inc.resize( size );
+
+    temp_input_gradient.clear();
+    temp_mean_field_gradient << output_gradient;
+    current_temp_output = output;
+    lateral_weights_gradient.clear();
+    for( int i=0; i<topographic_lateral_weights_gradient.length(); i++)
+        topographic_lateral_weights_gradient[i].clear();
+
+    real output_i;
+    for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+    {
+        for( int i=0 ; i<size ; i++ )
+        {
+            output_i = current_temp_output[i];
+
+            // Contribution from the mean field approximation
+            temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                output_i * (1-output_i) * temp_mean_field_gradient[i];
+            
+            // Contribution from the dampening
+            temp_mean_field_gradient[i] *= dampening_factor;
+        }
+
+        // Input gradient contribution
+        temp_input_gradient += temp_mean_field_gradient2;
+
+        // Lateral weights gradient contribution
+        if( topographic_lateral_weights.length() == 0)
+        {
+            externalSymetricProductAcc( lateral_weights_gradient, 
+                                        temp_mean_field_gradient2,
+                                        temp_output[t] );
+            
+            transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                temp_mean_field_gradient2);
+        }
+        else
+        {
+            productTopoLateralWeightsGradients( 
+                temp_output[t],
+                temp_mean_field_gradient,
+                temp_mean_field_gradient2,
+                topographic_lateral_weights_gradient);
+        }
+
+        current_temp_output = temp_output[t];
+    }
+    
+    for( int i=0 ; i<size ; i++ )
+    {
+        output_i = current_temp_output[i];
+        temp_mean_field_gradient[i] *= output_i * (1-output_i);
+    }
+
+    temp_input_gradient += temp_mean_field_gradient;
+
+    input_gradient += temp_input_gradient;
+
+    // Update bias
+    real in_grad_i;
+    for( int i=0 ; i<size ; i++ )
+    {
+        in_grad_i = temp_input_gradient[i];
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
+    }
+
+    if( topographic_lateral_weights.length() == 0)
+    {
+        if( momentum == 0. )
+        {
+            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                               lateral_weights);
+        }
+        else
+        {
+            multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
+                               lateral_weights_inc);
+            lateral_weights += lateral_weights_inc;
+        }
+    }
+    else
+    {
+        if( !do_not_learn_topographic_lateral_weights )
+        {
+            if( momentum == 0. )
+                for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                       -learning_rate,
+                                       topographic_lateral_weights[i]);
+            
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                        "topographic weights");
+        }
+    }
+
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();        
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+}
+
+void RBMLateralBinomialLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                                   Mat& input_gradients,
+                                   const Mat& output_gradients,
+                                   bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    int mbatch_size = inputs.length();
+    PLASSERT( outputs.length() == mbatch_size );
+    PLASSERT( output_gradients.length() == mbatch_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &&
+                input_gradients.length() == mbatch_size,
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(mbatch_size, size);
+        input_gradients.clear();
+    }
+
+    //if( momentum != 0. )
+    //    bias_inc.resize( size );
+
+    // TODO Can we do this more efficiently? (using BLAS)
+
+    // We use the average gradient over the mini-batch.
+    real avg_lr = learning_rate / inputs.length();
+    lateral_weights_gradient.clear();
+    real output_i;
+    for (int j = 0; j < mbatch_size; j++)
+    {
+        temp_input_gradient.clear();
+        temp_mean_field_gradient << output_gradients(j);
+        current_temp_output = outputs(j);
+
+        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+        {
+
+            for( int i=0 ; i<size ; i++ )
+            {
+                output_i = current_temp_output[i];
+                
+                // Contribution from the mean field approximation
+                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                    output_i * (1-output_i) * temp_mean_field_gradient[i];
+                
+                // Contribution from the dampening
+                temp_mean_field_gradient[i] *= dampening_factor;
+            }
+            
+            // Input gradient contribution
+            temp_input_gradient += temp_mean_field_gradient2;
+            
+            // Lateral weights gradient contribution
+            if( topographic_lateral_weights.length() == 0)
+            {
+                
+                externalSymetricProductAcc( lateral_weights_gradient, 
+                                            temp_mean_field_gradient2,
+                                            temp_outputs[t](j) );
+                
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                    temp_mean_field_gradient2);
+            }
+            else
+            {
+                productTopoLateralWeightsGradients( 
+                    temp_outputs[t](j),
+                    temp_mean_field_gradient,
+                    temp_mean_field_gradient2,
+                    topographic_lateral_weights_gradient);
+            }
+
+            current_temp_output = temp_outputs[t](j);
+        }
+    
+        for( int i=0 ; i<size ; i++ )
+        {
+            output_i = current_temp_output[i];
+            temp_mean_field_gradient[i] *= output_i * (1-output_i);
+        }
+
+        temp_input_gradient += temp_mean_field_gradient;
+        
+        input_gradients(j) += temp_input_gradient;
+
+        // Update bias
+        real in_grad_i;
+        for( int i=0 ; i<size ; i++ )
+        {
+            in_grad_i = temp_input_gradient[i];
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= avg_lr * in_grad_i;
+            }
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                        "momentum with mini-batches");
+        }        
+    }
+
+    if( topographic_lateral_weights.length() == 0)
+    {
+        if( momentum == 0. )
+            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                               lateral_weights);
+        else
+            PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                    "momentum with mini-batches");
+    }
+    else
+    {
+        if( !do_not_learn_topographic_lateral_weights )
+        {
+            if( momentum == 0. )
+                for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                       -learning_rate,
+                                       topographic_lateral_weights[i]);
+            
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                        "topographic weights");
+        }
+
+    }
+
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+}
+
+
+//! TODO: add "accumulate" here
+void RBMLateralBinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
+                                   const Vec& output,
+                                   Vec& input_gradient, Vec& rbm_bias_gradient,
+                                   const Vec& output_gradient)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( rbm_bias.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+    input_gradient.resize( size );
+    rbm_bias_gradient.resize( size );
+
+    temp_input_gradient.clear();
+    temp_mean_field_gradient << output_gradient;
+    current_temp_output = output;
+    lateral_weights_gradient.clear();
+
+    real output_i;
+    for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+    {
+
+        for( int i=0 ; i<size ; i++ )
+        {
+            output_i = current_temp_output[i];
+
+            // Contribution from the mean field approximation
+            temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                output_i * (1-output_i) * temp_mean_field_gradient[i];
+            
+            // Contribution from the dampening
+            temp_mean_field_gradient[i] *= dampening_factor;
+        }
+
+        // Input gradient contribution
+        temp_input_gradient += temp_mean_field_gradient2;
+
+        // Lateral weights gradient contribution
+        if( topographic_lateral_weights.length() == 0)
+        {
+
+            externalSymetricProductAcc( lateral_weights_gradient, 
+                                        temp_mean_field_gradient2,
+                                        temp_output[t] );
+            
+            transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                temp_mean_field_gradient2);
+        }
+        else
+        {
+            productTopoLateralWeightsGradients( 
+                temp_output[t],
+                temp_mean_field_gradient,
+                temp_mean_field_gradient2,
+                topographic_lateral_weights_gradient);
+        }
+
+        current_temp_output = temp_output[t];
+    }
+    
+    for( int i=0 ; i<size ; i++ )
+    {
+        output_i = current_temp_output[i];
+        temp_mean_field_gradient[i] *= output_i * (1-output_i);
+    }
+
+    temp_input_gradient += temp_mean_field_gradient;
+
+    input_gradient << temp_input_gradient;
+    rbm_bias_gradient << temp_input_gradient;
+
+    if( topographic_lateral_weights.length() == 0)
+    {
+        if( momentum == 0. )
+        {
+            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                               lateral_weights);
+        }
+        else
+        {
+            multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
+                               lateral_weights_inc);
+            lateral_weights += lateral_weights_inc;
+        }
+    }
+    else
+    {
+        if( !do_not_learn_topographic_lateral_weights )
+        {
+            if( momentum == 0. )
+                for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                       -learning_rate,
+                                       topographic_lateral_weights[i]);
+            
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                        "topographic weights");
+        }
+    }
+        
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+}
+
+real RBMLateralBinomialLayer::fpropNLL(const Vec& target)
+{
+    PLASSERT( target.size() == input_size );
+    computeExpectation(); 
+
+    real ret = 0;
+    real target_i, expectation_i;
+    for( int i=0 ; i<size ; i++ )
+    {
+        target_i = target[i];
+        expectation_i = expectation[i];
+        // TODO: implement more numerically stable version
+        if(!fast_exact_is_equal(target_i,0.0))
+            ret -= target_i*safeflog(expectation_i) ;
+        if(!fast_exact_is_equal(target_i,1.0))
+            ret -= (1-target_i)*safeflog(1-expectation_i);
+    }
+    return ret;
+}
+
+void RBMLateralBinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
+{
+    computeExpectations(); 
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    for (int k=0;k<batch_size;k++) // loop over minibatch
+    {
+        real nll = 0;
+        real* expectation = expectations[k];
+        real* target = targets[k];
+        for( int i=0 ; i<size ; i++ ) // loop over outputs
+        {
+            // TODO: implement more numerically stable version
+            if(!fast_exact_is_equal(target[i],0.0))
+                nll -= target[i]*safeflog(expectation[i]) ;
+            if(!fast_exact_is_equal(target[i],1.0))
+                nll -= (1-target[i])*safeflog(1-expectation[i]);
+        }
+        costs_column(k,0) = nll;
+    }
+}
+
+void RBMLateralBinomialLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)
+{
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+    bias_gradient.clear();
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, temp_mean_field_gradient);
+
+    current_temp_output = expectation;
+    lateral_weights_gradient.clear();
+
+    real output_i;
+    for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+    {
+        for( int i=0 ; i<size ; i++ )
+        {
+            output_i = current_temp_output[i];
+
+            // Contribution from the mean field approximation
+            temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                output_i * (1-output_i) * temp_mean_field_gradient[i];
+            
+            // Contribution from the dampening
+            temp_mean_field_gradient[i] *= dampening_factor;
+        }
+
+        // Input gradient contribution
+        bias_gradient += temp_mean_field_gradient2;
+
+        // Lateral weights gradient contribution
+        if( topographic_lateral_weights.length() == 0)
+        {
+            externalSymetricProductAcc( lateral_weights_gradient, 
+                                        temp_mean_field_gradient2,
+                                        temp_output[t] );
+            
+            transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                temp_mean_field_gradient2);
+        }
+        else
+        {
+            productTopoLateralWeightsGradients( 
+                temp_output[t],
+                temp_mean_field_gradient,
+                temp_mean_field_gradient2,
+                topographic_lateral_weights_gradient);
+        }
+
+        current_temp_output = temp_output[t];
+    }
+    
+    for( int i=0 ; i<size ; i++ )
+    {
+        output_i = current_temp_output[i];
+        temp_mean_field_gradient[i] *= output_i * (1-output_i);
+    }
+
+    bias_gradient += temp_mean_field_gradient;
+
+    if( topographic_lateral_weights.length() == 0)
+    {
+        // Update lateral connections
+        if( momentum == 0. )
+        {
+            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                               lateral_weights);
+        }
+        else
+        {
+            multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
+                               lateral_weights_inc);
+            lateral_weights += lateral_weights_inc;
+        }
+    }
+    else
+    {
+        if( !do_not_learn_topographic_lateral_weights )
+        {
+            if( momentum == 0. )
+                for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                       -learning_rate,
+                                       topographic_lateral_weights[i]);
+            
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
+                        "topographic weights");
+        }
+    }
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+}
+
+void RBMLateralBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
+                                Mat& bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+    bias_gradients.clear();
+
+
+    // TODO Can we do this more efficiently? (using BLAS)
+
+    // We use the average gradient over the mini-batch.
+    lateral_weights_gradient.clear();
+    real output_i;
+    for (int j = 0; j < batch_size; j++)
+    {
+        // top_gradient = expectations(j) - targets(j)
+        substract(expectations(j), targets(j), temp_mean_field_gradient);
+        current_temp_output = expectations(j);
+
+        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+        {
+            for( int i=0 ; i<size ; i++ )
+            {
+                output_i = current_temp_output[i];
+                
+                // Contribution from the mean field approximation
+                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                    output_i * (1-output_i) * temp_mean_field_gradient[i];
+                
+                // Contribution from the dampening
+                temp_mean_field_gradient[i] *= dampening_factor;
+            }
+            
+            // Input gradient contribution
+            bias_gradients(j) += temp_mean_field_gradient2;
+            
+            // Lateral weights gradient contribution
+            if( topographic_lateral_weights.length() == 0)
+            {
+
+                externalSymetricProductAcc( lateral_weights_gradient, 
+                                            temp_mean_field_gradient2,
+                                            temp_outputs[t](j) );
+                
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                    temp_mean_field_gradient2);
+            }
+            else
+            {
+                productTopoLateralWeightsGradients( 
+                    temp_outputs[t](j),
+                    temp_mean_field_gradient,
+                    temp_mean_field_gradient2,
+                    topographic_lateral_weights_gradient);
+            }
+            current_temp_output = temp_outputs[t](j);
+        }
+    
+        for( int i=0 ; i<size ; i++ )
+        {
+            output_i = current_temp_output[i];
+            temp_mean_field_gradient[i] *= output_i * (1-output_i);
+        }
+
+        bias_gradients(j) += temp_mean_field_gradient;
+    }
+
+    // Update lateral connections
+    if( topographic_lateral_weights.length() == 0 )
+    {
+        if( momentum == 0. )
+            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                               lateral_weights);
+        else
+            PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                    "momentum with mini-batches");
+    }
+    else
+    {
+        if( !do_not_learn_topographic_lateral_weights )
+        {
+            if( momentum == 0. )
+                for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                       -learning_rate,
+                                       topographic_lateral_weights[i]);
+            
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
+                        "topographic weights");
+        }
+    }
+
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+}
+
+void RBMLateralBinomialLayer::accumulatePosStats( const Vec& pos_values )
+{
+    inherited::accumulatePosStats( pos_values);
+    externalProductAcc(lateral_weights_pos_stats, pos_values, pos_values);
+}
+
+void RBMLateralBinomialLayer::accumulatePosStats( const Mat& pos_values )
+{
+    inherited::accumulatePosStats( pos_values);
+    transposeProductAcc(lateral_weights_pos_stats, pos_values, pos_values);
+}
+
+void RBMLateralBinomialLayer::accumulateNegStats( const Vec& neg_values )
+{
+    inherited::accumulateNegStats( neg_values);
+    externalProductAcc(lateral_weights_neg_stats, neg_values, neg_values);
+}
+
+void RBMLateralBinomialLayer::accumulateNegStats( const Mat& neg_values )
+{
+    inherited::accumulateNegStats( neg_values);
+    transposeProductAcc(lateral_weights_neg_stats, neg_values, neg_values);
+}
+
+
+void RBMLateralBinomialLayer::update()
+{ 
+    //real pos_factor = 0.5 * learning_rate / pos_count;
+    //real neg_factor = - 0.5 * learning_rate / neg_count;
+    real pos_factor = learning_rate / pos_count;
+    real neg_factor = - learning_rate / neg_count;
+
+    if( topographic_lateral_weights.length() != 0 )
+        PLERROR("In RBMLateralBinomialLayer:update - Not implemented for "
+                "topographic weights");
+
+    // Update lateral connections
+    if( momentum == 0. )
+    {
+        multiplyScaledAdd( lateral_weights_pos_stats, neg_factor, pos_factor,
+                           lateral_weights_neg_stats);
+        lateral_weights += lateral_weights_neg_stats; 
+    }
+    else
+    {
+        multiplyScaledAdd( lateral_weights_pos_stats, neg_factor, pos_factor,
+                           lateral_weights_neg_stats);
+        multiplyScaledAdd( lateral_weights_neg_stats, momentum, 1.0,
+                           lateral_weights_inc);
+        lateral_weights += lateral_weights_inc;
+    }
+
+    // Set diagonal to 0
+    if( lateral_weights.length() != 0 )
+    {
+        real *d = lateral_weights.data();
+        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+            *d = 0;
+    }
+
+    // Call to update() must be at the end, since update() calls clearStats()!
+    inherited::update();
+}
+
+void RBMLateralBinomialLayer::update( const Vec& grad)
+{
+    inherited::update( grad );
+    PLWARNING("RBMLateralBinomialLayer::update( grad ): does not update the\n"
+        "lateral connections.");
+}
+
+void RBMLateralBinomialLayer::update( const Vec& pos_values, const Vec& neg_values )
+{
+    // Update lateral connections
+    if( topographic_lateral_weights.length() == 0 )
+    {
+        if( momentum == 0. )
+        {
+            externalProductScaleAcc(lateral_weights, pos_values, pos_values,
+                                    //0.5 * learning_rate);
+                                    learning_rate);
+            externalProductScaleAcc(lateral_weights, neg_values, neg_values,
+                                    //- 0.5 * learning_rate);
+                                    -learning_rate);
+        }
+        else
+        {
+            lateral_weights_inc *= momentum;
+            externalProductScaleAcc(lateral_weights_inc, pos_values, pos_values,
+                                    //0.5 * learning_rate);
+                                    learning_rate);
+            externalProductScaleAcc(lateral_weights_inc, neg_values, neg_values,
+                                    //- 0.5 * learning_rate);
+                                    - learning_rate);
+            lateral_weights += lateral_weights_inc;
+        }    
+
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
+        {
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
+        }
+    }
+    else
+    {
+        if( momentum == 0. )
+            updateTopoLateralWeightsCD(pos_values, neg_values);
+        else
+            PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
+                    "topographic weights");
+    }
+
+    inherited::update( pos_values, neg_values );
+}
+
+void RBMLateralBinomialLayer::update( const Mat& pos_values, const Mat& neg_values )
+{
+    int n = pos_values.length();
+    PLASSERT( neg_values.length() == n );
+
+    // We take the average gradient over the mini-batch.
+    //real avg_lr = 0.5 * learning_rate / n;
+    real avg_lr = learning_rate / n;
+
+    // Update lateral connections
+    if( topographic_lateral_weights.length() == 0 )
+    {
+        if( momentum == 0. )
+        {
+            transposeProductScaleAcc(lateral_weights, pos_values, pos_values,
+                                     avg_lr, 1);
+            transposeProductScaleAcc(lateral_weights, neg_values, neg_values,
+                                     -avg_lr, 1);
+        }
+        else
+        {
+            lateral_weights_inc *= momentum;
+            transposeProductScaleAcc(lateral_weights_inc, pos_values, pos_values,
+                                     avg_lr, 1);
+            transposeProductScaleAcc(lateral_weights_inc, neg_values, neg_values,
+                                     -avg_lr, 1);
+            lateral_weights += lateral_weights_inc;
+        }
+
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
+        {
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
+        }
+    }
+    else
+    {
+        if( momentum == 0. )
+        {
+            for(int b=0; b<pos_values.length(); b++)
+                updateTopoLateralWeightsCD(pos_values(b), neg_values(b));
+            
+        }
+        else
+            PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
+                    "topographic weights");
+    }
+
+    inherited::update( pos_values, neg_values );
+}
+
+void RBMLateralBinomialLayer::updateCDandGibbs( const Mat& pos_values,
+                                 const Mat& cd_neg_values,
+                                 const Mat& gibbs_neg_values,
+                                 real background_gibbs_update_ratio )
+{
+    inherited::updateCDandGibbs( pos_values, cd_neg_values,
+                                 gibbs_neg_values, background_gibbs_update_ratio );
+    PLERROR("In RBMLateralBinomialLayer::updateCDandGibbs(): not implemented yet.");
+}
+
+void RBMLateralBinomialLayer::updateGibbs( const Mat& pos_values,
+                                           const Mat& gibbs_neg_values)
+{
+    inherited::updateGibbs( pos_values, gibbs_neg_values );
+    PLERROR("In RBMLateralBinomialLayer::updateCDandGibbs(): not implemented yet.");
+}
+
+void RBMLateralBinomialLayer::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "n_lateral_connections_passes", 
+                  &RBMLateralBinomialLayer::n_lateral_connections_passes,
+                  OptionBase::buildoption,
+                  "Number of passes through the lateral connections.\n");
+
+    declareOption(ol, "dampening_factor", 
+                  &RBMLateralBinomialLayer::dampening_factor,
+                  OptionBase::buildoption,
+                  "Dampening factor ( expectation_t = (1-df) * currrent mean field"
+                  " + df * expectation_{t-1}).\n");
+
+    declareOption(ol, "mean_field_precision_threshold", 
+                  &RBMLateralBinomialLayer::mean_field_precision_threshold,
+                  OptionBase::buildoption,
+                  "Mean-field precision threshold that, once reached, stops the mean-field\n"
+                  "expectation approximation computation. Used only in computeExpectation().\n"
+                  "Precision is computed as:\n"
+                  "  dist(last_mean_field, current_mean_field) / size\n");
+
+    declareOption(ol, "topographic_length", 
+                  &RBMLateralBinomialLayer::topographic_length,
+                  OptionBase::buildoption,
+                  "Length of the topographic map.\n");
+
+    declareOption(ol, "topographic_width", 
+                  &RBMLateralBinomialLayer::topographic_width,
+                  OptionBase::buildoption,
+                  "Width of the topographic map.\n");
+
+    declareOption(ol, "topographic_patch_vradius", 
+                  &RBMLateralBinomialLayer::topographic_patch_vradius,
+                  OptionBase::buildoption,
+                  "Vertical radius of the topographic local weight patches.\n");
+
+    declareOption(ol, "topographic_patch_hradius", 
+                  &RBMLateralBinomialLayer::topographic_patch_hradius,
+                  OptionBase::buildoption,
+                  "Horizontal radius of the topographic local weight patches.\n");
+
+    declareOption(ol, "topographic_lateral_weights_init_value", 
+                  &RBMLateralBinomialLayer::topographic_lateral_weights_init_value,
+                  OptionBase::buildoption,
+                  "Initial value for the topographic_lateral_weights.\n");
+
+    declareOption(ol, "do_not_learn_topographic_lateral_weights", 
+                  &RBMLateralBinomialLayer::do_not_learn_topographic_lateral_weights,
+                  OptionBase::buildoption,
+                  "Indication that the topographic_lateral_weights should\n"
+                  "be fixed at their initial value.\n");
+
+    declareOption(ol, "lateral_weights", 
+                  &RBMLateralBinomialLayer::lateral_weights,
+                  OptionBase::learntoption,
+                  "Lateral connections.\n");
+
+    declareOption(ol, "topographic_lateral_weights", 
+                  &RBMLateralBinomialLayer::topographic_lateral_weights,
+                  OptionBase::learntoption,
+                  "Local topographic lateral connections.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMLateralBinomialLayer::build_()
+{
+    if( n_lateral_connections_passes == 0 &&
+        !fast_exact_is_equal(dampening_factor, 0) )
+        PLERROR("In RBMLateralBinomialLayer::build_(): when not using the lateral\n"
+                "connections, dampening_factor should be 0.");
+
+    if( dampening_factor < 0 || dampening_factor > 1)
+        PLERROR("In RBMLateralBinomialLayer::build_(): dampening_factor should be\n"
+                "in [0,1].");
+
+    if( n_lateral_connections_passes < 0 )
+        PLERROR("In RBMLateralBinomialLayer::build_(): n_lateral_connections_passes\n"
+                " should be >= 0.");
+    
+    if( topographic_length <= 0 || topographic_width <= 0)
+    {
+        lateral_weights.resize(size,size);
+
+        lateral_weights_gradient.resize(size,size);
+        lateral_weights_pos_stats.resize(size,size);
+        lateral_weights_neg_stats.resize(size,size);
+        if( momentum != 0. )
+        {
+            bias_inc.resize( size );
+            lateral_weights_inc.resize(size,size);
+        }   
+    }
+    else
+    {
+        if( size != topographic_length * topographic_width )
+            PLERROR( "In RBMLateralBinomialLayer::build_(): size != "
+                     "topographic_length * topographic_width.\n" );
+
+        if( topographic_length-1 <= 2*topographic_patch_vradius )
+            PLERROR( "In RBMLateralBinomialLayer::build_(): "
+                     "topographic_patch_vradius is too large.\n" );
+
+        if( topographic_width-1 <= 2*topographic_patch_hradius )
+            PLERROR( "In RBMLateralBinomialLayer::build_(): "
+                     "topographic_patch_hradius is too large.\n" );
+
+        topographic_lateral_weights.resize(size);
+        topographic_lateral_weights_gradient.resize(size);
+        for( int i=0; i<size; i++ )
+        {
+            topographic_lateral_weights[i].resize( 
+                ( 2 * topographic_patch_hradius + 1 ) *
+                ( 2 * topographic_patch_vradius + 1 ) - 1 );
+            topographic_lateral_weights_gradient[i].resize( 
+                ( 2 * topographic_patch_hradius + 1 ) *
+                ( 2 * topographic_patch_vradius + 1 ) - 1 );
+        }
+
+        // Should probably have separate lateral_weights_*_stats
+    }
+
+    // Resizing temporary variables
+    dampening_expectation.resize(size);
+    temp_input_gradient.resize(size);
+    temp_mean_field_gradient.resize(size);
+    temp_mean_field_gradient2.resize(size);
+}
+
+void RBMLateralBinomialLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMLateralBinomialLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(lateral_weights,copies);
+    deepCopyField(topographic_lateral_weights,copies);
+    deepCopyField(lateral_weights_pos_stats,copies);
+    deepCopyField(lateral_weights_neg_stats,copies);
+    deepCopyField(dampening_expectation,copies);
+    deepCopyField(dampening_expectations,copies);
+    deepCopyField(temp_output,copies);
+    deepCopyField(temp_outputs,copies);
+    deepCopyField(current_temp_output,copies);
+    deepCopyField(previous_temp_output,copies);
+    deepCopyField(current_temp_outputs,copies);
+    deepCopyField(previous_temp_outputs,copies);
+    deepCopyField(bias_plus_input,copies);
+    deepCopyField(bias_plus_inputs,copies);
+    deepCopyField(temp_input_gradient,copies);
+    deepCopyField(temp_mean_field_gradient,copies);
+    deepCopyField(temp_mean_field_gradient2,copies);
+    deepCopyField(lateral_weights_gradient,copies);
+    deepCopyField(lateral_weights_inc,copies);
+    deepCopyField(topographic_lateral_weights_gradient,copies);
+}
+
+real RBMLateralBinomialLayer::energy(const Vec& unit_values) const
+{
+    if( topographic_lateral_weights.length() == 0 )
+        product(dampening_expectation, lateral_weights, unit_values);
+    else
+        productTopoLateralWeights( dampening_expectation, unit_values );
+    return -dot(unit_values, bias) - 0.5 * dot(unit_values, dampening_expectation);
+}
+
+real RBMLateralBinomialLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    PLERROR(
+        "In RBMLateralBinomialLayer::freeEnergyContribution(): not implemented.");
+    return -1;
+}
+
+int RBMLateralBinomialLayer::getConfigurationCount()
+{
+    return size < 31 ? 1<<size : INFINITE_CONFIGURATIONS;
+}
+
+void RBMLateralBinomialLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
+
+    for ( int i = 0; i < size; ++i ) {
+        output[i] = conf_index & 1;
+        conf_index >>= 1;
+    }
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMLateralBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-02-01 04:46:56 UTC (rev 8442)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-02-01 15:59:56 UTC (rev 8443)
@@ -0,0 +1,306 @@
+// -*- C++ -*-
+
+// RBMLateralBinomialLayer.h
+//
+// Copyright (C) 2006 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMLateralBinomialLayer.h */
+
+
+#ifndef RBMLateralBinomialLayer_INC
+#define RBMLateralBinomialLayer_INC
+
+#include "RBMLayer.h"
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Layer in an RBM formed with binomial units, with lateral connections
+ *
+ */
+class RBMLateralBinomialLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Number of passes through the lateral connections 
+    int n_lateral_connections_passes;
+
+    //! Dampening factor 
+    //! ( expectation_t = (1-df) * currrent mean field + df * expectation_{t-1})
+    real dampening_factor;
+
+    //! Mean-field precision threshold that, once reached, stops the mean-field 
+    //! expectation approximation computation. Used only in computeExpectation(). 
+    //! Precision is computed as:
+    //!   dist(last_mean_field, current_mean_field) / size
+    real mean_field_precision_threshold;
+
+    //! Length of the topographic map
+    int topographic_length;
+
+    //! Width of the topographic map
+    int topographic_width;
+
+    //! Vertical radius of the topographic local weight patches
+    int topographic_patch_vradius;
+
+    //! Horizontal radius of the topographic local weight patches
+    int topographic_patch_hradius;
+
+    //! Initial value for the topographic_lateral_weights
+    real topographic_lateral_weights_init_value;
+
+    //! Indication that the topographic_lateral_weights should
+    //! be fixed at their initial value.
+    bool do_not_learn_topographic_lateral_weights;
+    
+    //! Lateral connections
+    Mat lateral_weights;
+
+    //! Local topographic lateral connections
+    TVec< Vec > topographic_lateral_weights;
+
+    //! Accumulates positive contribution to the gradient of lateral weights
+    Mat lateral_weights_pos_stats;
+
+    //! Accumulates negative contribution to the gradient of lateral weights
+    Mat lateral_weights_neg_stats;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMLateralBinomialLayer( real the_learning_rate=0. );
+
+    //! resets activations, sample and expectation fields
+    virtual void reset();
+
+    //! resets the statistics and counts
+    virtual void clearStats();
+
+    //! forgets everything
+    virtual void forget();
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample() ;
+
+    //! Inherited.
+    virtual void generateSamples();
+
+    //! Compute expectation.
+    virtual void computeExpectation() ;
+
+    //! Compute mini-batch expectations.
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec& input, Vec& output ) const;
+
+    //! Batch forward propagation
+    virtual void fprop( const Mat& inputs, Mat& outputs ) const;
+
+    //! forward propagation with provided bias
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
+                        Vec& output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! back-propagates the output gradient to the input and the bias
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
+                             const Vec& output,
+                             Vec& input_gradient, Vec& rbm_bias_gradient,
+                             const Vec& output_gradient) ;
+
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec& target);
+    virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations.
+    //! Will also update the lateral weight connections according
+    //! to their gradient. Assumes computeExpectation(s) or
+    //! fpropNLL was called before.
+    virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+    virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
+                          Mat& bias_gradients);
+
+    //! Accumulates positive phase statistics
+    virtual void accumulatePosStats( const Vec& pos_values );
+    virtual void accumulatePosStats( const Mat& ps_values);
+
+    //! Accumulates negative phase statistics
+    virtual void accumulateNegStats( const Vec& neg_values );
+    virtual void accumulateNegStats( const Mat& neg_values );
+
+    //! Update bias and lateral connections parameters 
+    //! according to accumulated statistics
+    virtual void update();
+
+    //! Updates ONLY the bias parameters according to the given gradient
+    virtual void update( const Vec& grad );
+
+    //! Update bias and lateral connections 
+    //! parameters according to one pair of vectors
+    virtual void update( const Vec& pos_values, const Vec& neg_values );
+
+    //! Update bias and lateral connections 
+    //! parameters according to one pair of matrices.
+    virtual void update( const Mat& pos_values, const Mat& neg_values );
+
+    // neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+    //              +(1-gibbs_chain_statistics_forgetting_factor)
+    //               * gibbs_neg_values
+    // delta w = -lrate * ( pos_values
+    //                  - ( background_gibbs_update_ratio*neg_stats
+    //                     +(1-background_gibbs_update_ratio)
+    //                      * cd_neg_values ) )
+    virtual void updateCDandGibbs( const Mat& pos_values,
+                                   const Mat& cd_neg_values,
+                                   const Mat& gibbs_neg_values,
+                                   real background_gibbs_update_ratio );
+
+    // neg_stats <-- gibbs_chain_statistics_forgetting_factor * neg_stats
+    //              +(1-gibbs_chain_statistics_forgetting_factor)
+    //               * \sum_i gibbs_neg_values_i / minibatch_size
+    // delta bias = -lrate * \sum_i (pos_values_i - neg_stats) / minibatch_size
+    virtual void updateGibbs( const Mat& pos_values,
+                              const Mat& gibbs_neg_values );
+
+    //! compute -bias' unit_values
+    virtual real energy(const Vec& unit_values) const;
+
+    //! This function is not implemented for this class (returns an error)
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMLateralBinomialLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    mutable Vec dampening_expectation;
+    mutable Mat dampening_expectations;
+
+    mutable TVec<Vec> temp_output;
+    mutable TVec<Mat> temp_outputs;
+
+    mutable Vec current_temp_output, previous_temp_output;
+    mutable Mat current_temp_outputs, previous_temp_outputs;
+
+    mutable Vec bias_plus_input;
+    mutable Mat bias_plus_inputs;
+
+    Vec temp_input_gradient;
+    Vec temp_mean_field_gradient;
+    Vec temp_mean_field_gradient2;
+
+    Mat lateral_weights_gradient;
+    Mat lateral_weights_inc;
+
+    TVec< Vec > topographic_lateral_weights_gradient;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Computes mat[i][j] += 0.5 * (v1[i] * v2[j] +  v1[j] * v2[i])
+    void externalSymetricProductAcc(const Mat& mat, const Vec& v1, 
+                                    const Vec& v2);
+
+    void productTopoLateralWeights( const Vec& result, const Vec& input ) const;
+
+    void productTopoLateralWeightsGradients( const Vec& input, const Vec& input_gradient,
+                                             const Vec& result_gradient, 
+                                             const TVec< Vec >& weights_gradient );
+
+    void updateTopoLateralWeightsCD( const Vec& pos_values, const Vec& neg_values );
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMLateralBinomialLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Fri Feb  1 17:01:07 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Feb 2008 17:01:07 +0100
Subject: [Plearn-commits] r8444 - trunk/plearn_learners/online
Message-ID: <200802011601.m11G17tf018092@sheep.berlios.de>

Author: larocheh
Date: 2008-02-01 17:01:07 +0100 (Fri, 01 Feb 2008)
New Revision: 8444

Added:
   trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc
   trunk/plearn_learners/online/RBMMultitaskClassificationModule.h
Log:
RBM module to compute an approximate mean-field posterior p(y|x) in an RBM.


Added: trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc	2008-02-01 15:59:56 UTC (rev 8443)
+++ trunk/plearn_learners/online/RBMMultitaskClassificationModule.cc	2008-02-01 16:01:07 UTC (rev 8444)
@@ -0,0 +1,426 @@
+// -*- C++ -*-
+
+// RBMMultitaskClassificationModule.cc
+//
+// Copyright (C) 2006 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file RBMMultitaskClassificationModule.cc */
+
+
+#define PL_LOG_MODULE_NAME "RBMMultitaskClassificationModule"
+
+#include "RBMMultitaskClassificationModule.h"
+#include <plearn/io/pl_log.h>
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMMultitaskClassificationModule,
+    "Computes a mean-field approximate of p(y|x), with y a binary vector.",
+    "This module contains, from bottom to top:\n"
+    "  - an RBMConnection - previous_to_last,\n"
+    "  - an RBMBinomialLayer - last_layer,\n"
+    "  - an RBMMatrixConnection (transposed) - last_to_target,\n"
+    "  - and an RBMBinomialLayer - target_layer.\n"
+    "The two RBMConnections are combined in joint_connection.\n");
+
+RBMMultitaskClassificationModule::RBMMultitaskClassificationModule():
+    n_mean_field_iterations( 1 ),
+    fprop_outputs_activation( false )
+{
+}
+
+void RBMMultitaskClassificationModule::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "previous_to_last",
+                  &RBMMultitaskClassificationModule::previous_to_last,
+                  OptionBase::buildoption,
+                  "Connection between the previous layer, and last_layer.\n");
+
+    declareOption(ol, "last_layer", &RBMMultitaskClassificationModule::last_layer,
+                  OptionBase::buildoption,
+                  "Top-level layer (the one in the middle if we unfold).\n");
+
+    declareOption(ol, "last_to_target",
+                  &RBMMultitaskClassificationModule::last_to_target,
+                  OptionBase::buildoption,
+                  "Connection between last_layer and target_layer.\n");
+
+    declareOption(ol, "target_layer", &RBMMultitaskClassificationModule::target_layer,
+                  OptionBase::buildoption,
+                  "Layer containing the one-hot vector containing the target\n"
+                  "(or its prediction).\n");
+
+    declareOption(ol, "joint_connection",
+                  &RBMMultitaskClassificationModule::joint_connection,
+                  OptionBase::learntoption,
+                  "Connection grouping previous_to_last and last_to_target.\n");
+
+    declareOption(ol, "n_mean_field_iterations",
+                  &RBMMultitaskClassificationModule::n_mean_field_iterations,
+                  OptionBase::buildoption,
+                  "Number of mean-field iterations.\n");
+
+    declareOption(ol, "fprop_outputs_activation",
+                  &RBMMultitaskClassificationModule::fprop_outputs_activation,
+                  OptionBase::buildoption,
+                  "Indication that fprop should output the value of the "
+                  "activation\n"
+                  "before the squashing function and the application of the bias,\n"
+                  "instead of the mean-field approximation.\n");
+
+    declareOption(ol, "last_size", &RBMMultitaskClassificationModule::last_size,
+                  OptionBase::learntoption,
+                  "Size of last_layer.\n");
+    /*
+    declareOption(ol, "", &RBMMultitaskClassificationModule::,
+                  OptionBase::buildoption,
+                  "");
+     */
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMMultitaskClassificationModule::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+
+    if( !previous_to_last || !last_layer || !last_to_target || !target_layer )
+    {
+        MODULE_LOG << "build_() aborted because layers and connections were"
+           " not set" << endl;
+        return;
+    }
+
+    //! Check (and set) sizes
+    input_size = previous_to_last->down_size;
+    last_size = last_layer->size;
+    output_size = target_layer->size;
+
+    PLASSERT( previous_to_last->up_size == last_size );
+    PLASSERT( last_to_target->up_size == last_size );
+    PLASSERT( last_to_target->down_size == output_size );
+
+    //! build joint_connection
+    if( !joint_connection )
+        joint_connection = new RBMMixedConnection();
+
+    joint_connection->sub_connections.resize(1,2);
+    joint_connection->sub_connections(0,0) = previous_to_last;
+    joint_connection->sub_connections(0,1) = last_to_target;
+    joint_connection->build();
+
+    if( n_mean_field_iterations > 0 )
+    {
+        mean_field_activations_target.resize( n_mean_field_iterations );
+        mean_field_approximations_target.resize( n_mean_field_iterations );
+        mean_field_activations_hidden.resize( n_mean_field_iterations );
+        mean_field_approximations_hidden.resize( n_mean_field_iterations );
+        for( int i=0; i<n_mean_field_iterations; i++ )
+        {
+            mean_field_activations_target[i].resize( output_size );
+            mean_field_approximations_target[i].resize( output_size );
+            mean_field_activations_hidden[i].resize( last_size );
+            mean_field_approximations_hidden[i].resize( last_size );
+        }
+        mean_field_activations_gradient_target.resize( output_size );
+        mean_field_approximations_gradient_target.resize( output_size );
+        mean_field_activations_gradient_hidden.resize( last_size );
+        mean_field_approximations_gradient_hidden.resize( last_size );
+    }
+    else
+        PLERROR("In RBMMultitaskClassificationModule::build_(): "
+                "n_mean_field_iterations should be > 0\n");
+
+    last_to_target_gradient.resize( last_to_target->up_size, 
+                                    last_to_target->down_size );
+
+    // If we have a random_gen, share it with the ones who do not
+    if( random_gen )
+    {
+        if( !(previous_to_last->random_gen) )
+        {
+            previous_to_last->random_gen = random_gen;
+            previous_to_last->forget();
+        }
+        if( !(last_layer->random_gen) )
+        {
+            last_layer->random_gen = random_gen;
+            last_layer->forget();
+        }
+        if( !(last_to_target->random_gen) )
+        {
+            last_to_target->random_gen = random_gen;
+            last_to_target->forget();
+        }
+        if( !(target_layer->random_gen) )
+        {
+            target_layer->random_gen = random_gen;
+            target_layer->forget();
+        }
+        if( !(joint_connection->random_gen) )
+        {
+            joint_connection->random_gen = random_gen;
+            joint_connection->forget();
+        }
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void RBMMultitaskClassificationModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMMultitaskClassificationModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(previous_to_last, copies);
+    deepCopyField(last_layer, copies);
+    deepCopyField(last_to_target, copies);
+    deepCopyField(target_layer, copies);
+    deepCopyField(joint_connection, copies);
+    deepCopyField(mean_field_activations_target, copies);
+    deepCopyField(mean_field_approximations_target, copies);
+    deepCopyField(mean_field_activations_hidden, copies);
+    deepCopyField(mean_field_approximations_hidden, copies);
+    deepCopyField(last_to_target_gradient, copies);
+    deepCopyField(mean_field_activations_gradient_target, copies);
+    deepCopyField(mean_field_approximations_gradient_target, copies);
+    deepCopyField(mean_field_activations_gradient_hidden, copies);
+    deepCopyField(mean_field_approximations_gradient_hidden, copies);
+}
+
+void RBMMultitaskClassificationModule::fprop(const Vec& input, Vec& output) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+
+    previous_to_last->fprop( input, mean_field_activations_hidden[0] );
+    last_layer->fprop( mean_field_activations_hidden[0], 
+                       mean_field_approximations_hidden[0] );
+
+    Mat weights = last_to_target->weights;
+    for( int t=0; t<n_mean_field_iterations; t++ )
+    {
+        transposeProduct( mean_field_activations_target[t], weights, 
+                          mean_field_approximations_hidden[t] );
+        target_layer->fprop( mean_field_activations_target[t],
+                             mean_field_approximations_target[t] );
+        
+        if( t != n_mean_field_iterations -1 )
+        {
+            product( mean_field_activations_hidden[t+1], weights, 
+                     mean_field_approximations_target[t] );
+            mean_field_activations_hidden[t+1] += mean_field_activations_hidden[0];
+            last_layer->fprop( mean_field_activations_hidden[t+1],
+                               mean_field_approximations_hidden[t+1] );
+        }
+    }
+    
+    if( fprop_outputs_activation )
+    {
+        output << mean_field_activations_target.last();
+        //output += target_layer->bias;
+    }
+    else
+        output << mean_field_approximations_target.last();
+}
+
+/* THIS METHOD IS OPTIONAL
+//! Adapt based on the output gradient: this method should only
+//! be called just after a corresponding fprop; it should be
+//! called with the same arguments as fprop for the first two arguments
+//! (and output should not have been modified since then).
+//! Since sub-classes are supposed to learn ONLINE, the object
+//! is 'ready-to-be-used' just after any bpropUpdate.
+//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+//! JUST CALLS
+//!     bpropUpdate(input, output, input_gradient, output_gradient)
+//! AND IGNORES INPUT GRADIENT.
+void RBMMultitaskClassificationModule::bpropUpdate(const Vec& input, const Vec& output,
+                               const Vec& output_gradient)
+{
+}
+*/
+
+//! this version allows to obtain the input gradient as well
+void RBMMultitaskClassificationModule::bpropUpdate(const Vec& input, const Vec& output,
+                                          Vec& input_gradient,
+                                          const Vec& output_gradient,
+                                          bool accumulate)
+{
+    // size checks
+    PLASSERT( input.size() == input_size );
+    PLASSERT( output.size() == output_size );
+    PLASSERT( output_gradient.size() == output_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == input_size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+
+    last_to_target_gradient.clear();
+    Mat weights = last_to_target->weights;
+    if( fprop_outputs_activation )
+        mean_field_activations_gradient_target << output_gradient;
+    else
+        mean_field_approximations_gradient_target << output_gradient;
+
+    for( int t=n_mean_field_iterations-1; t>=0; t-- )
+    {
+        if( t != n_mean_field_iterations-1 || !fprop_outputs_activation )
+            target_layer->bpropUpdate( mean_field_activations_target[t],
+                                       mean_field_approximations_target[t],
+                                       mean_field_activations_gradient_target,
+                                       mean_field_approximations_gradient_target
+                );
+
+        externalProductAcc( last_to_target_gradient,
+                            mean_field_approximations_hidden[t],
+                            mean_field_activations_gradient_target);
+
+        product( mean_field_approximations_gradient_hidden, weights, 
+                          mean_field_activations_gradient_target);
+        
+        if( t != 0 )
+        {
+            last_layer->bpropUpdate( mean_field_activations_hidden[t],
+                                       mean_field_approximations_hidden[t],
+                                       mean_field_activations_gradient_hidden,
+                                       mean_field_approximations_gradient_hidden
+                );
+
+            externalProductAcc( last_to_target_gradient,
+                                mean_field_activations_gradient_hidden,
+                                mean_field_approximations_target[t-1]
+                                );
+
+            transposeProduct( mean_field_approximations_gradient_target, weights, 
+                              mean_field_activations_gradient_hidden);
+        }
+    }
+
+    last_layer->bpropUpdate( mean_field_activations_hidden[0],
+                             mean_field_approximations_hidden[0],
+                             mean_field_activations_gradient_hidden,
+                             mean_field_approximations_gradient_hidden
+        );
+
+    previous_to_last->bpropUpdate( input, mean_field_activations_hidden[0],
+                                   input_gradient, 
+                                   mean_field_activations_gradient_hidden,
+                                   accumulate);
+
+    multiplyAcc( weights, last_to_target_gradient, 
+                 - (last_to_target->learning_rate) );
+}
+
+//! reset the parameters to the state they would be BEFORE starting training.
+//! Note that this method is necessarily called from build().
+void RBMMultitaskClassificationModule::forget()
+{
+    if( !random_gen )
+    {
+        PLWARNING("RBMMultitaskClassificationModule: cannot forget() without"
+                  " random_gen");
+        return;
+    }
+
+    if( !(previous_to_last->random_gen) )
+        previous_to_last->random_gen = random_gen;
+    previous_to_last->forget();
+    if( !(last_to_target->random_gen) )
+        last_to_target->random_gen = random_gen;
+    last_to_target->forget();
+    if( !(joint_connection->random_gen) )
+        joint_connection->random_gen = random_gen;
+    joint_connection->forget();
+}
+
+/* THIS METHOD IS OPTIONAL
+//! Similar to bpropUpdate, but adapt based also on the estimation
+//! of the diagonal of the Hessian matrix, and propagates this
+//! back. If these methods are defined, you can use them INSTEAD of
+//! bpropUpdate(...)
+//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+//! JUST CALLS
+//!     bbpropUpdate(input, output, input_gradient, output_gradient,
+//!                  in_hess, out_hess)
+//! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+void RBMMultitaskClassificationModule::bbpropUpdate(const Vec& input, const Vec& output,
+                                           const Vec& output_gradient,
+                                           const Vec& output_diag_hessian)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+//! Similar to bpropUpdate, but adapt based also on the estimation
+//! of the diagonal of the Hessian matrix, and propagates this
+//! back. If these methods are defined, you can use them INSTEAD of
+//! bpropUpdate(...)
+//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+//! RAISES A PLERROR.
+void RBMMultitaskClassificationModule::bbpropUpdate(const Vec& input, const Vec& output,
+                                           Vec& input_gradient,
+                                           const Vec& output_gradient,
+                                           Vec& input_diag_hessian,
+                                           const Vec& output_diag_hessian,
+                                           bool accumulate)
+{
+}
+*/
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMMultitaskClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMMultitaskClassificationModule.h	2008-02-01 15:59:56 UTC (rev 8443)
+++ trunk/plearn_learners/online/RBMMultitaskClassificationModule.h	2008-02-01 16:01:07 UTC (rev 8444)
@@ -0,0 +1,218 @@
+// -*- C++ -*-
+
+// RBMMultitaskClassificationModule.h
+//
+// Copyright (C) 2006 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file RBMMultitaskClassificationModule.h */
+
+
+#ifndef RBMMultitaskClassificationModule_INC
+#define RBMMultitaskClassificationModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include "RBMConnection.h"
+#include "RBMMatrixConnection.h"
+#include "RBMMixedConnection.h"
+#include "RBMLayer.h"
+#include "RBMBinomialLayer.h"
+#include "RBMMultinomialLayer.h"
+
+namespace PLearn {
+
+/**
+ * Computes a mean-field approximate of p(y|x), with y a binary vector.
+ * This module contains an RBMConnection, an RBMBinomialLayer (hidden),
+ * an RBMMatrixConnection (transposed) and an RBMBinomialLayer (target).
+ * The two RBMConnections are combined in joint_connection.
+ */
+class RBMMultitaskClassificationModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+    //! Connection between the previous layer, and last_layer
+    PP<RBMConnection> previous_to_last;
+
+    //! Top-level layer (the one in the middle if we unfold)
+    PP<RBMBinomialLayer> last_layer;
+
+    //! Connection between last_layer and target_layer
+    PP<RBMMatrixConnection> last_to_target;
+
+    //! Layer containing the one-hot vector containing the target (or its
+    //! prediction)
+    PP<RBMBinomialLayer> target_layer;
+
+    //! Number of mean-field iterations
+    int n_mean_field_iterations;
+
+    //! Indication that fprop should output the value of the activation
+    //! before the squashing function and the application of the bias, 
+    //! instead of the mean-field approximation.
+    bool fprop_outputs_activation;
+
+    //#####  Public Learnt Options  ###########################################
+    //! Connection grouping previous_to_last and last_to_target
+    PP<RBMMixedConnection> joint_connection;
+
+    //! Size of last_layer
+    int last_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    RBMMultitaskClassificationModule();
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it appropriately)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop; it should be
+    //! called with the same arguments as fprop for the first two arguments
+    //! (and output should not have been modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! JUST CALLS
+    //!     bpropUpdate(input, output, input_gradient, output_gradient)
+    //! AND IGNORES INPUT GRADIENT.
+    // virtual void bpropUpdate(const Vec& input, const Vec& output,
+    //                          const Vec& output_gradient);
+
+    //! this version allows to obtain the input gradient as well
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+    //! WHICH JUST CALLS
+    //!     bbpropUpdate(input, output, input_gradient, output_gradient,
+    //!                  out_hess, in_hess)
+    //! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    // virtual void bbpropUpdate(const Vec& input, const Vec& output,
+    //                           const Vec& output_gradient,
+    //                           const Vec& output_diag_hessian);
+
+    //! this version allows to obtain the input gradient and diag_hessian
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! RAISES A PLERROR.
+    // virtual void bbpropUpdate(const Vec& input, const Vec& output,
+    //                           Vec& input_gradient,
+    //                           const Vec& output_gradient,
+    //                           Vec& input_diag_hessian,
+    //                           const Vec& output_diag_hessian
+    //                           bool accumulate=false);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(RBMMultitaskClassificationModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    mutable TVec< Vec > mean_field_activations_target;
+    mutable TVec< Vec > mean_field_approximations_target;
+    mutable TVec< Vec > mean_field_activations_hidden;
+    mutable TVec< Vec > mean_field_approximations_hidden;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    //! Stores the gradient of the weights between the target and the hidden layer
+    mutable Mat last_to_target_gradient;
+
+    //! Mean gradient propagation
+    mutable Vec mean_field_activations_gradient_target;
+    mutable Vec mean_field_approximations_gradient_target;
+    mutable Vec mean_field_activations_gradient_hidden;
+    mutable Vec mean_field_approximations_gradient_hidden;
+    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMMultitaskClassificationModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Fri Feb  1 17:46:16 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 1 Feb 2008 17:46:16 +0100
Subject: [Plearn-commits] r8445 - trunk/plearn/math
Message-ID: <200802011646.m11GkGK7025106@sheep.berlios.de>

Author: tihocan
Date: 2008-02-01 17:46:16 +0100 (Fri, 01 Feb 2008)
New Revision: 8445

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Added forget as a remote method

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2008-02-01 16:01:07 UTC (rev 8444)
+++ trunk/plearn/math/VecStatsCollector.cc	2008-02-01 16:46:16 UTC (rev 8445)
@@ -196,6 +196,10 @@
     // different than for declareOptions()
     rmm.inherited(inherited::_getRemoteMethodMap_());
 
+   declareMethod(
+        rmm, "forget", &VecStatsCollector::forget,
+        (BodyDoc("Clear all previously accumulated statistics.\n")));
+
     declareMethod(
         rmm, "getStat", &VecStatsCollector::getStat,
         (BodyDoc("Returns a particular statistic of a particular cost.\n"),



From larocheh at mail.berlios.de  Fri Feb  1 22:39:39 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 1 Feb 2008 22:39:39 +0100
Subject: [Plearn-commits] r8446 - trunk/plearn_learners_experimental
Message-ID: <200802012139.m11LddKg011472@sheep.berlios.de>

Author: larocheh
Date: 2008-02-01 22:39:39 +0100 (Fri, 01 Feb 2008)
New Revision: 8446

Modified:
   trunk/plearn_learners_experimental/DiscriminativeRBM.cc
   trunk/plearn_learners_experimental/DiscriminativeRBM.h
Log:
Added an option that can speed up generative learning


Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-02-01 16:46:16 UTC (rev 8445)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.cc	2008-02-01 21:39:39 UTC (rev 8446)
@@ -70,8 +70,8 @@
     do_not_use_discriminative_learning( false ),
     unlabeled_class_index_begin( 0 ),
     n_classes_at_test_time( -1 ),
-    n_mean_field_iterations( 1 )
-
+    n_mean_field_iterations( 1 ),
+    gen_learning_every_n_samples( 1 )
 {
     random_gen = new PRandom();
 }
@@ -162,6 +162,14 @@
                   "Number of mean field iterations for the approximate computation of p(y|x)\n"
                   "for multitask learning.\n");
 
+    declareOption(ol, "gen_learning_every_n_samples", 
+                  &DiscriminativeRBM::gen_learning_every_n_samples,
+                  OptionBase::buildoption,
+                  "Determines the frequency of a generative learning update.\n"
+                  "For example, set this option to 100 in order to do an\n"
+                  "update every 100 samples. The gen_learning_weight will\n"
+                  "then be multiplied by 100.");
+
     declareOption(ol, "classification_module",
                   &DiscriminativeRBM::classification_module,
                   OptionBase::learntoption,
@@ -620,12 +628,15 @@
         pb = new ProgressBar( "Training "
                               + classname(),
                               nstages - stage );
-        
+
+    //! This makes sure that the samples on which generative learning 
+    //! is done changes from one epoch to another
+    int offset = (int)round(stage/nstages) % gen_learning_every_n_samples;
+
     for( ; stage<nstages ; stage++ )
     {
         train_set->getExample(stage%nsamples, input, target, weight);
 
-
         if( pb )
             pb->update( stage - init_stage + 1 );
 
@@ -682,70 +693,73 @@
             disc_neg_up_val << hidden_layer->expectation;
         }
 
-        // ... for generative learning        
-        if( ( !is_missing(target[0]) || targetsize() > 1 ) && 
-            gen_learning_weight > 0 )
-        {
-            // Positive phase
-            if( !use_exact_disc_gradient && !do_not_use_discriminative_learning )
+        // ... for generative learning
+        if( (stage + offset) % gen_learning_every_n_samples == 0 )
+        {            
+            if( ( !is_missing(target[0]) || targetsize() > 1 ) && 
+                gen_learning_weight > 0 )
             {
-                // Use previous computations
-                gen_pos_down_val << disc_pos_down_val;
-                gen_pos_up_val << disc_pos_up_val;
+                // Positive phase
+                if( !use_exact_disc_gradient && !do_not_use_discriminative_learning )
+                {
+                    // Use previous computations
+                    gen_pos_down_val << disc_pos_down_val;
+                    gen_pos_up_val << disc_pos_up_val;
 
-                hidden_layer->setExpectation( gen_pos_up_val );
-                hidden_layer->generateSample();
-            }
-            else
-            {
-                // Clamp visible units
-                target_layer->sample << target_one_hot;
-                input_layer->sample << input ;
+                    hidden_layer->setExpectation( gen_pos_up_val );
+                    hidden_layer->generateSample();
+                }
+                else
+                {
+                    // Clamp visible units
+                    target_layer->sample << target_one_hot;
+                    input_layer->sample << input ;
                 
-                // Up pass
-                joint_connection->setAsDownInput( joint_layer->sample );
-                hidden_layer->getAllActivations( joint_connection );
-                hidden_layer->computeExpectation();
-                hidden_layer->generateSample();
+                    // Up pass
+                    joint_connection->setAsDownInput( joint_layer->sample );
+                    hidden_layer->getAllActivations( joint_connection );
+                    hidden_layer->computeExpectation();
+                    hidden_layer->generateSample();
                 
-                gen_pos_down_val << joint_layer->sample;
-                gen_pos_up_val << hidden_layer->expectation;
-            }
+                    gen_pos_down_val << joint_layer->sample;
+                    gen_pos_up_val << hidden_layer->expectation;
+                }
 
-            // Negative phase
+                // Negative phase
 
-            if( !use_multi_conditional_learning )
-            {
-                // Down pass
-                joint_connection->setAsUpInput( hidden_layer->sample );
-                joint_layer->getAllActivations( joint_connection );
-                joint_layer->computeExpectation();
-                joint_layer->generateSample();
+                if( !use_multi_conditional_learning )
+                {
+                    // Down pass
+                    joint_connection->setAsUpInput( hidden_layer->sample );
+                    joint_layer->getAllActivations( joint_connection );
+                    joint_layer->computeExpectation();
+                    joint_layer->generateSample();
                 
-                // Up pass
-                joint_connection->setAsDownInput( joint_layer->sample );
-                hidden_layer->getAllActivations( joint_connection );
-                hidden_layer->computeExpectation();
-            }
-            else
-            {
-                target_layer->sample << target_one_hot;
+                    // Up pass
+                    joint_connection->setAsDownInput( joint_layer->sample );
+                    hidden_layer->getAllActivations( joint_connection );
+                    hidden_layer->computeExpectation();
+                }
+                else
+                {
+                    target_layer->sample << target_one_hot;
 
-                // Down pass
-                connection->setAsUpInput( hidden_layer->sample );
-                input_layer->getAllActivations( connection );
-                input_layer->computeExpectation();
-                input_layer->generateSample();
+                    // Down pass
+                    connection->setAsUpInput( hidden_layer->sample );
+                    input_layer->getAllActivations( connection );
+                    input_layer->computeExpectation();
+                    input_layer->generateSample();
                 
-                // Up pass
-                joint_connection->setAsDownInput( joint_layer->sample );
-                hidden_layer->getAllActivations( joint_connection );
-                hidden_layer->computeExpectation(); 
-            }
+                    // Up pass
+                    joint_connection->setAsDownInput( joint_layer->sample );
+                    hidden_layer->getAllActivations( joint_connection );
+                    hidden_layer->computeExpectation(); 
+                }
 
-            gen_neg_down_val << joint_layer->sample;
-            gen_neg_up_val << hidden_layer->expectation;
+                gen_neg_down_val << joint_layer->sample;
+                gen_neg_up_val << hidden_layer->expectation;
 
+            }
         }
 
         // ... and for semi-supervised learning
@@ -895,16 +909,18 @@
                                 disc_neg_down_val, disc_neg_up_val);
         }
 
-        
-        if( !is_missing(target[0]) && gen_learning_weight > 0 )
-        {
-            setLearningRate( gen_learning_weight * disc_learning_rate / 
-                             (1. + disc_decrease_ct * stage ));
-            joint_layer->update( gen_pos_down_val, gen_neg_down_val );
-            hidden_layer->update( gen_pos_up_val, gen_neg_up_val );
-            joint_connection->update( gen_pos_down_val, gen_pos_up_val,
-                                gen_neg_down_val, gen_neg_up_val);
-        }
+        if( (stage + offset) % gen_learning_every_n_samples == 0 )
+        { 
+            if( !is_missing(target[0]) && gen_learning_weight > 0 )
+            {
+                setLearningRate( gen_learning_every_n_samples * gen_learning_weight * disc_learning_rate / 
+                                 (1. + disc_decrease_ct * stage ));
+                joint_layer->update( gen_pos_down_val, gen_neg_down_val );
+                hidden_layer->update( gen_pos_up_val, gen_neg_up_val );
+                joint_connection->update( gen_pos_down_val, gen_pos_up_val,
+                                          gen_neg_down_val, gen_neg_up_val);
+            }
+        }            
 
         if( is_missing(target[0]) && semi_sup_learning_weight > 0 )
         {

Modified: trunk/plearn_learners_experimental/DiscriminativeRBM.h
===================================================================
--- trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-02-01 16:46:16 UTC (rev 8445)
+++ trunk/plearn_learners_experimental/DiscriminativeRBM.h	2008-02-01 21:39:39 UTC (rev 8446)
@@ -120,6 +120,12 @@
     //! for multitask learning.
     int n_mean_field_iterations;
 
+    //! Determines the frequency of a generative learning update.
+    //! For example, set this option to 100 in order to do an
+    //! update every 100 samples. The gen_learning_weight will
+    //! then be multiplied by 100.
+    int gen_learning_every_n_samples;
+
     //#####  Public Learnt Options  ###########################################
     //! The module computing the probabilities of the different classes.
     PP<RBMClassificationModule> classification_module;



From plearner at mail.berlios.de  Sat Feb  2 16:01:04 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 2 Feb 2008 16:01:04 +0100
Subject: [Plearn-commits] r8447 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200802021501.m12F14GO009481@sheep.berlios.de>

Author: plearner
Date: 2008-02-02 16:01:03 +0100 (Sat, 02 Feb 2008)
New Revision: 8447

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/LocallyMagnifiedDistribution.cc
Log:
Experimental. Debugging class.


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/LocallyMagnifiedDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/LocallyMagnifiedDistribution.cc	2008-02-01 21:39:39 UTC (rev 8446)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/LocallyMagnifiedDistribution.cc	2008-02-02 15:01:03 UTC (rev 8447)
@@ -275,21 +275,24 @@
         local_trainset->defineSizes(w,0,1);        
     }
 
-    double log_local_p = trainLocalDistrAndEvaluateLogDensity(local_trainset, y);
 
     // perr << "local_trainset =" << endl << local_trainset->toMat() << endl;
+    double log_local_p = 0;
 
     switch(mode)
     {
     case 0:
+        log_local_p = trainLocalDistrAndEvaluateLogDensity(local_trainset, y);
         return log_local_p + pl_log((double)weightsum) - pl_log((double)l) - pl_log((double)weighting_kernel(input,input));
     case 1:
+        log_local_p = trainLocalDistrAndEvaluateLogDensity(local_trainset, y);
         return log_local_p;
     case 2:
         return pl_log((double)weightsum) - pl_log((double)l);
     case 3:
         return pl_log((double)weightsum);
     case 4:
+        log_local_p = trainLocalDistrAndEvaluateLogDensity(local_trainset, y);
         return log_local_p+pl_log((double)width_n)-pl_log((double)l);
     default:
         PLERROR("Invalid mode %d", mode);



From plearner at mail.berlios.de  Sat Feb  2 16:01:57 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sat, 2 Feb 2008 16:01:57 +0100
Subject: [Plearn-commits] r8448 - in trunk: commands/EXPERIMENTAL
	plearn_learners/generic/EXPERIMENTAL
Message-ID: <200802021501.m12F1vv9009537@sheep.berlios.de>

Author: plearner
Date: 2008-02-02 16:01:57 +0100 (Sat, 02 Feb 2008)
New Revision: 8448

Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
Temporary debugging for DeepReconstructor Net


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-02-02 15:01:03 UTC (rev 8447)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-02-02 15:01:57 UTC (rev 8448)
@@ -260,6 +260,7 @@
 // #include <plearn/vmat/CenteredVMatrix.h>
 // #include <plearn/vmat/ClassSubsetVMatrix.h>
 // #include <plearn/vmat/CompactVMatrix.h>
+#include <plearn/vmat/CompactFileVMatrix.h>
 // #include <plearn/vmat/CompressedVMatrix.h>
 // #include <plearn/vmat/CumVMatrix.h>
 // #include <plearn/vmat/DatedJoinVMatrix.h>

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-02 15:01:03 UTC (rev 8447)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-02 15:01:57 UTC (rev 8448)
@@ -433,6 +433,7 @@
     {
         inputs->getExample(i,in,target,weight);
         f->fprop(in, out);
+        displayFunction(f, true);
         outputs->putOrAppendRow(i,out);
     }
 }



From plearner at mail.berlios.de  Sun Feb  3 23:31:45 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sun, 3 Feb 2008 23:31:45 +0100
Subject: [Plearn-commits] r8449 - trunk/plearn/vmat
Message-ID: <200802032231.m13MVjnV022881@sheep.berlios.de>

Author: plearner
Date: 2008-02-03 23:31:44 +0100 (Sun, 03 Feb 2008)
New Revision: 8449

Modified:
   trunk/plearn/vmat/FileVMatrix.cc
Log:
Made sure deep copying a FileVMatrix which happens to be in write mode does 
not attempt reopening the file in write mode (to avoid erasing it).


Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-02-02 15:01:57 UTC (rev 8448)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-02-03 22:31:44 UTC (rev 8449)
@@ -340,7 +340,18 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
+    // it is unclear whether f should be shared or not...
+    // if we allow multi-threading, we should probably not share it
+    // so for now we will not share it. But a cleaner behavoiur would probably be
+    // to share it in multiple threads but make sure that getRow, putRow, etc... operations
+    // are atomic (no context switch to another thread).
+
     f = 0;   // Because we will open again the file (f should not be shared).
+    // however reopening the file twice in write mode is certainly a VERY bad idea.
+    // thus we switch to read-mode 
+    build_new_file = false;
+    writable = false;
+
     build(); // To open the file.
 }
 



From plearner at mail.berlios.de  Sun Feb  3 23:33:22 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sun, 3 Feb 2008 23:33:22 +0100
Subject: [Plearn-commits] r8450 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200802032233.m13MXMWu022992@sheep.berlios.de>

Author: plearner
Date: 2008-02-03 23:33:21 +0100 (Sun, 03 Feb 2008)
New Revision: 8450

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
Made sure intermediate ouputs are saved to file


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-03 22:31:44 UTC (rev 8449)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-03 22:33:21 UTC (rev 8450)
@@ -315,7 +315,7 @@
     deepCopyField(compute_layer, copies);
     deepCopyField(compute_output, copies);
     deepCopyField(output_and_target_to_cost, copies);
-    deepCopyField(outmat, copies);
+    // deepCopyField(outmat, copies); // deep copying vmatrices, especially if opened in write mode, is probably a bad idea
     deepCopyField(group_sizes, copies);
 }
 
@@ -376,7 +376,7 @@
                 trainHiddenLayer(k, dset);
                 PLearn::save(expdir/"learner.psave", *this);
                 // 'if' is a hack to avoid precomputing last hidden layer if not needed
-                if(k<nreconstructions-1 ||  must_train_supervised_layer) 
+                // if(k<nreconstructions-1 ||  must_train_supervised_layer) 
                 { 
                     int width = layers[k+1].width();
                     outmat[k] = new FileVMatrix(outmatfname+tostring(k+1)+".pmat",0,width);
@@ -391,6 +391,16 @@
                 trainSupervisedLayer(dset, targets);
                 PLearn::save(expdir/"learner.psave", *this);
             }
+
+            for(int k=0; k<reconstruction_costs.length(); k++)
+              {
+                if(outmat[k].isNotNull())
+                  {
+                    perr << "Closing outmat " << k+1 << endl;
+                    outmat[k] = 0;
+                  }
+              }
+            
             perr << "\n\n*********************************************" << endl;
             perr << "****      Now performing fine tuning     ****" << endl;
             perr << "********************************************* \n" << endl;
@@ -433,9 +443,10 @@
     {
         inputs->getExample(i,in,target,weight);
         f->fprop(in, out);
-        displayFunction(f, true);
+        // displayFunction(f, true);
         outputs->putOrAppendRow(i,out);
     }
+    outputs->flush();
 }
 
 void DeepReconstructorNet::prepareForFineTuning()



From plearner at mail.berlios.de  Sun Feb  3 23:40:49 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Sun, 3 Feb 2008 23:40:49 +0100
Subject: [Plearn-commits] r8451 - in trunk: plearn/display
	plearn_learners/generic/EXPERIMENTAL
Message-ID: <200802032240.m13Menm1023597@sheep.berlios.de>

Author: plearner
Date: 2008-02-03 23:40:49 +0100 (Sun, 03 Feb 2008)
New Revision: 8451

Modified:
   trunk/plearn/display/DisplayUtils.cc
   trunk/plearn/display/GhostScript.h
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
Better display for Var graphs


Modified: trunk/plearn/display/DisplayUtils.cc
===================================================================
--- trunk/plearn/display/DisplayUtils.cc	2008-02-03 22:33:21 UTC (rev 8450)
+++ trunk/plearn/display/DisplayUtils.cc	2008-02-03 22:40:49 UTC (rev 8451)
@@ -46,6 +46,7 @@
 #include "DisplayUtils.h"
 #include <plearn/io/openString.h>
 #include <plearn/io/TmpFilenames.h>
+#include <plearn/math/pl_math.h>
 
 #if defined(WIN32) && !defined(__CYGWIN__)
 #include <io.h>
@@ -312,6 +313,50 @@
     return v.subVec((l-n)/2,n);
   }
 
+  string summarizedVecString(Vec v, int maxn=16, string format="%2.2g")
+  {
+    string result = "";
+    int n = 0;
+    char buf[30];
+    int nsame = 1;
+    string val;
+    string prev_val;
+    int l = v.length();
+    result = tostring(l)+ " [ ";
+    int i;
+    for(i=0; i<l && n<maxn; i++)
+      {
+        snprintf(buf,20,format.c_str(),v[i]);
+        val = buf;
+        if(i==0)
+          prev_val = val;
+        else if(val==prev_val)
+          { nsame++; }
+        else 
+          {
+            result += prev_val;
+            if(nsame>1)
+              result += (string("*")+tostring(nsame));
+            result += " ";
+            n++;
+            nsame = 1;
+          }
+        prev_val = val;
+      }
+
+    if(l>0)
+      {
+        result += prev_val;
+        if(nsame>1)
+          result += (string("*")+tostring(nsame));
+      }
+    if(i<l)
+      result += " ...]";
+    else
+      result += " ]";
+    return result;
+  }
+
 /** VarGraph **/
 
 void displayVarGraph(const VarArray& outputs, bool display_values, real boxwidth, const char* the_filename, bool must_wait, VarArray display_only_these)
@@ -489,20 +534,20 @@
       if(display_values)
         {
           gs.usefont("Times-Bold", 11.0);
-          gs.centerShow(my_x, my_y+boxheight/4, descr.c_str());
+          gs.centerShow(my_x, my_y+boxheight/4, descr);
           gs.usefont("Times-Roman", 10.0);
           gs.centerShow(my_x, my_y, nameline);
           gs.usefont("Courrier", 6.0);
           if (v->rValue.length()>0) // print rvalue if there are some...
           {
-            gs.centerShow(my_x, my_y-boxheight/5, centerSubVec(v->value));
-            gs.centerShow(my_x, my_y-boxheight/3, centerSubVec(v->gradient));
-            gs.centerShow(my_x, my_y-boxheight/1, centerSubVec(v->rValue));
+            gs.centerShow(my_x, my_y-boxheight/5, summarizedVecString(v->value));
+            gs.centerShow(my_x, my_y-boxheight/3, summarizedVecString(v->gradient));
+            gs.centerShow(my_x, my_y-boxheight/1, summarizedVecString(v->rValue));
           }
           else
           {
-            gs.centerShow(my_x, my_y-boxheight/5, centerSubVec(v->value));
-            gs.centerShow(my_x, my_y-boxheight/2.5, centerSubVec(v->gradient));
+            gs.centerShow(my_x, my_y-boxheight/5, summarizedVecString(v->value));
+            gs.centerShow(my_x, my_y-boxheight/2.5, summarizedVecString(v->gradient));
           }
           /*
           cout << descr << " " << nameline << " (" << v->value.length() << ")" << endl;
@@ -749,12 +794,37 @@
     unlink(filename);
 }
 
+void tagVariables(VarArray vars, string tag)
+{
+  for(int i=0; i<vars.length(); i++)
+    {
+      string name = vars[i]->getName();
+      vars[i]->setName(tag+":"+name);
+    }
+}
+
+void untagVariables(VarArray vars, string tag)
+{
+  int startpos = tag.length()+1;
+  for(int i=0; i<vars.length(); i++)
+    {
+      string name = vars[i]->getName();
+      vars[i]->setName(name.substr(startpos,name.length()-startpos));
+    }
+}
+
 void displayFunction(Func f, bool display_values, bool display_differentiation, real boxwidth, const char* the_filename, bool must_wait)
 { 
+  tagVariables(f->inputs,"INPUT");
+  tagVariables(f->parameters,"PARAM");
+  tagVariables(f->outputs,"OUTPUT");
   if(display_differentiation)
     displayVarGraph(f->outputs & f->differentiate()->outputs, display_values, boxwidth, the_filename, must_wait);
   else
     displayVarGraph(f->outputs, display_values, boxwidth, the_filename, must_wait); 
+  untagVariables(f->outputs,"OUTPUT");
+  untagVariables(f->parameters,"PARAM");
+  untagVariables(f->inputs,"INPUT");
 }
 
 Mat compute2dGridOutputs(PP<PLearner> learner, real min_x, real max_x, real min_y, real max_y, int length, int width, real singleoutput_threshold)

Modified: trunk/plearn/display/GhostScript.h
===================================================================
--- trunk/plearn/display/GhostScript.h	2008-02-03 22:33:21 UTC (rev 8450)
+++ trunk/plearn/display/GhostScript.h	2008-02-03 22:40:49 UTC (rev 8451)
@@ -141,13 +141,22 @@
   void show(const char* str)
   { togs << "(" << str << ") show" << endl; }
 
+  void show(const string& str)
+  { togs << "(" << str << ") show" << endl; }
+
   //! halign can be 'l' (left), 'c' (center) or 'r' (right)
   //! valign can be 't' (top), 'm' (middle) or 'b' (bottom) 
   void show(real x, real y, const char* str, char halign='l', char valign='b');
 
+  inline void show(real x, real y, const string& str, char halign='l', char valign='b')
+  { show(x,y,str.c_str(), halign, valign); }
+
   void centerShow(real x, real y, const char* str)
   { show(x,y,str,'c','m'); }
 
+  void centerShow(real x, real y, const string& str)
+  { show(x,y,str,'c','m'); }
+
   //! accepts multiple line text (lines separated by \n) 
   void multilineShow(real x, real y, const string& text, real newlinesize, char halign='l', char valign='b');
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-03 22:33:21 UTC (rev 8450)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-03 22:40:49 UTC (rev 8451)
@@ -443,7 +443,13 @@
     {
         inputs->getExample(i,in,target,weight);
         f->fprop(in, out);
-        // displayFunction(f, true);
+        /*
+        if(i==0)
+        {
+            perr << "Function used for building hidden layer " << which_input_layer << endl;
+            displayFunction(f, true);
+        }
+        */
         outputs->putOrAppendRow(i,out);
     }
     outputs->flush();
@@ -453,6 +459,8 @@
 {
     Func f(layers[0]&target, supervised_costvec);
     Var totalcost = sumOf(train_set, f, minibatch_size);
+    perr << "Function used for fine tuning" << endl;
+    // displayFunction(f, true);
     // displayVarGraph(supervised_costvec);
     // displayVarGraph(totalcost);
 
@@ -468,7 +476,7 @@
     VarArray proppath = propagationPath(layers[0],layers[nlayers-1]);
     layers[0]->matValue << input;
     proppath.fprop();
-    perr << "Graph for computing representations" << endl;
+    // perr << "Graph for computing representations" << endl;
     // displayVarGraph(proppath,true, 333, "repr");
     for(int k=0; k<nlayers; k++)
         representations[k] = layers[k]->matValue.copy();
@@ -481,7 +489,7 @@
     {
         VarArray proppath = propagationPath(layers[k],reconstructed_layers[k-1]);
         proppath.fprop();
-        perr << "Graph for reconstructing layer " << k-1 << " from layer " << k << endl;
+        // perr << "Graph for reconstructing layer " << k-1 << " from layer " << k << endl;
         //displayVarGraph(proppath,true, 333, "reconstr");
 
         //WARNING MEGA-HACK
@@ -644,10 +652,11 @@
     perr << "*** Training (unsupervised) layer " << which_input_layer+1 << " for max. " << nepochs.second << " epochs " << endl;
     perr << "*** each epoch has " << l << " examples and " << l/minibatch_size << " optimizer stages (updates)" << endl;
     Func f(layers[which_input_layer], reconstruction_costs[which_input_layer]);
+    Var totalcost = sumOf(inputs, f, minibatch_size);
+    VarArray params = totalcost->parents();
     //displayVarGraph(reconstruction_costs[which_input_layer]);
     //displayFunction(f,false,false, 333, "train_func");
-    Var totalcost = sumOf(inputs, f, minibatch_size);
-    VarArray params = totalcost->parents();
+    //displayVarGraph(totalcost,true);
     
     if ( reconstruction_optimizers.size() !=0 )
     {
@@ -686,7 +695,7 @@
         Vec stderrs = st.getStdError();
         perr << "Epoch " << n+1 << ": " << means << " +- " << stderrs;
         real m = means[reconstr_cost_pos];
-        real er = stderrs[reconstr_cost_pos];
+        // real er = stderrs[reconstr_cost_pos];
         if(n>0)
         {
             relative_improvement = (prev_mean-m)/fabs(prev_mean);
@@ -727,6 +736,16 @@
         training_curve->flush();
 
         prev_mean = m;
+
+        /*
+        if(n==0)
+        {
+            perr << "Displaying reconstruciton_cost" << endl;
+            displayVarGraph(reconstruction_costs[which_input_layer],true);
+            perr << "Displaying optimized funciton f" << endl;
+            displayFunction(f,true,false, 333, "train_func");
+        }
+        */
     }
 }
 



From saintmlx at mail.berlios.de  Mon Feb  4 19:13:45 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 4 Feb 2008 19:13:45 +0100
Subject: [Plearn-commits] r8452 - trunk/python_modules/plearn/plide
Message-ID: <200802041813.m14IDjEt002906@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-04 19:13:35 +0100 (Mon, 04 Feb 2008)
New Revision: 8452

Modified:
   trunk/python_modules/plearn/plide/plide_options.py
Log:
- make sure float spin buttons have enough precision



Modified: trunk/python_modules/plearn/plide/plide_options.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_options.py	2008-02-03 22:40:49 UTC (rev 8451)
+++ trunk/python_modules/plearn/plide/plide_options.py	2008-02-04 18:13:35 UTC (rev 8452)
@@ -293,8 +293,16 @@
 
                 ## Spin button (float version)
                 elif type(option_value) == float:
-                    option_input = gtk.SpinButton(digits=3)
-                    option_input.set_increments(0.1, 1.0)
+                    # set precision according to option_value; min. 3 digits
+                    import math
+                    ndig= 3
+                    if option_value < 0:
+                        ndig= -math.log10(-option_value)
+                    elif option_value > 0:
+                        ndig= -math.log10(option_value)
+                    ndig= int(math.ceil(max(3, ndig)))
+                    option_input = gtk.SpinButton(digits=ndig)
+                    option_input.set_increments(10.**(-ndig), 1.0)
                     option_input.set_numeric(True)
                     option_input.set_range(*true_bounds(*option_bounds))
                     option_input.set_value(option_value)



From nouiz at mail.berlios.de  Mon Feb  4 22:16:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Feb 2008 22:16:07 +0100
Subject: [Plearn-commits] r8453 - trunk/plearn_learners/regressors
Message-ID: <200802042116.m14LG7WI015941@sheep.berlios.de>

Author: nouiz
Date: 2008-02-04 22:16:06 +0100 (Mon, 04 Feb 2008)
New Revision: 8453

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
one optimisation and a bugfix


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-04 18:13:35 UTC (rev 8452)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-04 21:16:06 UTC (rev 8453)
@@ -188,7 +188,6 @@
     train_set = the_train_set;
     leave = the_leave;
     leave_template = the_leave_template;
-    split_col = -1;
     int missing_leave_id = train_set->getNextId();
     int left_leave_id =  train_set->getNextId();
     int right_leave_id =  train_set->getNextId();
@@ -214,6 +213,8 @@
 
 void RegressionTreeNode::lookForBestSplit()
 {
+    if(leave->length<=1)
+        return;
     TVec<int> candidate(train_set->length());//list of candidate row to split
     candidate.resize(0);
     TVec<int> registered_row;
@@ -241,8 +242,10 @@
             }
         }
         missing_leave->getOutputAndError(tmp_vec, missing_error);
+
+        //in case of missing value
         if(candidate.size()==0)
-            return;
+            continue;
         int row = candidate.pop();
         while (candidate.size()>0)
         {



From nouiz at mail.berlios.de  Mon Feb  4 22:22:40 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 4 Feb 2008 22:22:40 +0100
Subject: [Plearn-commits] r8454 - in trunk: plearn/math
 plearn_learners/regressors
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/Lear!
 nerExpdir/Strat0/Trials2/Split0
Message-ID: <200802042122.m14LMeYp016443@sheep.berlios.de>

Author: nouiz
Date: 2008-02-04 22:22:39 +0100 (Mon, 04 Feb 2008)
New Revision: 8454

Modified:
   trunk/plearn/math/pl_math.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
Log:
fixed computation instability in PL_RegressionTree to different pressision of float/double in diferent computer.
For this I added is_more and fast_is_more in plearn/math/pl_math.h


Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn/math/pl_math.h	2008-02-04 21:22:39 UTC (rev 8454)
@@ -235,6 +235,23 @@
     return (a <= b && b <= a);
 }
 
+//! Test float inequality (but does not deal with 'nan' and 'inf' values).
+inline bool fast_is_more(real a, real b, real absolute_tolerance_threshold = 1.0, 
+                          real absolute_tolerance = ABSOLUTE_TOLERANCE,
+                          real relative_tolerance = RELATIVE_TOLERANCE)
+{
+    return !fast_is_equal(a,b,absolute_tolerance_threshold,absolute_tolerance,relative_tolerance) && a>b;
+}
+
+//! Test float inequality while dealling with 'nan' and 'inf' values.
+inline bool is_more(real a, real b, real absolute_tolerance_threshold = 1.0, 
+                          real absolute_tolerance = ABSOLUTE_TOLERANCE,
+                          real relative_tolerance = RELATIVE_TOLERANCE)
+{
+    return !is_equal(a,b,absolute_tolerance_threshold,absolute_tolerance,relative_tolerance) && a>b;
+
+}
+
 template<class T>
 inline T square(const T& x)
 { return x*x; }

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-04 21:22:39 UTC (rev 8454)
@@ -265,9 +265,10 @@
     if (left_leave_last_feature >= right_leave_first_feature) return;
     real work_error = missing_error[0] + missing_error[1] + left_error[0] + left_error[1] + right_error[0] + right_error[1];
     int work_balance = abs(left_leave->getLength() - right_leave->getLength());
-    if(split_col<0);
-    else if (work_error > after_split_error) return;
-    else if (work_error == after_split_error && work_balance > split_balance) return;
+    if (fast_is_more(work_error,after_split_error)) return;
+    else if (fast_is_equal(work_error,after_split_error) &&
+             fast_is_more(work_balance,split_balance)) return;
+
     split_col = col;
     split_feature_value = 0.5 * (right_leave_first_feature + left_leave_last_feature);
     after_split_error = work_error;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/RUN.log	2008-02-04 21:22:39 UTC (rev 8454)
@@ -1,5 +1,5 @@
 HyperLearner: starting the optimization
 split_cols: []
 
-split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 2 
-split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 2 3 3 3 1 3 3 1 2 1 3 3 3 1 3 1 1 1 1 3 3 
+split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 
+split_cols: 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 3 3 3 1 3 3 1 2 3 3 3 3 1 3 1 3 3 1 3 3 

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -15,9 +15,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = 8640.51363742984176 ;
-sumsquare_ = 1374774.3118040429 ;
-sumcube_ = 323112262.385960281 ;
-sumfourth_ = 93349681763.0307617 ;
+sumsquare_ = 1374774.31180404266 ;
+sumcube_ = 323112262.385960162 ;
+sumfourth_ = 93349681763.0307159 ;
 min_ = 0 ;
 max_ = 434.308100803600041 ;
 agmemin_ = 120 ;
@@ -57,9 +57,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = -17281.0272748596835 ;
-sumsquare_ = 5499097.24721617159 ;
-sumcube_ = -2584898099.08768225 ;
-sumfourth_ = 1493594908208.49219 ;
+sumsquare_ = 5499097.24721617065 ;
+sumcube_ = -2584898099.08768129 ;
+sumfourth_ = 1493594908208.49146 ;
 min_ = -867.616201607200082 ;
 max_ = 1 ;
 agmemin_ = 102 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -1392.4175074365653 ;
-sumsquare_ = 24435.3412616714304 ;
-sumcube_ = -524149.893896332418 ;
-sumfourth_ = 13079901.8943236303 ;
+sum_ = -1392.41750743656576 ;
+sumsquare_ = 24435.3412616714268 ;
+sumcube_ = -524149.89389633236 ;
+sumfourth_ = 13079901.8943236265 ;
 min_ = -40.6801200000000023 ;
 max_ = 1 ;
 agmemin_ = 102 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -14,7 +14,7 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 10321.7894884868674 ;
+sum_ = 10321.7894884868656 ;
 sumsquare_ = 71200030.8763843179 ;
 sumcube_ = 534446154111.910217 ;
 sumfourth_ = 4273059163655376.5 ;
@@ -56,7 +56,7 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -20643.5789769737348 ;
+sum_ = -20643.5789769737312 ;
 sumsquare_ = 284800123.505537271 ;
 sumcube_ = -4275569232895.28174 ;
 sumfourth_ = 68368946618486024 ;
@@ -77,7 +77,7 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -50.2688274199133431 ;
+sum_ = -50.2688274199132863 ;
 sumsquare_ = 38224.7447939705817 ;
 sumcube_ = -3656520.3377262922 ;
 sumfourth_ = 551819990.898841023 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -15,9 +15,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = 8640.51363742984176 ;
-sumsquare_ = 1374774.3118040429 ;
-sumcube_ = 323112262.385960281 ;
-sumfourth_ = 93349681763.0307617 ;
+sumsquare_ = 1374774.31180404266 ;
+sumcube_ = 323112262.385960162 ;
+sumfourth_ = 93349681763.0307159 ;
 min_ = 0 ;
 max_ = 434.308100803600041 ;
 agmemin_ = 120 ;
@@ -57,9 +57,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = -17281.0272748596835 ;
-sumsquare_ = 5499097.24721617159 ;
-sumcube_ = -2584898099.08768225 ;
-sumfourth_ = 1493594908208.49219 ;
+sumsquare_ = 5499097.24721617065 ;
+sumcube_ = -2584898099.08768129 ;
+sumfourth_ = 1493594908208.49146 ;
 min_ = -867.616201607200082 ;
 max_ = 1 ;
 agmemin_ = 102 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -1392.4175074365653 ;
-sumsquare_ = 24435.3412616714304 ;
-sumcube_ = -524149.893896332418 ;
-sumfourth_ = 13079901.8943236303 ;
+sum_ = -1392.41750743656576 ;
+sumsquare_ = 24435.3412616714268 ;
+sumcube_ = -524149.89389633236 ;
+sumfourth_ = 13079901.8943236265 ;
 min_ = -40.6801200000000023 ;
 max_ = 1 ;
 agmemin_ = 102 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -78,9 +78,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = 192.890804746032558 ;
-sumsquare_ = 3592.51670061680807 ;
-sumcube_ = 2141.61368714041009 ;
-sumfourth_ = 201415.693085262843 ;
+sumsquare_ = 3592.51670061680898 ;
+sumcube_ = 2141.61368714040918 ;
+sumfourth_ = 201415.693085262814 ;
 min_ = -21.6326733333333436 ;
 max_ = 1 ;
 agmemin_ = 16 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -14,10 +14,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 10974.0895327640446 ;
-sumsquare_ = 76330936.5701396614 ;
-sumcube_ = 569842056881.908203 ;
-sumfourth_ = 4522877790823688 ;
+sum_ = 11204.3270149540895 ;
+sumsquare_ = 76349558.7474929094 ;
+sumcube_ = 569851029912.986938 ;
+sumfourth_ = 4522879206793045 ;
 min_ = 0.650915482711108107 ;
 max_ = 8311.49117606439722 ;
 agmemin_ = 17 ;
@@ -56,10 +56,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -21948.1790655280893 ;
-sumsquare_ = 305323746.280558646 ;
-sumcube_ = -4558736455055.26562 ;
-sumfourth_ = 72366044653179008 ;
+sum_ = -22408.6540299081789 ;
+sumsquare_ = 305398234.989971638 ;
+sumcube_ = -4558808239303.89551 ;
+sumfourth_ = 72366067308688720 ;
 min_ = -16621.9823521287944 ;
 max_ = -0.301830965422216213 ;
 agmemin_ = 35 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -61.8945591190477131 ;
-sumsquare_ = 40932.6337231527941 ;
-sumcube_ = -4578700.6447631754 ;
-sumfourth_ = 688955792.048339963 ;
+sum_ = -84.9389746746032586 ;
+sumsquare_ = 40750.1377623234584 ;
+sumcube_ = -4587593.35613886733 ;
+sumfourth_ = 688820550.749982357 ;
 min_ = -181.33475999999996 ;
 max_ = -0.61358666666666295 ;
 agmemin_ = 35 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -78,9 +78,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = 192.890804746032558 ;
-sumsquare_ = 3592.51670061680807 ;
-sumcube_ = 2141.61368714041009 ;
-sumfourth_ = 201415.693085262843 ;
+sumsquare_ = 3592.51670061680898 ;
+sumcube_ = 2141.61368714040918 ;
+sumfourth_ = 201415.693085262814 ;
 min_ = -21.6326733333333436 ;
 max_ = 1 ;
 agmemin_ = 16 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -107,7 +107,7 @@
 metadatadir = ""  )
 ;
 next_id = 244 ;
-leave_register = 150 [ 165 106 64 97 79 105 97 151 99 166 109 58 135 105 153 91 79 106 136 166 91 135 70 121 70 106 166 99 165 75 67 88 97 72 109 64 105 105 165 72 136 79 60 108 165 118 54 154 121 117 97 81 105 88 28 97 99 121 94 79 79 96 165 109 109 96 135 55 54 118 153 109 96 151 55 58 90 63 87 96 105 150 87 87 64 93 135 117 150 121 118 78 70 73 81 78 105 91 153 106 78 97 166 69 91 88 63 121 67 54 69 109 108 121 97 93 64 153 96 99 70 136 121 153 69 60 153 87 99 120 88 88 97 64 109 64 153 121 99 96 109 90 117 121 97 55 105 91 58 151 ] ;
+leave_register = 150 [ 165 106 64 97 79 105 97 150 99 166 109 58 135 105 153 91 79 106 136 166 91 135 70 121 70 106 166 99 165 75 67 88 97 72 109 64 105 105 165 72 136 79 60 108 165 118 54 154 121 117 97 81 105 88 28 97 99 121 93 79 79 96 165 109 109 96 135 55 54 118 153 109 96 150 55 58 90 63 87 96 105 151 87 87 64 94 135 117 151 121 118 78 70 73 81 78 105 91 153 106 78 97 166 69 91 88 63 121 67 54 69 109 108 121 97 94 64 153 96 99 70 136 121 153 69 60 153 87 99 120 88 88 97 64 109 64 153 121 99 96 109 90 117 121 97 55 105 91 58 150 ] ;
 writable = 0 ;
 length = 150 ;
 width = 6 ;
@@ -491,11 +491,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -177.497500000000002 ;
-weighted_targets_sum = -1.18331666666666679 ;
-weighted_squared_targets_sum = 210.035750041666688 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -625,11 +625,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -175.837510000000009 ;
-weighted_targets_sum = -1.17225006666666687 ;
-weighted_squared_targets_sum = 206.1255328200007 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -728,10 +728,10 @@
 leave = *49  ;
 leave_output = 2 [ -150.613133333333337 1 ] ;
 leave_error = 3 [ 0.282992137883448458 0 0.0400000000000000008 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 1 ;
-split_feature_value = -1.51283000000000012 ;
-after_split_error = 0.00999469557589860447 ;
+split_feature_value = -3.13504000000000005 ;
+after_split_error = 0.00999469557601595504 ;
 missing_node = *0 ;
 missing_leave = *50 ->RegressionTreeLeave(
 id = 197 ;
@@ -1668,10 +1668,10 @@
 leave = *105  ;
 leave_output = 2 [ -70.2812399999999968 1 ] ;
 leave_error = 3 [ 0.553167864882681992 0 0.0400000000000000008 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 1 ;
-split_feature_value = -0.627214999999999967 ;
-after_split_error = 0.168978629290664761 ;
+split_feature_value = -2.23238499999999984 ;
+after_split_error = 0.168978629290691268 ;
 missing_node = *0 ;
 missing_leave = *106 ->RegressionTreeLeave(
 id = 143 ;
@@ -2019,10 +2019,10 @@
 leave = *126  ;
 leave_output = 2 [ -39.6486500000000035 1 ] ;
 leave_error = 3 [ 0.636608327434661891 0 0.0666666666666666657 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 3 ;
-split_feature_value = -0.401110000000000022 ;
-after_split_error = 0.405305692174637633 ;
+split_feature_value = -1.56711 ;
+after_split_error = 0.405305692174654175 ;
 missing_node = *0 ;
 missing_leave = *127 ->RegressionTreeLeave(
 id = 191 ;
@@ -2289,10 +2289,10 @@
 leave = *142  ;
 leave_output = 2 [ -23.9631660000000011 1 ] ;
 leave_error = 3 [ 2.9916037008709373 0 0.0666666666666666657 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 1 ;
-split_feature_value = -0.215794999999999987 ;
-after_split_error = 0.746847325971322307 ;
+split_feature_value = -2.12482499999999996 ;
+after_split_error = 0.746847325971338849 ;
 missing_node = *0 ;
 missing_leave = *143 ->RegressionTreeLeave(
 id = 149 ;
@@ -2321,11 +2321,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -62.1399899999999974 ;
-weighted_targets_sum = -0.41426660000000004 ;
-weighted_squared_targets_sum = 12.9961512182033356 ;
+length = 3 ;
+weights_sum = 0.0200000000000000004 ;
+targets_sum = -57.6758400000000009 ;
+weighted_targets_sum = -0.384505600000000003 ;
+weighted_squared_targets_sum = 7.64076145701733456 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -2334,12 +2334,12 @@
 ;
 train_set = *8  ;
 leave = *145  ;
-leave_output = 2 [ -31.0699950000000022 1 ] ;
-leave_error = 3 [ 0.2497800550726679 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -1.68244499999999997 ;
-after_split_error = 0 ;
+leave_output = 2 [ -19.2252800000000015 1 ] ;
+leave_error = 3 [ 0.4970672708986692 0 0.0400000000000000008 ] ;
+split_col = 1 ;
+split_balance = 1 ;
+split_feature_value = -0.10633999999999999 ;
+after_split_error = 0.00781290769066451431 ;
 missing_node = *0 ;
 missing_leave = *146 ->RegressionTreeLeave(
 id = 215 ;
@@ -2366,10 +2366,10 @@
 verbosity = 2 ;
 train_set = *8  ;
 length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -28.0094799999999964 ;
-weighted_targets_sum = -0.186729866666666688 ;
-weighted_squared_targets_sum = 5.23020646580266568 ;
+weights_sum = 0.00666666666666666536 ;
+targets_sum = -22.2395500000000013 ;
+weighted_targets_sum = -0.148263666666666655 ;
+weighted_squared_targets_sum = 3.29731722801666738 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -2383,11 +2383,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -34.130510000000001 ;
-weighted_targets_sum = -0.227536733333333352 ;
-weighted_squared_targets_sum = 7.76594475240066817 ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -35.4362899999999996 ;
+weighted_targets_sum = -0.236241933333333348 ;
+weighted_squared_targets_sum = 4.34344422900066718 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -2406,11 +2406,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -57.6758400000000009 ;
-weighted_targets_sum = -0.384505600000000058 ;
-weighted_squared_targets_sum = 7.64076145701733456 ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = -62.1399899999999974 ;
+weighted_targets_sum = -0.41426660000000004 ;
+weighted_squared_targets_sum = 12.9961512182033339 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -2419,12 +2419,12 @@
 ;
 train_set = *8  ;
 leave = *150  ;
-leave_output = 2 [ -19.2252800000000015 1 ] ;
-leave_error = 3 [ 0.497067270898664926 0 0.0400000000000000008 ] ;
-split_col = 1 ;
-split_balance = 1 ;
-split_feature_value = -0.10633999999999999 ;
-after_split_error = 0.00781290769066451431 ;
+leave_output = 2 [ -31.0699950000000022 1 ] ;
+leave_error = 3 [ 0.249780055072664348 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = -1.68244499999999997 ;
+after_split_error = 0 ;
 missing_node = *0 ;
 missing_leave = *151 ->RegressionTreeLeave(
 id = 218 ;
@@ -2451,10 +2451,10 @@
 verbosity = 2 ;
 train_set = *8  ;
 length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -22.2395500000000013 ;
-weighted_targets_sum = -0.148263666666666655 ;
-weighted_squared_targets_sum = 3.29731722801666738 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -28.0094799999999964 ;
+weighted_targets_sum = -0.186729866666666688 ;
+weighted_squared_targets_sum = 5.23020646580266568 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -2468,11 +2468,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -35.4362899999999996 ;
-weighted_targets_sum = -0.236241933333333348 ;
-weighted_squared_targets_sum = 4.34344422900066718 ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = -34.130510000000001 ;
+weighted_targets_sum = -0.227536733333333352 ;
+weighted_squared_targets_sum = 7.76594475240066817 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -2669,11 +2669,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 4.87410000000000032 ;
-weighted_targets_sum = 0.032494000000000002 ;
-weighted_squared_targets_sum = 0.158379005400000022 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4386,10 +4386,10 @@
 leave = *267  ;
 leave_output = 2 [ 115.786709166666668 1 ] ;
 leave_error = 3 [ 16.7309274270517392 0 0.160000000000000003 ] ;
-split_col = 2 ;
+split_col = 3 ;
 split_balance = 8 ;
-split_feature_value = 16.504294999999999 ;
-after_split_error = 6.78331317461814542 ;
+split_feature_value = 3.03715500000000027 ;
+after_split_error = 6.78331317461888439 ;
 missing_node = *0 ;
 missing_leave = *268 ->RegressionTreeLeave(
 id = 65 ;
@@ -4421,8 +4421,8 @@
 length = 10 ;
 weights_sum = 0.0666666666666666657 ;
 targets_sum = 1122.60447999999997 ;
-weighted_targets_sum = 7.48402986666666781 ;
-weighted_squared_targets_sum = 843.26523310528944 ;
+weighted_targets_sum = 7.48402986666666692 ;
+weighted_squared_targets_sum = 843.265233105289326 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4431,12 +4431,12 @@
 ;
 train_set = *8  ;
 leave = *270  ;
-leave_output = 2 [ 112.260448000000025 1 ] ;
-leave_error = 3 [ 6.20937485581771398 0 0.133333333333333331 ] ;
-split_col = 1 ;
+leave_output = 2 [ 112.260448000000011 1 ] ;
+leave_error = 3 [ 6.20937485581785609 0 0.133333333333333331 ] ;
+split_col = 3 ;
 split_balance = 8 ;
-split_feature_value = 1.01285500000000006 ;
-after_split_error = 1.72287732250566372 ;
+split_feature_value = -0.972770000000000024 ;
+after_split_error = 1.72287732250645886 ;
 missing_node = *0 ;
 missing_leave = *271 ->RegressionTreeLeave(
 id = 119 ;
@@ -4509,11 +4509,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 94.8581899999999933 ;
-weighted_targets_sum = 0.632387933333333319 ;
-weighted_squared_targets_sum = 59.9871747338406607 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4553,8 +4553,8 @@
 length = 9 ;
 weights_sum = 0.0600000000000000047 ;
 targets_sum = 1027.74629000000004 ;
-weighted_targets_sum = 6.85164193333333404 ;
-weighted_squared_targets_sum = 783.278058371448765 ;
+weighted_targets_sum = 6.85164193333333316 ;
+weighted_squared_targets_sum = 783.278058371448651 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -4563,8 +4563,8 @@
 ;
 train_set = *8  ;
 leave = *278  ;
-leave_output = 2 [ 114.194032222222219 1 ] ;
-leave_error = 3 [ 1.72287732250623149 0 0.120000000000000009 ] ;
+leave_output = 2 [ 114.194032222222205 1 ] ;
+leave_error = 3 [ 1.72287732250634451 0 0.120000000000000009 ] ;
 split_col = 1 ;
 split_balance = 7 ;
 split_feature_value = 1.03719500000000009 ;
@@ -4786,10 +4786,10 @@
 leave = *291  ;
 leave_output = 2 [ 132.685306666666662 1 ] ;
 leave_error = 3 [ 0.212531887024959398 0 0.0400000000000000008 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 1 ;
-split_feature_value = 1.33162500000000006 ;
-after_split_error = 0.0504540736025838044 ;
+split_feature_value = 0.0478799999999999781 ;
+after_split_error = 0.0504540736027003778 ;
 missing_node = *0 ;
 missing_leave = *292 ->RegressionTreeLeave(
 id = 137 ;
@@ -4871,10 +4871,10 @@
 leave = *296  ;
 leave_output = 2 [ 150.242892500000011 1 ] ;
 leave_error = 3 [ 0.171230390915363406 0 0.0533333333333333368 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 2 ;
-split_feature_value = 1.42736499999999999 ;
-after_split_error = 0.0180043344341506151 ;
+split_feature_value = 1.30223500000000003 ;
+after_split_error = 0.0180043344344157363 ;
 missing_node = *0 ;
 missing_leave = *297 ->RegressionTreeLeave(
 id = 140 ;
@@ -5273,10 +5273,10 @@
 leave = *320  ;
 leave_output = 2 [ 201.471246666666701 1 ] ;
 leave_error = 3 [ 2.40226730640564057 0 0.0400000000000000008 ] ;
-split_col = 1 ;
+split_col = 3 ;
 split_balance = 1 ;
-split_feature_value = 1.97313999999999989 ;
-after_split_error = 0.166055228482320016 ;
+split_feature_value = 1.45652000000000004 ;
+after_split_error = 0.166055228482390183 ;
 missing_node = *0 ;
 missing_leave = *321 ->RegressionTreeLeave(
 id = 92 ;
@@ -5305,11 +5305,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 392.368439999999964 ;
-weighted_targets_sum = 2.61578960000000027 ;
-weighted_squared_targets_sum = 513.259669974353301 ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 212.045299999999997 ;
+weighted_targets_sum = 1.41363533333333335 ;
+weighted_squared_targets_sum = 299.754728347266678 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5318,12 +5318,12 @@
 ;
 train_set = *8  ;
 leave = *323  ;
-leave_output = 2 [ 196.18422000000001 1 ] ;
-leave_error = 3 [ 0.166055228482390183 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 2.6831649999999998 ;
-after_split_error = 0 ;
+leave_output = 2 [ 212.045299999999997 1 ] ;
+leave_error = 3 [ 0 0 0 ] ;
+split_col = -1 ;
+split_balance = 2147483647 ;
+split_feature_value = 1.79769313486231571e+308 ;
+after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
 missing_leave = *324 ->RegressionTreeLeave(
 id = 221 ;
@@ -5349,11 +5349,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 193.688809999999961 ;
-weighted_targets_sum = 1.29125873333333341 ;
-weighted_squared_targets_sum = 250.102367461440622 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5367,11 +5367,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 198.679630000000003 ;
-weighted_targets_sum = 1.32453086666666686 ;
-weighted_squared_targets_sum = 263.157302512912679 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5390,11 +5390,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 212.045299999999997 ;
-weighted_targets_sum = 1.41363533333333335 ;
-weighted_squared_targets_sum = 299.754728347266678 ;
+length = 2 ;
+weights_sum = 0.0133333333333333342 ;
+targets_sum = 392.368439999999964 ;
+weighted_targets_sum = 2.61578960000000027 ;
+weighted_squared_targets_sum = 513.259669974353301 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5403,12 +5403,12 @@
 ;
 train_set = *8  ;
 leave = *328  ;
-leave_output = 2 [ 212.045299999999997 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
+leave_output = 2 [ 196.18422000000001 1 ] ;
+leave_error = 3 [ 0.166055228482390183 0 0.0266666666666666684 ] ;
+split_col = 3 ;
+split_balance = 0 ;
+split_feature_value = 2.6831649999999998 ;
+after_split_error = 0 ;
 missing_node = *0 ;
 missing_leave = *329 ->RegressionTreeLeave(
 id = 224 ;
@@ -5436,9 +5436,9 @@
 train_set = *8  ;
 length = 1 ;
 weights_sum = 0.00666666666666666709 ;
-targets_sum = 212.045299999999997 ;
-weighted_targets_sum = 1.41363533333333335 ;
-weighted_squared_targets_sum = 299.754728347266678 ;
+targets_sum = 193.688809999999961 ;
+weighted_targets_sum = 1.29125873333333341 ;
+weighted_squared_targets_sum = 250.102367461440622 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5452,11 +5452,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
+length = 1 ;
+weights_sum = 0.00666666666666666709 ;
+targets_sum = 198.679630000000003 ;
+weighted_targets_sum = 1.32453086666666686 ;
+weighted_squared_targets_sum = 263.157302512912679 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5523,11 +5523,11 @@
 loss_function_weight = 1 ;
 verbosity = 2 ;
 train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 287.378629999999987 ;
-weighted_targets_sum = 1.91585753333333342 ;
-weighted_squared_targets_sum = 550.576513204512594 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2 ;
 output = []
 ;
@@ -5565,12 +5565,12 @@
 verbosity = 2 ;
 maximum_number_of_nodes = 50 ;
 next_available_node = 35 ;
-nodes = 50 [ *85  *90  *157  *242  *205  *197  *309  *149  *170  *130  *104  *277  *192  *48  *213  *144  *69  *282  *109  *314  *290  *218  *175  *237  *117  *250  *255  *125  *53  *295  *229  *322  *80  *27  *64  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
+nodes = 50 [ *85  *90  *157  *242  *205  *197  *309  *144  *170  *130  *104  *277  *192  *48  *213  *314  *69  *282  *109  *149  *290  *218  *175  *237  *117  *250  *255  *125  *53  *295  *229  *327  *80  *27  *64  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
 ;
 first_leave = *13  ;
 first_leave_output = 2 [ 16.0069061333332954 1 ] ;
 first_leave_error = 3 [ 17733.6216534378291 0 2.00000000000000488 ] ;
-split_cols = 40 [ 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 2 3 3 3 1 3 3 1 2 1 3 3 3 1 3 1 1 1 1 3 3 ] ;
+split_cols = 40 [ 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 3 3 3 1 3 3 1 2 3 3 3 3 1 3 1 3 3 1 3 3 ] ;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 41 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -78,9 +78,9 @@
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
 sum_ = 192.890804746032558 ;
-sumsquare_ = 3592.51670061680807 ;
-sumcube_ = 2141.61368714041009 ;
-sumfourth_ = 201415.693085262843 ;
+sumsquare_ = 3592.51670061680898 ;
+sumcube_ = 2141.61368714040918 ;
+sumfourth_ = 201415.693085262814 ;
 min_ = -21.6326733333333436 ;
 max_ = 1 ;
 agmemin_ = 16 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -14,10 +14,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 10974.0895327640446 ;
-sumsquare_ = 76330936.5701396614 ;
-sumcube_ = 569842056881.908203 ;
-sumfourth_ = 4522877790823688 ;
+sum_ = 11204.3270149540895 ;
+sumsquare_ = 76349558.7474929094 ;
+sumcube_ = 569851029912.986938 ;
+sumfourth_ = 4522879206793045 ;
 min_ = 0.650915482711108107 ;
 max_ = 8311.49117606439722 ;
 agmemin_ = 17 ;
@@ -56,10 +56,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -21948.1790655280893 ;
-sumsquare_ = 305323746.280558646 ;
-sumcube_ = -4558736455055.26562 ;
-sumfourth_ = 72366044653179008 ;
+sum_ = -22408.6540299081789 ;
+sumsquare_ = 305398234.989971638 ;
+sumcube_ = -4558808239303.89551 ;
+sumfourth_ = 72366067308688720 ;
 min_ = -16621.9823521287944 ;
 max_ = -0.301830965422216213 ;
 agmemin_ = 35 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -61.8945591190477131 ;
-sumsquare_ = 40932.6337231527941 ;
-sumcube_ = -4578700.6447631754 ;
-sumfourth_ = 688955792.048339963 ;
+sum_ = -84.9389746746032586 ;
+sumsquare_ = 40750.1377623234584 ;
+sumcube_ = -4587593.35613886733 ;
+sumfourth_ = 688820550.749982357 ;
 min_ = -181.33475999999996 ;
 max_ = -0.61358666666666295 ;
 agmemin_ = 35 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-02-04 21:16:06 UTC (rev 8453)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/train_stats.psave	2008-02-04 21:22:39 UTC (rev 8454)
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 362.783279794880855 ;
-max_ = 362.783279794880855 ;
+min_ = 367.38802943868177 ;
+max_ = 367.38802943868177 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 362.783279794880855 ;
-last_ = 362.783279794880855 ;
+first_ = 367.38802943868177 ;
+last_ = 367.38802943868177 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = -724.56655958976171 ;
-max_ = -724.56655958976171 ;
+min_ = -733.776058877363539 ;
+max_ = -733.776058877363539 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = -724.56655958976171 ;
-last_ = -724.56655958976171 ;
+first_ = -733.776058877363539 ;
+last_ = -733.776058877363539 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -165,12 +165,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = -24.1796111823809525 ;
-max_ = -24.1796111823809525 ;
+min_ = -24.6404994934920616 ;
+max_ = -24.6404994934920616 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = -24.1796111823809525 ;
-last_ = -24.1796111823809525 ;
+first_ = -24.6404994934920616 ;
+last_ = -24.6404994934920616 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)



From lamblin at mail.berlios.de  Mon Feb  4 23:46:19 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:46:19 +0100
Subject: [Plearn-commits] r8455 - trunk/plearn_learners/online
Message-ID: <200802042246.m14MkJOp002324@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:46:18 +0100 (Mon, 04 Feb 2008)
New Revision: 8455

Modified:
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMClassificationModule.h
Log:
Basic minibatch version.


Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2008-02-04 21:22:39 UTC (rev 8454)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2008-02-04 22:46:18 UTC (rev 8455)
@@ -218,6 +218,18 @@
     output << target_layer->expectation;
 }
 
+void RBMClassificationModule::fprop(const Mat& inputs, Mat& outputs)
+{
+    int batch_size = inputs.length();
+    outputs.resize(batch_size, output_size);
+
+    for (int k=0; k<batch_size; k++)
+    {
+        Vec tmp_out = outputs(k);
+        fprop(inputs(k), tmp_out);
+    }
+}
+
 /* THIS METHOD IS OPTIONAL
 //! Adapt based on the output gradient: this method should only
 //! be called just after a corresponding fprop; it should be

Modified: trunk/plearn_learners/online/RBMClassificationModule.h
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.h	2008-02-04 21:22:39 UTC (rev 8454)
+++ trunk/plearn_learners/online/RBMClassificationModule.h	2008-02-04 22:46:18 UTC (rev 8455)
@@ -94,6 +94,7 @@
 
     //! given the input, compute the output (possibly resize it appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
+    virtual void fprop(const Mat& inputs, Mat& outputs);
 
     //! Adapt based on the output gradient: this method should only
     //! be called just after a corresponding fprop; it should be



From lamblin at mail.berlios.de  Mon Feb  4 23:47:08 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:47:08 +0100
Subject: [Plearn-commits] r8456 - trunk/plearn/base
Message-ID: <200802042247.m14Ml8Gd002427@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:47:07 +0100 (Mon, 04 Feb 2008)
New Revision: 8456

Modified:
   trunk/plearn/base/HelpSystem.cc
Log:
Fix formatting of one-line help strings.


Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2008-02-04 22:46:18 UTC (rev 8455)
+++ trunk/plearn/base/HelpSystem.cc	2008-02-04 22:47:07 UTC (rev 8456)
@@ -629,7 +629,7 @@
        && opt->level() <= OptionBase::getCurrentOptionLevel())
         s+= addprefix("# ", opt->optiontype() + ": " + opt->description())
             //+ addprefix("# ", "*OptionLevel: " + opt->levelString())
-            + opt->optionname() + " = " 
+            + "\n" + opt->optionname() + " = " 
             + getOptionDefaultVal(classname, optionname) + " ;\n\n";
 
     return s;



From lamblin at mail.berlios.de  Mon Feb  4 23:48:43 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:48:43 +0100
Subject: [Plearn-commits] r8457 - trunk/plearn_learners/online
Message-ID: <200802042248.m14Mmhxs002565@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:48:41 +0100 (Mon, 04 Feb 2008)
New Revision: 8457

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.cc
Log:
Fix initialisation of Gaussian layers (no need to explicitly call forget() anymore)


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-02-04 22:47:07 UTC (rev 8456)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-02-04 22:48:41 UTC (rev 8457)
@@ -323,6 +323,7 @@
 
 void RBMGaussianLayer::forget()
 {
+    clearStats();
     quad_coeff.fill( 1. );
     inherited::forget();
 }
@@ -350,17 +351,28 @@
 
 void RBMGaussianLayer::build_()
 {
+    bool needs_forget = false;
+
     if(share_quad_coeff)
         size_quad_coeff=1;
     else
         size_quad_coeff=size;
 
-    sigma.resize( size_quad_coeff );
-    sigma_is_up_to_date = false;
+    if (sigma.size() != size_quad_coeff)
+    {
+        sigma.resize( size_quad_coeff );
+        sigma_is_up_to_date = false;
+        quad_coeff.resize( size_quad_coeff );
+        needs_forget = true;
+    }
 
-    quad_coeff.resize( size_quad_coeff );
     quad_coeff_pos_stats.resize( size );
     quad_coeff_neg_stats.resize( size );
+
+    if (needs_forget)
+        forget();
+
+    clearStats();
 }
 
 void RBMGaussianLayer::build()

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2008-02-04 22:47:07 UTC (rev 8456)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2008-02-04 22:48:41 UTC (rev 8457)
@@ -118,7 +118,7 @@
         int init_pos = init_positions[i];
         PP<RBMLayer> layer = sub_layers[i];
         int layer_size = layer->size;
-        
+
         layer->setExpectationByRef( expectation.subVec(init_pos, layer_size) );
     }
 



From lamblin at mail.berlios.de  Mon Feb  4 23:50:31 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:50:31 +0100
Subject: [Plearn-commits] r8458 - trunk/doc
Message-ID: <200802042250.m14MoVsh002664@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:50:31 +0100 (Mon, 04 Feb 2008)
New Revision: 8458

Modified:
   trunk/doc/
Log:
ignore more generated files



Property changes on: trunk/doc
___________________________________________________________________
Name: svn:ignore
   - installation_guide
faq
tmp
machine_learning
programmers_guide
python_modules_html
tools_guide
users_guide
tutonly
LibraryReference-No-Dot
LibraryReference
LibraryReference-No-Source
*.dvi
*.ps
*.pdf


   + installation_guide
faq
tmp
machine_learning
programmers_guide
python_modules_html
tools_guide
users_guide
tutonly
LibraryReference-No-Dot
LibraryReference
LibraryReference-No-Source
*.dvi
*.ps
*.pdf
*.log
*.aux
*.toc
*.out




From lamblin at mail.berlios.de  Mon Feb  4 23:52:33 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:52:33 +0100
Subject: [Plearn-commits] r8459 - in trunk: plearn/vmat
	plearn_learners/regressors
Message-ID: <200802042252.m14MqXdl002962@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:52:33 +0100 (Mon, 04 Feb 2008)
New Revision: 8459

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Fix compilation warnings


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-04 22:50:31 UTC (rev 8458)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-04 22:52:33 UTC (rev 8459)
@@ -1956,8 +1956,8 @@
         }
         real tmean = tstats.mean();
         real lmean = lstats.mean();
-        real tstderror = sqrt(pow(tstats.stderror(),2.) + 
-                              pow(lstats.stderror(),2.));
+        real tstderror = sqrt(pow(tstats.stderror(), 2) + 
+                              pow(lstats.stderror(), 2));
         th = fabs(lmean-tmean)/tstderror;
         if(tstderror==0)
             PLWARNING("In VMatrix::compareStats - field %d(%s) have a"

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2008-02-04 22:50:31 UTC (rev 8458)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2008-02-04 22:52:33 UTC (rev 8459)
@@ -423,11 +423,13 @@
     // the confidence bounds.  If impossible, simply set missing-value for the
     // NLL cost.
     if (m_compute_confidence) {
+#ifdef BOUNDCHECK
         static const float PROBABILITY = pl_erf(1. / (2*sqrt(2.0)));  // 0.5 stddev
         bool confavail = computeConfidenceFromOutput(input, output, PROBABILITY,
                                                      m_intervals);
-        assert( confavail && m_intervals.size() == output.size() &&
-                output.size() == target.size() );
+#endif
+        PLASSERT( confavail && m_intervals.size() == output.size() &&
+                  output.size() == target.size() );
         static const real LN_2PI_OVER_2 = pl_log(2*M_PI) / 2.0;
         real nll = 0;
         for (int i=0, n=output.size() ; i<n ; ++i) {



From lamblin at mail.berlios.de  Mon Feb  4 23:55:14 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:55:14 +0100
Subject: [Plearn-commits] r8460 -
	trunk/plearn/base/test/.pytest/PL_stringutils
Message-ID: <200802042255.m14MtEmM003234@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:55:14 +0100 (Mon, 04 Feb 2008)
New Revision: 8460

Modified:
   trunk/plearn/base/test/.pytest/PL_stringutils/
Log:
Ignore PSAVEDIFF directory.



Property changes on: trunk/plearn/base/test/.pytest/PL_stringutils
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results

   + .plearn
run_results
PSAVEDIFF




From lamblin at mail.berlios.de  Mon Feb  4 23:56:42 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:56:42 +0100
Subject: [Plearn-commits] r8461 -
	trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree
Message-ID: <200802042256.m14Mugj4003335@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:56:42 +0100 (Mon, 04 Feb 2008)
New Revision: 8461

Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/
Log:
Ignore PSAVEDIFF



Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results

   + .plearn
run_results
PSAVEDIFF




From lamblin at mail.berlios.de  Mon Feb  4 23:57:27 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:57:27 +0100
Subject: [Plearn-commits] r8462 - trunk/plearn_learners/online
Message-ID: <200802042257.m14MvRD1003408@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:57:27 +0100 (Mon, 04 Feb 2008)
New Revision: 8462

Added:
   trunk/plearn_learners/online/RBMTrainer.cc
   trunk/plearn_learners/online/RBMTrainer.h
Log:
Trains an RBM


Added: trunk/plearn_learners/online/RBMTrainer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTrainer.cc	2008-02-04 22:56:42 UTC (rev 8461)
+++ trunk/plearn_learners/online/RBMTrainer.cc	2008-02-04 22:57:27 UTC (rev 8462)
@@ -0,0 +1,535 @@
+// -*- C++ -*-
+
+// RBMTrainer.cc
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file RBMTrainer.cc */
+
+
+#include "RBMTrainer.h"
+
+#include <plearn_learners/online/RBMBinomialLayer.h>
+#include <plearn_learners/online/RBMGaussianLayer.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
+#include <plearn/vmat/AutoVMatrix.h>
+#include <plearn/vmat/SubVMatrix.h>
+#include <plearn/io/openFile.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMTrainer,
+    "Trains an RBM",
+    "Glop"
+    );
+
+RBMTrainer::RBMTrainer():
+    n_visible(-1),
+    n_hidden(-1),
+    visible_type("binomial"),
+    update_with_h0_sample(false),
+    sample_v1_in_chain(true),
+    compute_log_likelihood(true),
+    n_stages(1),
+    learning_rate(0.01),
+    seed(1827),
+    n_train(-1),
+    n_valid(-1),
+    n_test(-1),
+    batch_size(1),
+    print_debug(false),
+    use_fast_approximations(false),
+    n_ports(0),
+    n_state_ports(0),
+    nll_index(-1),
+    visible_index(-1),
+    rec_err_index(-1)
+{
+}
+
+// ### Nothing to add here, simply calls build_
+void RBMTrainer::build()
+{
+    inherited::build();
+    build_();
+}
+
+void RBMTrainer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(trainvec, copies);
+}
+
+void RBMTrainer::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "n_visible", &RBMTrainer::n_visible,
+                  OptionBase::buildoption,
+                  "n_visible");
+
+    declareOption(ol, "n_hidden", &RBMTrainer::n_hidden,
+                  OptionBase::buildoption,
+                  "n_hidden");
+
+    declareOption(ol, "visible_type", &RBMTrainer::visible_type,
+                  OptionBase::buildoption,
+                  "visible_type");
+
+    declareOption(ol, "update_with_h0_sample",
+                  &RBMTrainer::update_with_h0_sample,
+                  OptionBase::buildoption,
+                  "update_with_h0_sample");
+
+    declareOption(ol, "sample_v1_in_chain", &RBMTrainer::sample_v1_in_chain,
+                  OptionBase::buildoption,
+                  "sample_v1_in_chain");
+
+    declareOption(ol, "compute_log_likelihood",
+                  &RBMTrainer::compute_log_likelihood,
+                  OptionBase::buildoption,
+                  "compute_log_likelihood");
+
+    declareOption(ol, "n_stages", &RBMTrainer::n_stages,
+                  OptionBase::buildoption,
+                  "n_stages");
+
+    declareOption(ol, "learning_rate", &RBMTrainer::learning_rate,
+                  OptionBase::buildoption,
+                  "learning_rate");
+
+    declareOption(ol, "seed", &RBMTrainer::seed,
+                  OptionBase::buildoption,
+                  "seed");
+
+    declareOption(ol, "n_train", &RBMTrainer::n_train,
+                  OptionBase::buildoption,
+                  "n_train");
+
+    declareOption(ol, "n_valid", &RBMTrainer::n_valid,
+                  OptionBase::buildoption,
+                  "n_valid");
+
+    declareOption(ol, "n_test", &RBMTrainer::n_test,
+                  OptionBase::buildoption,
+                  "n_test");
+
+    declareOption(ol, "batch_size", &RBMTrainer::batch_size,
+                  OptionBase::buildoption,
+                  "batch_size");
+
+    declareOption(ol, "data_filename", &RBMTrainer::data_filename,
+                  OptionBase::buildoption,
+                  "data_filename");
+
+    declareOption(ol, "save_path", &RBMTrainer::save_path,
+                  OptionBase::buildoption,
+                  "save_path");
+
+    declareOption(ol, "save_name", &RBMTrainer::save_name,
+                  OptionBase::buildoption,
+                  "save_name");
+
+    declareOption(ol, "print_debug", &RBMTrainer::print_debug,
+                  OptionBase::buildoption,
+                  "print_debug");
+
+    declareOption(ol, "use_fast_approximations",
+                  &RBMTrainer::use_fast_approximations,
+                  OptionBase::buildoption,
+                  "use_fast_approximations");
+
+    declareOption(ol, "data", &RBMTrainer::data,
+                  OptionBase::learntoption,
+                  "data");
+
+    declareOption(ol, "train_input", &RBMTrainer::train_input,
+                  OptionBase::learntoption,
+                  "train_input");
+
+    declareOption(ol, "valid_input", &RBMTrainer::valid_input,
+                  OptionBase::learntoption,
+                  "valid_input");
+
+    declareOption(ol, "test_input", &RBMTrainer::test_input,
+                  OptionBase::learntoption,
+                  "test_input");
+
+    declareOption(ol, "rbm", &RBMTrainer::rbm,
+                  OptionBase::learntoption,
+                  "rbm");
+
+    declareOption(ol, "visible", &RBMTrainer::visible,
+                  OptionBase::learntoption,
+                  "visible");
+
+    declareOption(ol, "hidden", &RBMTrainer::hidden,
+                  OptionBase::learntoption,
+                  "hidden");
+
+    declareOption(ol, "connection", &RBMTrainer::connection,
+                  OptionBase::learntoption,
+                  "connection");
+
+    /*
+    declareOption(ol, "", &RBMTrainer::,
+                  OptionBase::learntoption,
+                  "");
+    */
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMTrainer::declareMethods(RemoteMethodMap& rmm)
+{
+    // Make sure that inherited methods are declared
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm, "NLL", &RBMTrainer::NLL,
+                  (BodyDoc("Computes NLL"),
+                   ArgDoc ("examples", "The examples"),
+                   RetDoc ("The NLL")
+                  ));
+
+    declareMethod(rmm, "recError", &RBMTrainer::recError,
+                  (BodyDoc("Computes reconstruction error"),
+                   ArgDoc ("examples", "The examples"),
+                   RetDoc ("The reconstruction error")
+                  ));
+
+    declareMethod(rmm, "CD1", &RBMTrainer::CD1,
+                  (BodyDoc("Performs one step of CD"),
+                   ArgDoc ("examples", "The examples")));
+}
+
+void RBMTrainer::build_()
+{
+    visible_type = lowerstring(visible_type);
+    PLCHECK( visible_type == "binomial" || visible_type == "gaussian" );
+
+    // visible
+    if (visible_type == "binomial")
+        visible = new RBMBinomialLayer();
+    else if (visible_type == "gaussian")
+        visible = new RBMGaussianLayer();
+    else
+        PLERROR("Unknown visible_type (%s).", visible_type.c_str());
+
+    visible->size = n_visible;
+    visible->setLearningRate(learning_rate);
+    visible->use_fast_approximations = use_fast_approximations;
+    visible->build();
+
+    // hidden
+    hidden = new RBMBinomialLayer();
+    hidden->size = n_hidden;
+    hidden->setLearningRate(learning_rate);
+    hidden->use_fast_approximations = use_fast_approximations;
+    hidden->build();
+
+    // connection
+    connection = new RBMMatrixConnection();
+    connection->down_size = n_visible;
+    connection->up_size = n_hidden;
+    connection->setLearningRate(learning_rate);
+    connection->use_fast_approximations = use_fast_approximations;
+    connection->build();
+
+    // RBM
+    rbm = new RBMModule();
+    rbm->visible_layer = visible;
+    rbm->hidden_layer = hidden;
+    rbm->connection = connection;
+    rbm->reconstruction_connection = connection;
+    rbm->compute_log_likelihood = compute_log_likelihood;
+    rbm->random_gen = new PRandom(seed);
+    rbm->cd_learning_rate = learning_rate;
+    rbm->use_fast_approximations = use_fast_approximations;
+    rbm->build();
+
+    // data
+    PP<AutoVMatrix> data_ = new AutoVMatrix();
+    data_->filename = data_filename;
+    data_->defineSizes(n_visible, 1);
+    data_->build();
+    data = get_pointer(data_);
+
+    // train_input
+    train_input = new SubVMatrix(data,      // source
+                                 0,         // istart
+                                 0,         // jstart
+                                 n_train,   // length
+                                 n_visible, // width
+                                 true       // call_build
+                                 );
+
+    // valid_input
+    valid_input = new SubVMatrix(data,
+                                 n_train,
+                                 0,
+                                 n_valid,
+                                 n_visible,
+                                 true
+                                 );
+
+    // valid_input
+    test_input = new SubVMatrix(data,
+                                n_train + n_valid,
+                                0,
+                                n_test,
+                                n_visible,
+                                true
+                                );
+
+    // ports
+    ports = rbm->getPorts();
+    n_ports = ports.length();
+    for (int i=0; i<n_ports; i++)
+    {
+        if (ports[i].find(".state", 0) != string::npos)
+        {
+            state_ports.append(ports[i]);
+            state_ports_indices.append(i);
+        }
+    }
+
+    n_state_ports = state_ports.length();
+    nll_index = rbm->getPortIndex("neg_log_likelihood");
+    visible_index = rbm->getPortIndex("visible");
+    rec_err_index = rbm->getPortIndex("reconstruction_error.state");
+
+    // nll_values
+    nll_values.resize(n_ports);
+    for (int i=0; i<n_state_ports; i++)
+        nll_values[state_ports_indices[i]] = new Mat();
+
+    // rec_err_values
+    rec_err_values.resize(n_ports);
+    for (int i=0; i<n_state_ports; i++)
+        rec_err_values[state_ports_indices[i]] = new Mat();
+}
+
+Mat RBMTrainer::NLL(const Mat& examples)
+{
+    Mat nll;
+
+    for (int i=0; i<n_state_ports; i++)
+        nll_values[state_ports_indices[i]]->resize(0,0);
+
+    nll_values[nll_index] = &nll;
+    nll_values[visible_index] = const_cast<Mat *>(&examples);
+
+    rbm->fprop(nll_values);
+
+    if (print_debug)
+    {
+        pout << "In NLL:" << endl;
+        for (int i=0; i<n_state_ports; i++)
+        {
+            int portnum = state_ports_indices[i];
+            string portname = rbm->getPortName(portnum);
+            pout << portname << ": " << nll_values[portnum]->size() << endl;
+        }
+    }
+
+    PLASSERT(nll.length() == examples.length() && nll.width() == 1);
+    return nll;
+}
+
+Mat RBMTrainer::recError(const Mat& examples)
+{
+    Mat rec_err;
+    for (int i=0; i<n_state_ports; i++)
+        rec_err_values[state_ports_indices[i]]->resize(0,0);
+
+    rec_err_values[rec_err_index] = &rec_err;
+    rec_err_values[visible_index] = const_cast<Mat *>(&examples);
+
+    rbm->fprop(rec_err_values);
+
+    if (print_debug)
+    {
+        pout << "In recError:" << endl;
+        for (int i=0; i<n_state_ports; i++)
+        {
+            int portnum = state_ports_indices[i];
+            string portname = rbm->getPortName(portnum);
+            pout << portname << ": " << rec_err_values[portnum]->size() << endl;
+        }
+    }
+
+    PLASSERT(rec_err.length() == examples.length()
+             && rec_err.width() == 1);
+    return rec_err;
+}
+
+void RBMTrainer::CD1(const Mat& examples)
+{
+    int n_examples = examples.length();
+    if (print_debug)
+    {
+        pout << "v0 = " << endl << examples << endl;
+    }
+
+    // Positive phase
+    connection->setAsDownInputs(examples);
+    hidden->getAllActivations(connection, 0, true);
+    hidden->computeExpectations();
+    hidden->generateSamples();
+
+    h0_a = hidden->activations;
+    h0_e.resize(n_examples, n_hidden);
+    h0_e << hidden->getExpectations();
+    h0_s.resize(n_examples, n_hidden);
+    h0_s << hidden->samples.copy();
+
+    if (update_with_h0_sample)
+        h0 = h0_s;
+    else
+        h0 = h0_e;
+
+    if (print_debug)
+    {
+        pout << "h0_a = " << endl << h0_a << endl;
+        pout << "h0_e = " << endl << h0_e << endl;
+        pout << "h0_s = " << endl << h0_s << endl;
+    }
+
+    // Downward pass
+    connection->setAsUpInputs(h0_s);
+    visible->getAllActivations(connection, 0, true);
+    visible->computeExpectations();
+    visible->generateSamples();
+
+    v1_a = visible->activations;
+    v1_e = visible->getExpectations();
+    v1_s = visible->samples;
+
+    // Negative phase
+    if (sample_v1_in_chain)
+        v1 = v1_s;
+    else
+        v1 = v1_e;
+
+    if (print_debug)
+    {
+        pout << "v1_a = " << endl << v1_a << endl;
+        pout << "v1_e = " << endl << v1_e << endl;
+        pout << "v1_s = " << endl << v1_s << endl;
+    }
+
+    connection->setAsDownInputs(v1);
+    hidden->getAllActivations(connection, 0, true);
+    hidden->computeExpectations();
+
+    Mat h1 = hidden->getExpectations();
+    if (print_debug)
+        pout << "h1 = " << endl << h1 << endl;
+
+    rbm->CDUpdate(examples, h0, v1, h1);
+}
+
+void RBMTrainer::run()
+{
+    Mat results(n_stages+1, 6);
+    for (int stage=0; stage<n_stages; stage++)
+    {
+        pout << "stage: " << stage << endl;
+        results(stage, 0) = mean(NLL(train_input));
+        results(stage, 3) = mean(recError(train_input));
+
+        if (n_valid > 0)
+        {
+            results(stage, 1) = mean(NLL(valid_input));
+            results(stage, 4) = mean(recError(valid_input));
+        }
+
+        if (n_test > 0)
+        {
+            results(stage, 2) = mean(NLL(test_input));
+            results(stage, 5) = mean(recError(test_input));
+        }
+
+        pout << "NLL:      " << results(stage).subVec(0,3) << endl;
+        pout << "RecError: " << results(stage).subVec(3,3) << endl;
+
+        for (int i=0; i<n_train/batch_size; i++)
+            CD1(train_input.subMatRows(i*batch_size, batch_size));
+    }
+    pout << "stage: " << n_stages << endl;
+    results(n_stages, 0) = mean(NLL(train_input));
+    results(n_stages, 3) = mean(recError(train_input));
+    if (n_valid > 0)
+    {
+        results(n_stages, 1) = mean(NLL(valid_input));
+        results(n_stages, 4) = mean(recError(valid_input));
+    }
+    if (n_test > 0)
+    {
+        results(n_stages, 2) = mean(NLL(test_input));
+        results(n_stages, 5) = mean(recError(test_input));
+    }
+    pout << "NLL:      " << results(n_stages).subVec(0,3) << endl;
+    pout << "RecError: " << results(n_stages).subVec(3,3) << endl;
+    pout << "results = " << endl << results << endl;
+
+    if (!save_path.isEmpty())
+    {
+        PPath filename;
+        if (save_name.isEmpty())
+            filename = save_path / "results.amat";
+        else
+            filename = save_path / save_name;
+
+        PStream file = openFile(filename, PStream::raw_ascii, "w");
+        file << results << endl;
+    }
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMTrainer.h
===================================================================
--- trunk/plearn_learners/online/RBMTrainer.h	2008-02-04 22:56:42 UTC (rev 8461)
+++ trunk/plearn_learners/online/RBMTrainer.h	2008-02-04 22:57:27 UTC (rev 8462)
@@ -0,0 +1,191 @@
+// -*- C++ -*-
+
+// RBMTrainer.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file RBMTrainer.h */
+
+
+#ifndef RBMTrainer_INC
+#define RBMTrainer_INC
+
+#include <plearn/base/Object.h>
+#include <plearn/vmat/VMat.h>
+#include <plearn_learners/online/RBMModule.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class RBMTrainer : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    int n_visible;
+    int n_hidden;
+    string visible_type;
+    bool update_with_h0_sample;
+    bool sample_v1_in_chain;
+    bool compute_log_likelihood;
+    int n_stages;
+    real learning_rate;
+    int32_t seed;
+    int n_train;
+    int n_valid;
+    int n_test;
+    int batch_size;
+    PPath data_filename;
+    PPath save_path;
+    PPath save_name;
+    bool print_debug;
+    bool use_fast_approximations;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMTrainer();
+
+    // Your other public member functions go here
+    Mat NLL(const Mat& examples);
+
+    Mat recError(const Mat& examples);
+
+    void CD1(const Mat& examples);
+
+    virtual void run();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMTrainer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    VMat data;
+    VMat train_input;
+    VMat valid_input;
+    VMat test_input;
+
+    PP<RBMModule> rbm;
+    PP<RBMLayer> visible;
+    PP<RBMLayer> hidden;
+    PP<RBMConnection> connection;
+
+    TVec<string> ports;
+    TVec<string> state_ports;
+    int n_ports;
+    int n_state_ports;
+    TVec<int> state_ports_indices;
+    int nll_index;
+    int visible_index;
+    int rec_err_index;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    static void declareMethods(RemoteMethodMap& rmm);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    mutable TVec<Mat*> nll_values;
+    mutable TVec<Mat*> rec_err_values;
+
+    mutable Mat h0_a;
+    mutable Mat h0_e;
+    mutable Mat h0_s;
+    mutable Mat h0;
+
+    mutable Mat v1_a;
+    mutable Mat v1_e;
+    mutable Mat v1_s;
+    mutable Mat v1;
+
+
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMTrainer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Mon Feb  4 23:58:26 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Mon, 4 Feb 2008 23:58:26 +0100
Subject: [Plearn-commits] r8463 - trunk/plearn_learners/online
Message-ID: <200802042258.m14MwQI6003488@sheep.berlios.de>

Author: lamblin
Date: 2008-02-04 23:58:26 +0100 (Mon, 04 Feb 2008)
New Revision: 8463

Added:
   trunk/plearn_learners/online/InferenceRBM.cc
   trunk/plearn_learners/online/InferenceRBM.h
Log:
Semi-supervised RBM, that can be used to predict hidden|input, hidden|(input, target) or target|input.


Added: trunk/plearn_learners/online/InferenceRBM.cc
===================================================================
--- trunk/plearn_learners/online/InferenceRBM.cc	2008-02-04 22:57:27 UTC (rev 8462)
+++ trunk/plearn_learners/online/InferenceRBM.cc	2008-02-04 22:58:26 UTC (rev 8463)
@@ -0,0 +1,406 @@
+// -*- C++ -*-
+
+// InferenceRBM.cc
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file InferenceRBM.cc */
+
+#define PL_LOG_MODULE_NAME "InferenceRBM"
+
+#include "InferenceRBM.h"
+#include <plearn/io/pl_log.h>
+#include <plearn/base/RemoteDeclareMethod.h>
+
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    InferenceRBM,
+    "RBM to be used when doing joint supervised learning by CD.",
+    "We have input, target and hidden layer. We can compute hidden given\n"
+    "(input, target), target given input, or hidden given input."
+    );
+
+InferenceRBM::InferenceRBM():
+    n_gibbs_steps(0),
+    input_size(0),
+    target_size(0),
+    visible_size(0),
+    hidden_size(0)
+{
+}
+
+// ### Nothing to add here, simply calls build_
+void InferenceRBM::build()
+{
+    inherited::build();
+    build_();
+}
+
+void InferenceRBM::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(trainvec, copies);
+
+    deepCopyField(input_layer, copies);
+    deepCopyField(target_layer, copies);
+    deepCopyField(hidden_layer, copies);
+    deepCopyField(visible_layer, copies);
+
+    deepCopyField(input_to_hidden, copies);
+    deepCopyField(target_to_hidden, copies);
+    deepCopyField(visible_to_hidden, copies);
+}
+
+void InferenceRBM::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "myoption", &InferenceRBM::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+
+
+    declareOption(ol, "input_layer", &InferenceRBM::input_layer,
+                  OptionBase::buildoption,
+                  "Input layer (part of visible)");
+
+    declareOption(ol, "target_layer", &InferenceRBM::target_layer,
+                  OptionBase::buildoption,
+                  "Target layer (part of visible)");
+
+    declareOption(ol, "hidden_layer", &InferenceRBM::hidden_layer,
+                  OptionBase::buildoption,
+                  "Hidden layer");
+
+    declareOption(ol, "input_to_hidden", &InferenceRBM::input_to_hidden,
+                  OptionBase::buildoption,
+                  "Connection between input and hidden layers");
+
+    declareOption(ol, "target_to_hidden", &InferenceRBM::target_to_hidden,
+                  OptionBase::buildoption,
+                  "Connection between target and hidden layers");
+
+    declareOption(ol, "exp_method", &InferenceRBM::exp_method,
+                  OptionBase::buildoption,
+                  "How to compute hidden and target expectation given input.\n"
+                  "Possible values are:\n"
+                  "    - \"exact\": exact inference, O(target_size), default\n"
+                  "    - \"gibbs\": estimation by Gibbs sampling\n"
+                  );
+
+    declareOption(ol, "n_gibbs_steps", &InferenceRBM::n_gibbs_steps,
+                  OptionBase::buildoption,
+                  "Number of Gibbs steps to use if exp_method==\"gibbs\"");
+
+    declareOption(ol, "random_gen", &InferenceRBM::random_gen,
+                  OptionBase::buildoption,
+                  "Random numbers generator");
+
+
+    declareOption(ol, "visible_layer", &InferenceRBM::visible_layer,
+                  OptionBase::learntoption,
+                  "Visible layer (input+target)");
+
+    declareOption(ol, "visible_to_hidden", &InferenceRBM::visible_to_hidden,
+                  OptionBase::learntoption,
+                  "Connection between visible and hidden layers");
+
+    declareOption(ol, "input_size", &InferenceRBM::input_size,
+                  OptionBase::learntoption,
+                  "Size of input_layer");
+
+    declareOption(ol, "target_size", &InferenceRBM::target_size,
+                  OptionBase::learntoption,
+                  "Size of target_layer");
+
+    declareOption(ol, "visible_size", &InferenceRBM::visible_size,
+                  OptionBase::learntoption,
+                  "Size of visible_layer");
+
+    declareOption(ol, "hidden_size", &InferenceRBM::hidden_size,
+                  OptionBase::learntoption,
+                  "Size of hidden_layer");
+
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+////////////////////
+// declareMethods //
+////////////////////
+void InferenceRBM::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "hiddenExpGivenVisible",
+        &InferenceRBM::hiddenExpGivenVisible,
+        (BodyDoc("Computes the hidden layer's expectation given the visible"),
+         ArgDoc ("visible", "Visible layer's values")));
+
+    declareMethod(
+        rmm, "hiddenExpGivenInput",
+        &InferenceRBM::hiddenExpGivenInput,
+        (BodyDoc("Computes the hidden layer's expectation given the input"),
+         ArgDoc ("input", "Input layer's values")));
+
+    declareMethod(
+        rmm, "targetExpGivenInput",
+        &InferenceRBM::targetExpGivenInput,
+        (BodyDoc("Computes the target layer's expectation given the input"),
+         ArgDoc ("input", "Input layer's values")));
+
+    declareMethod(
+        rmm, "getHiddenExpGivenVisible",
+        &InferenceRBM::getHiddenExpGivenVisible,
+        (BodyDoc("Computes the hidden layer's expectation given the visible"),
+         ArgDoc ("visible", "Visible layer's values"),
+         RetDoc ("Hidden layer's expectation")));
+
+    declareMethod(
+        rmm, "getHiddenExpGivenInput",
+        &InferenceRBM::getHiddenExpGivenInput,
+        (BodyDoc("Computes the hidden layer's expectation given the input"),
+         ArgDoc ("input", "Input layer's values"),
+         RetDoc ("Hidden layer's expectation")));
+
+    declareMethod(
+        rmm, "getTargetExpGivenInput",
+        &InferenceRBM::getTargetExpGivenInput,
+        (BodyDoc("Computes the target layer's expectation given the input"),
+         ArgDoc ("input", "Input layer's values"),
+         RetDoc ("Target layer's expectation")));
+}
+
+
+void InferenceRBM::build_()
+{
+    MODULE_LOG << "build_() called" << endl;
+
+    if( !input_layer || !target_layer || !hidden_layer
+        || !input_to_hidden || !target_to_hidden )
+    {
+        MODULE_LOG << "build_() aborted because layers and connections were"
+            " not set" << endl;
+        return;
+    }
+
+    //! Check (and set) sizes
+    input_size = input_layer->size;
+    target_size = target_layer->size;
+    visible_size = input_size + target_size;
+    hidden_size = hidden_layer->size;
+
+    PLASSERT(input_to_hidden->down_size == input_size);
+    PLASSERT(input_to_hidden->up_size == hidden_size);
+    PLASSERT(target_to_hidden->down_size == target_size);
+    PLASSERT(target_to_hidden->up_size == hidden_size);
+
+    //! Build visible layer
+    visible_layer = new RBMMixedLayer();
+    visible_layer->sub_layers.resize(2);
+    visible_layer->sub_layers[0] = input_layer;
+    visible_layer->sub_layers[1] = target_layer;
+    visible_layer->build();
+    PLASSERT(visible_layer->size == visible_size);
+
+    //! Build visible_to_hidden connection
+    visible_to_hidden = new RBMMixedConnection();
+    visible_to_hidden->sub_connections.resize(1,2);
+    visible_to_hidden->sub_connections(0,0) = input_to_hidden;
+    visible_to_hidden->sub_connections(0,1) = target_to_hidden;
+    visible_to_hidden->build();
+    PLASSERT(visible_to_hidden->down_size == visible_size);
+    PLASSERT(visible_to_hidden->up_size == hidden_size);
+
+    //! If we have a random_gen, share it with the ones who do not
+    if (random_gen)
+    {
+       if (input_layer->random_gen.isNull())
+       {
+           input_layer->random_gen = random_gen;
+           input_layer->forget();
+       }
+       if (target_layer->random_gen.isNull())
+       {
+           target_layer->random_gen = random_gen;
+           target_layer->forget();
+       }
+       if (visible_layer->random_gen.isNull())
+       {
+           visible_layer->random_gen = random_gen;
+           visible_layer->forget();
+       }
+       if (hidden_layer->random_gen.isNull())
+       {
+           hidden_layer->random_gen = random_gen;
+           hidden_layer->forget();
+       }
+       if (input_to_hidden->random_gen.isNull())
+       {
+           input_to_hidden->random_gen = random_gen;
+           input_to_hidden->forget();
+       }
+       if (target_to_hidden->random_gen.isNull())
+       {
+           target_to_hidden->random_gen = random_gen;
+           target_to_hidden->forget();
+       }
+       if (visible_to_hidden->random_gen.isNull())
+       {
+           visible_to_hidden->random_gen = random_gen;
+           visible_to_hidden->forget();
+       }
+    }
+
+}
+
+void InferenceRBM::hiddenExpGivenVisible(const Mat& visible)
+{
+    PLASSERT(visible.width() == visible_size);
+
+    visible_to_hidden->setAsDownInputs(visible);
+    hidden_layer->getAllActivations(get_pointer(visible_to_hidden), 0, true);
+    hidden_layer->computeExpectations();
+}
+
+void InferenceRBM::targetExpGivenInput(const Mat& input)
+{
+    PLASSERT(input.width() == input_size);
+    int batch_size = input.length();
+
+    // input contains samples (or expectations) from input_layer
+    input_to_hidden->setAsDownInputs(input);
+
+    // hidden_layer->activations = bias + input_to_hidden.weights * input
+    hidden_layer->getAllActivations(get_pointer(input_to_hidden), 0, true);
+
+    target_layer->setBatchSize(batch_size);
+
+    // target_layer->activations[k][i] =
+    //      bias[i] + sum_j softplus(W_ji + hidden_layer->activations[k][j])
+    Mat hidden_act = hidden_layer->activations;
+    Mat target_act = target_layer->activations;
+    Vec target_b = target_layer->bias;
+    Mat t_to_h_w = target_to_hidden->weights;
+
+    for (int k=0; k<batch_size; k++)
+    {
+        target_act(k) << target_b;
+
+        real* target_act_k = target_act[k];
+        real* hidden_act_kj = hidden_act[k];
+        for (int j=0; j<hidden_size; j++, hidden_act_kj++)
+        {
+            real* target_act_ki = target_act_k; // copy
+            real* t_to_h_w_ji = t_to_h_w[j];
+            for (int i=0; i<target_size; i++, target_act_ki++, t_to_h_w_ji++)
+            {
+                PLASSERT(*target_act_ki == target_act(k,i));
+                PLASSERT(*t_to_h_w_ji == t_to_h_w(j,i));
+                PLASSERT(*hidden_act_kj == hidden_act(k,j));
+
+                *target_act_ki += softplus(*t_to_h_w_ji + *hidden_act_kj);
+            }
+        }
+    }
+
+    target_layer->expectations_are_up_to_date = false;
+    target_layer->computeExpectations();
+}
+
+void InferenceRBM::hiddenExpGivenInput(const Mat& input)
+{
+    PLASSERT(input.width() == input_size);
+    int batch_size = input.length();
+
+    targetExpGivenInput(input);
+    Mat target_exp = target_layer->getExpectations();
+
+    Mat visible(batch_size, visible_size);
+    visible.subMatColumns(0, input_size) << input;
+
+    Mat hidden_exp(batch_size, hidden_size);
+
+    for (int i=0; i<target_size; i++)
+    {
+        visible.subMatColumns(input_size, target_size).clear();
+        visible.column(input_size+i).fill(1.);
+
+        hiddenExpGivenVisible(visible);
+
+        for (int k=0; k<batch_size; k++)
+            hidden_exp(k) += target_exp(k,i) * hidden_layer->getExpectations()(k);
+    }
+
+    hidden_layer->setExpectations(hidden_exp);
+}
+
+Mat InferenceRBM::getHiddenExpGivenVisible(const Mat& visible)
+{
+    hiddenExpGivenVisible(visible);
+    return hidden_layer->getExpectations();
+}
+
+Mat InferenceRBM::getTargetExpGivenInput(const Mat& input)
+{
+    targetExpGivenInput(input);
+    return target_layer->getExpectations();
+}
+
+Mat InferenceRBM::getHiddenExpGivenInput(const Mat& input)
+{
+    hiddenExpGivenInput(input);
+    return hidden_layer->getExpectations();
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/InferenceRBM.h
===================================================================
--- trunk/plearn_learners/online/InferenceRBM.h	2008-02-04 22:57:27 UTC (rev 8462)
+++ trunk/plearn_learners/online/InferenceRBM.h	2008-02-04 22:58:26 UTC (rev 8463)
@@ -0,0 +1,189 @@
+// -*- C++ -*-
+
+// InferenceRBM.h
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file InferenceRBM.h */
+
+
+#ifndef InferenceRBM_INC
+#define InferenceRBM_INC
+
+#include <plearn/base/Object.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMultinomialLayer.h>
+#include <plearn_learners/online/RBMBinomialLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMMatrixConnection.h>
+#include <plearn_learners/online/RBMMixedConnection.h>
+
+namespace PLearn {
+
+/**
+ * RBM to be used when doing joint supervised learning by CD.
+ * We have input, target and hidden layer. We can compute hidden given
+ * (input, target), target given input, or hidden given input.
+ *
+ * @todo Write class to-do's here if there are any.
+ */
+class InferenceRBM : public Object
+{
+    typedef Object inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Input layer (part of visible)
+    PP<RBMLayer> input_layer;
+
+    //! Target layer (other part of visible)
+    PP<RBMMultinomialLayer> target_layer;
+
+    //! Hidden
+    PP<RBMBinomialLayer> hidden_layer;
+
+    //! Connection between input and hidden
+    PP<RBMMatrixConnection> input_to_hidden;
+
+    //! Connection between target and hidden
+    PP<RBMMatrixConnection> target_to_hidden;
+
+    //! How to compute hidden and target expectation given input
+    //! Possible values are:
+    //!     - "exact": exact inference, O(target_size), default
+    //!     - "gibbs": estimation by Gibbs sampling
+    string exp_method;
+
+    //! Number of Gibbs steps to use if exp_method=="gibbs"
+    int n_gibbs_steps;
+
+    //! Random numbers generator
+    PP<PRandom> random_gen;
+
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    InferenceRBM();
+
+    // Your other public member functions go here
+
+    void hiddenExpGivenVisible(const Mat& visible);
+    void hiddenExpGivenInput(const Mat& input);
+    void targetExpGivenInput(const Mat& input);
+
+    Mat getHiddenExpGivenVisible(const Mat& visible);
+    Mat getHiddenExpGivenInput(const Mat& input);
+    Mat getTargetExpGivenInput(const Mat& input);
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(InferenceRBM);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    //! Visible layer (concatenation of input and target)
+    PP<RBMMixedLayer> visible_layer;
+
+    PP<RBMMixedConnection> visible_to_hidden;
+
+    //! Size of input layer
+    int input_size;
+
+    //! Size of target layer
+    int target_size;
+
+    //! Size of visible layer
+    int visible_size;
+
+    //! Size of hidden layer
+    int hidden_size;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Declares the class methods.
+    static void declareMethods(RemoteMethodMap& rmm);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(InferenceRBM);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Tue Feb  5 00:00:22 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 5 Feb 2008 00:00:22 +0100
Subject: [Plearn-commits] r8464 - trunk/commands
Message-ID: <200802042300.m14N0Mn3003765@sheep.berlios.de>

Author: lamblin
Date: 2008-02-05 00:00:21 +0100 (Tue, 05 Feb 2008)
New Revision: 8464

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Include RBMTrainer and InferenceRBM.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-02-04 22:58:26 UTC (rev 8463)
+++ trunk/commands/plearn_noblas_inc.h	2008-02-04 23:00:21 UTC (rev 8464)
@@ -208,6 +208,7 @@
 #include <plearn_learners/online/ForwardModule.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
 #include <plearn_learners/online/IdentityModule.h>
+#include <plearn_learners/online/InferenceRBM.h>
 #include <plearn_learners/online/LayerCostModule.h>
 #include <plearn_learners/online/LinearCombinationModule.h>
 #include <plearn_learners/online/LinearFilterModule.h>
@@ -234,6 +235,7 @@
 #include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMModule.h>
 #include <plearn_learners/online/RBMMultinomialLayer.h>
+#include <plearn_learners/online/RBMTrainer.h>
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SplitModule.h>



From nouiz at mail.berlios.de  Tue Feb  5 17:37:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Feb 2008 17:37:39 +0100
Subject: [Plearn-commits] r8465 - in trunk/plearn_learners/regressors: .
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata
	test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata
Message-ID: <200802051637.m15GbdWm023955@sheep.berlios.de>

Author: nouiz
Date: 2008-02-05 17:37:38 +0100 (Tue, 05 Feb 2008)
New Revision: 8465

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
Log:
-bugfix in RegressionTreeMulticlassLeave
-bugfix in RegressionTreeNode
-added cost class_error
-added PLASSERT
-code cleanup


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-02-05 16:37:38 UTC (rev 8465)
@@ -101,12 +101,18 @@
                   "The heap to store potential nodes to expand\n");
     declareOption(ol, "first_leave", &RegressionTree::first_leave, OptionBase::learntoption,
                   "The first leave built with the root containing all train set rows at the beginning\n");
-    declareOption(ol, "first_leave_output", &RegressionTree::first_leave_output, OptionBase::learntoption,
-                  "The vector to compute the ouput and the confidence function of the first leave.\n");
-    declareOption(ol, "first_leave_error", &RegressionTree::first_leave_error, OptionBase::learntoption,
-                  "The vector to compute the errors of the first leave.\n");
     declareOption(ol, "split_cols", &RegressionTree::split_cols, OptionBase::learntoption,
                   "contain in order of first to last the columns used to split the tree.\n");
+
+
+    declareOption(ol, "first_leave_output", &RegressionTree::tmp_vec,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED\n");
+    declareOption(ol, "first_leave_error", &RegressionTree::tmp_vec,
+                  OptionBase::learntoption | OptionBase::nosave,
+                  "DEPRECATED\n");
+
+
     inherited::declareOptions(ol);
 }
 
@@ -124,9 +130,8 @@
     deepCopyField(root, copies);
     deepCopyField(priority_queue, copies);
     deepCopyField(first_leave, copies);
-    deepCopyField(first_leave_output, copies);
-    deepCopyField(first_leave_error, copies);
     deepCopyField(split_cols, copies);
+    deepCopyField(tmp_vec, copies);
 }
 
 void RegressionTree::build()
@@ -137,36 +142,40 @@
 
 void RegressionTree::build_()
 {
+    PP<VMatrix> the_train_set;
     if(sorted_train_set)
     {
-        length = sorted_train_set->length();
-        
-        if (length < 1) PLERROR("RegressionTree: the training set must contain at least one sample, got %d", length);
-        inputsize = sorted_train_set->inputsize();
-        targetsize = sorted_train_set->targetsize();
-        weightsize = sorted_train_set->weightsize();
-        if (inputsize < 1) PLERROR("RegressionTree: expected  inputsize greater than 0, got %d", inputsize);
-        if (targetsize != 1) PLERROR("RegressionTree: expected targetsize to be 1, got %d", targetsize);
-        if (weightsize != 1 && weightsize != 0)  PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d", weightsize);
-        sample_input.resize(inputsize);
-        sample_target.resize(targetsize);
+        the_train_set = sorted_train_set;
     }
     else if (train_set)
     { 
-        length = train_set->length();
-        if (length < 1) PLERROR("RegressionTree: the training set must contain at least one sample, got %d", length);
-        inputsize = train_set->inputsize();
-        targetsize = train_set->targetsize();
-        weightsize = train_set->weightsize();
-        if (inputsize < 1) PLERROR("RegressionTree: expected  inputsize greater than 0, got %d", inputsize);
-        if (targetsize != 1) PLERROR("RegressionTree: expected targetsize to be 1, got %d", targetsize);
-        if (weightsize != 1 && weightsize != 0)  PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d", weightsize);
+        the_train_set = train_set;
+    }
+    if(the_train_set)
+    {
+        length = the_train_set->length();
+        int inputsize = the_train_set->inputsize();
+        int targetsize = the_train_set->targetsize();
+        int weightsize = the_train_set->weightsize();
+
+        if (length < 1)
+            PLERROR("RegressionTree: the training set must contain at least one"
+                    " sample, got %d", length);
+        if (inputsize < 1)
+            PLERROR("RegressionTree: expected  inputsize greater than 0, got %d",
+                    inputsize);
+        if (targetsize != 1)
+            PLERROR("RegressionTree: expected targetsize to be 1,"" got %d",
+                    targetsize);
+        if (weightsize != 1 && weightsize != 0)
+            PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d",
+                    weightsize);
         sample_input.resize(inputsize);
         sample_target.resize(targetsize);
+        sample_output.resize(outputsize());
     }
+    sample_costs.resize(getTestCostNames().size());
 
-    sample_output.resize(2);
-    sample_costs.resize(4);
     if (loss_function_weight != 0.0)
     {
         l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
@@ -192,8 +201,8 @@
         if (stage > 0)
         {
             int split_col = expandTree();
+            if (split_col < 0) break;
             split_cols.append(split_col);
-            if (split_col < 0) break;
         }
         if (report_progress) pb->update(stage);
     }
@@ -203,13 +212,16 @@
         pb = new ProgressBar("RegressionTree : computing the statistics: ", length);
     } 
     train_stats->forget();
-    for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
+    real sample_weight;
+        
+    for (int train_sample_index = 0; train_sample_index < length;
+         train_sample_index++)
     {  
-        sorted_train_set->getExample(each_train_sample_index, sample_input, sample_target, sample_weight);
+        sorted_train_set->getExample(train_sample_index, sample_input, sample_target, sample_weight);
         computeOutput(sample_input, sample_output);
         computeCostsFromOutputs(sample_input, sample_output, sample_target, sample_costs); 
         train_stats->update(sample_costs);
-        if (report_progress) pb->update(each_train_sample_index);
+        if (report_progress) pb->update(train_sample_index);
     }
     train_stats->finalize();
     verbose("split_cols: "+tostring(split_cols),2);
@@ -246,16 +258,15 @@
     leave_template->setOption("verbosity", tostring(verbosity));
     leave_template->initStats();
 
-    first_leave_output.resize(2);
-    first_leave_error.resize(3);
     first_leave = ::PLearn::deepCopy(leave_template);
     first_leave->setOption("id", tostring(sorted_train_set->getNextId()));
     first_leave->initLeave(sorted_train_set);
 
-    for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
+    for (int train_sample_index = 0; train_sample_index < length;
+         train_sample_index++)
     {
-        first_leave->addRow(each_train_sample_index, first_leave_output, first_leave_error);
-        first_leave->registerRow(each_train_sample_index);
+        first_leave->addRow(train_sample_index);
+        first_leave->registerRow(train_sample_index);
     }
     root = new RegressionTreeNode();
     root->setOption("missing_is_valid", tostring(missing_is_valid));
@@ -278,7 +289,7 @@
         verbose("RegressionTree: priority queue empty, stage: " + tostring(stage), 3);
         return -1;
     }
-    node = priority_queue->popHeap();
+    PP<RegressionTreeNode> node = priority_queue->popHeap();
     if (node->getErrorImprovment() < complexity_penalty_factor * sqrt((real)stage))
     {
         verbose("RegressionTree: early stopping at stage: " + tostring(stage)
@@ -312,11 +323,12 @@
 
 TVec<string> RegressionTree::getTrainCostNames() const
 {
-    TVec<string> return_msg(4);
+    TVec<string> return_msg(5);
     return_msg[0] = "mse";
     return_msg[1] = "base_confidence";
     return_msg[2] = "base_reward_l2";
     return_msg[3] = "base_reward_l1";
+    return_msg[4] = "class_error";
     return return_msg;
 }
 
@@ -349,6 +361,7 @@
     costsv[1] = outputv[1];
     costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
     costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));
+    costsv[4] = !fast_is_equal(targetv[0],outputv[0]);
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-02-05 16:37:38 UTC (rev 8465)
@@ -76,28 +76,21 @@
     PP<RegressionTreeNode> root;
     PP<RegressionTreeLeave> first_leave;
     PP<RegressionTreeQueue> priority_queue;
-    Vec first_leave_output;
-    Vec first_leave_error;
  
 /*
   Work fields: they are sized and initialized if need be, at buid time
 */  
  
     int length;
-    int inputsize;
-    int targetsize;
-    int weightsize;
     real l2_loss_function_factor;
     real l1_loss_function_factor;
-    int each_train_sample_index;
     Vec sample_input;
     Vec sample_output;
     Vec sample_target;
     Vec sample_costs;
-    real sample_weight;
-    PP<RegressionTreeNode> node;
     TVec<int> split_cols;
 
+    Vec tmp_vec;
 public:
     RegressionTree();
     virtual              ~RegressionTree();

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-02-05 16:37:38 UTC (rev 8465)
@@ -159,12 +159,18 @@
     getOutputAndError(outputv,errorv);
 }
 
-void RegressionTreeMulticlassLeave::getOutputAndError(Vec output, Vec error)
+void RegressionTreeMulticlassLeave::getOutputAndError(Vec& output, Vec& error)
 {
 #ifdef BOUNDCHECK
     if(multiclass_outputs.length()<=0)
         PLERROR("In RegressionTreeMulticlassLeave::getOutputAndError() - multiclass_outputs must not be empty");
 #endif
+    if(length==0){        
+        output.clear();
+        output[0]=MISSING_VALUE;
+        error.clear();
+        return;
+    }
     multiclass_winer = 0;
     for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-02-05 16:37:38 UTC (rev 8465)
@@ -86,7 +86,7 @@
     void         addRow(int row, Vec outputv, Vec errorv);
     void         addRow(int row);
     void         removeRow(int row, Vec outputv, Vec errorv);
-    virtual void getOutputAndError(Vec output, Vec error);
+    virtual void getOutputAndError(Vec& output, Vec& error);
     void         printStats();
   
 private:

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-05 16:37:38 UTC (rev 8465)
@@ -222,6 +222,8 @@
     Vec left_error(3);
     Vec right_error(3);
     Vec missing_error(3);
+    missing_error.clear();
+
     int leave_id = leave->id;
     for (int col = 0; col < train_set->inputsize(); col++)
     {
@@ -351,7 +353,9 @@
 real RegressionTreeNode::getErrorImprovment()
 {
     if (split_col < 0) return -1.0;
-    return leave_error[0] + leave_error[1] - after_split_error;
+    real err=leave_error[0] + leave_error[1] - after_split_error;
+    PLASSERT(!isnan(err)&&err>=0);
+    return err;
 }
 
 TVec< PP<RegressionTreeNode> > RegressionTreeNode::getNodes()

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 149 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -2 ;
+sumsquare_ = 2 ;
+sumcube_ = -2 ;
+sumfourth_ = 2 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 120 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -2 ;
+sumsquare_ = 2 ;
+sumcube_ = -2 ;
+sumfourth_ = 2 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 120 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 120 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 120 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -5568,8 +5568,6 @@
 nodes = 50 [ *85  *90  *157  *242  *205  *197  *309  *144  *170  *130  *104  *277  *192  *48  *213  *314  *69  *282  *109  *149  *290  *218  *175  *237  *117  *250  *255  *125  *53  *295  *229  *327  *80  *27  *64  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
 ;
 first_leave = *13  ;
-first_leave_output = 2 [ 16.0069061333332954 1 ] ;
-first_leave_error = 3 [ 17733.6216534378291 0 2.00000000000000488 ] ;
 split_cols = 40 [ 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 3 3 3 1 3 3 1 2 3 3 3 3 1 3 1 3 3 1 3 3 ] ;
 random_gen = *0 ;
 seed = 1827 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-02-05 16:37:38 UTC (rev 8465)
@@ -2,3 +2,4 @@
 base_confidence	0
 base_reward_l2	0
 base_reward_l1	0
+class_error	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 150 ;
+sumsquarew_ = 150 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 120 ;
+agemax_ = 149 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-02-05 16:37:38 UTC (rev 8465)
@@ -2,3 +2,4 @@
 base_confidence	0
 base_reward_l2	0
 base_reward_l1	0
+class_error	0

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_stats.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,13 +1,13 @@
 *1 ->VecStatsCollector(
 maxnvalues = 0 ;
-fieldnames = 4 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" ] ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
 full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
-stats = 4 [ StatsCollector(
+stats = 5 [ StatsCollector(
 epsilon = 0 ;
 maxnvalues = 0 ;
 no_removal_warnings = 0 ;
@@ -91,6 +91,27 @@
 integer_ = 0 ;
 counts = {};
 more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 50 ;
+sumsquarew_ = 50 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 1 ;
+max_ = 1 ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
 ] ;
 cov = 0  0  [ 
 ]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-02-05 16:37:38 UTC (rev 8465)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8439"
+__REVISION__ = "PL8454"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/test_cost_names.txt	2008-02-05 16:37:38 UTC (rev 8465)
@@ -2,3 +2,4 @@
 base_confidence
 base_reward_l2
 base_reward_l1
+class_error

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-02-04 23:00:21 UTC (rev 8464)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-02-05 16:37:38 UTC (rev 8465)
@@ -65,10 +65,6 @@
 root = *0 ;
 priority_queue = *0 ;
 first_leave = *0 ;
-first_leave_output = []
-;
-first_leave_error = []
-;
 split_cols = []
 ;
 random_gen = *0 ;



From nouiz at mail.berlios.de  Tue Feb  5 18:21:53 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Feb 2008 18:21:53 +0100
Subject: [Plearn-commits] r8466 - trunk/speedtest
Message-ID: <200802051721.m15HLrLu030240@sheep.berlios.de>

Author: nouiz
Date: 2008-02-05 18:21:53 +0100 (Tue, 05 Feb 2008)
New Revision: 8466

Modified:
   trunk/speedtest/speedtest.FloatVsDouble
   trunk/speedtest/speedtest.NatGradNNet
   trunk/speedtest/speedtest.NbThread
   trunk/speedtest/speedtest.OptVsDbg
   trunk/speedtest/speedtest.gemm
   trunk/speedtest/speedtest.gemv
   trunk/speedtest/speedtest.general
Log:
better result filename


Modified: trunk/speedtest/speedtest.FloatVsDouble
===================================================================
--- trunk/speedtest/speedtest.FloatVsDouble	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.FloatVsDouble	2008-02-05 17:21:53 UTC (rev 8466)
@@ -5,7 +5,8 @@
 SIZE=( 100 200 400 800 1023 1024 )
 TITLE="Comparaison of execution speed of float vs double program on $HOSTNAME"
 DATE=`date +%s`
-RESFILE=RES/floatvsdouble_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
 
 
 function f(){

Modified: trunk/speedtest/speedtest.NatGradNNet
===================================================================
--- trunk/speedtest/speedtest.NatGradNNet	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.NatGradNNet	2008-02-05 17:21:53 UTC (rev 8466)
@@ -12,7 +12,8 @@
 TITLE="Comparison of execution time of different programme on $HOSTNAME"
 FNAME=${BP//.\//_}
 DATE=`date +%s`
-RESFILE=RES/NatGradNNet_${HOSTNAME%.iro.umontreal.ca}_${FNAME//\//_}_${DATE}_${SIZE2}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${FNAME//\//_}_${DATE}_${SIZE2}
 
 
 function f(){

Modified: trunk/speedtest/speedtest.NbThread
===================================================================
--- trunk/speedtest/speedtest.NbThread	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.NbThread	2008-02-05 17:21:53 UTC (rev 8466)
@@ -12,7 +12,8 @@
 NBTHREADS=( 1 2 4 8 16 ) 
 TITLE="Comparaison of wall execution time for different number of thread for ${BP} on $HOSTNAME"
 DATE=`date +%s`
-RESFILE=RES/nbthread_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
 
 
 function f(){

Modified: trunk/speedtest/speedtest.OptVsDbg
===================================================================
--- trunk/speedtest/speedtest.OptVsDbg	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.OptVsDbg	2008-02-05 17:21:53 UTC (rev 8466)
@@ -6,7 +6,8 @@
 PROGEXT=( -opt -dbg -safeopt -checkopt -opt64 )
 TITLE="Compare different compilation optimisation flags with default blas on $HOSTNAME"
 DATE=`date +%s`
-RESFILE=RES/optcompare_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
 
 function f(){
 #param1 time prog

Modified: trunk/speedtest/speedtest.gemm
===================================================================
--- trunk/speedtest/speedtest.gemm	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.gemm	2008-02-05 17:21:53 UTC (rev 8466)
@@ -11,7 +11,8 @@
 SIZE2=32 #1, 3, 10, 30, 32, 96, 100, 128
 TITLE="Comparison of execution time of different programme on $HOSTNAME"
 DATE=`date +%s`
-RESFILE=RES/blascompare_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}_${SIZE2}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}_${SIZE2}
 
 
 function f(){

Modified: trunk/speedtest/speedtest.gemv
===================================================================
--- trunk/speedtest/speedtest.gemv	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.gemv	2008-02-05 17:21:53 UTC (rev 8466)
@@ -12,7 +12,8 @@
 
 TITLE="Comparison of execution time of different programme on $HOSTNAME"
 DATE=`date +%s`
-RESFILE=RES/blascompare_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
 
 function f(){
 #param1 time prog

Modified: trunk/speedtest/speedtest.general
===================================================================
--- trunk/speedtest/speedtest.general	2008-02-05 16:37:38 UTC (rev 8465)
+++ trunk/speedtest/speedtest.general	2008-02-05 17:21:53 UTC (rev 8466)
@@ -11,7 +11,8 @@
 #SIZE=( 1023 1024 1055 1056 )
 TITLE="Comparison of execution time of different programme on $HOSTNAME"
 DATE=`date +%s`
-RESFILE=RES/blascompare_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
+SCRIPT=`basename $0`
+RESFILE=RES/${SCRIPT#speedtest.}_${HOSTNAME%.iro.umontreal.ca}_${BP//.\//_}_${DATE}
 APPENDRESULT="NO" #"YES" append result to old one, otherwise we erase old results
 
 function f(){



From nouiz at mail.berlios.de  Tue Feb  5 21:50:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Feb 2008 21:50:47 +0100
Subject: [Plearn-commits] r8467 - trunk/plearn_learners/regressors
Message-ID: <200802052050.m15KolVt001791@sheep.berlios.de>

Author: nouiz
Date: 2008-02-05 21:50:47 +0100 (Tue, 05 Feb 2008)
New Revision: 8467

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
-made an assert more stable
-added assert
-small code cleanup


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-02-05 17:21:53 UTC (rev 8466)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-02-05 20:50:47 UTC (rev 8467)
@@ -148,6 +148,8 @@
     real target = train_set->getTarget(row);
     length -= 1;
     weights_sum -= weight;
+    PLASSERT(length>=0);
+    PLASSERT(weights_sum>=0);
     for (int multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {
         if (target == multiclass_outputs[multiclass_ind])
@@ -171,7 +173,7 @@
         error.clear();
         return;
     }
-    multiclass_winer = 0;
+    int multiclass_winer = 0;
     for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {
         if (multiclass_weights_sum[multiclass_ind] > multiclass_weights_sum[multiclass_winer]) multiclass_winer = multiclass_ind;
@@ -223,7 +225,6 @@
     cout << " e0 " << error[0];
     cout << " e1 " << error[1];
     cout << " ws " << weights_sum;
-    cout << " win " << multiclass_winer;
     cout << endl;
     cout << " mws " << multiclass_weights_sum << endl;
 }

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-05 17:21:53 UTC (rev 8466)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-02-05 20:50:47 UTC (rev 8467)
@@ -354,7 +354,7 @@
 {
     if (split_col < 0) return -1.0;
     real err=leave_error[0] + leave_error[1] - after_split_error;
-    PLASSERT(!isnan(err)&&err>=0);
+    PLASSERT(is_equal(err,0)||err>0);
     return err;
 }
 



From nouiz at mail.berlios.de  Tue Feb  5 21:51:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Feb 2008 21:51:27 +0100
Subject: [Plearn-commits] r8468 - trunk/plearn_learners/regressors
Message-ID: <200802052051.m15KpRGF001832@sheep.berlios.de>

Author: nouiz
Date: 2008-02-05 21:51:27 +0100 (Tue, 05 Feb 2008)
New Revision: 8468

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
Log:
forget in last commit


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-02-05 20:50:47 UTC (rev 8467)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-02-05 20:51:27 UTC (rev 8468)
@@ -68,12 +68,6 @@
     real l2_loss_function_factor;
     Vec multiclass_weights_sum;
  
-/*
-  Work fields: they are sized and initialized if need be, at buid time
-*/  
- 
-    int multiclass_winer;
-  
 public:
     RegressionTreeMulticlassLeave();
     virtual              ~RegressionTreeMulticlassLeave();



From nouiz at mail.berlios.de  Tue Feb  5 21:52:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 5 Feb 2008 21:52:46 +0100
Subject: [Plearn-commits] r8469 - in
 trunk/plearn_learners/regressors/test/RegressionTree: . .pytest
 .pytest/PL_RegressionTree_MultiClass
 .pytest/PL_RegressionTree_MultiClass/expected_results
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split!
 0/LearnerExpdir/Strat0/Trials1
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3
 .pytest/PL_RegressionTree_MultiClass/expected_results/expdir/!
 Split0/LearnerExpdir/Strat0/Trials3/Split0 .pytest/PL_Regressi! onTree_M
Message-ID: <200802052052.m15Kqk1C001967@sheep.berlios.de>

Author: nouiz
Date: 2008-02-05 21:52:44 +0100 (Tue, 05 Feb 2008)
New Revision: 8469

Added:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/LearnerExpdir/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/train_cost_names.txt
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
Log:
Added test for RegressionTree that do classification



Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,6 @@
+HyperLearner: starting the optimization
+split_cols: []
+
+split_cols: 1 
+split_cols: 1 0 
+split_cols: 1 0 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test1_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/test2_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3177 ;
+sumsquare_ = 3177 ;
+sumcube_ = 3177 ;
+sumfourth_ = 3177 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+agmemin_ = 6830 ;
+agemax_ = 6830 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -6354 ;
+sumsquare_ = 12708 ;
+sumcube_ = -25416 ;
+sumfourth_ = 50832 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -6354 ;
+sumsquare_ = 12708 ;
+sumcube_ = -25416 ;
+sumfourth_ = 50832 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 3177 ;
+sumsquare_ = 3177 ;
+sumcube_ = 3177 ;
+sumfourth_ = 3177 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/Split0/train_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.5 ;
+max_ = 0.5 ;
+agmemin_ = 199 ;
+agemax_ = 199 ;
+first_ = 0.5 ;
+last_ = 0.5 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -200 ;
+sumsquare_ = 400 ;
+sumcube_ = -800 ;
+sumfourth_ = 1600 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 99 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 100 ;
+sumsquare_ = 100 ;
+sumcube_ = 100 ;
+sumfourth_ = 100 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 99 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/global_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials0/split_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 14.2702702702702666 ;
+sumsquare_ = 3.91616564589538418 ;
+sumcube_ = 1.07470658057835911 ;
+sumfourth_ = 0.294929872424830264 ;
+min_ = 0.648648648648648574 ;
+max_ = 0.923076923076923128 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = 0.648648648648648574 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 288 ;
+sumsquare_ = 576 ;
+sumcube_ = 1152 ;
+sumfourth_ = 2304 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 288 ;
+sumsquare_ = 576 ;
+sumcube_ = 1152 ;
+sumfourth_ = 2304 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1049 ;
+sumsquare_ = 1049 ;
+sumcube_ = 1049 ;
+sumfourth_ = 1049 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1060.3908523908317 ;
+sumsquare_ = 291.001231841168249 ;
+sumcube_ = -79.8589659106670098 ;
+sumfourth_ = 21.9155582124898878 ;
+min_ = 0.648648648648648574 ;
+max_ = 0.923076923076923128 ;
+agmemin_ = 3863 ;
+agemax_ = 6830 ;
+first_ = 0.923076923076923128 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2098 ;
+sumsquare_ = 4196 ;
+sumcube_ = -8392 ;
+sumfourth_ = 16784 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2098 ;
+sumsquare_ = 4196 ;
+sumcube_ = -8392 ;
+sumfourth_ = 16784 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1049 ;
+sumsquare_ = 1049 ;
+sumcube_ = 1049 ;
+sumfourth_ = 1049 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 14.2702702702702666 ;
+sumsquare_ = 3.91616564589538418 ;
+sumcube_ = 1.07470658057835911 ;
+sumfourth_ = 0.294929872424830264 ;
+min_ = 0.648648648648648574 ;
+max_ = 0.923076923076923128 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = 0.648648648648648574 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 288 ;
+sumsquare_ = 576 ;
+sumcube_ = 1152 ;
+sumfourth_ = 2304 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 288 ;
+sumsquare_ = 576 ;
+sumcube_ = 1152 ;
+sumfourth_ = 2304 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -145 ;
+sumsquare_ = 145 ;
+sumcube_ = -145 ;
+sumfourth_ = 145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 15.2702702702702631 ;
+sumsquare_ = 4.48764662914106438 ;
+sumcube_ = 1.3200231277098966 ;
+sumfourth_ = 0.388694875255762629 ;
+min_ = 0.648648648648648574 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 46 ;
+first_ = 0.648648648648648574 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 290 ;
+sumsquare_ = 580 ;
+sumcube_ = 1160 ;
+sumfourth_ = 2320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 290 ;
+sumsquare_ = 580 ;
+sumcube_ = 1160 ;
+sumfourth_ = 2320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -145 ;
+sumsquare_ = 145 ;
+sumcube_ = -145 ;
+sumfourth_ = 145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1507 ;
+sumsquare_ = 1507 ;
+sumcube_ = 1507 ;
+sumfourth_ = 1507 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6775 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1094.91573926876504 ;
+sumsquare_ = 332.735287798785521 ;
+sumcube_ = -96.6024483526156672 ;
+sumfourth_ = 28.3019556147200539 ;
+min_ = 0.648648648648648574 ;
+max_ = 1 ;
+agmemin_ = 3863 ;
+agemax_ = 6775 ;
+first_ = 0.941176470588235392 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -3014 ;
+sumsquare_ = 6028 ;
+sumcube_ = -12056 ;
+sumfourth_ = 24112 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 6775 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -3014 ;
+sumsquare_ = 6028 ;
+sumcube_ = -12056 ;
+sumfourth_ = 24112 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 6775 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1507 ;
+sumsquare_ = 1507 ;
+sumcube_ = 1507 ;
+sumfourth_ = 1507 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6775 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -145 ;
+sumsquare_ = 145 ;
+sumcube_ = -145 ;
+sumfourth_ = 145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 15.2702702702702631 ;
+sumsquare_ = 4.48764662914106438 ;
+sumcube_ = 1.3200231277098966 ;
+sumfourth_ = 0.388694875255762629 ;
+min_ = 0.648648648648648574 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 46 ;
+first_ = 0.648648648648648574 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 290 ;
+sumsquare_ = 580 ;
+sumcube_ = 1160 ;
+sumfourth_ = 2320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 290 ;
+sumsquare_ = 580 ;
+sumcube_ = 1160 ;
+sumfourth_ = 2320 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -145 ;
+sumsquare_ = 145 ;
+sumcube_ = -145 ;
+sumfourth_ = 145 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -146 ;
+sumsquare_ = 146 ;
+sumcube_ = -146 ;
+sumfourth_ = 146 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 15.3877551020408116 ;
+sumsquare_ = 4.4742631746576258 ;
+sumcube_ = 1.30326420202302096 ;
+sumfourth_ = 0.380403205233986463 ;
+min_ = 0.653061224489795866 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0.653061224489795866 ;
+last_ = 0.653061224489795866 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 292 ;
+sumsquare_ = 584 ;
+sumcube_ = 1168 ;
+sumfourth_ = 2336 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 292 ;
+sumsquare_ = 584 ;
+sumcube_ = 1168 ;
+sumfourth_ = 2336 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -146 ;
+sumsquare_ = 146 ;
+sumcube_ = -146 ;
+sumfourth_ = 146 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1675 ;
+sumsquare_ = 1675 ;
+sumcube_ = 1675 ;
+sumfourth_ = 1675 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6775 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1019.5798319327115 ;
+sumsquare_ = 309.470779908601799 ;
+sumcube_ = -88.2388801657493929 ;
+sumfourth_ = 25.4773413654550858 ;
+min_ = 0.653061224489795866 ;
+max_ = 1 ;
+agmemin_ = 3863 ;
+agemax_ = 6775 ;
+first_ = 0.941176470588235392 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -3350 ;
+sumsquare_ = 6700 ;
+sumcube_ = -13400 ;
+sumfourth_ = 26800 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 6775 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -3350 ;
+sumsquare_ = 6700 ;
+sumcube_ = -13400 ;
+sumfourth_ = 26800 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 6775 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = -1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1675 ;
+sumsquare_ = 1675 ;
+sumcube_ = 1675 ;
+sumfourth_ = 1675 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 6775 ;
+first_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -146 ;
+sumsquare_ = 146 ;
+sumcube_ = -146 ;
+sumfourth_ = 146 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 15.3877551020408116 ;
+sumsquare_ = 4.4742631746576258 ;
+sumcube_ = 1.30326420202302096 ;
+sumfourth_ = 0.380403205233986463 ;
+min_ = 0.653061224489795866 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 140 ;
+first_ = 0.653061224489795866 ;
+last_ = 0.653061224489795866 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 292 ;
+sumsquare_ = 584 ;
+sumcube_ = 1168 ;
+sumfourth_ = 2336 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 292 ;
+sumsquare_ = 584 ;
+sumcube_ = 1168 ;
+sumfourth_ = 2336 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -146 ;
+sumsquare_ = 146 ;
+sumcube_ = -146 ;
+sumfourth_ = 146 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,11 @@
+_trial_	0
+_objective_	0
+nstages	0
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,507 @@
+*1 ->HyperLearner(
+tester = *2 ->PTester(
+expdir = "PYTEST__PL_RegressionTree_MultiClass__RESULTS:expdir/Split0/LearnerExpdir/Strat0/Trials3/" ;
+dataset = *3 ->SubVMatrix(
+parent = *4 ->ConcatRowsVMatrix(
+sources = 2 [ *5 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 200 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/"  )
+*6 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 6831 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat.metadata/"  )
+] ;
+fill_missing = 0 ;
+fully_check_mappings = 0 ;
+only_common_fields = 0 ;
+writable = 0 ;
+length = 7031 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+istart = 0 ;
+jstart = 0 ;
+fistart = -1 ;
+flength = -1 ;
+source = *4  ;
+writable = 0 ;
+length = 7031 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+splitter = *7 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 200 )	(0 , 200 )	(200 , 1 )	
+]
+ )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *8 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 50 ;
+compute_train_stats = 1 ;
+complexity_penalty_factor = 0 ;
+multiclass_outputs = []
+;
+leave_template = *9 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0 0 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *0 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+sorted_train_set = *10 ->RegressionTreeRegisters(
+report_progress = 1 ;
+verbosity = 2 ;
+tsource = *11 ->MemoryVMatrixNoSave(
+source = *12 ->TransposeVMatrix(
+source = *13 ->SubVMatrix(
+parent = *4  ;
+istart = 0 ;
+jstart = 0 ;
+fistart = -1 ;
+flength = -1 ;
+source = *4  ;
+writable = 0 ;
+length = 200 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+writable = 0 ;
+length = 3 ;
+width = 200 ;
+inputsize = 200 ;
+targetsize = 0 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+fieldnames = []
+;
+deep_copy_memory_data = 1 ;
+writable = 0 ;
+length = 3 ;
+width = 200 ;
+inputsize = 200 ;
+targetsize = 0 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+next_id = 10 ;
+leave_register = 200 [ 4 3 4 4 4 3 3 4 3 4 3 3 4 4 4 3 4 4 3 4 3 3 4 3 4 3 4 3 3 4 4 3 3 4 4 4 4 4 3 3 3 4 3 4 4 3 4 3 3 3 4 4 3 4 3 4 4 4 4 4 4 4 4 4 3 4 3 3 4 3 3 3 3 3 3 3 4 3 3 4 3 4 3 3 4 3 4 3 4 3 4 3 4 3 4 3 4 4 4 3 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ] ;
+writable = 0 ;
+length = 200 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+root = *14 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *15 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0.500000000000000333 0.500000000000000333 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 1 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 200 ;
+weights_sum = 1.00000000000000067 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *10  ;
+leave = *15  ;
+leave_output = 2 [ 0 0.5 ] ;
+leave_error = 3 [ 200 100 2.00000000000000133 ] ;
+split_col = 1 ;
+split_balance = 96 ;
+split_feature_value = 0.14412705026016423 ;
+after_split_error = 63.9999999999999787 ;
+missing_node = *0 ;
+missing_leave = *16 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0 0 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 2 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *17 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *18 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0.24000000000000013 0.0200000000000000004 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 3 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 52 ;
+weights_sum = 0.26000000000000012 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *10  ;
+leave = *18  ;
+leave_output = 2 [ 0 0.923076923076923128 ] ;
+leave_error = 3 [ 7.99999999999999645 3.99999999999999734 0.52000000000000024 ] ;
+split_col = 0 ;
+split_balance = 50 ;
+split_feature_value = 2.81501972474946704 ;
+after_split_error = 8.99999999999999289 ;
+missing_node = *0 ;
+missing_leave = *19 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0 0 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 5 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *20 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0.00499999999999998449 -1.73472347597680709e-18 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 6 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 1 ;
+weights_sum = 0.00499999999999995674 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *21 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0.235000000000000125 0.0200000000000000004 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 7 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 51 ;
+weights_sum = 0.255000000000000115 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+ )
+;
+left_leave = *18  ;
+right_node = *22 ->RegressionTreeNode(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+leave_template = *23 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0.26000000000000012 0.480000000000000315 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 4 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 148 ;
+weights_sum = 0.740000000000000546 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+train_set = *10  ;
+leave = *23  ;
+leave_output = 2 [ 1 0.648648648648648574 ] ;
+leave_error = 3 [ 0 52.0000000000000142 0 ] ;
+split_col = 0 ;
+split_balance = 146 ;
+split_feature_value = 3.90430461537466744 ;
+after_split_error = 51.0000000000000071 ;
+missing_node = *0 ;
+missing_leave = *24 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0 0 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 8 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+left_node = *0 ;
+left_leave = *25 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ -4.33680868994201774e-17 0.00499999999999995674 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 9 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 1 ;
+weights_sum = 0.00499999999999995674 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+right_node = *0 ;
+right_leave = *26 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = 2 [ 0.26000000000000012 0.475000000000000311 ] ;
+l1_loss_function_factor = 2 ;
+l2_loss_function_factor = 2 ;
+id = 10 ;
+missing_leave = 0 ;
+loss_function_weight = 1 ;
+verbosity = 2 ;
+train_set = *10  ;
+length = 147 ;
+weights_sum = 0.735000000000000542 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+ )
+;
+right_leave = *23   )
+;
+priority_queue = *27 ->RegressionTreeQueue(
+verbosity = 2 ;
+maximum_number_of_nodes = 50 ;
+next_available_node = 2 ;
+nodes = 50 [ *17  *22  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
+;
+first_leave = *15  ;
+split_cols = 1 [ 1 ] ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 2 ;
+n_examples = 200 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 1 ;
+nstages = 2 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 0 ;
+save_stat_collectors = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 0 ;
+call_forget_in_run = 1 ;
+save_test_costs = 0 ;
+save_test_names = 0 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1  )
+;
+option_fields = 1 [ "nstages" ] ;
+dont_restart_upon_change = 1 [ "nstages" ] ;
+strategy = 1 [ *28 ->HyperOptimize(
+which_cost = "E[test2.E[class_error]]" ;
+min_n_trials = 0 ;
+oracle = *29 ->EarlyStoppingOracle(
+option = "nstages" ;
+values = []
+;
+range = 3 [ 1 5 1 ] ;
+min_value = -3.40282000000000014e+38 ;
+max_value = 3.40282000000000014e+38 ;
+max_degradation = 3.40282000000000014e+38 ;
+relative_max_degradation = -1 ;
+min_improvement = -3.40282000000000014e+38 ;
+relative_min_improvement = -1 ;
+max_degraded_steps = 120 ;
+min_n_steps = 2  )
+;
+provide_tester_expdir = 1 ;
+sub_strategy = []
+;
+rerun_after_sub = 0 ;
+provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
+splitter = *0  )
+] ;
+provide_strategy_expdir = 1 ;
+save_final_learner = 0 ;
+learner = *8  ;
+provide_learner_expdir = 1 ;
+expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
+stage = 1 ;
+n_examples = 7031 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,5 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0
+class_error	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,2 @@
+out0	0
+out1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 14.2702702702702666 ;
+sumsquare_ = 3.91616564589538418 ;
+sumcube_ = 1.07470658057835911 ;
+sumfourth_ = 0.294929872424830264 ;
+min_ = 0.648648648648648574 ;
+max_ = 0.923076923076923128 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = 0.648648648648648574 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 288 ;
+sumsquare_ = 576 ;
+sumcube_ = 1152 ;
+sumfourth_ = 2304 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = 288 ;
+sumsquare_ = 576 ;
+sumcube_ = 1152 ;
+sumfourth_ = 2304 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 198 ;
+first_ = -1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 200 ;
+sumsquarew_ = 200 ;
+sum_ = -144 ;
+sumsquare_ = 144 ;
+sumcube_ = -144 ;
+sumfourth_ = 144 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 198 ;
+agemax_ = 199 ;
+first_ = 1 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,5 @@
+mse	0
+base_confidence	0
+base_reward_l2	0
+base_reward_l1	0
+class_error	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,2 @@
+out0	0
+out1	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,129 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 5 [ "mse" "base_confidence" "base_reward_l2" "base_reward_l1" "class_error" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 5 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1049 ;
+sumsquare_ = 1049 ;
+sumcube_ = 1049 ;
+sumfourth_ = 1049 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -1060.3908523908317 ;
+sumsquare_ = 291.001231841168249 ;
+sumcube_ = -79.8589659106670098 ;
+sumfourth_ = 21.9155582124898878 ;
+min_ = 0.648648648648648574 ;
+max_ = 0.923076923076923128 ;
+agmemin_ = 3863 ;
+agemax_ = 6830 ;
+first_ = 0.923076923076923128 ;
+last_ = 0.648648648648648574 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2098 ;
+sumsquare_ = 4196 ;
+sumcube_ = -8392 ;
+sumfourth_ = 16784 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = -2098 ;
+sumsquare_ = 4196 ;
+sumcube_ = -8392 ;
+sumfourth_ = 16784 ;
+min_ = -1 ;
+max_ = 1 ;
+agmemin_ = 4899 ;
+agemax_ = 6830 ;
+first_ = 1 ;
+last_ = 1 ;
+binary_ = 0 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 6831 ;
+sumsquarew_ = 6831 ;
+sum_ = 1049 ;
+sumsquare_ = 1049 ;
+sumcube_ = 1049 ;
+sumfourth_ = 1049 ;
+min_ = 0 ;
+max_ = 1 ;
+agmemin_ = 6830 ;
+agemax_ = 4899 ;
+first_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
+integer_ = 1 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,192 @@
+*1 ->VecStatsCollector(
+maxnvalues = 0 ;
+fieldnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+compute_covariance = 0 ;
+epsilon = 0 ;
+window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
+no_removal_warnings = 0 ;
+stats = 8 [ StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.280000000000000027 ;
+max_ = 0.280000000000000027 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.280000000000000027 ;
+last_ = 0.280000000000000027 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.719999999999999862 ;
+max_ = 0.719999999999999862 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.719999999999999862 ;
+last_ = 0.719999999999999862 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.440000000000000002 ;
+max_ = 0.440000000000000002 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.440000000000000002 ;
+last_ = 0.440000000000000002 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.440000000000000002 ;
+max_ = 0.440000000000000002 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.440000000000000002 ;
+last_ = 0.440000000000000002 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.153564631825501396 ;
+max_ = 0.153564631825501396 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.153564631825501396 ;
+last_ = 0.153564631825501396 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.767844767844770959 ;
+max_ = 0.767844767844770959 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.767844767844770959 ;
+last_ = 0.767844767844770959 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.692870736348997207 ;
+max_ = 0.692870736348997207 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.692870736348997207 ;
+last_ = 0.692870736348997207 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+StatsCollector(
+epsilon = 0 ;
+maxnvalues = 0 ;
+no_removal_warnings = 0 ;
+nmissing_ = 0 ;
+nnonmissing_ = 1 ;
+sumsquarew_ = 1 ;
+sum_ = 0 ;
+sumsquare_ = 0 ;
+sumcube_ = 0 ;
+sumfourth_ = 0 ;
+min_ = 0.692870736348997207 ;
+max_ = 0.692870736348997207 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.692870736348997207 ;
+last_ = 0.692870736348997207 ;
+binary_ = 0 ;
+integer_ = 0 ;
+counts = {};
+more_than_maxnvalues = 1  )
+] ;
+cov = 0  0  [ 
+]
+;
+sum_cross = 0  0  [ 
+]
+;
+sum_cross_weights = 0  0  [ 
+]
+;
+sum_cross_square_weights = 0  0  [ 
+]
+;
+sum_non_missing_weights = 0 ;
+sum_non_missing_square_weights = 0  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,119 @@
+*12 -> PTester(
+    dataset = *3 -> ConcatRowsVMatrix(
+        inputsize = 2,
+        sources = [
+            *1 -> AutoVMatrix(
+                inputsize = 2,
+                specification = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat",
+                targetsize = 1
+                ),
+            *2 -> AutoVMatrix(
+                inputsize = 2,
+                specification = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat",
+                targetsize = 1
+                )
+            ],
+        targetsize = 1,
+        weightsize = 0
+        ),
+    expdir = "expdir",
+    learner = *10 -> HyperLearner(
+        dont_restart_upon_change = [ "nstages" ],
+        forget_when_training_set_changes = 0,
+        learner = *5 -> RegressionTree(
+            complexity_penalty_factor = 0.0,
+            compute_train_stats = 1,
+            forget_when_training_set_changes = 1,
+            leave_template = *4 -> RegressionTreeMulticlassLeave(
+                multiclass_outputs = [
+                    0,
+                    1
+                    ]
+                ),
+            loss_function_weight = 1,
+            maximum_number_of_nodes = 50,
+            nstages = 10,
+            report_progress = 1,
+            verbosity = 2
+            ),
+        nstages = 1,
+        option_fields = [ "nstages" ],
+        provide_learner_expdir = 1,
+        provide_strategy_expdir = 1,
+        report_progress = 1,
+        save_final_learner = 0,
+        strategy = [
+            *7 -> HyperOptimize(
+                oracle = *6 -> EarlyStoppingOracle(
+                    max_degradation = 3.40282e+38,
+                    max_degraded_steps = 120,
+                    max_value = 3.40282e+38,
+                    min_improvement = -3.40282e+38,
+                    min_n_steps = 2,
+                    min_value = -3.40282e+38,
+                    option = "nstages",
+                    range = [
+                        1,
+                        5,
+                        1
+                        ],
+                    relative_max_degradation = -1,
+                    relative_min_improvement = -1
+                    ),
+                provide_tester_expdir = 1,
+                which_cost = "E[test2.E[class_error]]"
+                )
+            ],
+        tester = *9 -> PTester(
+            provide_learner_expdir = 1,
+            report_stats = 1,
+            save_data_sets = 0,
+            save_initial_learners = 0,
+            save_initial_tester = 0,
+            save_learners = 0,
+            save_test_confidence = 0,
+            save_test_costs = 0,
+            save_test_names = 0,
+            save_test_outputs = 0,
+            splitter = *8 -> FractionSplitter(
+                splits = 1 3 [
+                        (0, 200),
+                        (0, 200),
+                        (200, 1)
+                        ]
+                ),
+            statnames = [
+                "E[test1.E[class_error]]",
+                "E[test1.E[base_confidence]]",
+                "E[test1.E[base_reward_l2]]",
+                "E[test1.E[base_reward_l1]]",
+                "E[test2.E[class_error]]",
+                "E[test2.E[base_confidence]]",
+                "E[test2.E[base_reward_l2]]",
+                "E[test2.E[base_reward_l1]]"
+                ]
+            ),
+        verbosity = 2
+        ),
+    provide_learner_expdir = 1,
+    save_test_confidence = 1,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    splitter = *11 -> FractionSplitter(
+        splits = 1 3 [
+                (0, 1),
+                (0, 200),
+                (200, 1)
+                ]
+        ),
+    statnames = [
+        "E[test1.E[class_error]]",
+        "E[test1.E[base_confidence]]",
+        "E[test1.E[base_reward_l2]]",
+        "E[test1.E[base_reward_l1]]",
+        "E[test2.E[class_error]]",
+        "E[test2.E[base_confidence]]",
+        "E[test2.E[base_reward_l2]]",
+        "E[test2.E[base_reward_l1]]"
+        ]
+    )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]	0
+E[test1.E[base_confidence]]	0
+E[test1.E[base_reward_l2]]	0
+E[test1.E[base_reward_l1]]	0
+E[test2.E[class_error]]	0
+E[test2.E[base_confidence]]	0
+E[test2.E[base_reward_l2]]	0
+E[test2.E[base_reward_l1]]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,3 @@
+__REVISION__ = "PL8454"
+datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
+datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/fieldnames	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,9 @@
+splitnum	0
+test1.E[class_error]	0
+test1.E[base_confidence]	0
+test1.E[base_reward_l2]	0
+test1.E[base_reward_l1]	0
+test2.E[class_error]	0
+test2.E[base_confidence]	0
+test2.E[base_reward_l2]	0
+test2.E[base_reward_l1]	0

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/sizes	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat.metadata/sizes	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/test_cost_names.txt	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,5 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1
+class_error

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,204 @@
+PTester(
+expdir = "PYTEST__PL_RegressionTree_MultiClass__RESULTS:expdir/" ;
+dataset = *1 ->ConcatRowsVMatrix(
+sources = 2 [ *2 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 200 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/"  )
+*3 ->AutoVMatrix(
+filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat" ;
+load_in_memory = 0 ;
+writable = 0 ;
+length = 6831 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat.metadata/"  )
+] ;
+fill_missing = 0 ;
+fully_check_mappings = 0 ;
+only_common_fields = 0 ;
+writable = 0 ;
+length = 7031 ;
+width = 3 ;
+inputsize = 2 ;
+targetsize = 1 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+splitter = *4 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 1 )	(0 , 200 )	(200 , 1 )	
+]
+ )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *5 ->HyperLearner(
+tester = *6 ->PTester(
+expdir = "" ;
+dataset = *0 ;
+splitter = *7 ->FractionSplitter(
+round_to_closest = 0 ;
+splits = 1  3  [ 
+(0 , 200 )	(0 , 200 )	(200 , 1 )	
+]
+ )
+;
+statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
+statmask = []
+;
+learner = *8 ->RegressionTree(
+missing_is_valid = 0 ;
+loss_function_weight = 1 ;
+maximum_number_of_nodes = 50 ;
+compute_train_stats = 1 ;
+complexity_penalty_factor = 0 ;
+multiclass_outputs = []
+;
+leave_template = *9 ->RegressionTreeMulticlassLeave(
+multiclass_outputs = 2 [ 0 1 ] ;
+objective_function = "l1" ;
+multiclass_weights_sum = []
+;
+l1_loss_function_factor = 0 ;
+l2_loss_function_factor = 0 ;
+id = -1 ;
+missing_leave = 0 ;
+loss_function_weight = 0 ;
+verbosity = 0 ;
+train_set = *0 ;
+length = 0 ;
+weights_sum = 0 ;
+targets_sum = 0 ;
+weighted_targets_sum = 0 ;
+weighted_squared_targets_sum = 0 ;
+loss_function_factor = 1 ;
+output = []
+;
+error = []
+ )
+;
+sorted_train_set = *0 ;
+root = *0 ;
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
+;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 1 ;
+nstages = 10 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 0 ;
+save_stat_collectors = 1 ;
+save_learners = 0 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 0 ;
+call_forget_in_run = 1 ;
+save_test_costs = 0 ;
+save_test_names = 0 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 0 ;
+enforce_clean_expdir = 1  )
+;
+option_fields = 1 [ "nstages" ] ;
+dont_restart_upon_change = 1 [ "nstages" ] ;
+strategy = 1 [ *10 ->HyperOptimize(
+which_cost = "E[test2.E[class_error]]" ;
+min_n_trials = 0 ;
+oracle = *11 ->EarlyStoppingOracle(
+option = "nstages" ;
+values = []
+;
+range = 3 [ 1 5 1 ] ;
+min_value = -3.40282000000000014e+38 ;
+max_value = 3.40282000000000014e+38 ;
+max_degradation = 3.40282000000000014e+38 ;
+relative_max_degradation = -1 ;
+min_improvement = -3.40282000000000014e+38 ;
+relative_min_improvement = -1 ;
+max_degraded_steps = 120 ;
+min_n_steps = 2  )
+;
+provide_tester_expdir = 1 ;
+sub_strategy = []
+;
+rerun_after_sub = 0 ;
+provide_sub_expdir = 1 ;
+save_best_learner = 0 ;
+splitter = *0  )
+] ;
+provide_strategy_expdir = 1 ;
+save_final_learner = 0 ;
+learner = *8  ;
+provide_learner_expdir = 1 ;
+expdir_append = "" ;
+forward_nstages = 0 ;
+random_gen = *0 ;
+stage = 0 ;
+n_examples = -1 ;
+inputsize = -1 ;
+targetsize = -1 ;
+weightsize = -1 ;
+forget_when_training_set_changes = 0 ;
+nstages = 1 ;
+report_progress = 1 ;
+verbosity = 2 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )
+;
+perf_evaluators = {};
+report_stats = 1 ;
+save_initial_tester = 1 ;
+save_stat_collectors = 1 ;
+save_learners = 1 ;
+save_initial_learners = 0 ;
+save_data_sets = 0 ;
+save_test_outputs = 1 ;
+call_forget_in_run = 1 ;
+save_test_costs = 1 ;
+save_test_names = 1 ;
+provide_learner_expdir = 1 ;
+should_train = 1 ;
+should_test = 1 ;
+template_stats_collector = *0 ;
+global_template_stats_collector = *0 ;
+final_commands = []
+;
+save_test_confidence = 1 ;
+enforce_clean_expdir = 1  )

Added: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/train_cost_names.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/train_cost_names.txt	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/train_cost_names.txt	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,8 @@
+E[test1.E[class_error]]
+E[test1.E[base_confidence]]
+E[test1.E[base_reward_l2]]
+E[test1.E[base_reward_l1]]
+E[test2.E[class_error]]
+E[test2.E[base_confidence]]
+E[test2.E[base_reward_l2]]
+E[test2.E[base_reward_l1]]

Modified: trunk/plearn_learners/regressors/test/RegressionTree/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/pytest.config	2008-02-05 20:52:44 UTC (rev 8469)
@@ -74,6 +74,22 @@
     
 """
 Test(
+    name = "PL_RegressionTree_MultiClass",
+    description = "",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "regression_tree_multiclass.pyplearn",
+    resources = [ "regression_tree_multiclass.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None
+    )
+
+Test(
     name = "PL_RegressionTree",
     description = "Exercise basic functionality of RegressionTree",
     category = "General",
@@ -85,5 +101,6 @@
     resources = [ "regression_tree.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None
     )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-02-05 20:52:44 UTC (rev 8469)
@@ -51,23 +51,6 @@
     save_test_names = 0,
     ),
     strategy = [
-#    HyperOptimize(
-#    which_cost = "E[test2.E[square_class_error]]" ,
-#    min_n_trials = 1 ,
-#    provide_tester_expdir = 1 ,
-#    rerun_after_sub = 0 ,
-#    provide_sub_expdir = 1 ,
-#    oracle =
-#    OptimizeOptionOracle(
-#    option = "learner.base_regressor_template.loss_function_weight" ,
-###    max_steps = 1 ,
-#    start_value = 1 ,
-#    min_value = .5 ,
-#    factor = 1.5 ,
-#    relative_precision = 0.1 ,
-#    start_direction = "up" ,
-#    ) , # end of OptimizeOptionOracle
-#    sub_strategy = [
 
     pl.HyperOptimize(
     which_cost = "E[test2.E[mse]]" ,
@@ -85,8 +68,6 @@
     min_n_steps = 2 
     )  # end of EarlyStoppingOracle
     )  # end of sub_strategy.HyperOptimize
-#    ]  # end of sub_strategy
-#    )  # end of strategy.HyperOptimize
     ]  # end of HyperLearner strategy
     )
 splitter = pl.FractionSplitter(

Added: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-02-05 20:51:27 UTC (rev 8468)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-02-05 20:52:44 UTC (rev 8469)
@@ -0,0 +1,93 @@
+import os.path
+from plearn.pyplearn import *
+
+plarg_defaults.datatrain  = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat"
+plarg_defaults.datatest   = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat"
+
+dataset = pl.ConcatRowsVMatrix(
+sources = [ pl.AutoVMatrix(specification=plargs.datatrain,inputsize=2,targetsize=1),
+	pl.AutoVMatrix(specification=plargs.datatest,inputsize=2,targetsize=1) ],
+inputsize=2,
+targetsize=1,
+weightsize=0
+)
+
+learner = pl.HyperLearner(
+    option_fields = [ "nstages" ],
+    dont_restart_upon_change = [ "nstages" ] ,
+    provide_strategy_expdir = 1 ,
+    save_final_learner = 0 ,
+    provide_learner_expdir = 1 ,
+    forget_when_training_set_changes = 0 ,
+    nstages = 1 ,
+    report_progress = 1 ,
+    verbosity = 2 ,
+    learner = pl.RegressionTree(
+        nstages = 10
+        ,loss_function_weight = 1
+#        ,missing_is_valid = 0
+#        ,multiclass_outputs = []
+        ,maximum_number_of_nodes = 50
+        ,compute_train_stats = 1
+        ,complexity_penalty_factor = 0.0
+        ,verbosity = 2
+        ,report_progress = 1
+        ,forget_when_training_set_changes = 1
+#        ,conf_rated_adaboost = 0
+        ,leave_template = pl.RegressionTreeMulticlassLeave(multiclass_outputs=[ 0, 1 ] )
+        ),
+    tester = pl.PTester(
+    splitter = pl.FractionSplitter(splits = TMat(1,3,[ (0,200), (0,200), (200,1) ])),
+    statnames = [ #'E[train.E[class_error]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[class_error]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[class_error]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    save_test_outputs = 0 ,
+    report_stats = 1  ,
+    save_initial_tester = 0 ,
+    save_learners = 0 ,
+    save_initial_learners = 0  ,
+    save_data_sets = 0  ,
+    save_test_costs = 0  ,
+    provide_learner_expdir = 1  ,
+    save_test_confidence = 0  ,
+    save_test_names = 0,
+    ),
+    strategy = [
+
+    pl.HyperOptimize(
+    which_cost = "E[test2.E[class_error]]" ,
+    provide_tester_expdir = 1 ,
+    oracle = pl.EarlyStoppingOracle(
+    option = "nstages" ,
+    range = [ 1, 5, 1 ],
+    min_value = -3.40282e+38 ,
+    max_value = 3.40282e+38 ,
+    max_degradation = 3.40282e+38 ,
+    relative_max_degradation = -1 ,
+    min_improvement = -3.40282e+38 ,
+    relative_min_improvement = -1 ,
+    max_degraded_steps = 120 ,
+    min_n_steps = 2 
+    )  # end of EarlyStoppingOracle
+    )  # end of sub_strategy.HyperOptimize
+    ]  # end of HyperLearner strategy
+    )
+splitter = pl.FractionSplitter(
+    splits = TMat(1,3, [ (0,1), (0,200), (200,1) ])
+    )
+tester = pl.PTester(
+    expdir = plargs.expdir,
+    dataset = dataset,
+    splitter = splitter,
+    learner = learner,
+    statnames = [#'E[train.E[class_error]]', 'E[train.E[base_confidence]]', 'E[train.E[base_reward_l2]]', 'E[train.E[base_reward_l1]]',
+                 'E[test1.E[class_error]]',  'E[test1.E[base_confidence]]',  'E[test1.E[base_reward_l2]]',  'E[test1.E[base_reward_l1]]',
+                 'E[test2.E[class_error]]',  'E[test2.E[base_confidence]]',  'E[test2.E[base_reward_l2]]',  'E[test2.E[base_reward_l1]]'],
+    provide_learner_expdir = 1,
+    save_test_costs = 1,
+    save_test_outputs = 1,
+    save_test_confidence = 1
+    )
+
+def main():
+    return tester



From plearner at mail.berlios.de  Wed Feb  6 01:14:43 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 6 Feb 2008 01:14:43 +0100
Subject: [Plearn-commits] r8470 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200802060014.m160EhJ5007530@sheep.berlios.de>

Author: plearner
Date: 2008-02-06 01:14:41 +0100 (Wed, 06 Feb 2008)
New Revision: 8470

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
Log:
Bugfix: during fine-tuning, the supervised_optimizer was used instead of the fine_tuning_optimizer,
so the learning rate of the fine_tuning optimizer was never actually taken into account.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-05 20:52:44 UTC (rev 8469)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-02-06 00:14:41 UTC (rev 8470)
@@ -465,7 +465,7 @@
     // displayVarGraph(totalcost);
 
     VarArray params = totalcost->parents();
-    supervised_optimizer->setToOptimize(params, totalcost);
+    fine_tuning_optimizer->setToOptimize(params, totalcost);
 }
 
 
@@ -530,9 +530,9 @@
         train_stats = new VecStatsCollector();
 
     int l = train_set->length();
-    supervised_optimizer->reset();
-    supervised_optimizer->nstages = l/minibatch_size;
-    supervised_optimizer->optimizeN(*train_stats);
+    fine_tuning_optimizer->reset();
+    fine_tuning_optimizer->nstages = l/minibatch_size;
+    fine_tuning_optimizer->optimizeN(*train_stats);
 }
 
 /*
@@ -552,8 +552,8 @@
     for(int n=0; n<nepochs && relative_improvement >= fine_tuning_improvement_rate; n++)
     {
         st.forget();
-        supervised_optimizer->nstages = l/minibatch_size;
-        supervised_optimizer->optimizeN(st);
+        fine_tuning_optimizer->nstages = l/minibatch_size;
+        fine_tuning_optimizer->optimizeN(st);
         const StatsCollector& s = st.getStats(0);
         real m = s.mean();
         perr << "Epoch " << n+1 << " mean error: " << m << " +- " << s.stderror() << endl;



From tihocan at mail.berlios.de  Wed Feb  6 16:49:07 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Feb 2008 16:49:07 +0100
Subject: [Plearn-commits] r8471 - trunk/plearn/misc
Message-ID: <200802061549.m16Fn7jI009345@sheep.berlios.de>

Author: tihocan
Date: 2008-02-06 16:49:07 +0100 (Wed, 06 Feb 2008)
New Revision: 8471

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Minor typo fix

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-06 00:14:41 UTC (rev 8470)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-06 15:49:07 UTC (rev 8471)
@@ -961,7 +961,7 @@
         real sumdiff_missing = 0;
         int diff = m1->compareStats(m2, stderror_threshold, missing_threshold,
                                     &sumdiff_stderr, &sumdiff_missing);
-        cout<<"Their is "<<diff<<"/"<<m1.width()
+        cout<<"There are "<<diff<<"/"<<m1.width()
             <<" fields that have different stats"<<endl;
         cout <<"The sum of stderror difference is "<<sumdiff_stderr<<endl;
         cout <<"The sum of missing difference is "<<sumdiff_missing<<endl;



From tihocan at mail.berlios.de  Wed Feb  6 16:50:21 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Feb 2008 16:50:21 +0100
Subject: [Plearn-commits] r8472 - trunk/plearn/misc
Message-ID: <200802061550.m16FoLw8009621@sheep.berlios.de>

Author: tihocan
Date: 2008-02-06 16:50:20 +0100 (Wed, 06 Feb 2008)
New Revision: 8472

Modified:
   trunk/plearn/misc/viewVMat.cc
   trunk/plearn/misc/viewVMat.h
Log:
- Using PPath instead of string for dataset spec.
- Fixed bug where reloading the current dataset did no work.


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2008-02-06 15:49:07 UTC (rev 8471)
+++ trunk/plearn/misc/viewVMat.cc	2008-02-06 15:50:20 UTC (rev 8472)
@@ -155,7 +155,7 @@
     return invalidInput;
 }
 
-void viewVMat(const VMat& vm, string dataset_spec)
+void viewVMat(const VMat& vm, PPath filename)
 {
     initscr();
     cbreak();
@@ -621,12 +621,12 @@
                 move(LINES-1, (int) strlen(strmsg));
                 char c[200];
                 getnstr(c, 200);
-                if (string(c) != "")
-                    dataset_spec = c;
+                if (!string(c).empty())
+                    filename = string(c);
                 VMat new_vm;
                 bool error = false;
                 try {
-                    new_vm = getDataSet(dataset_spec);
+                    new_vm = getDataSet(filename);
                 } catch(const PLearnError&) {
                     error = true;
                 }
@@ -645,7 +645,7 @@
                     refresh();
                     endwin();
                     // And launch the new one.
-                    viewVMat(new_vm);
+                    viewVMat(new_vm, filename);
                 }
             }
             break;
@@ -995,8 +995,8 @@
                     (BodyDoc("Displays a VMat's contents using curses.\n"),
                      ArgDoc("vm",
                             "the VMat to display"),
-                     ArgDoc("dataset_spec",
-                            "optional specification of the dataset that will be used to 'reload' it (\"\" works just fine)")));
+                     ArgDoc("filename",
+                            "optional filename of the dataset, that may be used to reload it (\"\" works just fine)")));
 
 END_DECLARE_REMOTE_FUNCTIONS
 

Modified: trunk/plearn/misc/viewVMat.h
===================================================================
--- trunk/plearn/misc/viewVMat.h	2008-02-06 15:49:07 UTC (rev 8471)
+++ trunk/plearn/misc/viewVMat.h	2008-02-06 15:50:20 UTC (rev 8472)
@@ -45,8 +45,9 @@
 using namespace std;
 
 //! Enters curses interactive view of dataset vm.
-//! dataset_spec is the optional specification of the dataset that will be used to "reload" it
-void viewVMat(const VMat& vm, string dataset_spec="");
+//! 'filename' is the optional filename of the dataset, that may be used to
+//! reload it.
+void viewVMat(const VMat& vm, PPath filename = "");
 
 } // end of namespace PLearn
 



From tihocan at mail.berlios.de  Wed Feb  6 16:50:56 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Feb 2008 16:50:56 +0100
Subject: [Plearn-commits] r8473 - trunk/commands/PLearnCommands
Message-ID: <200802061550.m16FouIv009706@sheep.berlios.de>

Author: tihocan
Date: 2008-02-06 16:50:55 +0100 (Wed, 06 Feb 2008)
New Revision: 8473

Modified:
   trunk/commands/PLearnCommands/VMatViewCommand.cc
Log:
Fixed bug that prevented to reload a dataset when viewing it

Modified: trunk/commands/PLearnCommands/VMatViewCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatViewCommand.cc	2008-02-06 15:50:20 UTC (rev 8472)
+++ trunk/commands/PLearnCommands/VMatViewCommand.cc	2008-02-06 15:50:55 UTC (rev 8473)
@@ -60,9 +60,9 @@
 //! The actual implementation of the 'VMatViewCommand' command
 void VMatViewCommand::run(const vector<string>& args)
 {
-    string vmat_view_dataset = string(args[0]);
+    PPath vmat_view_dataset(args[0]);
     VMat vm = getDataSet(vmat_view_dataset);
-    viewVMat(vm);
+    viewVMat(vm, vmat_view_dataset);
 }
 
 } // end of namespace PLearn



From saintmlx at mail.berlios.de  Wed Feb  6 16:52:30 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 6 Feb 2008 16:52:30 +0100
Subject: [Plearn-commits] r8474 - trunk/python_modules/plearn/pyplearn
Message-ID: <200802061552.m16FqUJj009871@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-06 16:52:30 +0100 (Wed, 06 Feb 2008)
New Revision: 8474

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
- new fn. to print all options



Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2008-02-06 15:50:55 UTC (rev 8473)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2008-02-06 15:52:30 UTC (rev 8474)
@@ -1031,6 +1031,14 @@
 #         return super(ListMap, self).__getitem__(key)
     
 
+
+def printAllOptions(out= sys.stdout):
+    for namespace in plargs.getNamespaces():
+        for opt in plopt.iterator(namespace):
+            print >>out, namespace.__name__+'.'+opt.getName()+'='+str(opt.get())
+
+
+
 #######  For backward compatibily: will be deprecated soon  ###################
 
 class plargs_binder(plargs):



From tihocan at mail.berlios.de  Wed Feb  6 16:52:54 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Feb 2008 16:52:54 +0100
Subject: [Plearn-commits] r8475 - trunk/plearn_learners/online
Message-ID: <200802061552.m16FqsG7009962@sheep.berlios.de>

Author: tihocan
Date: 2008-02-06 16:52:53 +0100 (Wed, 06 Feb 2008)
New Revision: 8475

Modified:
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
Log:
- Fixed crash that could occur when port_sizes was used before being initialized.
- Added new valid port configuration.


Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-02-06 15:52:30 UTC (rev 8474)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-02-06 15:52:53 UTC (rev 8475)
@@ -152,18 +152,18 @@
         PLASSERT( cost );
         PLASSERT( !target_grad );
 
-        PLASSERT( prediction->width() == port_sizes(0,1) );
-        PLASSERT( target->width() == port_sizes(1,1) );
-        PLASSERT( cost->width() == port_sizes(2,1) );
-        PLASSERT( prediction_grad->width() == port_sizes(0,1) );
-        PLASSERT( cost_grad->width() == port_sizes(2,1) );
+        PLASSERT( prediction->width() == getPortSizes()(0,1) );
+        PLASSERT( target->width() == getPortSizes()(1,1) );
+        PLASSERT( cost->width() == getPortSizes()(2,1) );
+        PLASSERT( prediction_grad->width() == getPortSizes()(0,1) );
+        PLASSERT( cost_grad->width() == getPortSizes()(2,1) );
 
         int batch_size = prediction->length();
         PLASSERT( target->length() == batch_size );
         PLASSERT( cost->length() == batch_size );
         PLASSERT( cost_grad->length() == batch_size );
 
-        prediction_grad->resize(batch_size, port_sizes(0,1));
+        prediction_grad->resize(batch_size, getPortSizes()(0,1));
 
         for( int i=0; i < batch_size; i++ )
             for ( int j=0; j < target->width(); j++ ) 
@@ -171,7 +171,12 @@
                 (*cost_grad)(i,0)*((*target)(i,j) - sigmoid(-(*prediction)(i,j) ));
     }
 
-    else if( !prediction_grad && !target_grad && !cost_grad )
+    else if( !prediction_grad && !target_grad &&
+               (!cost_grad || !cost_grad->isEmpty()) )
+        // We do not care about the gradient w.r.t prediction and target, and
+        // either we do not care about the gradient w.r.t. cost or there is a
+        // gradient provided (that we will not use).
+        // In such situations, there is nothing to do.
         return;
     else if( !cost_grad && prediction_grad && prediction_grad->isEmpty() )
         PLERROR("In CrossEntropyCostModule::bpropAccUpdate - cost gradient is NULL,\n"



From tihocan at mail.berlios.de  Wed Feb  6 16:55:35 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Feb 2008 16:55:35 +0100
Subject: [Plearn-commits] r8476 - trunk/plearn_learners/online
Message-ID: <200802061555.m16FtZQS010681@sheep.berlios.de>

Author: tihocan
Date: 2008-02-06 16:55:34 +0100 (Wed, 06 Feb 2008)
New Revision: 8476

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
- Added an option to use stochastic reconstruction instead of a deterministic one.
- Added ability to provide a gradient on the activations of the hidden units.
- Removed potential double call to forget on the same connection (this is mainly to be able to get the same deterministic results when using a reconstruction connection that is the same as the connection itself).


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-02-06 15:52:53 UTC (rev 8475)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-02-06 15:55:34 UTC (rev 8476)
@@ -59,7 +59,7 @@
     "  - 'hidden_activations.state' : activations of hidden units (given visible)\n"
     "  - 'visible_sample' : random sample obtained on visible units (input or output port)\n"
     "  - 'visible_expectation' : expectation of visible units (output port ONLY)\n"
-    "  - 'visible_activation' : ectation of visible units (output port ONLY)\n"
+    "  - 'visible_activation' : activation of visible units (output port ONLY)\n"
     "  - 'hidden_sample' : random sample obtained on hidden units\n"
     "  - 'energy' : energy of the joint (visible,hidden) pair or free-energy\n"
     "               of the visible (if given) or of the hidden (if given).\n"
@@ -87,6 +87,8 @@
     "     values (expectations) through the conditional expectations of hidden | visible.\n"
     "  - 'reconstruction_error.state' : the auto-associator reconstruction error (NLL)\n"
     "    obtained by matching the visible_reconstruction with the given visible.\n"
+    "Note that the above determnistic reconstruction may be made stochastic\n"
+    "by using the advanced option 'stochastic_reconstruction'.\n"
     "If compute_contrastive_divergence is true, then the RBM also has these ports\n"
     "  - 'contrastive_divergence' : the quantity minimized by contrastive-divergence training.\n"
     "  - 'negative_phase_visible_samples.state' : the negative phase stochastic reconstruction\n"
@@ -116,6 +118,7 @@
     log_partition_function(0),
     partition_function_is_stale(true),
     deterministic_reconstruction_in_cd(false),
+    stochastic_reconstruction(false),
     standard_cd_grad(true),
     standard_cd_bias_grad(true),
     standard_cd_weights_grad(true),
@@ -131,6 +134,8 @@
 ////////////////////
 void RBMModule::declareOptions(OptionList& ol)
 {
+    // Build options.
+    
     declareOption(ol, "visible_layer", &RBMModule::visible_layer,
                   OptionBase::buildoption,
         "Visible layer of the RBM.");
@@ -148,6 +153,14 @@
                   OptionBase::buildoption,
         "Reconstruction connection between the hidden and visible layers.");
 
+    declareOption(ol, "stochastic_reconstruction",
+                  &RBMModule::stochastic_reconstruction,
+                  OptionBase::buildoption,
+        "If set to true, then reconstruction is not deterministic. Instead,\n"
+        "we sample a hidden vector given the visible input, then use the\n"
+        "visible layer's expectation given this sample as reconstruction.",
+                  OptionBase::advanced_level);
+
     declareOption(ol, "grad_learning_rate", &RBMModule::grad_learning_rate,
                   OptionBase::buildoption,
         "Learning rate for the gradient descent step.");
@@ -239,6 +252,8 @@
                   "i.e. take stochastic gradient steps w.r.t. the log-likelihood instead\n"
                   "of w.r.t. the contrastive divergence.\n");
 
+    // Learnt options.
+    
     declareOption(ol, "Gibbs_step",
                   &RBMModule::Gibbs_step,
                   OptionBase::learntoption,
@@ -946,16 +961,21 @@
         PLASSERT( ports_value.length() == nPorts() );
 
         Mat h;
-        if (hidden && !hidden_is_output)
+        if (hidden && !hidden_is_output) {
             h = *hidden;
-        else {
+            PLASSERT(!stochastic_reconstruction);
+        } else {
             if(!hidden_expectations_are_computed)
             {
                 computePositivePhaseHiddenActivations(*visible);
                 hidden_layer->computeExpectations();
                 hidden_expectations_are_computed=true;
             }
-            h = hidden_layer->getExpectations();
+            if (stochastic_reconstruction) {
+                hidden_layer->generateSamples();
+                h = hidden_layer->samples;
+            } else
+                h = hidden_layer->getExpectations();
         }
 
         // Don't need to verify if they are asked in a port, this was done previously
@@ -995,6 +1015,8 @@
     else if ( visible_reconstruction && visible_reconstruction_is_output
          && hidden && !hidden_is_output)
     {
+        PLASSERT_MSG(!stochastic_reconstruction,
+                     "Not yet implemented");
         // Don't need to verify if they are asked in a port, this was done previously
         computeVisibleActivations(*hidden,true);
         if(visible_reconstruction_activations)
@@ -1333,6 +1355,8 @@
     Mat* visible = ports_value[getPortIndex("visible")];
     Mat* visible_grad = ports_gradient[getPortIndex("visible")];
     Mat* hidden_grad = ports_gradient[getPortIndex("hidden.state")];
+    Mat* hidden_activations_grad =
+        ports_gradient[getPortIndex("hidden_activations.state")];
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
     Mat* visible_activations = ports_value[getPortIndex("visible_activations.state")];
@@ -1372,11 +1396,14 @@
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
     bool compute_hidden_grad = hidden_grad && hidden_grad->isEmpty();
     bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
+    bool provided_hidden_grad = hidden_grad && !hidden_grad->isEmpty();
+    bool provided_hidden_act_grad = hidden_activations_grad &&
+                                    !hidden_activations_grad->isEmpty();
 
     int mbs = (visible && !visible->isEmpty()) ? visible->length() : -1;
 
     // BPROP of UPWARD FPROP
-    if (hidden_grad && !hidden_grad->isEmpty())
+    if (provided_hidden_grad || provided_hidden_act_grad)
     {
         // Note: the assert below is for behavior compatibility with previous
         // code. It might not be necessary, or might need to be modified.
@@ -1390,11 +1417,27 @@
         else
            setAllLearningRates(grad_learning_rate);
 
-        PLASSERT_MSG( hidden && hidden_act , "To compute gradients in bprop, the hidden_activations.state port must have been filled during fprop");
+        PLASSERT_MSG( hidden && hidden_act ,
+                      "To compute gradients in bprop, the "
+                      "hidden_activations.state port must have been filled "
+                      "during fprop" );
+
         // Compute gradient w.r.t. activations of the hidden layer.
-        hidden_layer->bpropUpdate(
-                *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
-                false);
+        if (provided_hidden_grad)
+            hidden_layer->bpropUpdate(
+                    *hidden_act, *hidden, hidden_act_grad, *hidden_grad,
+                    false);
+        if (provided_hidden_act_grad) {
+            if (!provided_hidden_grad) {
+                // 'hidden_act_grad' will not have been resized nor filled yet,
+                // so we need to do it now.
+                hidden_act_grad.resize(hidden_activations_grad->length(),
+                                       hidden_activations_grad->width());
+                hidden_act_grad.clear();
+            }
+            hidden_act_grad += *hidden_activations_grad;
+        }
+
         if (hidden_bias_grad)
         {
             PLASSERT( hidden_bias_grad->isEmpty() &&
@@ -1920,7 +1963,8 @@
     hidden_layer->forget();
     visible_layer->forget();
     connection->forget();
-    if (reconstruction_connection)
+    if (reconstruction_connection && reconstruction_connection != connection)
+        // We avoid to call forget() twice if the connections are the same.
         reconstruction_connection->forget();
 }
 

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2008-02-06 15:52:53 UTC (rev 8475)
+++ trunk/plearn_learners/online/RBMModule.h	2008-02-06 15:55:34 UTC (rev 8476)
@@ -93,6 +93,7 @@
     bool partition_function_is_stale;
 
     bool deterministic_reconstruction_in_cd;
+    bool stochastic_reconstruction;
 
     bool standard_cd_grad;
     bool standard_cd_bias_grad;



From tihocan at mail.berlios.de  Wed Feb  6 17:06:24 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 6 Feb 2008 17:06:24 +0100
Subject: [Plearn-commits] r8477 - trunk/plearn/math
Message-ID: <200802061606.m16G6OYL013542@sheep.berlios.de>

Author: tihocan
Date: 2008-02-06 17:06:23 +0100 (Wed, 06 Feb 2008)
New Revision: 8477

Modified:
   trunk/plearn/math/pl_math.h
Log:
Optimization in fast_is_more and is_more functions

Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2008-02-06 15:55:34 UTC (rev 8476)
+++ trunk/plearn/math/pl_math.h	2008-02-06 16:06:23 UTC (rev 8477)
@@ -240,7 +240,7 @@
                           real absolute_tolerance = ABSOLUTE_TOLERANCE,
                           real relative_tolerance = RELATIVE_TOLERANCE)
 {
-    return !fast_is_equal(a,b,absolute_tolerance_threshold,absolute_tolerance,relative_tolerance) && a>b;
+    return a>b && !fast_is_equal(a,b,absolute_tolerance_threshold,absolute_tolerance,relative_tolerance);
 }
 
 //! Test float inequality while dealling with 'nan' and 'inf' values.
@@ -248,7 +248,7 @@
                           real absolute_tolerance = ABSOLUTE_TOLERANCE,
                           real relative_tolerance = RELATIVE_TOLERANCE)
 {
-    return !is_equal(a,b,absolute_tolerance_threshold,absolute_tolerance,relative_tolerance) && a>b;
+    return a>b && !is_equal(a,b,absolute_tolerance_threshold,absolute_tolerance,relative_tolerance);
 
 }
 



From nouiz at mail.berlios.de  Wed Feb  6 17:11:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 17:11:56 +0100
Subject: [Plearn-commits] r8478 - trunk
Message-ID: <200802061611.m16GBuBb015273@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 17:11:56 +0100 (Wed, 06 Feb 2008)
New Revision: 8478

Modified:
   trunk/pymake.config.model
Log:
Added an option to compile with openmp with gcc


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-02-06 16:06:23 UTC (rev 8477)
+++ trunk/pymake.config.model	2008-02-06 16:11:56 UTC (rev 8478)
@@ -257,7 +257,7 @@
     'purify', 'quantify', 'vc++', 'condor' ],
   
   [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
-    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc' ],
+    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc', 'openmpgcc' ],
   
   [ 'double', 'float' ],
   
@@ -644,6 +644,12 @@
               compileroptions = '-Wall -O3 -ftree-vectorize -ftree-vectorizer-verbose=5 -msse2',
               cpp_definitions = ['NDEBUG'] )
 
+pymakeOption( name = 'openmpgcc',
+              description = 'vectorized with gcc compiler in opt mode',
+              compileroptions = '-Wall -O3 -fopenmp -msse2',
+              linkeroptions = '-lgomp',
+              cpp_definitions = ['NDEBUG'] )
+
 pymakeOption( name = 'pintel',
               description = 'parallelized for intel compiler',
               compileroptions = '-O3 -parallel',



From nouiz at mail.berlios.de  Wed Feb  6 17:23:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 17:23:45 +0100
Subject: [Plearn-commits] r8479 - trunk/plearn/math
Message-ID: <200802061623.m16GNjnf018518@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 17:23:44 +0100 (Wed, 06 Feb 2008)
New Revision: 8479

Modified:
   trunk/plearn/math/StatsCollector.cc
Log:
bugfix: only recalcul the binary_ and integer_ flag when they where not previously calculated


Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2008-02-06 16:11:56 UTC (rev 8478)
+++ trunk/plearn/math/StatsCollector.cc	2008-02-06 16:23:44 UTC (rev 8479)
@@ -1522,8 +1522,9 @@
 
 void StatsCollector::calculate_binary_integer()
 {
-    if(maxnvalues!=0 && nnonmissing_>0)
+    if(binary_==-1 && maxnvalues!=0 && nnonmissing_>0)
     {
+        PLCHECK(integer_==-1);
         binary_  = true;
         integer_ = true;
         for(map<real, StatsCollectorCounts>::iterator it = counts.begin();



From nouiz at mail.berlios.de  Wed Feb  6 17:47:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 17:47:19 +0100
Subject: [Plearn-commits] r8480 - trunk/plearn/vmat
Message-ID: <200802061647.m16GlJ0B021042@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 17:47:18 +0100 (Wed, 06 Feb 2008)
New Revision: 8480

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
-Added function VMatrix::compatibleSizeError, that check if two matrice are compatible(same inputsize,targetsize,weightsize,extrasize, width). If not give an error
-bugfix in vmat compare_stats


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-06 16:23:44 UTC (rev 8479)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-06 16:47:18 UTC (rev 8480)
@@ -1243,6 +1243,23 @@
         || this->extrasize()  != m->extrasize() );
 }
 
+void VMatrix::compatibleSizeError(const VMat& m){
+#define MY_PRINT_ERROR_MST(NAME) PLERROR("In VMatrix::looksTheSameAsError The matrix are not compatible!\n m1."#NAME"=%d and m2."#NAME"=%d", this->NAME(), m->NAME());
+
+    if(this->width()      != m->width())
+        MY_PRINT_ERROR_MST(width)
+    else if(this->inputsize()  != m->inputsize())
+        MY_PRINT_ERROR_MST(inputsize)
+    else if(this->weightsize() != m->weightsize())
+        MY_PRINT_ERROR_MST(weightsize)
+    else if(this->targetsize() != m->targetsize())
+        MY_PRINT_ERROR_MST(targetsize)
+    else if(this->extrasize()  != m->extrasize() )
+        MY_PRINT_ERROR_MST(extrasize)
+#undef MY_PRINT_ERROR_MST
+}
+
+
 ////////////
 // getPid //
 ////////////
@@ -1940,8 +1957,15 @@
         real lmissing = lstats.nmissing()/lstats.n();
         real terr = sqrt(tmissing*(1-tmissing)+lmissing*(1-lmissing));
         real th = fabs(tmissing-lmissing)/terr;
-        if(terr==0)
-            PLCHECK(tmissing==0 && lmissing==0);
+        if(fast_is_equal(terr,0))
+        {
+            if(!fast_is_equal(tmissing,0)||!fast_is_equal(lmissing,0))
+                PLWARNING("In VMatrix::compareStats - field %d(%s)terr=%f,"
+                          " tmissing=%f, lmissing=%f!",i, fieldName(i).c_str(),
+                          terr, tmissing, lmissing);
+            PLCHECK((fast_is_equal(tmissing,0)||fast_is_equal(tmissing,1))
+                    && (fast_is_equal(lmissing,0)||fast_is_equal(lmissing,1)));
+        }
         else if(isnan(th))
             PLWARNING("In VMatrix::compareStats - should not happen!");
         else

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-06 16:23:44 UTC (rev 8479)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-06 16:47:18 UTC (rev 8480)
@@ -265,6 +265,10 @@
     /// width and length.
     bool looksTheSameAs(const VMat& m);
 
+    /// generate an PLERROR iif it don't looks like the same matrix,
+    /// i.e. it has same sizes and width.
+    void compatibleSizeError(const VMat& m);
+
     /**
      *  This should be called by the build method of every VMatrix that has a
      *  metadatadir.  It will create said directory if it doesn's already



From nouiz at mail.berlios.de  Wed Feb  6 21:03:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 21:03:54 +0100
Subject: [Plearn-commits] r8481 - in trunk/plearn: misc vmat
Message-ID: <200802062003.m16K3sBc022535@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 21:03:54 +0100 (Wed, 06 Feb 2008)
New Revision: 8481

Modified:
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
-modified VMatrix::getFieldIndex to
int getFieldIndex(const string& fieldname_or_num,const bool error=true) const
to allow using it will it don't generate an error.
-in 'plearn vmat compare_stats', load the matrix as the other 'plearn vmat' command and check compatibility of the two matrix
-better printing to 'plearn vmat cdf' and 'plearn vmat compare_stats'


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-06 16:47:18 UTC (rev 8480)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-06 20:03:54 UTC (rev 8481)
@@ -180,6 +180,7 @@
     for(int i=0; i<k; i++)
     {
         name[i] = vmats[i]->getMetaDataDir();
+        name[i] = name[i].erase(name[i].size()-10);
         pout << name[i] << ": \t " << vmats[i]->length() << " x " << vmats[i]->width() << endl;
     }
 
@@ -191,19 +192,23 @@
     {
         // TVec<RealMapping> ranges = vm->getRanges();
 
-        pout << "Field (0.." << w-1 << ") [low high] ? " << flush;
         vector<string> command;
         int varnum = -1;
         real low = -FLT_MAX; // means autorange
         real high = FLT_MAX; // means autorange
         do
         {
+            pout << "Field (0.." << w-1 << ") [low high] or exit? " << flush;
             command = split(pgetline(cin));
             if(command.size()==0)
                 vmats[0]->printFields(pout);
+            else if(command.size()==1&&command[0]=="exit")
+                exit(0);
             else
             {
-                varnum = vmats[0]->getFieldIndex(command[0]);
+                varnum = vmats[0]->getFieldIndex(command[0],false);
+                if(varnum == -1)
+                    pout<<"Bad column name or number("<<command[0]<<")"<<endl;
                 if(varnum<0 || varnum>=w)
                     vmats[0]->printFields(pout);
                 else if(command.size()==3)
@@ -235,7 +240,8 @@
         if(is_equal(low,-FLT_MAX))
             gp << "set xrange [*:*]" << endl;      
         else
-            gp << "set xrange [" << low << ":" << high << "]" << endl;
+            //The -0.01 and 0.01 is to clearly see the end value.
+            gp << "set xrange [" << low-0.01 << ":" << high+0.01 << "]" << endl;
 
         if(k>=4)
             gp.plot(m[0],"title '"+name[0]+"'", m[1], "title '" + name[1]+"'", m[2], "title '" + name[2]+"'", m[3], "title '"+name[3]+"'");    
@@ -949,8 +955,12 @@
     {
         if(!(argc==4||argc==5||argc==6))
             PLERROR("vmat compare_stats must be used that way: vmat compare_stats <dataset1> <dataset2> [stderror threshold] [missing threshold]");
-        VMat m1 = getDataSet(argv[2]);
-        VMat m2 = getDataSet(argv[3]);
+
+        VMat m1 = getVMat(argv[2], indexf);
+        VMat m2 = getVMat(argv[3], indexf);
+
+        m1->compatibleSizeError(m2);
+
         real stderror_threshold = 1;
         real missing_threshold = 10;
         if(argc>4)
@@ -959,12 +969,13 @@
             missing_threshold=toreal(argv[5]);
         real sumdiff_stderr = 0;
         real sumdiff_missing = 0;
+        pout << "Test of difference that suppose gaussiane variable"<<endl;
         int diff = m1->compareStats(m2, stderror_threshold, missing_threshold,
                                     &sumdiff_stderr, &sumdiff_missing);
-        cout<<"There are "<<diff<<"/"<<m1.width()
+        pout<<"There are "<<diff<<"/"<<m1.width()
             <<" fields that have different stats"<<endl;
-        cout <<"The sum of stderror difference is "<<sumdiff_stderr<<endl;
-        cout <<"The sum of missing difference is "<<sumdiff_missing<<endl;
+        pout <<"The sum of stderror difference is "<<sumdiff_stderr<<endl;
+        pout <<"The sum of missing difference is "<<sumdiff_missing<<endl;
 
     }
     else

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-06 16:47:18 UTC (rev 8480)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-06 20:03:54 UTC (rev 8481)
@@ -404,7 +404,7 @@
 ///////////////////
 // getFieldIndex //
 ///////////////////
-int VMatrix::getFieldIndex(const string& fieldname_or_num) const
+int VMatrix::getFieldIndex(const string& fieldname_or_num, const bool error) const
 {
     int i = fieldIndex(fieldname_or_num);
     if(i==-1 && pl_islong(fieldname_or_num)) {
@@ -416,7 +416,7 @@
         if (tostring(i) != fieldname_or_num)
             i = -1;
     }
-    if (i < 0 || i >= width())
+    if ((i < 0 || i >= width()) && error)
         PLERROR("In VMatrix::getFieldIndex - Asked for an invalid column number: '%s'",
                 fieldname_or_num.c_str());
     return i;

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-06 16:47:18 UTC (rev 8480)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-06 20:03:54 UTC (rev 8481)
@@ -191,9 +191,9 @@
      *  This first calls fieldIndex to try and get the index corresponding to
      *  the given string If this fails, the given string is assumed to hold the
      *  numerical index, and its conversion to int will be returned (or a
-     *  PLEARNERROR issued if this fails).
+     *  PLERROR issued if this fails).
      */
-    int getFieldIndex(const string& fieldname_or_num) const;
+    int getFieldIndex(const string& fieldname_or_num,const bool error=true) const;
 
     /// Return the field name at a given index
     string fieldName(int fieldindex) const



From nouiz at mail.berlios.de  Wed Feb  6 21:39:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 21:39:30 +0100
Subject: [Plearn-commits] r8482 - trunk/python_modules/plearn/pytest
Message-ID: <200802062039.m16KdUaK025441@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 21:39:30 +0100 (Wed, 06 Feb 2008)
New Revision: 8482

Modified:
   trunk/python_modules/plearn/pytest/modes.py
   trunk/python_modules/plearn/pytest/tests.py
Log:
Added an option to pytest that redo the diff for the last run. This do exactly the same as run execpt that it do not generate new results.


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2008-02-06 20:03:54 UTC (rev 8481)
+++ trunk/python_modules/plearn/pytest/modes.py	2008-02-06 20:39:30 UTC (rev 8482)
@@ -841,3 +841,18 @@
 
     ## Verify that directory is empty
     print os.listdir(os.getcwd())
+
+
+class rundiff(ResultsBasedMode):
+    """Redo the diff of last execution.
+    
+    Usage: pytest rundiff <test_name>
+    """
+    RoutineType = DiffTestRoutine
+    def empty(self):
+        pass
+
+    def __init__(self, targets, options):
+        super(rundiff, self).__init__(targets, options)
+
+

Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2008-02-06 20:03:54 UTC (rev 8481)
+++ trunk/python_modules/plearn/pytest/tests.py	2008-02-06 20:39:30 UTC (rev 8482)
@@ -853,3 +853,25 @@
             toolkit.lines_to_file(diffs, self.report_path)
             self.test.setStatus("FAILED", log="Should contain diff lines!")
         
+class DiffTestRoutine( RunTestRoutine ):
+    def start(self):
+        self.test.sanity_check()
+        self.status_hook()
+    
+    def __init__( self, **overrides ):
+        ResultsRelatedRoutine.__init__(self, **overrides)
+        self.expected_results = self.test.resultsDirectory( EXPECTED_RESULTS )
+        self.run_results      = self.test.resultsDirectory( RUN_RESULTS )
+
+        self.report_path = os.path.join(self.test.resultsDirectory(), self.failure_log)
+        if os.path.exists(self.report_path):
+            os.remove(self.report_path)
+        
+        if ( not self.test.disabled and
+             not os.path.exists(self.expected_results) ):
+            raise PyTestUsageError(
+                "%s\n Expected results must be generated by the 'results' mode "
+                "prior to any use of the 'run' mode."
+                % self.test.getPath()
+                )
+



From nouiz at mail.berlios.de  Wed Feb  6 21:43:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 21:43:34 +0100
Subject: [Plearn-commits] r8483 -
	trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results
Message-ID: <200802062043.m16KhYHN025677@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 21:43:34 +0100 (Wed, 06 Feb 2008)
New Revision: 8483

Modified:
   trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log
Log:
forgeted in last commit


Modified: trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log
===================================================================
--- trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log	2008-02-06 20:39:30 UTC (rev 8482)
+++ trunk/python_modules/plearn/pytest/.pytest/PL_PyTestCore/expected_results/RUN.log	2008-02-06 20:43:34 UTC (rev 8483)
@@ -55,6 +55,10 @@
 #
 
 #
+#  pytest rundiff -h
+#
+
+#
 #  pytest update -h
 #
 



From nouiz at mail.berlios.de  Wed Feb  6 22:08:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 6 Feb 2008 22:08:27 +0100
Subject: [Plearn-commits] r8484 - trunk/plearn/vmat
Message-ID: <200802062108.m16L8RnZ027762@sheep.berlios.de>

Author: nouiz
Date: 2008-02-06 22:08:27 +0100 (Wed, 06 Feb 2008)
New Revision: 8484

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
bugfix for python


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-06 20:43:34 UTC (rev 8483)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-06 21:08:27 UTC (rev 8484)
@@ -189,7 +189,7 @@
          RetDoc ("TVec of field names.\n")));
 
      declareMethod(
-        rmm, "getFieldIndex", &VMatrix::getFieldIndex,
+        rmm, "getFieldIndex", &VMatrix::remote_getFieldIndex,
         (BodyDoc("Returns the index of a field.\n"),
          ArgDoc ("fname_or_num",
              "Field name or index (as a string) of the field.\n"),

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-06 20:43:34 UTC (rev 8483)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-06 21:08:27 UTC (rev 8484)
@@ -194,6 +194,9 @@
      *  PLERROR issued if this fails).
      */
     int getFieldIndex(const string& fieldname_or_num,const bool error=true) const;
+    int remote_getFieldIndex(const string& fieldname_or_num) const{
+        return getFieldIndex(fieldname_or_num);
+    }
 
     /// Return the field name at a given index
     string fieldName(int fieldindex) const



From ducharme at mail.berlios.de  Wed Feb  6 22:29:35 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 6 Feb 2008 22:29:35 +0100
Subject: [Plearn-commits] r8485 - trunk/plearn_learners/generic
Message-ID: <200802062129.m16LTZZI029238@sheep.berlios.de>

Author: ducharme
Date: 2008-02-06 22:29:35 +0100 (Wed, 06 Feb 2008)
New Revision: 8485

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Bug fix dans useOnTrain lorsque la matrice outputs n'est pas convenablement initialisee.


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-02-06 21:08:27 UTC (rev 8484)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-02-06 21:29:35 UTC (rev 8485)
@@ -881,6 +881,7 @@
 // useOnTrain //
 ////////////////
 void PLearner::useOnTrain(Mat& outputs) const {
+    outputs.resize(train_set.length(), outputsize());
     VMat train_output(outputs);
     use(train_set, train_output);
 }



From nouiz at mail.berlios.de  Thu Feb  7 17:33:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Feb 2008 17:33:27 +0100
Subject: [Plearn-commits] r8486 - trunk/python_modules/plearn/vmat
Message-ID: <200802071633.m17GXRoQ027992@sheep.berlios.de>

Author: nouiz
Date: 2008-02-07 17:33:26 +0100 (Thu, 07 Feb 2008)
New Revision: 8486

Modified:
   trunk/python_modules/plearn/vmat/PMat.py
Log:
Added a function: checkzerorow(row)


Modified: trunk/python_modules/plearn/vmat/PMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/PMat.py	2008-02-06 21:29:35 UTC (rev 8485)
+++ trunk/python_modules/plearn/vmat/PMat.py	2008-02-07 16:33:26 UTC (rev 8486)
@@ -362,6 +362,19 @@
             ar.byteswap(True)
         return ar
 
+    def checkzerorow(self,i):
+        if i<0 or i>self.length:
+            raise IndexError('PMat index out of range')
+        self.f.seek(64+i*self.rowsize)
+        data = self.f.read(self.rowsize)
+        ar = numpy.numarray.fromstring(data, self.elemtype, (len(data)/self.elemsize,))
+        if self.swap_bytes:
+            ar.byteswap(True)
+        for elem in ar:
+            if elem!=0:
+                return False
+        return True
+    
     def putRow(self,i,row):
         if i<0 or i>=self.length:
             raise IndexError('PMat index out of range')



From nouiz at mail.berlios.de  Thu Feb  7 17:35:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Feb 2008 17:35:15 +0100
Subject: [Plearn-commits] r8487 - trunk/python_modules/plearn/pytest
Message-ID: <200802071635.m17GZFMu028183@sheep.berlios.de>

Author: nouiz
Date: 2008-02-07 17:35:14 +0100 (Thu, 07 Feb 2008)
New Revision: 8487

Modified:
   trunk/python_modules/plearn/pytest/modes.py
   trunk/python_modules/plearn/pytest/tests.py
Log:
-added the diff time.
-now print run and diff time with option --showtime 


Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2008-02-07 16:33:26 UTC (rev 8486)
+++ trunk/python_modules/plearn/pytest/modes.py	2008-02-07 16:35:14 UTC (rev 8487)
@@ -759,6 +759,9 @@
         ogroups[1].add_option( '--no-compile', default=False,
                                action="store_true",
                                help='Any program compilation is bypassed.' )
+        ogroups[1].add_option( '--showtime', default=False,
+                               action = "store_true",
+                               help="If true, print the run time and diff time" )
 
         return ogroups        
     option_groups = classmethod(option_groups)
@@ -766,6 +769,8 @@
     def __init__(self, targets, options):
         logging.debug("--no-compile (=%s) option forwarded to Program."%options.no_compile)
         Program.compilation_disabled = options.no_compile        
+        logging.debug("--showtime (=%s) option forwarded to Program."%options.showtime)
+        Program.showtime = options.showtime
         super(ResultsBasedMode, self).__init__(targets, options)
     
 class results(ResultsBasedMode):
@@ -795,7 +800,19 @@
 class run(ResultsBasedMode):    
     RoutineType = RunTestRoutine
 
+class rundiff(ResultsBasedMode):
+    """Redo the diff of last execution.
+    
+    Usage: pytest rundiff <test_name>
+    """
+    RoutineType = DiffTestRoutine
 
+    def empty(self):
+        pass
+
+    def __init__(self, targets, options):
+        super(rundiff, self).__init__(targets, options)
+
 #######  Builtin Unit Tests  ##################################################
 
 def testAllModes():
@@ -843,16 +860,3 @@
     print os.listdir(os.getcwd())
 
 
-class rundiff(ResultsBasedMode):
-    """Redo the diff of last execution.
-    
-    Usage: pytest rundiff <test_name>
-    """
-    RoutineType = DiffTestRoutine
-    def empty(self):
-        pass
-
-    def __init__(self, targets, options):
-        super(rundiff, self).__init__(targets, options)
-
-

Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2008-02-07 16:33:26 UTC (rev 8486)
+++ trunk/python_modules/plearn/pytest/tests.py	2008-02-07 16:35:14 UTC (rev 8487)
@@ -346,6 +346,7 @@
     ignored_files_re = PLOption([])
     disabled         = PLOption(False)
     runtime          = PLOption(None)
+    difftime         = PLOption(None)
 
     #######  Class Variables and Methods  #########################################
     # NB: Class variables could now all be 'public', laziness is why it isn't so...
@@ -571,21 +572,25 @@
         statsHeader = TestStatus.summaryHeader()
 
         # Hackish hardcoded display summing to 80...
-        C = 6; S = len("** FAILED **")+2; H = len(statsHeader)+3; T = 11
+        C = 6; S = len("** FAILED **")+2; H = len(statsHeader)+3; T = 0
+        if Program.showtime:
+            T = 16
         N = 80 - (C+S+H+T); 
-        def vpformat(c, n, s, h, t):
+        def vpformat(c, n, s, h, t, d):
             if len(n) < N:
-                logging.info( c.ljust(C)+n.ljust(N)+s.center(S)+h.center(H)
-                              +t.ljust(T))
+                str=c.ljust(C)+n.ljust(N)+s.center(S)+h.center(H)
             else:
                 logging.info( c.ljust(C)+n.ljust(N) )
-                logging.info( ((C+N)*" ")+s.center(S)+h.center(H)+t.ljust(T))
+                str=((C+N)*" ")+s.center(S)+h.center(H)
+            if Program.showtime:
+                str+=t.rjust(T/2)+d.ljust(T/2)
+            logging.info(str)
         
         if self._status.isCompleted():
             if not self._logged_header:
                 logging.info(TestStatus.headerLegend(C+N+S+H+T)+'\n')
                 vpformat("N/%d"%self._test_count, "Test Name", "Status",
-                         statsHeader,"Run time(s)")
+                         statsHeader,"Run/Diff"," time(s)")
                 self.__class__._logged_header = True
 
             if self._log_count%5 == 0:
@@ -594,7 +599,7 @@
             self.__class__._log_count += 1
             vpformat(str(self._log_count), self.getName(),
                      str(self._status), TestStatus.summary(),
-                     "%10.4s"%self.runtime)
+                     "%.6s/"%self.runtime,"%.6s"%self.difftime)
 
     def toBeNeglected(self):
         neglect = False
@@ -834,6 +839,8 @@
 
         diffs = []
         plearn_exec = None
+
+        starttime = time()
         if self.test.pfileprg is not None:
             if self.test.pfileprg.compile():
                 plearn_exec = self.test.pfileprg.getInternalExecPath()
@@ -843,8 +850,11 @@
         diffs.extend(
             pldiff.pldiff(self.expected_results, self.run_results,
                           self.test.precision, plearn_exec, self.test.ignored_files_re))
+        endtime = time()
         popd()
 
+        self.test.difftime = endtime - starttime
+
         # Set status
         logging.debug("diffs: %s"%str(diffs))
         if diffs == []:



From nouiz at mail.berlios.de  Thu Feb  7 21:25:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 7 Feb 2008 21:25:22 +0100
Subject: [Plearn-commits] r8488 - trunk/commands/PLearnCommands
Message-ID: <200802072025.m17KPMo8011776@sheep.berlios.de>

Author: nouiz
Date: 2008-02-07 21:25:21 +0100 (Thu, 07 Feb 2008)
New Revision: 8488

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
Log:
better formating


Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2008-02-07 16:35:14 UTC (rev 8487)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2008-02-07 20:25:21 UTC (rev 8488)
@@ -62,7 +62,18 @@
         "To get a short description of available commands:   " + prgname() + " help commands \n"
         "To get detailed help on a specific command:         " + prgname() + " help <command_name> \n"
         "To get help on a specific PLearn object:            " + prgname() + " help <object_type_name> \n"
-        "To get help on datasets:                            " + prgname() + " help datasets \n" 
+        "To get help on datasets:                            " + prgname() + " help datasets \n\n" 
+        ""
+        "Global parameter:\n"
+        "                 --no-version: do not print the version \n"
+        "                 --verbosity LEVEL: The level of log to print.\n"
+        "                     Must have been compiled with the same level of more\n"
+        "                     Available level:\n"
+        "                             VLEVEL_MAND    // Mandatory\n"
+        "                             VLEVEL_IMP     // Important\n"
+        "                             VLEVEL_NORMAL  // Normal (Default)\n"
+        "                             VLEVEL_DBG     // Debug Info\n"
+        "                             VLEVEL_EXTREME // Extreme Verbosity\n"
          << endl;
 }
 



From nouiz at mail.berlios.de  Fri Feb  8 16:09:41 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Feb 2008 16:09:41 +0100
Subject: [Plearn-commits] r8489 - trunk/plearn/vmat
Message-ID: <200802081509.m18F9f4e011664@sheep.berlios.de>

Author: nouiz
Date: 2008-02-08 16:09:41 +0100 (Fri, 08 Feb 2008)
New Revision: 8489

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
print the warning of compareStats in sorted order.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-07 20:25:21 UTC (rev 8488)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-08 15:09:41 UTC (rev 8489)
@@ -1948,6 +1948,8 @@
     int nbdiff            = 0;
     real sumdiff_stderr_  = 0;
     real sumdiff_missing_ = 0;
+    Mat score(width(),3);
+
     for(int i=0;i<width();i++)
     {
         const StatsCollector tstats = target->getStats(i);
@@ -1956,7 +1958,7 @@
         real tmissing = tstats.nmissing()/tstats.n();
         real lmissing = lstats.nmissing()/lstats.n();
         real terr = sqrt(tmissing*(1-tmissing)+lmissing*(1-lmissing));
-        real th = fabs(tmissing-lmissing)/terr;
+        real th_missing = fabs(tmissing-lmissing)/terr;
         if(fast_is_equal(terr,0))
         {
             if(!fast_is_equal(tmissing,0)||!fast_is_equal(lmissing,0))
@@ -1966,38 +1968,65 @@
             PLCHECK((fast_is_equal(tmissing,0)||fast_is_equal(tmissing,1))
                     && (fast_is_equal(lmissing,0)||fast_is_equal(lmissing,1)));
         }
-        else if(isnan(th))
+        else if(isnan(th_missing))
             PLWARNING("In VMatrix::compareStats - should not happen!");
         else
-            sumdiff_missing_ += th;
-        if(th>missing_threshold)
-        {
-            PLWARNING("In VMatrix::compareStats - field %d(%s) have %f"
-                      " missing while target stats have %f."
-                      " The stats difference is %f.", 
-                      i, fieldName(i).c_str(), lmissing, tmissing, th);
-            nbdiff++;
-        }
+            sumdiff_missing_ += th_missing;
+        
         real tmean = tstats.mean();
         real lmean = lstats.mean();
         real tstderror = sqrt(pow(tstats.stderror(), 2) + 
                               pow(lstats.stderror(), 2));
-        th = fabs(lmean-tmean)/tstderror;
+        real th_stderror = fabs(lmean-tmean)/tstderror;
         if(tstderror==0)
             PLWARNING("In VMatrix::compareStats - field %d(%s) have a"
                       " stderror of 0 for both matrice.",
                       i, fieldName(i).c_str());
         else
-            sumdiff_stderr_+=th;
-        if(th>stderror_threshold)
+            sumdiff_stderr_+=th_stderror;
+        score(i,0)=i;
+        score(i,1)=th_stderror;
+        score(i,2)=th_missing;
+
+    }
+    pout<<"Print the field that do not pass the threshold sorted by the stderror"<<endl;
+    sortRows(score,1,false);
+    for(int i=0;i<score.length();i++)
+    {
+        if(score(i,1)>stderror_threshold)
         {
-            PLWARNING("In VMatrix::compareStats - field %d(%s) have mean %f"
-                      " while target mean is %f and target stderror is %f."
-                      " They differ by %f stderror",
-                      i, fieldName(i).c_str(), lmean, tmean, tstderror, th);
+            const StatsCollector tstats = target->getStats(i);
+            const StatsCollector lstats = getStats(i);
+            real tmean = tstats.mean();
+            real lmean = lstats.mean();
+            real tstderror = sqrt(pow(tstats.stderror(), 2) + 
+                                  pow(lstats.stderror(), 2));
+
+            pout<<i<<"("<<fieldName(int(round(score(i,0))))<<")"
+                <<" differ by "<<score(i,1)<<" stderror."
+                <<" The mean is "<<lmean<<" while the target mean is "<<tmean
+                <<" and the used stderror is "<<tstderror<<endl;
             nbdiff++;
         }
     }
+
+    cout<<"Print the field that do not pass the threshold sorted by the missing error"<<endl;
+    sortRows(score,2,false);
+    for(int i=0;i<score.length();i++)
+    {
+        if(score(i,2)>missing_threshold)
+        {
+            const StatsCollector tstats = target->getStats(i);
+            const StatsCollector lstats = getStats(i);
+            real tmissing = tstats.nmissing()/tstats.n();
+            real lmissing = lstats.nmissing()/lstats.n();
+            pout<<i<<"("<<fieldName(int(round(score(i,0))))<<")"
+                <<" The missing stats difference is "<< score(i,2)
+                <<". Their is "<<lmissing<<" missing while target have "
+                <<tmissing<<" missing."<<endl;
+            nbdiff++;
+        }
+    }
     if(sumdiff_stderr!=NULL)
         *sumdiff_stderr = sumdiff_stderr_;
     if(sumdiff_missing!=NULL)



From nouiz at mail.berlios.de  Fri Feb  8 17:37:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Feb 2008 17:37:09 +0100
Subject: [Plearn-commits] r8490 - trunk/plearn/math
Message-ID: <200802081637.m18Gb9lo019112@sheep.berlios.de>

Author: nouiz
Date: 2008-02-08 17:37:09 +0100 (Fri, 08 Feb 2008)
New Revision: 8490

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
removed warning


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-02-08 15:09:41 UTC (rev 8489)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-02-08 16:37:09 UTC (rev 8490)
@@ -2895,8 +2895,8 @@
 void transposeProduct(const TVec<T>& result, const TMat<T>& m, const TVec<T>& v)
 {
     int l=m.length();
+#ifdef BOUNDCHECK
     int w=m.width();
-#ifdef BOUNDCHECK
     if (l!=v.length() || w!=result.length())
         PLERROR("transposeProduct(TVec, TMat, TVec), incompatible arguments:\n"
                 "%d <- %dx%d' times %d",



From nouiz at mail.berlios.de  Fri Feb  8 17:54:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 8 Feb 2008 17:54:09 +0100
Subject: [Plearn-commits] r8491 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200802081654.m18Gs9hd020199@sheep.berlios.de>

Author: nouiz
Date: 2008-02-08 17:54:08 +0100 (Fri, 08 Feb 2008)
New Revision: 8491

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
Log:
made it compilable with -float


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2008-02-08 16:37:09 UTC (rev 8490)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/SecondIterationWrapper.cc	2008-02-08 16:54:08 UTC (rev 8491)
@@ -196,8 +196,8 @@
         else if (sales_prediction < 10000000.0 && commitment < 1000000.0) predicted_class = 2.0;
         else if (sales_prediction < 100000000.0 && commitment < 20000000.0) predicted_class = 3.0;
         else predicted_class = 4.0;
-        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2.0);
-        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2.0);
+        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2);
+        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2);
         if (predicted_class == reference_vector[tclass]) sample_costs[2] = 0.0;
         else sample_costs[2] = 1.0;
         train_stats->update(sample_costs);
@@ -225,8 +225,8 @@
         else if (sales_prediction < 10000000.0 && commitment < 1000000.0) predicted_class = 2.0;
         else if (sales_prediction < 100000000.0 && commitment < 20000000.0) predicted_class = 3.0;
         else predicted_class = 4.0;
-        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2.0);
-        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2.0);
+        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2);
+        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2);
         if (predicted_class == reference_vector[tclass]) sample_costs[2] = 0.0;
         else sample_costs[2] = 1.0;
         train_stats->update(sample_costs);
@@ -254,8 +254,8 @@
         else if (sales_prediction < 10000000.0 && commitment < 1000000.0) predicted_class = 2.0;
         else if (sales_prediction < 100000000.0 && commitment < 20000000.0) predicted_class = 3.0;
         else predicted_class = 4.0;
-        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2.0);
-        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2.0);
+        sample_costs[0] = pow((sales_prediction - reference_vector[sales]), 2);
+        sample_costs[1] = pow((predicted_class - reference_vector[tclass]), 2);
         if (predicted_class == reference_vector[tclass]) sample_costs[2] = 0.0;
         else sample_costs[2] = 1.0;
         train_stats->update(sample_costs);
@@ -349,7 +349,7 @@
 
 void SecondIterationWrapper::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, const Vec& targetv, Vec& costsv) const
 {
-    costsv[0] = pow((outputv[0] - targetv[0]), 2.0);
+    costsv[0] = pow((outputv[0] - targetv[0]), 2);
     if (class_prediction == 1)
     {
         real class_pred;
@@ -357,7 +357,7 @@
         else if (outputv[0] <= 1.5) class_pred = 1.0;
         else if (outputv[0] <= 2.5) class_pred = 2.0;
         else class_pred = 3.0;
-        costsv[1] = pow((class_pred - targetv[0]), 2.0);
+        costsv[1] = pow((class_pred - targetv[0]), 2);
         costsv[2] = fabs(class_pred - targetv[0]);
         costsv[3] = class_pred == targetv[0]?0:1;
         return;



From nouiz at mail.berlios.de  Mon Feb 11 17:50:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 11 Feb 2008 17:50:50 +0100
Subject: [Plearn-commits] r8492 - trunk/plearn_learners
Message-ID: <200802111650.m1BGooZG029168@sheep.berlios.de>

Author: nouiz
Date: 2008-02-11 17:50:50 +0100 (Mon, 11 Feb 2008)
New Revision: 8492

Added:
   trunk/plearn_learners/second_iteration/
Log:
Merge the branch from Gilles Godbout


Copied: trunk/plearn_learners/second_iteration (from rev 8491, branches/cgi-desjardin/plearn_learners/second_iteration)



From nouiz at mail.berlios.de  Mon Feb 11 17:51:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 11 Feb 2008 17:51:29 +0100
Subject: [Plearn-commits] r8493 - trunk/plearn_learners
Message-ID: <200802111651.m1BGpTcG029218@sheep.berlios.de>

Author: nouiz
Date: 2008-02-11 17:51:29 +0100 (Mon, 11 Feb 2008)
New Revision: 8493

Added:
   trunk/plearn_learners/cgi/
Removed:
   trunk/plearn_learners/second_iteration/
Log:
renaded the folder to a better name


Copied: trunk/plearn_learners/cgi (from rev 8492, trunk/plearn_learners/second_iteration)



From nouiz at mail.berlios.de  Tue Feb 12 16:08:42 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 12 Feb 2008 16:08:42 +0100
Subject: [Plearn-commits] r8494 - in
	trunk/plearn_learners/regressors/test/RegressionTree: .
	.pytest/PL_RegressionTree/expected_results/expdir
	.pytest/PL_RegressionTree/expected_results/expdir/Split0
	.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
	.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0
Message-ID: <200802121508.m1CF8gaX008130@sheep.berlios.de>

Author: nouiz
Date: 2008-02-12 16:08:42 +0100 (Tue, 12 Feb 2008)
New Revision: 8494

Removed:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat.metadata/
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat.metadata/
Modified:
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
   trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
Log:
modified the test to have it run in a 5-10s instead of 500s...


Deleted: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/final_learner.psave	2008-02-12 15:08:42 UTC (rev 8494)
@@ -1,5655 +0,0 @@
-*1 ->HyperLearner(
-tester = *2 ->PTester(
-expdir = "PYTEST__PL_RegressionTree__RESULTS:expdir/Split0/LearnerExpdir/Strat0/Trials2/" ;
-dataset = *3 ->SubVMatrix(
-parent = *4 ->FileVMatrix(
-filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat" ;
-remove_when_done = -1 ;
-track_ref = -1 ;
-writable = 0 ;
-length = 200 ;
-width = 6 ;
-inputsize = 6 ;
-targetsize = 0 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/"  )
-;
-istart = 0 ;
-jstart = 0 ;
-fistart = -1 ;
-flength = -1 ;
-source = *4  ;
-writable = 0 ;
-length = 200 ;
-width = 6 ;
-inputsize = 4 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-splitter = *5 ->FractionSplitter(
-round_to_closest = 0 ;
-splits = 1  3  [ 
-(0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
-]
- )
-;
-statnames = 8 [ "E[test1.E[mse]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[mse]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
-statmask = []
-;
-learner = *6 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 50 ;
-compute_train_stats = 1 ;
-complexity_penalty_factor = 0 ;
-multiclass_outputs = []
-;
-leave_template = *7 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *0 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-sorted_train_set = *8 ->RegressionTreeRegisters(
-report_progress = 1 ;
-verbosity = 2 ;
-tsource = *9 ->MemoryVMatrixNoSave(
-source = *10 ->TransposeVMatrix(
-source = *11 ->SubVMatrix(
-parent = *4  ;
-istart = 0 ;
-jstart = 0 ;
-fistart = -1 ;
-flength = -1 ;
-source = *4  ;
-writable = 0 ;
-length = 150 ;
-width = 6 ;
-inputsize = 4 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-writable = 0 ;
-length = 6 ;
-width = 150 ;
-inputsize = 150 ;
-targetsize = 0 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-fieldnames = []
-;
-deep_copy_memory_data = 1 ;
-writable = 0 ;
-length = 6 ;
-width = 150 ;
-inputsize = 150 ;
-targetsize = 0 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-next_id = 244 ;
-leave_register = 150 [ 165 106 64 97 79 105 97 150 99 166 109 58 135 105 153 91 79 106 136 166 91 135 70 121 70 106 166 99 165 75 67 88 97 72 109 64 105 105 165 72 136 79 60 108 165 118 54 154 121 117 97 81 105 88 28 97 99 121 93 79 79 96 165 109 109 96 135 55 54 118 153 109 96 150 55 58 90 63 87 96 105 151 87 87 64 94 135 117 151 121 118 78 70 73 81 78 105 91 153 106 78 97 166 69 91 88 63 121 67 54 69 109 108 121 97 94 64 153 96 99 70 136 121 153 69 60 153 87 99 120 88 88 97 64 109 64 153 121 99 96 109 90 117 121 97 55 105 91 58 150 ] ;
-writable = 0 ;
-length = 150 ;
-width = 6 ;
-inputsize = 4 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-root = *12 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *13 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 150 ;
-weights_sum = 1.00000000000000244 ;
-targets_sum = 2401.03592000000026 ;
-weighted_targets_sum = 16.0069061333333345 ;
-weighted_squared_targets_sum = 9123.03187068025909 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *13  ;
-leave_output = 2 [ 16.0069061333332954 1 ] ;
-leave_error = 3 [ 17733.6216534378291 0 2.00000000000000488 ] ;
-split_col = 1 ;
-split_balance = 10 ;
-split_feature_value = 0.036194999999999998 ;
-after_split_error = 6339.63754059466464 ;
-missing_node = *0 ;
-missing_leave = *14 ->RegressionTreeLeave(
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *15 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *16 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 70 ;
-weights_sum = 0.466666666666666063 ;
-targets_sum = -4527.80441000000064 ;
-weighted_targets_sum = -30.185362733333335 ;
-weighted_squared_targets_sum = 3339.66270165469314 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *16  ;
-leave_output = 2 [ -64.6829201428572276 1 ] ;
-leave_error = 3 [ 2774.37058898262922 0 0.933333333333332127 ] ;
-split_col = 1 ;
-split_balance = 12 ;
-split_feature_value = -0.796699999999999964 ;
-after_split_error = 791.868290498600572 ;
-missing_node = *0 ;
-missing_leave = *17 ->RegressionTreeLeave(
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *18 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *19 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 29 ;
-weights_sum = 0.193333333333333218 ;
-targets_sum = -3465.00773000000072 ;
-weighted_targets_sum = -23.1000515333333354 ;
-weighted_squared_targets_sum = 2997.85468750103064 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *19  ;
-leave_output = 2 [ -119.483025172413875 1 ] ;
-leave_error = 3 [ 475.58129731941159 0 0.386666666666666436 ] ;
-split_col = 1 ;
-split_balance = 9 ;
-split_feature_value = -1.31519999999999992 ;
-after_split_error = 133.463062402859748 ;
-missing_node = *0 ;
-missing_leave = *20 ->RegressionTreeLeave(
-id = 17 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *21 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *22 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 10 ;
-weights_sum = 0.0666666666666666657 ;
-targets_sum = -1604.8422300000002 ;
-weighted_targets_sum = -10.698948200000002 ;
-weighted_squared_targets_sum = 1746.29511187725006 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *22  ;
-leave_output = 2 [ -160.484223000000043 1 ] ;
-leave_error = 3 [ 58.5654461660012444 0 0.133333333333333331 ] ;
-split_col = 1 ;
-split_balance = 4 ;
-split_feature_value = -1.76971500000000015 ;
-after_split_error = 16.8789931264006015 ;
-missing_node = *0 ;
-missing_leave = *23 ->RegressionTreeLeave(
-id = 29 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *24 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *25 ->RegressionTreeLeave(
-id = 30 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -562.481160000000045 ;
-weighted_targets_sum = -3.74987440000000039 ;
-weighted_squared_targets_sum = 704.078692073113416 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *25  ;
-leave_output = 2 [ -187.493720000000025 1 ] ;
-leave_error = 3 [ 2.00158256869046891 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -1.65359500000000015 ;
-after_split_error = 0.00309428292258406845 ;
-missing_node = *0 ;
-missing_leave = *26 ->RegressionTreeLeave(
-id = 71 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *27 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *28 ->RegressionTreeLeave(
-id = 72 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -384.983659999999986 ;
-weighted_targets_sum = -2.56655773333333359 ;
-weighted_squared_targets_sum = 494.042942031446728 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *28  ;
-leave_output = 2 [ -192.491829999999993 1 ] ;
-leave_error = 3 [ 0.00309428292276214822 0 0.00309428292276214822 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -2.05787500000000012 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
-id = 233 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *30 ->RegressionTreeLeave(
-id = 234 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -192.832469999999972 ;
-weighted_targets_sum = -1.28554980000000008 ;
-weighted_squared_targets_sum = 247.895743242006034 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *31 ->RegressionTreeLeave(
-id = 235 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -192.151190000000014 ;
-weighted_targets_sum = -1.28100793333333351 ;
-weighted_squared_targets_sum = 246.147198789440694 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *28  ;
-right_node = *32 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *33 ->RegressionTreeLeave(
-id = 73 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -177.497500000000002 ;
-weighted_targets_sum = -1.18331666666666679 ;
-weighted_squared_targets_sum = 210.035750041666688 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *33  ;
-leave_output = 2 [ -177.497500000000002 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *34 ->RegressionTreeLeave(
-id = 236 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *35 ->RegressionTreeLeave(
-id = 237 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *36 ->RegressionTreeLeave(
-id = 238 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *33   )
-;
-left_leave = *25  ;
-right_node = *37 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *38 ->RegressionTreeLeave(
-id = 31 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = -1042.36106999999993 ;
-weighted_targets_sum = -6.94907379999999986 ;
-weighted_squared_targets_sum = 1042.21641980413688 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *38  ;
-leave_output = 2 [ -148.908724285714271 1 ] ;
-leave_error = 3 [ 14.8774105577123557 0 0.0933333333333333376 ] ;
-split_col = 3 ;
-split_balance = 5 ;
-split_feature_value = -4.74906000000000006 ;
-after_split_error = 3.5971516681086797 ;
-missing_node = *0 ;
-missing_leave = *39 ->RegressionTreeLeave(
-id = 74 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *40 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *41 ->RegressionTreeLeave(
-id = 75 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -175.837510000000009 ;
-weighted_targets_sum = -1.17225006666666687 ;
-weighted_squared_targets_sum = 206.1255328200007 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *41  ;
-leave_output = 2 [ -175.837510000000009 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *42 ->RegressionTreeLeave(
-id = 113 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *43 ->RegressionTreeLeave(
-id = 114 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *44 ->RegressionTreeLeave(
-id = 115 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *41  ;
-right_node = *45 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *46 ->RegressionTreeLeave(
-id = 76 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 6 ;
-weights_sum = 0.0400000000000000008 ;
-targets_sum = -866.523559999999975 ;
-weighted_targets_sum = -5.77682373333333388 ;
-weighted_squared_targets_sum = 836.090886984136091 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *46  ;
-leave_output = 2 [ -144.420593333333358 1 ] ;
-leave_error = 3 [ 3.59715166810845233 0 0.0800000000000000017 ] ;
-split_col = 1 ;
-split_balance = 0 ;
-split_feature_value = -1.43682500000000002 ;
-after_split_error = 0.529347535980104866 ;
-missing_node = *0 ;
-missing_leave = *47 ->RegressionTreeLeave(
-id = 116 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *48 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *49 ->RegressionTreeLeave(
-id = 117 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -451.839400000000012 ;
-weighted_targets_sum = -3.01226266666666698 ;
-weighted_squared_targets_sum = 453.827814718630691 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *49  ;
-leave_output = 2 [ -150.613133333333337 1 ] ;
-leave_error = 3 [ 0.282992137883448458 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -3.13504000000000005 ;
-after_split_error = 0.00999469557601595504 ;
-missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
-id = 197 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *51 ->RegressionTreeLeave(
-id = 198 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -146.918560000000014 ;
-weighted_targets_sum = -0.979457066666666876 ;
-weighted_squared_targets_sum = 143.900421816490706 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *52 ->RegressionTreeLeave(
-id = 199 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -304.920839999999998 ;
-weighted_targets_sum = -2.0328056000000001 ;
-weighted_squared_targets_sum = 309.927392902140014 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *49  ;
-right_node = *53 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *54 ->RegressionTreeLeave(
-id = 118 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -414.68416000000002 ;
-weighted_targets_sum = -2.7645610666666669 ;
-weighted_squared_targets_sum = 382.2630722655054 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *54  ;
-leave_output = 2 [ -138.228053333333349 1 ] ;
-leave_error = 3 [ 0.246355398096885225 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -2.44208500000000006 ;
-after_split_error = 0.183220482032616871 ;
-missing_node = *0 ;
-missing_leave = *55 ->RegressionTreeLeave(
-id = 200 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *56 ->RegressionTreeLeave(
-id = 201 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -136.49520000000004 ;
-weighted_targets_sum = -0.909967999999999999 ;
-weighted_squared_targets_sum = 124.206264153600031 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *57 ->RegressionTreeLeave(
-id = 202 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -278.188960000000009 ;
-weighted_targets_sum = -1.85459306666666679 ;
-weighted_squared_targets_sum = 258.056808111905355 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *54   )
-;
-right_leave = *46   )
-;
-right_leave = *38   )
-;
-left_leave = *22  ;
-right_node = *58 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *59 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 19 ;
-weights_sum = 0.126666666666666677 ;
-targets_sum = -1860.16549999999984 ;
-weighted_targets_sum = -12.4011033333333351 ;
-weighted_squared_targets_sum = 1251.55957562378012 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *59  ;
-leave_output = 2 [ -97.9034473684210553 1 ] ;
-leave_error = 3 [ 74.8976162368580418 0 0.253333333333333355 ] ;
-split_col = 1 ;
-split_balance = 7 ;
-split_feature_value = -1.16121499999999989 ;
-after_split_error = 21.5111374778552857 ;
-missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
-id = 32 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *61 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *62 ->RegressionTreeLeave(
-id = 33 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 6 ;
-weights_sum = 0.0400000000000000008 ;
-targets_sum = -715.629170000000045 ;
-weighted_targets_sum = -4.77086113333333373 ;
-weighted_squared_targets_sum = 570.045923418595407 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *62  ;
-leave_output = 2 [ -119.271528333333336 1 ] ;
-leave_error = 3 [ 2.03604915965981625 0 0.0800000000000000017 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -1.10694000000000004 ;
-after_split_error = 0.221228036537741812 ;
-missing_node = *0 ;
-missing_leave = *63 ->RegressionTreeLeave(
-id = 53 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *64 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *65 ->RegressionTreeLeave(
-id = 54 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -372.10329999999999 ;
-weighted_targets_sum = -2.48068866666666699 ;
-weighted_squared_targets_sum = 307.693557698994709 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *65  ;
-leave_output = 2 [ -124.034433333333354 1 ] ;
-leave_error = 3 [ 0.00548930514478163101 0 0.00548930514478163101 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -1.69603999999999999 ;
-after_split_error = 0.000219082922519264756 ;
-missing_node = *0 ;
-missing_leave = *66 ->RegressionTreeLeave(
-id = 239 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *67 ->RegressionTreeLeave(
-id = 240 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -124.200460000000007 ;
-weighted_targets_sum = -0.828003066666666898 ;
-weighted_squared_targets_sum = 102.838361761410681 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *68 ->RegressionTreeLeave(
-id = 241 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -247.902839999999998 ;
-weighted_targets_sum = -1.65268560000000009 ;
-weighted_squared_targets_sum = 204.855195937584028 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *65  ;
-right_node = *69 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *70 ->RegressionTreeLeave(
-id = 55 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -343.525869999999998 ;
-weighted_targets_sum = -2.29017246666666674 ;
-weighted_squared_targets_sum = 262.352365719600698 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *70  ;
-leave_output = 2 [ -114.508623333333333 1 ] ;
-leave_error = 3 [ 0.215738731392987493 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.509650000000000047 ;
-after_split_error = 0.128841484970660891 ;
-missing_node = *0 ;
-missing_leave = *71 ->RegressionTreeLeave(
-id = 242 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *72 ->RegressionTreeLeave(
-id = 243 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -113.352759999999989 ;
-weighted_targets_sum = -0.755685066666666683 ;
-weighted_squared_targets_sum = 85.65898799745068 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *73 ->RegressionTreeLeave(
-id = 244 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -230.173110000000008 ;
-weighted_targets_sum = -1.53448739999999995 ;
-weighted_squared_targets_sum = 176.693377722149989 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *70   )
-;
-left_leave = *62  ;
-right_node = *74 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *75 ->RegressionTreeLeave(
-id = 34 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 13 ;
-weights_sum = 0.0866666666666666696 ;
-targets_sum = -1144.53632999999968 ;
-weighted_targets_sum = -7.63024219999999964 ;
-weighted_squared_targets_sum = 681.513652205184826 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *75  ;
-leave_output = 2 [ -88.0412561538461489 1 ] ;
-leave_error = 3 [ 19.4750883181964802 0 0.173333333333333339 ] ;
-split_col = 2 ;
-split_balance = 7 ;
-split_feature_value = -6.68825000000000003 ;
-after_split_error = 5.81526685152111966 ;
-missing_node = *0 ;
-missing_leave = *76 ->RegressionTreeLeave(
-id = 56 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *77 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *78 ->RegressionTreeLeave(
-id = 57 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 10 ;
-weights_sum = 0.0666666666666666657 ;
-targets_sum = -929.035599999999931 ;
-weighted_targets_sum = -6.19357066666666611 ;
-weighted_squared_targets_sum = 577.502672231901329 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *78  ;
-leave_output = 2 [ -92.9035599999999988 1 ] ;
-leave_error = 3 [ 4.19581637398951024 0 0.133333333333333331 ] ;
-split_col = 1 ;
-split_balance = 6 ;
-split_feature_value = -1.04829499999999998 ;
-after_split_error = 2.15172959846818168 ;
-missing_node = *0 ;
-missing_leave = *79 ->RegressionTreeLeave(
-id = 107 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *80 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *81 ->RegressionTreeLeave(
-id = 108 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -201.468869999999981 ;
-weighted_targets_sum = -1.34312580000000015 ;
-weighted_squared_targets_sum = 135.305857542979339 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *81  ;
-leave_output = 2 [ -100.734435000000005 1 ] ;
-leave_error = 3 [ 0.0136778921126326503 0 0.0136778921126326503 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -0.441430000000000045 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *82 ->RegressionTreeLeave(
-id = 227 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *83 ->RegressionTreeLeave(
-id = 228 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -100.018249999999981 ;
-weighted_targets_sum = -0.666788333333333205 ;
-weighted_squared_targets_sum = 66.6910022204166637 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *84 ->RegressionTreeLeave(
-id = 229 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -101.450620000000001 ;
-weighted_targets_sum = -0.67633746666666672 ;
-weighted_squared_targets_sum = 68.6148553225626756 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *81  ;
-right_node = *85 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *86 ->RegressionTreeLeave(
-id = 109 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = -727.566730000000007 ;
-weighted_targets_sum = -4.85044486666666685 ;
-weighted_squared_targets_sum = 442.196814688922075 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *86  ;
-leave_output = 2 [ -90.9458412500000009 1 ] ;
-leave_error = 3 [ 2.13805170635598518 0 0.106666666666666674 ] ;
-split_col = 2 ;
-split_balance = 2 ;
-split_feature_value = -9.18992000000000075 ;
-after_split_error = 0.337813845131658352 ;
-missing_node = *0 ;
-missing_leave = *87 ->RegressionTreeLeave(
-id = 230 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *88 ->RegressionTreeLeave(
-id = 231 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = -91.9765299999998547 ;
-weighted_targets_sum = -0.613176866666666598 ;
-weighted_squared_targets_sum = 56.397880472272675 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *89 ->RegressionTreeLeave(
-id = 232 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = -635.590200000000095 ;
-weighted_targets_sum = -4.23726800000000026 ;
-weighted_squared_targets_sum = 385.7989342166494 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *86   )
-;
-left_leave = *78  ;
-right_node = *90 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *91 ->RegressionTreeLeave(
-id = 58 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -215.500729999999976 ;
-weighted_targets_sum = -1.43667153333333331 ;
-weighted_squared_targets_sum = 104.010979973283341 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *91  ;
-leave_output = 2 [ -71.8335766666666586 1 ] ;
-leave_error = 3 [ 1.61945047753160898 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 2.85157499999999997 ;
-after_split_error = 0.0267389822939781974 ;
-missing_node = *0 ;
-missing_leave = *92 ->RegressionTreeLeave(
-id = 110 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *93 ->RegressionTreeLeave(
-id = 111 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -75.2941599999999909 ;
-weighted_targets_sum = -0.501961066666666733 ;
-weighted_squared_targets_sum = 37.7947368673706734 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *94 ->RegressionTreeLeave(
-id = 112 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -140.206569999999999 ;
-weighted_targets_sum = -0.934710466666666684 ;
-weighted_squared_targets_sum = 66.2162431059126675 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *91   )
-;
-right_leave = *75   )
-;
-right_leave = *59   )
-;
-left_leave = *19  ;
-right_node = *95 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *96 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 41 ;
-weights_sum = 0.273333333333333095 ;
-targets_sum = -1062.7966799999997 ;
-weighted_targets_sum = -7.08531119999999959 ;
-weighted_squared_targets_sum = 341.808014153662668 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *96  ;
-leave_output = 2 [ -25.9218702439024611 1 ] ;
-leave_error = 3 [ 316.286993179187675 0 0.54666666666666619 ] ;
-split_col = 1 ;
-split_balance = 3 ;
-split_feature_value = -0.34517500000000001 ;
-after_split_error = 105.868276535697049 ;
-missing_node = *0 ;
-missing_leave = *97 ->RegressionTreeLeave(
-id = 20 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *98 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *99 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 19 ;
-weights_sum = 0.126666666666666677 ;
-targets_sum = -893.630649999999946 ;
-weighted_targets_sum = -5.95753766666666706 ;
-weighted_squared_targets_sum = 305.178639377724721 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *99  ;
-leave_output = 2 [ -47.0331921052631543 1 ] ;
-leave_error = 3 [ 49.9532516541003346 0 0.253333333333333355 ] ;
-split_col = 2 ;
-split_balance = 3 ;
-split_feature_value = -7.35407999999999973 ;
-after_split_error = 19.8796985770064367 ;
-missing_node = *0 ;
-missing_leave = *100 ->RegressionTreeLeave(
-id = 41 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *101 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *102 ->RegressionTreeLeave(
-id = 42 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = -478.474159999999983 ;
-weighted_targets_sum = -3.1898277333333338 ;
-weighted_squared_targets_sum = 194.724798247468016 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *102  ;
-leave_output = 2 [ -59.809270000000005 1 ] ;
-leave_error = 3 [ 7.88706018209330217 0 0.106666666666666674 ] ;
-split_col = 1 ;
-split_balance = 2 ;
-split_feature_value = -0.581860000000000044 ;
-after_split_error = 0.868682218515694049 ;
-missing_node = *0 ;
-missing_leave = *103 ->RegressionTreeLeave(
-id = 77 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *104 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *105 ->RegressionTreeLeave(
-id = 78 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -210.84371999999999 ;
-weighted_targets_sum = -1.40562480000000001 ;
-weighted_squared_targets_sum = 99.065637851193344 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *105  ;
-leave_output = 2 [ -70.2812399999999968 1 ] ;
-leave_error = 3 [ 0.553167864882681992 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -2.23238499999999984 ;
-after_split_error = 0.168978629290691268 ;
-missing_node = *0 ;
-missing_leave = *106 ->RegressionTreeLeave(
-id = 143 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *107 ->RegressionTreeLeave(
-id = 144 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -65.8983800000000031 ;
-weighted_targets_sum = -0.439322533333333376 ;
-weighted_squared_targets_sum = 28.9506432441626735 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *108 ->RegressionTreeLeave(
-id = 145 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -144.945339999999987 ;
-weighted_targets_sum = -0.966302266666666632 ;
-weighted_squared_targets_sum = 70.1149946070306669 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *105  ;
-right_node = *109 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *110 ->RegressionTreeLeave(
-id = 79 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = -267.630440000000021 ;
-weighted_targets_sum = -1.78420293333333335 ;
-weighted_squared_targets_sum = 95.6591603962746717 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *110  ;
-leave_output = 2 [ -53.5260880000000014 1 ] ;
-leave_error = 3 [ 0.315514353633076561 0 0.0666666666666666657 ] ;
-split_col = 1 ;
-split_balance = 1 ;
-split_feature_value = -0.501179999999999959 ;
-after_split_error = 0.0204439538035167401 ;
-missing_node = *0 ;
-missing_leave = *111 ->RegressionTreeLeave(
-id = 146 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *112 ->RegressionTreeLeave(
-id = 147 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = -52.4257500000000078 ;
-weighted_targets_sum = -0.349504999999999844 ;
-weighted_squared_targets_sum = 18.3230617537499931 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *113 ->RegressionTreeLeave(
-id = 148 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = -215.204690000000028 ;
-weighted_targets_sum = -1.4346979333333334 ;
-weighted_squared_targets_sum = 77.336098642524675 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *110   )
-;
-left_leave = *102  ;
-right_node = *114 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *115 ->RegressionTreeLeave(
-id = 43 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 11 ;
-weights_sum = 0.0733333333333333337 ;
-targets_sum = -415.156490000000019 ;
-weighted_targets_sum = -2.76770993333333326 ;
-weighted_squared_targets_sum = 110.453841130256677 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *115  ;
-leave_output = 2 [ -37.7414990909090875 1 ] ;
-leave_error = 3 [ 11.9926383949132571 0 0.146666666666666667 ] ;
-split_col = 1 ;
-split_balance = 7 ;
-split_feature_value = -0.617779999999999996 ;
-after_split_error = 5.7658396809937118 ;
-missing_node = *0 ;
-missing_leave = *116 ->RegressionTreeLeave(
-id = 80 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *117 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *118 ->RegressionTreeLeave(
-id = 81 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -103.127129999999994 ;
-weighted_targets_sum = -0.687514200000000075 ;
-weighted_squared_targets_sum = 35.6753355603233402 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *118  ;
-leave_output = 2 [ -51.5635650000000041 1 ] ;
-leave_error = 3 [ 0.449304840400662542 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 2.03620000000000001 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *119 ->RegressionTreeLeave(
-id = 161 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *120 ->RegressionTreeLeave(
-id = 162 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -55.6683099999999911 ;
-weighted_targets_sum = -0.371122066666666695 ;
-weighted_squared_targets_sum = 20.6597382550406685 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *121 ->RegressionTreeLeave(
-id = 163 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -47.4588200000000029 ;
-weighted_targets_sum = -0.316392133333333381 ;
-weighted_squared_targets_sum = 15.0155973052826699 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *118  ;
-right_node = *122 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *123 ->RegressionTreeLeave(
-id = 82 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 9 ;
-weights_sum = 0.0600000000000000047 ;
-targets_sum = -312.029360000000054 ;
-weighted_targets_sum = -2.08019573333333341 ;
-weighted_squared_targets_sum = 74.7785055699333441 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *123  ;
-leave_output = 2 [ -34.6699288888888901 1 ] ;
-leave_error = 3 [ 5.31653484059320025 0 0.120000000000000009 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.591400000000000037 ;
-after_split_error = 1.59838525525966713 ;
-missing_node = *0 ;
-missing_leave = *124 ->RegressionTreeLeave(
-id = 164 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *125 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *126 ->RegressionTreeLeave(
-id = 165 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = -198.243249999999989 ;
-weighted_targets_sum = -1.32162166666666669 ;
-weighted_squared_targets_sum = 52.7188190578006726 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *126  ;
-leave_output = 2 [ -39.6486500000000035 1 ] ;
-leave_error = 3 [ 0.636608327434661891 0 0.0666666666666666657 ] ;
-split_col = 3 ;
-split_balance = 3 ;
-split_feature_value = -1.56711 ;
-after_split_error = 0.405305692174654175 ;
-missing_node = *0 ;
-missing_leave = *127 ->RegressionTreeLeave(
-id = 191 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *128 ->RegressionTreeLeave(
-id = 192 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = -35.9233100000000007 ;
-weighted_targets_sum = -0.23948873333333337 ;
-weighted_squared_targets_sum = 8.60322800904066831 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *129 ->RegressionTreeLeave(
-id = 193 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = -162.319939999999974 ;
-weighted_targets_sum = -1.08213293333333338 ;
-weighted_squared_targets_sum = 44.1155910487599954 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *126  ;
-right_node = *130 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *131 ->RegressionTreeLeave(
-id = 166 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = -113.786109999999994 ;
-weighted_targets_sum = -0.758574066666666713 ;
-weighted_squared_targets_sum = 22.059686512132668 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *131  ;
-leave_output = 2 [ -28.4465274999999984 1 ] ;
-leave_error = 3 [ 0.961776927825005234 0 0.0533333333333333368 ] ;
-split_col = 1 ;
-split_balance = 2 ;
-split_feature_value = -0.412640000000000007 ;
-after_split_error = 0.122110760935989035 ;
-missing_node = *0 ;
-missing_leave = *132 ->RegressionTreeLeave(
-id = 194 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *133 ->RegressionTreeLeave(
-id = 195 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -33.20474999999999 ;
-weighted_targets_sum = -0.221365000000000062 ;
-weighted_squared_targets_sum = 7.35036948375000065 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *134 ->RegressionTreeLeave(
-id = 196 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -80.5813599999999894 ;
-weighted_targets_sum = -0.537209066666666679 ;
-weighted_squared_targets_sum = 14.7093170283826673 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *131   )
-;
-right_leave = *123   )
-;
-right_leave = *115   )
-;
-left_leave = *99  ;
-right_node = *135 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *136 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 22 ;
-weights_sum = 0.14666666666666664 ;
-targets_sum = -169.166030000000035 ;
-weighted_targets_sum = -1.12777353333333341 ;
-weighted_squared_targets_sum = 36.6293747759380111 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *136  ;
-leave_output = 2 [ -7.68936500000000223 1 ] ;
-leave_error = 3 [ 55.9150248815966791 0 0.293333333333333279 ] ;
-split_col = 1 ;
-split_balance = 4 ;
-split_feature_value = -0.0821300000000000086 ;
-after_split_error = 26.8904632651653444 ;
-missing_node = *0 ;
-missing_leave = *137 ->RegressionTreeLeave(
-id = 44 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *138 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *139 ->RegressionTreeLeave(
-id = 45 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 13 ;
-weights_sum = 0.0866666666666666696 ;
-targets_sum = -207.557480000000027 ;
-weighted_targets_sum = -1.38371653333333322 ;
-weighted_squared_targets_sum = 30.1523790784066676 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *139  ;
-leave_output = 2 [ -15.965959999999999 1 ] ;
-leave_error = 3 [ 16.1200325117360102 0 0.173333333333333339 ] ;
-split_col = 3 ;
-split_balance = 3 ;
-split_feature_value = -1.15891499999999992 ;
-after_split_error = 9.19154126603875987 ;
-missing_node = *0 ;
-missing_leave = *140 ->RegressionTreeLeave(
-id = 83 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *141 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *142 ->RegressionTreeLeave(
-id = 84 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = -119.815830000000005 ;
-weighted_targets_sum = -0.798772200000000043 ;
-weighted_squared_targets_sum = 20.6369126752206711 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *142  ;
-leave_output = 2 [ -23.9631660000000011 1 ] ;
-leave_error = 3 [ 2.9916037008709373 0 0.0666666666666666657 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -2.12482499999999996 ;
-after_split_error = 0.746847325971338849 ;
-missing_node = *0 ;
-missing_leave = *143 ->RegressionTreeLeave(
-id = 149 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *144 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *145 ->RegressionTreeLeave(
-id = 150 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -57.6758400000000009 ;
-weighted_targets_sum = -0.384505600000000003 ;
-weighted_squared_targets_sum = 7.64076145701733456 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *145  ;
-leave_output = 2 [ -19.2252800000000015 1 ] ;
-leave_error = 3 [ 0.4970672708986692 0 0.0400000000000000008 ] ;
-split_col = 1 ;
-split_balance = 1 ;
-split_feature_value = -0.10633999999999999 ;
-after_split_error = 0.00781290769066451431 ;
-missing_node = *0 ;
-missing_leave = *146 ->RegressionTreeLeave(
-id = 215 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *147 ->RegressionTreeLeave(
-id = 216 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -22.2395500000000013 ;
-weighted_targets_sum = -0.148263666666666655 ;
-weighted_squared_targets_sum = 3.29731722801666738 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *148 ->RegressionTreeLeave(
-id = 217 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -35.4362899999999996 ;
-weighted_targets_sum = -0.236241933333333348 ;
-weighted_squared_targets_sum = 4.34344422900066718 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *145  ;
-right_node = *149 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *150 ->RegressionTreeLeave(
-id = 151 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = -62.1399899999999974 ;
-weighted_targets_sum = -0.41426660000000004 ;
-weighted_squared_targets_sum = 12.9961512182033339 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *150  ;
-leave_output = 2 [ -31.0699950000000022 1 ] ;
-leave_error = 3 [ 0.249780055072664348 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -1.68244499999999997 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *151 ->RegressionTreeLeave(
-id = 218 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *152 ->RegressionTreeLeave(
-id = 219 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -28.0094799999999964 ;
-weighted_targets_sum = -0.186729866666666688 ;
-weighted_squared_targets_sum = 5.23020646580266568 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *153 ->RegressionTreeLeave(
-id = 220 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = -34.130510000000001 ;
-weighted_targets_sum = -0.227536733333333352 ;
-weighted_squared_targets_sum = 7.76594475240066817 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *150   )
-;
-left_leave = *142  ;
-right_node = *154 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *155 ->RegressionTreeLeave(
-id = 85 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = -87.7416499999999928 ;
-weighted_targets_sum = -0.584944333333333399 ;
-weighted_squared_targets_sum = 9.5154664031860019 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *155  ;
-leave_output = 2 [ -10.9677062500000009 1 ] ;
-leave_error = 3 [ 6.19993756516783456 0 0.106666666666666674 ] ;
-split_col = 3 ;
-split_balance = 6 ;
-split_feature_value = 3.08020000000000005 ;
-after_split_error = 2.37574213259581013 ;
-missing_node = *0 ;
-missing_leave = *156 ->RegressionTreeLeave(
-id = 152 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *157 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *158 ->RegressionTreeLeave(
-id = 153 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = -92.6157499999999914 ;
-weighted_targets_sum = -0.617438333333333422 ;
-weighted_squared_targets_sum = 9.3570873977860014 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *158  ;
-leave_output = 2 [ -13.2308214285714296 1 ] ;
-leave_error = 3 [ 2.37574213259581013 0 0.0933333333333333376 ] ;
-split_col = 1 ;
-split_balance = 1 ;
-split_feature_value = -0.262695000000000012 ;
-after_split_error = 0.82728846988722271 ;
-missing_node = *0 ;
-missing_leave = *159 ->RegressionTreeLeave(
-id = 185 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *160 ->RegressionTreeLeave(
-id = 186 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = -10.5931799999999878 ;
-weighted_targets_sum = -0.0706212000000001061 ;
-weighted_squared_targets_sum = 0.748103083416000847 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *161 ->RegressionTreeLeave(
-id = 187 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 6 ;
-weights_sum = 0.0400000000000000008 ;
-targets_sum = -82.0225700000000018 ;
-weighted_targets_sum = -0.546817133333333372 ;
-weighted_squared_targets_sum = 8.60898431436999978 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *158  ;
-right_node = *162 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *163 ->RegressionTreeLeave(
-id = 154 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 4.87410000000000032 ;
-weighted_targets_sum = 0.032494000000000002 ;
-weighted_squared_targets_sum = 0.158379005400000022 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *163  ;
-leave_output = 2 [ 4.87410000000000032 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *164 ->RegressionTreeLeave(
-id = 188 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *165 ->RegressionTreeLeave(
-id = 189 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *166 ->RegressionTreeLeave(
-id = 190 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *163   )
-;
-right_leave = *155   )
-;
-left_leave = *139  ;
-right_node = *167 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *168 ->RegressionTreeLeave(
-id = 46 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 9 ;
-weights_sum = 0.0600000000000000047 ;
-targets_sum = 38.391449999999999 ;
-weighted_targets_sum = 0.255943000000000032 ;
-weighted_squared_targets_sum = 6.47699569753133275 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *168  ;
-leave_output = 2 [ 4.26571666666666705 1 ] ;
-leave_error = 3 [ 10.7704307534293324 0 0.120000000000000009 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -0.0792649999999999744 ;
-after_split_error = 2.14048019403406808 ;
-missing_node = *0 ;
-missing_leave = *169 ->RegressionTreeLeave(
-id = 86 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *170 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *171 ->RegressionTreeLeave(
-id = 87 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = -20.8623900000000013 ;
-weighted_targets_sum = -0.139082600000000001 ;
-weighted_squared_targets_sum = 1.25263627282333334 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *171  ;
-leave_output = 2 [ -5.21559749999999944 1 ] ;
-leave_error = 3 [ 1.05447482393966685 0 0.0533333333333333368 ] ;
-split_col = 3 ;
-split_balance = 2 ;
-split_feature_value = -1.85971500000000001 ;
-after_split_error = 0.139893091254222196 ;
-missing_node = *0 ;
-missing_leave = *172 ->RegressionTreeLeave(
-id = 125 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *173 ->RegressionTreeLeave(
-id = 126 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = -12.3881300000000003 ;
-weighted_targets_sum = -0.0825875333333333378 ;
-weighted_squared_targets_sum = 1.02310509931266669 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *174 ->RegressionTreeLeave(
-id = 127 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = -8.47425999999999924 ;
-weighted_targets_sum = -0.0564950666666666698 ;
-weighted_squared_targets_sum = 0.229531173510666681 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *171  ;
-right_node = *175 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *176 ->RegressionTreeLeave(
-id = 88 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 59.2538400000000038 ;
-weighted_targets_sum = 0.395025600000000032 ;
-weighted_squared_targets_sum = 5.22435942470800008 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *176  ;
-leave_output = 2 [ 11.8507680000000004 1 ] ;
-leave_error = 3 [ 1.08600537009439835 0 0.0666666666666666657 ] ;
-split_col = 3 ;
-split_balance = 3 ;
-split_feature_value = 2.88428500000000021 ;
-after_split_error = 0.734990923745331592 ;
-missing_node = *0 ;
-missing_leave = *177 ->RegressionTreeLeave(
-id = 128 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *178 ->RegressionTreeLeave(
-id = 129 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 10.0296400000000041 ;
-weighted_targets_sum = 0.0668642666666666996 ;
-weighted_squared_targets_sum = 0.670624523530667149 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *179 ->RegressionTreeLeave(
-id = 130 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 49.2241999999999962 ;
-weighted_targets_sum = 0.328161333333333305 ;
-weighted_squared_targets_sum = 4.55373490117733315 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *176   )
-;
-right_leave = *168   )
-;
-right_leave = *136   )
-;
-right_leave = *96   )
-;
-left_leave = *16  ;
-right_node = *180 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *181 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 80 ;
-weights_sum = 0.533333333333332882 ;
-targets_sum = 6928.84032999999908 ;
-weighted_targets_sum = 46.1922688666666588 ;
-weighted_squared_targets_sum = 5783.36916902556004 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *181  ;
-leave_output = 2 [ 86.6105041250000625 1 ] ;
-leave_error = 3 [ 3565.26695161203133 0 1.06666666666666576 ] ;
-split_col = 1 ;
-split_balance = 20 ;
-split_feature_value = 0.937115000000000031 ;
-after_split_error = 1048.37002708401451 ;
-missing_node = *0 ;
-missing_leave = *182 ->RegressionTreeLeave(
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *183 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *184 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 50 ;
-weights_sum = 0.333333333333332982 ;
-targets_sum = 2449.19952999999987 ;
-weighted_targets_sum = 16.3279968666666662 ;
-weighted_squared_targets_sum = 1012.36507460954613 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *184  ;
-leave_output = 2 [ 48.9839906000000482 1 ] ;
-leave_error = 3 [ 425.10925915183185 0 0.666666666666665964 ] ;
-split_col = 1 ;
-split_balance = 6 ;
-split_feature_value = 0.432620000000000005 ;
-after_split_error = 121.842470789270692 ;
-missing_node = *0 ;
-missing_leave = *185 ->RegressionTreeLeave(
-id = 11 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *186 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *187 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 28 ;
-weights_sum = 0.186666666666666564 ;
-targets_sum = 842.195039999999949 ;
-weighted_targets_sum = 5.61463360000000122 ;
-weighted_squared_targets_sum = 189.51913846573197 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *187  ;
-leave_output = 2 [ 30.0783942857143103 1 ] ;
-leave_error = 3 [ 41.2799505502247399 0 0.373333333333333128 ] ;
-split_col = 1 ;
-split_balance = 2 ;
-split_feature_value = 0.26894499999999999 ;
-after_split_error = 22.4639602548301482 ;
-missing_node = *0 ;
-missing_leave = *188 ->RegressionTreeLeave(
-id = 35 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *189 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *190 ->RegressionTreeLeave(
-id = 36 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 15 ;
-weights_sum = 0.100000000000000006 ;
-targets_sum = 352.039670000000001 ;
-weighted_targets_sum = 2.34693113333333336 ;
-weighted_squared_targets_sum = 59.5814975016619925 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *190  ;
-leave_output = 2 [ 23.4693113333333336 1 ] ;
-leave_error = 3 [ 9.00128011113828741 0 0.200000000000000011 ] ;
-split_col = 3 ;
-split_balance = 3 ;
-split_feature_value = 0.287649999999999961 ;
-after_split_error = 4.55178643471853928 ;
-missing_node = *0 ;
-missing_leave = *191 ->RegressionTreeLeave(
-id = 95 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *192 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *193 ->RegressionTreeLeave(
-id = 96 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 6 ;
-weights_sum = 0.0400000000000000008 ;
-targets_sum = 106.155180000000001 ;
-weighted_targets_sum = 0.70770120000000003 ;
-weighted_squared_targets_sum = 13.2969076162293316 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *193  ;
-leave_output = 2 [ 17.6925300000000014 1 ] ;
-leave_error = 3 [ 1.55176580838666167 0 0.0800000000000000017 ] ;
-split_col = 1 ;
-split_balance = 4 ;
-split_feature_value = 0.107589999999999991 ;
-after_split_error = 0.446338964225057477 ;
-missing_node = *0 ;
-missing_leave = *194 ->RegressionTreeLeave(
-id = 179 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *195 ->RegressionTreeLeave(
-id = 180 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 21.1915699999999987 ;
-weighted_targets_sum = 0.141277133333333388 ;
-weighted_squared_targets_sum = 2.9938842604326652 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *196 ->RegressionTreeLeave(
-id = 181 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 84.9636100000000027 ;
-weighted_targets_sum = 0.56642406666666667 ;
-weighted_squared_targets_sum = 10.3030233557966664 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *193  ;
-right_node = *197 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *198 ->RegressionTreeLeave(
-id = 97 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 9 ;
-weights_sum = 0.0600000000000000047 ;
-targets_sum = 245.884489999999971 ;
-weighted_targets_sum = 1.63922993333333356 ;
-weighted_squared_targets_sum = 46.2845898854326663 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *198  ;
-leave_output = 2 [ 27.320498888888892 1 ] ;
-leave_error = 3 [ 3.00002062633182565 0 0.120000000000000009 ] ;
-split_col = 3 ;
-split_balance = 3 ;
-split_feature_value = 2.14775000000000027 ;
-after_split_error = 1.60150674033000873 ;
-missing_node = *0 ;
-missing_leave = *199 ->RegressionTreeLeave(
-id = 182 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *200 ->RegressionTreeLeave(
-id = 183 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 30.6246100000000006 ;
-weighted_targets_sum = 0.204164066666666616 ;
-weighted_squared_targets_sum = 6.25244491768066712 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *201 ->RegressionTreeLeave(
-id = 184 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = 215.25988000000001 ;
-weighted_targets_sum = 1.43506586666666669 ;
-weighted_squared_targets_sum = 40.0321449677520036 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *198   )
-;
-left_leave = *190  ;
-right_node = *202 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *203 ->RegressionTreeLeave(
-id = 37 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 13 ;
-weights_sum = 0.0866666666666666696 ;
-targets_sum = 490.155370000000005 ;
-weighted_targets_sum = 3.26770246666666697 ;
-weighted_squared_targets_sum = 129.937640964069999 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *203  ;
-leave_output = 2 [ 37.7042592307692317 1 ] ;
-leave_error = 3 [ 13.4626801436918626 0 0.173333333333333339 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = -0.349470000000000003 ;
-after_split_error = 5.07508760070883014 ;
-missing_node = *0 ;
-missing_leave = *204 ->RegressionTreeLeave(
-id = 98 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *205 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *206 ->RegressionTreeLeave(
-id = 99 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 6 ;
-weights_sum = 0.0400000000000000008 ;
-targets_sum = 181.14364999999998 ;
-weighted_targets_sum = 1.20762433333333341 ;
-weighted_squared_targets_sum = 37.1207787902646729 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *206  ;
-leave_output = 2 [ 30.1906083333333335 1 ] ;
-leave_error = 3 [ 1.32373105759045329 0 0.0800000000000000017 ] ;
-split_col = 3 ;
-split_balance = 2 ;
-split_feature_value = -1.86292999999999997 ;
-after_split_error = 0.274094608278331597 ;
-missing_node = *0 ;
-missing_leave = *207 ->RegressionTreeLeave(
-id = 131 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *208 ->RegressionTreeLeave(
-id = 132 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 26.9559800000000003 ;
-weighted_targets_sum = 0.179706533333333446 ;
-weighted_squared_targets_sum = 4.84416571840267363 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *209 ->RegressionTreeLeave(
-id = 133 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 154.187669999999997 ;
-weighted_targets_sum = 1.02791779999999999 ;
-weighted_squared_targets_sum = 32.2766130718620019 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *206  ;
-right_node = *210 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *211 ->RegressionTreeLeave(
-id = 100 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = 309.011719999999968 ;
-weighted_targets_sum = 2.06007813333333356 ;
-weighted_squared_targets_sum = 92.8168621738053332 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *211  ;
-leave_output = 2 [ 44.1445314285714332 1 ] ;
-leave_error = 3 [ 3.75135654311842615 0 0.0933333333333333376 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 1.28273500000000018 ;
-after_split_error = 1.16191341589332642 ;
-missing_node = *0 ;
-missing_leave = *212 ->RegressionTreeLeave(
-id = 134 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *213 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *214 ->RegressionTreeLeave(
-id = 135 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 158.331799999999987 ;
-weighted_targets_sum = 1.05554533333333356 ;
-weighted_squared_targets_sum = 42.328464357964009 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *214  ;
-leave_output = 2 [ 39.5829500000000039 1 ] ;
-leave_error = 3 [ 1.09373241179465586 0 0.0533333333333333368 ] ;
-split_col = 1 ;
-split_balance = 0 ;
-split_feature_value = 0.299314999999999998 ;
-after_split_error = 0.240273207153327617 ;
-missing_node = *0 ;
-missing_leave = *215 ->RegressionTreeLeave(
-id = 203 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *216 ->RegressionTreeLeave(
-id = 204 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 46.3773499999999856 ;
-weighted_targets_sum = 0.309182333333333503 ;
-weighted_squared_targets_sum = 14.3390572868166686 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *217 ->RegressionTreeLeave(
-id = 205 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 111.954450000000008 ;
-weighted_targets_sum = 0.74636300000000011 ;
-weighted_squared_targets_sum = 27.9894070711473368 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *214  ;
-right_node = *218 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *219 ->RegressionTreeLeave(
-id = 136 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 150.679919999999981 ;
-weighted_targets_sum = 1.0045328 ;
-weighted_squared_targets_sum = 50.4883978158413385 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *219  ;
-leave_output = 2 [ 50.2266399999999962 1 ] ;
-leave_error = 3 [ 0.0681810040986781524 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 2.04712499999999986 ;
-after_split_error = 0.00392131309066329203 ;
-missing_node = *0 ;
-missing_leave = *220 ->RegressionTreeLeave(
-id = 206 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *221 ->RegressionTreeLeave(
-id = 207 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 48.4341599999999914 ;
-weighted_targets_sum = 0.32289439999999997 ;
-weighted_squared_targets_sum = 15.6391190327040039 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *222 ->RegressionTreeLeave(
-id = 208 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 102.24575999999999 ;
-weighted_targets_sum = 0.681638399999999978 ;
-weighted_squared_targets_sum = 34.8492787831373292 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *219   )
-;
-right_leave = *211   )
-;
-right_leave = *203   )
-;
-left_leave = *187  ;
-right_node = *223 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *224 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 22 ;
-weights_sum = 0.14666666666666664 ;
-targets_sum = 1607.00448999999981 ;
-weighted_targets_sum = 10.7133632666666685 ;
-weighted_squared_targets_sum = 822.845936143814129 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *224  ;
-leave_output = 2 [ 73.0456586363636688 1 ] ;
-leave_error = 3 [ 80.5625202390456536 0 0.293333333333333279 ] ;
-split_col = 1 ;
-split_balance = 6 ;
-split_feature_value = 0.756380000000000052 ;
-after_split_error = 32.2963206990872962 ;
-missing_node = *0 ;
-missing_leave = *225 ->RegressionTreeLeave(
-id = 38 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *226 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *227 ->RegressionTreeLeave(
-id = 39 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 14 ;
-weights_sum = 0.0933333333333333376 ;
-targets_sum = 886.886090000000081 ;
-weighted_targets_sum = 5.91257393333333425 ;
-weighted_squared_targets_sum = 386.540563219147373 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *227  ;
-leave_output = 2 [ 63.3490064285714354 1 ] ;
-leave_error = 3 [ 23.969758214020132 0 0.186666666666666675 ] ;
-split_col = 3 ;
-split_balance = 10 ;
-split_feature_value = -2.19003499999999995 ;
-after_split_error = 9.66110139106137744 ;
-missing_node = *0 ;
-missing_leave = *228 ->RegressionTreeLeave(
-id = 59 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *229 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *230 ->RegressionTreeLeave(
-id = 60 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 83.8064699999999903 ;
-weighted_targets_sum = 0.558709800000000034 ;
-weighted_squared_targets_sum = 23.4124120810993332 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *230  ;
-leave_output = 2 [ 41.9032350000000022 1 ] ;
-leave_error = 3 [ 0.00132806979265908537 0 0.00132806979265908537 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -3.1773049999999996 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *231 ->RegressionTreeLeave(
-id = 101 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *232 ->RegressionTreeLeave(
-id = 102 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 41.6800699999999935 ;
-weighted_targets_sum = 0.277867133333333349 ;
-weighted_squared_targets_sum = 11.5815215680326666 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *233 ->RegressionTreeLeave(
-id = 103 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 42.1263999999999967 ;
-weighted_targets_sum = 0.280842666666666685 ;
-weighted_squared_targets_sum = 11.8308905130666666 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *230  ;
-right_node = *234 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *235 ->RegressionTreeLeave(
-id = 61 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 12 ;
-weights_sum = 0.0800000000000000017 ;
-targets_sum = 803.079619999999977 ;
-weighted_targets_sum = 5.35386413333333344 ;
-weighted_squared_targets_sum = 363.128151138048054 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *235  ;
-leave_output = 2 [ 66.92330166666666 1 ] ;
-leave_error = 3 [ 9.65977332126890786 0 0.160000000000000003 ] ;
-split_col = 3 ;
-split_balance = 4 ;
-split_feature_value = 2.82453999999999983 ;
-after_split_error = 2.91998118893811931 ;
-missing_node = *0 ;
-missing_leave = *236 ->RegressionTreeLeave(
-id = 104 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *237 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *238 ->RegressionTreeLeave(
-id = 105 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = 498.671859999999981 ;
-weighted_targets_sum = 3.32447906666666659 ;
-weighted_squared_targets_sum = 208.09516304127601 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *238  ;
-leave_output = 2 [ 62.3339824999999976 1 ] ;
-leave_error = 3 [ 1.73428615611938763 0 0.106666666666666674 ] ;
-split_col = 3 ;
-split_balance = 6 ;
-split_feature_value = 2.35129499999999991 ;
-after_split_error = 1.20228931743125678 ;
-missing_node = *0 ;
-missing_leave = *239 ->RegressionTreeLeave(
-id = 155 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *240 ->RegressionTreeLeave(
-id = 156 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 62.9106799999999993 ;
-weighted_targets_sum = 0.419404533333332941 ;
-weighted_squared_targets_sum = 26.3850243870826695 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *241 ->RegressionTreeLeave(
-id = 157 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = 435.761180000000024 ;
-weighted_targets_sum = 2.90507453333333343 ;
-weighted_squared_targets_sum = 181.710138654193344 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *238  ;
-right_node = *242 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *243 ->RegressionTreeLeave(
-id = 106 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 304.407759999999996 ;
-weighted_targets_sum = 2.02938506666666685 ;
-weighted_squared_targets_sum = 155.032988096772016 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *243  ;
-leave_output = 2 [ 76.101939999999999 1 ] ;
-leave_error = 3 [ 1.18569503281867772 0 0.0533333333333333368 ] ;
-split_col = 1 ;
-split_balance = 0 ;
-split_feature_value = 0.592494999999999994 ;
-after_split_error = 0.00355264134659952213 ;
-missing_node = *0 ;
-missing_leave = *244 ->RegressionTreeLeave(
-id = 158 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *245 ->RegressionTreeLeave(
-id = 159 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 80.5889199999999875 ;
-weighted_targets_sum = 0.537259466666666796 ;
-weighted_squared_targets_sum = 43.2971601784426596 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *246 ->RegressionTreeLeave(
-id = 160 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 223.818840000000023 ;
-weighted_targets_sum = 1.49212560000000005 ;
-weighted_squared_targets_sum = 111.735827918329349 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *243   )
-;
-right_leave = *235   )
-;
-left_leave = *227  ;
-right_node = *247 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *248 ->RegressionTreeLeave(
-id = 40 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = 720.118400000000065 ;
-weighted_targets_sum = 4.80078933333333335 ;
-weighted_squared_targets_sum = 436.305372924666642 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *248  ;
-leave_output = 2 [ 90.0147999999999939 1 ] ;
-leave_error = 3 [ 8.32656248506665619 0 0.106666666666666674 ] ;
-split_col = 2 ;
-split_balance = 4 ;
-split_feature_value = 5.85068500000000036 ;
-after_split_error = 2.39782196026289407 ;
-missing_node = *0 ;
-missing_leave = *249 ->RegressionTreeLeave(
-id = 62 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *250 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *251 ->RegressionTreeLeave(
-id = 63 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 154.203579999999988 ;
-weighted_targets_sum = 1.02802386666666679 ;
-weighted_squared_targets_sum = 79.2874180128546868 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *251  ;
-leave_output = 2 [ 77.1017900000000083 1 ] ;
-leave_error = 3 [ 0.0498754602666810209 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = -1.04825499999999994 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *252 ->RegressionTreeLeave(
-id = 167 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *253 ->RegressionTreeLeave(
-id = 168 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 75.7341899999999839 ;
-weighted_targets_sum = 0.504894600000000082 ;
-weighted_squared_targets_sum = 38.2377835663740129 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *254 ->RegressionTreeLeave(
-id = 169 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 78.4693900000000042 ;
-weighted_targets_sum = 0.523129266666666704 ;
-weighted_squared_targets_sum = 41.0496344464806739 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *251  ;
-right_node = *255 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *256 ->RegressionTreeLeave(
-id = 64 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 6 ;
-weights_sum = 0.0400000000000000008 ;
-targets_sum = 565.914819999999963 ;
-weighted_targets_sum = 3.77276546666666679 ;
-weighted_squared_targets_sum = 357.017954911812012 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *256  ;
-leave_output = 2 [ 94.3191366666666653 1 ] ;
-leave_error = 3 [ 2.34794649999640193 0 0.0800000000000000017 ] ;
-split_col = 3 ;
-split_balance = 2 ;
-split_feature_value = 0.139270000000000005 ;
-after_split_error = 1.53879167940953376 ;
-missing_node = *0 ;
-missing_leave = *257 ->RegressionTreeLeave(
-id = 170 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *258 ->RegressionTreeLeave(
-id = 171 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 96.6401699999999408 ;
-weighted_targets_sum = 0.644267800000000279 ;
-weighted_squared_targets_sum = 62.2621497175260004 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *259 ->RegressionTreeLeave(
-id = 172 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 469.274649999999951 ;
-weighted_targets_sum = 3.12849766666666662 ;
-weighted_squared_targets_sum = 294.755805194285983 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *256   )
-;
-right_leave = *248   )
-;
-right_leave = *224   )
-;
-left_leave = *184  ;
-right_node = *260 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *261 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 30 ;
-weights_sum = 0.199999999999999872 ;
-targets_sum = 4479.64080000000013 ;
-weighted_targets_sum = 29.8642719999999997 ;
-weighted_squared_targets_sum = 4771.0040944160155 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *261  ;
-leave_output = 2 [ 149.321360000000084 1 ] ;
-leave_error = 3 [ 623.260767932185786 0 0.399999999999999745 ] ;
-split_col = 1 ;
-split_balance = 8 ;
-split_feature_value = 1.44567500000000004 ;
-after_split_error = 238.043712468330853 ;
-missing_node = *0 ;
-missing_leave = *262 ->RegressionTreeLeave(
-id = 14 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *263 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *264 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 19 ;
-weights_sum = 0.126666666666666677 ;
-targets_sum = 2388.4680000000003 ;
-weighted_targets_sum = 15.9231200000000008 ;
-weighted_squared_targets_sum = 2035.13483177585226 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *264  ;
-leave_output = 2 [ 125.708842105263159 1 ] ;
-leave_error = 3 [ 66.9157077453886728 0 0.253333333333333355 ] ;
-split_col = 1 ;
-split_balance = 5 ;
-split_feature_value = 1.2774350000000001 ;
-after_split_error = 24.1608341688744517 ;
-missing_node = *0 ;
-missing_leave = *265 ->RegressionTreeLeave(
-id = 23 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *266 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *267 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 12 ;
-weights_sum = 0.0800000000000000017 ;
-targets_sum = 1389.4405099999999 ;
-weighted_targets_sum = 9.26293673333333345 ;
-weighted_squared_targets_sum = 1080.890425285226 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *267  ;
-leave_output = 2 [ 115.786709166666668 1 ] ;
-leave_error = 3 [ 16.7309274270517392 0 0.160000000000000003 ] ;
-split_col = 3 ;
-split_balance = 8 ;
-split_feature_value = 3.03715500000000027 ;
-after_split_error = 6.78331317461888439 ;
-missing_node = *0 ;
-missing_leave = *268 ->RegressionTreeLeave(
-id = 65 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *269 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *270 ->RegressionTreeLeave(
-id = 66 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 10 ;
-weights_sum = 0.0666666666666666657 ;
-targets_sum = 1122.60447999999997 ;
-weighted_targets_sum = 7.48402986666666692 ;
-weighted_squared_targets_sum = 843.265233105289326 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *270  ;
-leave_output = 2 [ 112.260448000000011 1 ] ;
-leave_error = 3 [ 6.20937485581785609 0 0.133333333333333331 ] ;
-split_col = 3 ;
-split_balance = 8 ;
-split_feature_value = -0.972770000000000024 ;
-after_split_error = 1.72287732250645886 ;
-missing_node = *0 ;
-missing_leave = *271 ->RegressionTreeLeave(
-id = 119 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *272 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *273 ->RegressionTreeLeave(
-id = 120 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 94.8581899999999933 ;
-weighted_targets_sum = 0.632387933333333319 ;
-weighted_squared_targets_sum = 59.9871747338406607 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *273  ;
-leave_output = 2 [ 94.8581899999999933 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *274 ->RegressionTreeLeave(
-id = 173 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *275 ->RegressionTreeLeave(
-id = 174 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *276 ->RegressionTreeLeave(
-id = 175 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *273  ;
-right_node = *277 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *278 ->RegressionTreeLeave(
-id = 121 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 9 ;
-weights_sum = 0.0600000000000000047 ;
-targets_sum = 1027.74629000000004 ;
-weighted_targets_sum = 6.85164193333333316 ;
-weighted_squared_targets_sum = 783.278058371448651 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *278  ;
-leave_output = 2 [ 114.194032222222205 1 ] ;
-leave_error = 3 [ 1.72287732250634451 0 0.120000000000000009 ] ;
-split_col = 1 ;
-split_balance = 7 ;
-split_feature_value = 1.03719500000000009 ;
-after_split_error = 1.07265289472230152 ;
-missing_node = *0 ;
-missing_leave = *279 ->RegressionTreeLeave(
-id = 176 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *280 ->RegressionTreeLeave(
-id = 177 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 111.287989999999937 ;
-weighted_targets_sum = 0.741919933333333392 ;
-weighted_squared_targets_sum = 82.566778121600592 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *281 ->RegressionTreeLeave(
-id = 178 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = 916.458299999999895 ;
-weighted_targets_sum = 6.10972200000000054 ;
-weighted_squared_targets_sum = 700.711280249848187 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *278   )
-;
-left_leave = *270  ;
-right_node = *282 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *283 ->RegressionTreeLeave(
-id = 67 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 266.836029999999994 ;
-weighted_targets_sum = 1.77890686666666675 ;
-weighted_squared_targets_sum = 237.625192179936676 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *283  ;
-leave_output = 2 [ 133.418014999999997 1 ] ;
-leave_error = 3 [ 0.573938318800659264 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 4.0628700000000002 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *284 ->RegressionTreeLeave(
-id = 122 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *285 ->RegressionTreeLeave(
-id = 123 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 138.057269999999988 ;
-weighted_targets_sum = 0.920381800000000028 ;
-weighted_squared_targets_sum = 127.06539866568599 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *286 ->RegressionTreeLeave(
-id = 124 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 128.778760000000005 ;
-weighted_targets_sum = 0.858525066666666725 ;
-weighted_squared_targets_sum = 110.559793514250686 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *283   )
-;
-left_leave = *267  ;
-right_node = *287 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *288 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = 999.027489999999943 ;
-weighted_targets_sum = 6.66018326666666649 ;
-weighted_squared_targets_sum = 954.244406490626147 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *288  ;
-leave_output = 2 [ 142.718212857142845 1 ] ;
-leave_error = 3 [ 7.42990674182374722 0 0.0933333333333333376 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.852400000000000047 ;
-after_split_error = 0.383762277940444818 ;
-missing_node = *0 ;
-missing_leave = *289 ->RegressionTreeLeave(
-id = 68 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *290 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *291 ->RegressionTreeLeave(
-id = 69 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 398.055919999999958 ;
-weighted_targets_sum = 2.65370613333333338 ;
-weighted_squared_targets_sum = 352.214078048060003 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *291  ;
-leave_output = 2 [ 132.685306666666662 1 ] ;
-leave_error = 3 [ 0.212531887024959398 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 0.0478799999999999781 ;
-after_split_error = 0.0504540736027003778 ;
-missing_node = *0 ;
-missing_leave = *292 ->RegressionTreeLeave(
-id = 137 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *293 ->RegressionTreeLeave(
-id = 138 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 129.886429999999962 ;
-weighted_targets_sum = 0.865909533333333203 ;
-weighted_squared_targets_sum = 112.469897987632677 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *294 ->RegressionTreeLeave(
-id = 139 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 268.169489999999996 ;
-weighted_targets_sum = 1.78779660000000007 ;
-weighted_squared_targets_sum = 239.74418006042734 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *291  ;
-right_node = *295 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *296 ->RegressionTreeLeave(
-id = 70 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 600.971569999999929 ;
-weighted_targets_sum = 4.006477133333334 ;
-weighted_squared_targets_sum = 602.030328442565974 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *296  ;
-leave_output = 2 [ 150.242892500000011 1 ] ;
-leave_error = 3 [ 0.171230390915363406 0 0.0533333333333333368 ] ;
-split_col = 3 ;
-split_balance = 2 ;
-split_feature_value = 1.30223500000000003 ;
-after_split_error = 0.0180043344344157363 ;
-missing_node = *0 ;
-missing_leave = *297 ->RegressionTreeLeave(
-id = 140 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *298 ->RegressionTreeLeave(
-id = 141 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 153.178699999999935 ;
-weighted_targets_sum = 1.02119133333333378 ;
-weighted_squared_targets_sum = 156.424760891266658 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *299 ->RegressionTreeLeave(
-id = 142 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 447.792869999999994 ;
-weighted_targets_sum = 2.98528580000000021 ;
-weighted_squared_targets_sum = 445.605567551299316 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *296   )
-;
-right_leave = *288   )
-;
-left_leave = *264  ;
-right_node = *300 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *301 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 11 ;
-weights_sum = 0.0733333333333333337 ;
-targets_sum = 2091.17280000000028 ;
-weighted_targets_sum = 13.9411520000000007 ;
-weighted_squared_targets_sum = 2735.86926264016302 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *301  ;
-leave_output = 2 [ 190.106618181818192 1 ] ;
-leave_error = 3 [ 171.12800472294353 0 0.146666666666666667 ] ;
-split_col = 2 ;
-split_balance = 9 ;
-split_feature_value = 24.753779999999999 ;
-after_split_error = 32.3542885699819749 ;
-missing_node = *0 ;
-missing_leave = *302 ->RegressionTreeLeave(
-id = 26 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *303 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *304 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 10 ;
-weights_sum = 0.0666666666666666657 ;
-targets_sum = 1803.79416999999989 ;
-weighted_targets_sum = 12.0252944666666668 ;
-weighted_squared_targets_sum = 2185.29274943564997 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *304  ;
-leave_output = 2 [ 180.379417000000018 1 ] ;
-leave_error = 3 [ 32.3542885699819749 0 0.133333333333333331 ] ;
-split_col = 1 ;
-split_balance = 4 ;
-split_feature_value = 1.77604000000000006 ;
-after_split_error = 6.9334155021134114 ;
-missing_node = *0 ;
-missing_leave = *305 ->RegressionTreeLeave(
-id = 47 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *306 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *307 ->RegressionTreeLeave(
-id = 48 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 7 ;
-weights_sum = 0.0466666666666666688 ;
-targets_sum = 1199.38043000000016 ;
-weighted_targets_sum = 7.99586953333333383 ;
-weighted_squared_targets_sum = 1372.27835111403033 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *307  ;
-leave_output = 2 [ 171.340061428571431 1 ] ;
-leave_error = 3 [ 4.53114819570822469 0 0.0933333333333333376 ] ;
-split_col = 1 ;
-split_balance = 3 ;
-split_feature_value = 1.58981499999999998 ;
-after_split_error = 2.24276698607739045 ;
-missing_node = *0 ;
-missing_leave = *308 ->RegressionTreeLeave(
-id = 89 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *309 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *310 ->RegressionTreeLeave(
-id = 90 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 327.021780000000035 ;
-weighted_targets_sum = 2.18014520000000012 ;
-weighted_squared_targets_sum = 357.150820013889415 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *310  ;
-leave_output = 2 [ 163.510889999999989 1 ] ;
-leave_error = 3 [ 1.34667606532281203 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 2.63016499999999986 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *311 ->RegressionTreeLeave(
-id = 209 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *312 ->RegressionTreeLeave(
-id = 210 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 156.404530000000022 ;
-weighted_targets_sum = 1.04269686666666672 ;
-weighted_squared_targets_sum = 163.082513363472628 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *313 ->RegressionTreeLeave(
-id = 211 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 170.617250000000013 ;
-weighted_targets_sum = 1.13744833333333339 ;
-weighted_squared_targets_sum = 194.06830665041673 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *310  ;
-right_node = *314 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *315 ->RegressionTreeLeave(
-id = 91 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 872.358650000000011 ;
-weighted_targets_sum = 5.81572433333333372 ;
-weighted_squared_targets_sum = 1015.12753110014091 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *315  ;
-leave_output = 2 [ 174.471730000000008 1 ] ;
-leave_error = 3 [ 0.896090920754980758 0 0.0666666666666666657 ] ;
-split_col = 2 ;
-split_balance = 3 ;
-split_feature_value = 20.3570750000000018 ;
-after_split_error = 0.551595038088242617 ;
-missing_node = *0 ;
-missing_leave = *316 ->RegressionTreeLeave(
-id = 212 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *317 ->RegressionTreeLeave(
-id = 213 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 170.817769999999967 ;
-weighted_targets_sum = 1.13878513333333364 ;
-weighted_squared_targets_sum = 194.524736985152657 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *318 ->RegressionTreeLeave(
-id = 214 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 701.540880000000016 ;
-weighted_targets_sum = 4.67693920000000052 ;
-weighted_squared_targets_sum = 820.602794114988228 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *315   )
-;
-left_leave = *307  ;
-right_node = *319 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *320 ->RegressionTreeLeave(
-id = 49 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 604.413739999999962 ;
-weighted_targets_sum = 4.02942493333333385 ;
-weighted_squared_targets_sum = 813.014398321619979 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *320  ;
-leave_output = 2 [ 201.471246666666701 1 ] ;
-leave_error = 3 [ 2.40226730640564057 0 0.0400000000000000008 ] ;
-split_col = 3 ;
-split_balance = 1 ;
-split_feature_value = 1.45652000000000004 ;
-after_split_error = 0.166055228482390183 ;
-missing_node = *0 ;
-missing_leave = *321 ->RegressionTreeLeave(
-id = 92 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *322 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *323 ->RegressionTreeLeave(
-id = 93 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 212.045299999999997 ;
-weighted_targets_sum = 1.41363533333333335 ;
-weighted_squared_targets_sum = 299.754728347266678 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *323  ;
-leave_output = 2 [ 212.045299999999997 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *324 ->RegressionTreeLeave(
-id = 221 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *325 ->RegressionTreeLeave(
-id = 222 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *326 ->RegressionTreeLeave(
-id = 223 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *323  ;
-right_node = *327 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *328 ->RegressionTreeLeave(
-id = 94 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 392.368439999999964 ;
-weighted_targets_sum = 2.61578960000000027 ;
-weighted_squared_targets_sum = 513.259669974353301 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *328  ;
-leave_output = 2 [ 196.18422000000001 1 ] ;
-leave_error = 3 [ 0.166055228482390183 0 0.0266666666666666684 ] ;
-split_col = 3 ;
-split_balance = 0 ;
-split_feature_value = 2.6831649999999998 ;
-after_split_error = 0 ;
-missing_node = *0 ;
-missing_leave = *329 ->RegressionTreeLeave(
-id = 224 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *330 ->RegressionTreeLeave(
-id = 225 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 193.688809999999961 ;
-weighted_targets_sum = 1.29125873333333341 ;
-weighted_squared_targets_sum = 250.102367461440622 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *331 ->RegressionTreeLeave(
-id = 226 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 198.679630000000003 ;
-weighted_targets_sum = 1.32453086666666686 ;
-weighted_squared_targets_sum = 263.157302512912679 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *328   )
-;
-right_leave = *320   )
-;
-left_leave = *304  ;
-right_node = *332 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *333 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 1 ;
-weights_sum = 0.00666666666666666709 ;
-targets_sum = 287.378629999999987 ;
-weighted_targets_sum = 1.91585753333333342 ;
-weighted_squared_targets_sum = 550.576513204512594 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-train_set = *8  ;
-leave = *333  ;
-leave_output = 2 [ 287.378629999999987 1 ] ;
-leave_error = 3 [ 0 0 0 ] ;
-split_col = -1 ;
-split_balance = 2147483647 ;
-split_feature_value = 1.79769313486231571e+308 ;
-after_split_error = 1.79769313486231571e+308 ;
-missing_node = *0 ;
-missing_leave = *334 ->RegressionTreeLeave(
-id = 50 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *335 ->RegressionTreeLeave(
-id = 51 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *336 ->RegressionTreeLeave(
-id = 52 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *8  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *333   )
-;
-right_leave = *301   )
-;
-right_leave = *261   )
-;
-right_leave = *181   )
-;
-priority_queue = *337 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 50 ;
-next_available_node = 35 ;
-nodes = 50 [ *85  *90  *157  *242  *205  *197  *309  *144  *170  *130  *104  *277  *192  *48  *213  *314  *69  *282  *109  *149  *290  *218  *175  *237  *117  *250  *255  *125  *53  *295  *229  *327  *80  *27  *64  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
-;
-first_leave = *13  ;
-split_cols = 40 [ 1 1 1 1 1 1 1 2 1 1 1 1 2 1 1 1 3 2 3 3 3 3 3 1 3 3 1 2 3 3 3 3 1 3 1 3 3 1 3 3 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 41 ;
-n_examples = 150 ;
-inputsize = 4 ;
-targetsize = 1 ;
-weightsize = 0 ;
-forget_when_training_set_changes = 1 ;
-nstages = 41 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-;
-perf_evaluators = {};
-report_stats = 1 ;
-save_initial_tester = 0 ;
-save_stat_collectors = 1 ;
-save_learners = 0 ;
-save_initial_learners = 0 ;
-save_data_sets = 0 ;
-save_test_outputs = 0 ;
-call_forget_in_run = 1 ;
-save_test_costs = 0 ;
-save_test_names = 0 ;
-provide_learner_expdir = 1 ;
-should_train = 1 ;
-should_test = 1 ;
-template_stats_collector = *0 ;
-global_template_stats_collector = *0 ;
-final_commands = []
-;
-save_test_confidence = 0 ;
-enforce_clean_expdir = 1  )
-;
-option_fields = 1 [ "nstages" ] ;
-dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *338 ->HyperOptimize(
-which_cost = "E[test2.E[mse]]" ;
-min_n_trials = 0 ;
-oracle = *339 ->EarlyStoppingOracle(
-option = "nstages" ;
-values = []
-;
-range = 3 [ 1 61 20 ] ;
-min_value = -3.40282000000000014e+38 ;
-max_value = 3.40282000000000014e+38 ;
-max_degradation = 3.40282000000000014e+38 ;
-relative_max_degradation = -1 ;
-min_improvement = -3.40282000000000014e+38 ;
-relative_min_improvement = -1 ;
-max_degraded_steps = 120 ;
-min_n_steps = 2  )
-;
-provide_tester_expdir = 1 ;
-sub_strategy = []
-;
-rerun_after_sub = 0 ;
-provide_sub_expdir = 1 ;
-save_best_learner = 0 ;
-splitter = *0  )
-] ;
-provide_strategy_expdir = 1 ;
-save_final_learner = 0 ;
-learner = *6  ;
-provide_learner_expdir = 1 ;
-expdir_append = "" ;
-forward_nstages = 0 ;
-random_gen = *0 ;
-stage = 1 ;
-n_examples = 200 ;
-inputsize = 4 ;
-targetsize = 1 ;
-weightsize = 0 ;
-forget_when_training_set_changes = 0 ;
-nstages = 1 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )

Deleted: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)

Deleted: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/Split0/test2_confidence.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/experiment.plearn	2008-02-12 15:08:42 UTC (rev 8494)
@@ -80,7 +80,8 @@
         verbosity = 2
         ),
     provide_learner_expdir = 1,
-    save_test_confidence = 1,
+    save_learners = 0,
+    save_test_confidence = 0,
     save_test_costs = 1,
     save_test_outputs = 1,
     splitter = *9 -> FractionSplitter(

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/metainfos.txt	2008-02-12 15:08:42 UTC (rev 8494)
@@ -1,2 +1,2 @@
-__REVISION__ = "PL8454"
+__REVISION__ = "PL8480"
 data                                          = PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree/expected_results/expdir/tester.psave	2008-02-12 15:08:42 UTC (rev 8494)
@@ -156,7 +156,7 @@
 report_stats = 1 ;
 save_initial_tester = 1 ;
 save_stat_collectors = 1 ;
-save_learners = 1 ;
+save_learners = 0 ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
 save_test_outputs = 1 ;
@@ -170,5 +170,5 @@
 global_template_stats_collector = *0 ;
 final_commands = []
 ;
-save_test_confidence = 1 ;
+save_test_confidence = 0 ;
 enforce_clean_expdir = 1  )

Deleted: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/final_learner.psave	2008-02-12 15:08:42 UTC (rev 8494)
@@ -1,507 +0,0 @@
-*1 ->HyperLearner(
-tester = *2 ->PTester(
-expdir = "PYTEST__PL_RegressionTree_MultiClass__RESULTS:expdir/Split0/LearnerExpdir/Strat0/Trials3/" ;
-dataset = *3 ->SubVMatrix(
-parent = *4 ->ConcatRowsVMatrix(
-sources = 2 [ *5 ->AutoVMatrix(
-filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat" ;
-load_in_memory = 0 ;
-writable = 0 ;
-length = 200 ;
-width = 3 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/"  )
-*6 ->AutoVMatrix(
-filename = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat" ;
-load_in_memory = 0 ;
-writable = 0 ;
-length = 6831 ;
-width = 3 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat.metadata/"  )
-] ;
-fill_missing = 0 ;
-fully_check_mappings = 0 ;
-only_common_fields = 0 ;
-writable = 0 ;
-length = 7031 ;
-width = 3 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-istart = 0 ;
-jstart = 0 ;
-fistart = -1 ;
-flength = -1 ;
-source = *4  ;
-writable = 0 ;
-length = 7031 ;
-width = 3 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-splitter = *7 ->FractionSplitter(
-round_to_closest = 0 ;
-splits = 1  3  [ 
-(0 , 200 )	(0 , 200 )	(200 , 1 )	
-]
- )
-;
-statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[base_confidence]]" "E[test1.E[base_reward_l2]]" "E[test1.E[base_reward_l1]]" "E[test2.E[class_error]]" "E[test2.E[base_confidence]]" "E[test2.E[base_reward_l2]]" "E[test2.E[base_reward_l1]]" ] ;
-statmask = []
-;
-learner = *8 ->RegressionTree(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-maximum_number_of_nodes = 50 ;
-compute_train_stats = 1 ;
-complexity_penalty_factor = 0 ;
-multiclass_outputs = []
-;
-leave_template = *9 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0 0 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *0 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-sorted_train_set = *10 ->RegressionTreeRegisters(
-report_progress = 1 ;
-verbosity = 2 ;
-tsource = *11 ->MemoryVMatrixNoSave(
-source = *12 ->TransposeVMatrix(
-source = *13 ->SubVMatrix(
-parent = *4  ;
-istart = 0 ;
-jstart = 0 ;
-fistart = -1 ;
-flength = -1 ;
-source = *4  ;
-writable = 0 ;
-length = 200 ;
-width = 3 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-writable = 0 ;
-length = 3 ;
-width = 200 ;
-inputsize = 200 ;
-targetsize = 0 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-fieldnames = []
-;
-deep_copy_memory_data = 1 ;
-writable = 0 ;
-length = 3 ;
-width = 200 ;
-inputsize = 200 ;
-targetsize = 0 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-next_id = 10 ;
-leave_register = 200 [ 4 3 4 4 4 3 3 4 3 4 3 3 4 4 4 3 4 4 3 4 3 3 4 3 4 3 4 3 3 4 4 3 3 4 4 4 4 4 3 3 3 4 3 4 4 3 4 3 3 3 4 4 3 4 3 4 4 4 4 4 4 4 4 4 3 4 3 3 4 3 3 3 3 3 3 3 4 3 3 4 3 4 3 3 4 3 4 3 4 3 4 3 4 3 4 3 4 4 4 3 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 ] ;
-writable = 0 ;
-length = 200 ;
-width = 3 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-extrasize = 0 ;
-metadatadir = ""  )
-;
-root = *14 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *15 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0.500000000000000333 0.500000000000000333 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 200 ;
-weights_sum = 1.00000000000000067 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-train_set = *10  ;
-leave = *15  ;
-leave_output = 2 [ 0 0.5 ] ;
-leave_error = 3 [ 200 100 2.00000000000000133 ] ;
-split_col = 1 ;
-split_balance = 96 ;
-split_feature_value = 0.14412705026016423 ;
-after_split_error = 63.9999999999999787 ;
-missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0 0 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 2 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-left_node = *17 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *18 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0.24000000000000013 0.0200000000000000004 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 52 ;
-weights_sum = 0.26000000000000012 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-train_set = *10  ;
-leave = *18  ;
-leave_output = 2 [ 0 0.923076923076923128 ] ;
-leave_error = 3 [ 7.99999999999999645 3.99999999999999734 0.52000000000000024 ] ;
-split_col = 0 ;
-split_balance = 50 ;
-split_feature_value = 2.81501972474946704 ;
-after_split_error = 8.99999999999999289 ;
-missing_node = *0 ;
-missing_leave = *19 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0 0 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 5 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *20 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0.00499999999999998449 -1.73472347597680709e-18 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 1 ;
-weights_sum = 0.00499999999999995674 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *21 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0.235000000000000125 0.0200000000000000004 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 51 ;
-weights_sum = 0.255000000000000115 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
- )
-;
-left_leave = *18  ;
-right_node = *22 ->RegressionTreeNode(
-missing_is_valid = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-leave_template = *23 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0.26000000000000012 0.480000000000000315 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 148 ;
-weights_sum = 0.740000000000000546 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-train_set = *10  ;
-leave = *23  ;
-leave_output = 2 [ 1 0.648648648648648574 ] ;
-leave_error = 3 [ 0 52.0000000000000142 0 ] ;
-split_col = 0 ;
-split_balance = 146 ;
-split_feature_value = 3.90430461537466744 ;
-after_split_error = 51.0000000000000071 ;
-missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0 0 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 8 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-left_node = *0 ;
-left_leave = *25 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ -4.33680868994201774e-17 0.00499999999999995674 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 1 ;
-weights_sum = 0.00499999999999995674 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
-;
-right_node = *0 ;
-right_leave = *26 ->RegressionTreeMulticlassLeave(
-multiclass_outputs = 2 [ 0 1 ] ;
-objective_function = "l1" ;
-multiclass_weights_sum = 2 [ 0.26000000000000012 0.475000000000000311 ] ;
-l1_loss_function_factor = 2 ;
-l2_loss_function_factor = 2 ;
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-train_set = *10  ;
-length = 147 ;
-weights_sum = 0.735000000000000542 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 1 ;
-output = []
-;
-error = []
- )
- )
-;
-right_leave = *23   )
-;
-priority_queue = *27 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 50 ;
-next_available_node = 2 ;
-nodes = 50 [ *17  *22  *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 *0 ]  )
-;
-first_leave = *15  ;
-split_cols = 1 [ 1 ] ;
-random_gen = *0 ;
-seed = 1827 ;
-stage = 2 ;
-n_examples = 200 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-forget_when_training_set_changes = 1 ;
-nstages = 2 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )
-;
-perf_evaluators = {};
-report_stats = 1 ;
-save_initial_tester = 0 ;
-save_stat_collectors = 1 ;
-save_learners = 0 ;
-save_initial_learners = 0 ;
-save_data_sets = 0 ;
-save_test_outputs = 0 ;
-call_forget_in_run = 1 ;
-save_test_costs = 0 ;
-save_test_names = 0 ;
-provide_learner_expdir = 1 ;
-should_train = 1 ;
-should_test = 1 ;
-template_stats_collector = *0 ;
-global_template_stats_collector = *0 ;
-final_commands = []
-;
-save_test_confidence = 0 ;
-enforce_clean_expdir = 1  )
-;
-option_fields = 1 [ "nstages" ] ;
-dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *28 ->HyperOptimize(
-which_cost = "E[test2.E[class_error]]" ;
-min_n_trials = 0 ;
-oracle = *29 ->EarlyStoppingOracle(
-option = "nstages" ;
-values = []
-;
-range = 3 [ 1 5 1 ] ;
-min_value = -3.40282000000000014e+38 ;
-max_value = 3.40282000000000014e+38 ;
-max_degradation = 3.40282000000000014e+38 ;
-relative_max_degradation = -1 ;
-min_improvement = -3.40282000000000014e+38 ;
-relative_min_improvement = -1 ;
-max_degraded_steps = 120 ;
-min_n_steps = 2  )
-;
-provide_tester_expdir = 1 ;
-sub_strategy = []
-;
-rerun_after_sub = 0 ;
-provide_sub_expdir = 1 ;
-save_best_learner = 0 ;
-splitter = *0  )
-] ;
-provide_strategy_expdir = 1 ;
-save_final_learner = 0 ;
-learner = *8  ;
-provide_learner_expdir = 1 ;
-expdir_append = "" ;
-forward_nstages = 0 ;
-random_gen = *0 ;
-stage = 1 ;
-n_examples = 7031 ;
-inputsize = 2 ;
-targetsize = 1 ;
-weightsize = 0 ;
-forget_when_training_set_changes = 0 ;
-nstages = 1 ;
-report_progress = 1 ;
-verbosity = 2 ;
-nservers = 0 ;
-save_trainingset_prefix = "" ;
-test_minibatch_size = 1 ;
-use_a_separate_random_generator_for_testing = 1827  )

Deleted: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_confidence.pmat
===================================================================
(Binary files differ)

Deleted: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_confidence.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/experiment.plearn	2008-02-12 15:08:42 UTC (rev 8494)
@@ -96,7 +96,8 @@
         verbosity = 2
         ),
     provide_learner_expdir = 1,
-    save_test_confidence = 1,
+    save_learners = 0,
+    save_test_confidence = 0,
     save_test_costs = 1,
     save_test_outputs = 1,
     splitter = *11 -> FractionSplitter(

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-02-12 15:08:42 UTC (rev 8494)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL8454"
+__REVISION__ = "PL8480"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/tester.psave	2008-02-12 15:08:42 UTC (rev 8494)
@@ -186,7 +186,7 @@
 report_stats = 1 ;
 save_initial_tester = 1 ;
 save_stat_collectors = 1 ;
-save_learners = 1 ;
+save_learners = 0 ;
 save_initial_learners = 0 ;
 save_data_sets = 0 ;
 save_test_outputs = 1 ;
@@ -200,5 +200,5 @@
 global_template_stats_collector = *0 ;
 final_commands = []
 ;
-save_test_confidence = 1 ;
+save_test_confidence = 0 ;
 enforce_clean_expdir = 1  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree.pyplearn	2008-02-12 15:08:42 UTC (rev 8494)
@@ -84,7 +84,8 @@
     provide_learner_expdir = 1,
     save_test_costs = 1,
     save_test_outputs = 1,
-    save_test_confidence = 1
+    save_test_confidence = 0,
+    save_learners = 0 
     )
 
 def main():

Modified: trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-02-11 16:51:29 UTC (rev 8493)
+++ trunk/plearn_learners/regressors/test/RegressionTree/regression_tree_multiclass.pyplearn	2008-02-12 15:08:42 UTC (rev 8494)
@@ -86,7 +86,9 @@
     provide_learner_expdir = 1,
     save_test_costs = 1,
     save_test_outputs = 1,
-    save_test_confidence = 1
+    save_test_confidence = 0,
+    save_learners = 0
+
     )
 
 def main():



From nouiz at mail.berlios.de  Tue Feb 12 20:15:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 12 Feb 2008 20:15:00 +0100
Subject: [Plearn-commits] r8495 - trunk/plearn/math
Message-ID: <200802121915.m1CJF0cc028716@sheep.berlios.de>

Author: nouiz
Date: 2008-02-12 20:15:00 +0100 (Tue, 12 Feb 2008)
New Revision: 8495

Modified:
   trunk/plearn/math/stats_utils.cc
   trunk/plearn/math/stats_utils.h
Log:
Added a function to do the Kologorov Sminov test on two matrix


Modified: trunk/plearn/math/stats_utils.cc
===================================================================
--- trunk/plearn/math/stats_utils.cc	2008-02-12 15:08:42 UTC (rev 8494)
+++ trunk/plearn/math/stats_utils.cc	2008-02-12 19:15:00 UTC (rev 8495)
@@ -278,6 +278,24 @@
     p_value = KS_test(D,N,conv);
 }
 
+void KS_test(VMat& m1, VMat& m2, int conv, Vec& Ds, Vec& p_values)
+{
+    m1->compatibleSizeError(m2);
+    Ds.resize(m1->width());
+    p_values.resize(m1->width());
+    for(int col = 0;col<m1->width();col++)
+    {
+        Vec row1(m1->length());
+        Vec row2(m2->length());
+        m1->getColumn(col,row1);
+        m2->getColumn(col,row2);
+        real D;
+        real p_value;
+        KS_test(row1,row2,conv,D,p_value);
+        Ds[col]=D;
+        p_values[col]=p_value;
+    }
+}
 real KS_test(Vec& v1, Vec& v2, int conv)
 {
     real D, ks_stat;

Modified: trunk/plearn/math/stats_utils.h
===================================================================
--- trunk/plearn/math/stats_utils.h	2008-02-12 15:08:42 UTC (rev 8494)
+++ trunk/plearn/math/stats_utils.h	2008-02-12 19:15:00 UTC (rev 8495)
@@ -142,6 +142,13 @@
  */
 real KS_test(Vec& v1, Vec& v2, int conv=10);
 
+
+/** 
+ * Returns result of Kolmogorov-Smirnov test for each pair of variable
+ * between the two VMat
+ */
+void KS_test(VMat& m1, VMat& m2, int conv, Vec& Ds, Vec& p_values);
+
 /**
  * Given two paired sets u and v of n measured values, the paired t-test 
  * determines whether they differ from each other in a significant way under 



From tihocan at mail.berlios.de  Wed Feb 13 15:19:23 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 13 Feb 2008 15:19:23 +0100
Subject: [Plearn-commits] r8496 - trunk/plearn_learners/distributions
Message-ID: <200802131419.m1DEJNPA004625@sheep.berlios.de>

Author: tihocan
Date: 2008-02-13 15:19:23 +0100 (Wed, 13 Feb 2008)
New Revision: 8496

Modified:
   trunk/plearn_learners/distributions/UniformDistribution.cc
Log:
Implemented log_density

Modified: trunk/plearn_learners/distributions/UniformDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/UniformDistribution.cc	2008-02-12 19:15:00 UTC (rev 8495)
+++ trunk/plearn_learners/distributions/UniformDistribution.cc	2008-02-13 14:19:23 UTC (rev 8496)
@@ -193,7 +193,13 @@
 /////////////////
 real UniformDistribution::log_density(const Vec& x) const
 {
-    PLERROR("density not implemented for UniformDistribution"); return 0;
+    real sum = 0;
+    for (int i = 0; i < n_dim; i++) {
+        if (x[i] > max[i] || x[i] < min[i])
+            return -INFINITY;
+        sum += pl_log(max[i] - min[i]);
+    }
+    return -sum;
 }
 
 /////////////////////////////////



From larocheh at mail.berlios.de  Wed Feb 13 16:45:09 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 16:45:09 +0100
Subject: [Plearn-commits] r8497 - trunk/commands
Message-ID: <200802131545.m1DFj99r013381@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 16:45:09 +0100 (Wed, 13 Feb 2008)
New Revision: 8497

Modified:
   trunk/commands/nlplearn_inc.h
Log:
Adde VMatViewCommand...


Modified: trunk/commands/nlplearn_inc.h
===================================================================
--- trunk/commands/nlplearn_inc.h	2008-02-13 14:19:23 UTC (rev 8496)
+++ trunk/commands/nlplearn_inc.h	2008-02-13 15:45:09 UTC (rev 8497)
@@ -85,6 +85,7 @@
 #include <commands/PLearnCommands/TestDependenciesCommand.h>
 #include <commands/PLearnCommands/TestDependencyCommand.h>
 #include <commands/PLearnCommands/VMatCommand.h>
+#include <commands/PLearnCommands/VMatViewCommand.h>
 
 /**************
  * Dictionary *



From larocheh at mail.berlios.de  Wed Feb 13 16:46:05 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 16:46:05 +0100
Subject: [Plearn-commits] r8498 - trunk/commands/PLearnCommands
Message-ID: <200802131546.m1DFk57P013437@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 16:46:04 +0100 (Wed, 13 Feb 2008)
New Revision: 8498

Modified:
   trunk/commands/PLearnCommands/OutputFeaturesCommand.cc
Log:
Corrected a comment.


Modified: trunk/commands/PLearnCommands/OutputFeaturesCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/OutputFeaturesCommand.cc	2008-02-13 15:45:09 UTC (rev 8497)
+++ trunk/commands/PLearnCommands/OutputFeaturesCommand.cc	2008-02-13 15:46:04 UTC (rev 8498)
@@ -62,7 +62,7 @@
         "       format and if output_string == 0, the feature IDs will be given.\n"
         "       Optionaly, feat_set can be saved in a new file (used_feat_set)\n"
         "       after it has been used. This can be useful when the feature set\n"
-        "       maintains a cache of the previously requested features of tokens\n."
+        "       maintains a cache of the previously requested features of tokens.\n"
         "       Only the input fields' features are output, with the features of\n"
         "       a single token separated with by ' ' and the features from \n"
         "       different fields separated by '\\t'."



From larocheh at mail.berlios.de  Wed Feb 13 16:49:04 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 16:49:04 +0100
Subject: [Plearn-commits] r8499 - trunk/plearn_learners_experimental
Message-ID: <200802131549.m1DFn4Lc013708@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 16:49:04 +0100 (Wed, 13 Feb 2008)
New Revision: 8499

Modified:
   trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.cc
   trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.h
Log:
Latest version. This object is slowly becomming deprecated (should use StackedAutoassociatorsNet).


Modified: trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.cc	2008-02-13 15:46:04 UTC (rev 8498)
+++ trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.cc	2008-02-13 15:49:04 UTC (rev 8499)
@@ -262,9 +262,7 @@
                   "But for classification problems where target is just\n"
                   "the class number, noutputs is usually of dimensionality \n"
                   "number of classes (as we want to output a score or\n"
-                  "probability vector, one per class)\n"
-                  "If the network only extracts features in an unsupervised\n"
-                  "manner, then let noutputs be 0.");    
+                  "probability vector, one per class)\n");    
 
     declareOption(ol, "use_same_input_and_output_weights", 
                   &DeepFeatureExtractorNNet::use_same_input_and_output_weights, 
@@ -463,6 +461,9 @@
                     output = hiddenLayer(
                         output,w_weights,w_biases,false,"sigmoid",
                         before_transfer_function,use_activations_with_cubed_input);
+                    //output = hiddenLayer(
+                    //    output,w,"sigmoid",
+                    //    before_transfer_function,use_activations_with_cubed_input);
                 }
                 else // ... or have different set of weights.
                 {
@@ -578,6 +579,13 @@
                             true, rec_trans_func,
                             before_transfer_function,
                             use_activations_with_cubed_input);
+                        //output =  hiddenLayer(
+                        //    output, 
+                        //    vconcat(biases[biases.size()-it-1]
+                        //            & transpose(weights[weights.size()-it])),
+                        //    rec_trans_func,
+                        //    before_transfer_function,
+                        //    use_activations_with_cubed_input);
                     }
                     else
                     {

Modified: trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.h
===================================================================
--- trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.h	2008-02-13 15:46:04 UTC (rev 8498)
+++ trunk/plearn_learners_experimental/DeepFeatureExtractorNNet.h	2008-02-13 15:49:04 UTC (rev 8499)
@@ -53,6 +53,10 @@
 
 /**
  * Deep Neural Network that extracts features in a greedy, mostly unsupervised way.
+ * 
+ * TODO: - change comments about nhidden_schedule_position (can train only top weights)
+ *       - potentially change relative_minimum_improvement so that it doesn't train
+ *         top weights only...
  */
 class DeepFeatureExtractorNNet : public PLearner
 {



From larocheh at mail.berlios.de  Wed Feb 13 16:50:12 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 16:50:12 +0100
Subject: [Plearn-commits] r8500 - trunk/plearn_learners_experimental
Message-ID: <200802131550.m1DFoCa2013885@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 16:50:11 +0100 (Wed, 13 Feb 2008)
New Revision: 8500

Modified:
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
   trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
Log:
Still coding and debugging...


Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-02-13 15:49:04 UTC (rev 8499)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.cc	2008-02-13 15:50:11 UTC (rev 8500)
@@ -44,6 +44,7 @@
 #include <plearn/vmat/VMat_computeNearestNeighbors.h>
 #include <plearn/vmat/GetInputVMatrix.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn/math/plapack.h>
 
 namespace PLearn {
 using namespace std;
@@ -67,10 +68,10 @@
     n_classes( -1 ),
     output_connections_l1_penalty_factor( 0 ),
     output_connections_l2_penalty_factor( 0 ),
-    n_layers( 0 ),
     save_manifold_parzen_parameters( false ),
-    manifold_parzen_parameters_are_up_to_date( false ),
-    currently_trained_layer( 0 )
+    n_layers( 0 ),
+    currently_trained_layer( 0 ),
+    manifold_parzen_parameters_are_up_to_date( false )
 {
     // random_gen will be initialized in PLearner::build_()
     random_gen = new PRandom();
@@ -144,16 +145,6 @@
                   OptionBase::buildoption,
                   "The reconstruction weights of the autoassociators");
 
-    declareOption(ol, "unsupervised_layers", 
-                  &DeepNonLocalManifoldParzen::unsupervised_layers,
-                  OptionBase::buildoption,
-                  "Additional units for greedy unsupervised learning");
-
-    declareOption(ol, "unsupervised_connections", 
-                  &DeepNonLocalManifoldParzen::unsupervised_connections,
-                  OptionBase::buildoption,
-                  "Additional connections for greedy unsupervised learning");
-
     declareOption(ol, "k_neighbors", 
                   &DeepNonLocalManifoldParzen::k_neighbors,
                   OptionBase::buildoption,
@@ -165,6 +156,11 @@
                   OptionBase::buildoption,
                   "Dimensionality of the manifold");
 
+    declareOption(ol, "min_sigma_noise", 
+                  &DeepNonLocalManifoldParzen::min_sigma_noise,
+                  OptionBase::buildoption,
+                  "Minimum value for the noise variance");
+
     declareOption(ol, "n_classes", 
                   &DeepNonLocalManifoldParzen::n_classes,
                   OptionBase::buildoption,
@@ -187,12 +183,6 @@
                   "windows estimator should be saved during test, to speed up "
                   "testing.");
 
-    declareOption(ol, "manifold_parzen_parameters_are_up_to_date", 
-                  &DeepNonLocalManifoldParzen::manifold_parzen_parameters_are_up_to_date,
-                  OptionBase::buildoption,
-                  "Indication that the saved manifold parzen parameters are\n"
-                  "up to date.");
-
     declareOption(ol, "greedy_stages", 
                   &DeepNonLocalManifoldParzen::greedy_stages,
                   OptionBase::learntoption,
@@ -269,7 +259,7 @@
 
         if( min_sigma_noise < 0)
             PLERROR("DeepNonLocalManifoldParzen::build_() - \n"
-                    "min_sigma_noise should be > or = to 0.\n")
+                    "min_sigma_noise should be > or = to 0.\n");
 
         if(greedy_stages.length() == 0)
         {
@@ -372,7 +362,7 @@
     activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
     expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
 
-    int output_size = n_components*inputsize + (predict_mu ? inputsize : 0) + 1;
+    int output_size = n_components*inputsize() + inputsize() + 1;
     all_outputs.resize( output_size );
 
     if( !output_connections || output_connections->output_size != output_size)
@@ -382,9 +372,16 @@
         ow->output_size = output_size;
         ow->L1_penalty_factor = output_connections_l1_penalty_factor;
         ow->L2_penalty_factor = output_connections_l2_penalty_factor;
+        ow->random_gen = random_gen;
         ow->build();
         output_connections = ow;
     }
+
+    if( !(output_connections->random_gen) )
+    {
+        output_connections->random_gen = random_gen;
+        output_connections->forget();
+    }
 }
 
 // ### Nothing to add here, simply calls build_
@@ -428,13 +425,11 @@
     deepCopyField(U, copies);
     deepCopyField(V, copies);
     deepCopyField(z, copies);
-    deepCopyField(invSigma_F, copies);
-    deepCopyField(invSigma_z, copies);
+    deepCopyField(inv_Sigma_F, copies);
+    deepCopyField(inv_Sigma_z, copies);
     deepCopyField(temp_ncomp, copies);
     deepCopyField(diff_neighbor_input, copies);
     deepCopyField(sm_svd, copies);
-    deepCopyField(sn, copies);
-    deepCopyField(S, copies);
     deepCopyField(uk, copies);
     deepCopyField(fk, copies);
     deepCopyField(uk2, copies);
@@ -549,9 +544,9 @@
         reconstruction_expectation_gradients.resize(layers[i]->size);
 
         pos_down_val.resize(layers[i]->size);
-        pos_up_val.resize(greedy_layers[i]->size);
+        pos_up_val.resize(layers[i]->size);
         neg_down_val.resize(layers[i]->size);
-        neg_up_val.resize(greedy_layers[i]->size);
+        neg_up_val.resize(layers[i]->size);
 
         for( ; *this_stage<end_stage ; (*this_stage)++ )
         {
@@ -813,14 +808,14 @@
 
     computeManifoldParzenParameters( input, F, mu, pre_sigma_noise, U, sm_svd );
 
-    real sigma_noise = square(pre_sigma_noise[0], 2) + min_sigma_noise;
+    real sigma_noise = pre_sigma_noise[0]* pre_sigma_noise[0] + min_sigma_noise;
 
     real mahal = 0;
     real norm_term = 0;
     real dotp = 0;
     real coef = 0;
     real n = inputsize();
-    inv_Sigma_z.resize(inputsize());
+    inv_Sigma_z.resize(k_neighbors,inputsize());
     inv_Sigma_z.clear();
     real tr_inv_Sigma = 0;
     train_costs.last() = 0;
@@ -846,7 +841,7 @@
             dotp = dot(zj,uk);
             coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
             multiplyAcc(inv_sigma_zj,uk,dotp*coef);
-            mahal -= square(dotp)*0.5*coef;
+            mahal -= dotp*dotp*0.5*coef;
             norm_term -= 0.5*pl_log(sm_svd[k]);
             if(j==0)
                 tr_inv_Sigma += coef;
@@ -855,7 +850,7 @@
         train_costs.last() += -1*(norm_term + mahal);
     }
 
-    train_costs.last() / k_neighbors;
+    train_costs.last() /= k_neighbors;
 
     inv_Sigma_F.resize( n_components, inputsize() );
     inv_Sigma_F.clear();
@@ -875,12 +870,12 @@
     }
 
     all_outputs_gradient.clear();
-    real coef = 1/train_set->length();
+    coef = 1/train_set->length();
     for(int neighbor=0; neighbor<k_neighbors; neighbor++)
     {
         // dNLL/dF
         product(temp_ncomp,F,inv_Sigma_z(neighbor));
-        bprop_to_bases(all_outputs_gradient.toVec(0,n_components * inputsize()).toMat(n_components,inputsize()),
+        bprop_to_bases(all_outputs_gradient.subVec(0,n_components * inputsize()).toMat(n_components,inputsize()),
                        inv_Sigma_F,
                        temp_ncomp,inv_Sigma_z(neighbor),
                        coef);
@@ -965,6 +960,15 @@
 
 void DeepNonLocalManifoldParzen::computeOutput(const Vec& input, Vec& output) const
 {
+
+    if( currently_trained_layer<n_layers
+        && reconstruction_connections.length() != 0 )
+    {
+        computeRepresentation(input, input_representation, 
+                              currently_trained_layer);
+        return;
+    }
+
     test_votes.resize(n_classes);
     test_votes.clear();
 
@@ -990,13 +994,13 @@
         {
             for( int j=0; j<class_datasets[i]->length(); j++ )
             {
-                class_datasets[i]->getExample(input_j,target,weight);
+                class_datasets[i]->getExample(j,input_j,target,weight);
 
                 input_j_index = class_datasets[i]->indices[j];
                 U << eigenvectors[input_j_index];
-                sm_svd << eigenvalues[input_j_index];
+                sm_svd << eigenvalues(input_j_index);
                 sigma_noise = sigma_noises[input_j_index];
-                mu << mus[input_j_index];
+                mu << mus(input_j_index);
 
                 substract(input,input_j,diff_neighbor_input); 
                 substract(diff_neighbor_input,mu,diff); 
@@ -1010,7 +1014,7 @@
                     uk = U(k);
                     dotp = dot(diff,uk);
                     coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
-                    mahal -= square(dotp)*0.5*coef;
+                    mahal -= dotp*dotp*0.5*coef;
                     norm_term -= 0.5*pl_log(sm_svd[k]);
                 }
                 
@@ -1030,12 +1034,13 @@
         {
             for( int j=0; j<class_datasets[i]->length(); j++ )
             {
-                class_datasets[i]->getExample(input_j,target,weight);
+                class_datasets[i]->getExample(j,input_j,target,weight);
 
                 computeManifoldParzenParameters( input_j, F, mu, 
                                                  pre_sigma_noise, U, sm_svd );
                 
-                sigma_noise = square(pre_sigma_noise[0], 2) + min_sigma_noise;
+                sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] 
+                    + min_sigma_noise;
                 
                 substract(input,input_j,diff_neighbor_input); 
                 substract(diff_neighbor_input,mu,diff); 
@@ -1049,7 +1054,7 @@
                     uk = U(k);
                     dotp = dot(diff,uk);
                     coef = (1.0/(sm_svd[k]+sigma_noise) - 1.0/sigma_noise);
-                    mahal -= square(dotp)*0.5*coef;
+                    mahal -= dotp*dotp*0.5*coef;
                     norm_term -= 0.5*pl_log(sm_svd[k]);
                 }
                 
@@ -1079,15 +1084,8 @@
     if( currently_trained_layer<n_layers 
         && reconstruction_connections.length() != 0 )
     {
-        greedy_connections[currently_trained_layer-1]->fprop(
-            expectations[currently_trained_layer-1],
-            greedy_activation);
-        
-        greedy_layers[currently_trained_layer-1]->fprop(greedy_activation,
-                                    greedy_expectation);
-        
         reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
-            greedy_expectation,
+            expectations[currently_trained_layer],
             reconstruction_activations);
         layers[ currently_trained_layer-1 ]->fprop( 
             reconstruction_activations,
@@ -1101,11 +1099,13 @@
             layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[currently_trained_layer-1]);
     }
-
-    if( ((int)round(output[0])) == ((int)round(target[0])) )
-        costs[n_layers-1] = 0;
     else
-        costs[n_layers-1] = 1;
+    {
+        if( ((int)round(output[0])) == ((int)round(target[0])) )
+            costs[n_layers-1] = 0;
+        else
+            costs[n_layers-1] = 1;
+    }
 }
 
 //////////
@@ -1119,6 +1119,7 @@
         Vec input( inputsize() );
         Vec target( targetsize() );
         real weight;
+        real sigma_noise;
 
         eigenvectors.resize(train_set->length());
         eigenvalues.resize(train_set->length(),n_components);
@@ -1127,18 +1128,18 @@
 
         for( int i=0; i<train_set->length(); i++ )
         {
-            train_set[i]->getExample(input,target,weight);
+            train_set->getExample(i,input,target,weight);
 
             computeManifoldParzenParameters( input, F, mu, 
                                              pre_sigma_noise, U, sm_svd );
             
-            sigma_noise = square(pre_sigma_noise[0], 2) + min_sigma_noise;
+            sigma_noise = pre_sigma_noise[0]*pre_sigma_noise[0] + min_sigma_noise;
 
             eigenvectors[i].resize(n_components,inputsize());
             eigenvectors[i] << U;
-            eigenvalues[i] << sm_svd;
+            eigenvalues(i) << sm_svd;
             sigma_noises[i] = sigma_noise;
-            mus[i] << mu;
+            mus(i) << mu;
         }
         
         manifold_parzen_parameters_are_up_to_date = true;
@@ -1174,10 +1175,6 @@
     
     manifold_parzen_parameters_are_up_to_date = false;
 
-    Vec input( inputsize() );
-    Vec target( targetsize() );
-    real weight; // unused
-    
     // Separate classes
     class_datasets.resize(n_classes);
     for(int k=0; k<n_classes; k++)

Modified: trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h
===================================================================
--- trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-02-13 15:49:04 UTC (rev 8499)
+++ trunk/plearn_learners_experimental/DeepNonLocalManifoldParzen.h	2008-02-13 15:50:11 UTC (rev 8500)
@@ -103,7 +103,7 @@
     int k_neighbors;
 
     //! Dimensionality of the manifold
-    real n_components;
+    int n_components;
 
     //! Minimum value for the noise variance.
     real min_sigma_noise;
@@ -121,9 +121,6 @@
     //! windows estimator should be saved during test, to speed up testing.
     bool save_manifold_parzen_parameters;
 
-    //! Indication that the saved manifold parzen parameters are up to date.
-    bool manifold_parzen_parameters_are_up_to_date;
-
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -182,8 +179,7 @@
     virtual void setTrainingSet(VMat training_set, bool call_forget=true);
 
     void greedyStep( const Vec& input, const Vec& target, int index, 
-                     Vec train_costs, int stage, Vec similar_example,
-                     Vec dissimilar_example);
+                     Vec train_costs, int stage);
 
     void fineTuningStep( const Vec& input, const Vec& target,
                          Vec& train_costs, Mat nearest_neighbors);
@@ -241,7 +237,7 @@
     mutable Vec reconstruction_expectation_gradients;
 
     //! Output weights
-    mutable PP<OnlineLearningModuling> output_connections;
+    mutable PP<OnlineLearningModule> output_connections;
     
     //! Example representation
     mutable Vec input_representation;
@@ -261,8 +257,8 @@
     mutable Vec pre_sigma_noise;
 
     //! Variables for the SVD and gradient computation
-    mutable Mat Ut, U, V, z, invSigma_F, invSigma_z;
-    mutable Vec temp_ncomp, diff_neighbor_input, sm_svd, sn, S;
+    mutable Mat Ut, U, V, z, inv_Sigma_F, inv_Sigma_z;
+    mutable Vec temp_ncomp, diff_neighbor_input, sm_svd, S;
     mutable Vec uk, fk, uk2, inv_sigma_zj, zj, inv_sigma_fk, diff;
 
     //! Positive down statistic
@@ -277,13 +273,13 @@
     // Saved components of manifold parzen windows
 
     //! Eigenvectors
-    TVec<Mat> eigenvectors;
+    mutable TVec<Mat> eigenvectors;
     //! Eigenvalues
-    Mat eigenvalues;
+    mutable Mat eigenvalues;
     //! Sigma noises
-    Vec sigma_noises;
+    mutable Vec sigma_noises;
     //! Mus
-    Mat mus;
+    mutable Mat mus;
 
     //! Datasets for each class
     TVec< PP<ClassSubsetVMatrix> > class_datasets;
@@ -295,11 +291,8 @@
     //! Nearest neighbors for each training example
     TMat<int> nearest_neighbors_indices;
 
-    //! Nearest neighbors for each test example
-    mutable TVec<int> test_nearest_neighbors_indices;
-
     //! Nearest neighbor votes for test example
-    TVec<int> test_votes;
+    mutable Vec test_votes;
 
     //! Stages of the different greedy phases
     TVec<int> greedy_stages;
@@ -308,6 +301,9 @@
     //! n_layers means the output layer)
     int currently_trained_layer;
 
+    //! Indication that the saved manifold parzen parameters are up to date.
+    mutable bool manifold_parzen_parameters_are_up_to_date;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From larocheh at mail.berlios.de  Wed Feb 13 16:51:32 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 16:51:32 +0100
Subject: [Plearn-commits] r8501 - trunk/plearn_learners_experimental
Message-ID: <200802131551.m1DFpW01014049@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 16:51:31 +0100 (Wed, 13 Feb 2008)
New Revision: 8501

Modified:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
..


Modified: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2008-02-13 15:50:11 UTC (rev 8500)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2008-02-13 15:51:31 UTC (rev 8501)
@@ -57,10 +57,9 @@
     greedy_decrease_ct( 0. ),
     fine_tuning_learning_rate( 0. ),
     fine_tuning_decrease_ct( 0. ),
-    batch_size(50),
+    minibatch_size(50),
     global_output_layer(false),
-    fill_in_null_diagonal(true),
-    relative_min_improvement(1e-3),
+    fill_in_null_diagonal(false),
     n_layers( 0 )
 {
     // random_gen will be initialized in PLearner::build_()
@@ -96,11 +95,21 @@
                   "fine tuning\n"
                   "gradient descent.\n");
 
-    declareOption(ol, "batch_size", 
-                  &StackedSVDNet::batch_size,
+    declareOption(ol, "minibatch_size", 
+                  &StackedSVDNet::minibatch_size,
                   OptionBase::buildoption,
                   "Size of mini-batch for gradient descent");
 
+    declareOption(ol, "training_schedule", &StackedSVDNet::training_schedule,
+                  OptionBase::buildoption,
+                  "Number of examples to use during each phase of learning:\n"
+                  "first the greedy phases, and then the fine-tuning phase.\n"
+                  "However, the learning will stop as soon as we reach nstages.\n"
+                  "For example for 2 hidden layers, with 1000 examples in each\n"
+                  "greedy phase, and 500 in the fine-tuning phase, this option\n"
+                  "should be [1000 1000 500], and nstages should be at least 2500.\n"
+        );
+    
     declareOption(ol, "global_output_layer", 
                   &StackedSVDNet::global_output_layer,
                   OptionBase::buildoption,
@@ -115,12 +124,6 @@
                   "logistic auto-regression should be filled with the\n"
                   "maximum absolute value of each corresponding row.\n");
 
-    declareOption(ol, "relative_min_improvement", 
-                  &StackedSVDNet::relative_min_improvement,
-                  OptionBase::buildoption,
-                  "Minimum relative improvement convergence criteria \n"
-                  "for the logistic auto-regression.");
-
     declareOption(ol, "layers", &StackedSVDNet::layers,
                   OptionBase::buildoption,
                   "The layers of units in the network. The first element\n"
@@ -165,6 +168,22 @@
         // Initialize some learnt variables
         n_layers = layers.length();
         
+        cumulative_schedule.resize( n_layers+1 );
+        cumulative_schedule[0] = 0;
+        for( int i=0 ; i<n_layers ; i++ )
+        {
+            cumulative_schedule[i+1] = cumulative_schedule[i] +
+                training_schedule[i];
+        }
+
+        reconstruction_test_costs.resize( n_layers-1 );
+        reconstruction_test_costs.fill( MISSING_VALUE );
+
+        if( training_schedule.length() != n_layers )
+            PLERROR("StackedSVDNet::build_() - \n"
+                    "training_schedule should have %d elements.\n",
+                    n_layers-1);
+
         if( weightsize_ > 0 )
             PLERROR("StackedSVDNet::build_() - \n"
                     "usage of weighted samples (weight size > 0) is not\n"
@@ -175,7 +194,7 @@
                     "layers[0] should have a size of %d.\n",
                     inputsize_);
 
-        reconstruction_costs.resize(batch_size,1);    
+        reconstruction_costs.resize(minibatch_size,1);    
 
         activation_gradients.resize( n_layers );
         expectation_gradients.resize( n_layers );
@@ -192,18 +211,18 @@
                 PLERROR("In StackedSVDNet::build()_: "
                     "layers must have decreasing sizes from bottom to top.");
                 
-            activation_gradients[i].resize( batch_size, layers[i]->size );
-            expectation_gradients[i].resize( batch_size, layers[i]->size );
+            activation_gradients[i].resize( minibatch_size, layers[i]->size );
+            expectation_gradients[i].resize( minibatch_size, layers[i]->size );
         }
 
         if( !final_cost )
             PLERROR("StackedSVDNet::build_costs() - \n"
                     "final_cost should be provided.\n");
 
-        final_cost_inputs.resize( batch_size, final_cost->input_size );
+        final_cost_inputs.resize( minibatch_size, final_cost->input_size );
         final_cost_value.resize( final_cost->output_size );
-        final_cost_values.resize( batch_size, final_cost->output_size );
-        final_cost_gradients.resize( batch_size, final_cost->input_size );
+        final_cost_values.resize( minibatch_size, final_cost->output_size );
+        final_cost_gradients.resize( minibatch_size, final_cost->input_size );
         final_cost->setLearningRate( fine_tuning_learning_rate );
 
         if( !(final_cost->random_gen) )
@@ -227,12 +246,12 @@
                         sum);
 
             global_output_layer_input.resize(sum);
-            global_output_layer_inputs.resize(batch_size,sum);
-            global_output_layer_input_gradients.resize(batch_size,sum);
+            global_output_layer_inputs.resize(minibatch_size,sum);
+            global_output_layer_input_gradients.resize(minibatch_size,sum);
             expectation_gradients[n_layers-1] = 
                 global_output_layer_input_gradients.subMat(
                     0, sum-layers[n_layers-1]->size, 
-                    batch_size, layers[n_layers-1]->size);
+                    minibatch_size, layers[n_layers-1]->size);
         }
         else
         {
@@ -276,6 +295,7 @@
 
     // deepCopyField(, copies);
 
+    deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
@@ -286,6 +306,7 @@
     deepCopyField(reconstruction_layer, copies);
     deepCopyField(reconstruction_targets, copies);
     deepCopyField(reconstruction_costs, copies);
+    deepCopyField(reconstruction_test_costs, copies);
     deepCopyField(reconstruction_activation_gradient, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
     deepCopyField(reconstruction_input_gradients, copies);
@@ -296,6 +317,7 @@
     deepCopyField(final_cost_value, copies);
     deepCopyField(final_cost_values, copies);
     deepCopyField(final_cost_gradients, copies);
+    deepCopyField(cumulative_schedule, copies);
     
     //PLERROR("In StackedSVDNet::makeDeepCopyFromShallowCopy(): "
     //        "not implemented yet.");
@@ -304,6 +326,11 @@
 
 int StackedSVDNet::outputsize() const
 {
+    if( stage == 0 )
+        return layers[0]->size;
+    for( int i=1; i<n_layers; i++ )
+        if( stage <= cumulative_schedule[i] )
+            return layers[i-1]->size;
     return final_module->output_size;
 }
 
@@ -327,11 +354,17 @@
 {
     MODULE_LOG << "train() called " << endl;
 
+    // Enforce value of cumulative_schedule because build_() might
+    // not be called if we change training_schedule inside a HyperLearner
+    for( int i=0 ; i<n_layers ; i++ )
+        cumulative_schedule[i+1] = cumulative_schedule[i] +
+            training_schedule[i];
+
     Vec input( inputsize() );
     Vec target( targetsize() );
-    real weight; // unused
-    Mat inputs( batch_size, inputsize() );
-    Mat targets( batch_size, targetsize() );
+    Mat inputs( minibatch_size, inputsize() );
+    Mat targets( minibatch_size, targetsize() );
+    Vec weights( minibatch_size );
 
     TVec<string> train_cost_names = getTrainCostNames() ;
     Vec train_costs( train_cost_names.length() );
@@ -344,92 +377,46 @@
 
     real lr = 0;
     int init_stage;
+    int end_stage;
 
     /***** initial greedy training *****/
-    if(stage == 0)
+    connections.resize(n_layers-1);
+    rbm_connections.resize(n_layers-1);
+    TVec< Vec > biases(n_layers-1);
+    for( int i=0 ; i<n_layers-1 ; i++ )
     {
-        connections.resize(n_layers-1);
-        rbm_connections.resize(n_layers-1);
-        TVec< Vec > biases(n_layers-1);
-        for( int i=0 ; i<n_layers-1 ; i++ )
-        {
-            MODULE_LOG << "Training connection weights between layers " << i
-                       << " and " << i+1 << endl;
 
-            connections[i] = new RBMMatrixConnection();
-            connections[i]->up_size = layers[i]->size;
-            connections[i]->down_size = layers[i]->size;
-            connections[i]->random_gen = random_gen;
-            connections[i]->build();
-            for(int j=0; j < layers[i]->size; j++)
-                connections[i]->weights(j,j) = 0;
+        end_stage = min(cumulative_schedule[i+1], nstages);
+        if( stage >= end_stage )
+            continue;
 
-            rbm_connections[i] = (RBMMatrixConnection *) connections[i];
+        MODULE_LOG << "Training connection weights between layers " << i
+                   << " and " << i+1 << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+        MODULE_LOG << "  greedy_learning_rate = " 
+                   << greedy_learning_rate << endl;
 
-            CopiesMap map;
-            reconstruction_layer = layers[ i ]->deepCopy( map );
-            reconstruction_targets.resize( batch_size, layers[ i ]->size );
-            reconstruction_activation_gradient.resize( layers[ i ]->size );
-            reconstruction_activation_gradients.resize( 
-                batch_size, layers[ i ]->size );
-            reconstruction_input_gradients.resize( 
-                batch_size, layers[ i ]->size );
+        if( report_progress )
+            pb = new ProgressBar( "Training layer "+tostring(i)
+                                  +" of "+classname(),
+                                  end_stage - stage );
 
-            lr = greedy_learning_rate;
-            connections[i]->setLearningRate( lr );
-            reconstruction_layer->setLearningRate( lr );
 
-            real cost = 0;
-            real last_cost = 0;
-            int nupdates = 0;
-            int nepochs = 0;
-            while( nepochs < 2 ||
-                   (last_cost - cost) / last_cost >= relative_min_improvement )
-            {
-                train_stats->forget();
-                for(int sample = 0; 
-                    sample < train_set.length()/batch_size; 
-                    sample++)
-                {
-                    if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
-                    {
-                        lr = greedy_learning_rate/(1 + greedy_decrease_ct 
-                                                   * nupdates);
-                        connections[i]->setLearningRate( lr );
-                        reconstruction_layer->setLearningRate( lr );                
-                    }
-                    
-                    for(int j=0; j<batch_size; j++)
-                    {
-                        train_set->getExample(sample*batch_size + j, 
-                                              input, target, weight);
-                        inputs(j) << input;
-                        targets(j) << target;
-                    }
-                    greedyStep( inputs, targets, i, train_costs );
-                    nupdates++;
-                    train_stats->update( train_costs );
-                }
-                train_stats->finalize();
-                nepochs++;
-                last_cost = cost;
-                cost = train_stats->getMean()[i];
-                if(verbosity > 2)
-                    cout << "reconstruction error at iteration " << nepochs << 
-                        ": " << 
-                        cost << " or " << cost/layers[i]->size << " (rel)" << endl;
-            }
-
+        // Finalize training of last layer (if any)
+        if( i>0 && stage < end_stage && stage == cumulative_schedule[i] )
+        {
             if(fill_in_null_diagonal)
             {
                 // Fill in the empty diagonal
                 for(int j=0; j<layers[i]->size; j++)
                 {
-                    connections[i]->weights(j,j) = maxabs(connections[i]->weights(j));
+                    connections[i-1]->weights(j,j) = 
+                        maxabs(connections[i-1]->weights(j));
                 }
             }
             
-            if(layers[i]->size != layers[i+1]->size)
+            if(layers[i-1]->size != layers[i]->size)
             {
                 Mat A,U,Vt;
                 Vec S;
@@ -438,85 +425,172 @@
                 A.column( 0 ) << reconstruction_layer->bias;
                 A.subMat( 0, 1, reconstruction_layer->size, 
                           reconstruction_layer->size ) << 
-                    connections[i]->weights;
+                    connections[i-1]->weights;
                 SVD( A, U, S, Vt );
-                connections[ i ]->up_size = layers[ i+1 ]->size;
-                connections[ i ]->down_size = layers[ i ]->size;
-                connections[ i ]->build();
-                connections[ i ]->weights << Vt.subMat( 
-                    0, 1, layers[ i+1 ]->size, Vt.width()-1 );
-                biases[ i ].resize( layers[i+1]->size );
-                for(int j=0; j<biases[ i ].length(); j++)
-                    biases[ i ][ j ] = Vt(j,0);
-
-                for(int j=0; j<connections[ i ]->up_size; j++)
+                connections[ i-1 ]->up_size = layers[ i ]->size;
+                connections[ i-1 ]->down_size = layers[ i-1 ]->size;
+                connections[ i-1 ]->build();
+                connections[ i-1 ]->weights << Vt.subMat( 
+                    0, 1, layers[ i ]->size, Vt.width()-1 );
+                biases[ i-1 ].resize( layers[i]->size );
+                for(int j=0; j<biases[ i-1 ].length(); j++)
+                    biases[ i-1 ][ j ] = Vt(j,0);
+                
+                for(int j=0; j<connections[ i-1 ]->up_size; j++)
                 {
-                    connections[ i ]->weights( j ) *= S[ j ];
-                    biases[ i ][ j ] *= S[ j ];
+                    connections[ i-1 ]->weights( j ) *= S[ j ];
+                    biases[ i-1 ][ j ] *= S[ j ];
                 }
             }
             else
             {
-                biases[ i ].resize( layers[ i+1 ]->size );
-                biases[ i ] << reconstruction_layer->bias;
+                biases[ i-1 ].resize( layers[ i ]->size );
+                biases[ i-1 ] << reconstruction_layer->bias;
             }
+            layers[ i ]->bias << biases[ i-1 ];
         }
-        stage++;
-        for(int i=0; i<biases.length(); i++)
+
+        // Create connections
+        if(stage == cumulative_schedule[i])
         {
-            layers[ i+1 ]->bias << biases[ i ];
+            connections[i] = new RBMMatrixConnection();
+            connections[i]->up_size = layers[i]->size;
+            connections[i]->down_size = layers[i]->size;
+            connections[i]->random_gen = random_gen;
+            connections[i]->build();
+            for(int j=0; j < layers[i]->size; j++)
+                connections[i]->weights(j,j) = 0;
+
+            rbm_connections[i] = (RBMMatrixConnection *) connections[i];
+
+            CopiesMap map;
+            reconstruction_layer = layers[ i ]->deepCopy( map );
+            reconstruction_targets.resize( minibatch_size, layers[ i ]->size );
+            reconstruction_activation_gradient.resize( layers[ i ]->size );
+            reconstruction_activation_gradients.resize( 
+                minibatch_size, layers[ i ]->size );
+            reconstruction_input_gradients.resize( 
+                minibatch_size, layers[ i ]->size );
+
+            lr = greedy_learning_rate;
+            connections[i]->setLearningRate( lr );
+            reconstruction_layer->setLearningRate( lr );
         }
+
+        for( ; stage<end_stage ; stage++)
+        {
+            train_stats->forget();
+            
+            if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            {
+                lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                           * (stage - cumulative_schedule[i]) );
+                connections[i]->setLearningRate( lr );
+                reconstruction_layer->setLearningRate( lr );                
+            }
+            
+            train_set->getExamples((stage*minibatch_size)%train_set->length(),
+                                   minibatch_size, inputs, targets, weights,
+                                   NULL, true);
+                                   
+            greedyStep( inputs, targets, i, train_costs );
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( stage - cumulative_schedule[i] + 1 );
+        }
+        train_stats->finalize();
     }
 
     /***** fine-tuning by gradient descent *****/
-    if( stage < nstages )
-    {
 
-        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  nstages = " << nstages << endl;
-        MODULE_LOG << "  fine_tuning_learning_rate = " << 
-            fine_tuning_learning_rate << endl;
+    end_stage = min(cumulative_schedule[n_layers], nstages);
+    if( stage >= end_stage )
+        return;
 
-        init_stage = stage;
-        if( report_progress && stage < nstages )
-            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
-                                  + classname(),
-                                  nstages - init_stage );
-
-        setLearningRate( fine_tuning_learning_rate );
-        train_costs.fill(MISSING_VALUE);
-
-        for( ; stage<nstages ; stage++ )
+    // Finalize training of last layer (if any)
+    if( n_layers>1 && stage < end_stage && stage == cumulative_schedule[n_layers-1] )
+    {
+        if(fill_in_null_diagonal)
         {
-            for( int sample = 0; 
-                 sample<train_set->length()/batch_size; 
-                 sample++)
+            // Fill in the empty diagonal
+            for(int j=0; j<layers[n_layers-1]->size; j++)
             {
-                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                    setLearningRate( fine_tuning_learning_rate
-                                     / (1. + fine_tuning_decrease_ct * stage ) );
-
-                for(int j=0; j<batch_size; j++)
-                {
-                    train_set->getExample(sample*batch_size + j, 
-                                          input, target, weight);
-                    inputs(j) << input;
-                    targets(j) << target;
-                }
-                fineTuningStep( inputs, targets, train_costs );
-                train_stats->update( train_costs );
-                
-                if( pb )
-                    pb->update( stage - init_stage + 1 );
+                connections[n_layers-2]->weights(j,j) = 
+                    maxabs(connections[n_layers-2]->weights(j));
             }
-            if(verbosity > 2)
-                cout << "error at stage " << stage << ": " << 
-                    train_stats->getMean() << endl;
-
         }
+        
+        if(layers[n_layers-2]->size != layers[n_layers-1]->size)
+        {
+            Mat A,U,Vt;
+            Vec S;
+            A.resize( reconstruction_layer->size, 
+                      reconstruction_layer->size+1);
+            A.column( 0 ) << reconstruction_layer->bias;
+            A.subMat( 0, 1, reconstruction_layer->size, 
+                      reconstruction_layer->size ) << 
+                connections[n_layers-2]->weights;
+            SVD( A, U, S, Vt );
+            connections[ n_layers-2 ]->up_size = layers[ n_layers-1 ]->size;
+            connections[ n_layers-2 ]->down_size = layers[ n_layers-2 ]->size;
+            connections[ n_layers-2 ]->build();
+            connections[ n_layers-2 ]->weights << Vt.subMat( 
+                0, 1, layers[ n_layers-1 ]->size, Vt.width()-1 );
+            biases[ n_layers-2 ].resize( layers[n_layers-1]->size );
+            for(int j=0; j<biases[ n_layers-2 ].length(); j++)
+                biases[ n_layers-2 ][ j ] = Vt(j,0);
+            
+            for(int j=0; j<connections[ n_layers-2 ]->up_size; j++)
+            {
+                connections[ n_layers-2 ]->weights( j ) *= S[ j ];
+                biases[ n_layers-2 ][ j ] *= S[ j ];
+            }
+        }
+        else
+        {
+            biases[ n_layers-2 ].resize( layers[ n_layers-1 ]->size );
+            biases[ n_layers-2 ] << reconstruction_layer->bias;
+        }
+        layers[ n_layers-1 ]->bias << biases[ n_layers-2 ];
     }
+
+    MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+    MODULE_LOG << "  stage = " << stage << endl;
+    MODULE_LOG << "  end_stage = " << end_stage << endl;
+    MODULE_LOG << "  fine_tuning_learning_rate = " 
+               << fine_tuning_learning_rate << endl;
     
+    init_stage = stage;
+    if( report_progress && stage < end_stage )
+        pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                              + classname(),
+                              end_stage - init_stage );
+    
+    setLearningRate( fine_tuning_learning_rate );
+    train_costs.fill(MISSING_VALUE);
+    
+    for( ; stage<end_stage ; stage++ )
+    {
+        if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+            setLearningRate( fine_tuning_learning_rate
+                             / (1. + fine_tuning_decrease_ct * 
+                                (stage - cumulative_schedule[n_layers]) ) );
+            
+        train_set->getExamples((stage*minibatch_size)%train_set->length(),
+                               minibatch_size, inputs, targets, weights,
+                               NULL, true);
+        
+        fineTuningStep( inputs, targets, train_costs );
+        train_stats->update( train_costs );
+        
+        if( pb )
+            pb->update( stage - init_stage + 1 );
+    }
+
+    if(verbosity > 2)
+        cout << "error at stage " << stage << ": " << 
+            train_stats->getMean() << endl;
     train_stats->finalize();
 }
 
@@ -540,7 +614,7 @@
     
     reconstruction_layer->fpropNLL( layers[ index ]->getExpectations(), 
                                     reconstruction_costs);
-    train_costs[index] = sum( reconstruction_costs )/batch_size;
+    train_costs[index] = sum( reconstruction_costs )/minibatch_size;
 
     reconstruction_layer->bpropNLL( 
         layers[ index ]->getExpectations(), reconstruction_costs,
@@ -580,7 +654,7 @@
         for(int i=0; i<layers.length(); i++)
         {
             global_output_layer_inputs.subMat(0, offset, 
-                                              batch_size, layers[i]->size)
+                                              minibatch_size, layers[i]->size)
                 << layers[i]->getExpectations();
             offset += layers[i]->size;
         }
@@ -625,7 +699,7 @@
             expectation_gradients[ i ] +=  
                 global_output_layer_input_gradients.subMat(
                     0, sum - layers[i]->size,
-                    batch_size, layers[i]->size);
+                    minibatch_size, layers[i]->size);
             sum -= layers[i]->size;
         }
                 
@@ -648,9 +722,24 @@
     layers[ 0 ]->expectation <<  input ;
     layers[ 0 ]->expectation_is_up_to_date = true;
     
+    if( stage == 0 )
+    {
+        output << input;
+        return;
+    }
+
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         connections[ i ]->setAsDownInput( layers[i]->expectation );
+        if( stage <= cumulative_schedule[i+1] )
+        {
+            reconstruction_layer->getAllActivations( rbm_connections[i], 0, false );
+            reconstruction_layer->computeExpectation();
+            reconstruction_test_costs[i] = 
+                reconstruction_layer->fpropNLL( layers[i]->expectation );
+            output << reconstruction_layer->expectation;
+            return;
+        }
         layers[ i+1 ]->getAllActivations( rbm_connections[i], 0, false );
         layers[ i+1 ]->computeExpectation();
     }
@@ -680,8 +769,21 @@
 
     costs.resize( getTestCostNames().length() );
     costs.fill( MISSING_VALUE );
+
+    if( stage == 0 )
+        return;
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        if( stage <= cumulative_schedule[i+1] )
+        {
+            costs[i] = reconstruction_test_costs[i];
+            return;
+        }
+    }
     
     final_cost->fprop( output, target, final_cost_value );
+    costs.subVec(0, reconstruction_test_costs.length()) << reconstruction_test_costs;
     costs.subVec(costs.length()-final_cost_value.length(),
                  final_cost_value.length()) <<
         final_cost_value;
@@ -696,7 +798,7 @@
     TVec<string> cost_names(0);
 
     for( int i=0; i<layers.size()-1; i++)
-        cost_names.push_back("reconstruction_error_" + tostring(i+1));
+        cost_names.push_back("layer"+tostring(i)+".reconstruction_error");
     
     cost_names.append( final_cost->name() );
 

Modified: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2008-02-13 15:50:11 UTC (rev 8500)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2008-02-13 15:51:31 UTC (rev 8501)
@@ -76,7 +76,7 @@
     real fine_tuning_decrease_ct;
 
     //! Size of mini-batch for gradient descent
-    int batch_size;
+    int minibatch_size;
 
     //! Indication that the output layer (given by the final module)
     //! should have as input all units of the network (including the input units)
@@ -86,11 +86,16 @@
     //! logistic auto-regression should be filled with the
     //! maximum absolute value of each corresponding row.
     bool fill_in_null_diagonal;
-    
-    //! Minimum relative improvement convergence criteria
-    //! for the logistic auto-regression.
-    real relative_min_improvement;
-    
+        
+    //! Number of examples to use during each phase of learning:
+    //! first the greedy phases, and then the fine-tuning phase.
+    //! However, the learning will stop as soon as we reach nstages.
+    //! For example for 2 hidden layers, with 1000 examples in each
+    //! greedy phase, and 500 in the fine-tuning phase, this option
+    //! should be [1000 1000 500], and nstages should be at least 2500.
+    //! When online = true, this vector is ignored and should be empty.
+    TVec<int> training_schedule;
+
     //! The layers of units in the network
     TVec< PP<RBMLayer> > layers;
 
@@ -198,6 +203,9 @@
     //! Reconstruction costs
     mutable Mat reconstruction_costs;
 
+    //! Reconstruction costs (when using computeOutput and computeCostsFromOutput)
+    mutable Vec reconstruction_test_costs;
+
     //! Reconstruction activation gradient
     mutable Vec reconstruction_activation_gradient;
 
@@ -228,6 +236,10 @@
     //! Stores the gradients of the cost at the inputs of final_cost
     mutable Mat final_cost_gradients;
 
+    //! Cumulative training schedule
+    TVec<int> cumulative_schedule;
+
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From larocheh at mail.berlios.de  Wed Feb 13 16:52:55 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 16:52:55 +0100
Subject: [Plearn-commits] r8502 - trunk/plearn_learners/online
Message-ID: <200802131552.m1DFqtkB014136@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 16:52:54 +0100 (Wed, 13 Feb 2008)
New Revision: 8502

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
Log:
Added a batch version of addBiasDecay...


Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2008-02-13 15:51:31 UTC (rev 8501)
+++ trunk/plearn_learners/online/RBMLayer.cc	2008-02-13 15:52:54 UTC (rev 8502)
@@ -724,6 +724,32 @@
 
 }
 
+void RBMLayer::addBiasDecay(Mat& bias_gradients)
+{
+    PLASSERT(bias_gradients.width()==size);
+    if (bias_decay_type=="none")
+        return;
+
+    real avg_lr = learning_rate / bias_gradients.length();
+
+    for(int b=0; b<bias_gradients.length(); b++)
+    {
+        real *bg = bias_gradients[b];
+        real *b = bias.data();
+        bias_decay_type = lowerstring(bias_decay_type);
+        
+        if (bias_decay_type=="negative")  // Pushes the biases towards -\infty
+            for( int i=0 ; i<size ; i++ )
+                bg[i] += avg_lr * bias_decay_parameter;
+        else if (bias_decay_type=="l2")  // L2 penalty on the biases
+            for (int i=0 ; i<size ; i++ )
+                bg[i] += avg_lr * bias_decay_parameter * b[i];
+        else
+            PLERROR("RBMLayer::addBiasDecay(string) bias_decay_type %s is not in"
+                    " the list, in subclass %s\n",bias_decay_type.c_str(),classname().c_str());
+    }
+}
+
 void RBMLayer::applyBiasDecay()
 {
     

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2008-02-13 15:51:31 UTC (rev 8501)
+++ trunk/plearn_learners/online/RBMLayer.h	2008-02-13 15:52:54 UTC (rev 8502)
@@ -297,6 +297,9 @@
     //! Adds the bias decay to the bias gradients
     virtual void addBiasDecay(Vec& bias_gradient);
 
+    //! Adds the bias decay to the bias gradients
+    virtual void addBiasDecay(Mat& bias_gradient);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From larocheh at mail.berlios.de  Wed Feb 13 17:04:35 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 17:04:35 +0100
Subject: [Plearn-commits] r8503 - trunk/plearn_learners/online
Message-ID: <200802131604.m1DG4ZBa015494@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 17:04:35 +0100 (Wed, 13 Feb 2008)
New Revision: 8503

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
Log:
Added the bias decay in bpropUpdate and bpropNLL methods.


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-02-13 15:52:54 UTC (rev 8502)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2008-02-13 16:04:35 UTC (rev 8503)
@@ -304,6 +304,8 @@
             */
         }
     }
+
+    applyBiasDecay();
 }
 
 void RBMGaussianLayer::reset()
@@ -744,6 +746,7 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
+    addBiasDecay(bias_gradient);
 }
 
 void RBMGaussianLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -759,6 +762,7 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
+    addBiasDecay(bias_gradients);
 }
 
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-02-13 15:52:54 UTC (rev 8502)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2008-02-13 16:04:35 UTC (rev 8503)
@@ -200,6 +200,7 @@
             b[i] += binc[i];
         }
     }
+    applyBiasDecay();
 }
 
 void RBMMultinomialLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
@@ -267,6 +268,7 @@
             }
         }
     }
+    applyBiasDecay();
 }
 
 //! TODO: add "accumulate" here
@@ -293,6 +295,7 @@
         ing[i] = (outg[i] - outg_dot_out) * out[i];
 
     rbm_bias_gradient << input_gradient;
+    addBiasDecay(rbm_bias_gradient);
 }
 
 //////////////
@@ -382,6 +385,7 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
+    addBiasDecay(bias_gradient);
 }
 
 void RBMMultinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -397,6 +401,7 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
+    addBiasDecay(bias_gradients);
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2008-02-13 15:52:54 UTC (rev 8502)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2008-02-13 16:04:35 UTC (rev 8503)
@@ -198,6 +198,8 @@
             bias[i] += bias_inc[i];
         }
     }
+
+    applyBiasDecay();
 }
 
 



From larocheh at mail.berlios.de  Wed Feb 13 17:05:11 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 17:05:11 +0100
Subject: [Plearn-commits] r8504 - trunk/plearn_learners/online
Message-ID: <200802131605.m1DG5BEs015551@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 17:05:10 +0100 (Wed, 13 Feb 2008)
New Revision: 8504

Modified:
   trunk/plearn_learners/online/RBMClassificationModule.cc
Log:
Changed the order of the deepcopies...


Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2008-02-13 16:04:35 UTC (rev 8503)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2008-02-13 16:05:10 UTC (rev 8504)
@@ -181,6 +181,7 @@
     deepCopyField(joint_connection, copies);
     deepCopyField(out_act, copies);
     deepCopyField(d_target_act, copies);
+    deepCopyField(d_last_act, copies);
 }
 
 //! given the input, compute the output (possibly resize it  appropriately)



From larocheh at mail.berlios.de  Wed Feb 13 17:06:08 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 17:06:08 +0100
Subject: [Plearn-commits] r8505 - trunk/plearn_learners/online
Message-ID: <200802131606.m1DG68sR015582@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 17:06:08 +0100 (Wed, 13 Feb 2008)
New Revision: 8505

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
Added addBiasDecay() to bpropUpdate and bpropNLL methods...


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-02-13 16:05:10 UTC (rev 8504)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-02-13 16:06:08 UTC (rev 8505)
@@ -238,6 +238,8 @@
             bias[i] += bias_inc[i];
         }
     }
+
+    applyBiasDecay();
 }
 
 void RBMBinomialLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
@@ -298,6 +300,8 @@
             }
         }
     }
+
+    applyBiasDecay();
 }
 
 
@@ -321,6 +325,7 @@
     }
 
     rbm_bias_gradient << input_gradient;
+    addBiasDecay(rbm_bias_gradient);
 }
 
 real RBMBinomialLayer::fpropNLL(const Vec& target)
@@ -412,6 +417,7 @@
 
     // bias_gradient = expectation - target
     substract(expectation, target, bias_gradient);
+    addBiasDecay(bias_gradient);
 }
 
 void RBMBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -427,6 +433,8 @@
 
     // bias_gradients = expectations - targets
     substract(expectations, targets, bias_gradients);
+
+    addBiasDecay(bias_gradients);
 }
 
 void RBMBinomialLayer::declareOptions(OptionList& ol)



From larocheh at mail.berlios.de  Wed Feb 13 17:18:32 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 13 Feb 2008 17:18:32 +0100
Subject: [Plearn-commits] r8506 - trunk/plearn_learners_experimental
Message-ID: <200802131618.m1DGIWcc017182@sheep.berlios.de>

Author: larocheh
Date: 2008-02-13 17:18:31 +0100 (Wed, 13 Feb 2008)
New Revision: 8506

Modified:
   trunk/plearn_learners_experimental/ManifoldKNNDistribution.cc
   trunk/plearn_learners_experimental/ManifoldKNNDistribution.h
Log:
Debugging...


Modified: trunk/plearn_learners_experimental/ManifoldKNNDistribution.cc
===================================================================
--- trunk/plearn_learners_experimental/ManifoldKNNDistribution.cc	2008-02-13 16:06:08 UTC (rev 8505)
+++ trunk/plearn_learners_experimental/ManifoldKNNDistribution.cc	2008-02-13 16:18:31 UTC (rev 8506)
@@ -56,7 +56,8 @@
 ManifoldKNNDistribution::ManifoldKNNDistribution()
     : manifold_dimensionality(5),
       min_sigma_square(1e-5),
-      center_around_manifold_neighbors(false)
+      center_around_manifold_neighbors(false),
+      use_gaussian_distribution(false)
 {}
 
 ////////////////////
@@ -95,6 +96,20 @@
                   "not around the test point."
                   );
 
+    declareOption(ol, "use_gaussian_distribution", 
+                  &ManifoldKNNDistribution::use_gaussian_distribution,
+                  OptionBase::buildoption,
+                  "Indication that a Gaussian distribution should be used as the\n"
+                  "knn_manifold nearest neighbors distribution, instead of the\n"
+                  "uniform in the ellipsoid."
+                  );
+
+    declareOption(ol, "density_learner", 
+                  &ManifoldKNNDistribution::density_learner,
+                  OptionBase::buildoption,
+                  "Generic density learner for knn_manifold nearest neighbors."
+                  );
+
     // Now call the parent class' declareOptions().
     inherited::declareOptions(ol);
 }
@@ -145,7 +160,18 @@
     {
         knn_manifold->setTrainingSet(train_set,true);
         knn_density->setTrainingSet(train_set,true);
+        
+        knn_manifold->train();
+        knn_density->train();
     }
+
+    if(use_gaussian_distribution && !center_around_manifold_neighbors)
+    {
+        PLWARNING("In ManifoldKNNDistribution::build_(): when using "
+                  "use_gaussian_distribution=true, center_around_manifold_neighbors"
+                  "must be true too. Setting center_around_manifold_neighbors=true...");
+        center_around_manifold_neighbors = true;
+    }
 }
 
 /////////
@@ -184,38 +210,79 @@
 /////////////////
 real ManifoldKNNDistribution::log_density(const Vec& y) const
 {
-    computeLocalPrincipalComponents(y,eig_values,eig_vectors);
 
-    // Find volume of ellipsoid defined by eig_values, eig_vectors and
-    // min_sigma_square that covers all the nearest_neighbors found by knn_density
-    knn_density->computeOutput(y,nearest_neighbors_density_vec);
-    nearest_neighbors_density = 
-        nearest_neighbors_density_vec.toMat(knn_density->num_neighbors,inputsize_);
-    nearest_neighbors_density -= y;
-    real max = -1;
-    real scaled_projection;
-    for(int i=0; i<nearest_neighbors_density.length(); i++)
+    real ret = 0;
+    if(density_learner)
     {
-        scaled_projection = 0;
-        product(eig_vectors_projection,eig_vectors,nearest_neighbors_density(i));
+        knn_manifold->computeOutput(y,nearest_neighbors_manifold_vec);
+        nearest_neighbors_manifold = 
+            nearest_neighbors_manifold_vec.toMat(
+                knn_manifold->num_neighbors,inputsize_);
+        density_learner_train_set = VMat( nearest_neighbors_manifold );
+        density_learner_train_set->defineSizes(inputsize_,0);
+        density_learner->setTrainingSet(density_learner_train_set,true);
+        density_learner->train();
+        density_learner->log_density(y);
+        ret = density_learner->log_density(y) + 
+            pl_log((real)knn_manifold->num_neighbors)-pl_log((real)n_examples);
+    }
+    else if(use_gaussian_distribution)
+    {
+        computeLocalPrincipalComponents(y,eig_values,eig_vectors);
+        if(!center_around_manifold_neighbors)
+            columnMean(nearest_neighbors_manifold,neighbors_mean);
+
+        // Compute log-normalization constant
+        ret = - inputsize_ *0.5 *Log2Pi;
+        for(int i=0; i<manifold_dimensionality; i++)
+            ret -= 0.5 * pl_log(eig_values[i]+min_sigma_square);
+        ret -= (inputsize_-manifold_dimensionality)*0.5*pl_log(min_sigma_square);
+        substract(y,neighbors_mean,test_minus_mean);
+        product(eig_vectors_projection,eig_vectors,test_minus_mean);
         for(int j=0; j<eig_values.length(); j++)
-            scaled_projection += mypow(eig_vectors_projection[j],2) * 
+            ret -= mypow(eig_vectors_projection[j],2) * 
                 (1/(eig_values[j]+min_sigma_square) 
                  - 1/min_sigma_square) ;
-        scaled_projection += pownorm(nearest_neighbors_density(i),2)
+        ret -= pownorm(test_minus_mean,2)
             /min_sigma_square;
-        if(max < scaled_projection)
-            max = scaled_projection;
+        ret += pl_log((real)knn_manifold->num_neighbors)-pl_log((real)n_examples);
     }
+    else
+    {
+        computeLocalPrincipalComponents(y,eig_values,eig_vectors);
 
-    // Compute log-volume of the ellipsoid: pi
-    real log_vol = 0.5 * inputsize_ * pl_log(scaled_projection);
-    for(int i=0; i<manifold_dimensionality; i++)
-        log_vol += 0.5 * pl_log(eig_values[i]+min_sigma_square);
-    log_vol += (inputsize_-manifold_dimensionality)*0.5*pl_log(min_sigma_square);
-    log_vol += 0.5*inputsize_*pl_log(Pi) - pl_gammln(0.5*inputsize_+1);
-    
-    return pl_log((real)knn_density->num_neighbors)-pl_log((real)n_examples)-log_vol;
+        // Find volume of ellipsoid defined by eig_values, eig_vectors and
+        // min_sigma_square that covers all the nearest_neighbors found by knn_density
+        knn_density->computeOutput(y,nearest_neighbors_density_vec);
+        nearest_neighbors_density = 
+            nearest_neighbors_density_vec.toMat(knn_density->num_neighbors,inputsize_);
+        nearest_neighbors_density -= y;
+        real max = -1;
+        real scaled_projection=0;
+        for(int i=0; i<nearest_neighbors_density.length(); i++)
+        {
+            scaled_projection = 0;
+            product(eig_vectors_projection,eig_vectors,nearest_neighbors_density(i));
+            for(int j=0; j<eig_values.length(); j++)
+                scaled_projection += mypow(eig_vectors_projection[j],2) * 
+                    (1/(eig_values[j]+min_sigma_square) 
+                     - 1/min_sigma_square) ;
+            scaled_projection += pownorm(nearest_neighbors_density(i),2)
+                /min_sigma_square;
+            if(max < scaled_projection)
+                max = scaled_projection;
+        }
+        
+        // Compute log-volume of the ellipsoid: pi
+        real log_vol = 0.5 * inputsize_ * pl_log(scaled_projection);
+        for(int i=0; i<manifold_dimensionality; i++)
+            log_vol += 0.5 * pl_log(eig_values[i]+min_sigma_square);
+        log_vol += (inputsize_-manifold_dimensionality)*0.5*pl_log(min_sigma_square);
+        log_vol += 0.5*inputsize_*pl_log(Pi) - pl_gammln(0.5*inputsize_+1);
+        
+        ret = pl_log((real)knn_density->num_neighbors)-pl_log((real)n_examples)-log_vol;
+    }
+    return ret;
 }
 
 /////////////////////////////////
@@ -227,6 +294,7 @@
 
     deepCopyField(knn_manifold, copies);
     deepCopyField(knn_density, copies);
+    deepCopyField(density_learner, copies);
     deepCopyField(nearest_neighbors_manifold, copies);
     deepCopyField(nearest_neighbors_manifold_vec, copies);
     deepCopyField(nearest_neighbors_density, copies);
@@ -238,6 +306,8 @@
     deepCopyField(S, copies);
     deepCopyField(eig_vectors_projection, copies);
     deepCopyField(neighbors_mean,copies);
+    deepCopyField(test_minus_mean,copies);
+    deepCopyField(density_learner_train_set,copies);
 }
 
 ////////////////////

Modified: trunk/plearn_learners_experimental/ManifoldKNNDistribution.h
===================================================================
--- trunk/plearn_learners_experimental/ManifoldKNNDistribution.h	2008-02-13 16:06:08 UTC (rev 8505)
+++ trunk/plearn_learners_experimental/ManifoldKNNDistribution.h	2008-02-13 16:18:31 UTC (rev 8506)
@@ -78,6 +78,14 @@
     //! not around the test point.
     bool center_around_manifold_neighbors;
 
+    //! Indication that a Gaussian distribution should be used as the
+    //! knn_manifold nearest neighbors distribution, instead of the
+    //! uniform in the ellipsoid.
+    bool use_gaussian_distribution;
+
+    //! Generic density learner for knn_density nearest neighbors
+    PP<PDistribution> density_learner;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -158,7 +166,12 @@
     mutable Vec eig_vectors_projection;
     //! Mean vector of neighbors
     mutable Vec neighbors_mean;
+    //! Difference between test point and neighbors_mean;
+    mutable Vec test_minus_mean;
 
+    //! The density learner training set
+    mutable VMat density_learner_train_set;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From chrish at mail.berlios.de  Wed Feb 13 21:07:04 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 13 Feb 2008 21:07:04 +0100
Subject: [Plearn-commits] r8507 - trunk/python_modules/plearn/utilities
Message-ID: <200802132007.m1DK74uw032050@sheep.berlios.de>

Author: chrish
Date: 2008-02-13 21:07:03 +0100 (Wed, 13 Feb 2008)
New Revision: 8507

Modified:
   trunk/python_modules/plearn/utilities/toolkit.py
Log:
Direct people that want to use plearn.utilities.toolkit.command_output to the new subprocess module in the python standard library instead.

Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2008-02-13 16:18:31 UTC (rev 8506)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2008-02-13 20:07:03 UTC (rev 8507)
@@ -52,14 +52,8 @@
 def command_output(command, stderr = True, stdout = True):
     """Returns the output lines of a shell command.    
     
-    The U{commands<http://docs.python.org/lib/module-commands.html>} module
-    provides a similar function, getoutput(), that returns a single
-    string. Since the
-    U{commands<http://docs.python.org/lib/module-commands.html>} module
-    relies on U{popen<http://docs.python.org/lib/module-popen2.html>}
-    objects, it is more efficient to use directly
-    U{popen<http://docs.python.org/lib/module-popen2.html>} here than
-    splitting the results of the commands.getoutput() in lines.
+    Deprecated: please consider using the new python 2.4 subprocess module
+    instead.
 
     @param command: The shell command to execute.
     @type command: String



From nouiz at mail.berlios.de  Wed Feb 13 21:41:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 13 Feb 2008 21:41:46 +0100
Subject: [Plearn-commits] r8508 - trunk/plearn/misc
Message-ID: <200802132041.m1DKfk6r003530@sheep.berlios.de>

Author: nouiz
Date: 2008-02-13 21:41:46 +0100 (Wed, 13 Feb 2008)
New Revision: 8508

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
Removed a warning generated by gcc 4.2.3.


Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2008-02-13 20:07:03 UTC (rev 8507)
+++ trunk/plearn/misc/viewVMat.cc	2008-02-13 20:41:46 UTC (rev 8508)
@@ -186,7 +186,7 @@
     int valwidth = 15;
     int valstrwidth = valwidth-1;
 
-    char* valstrformat = "%14s";
+    const char* valstrformat = "%14s";
 
     int curi = 0;
     int curj = 0;



From laulysta at mail.berlios.de  Thu Feb 14 06:19:34 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 14 Feb 2008 06:19:34 +0100
Subject: [Plearn-commits] r8509 - trunk/plearn_learners_experimental
Message-ID: <200802140519.m1E5JYJN011660@sheep.berlios.de>

Author: laulysta
Date: 2008-02-14 06:19:32 +0100 (Thu, 14 Feb 2008)
New Revision: 8509

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-02-13 20:41:46 UTC (rev 8508)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-02-14 05:19:32 UTC (rev 8509)
@@ -41,6 +41,7 @@
 #include <plearn/io/pl_log.h>
 
 #include "DynamicallyLinkedRBMsModel.h"
+#include "plearn/math/plapack.h"
 
 namespace PLearn {
 using namespace std;
@@ -58,10 +59,10 @@
     fine_tuning_learning_rate( 0.01 ),
     recurrent_net_learning_rate( 0.01),
     untie_weights( false ),
-    rbm_nstages( 1 ),
-    dynamic_nstages( 1 ),
-    fine_tuning_nstages( -1 ),
-    recurrent_nstages(-1),
+    rbm_nstages( 0 ),
+    dynamic_nstages( 0 ),
+    fine_tuning_nstages( 0 ),
+    recurrent_nstages(0),
     visible_connections_option(0),
     visible_size( -1 )
 {
@@ -143,6 +144,10 @@
                   "Option for the onlineLearningModule corresponding to dynamic links "
                   "between RBMs' visible layers");
 
+    declareOption(ol, "dynamic_connections_copy", &DynamicallyLinkedRBMsModel::dynamic_connections_copy,
+                  OptionBase::learntoption,
+                  "Independent copy of dynamic connections");
+
     /*
     declareOption(ol, "", &DynamicallyLinkedRBMsModel::,
                   OptionBase::learntoption,
@@ -237,7 +242,7 @@
         dynamic_connections->build();
         visible_connections->build();
         connections_transpose = new RBMMatrixTransposeConnection(connections);
-
+        connections_idem_t = connections_transpose;
     }
 
 }
@@ -261,6 +266,7 @@
     deepCopyField( visible_connections , copies);
     deepCopyField( connections , copies);
     deepCopyField( connections_idem , copies);
+    deepCopyField( connections_idem_t , copies);
     deepCopyField( connections_transpose, copies);
     deepCopyField( connections_transpose_copy, copies);
     deepCopyField( symbol_sizes , copies);
@@ -271,6 +277,7 @@
     deepCopyField( hidden_layer_target , copies);
     deepCopyField( input_gradient , copies);
     deepCopyField( hidden_gradient , copies);
+    deepCopyField( hidden_gradient2 , copies);
     deepCopyField( hidden_temporal_gradient , copies);
     deepCopyField( previous_input , copies);
     deepCopyField( previous_hidden_layer , copies);
@@ -291,6 +298,7 @@
     deepCopyField( input_prediction_activations_list , copies);
     deepCopyField( input_list , copies);
     deepCopyField( nll_list , copies);
+    deepCopyField( input_expectation , copies);
 
     // deepCopyField(, copies);
 
@@ -615,7 +623,9 @@
     }
 
     //cout << dynamic_connections->weights - dynamic_connections_copy->weights<< endl;    
+    
 
+
  /***** Recurrent phase *****/
     if( stage >= nstages )
         return;
@@ -656,11 +666,25 @@
             if(untie_weights && 
                stage == rbm_nstages + dynamic_nstages + fine_tuning_nstages)
             {
-                CopiesMap map;
-                dynamic_connections_copy = dynamic_connections->deepCopy(map);
-                CopiesMap map2;
-                connections_transpose_copy = connections_transpose->deepCopy(map2);
-                connections_transpose = connections_transpose_copy;                
+                
+                //CopiesMap map;
+                //dynamic_connections_copy = dynamic_connections->deepCopy(map);
+
+                //CopiesMap map2;
+                //connections_transpose_copy = connections_transpose->deepCopy(map2);
+                //connections_transpose = connections_transpose_copy;  
+/*
+                TMat<real> U,V;//////////*********************crap James
+                TVec<real> S;
+                U.resize(hidden_layer->size,hidden_layer->size);
+                V.resize(hidden_layer->size,hidden_layer->size);
+                S.resize(hidden_layer->size);
+                U << dynamic_connections->weights;
+                
+                SVD(U,dynamic_connections->weights,S,V);
+                S.fill(-0.5);
+                productScaleAcc(dynamic_connections->bias,dynamic_connections->weights,S,1,0);
+*/
             }
             
             for(int sample=0 ; sample<train_set->length() ; sample++ )
@@ -668,6 +692,19 @@
 
                 train_set->getExample(sample, input, target, weight);
                 
+
+                hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
+                hidden_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
+                hidden2_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
+                hidden2_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
+                input_prediction_list.resize(
+                    ith_sample_in_sequence+1,visible_layer->size);
+                input_prediction_activations_list.resize(
+                    ith_sample_in_sequence+1,visible_layer->size);
+                input_list.resize(ith_sample_in_sequence+1,visible_layer->size);
+                nll_list.resize(ith_sample_in_sequence+1);
+
+
                 if(train_set->getString(sample,0) == "<oov>")
                 {/*
                     nb_oov++;
@@ -690,6 +727,24 @@
                     nll_list.clear();
                     continue;
                  */
+                    input_list(ith_sample_in_sequence) << previous_input;
+                    connections->setAsDownInput( previous_input );
+                    hidden_layer->getAllActivations( connections_idem );
+                    hidden_layer->computeExpectation();
+                    previous_hidden_layer << hidden_layer->expectation;
+                    previous_hidden_layer_activation << hidden_layer->activation;
+                    hidden_list(ith_sample_in_sequence) << previous_hidden_layer;
+                    hidden_activations_list(ith_sample_in_sequence) 
+                        << previous_hidden_layer_activation;
+                    hidden2_list(ith_sample_in_sequence) << hidden_layer->expectation;
+                    hidden2_activations_list(ith_sample_in_sequence) << 
+                        hidden_layer->activation;
+                    input_prediction_list(ith_sample_in_sequence) << 
+                        visible_layer->expectation;
+                    input_prediction_activations_list(ith_sample_in_sequence) << 
+                        visible_layer->activation;
+                    //cout << "hidden_expectation crap james :" <<  hidden_list << endl;
+                    //update
                     nb_oov++;
                     recurrent_update();
                     
@@ -701,7 +756,7 @@
                     nll_list.clear();
                     continue;
                 }
-
+/*
                 hidden_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
                 hidden_activations_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
                 hidden2_list.resize(ith_sample_in_sequence+1,hidden_layer->size);
@@ -712,7 +767,7 @@
                     ith_sample_in_sequence+1,visible_layer->size);
                 input_list.resize(ith_sample_in_sequence+1,visible_layer->size);
                 nll_list.resize(ith_sample_in_sequence+1);
-
+*/
          
                 clamp_visible_units(input);
 
@@ -741,7 +796,7 @@
                     previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
                     previous_hidden_layer_activation << hidden_layer->activation;
             
-
+                    
                     //h*_{t}
                     ////////////
                     if(dynamic_connections_copy)
@@ -762,6 +817,7 @@
                     input_list(ith_sample_in_sequence).fill(-1);
  
                     previous_hidden_layer.clear();//h_{t-1}
+                    //previous_hidden_layer.fill(0.5);//**************************crap James
                     previous_hidden_layer_activation.clear();//h_{t-1}
 
                     if(dynamic_connections_copy)
@@ -795,10 +851,13 @@
                 }
 
                 // cout << "trililililililili" << endl;
-                connections->setAsUpInput( hidden_layer->expectation );
 
-                visible_layer->getAllActivations( connections_idem );
+                connections_transpose->setAsDownInput( hidden_layer->expectation );
+                visible_layer->getAllActivations( connections_idem_t );
 
+                //connections->setAsUpInput( hidden_layer->expectation );
+                //visible_layer->getAllActivations( connections_idem );
+
                 visible_layer->computeExpectation();
  
                 // Copies for backprop
@@ -851,6 +910,7 @@
 {
     int it = 0;
     int ss = -1;
+    input_expectation.resize(visible_layer->size);
     for(int i=0; i<inputsize_; i++)
     {
         ss = symbol_sizes[i];
@@ -858,16 +918,17 @@
         if(ss < 0) 
         {
             //cout << "yoyoyoyoyo" << endl;
-            visible_layer->expectation[it++] = input[i];
+            input_expectation[it++] = input[i];
         }
         else // ... or a symbol
         {
             // Convert to one-hot vector
-            visible_layer->expectation.subVec(it,ss).clear();
-            visible_layer->expectation[it+(int)input[i]] = 1;
+            input_expectation.subVec(it,ss).clear();
+            input_expectation[it+(int)input[i]] = 1;
             it += ss;
         }
     }
+    visible_layer->setExpectation(input_expectation);
 }
 
 real DynamicallyLinkedRBMsModel::rbm_update()
@@ -976,7 +1037,7 @@
     
     
     //////////////////// VISIBLE DYNAMIC CONNECTION
-    if (visible_connections_option){
+    /*  if (visible_connections_option){
         visible_layer_sample << visible_layer->expectation;
         
         // Use "previous_visible_layer" field and "visible_connections" module 
@@ -996,7 +1057,7 @@
         visible_connections->bpropUpdate(previous_visible_layer,
                                          visible_layer->activation,
                                          input_gradient, visi_bias_gradient);
-    }
+                                         }*/
 
     return nll;
 }
@@ -1161,50 +1222,86 @@
 {
     // Notes: 
     //    - not all lists are useful (some *_activations_* are not)
-
-    hidden_temporal_gradient.clear();
-    for(int i=hidden_list.length()-1; i>=0; i--){        
-        visible_layer->expectation << input_prediction_list(i);        
-        visible_layer->bpropNLL(input_list(i),nll_list[i],visi_bias_gradient);
-        visible_layer->update(visi_bias_gradient);
-        connections_transpose->bpropUpdate(
-            hidden2_list(i),input_prediction_activations_list(i),
-            hidden_gradient, visi_bias_gradient);
-        hidden_layer->bpropUpdate(
-            hidden2_activations_list(i), hidden2_list(i),
-            bias_gradient, hidden_gradient);
-        if(dynamic_connections_copy)
-            dynamic_connections_copy->bpropUpdate(
-                hidden_list(i),
-                hidden2_activations_list(i), 
-                hidden_gradient, bias_gradient);        
-        else
-            dynamic_connections->bpropUpdate(
-                hidden_list(i),
-                hidden2_activations_list(i), 
-                hidden_gradient, bias_gradient);
-        if(i!=0)
-        {
-            hidden_gradient += hidden_temporal_gradient;
+    int segment = hidden_list.length()/2;
+    int seg =0;
+    for(int k=hidden_list.length()-3; k>=-segment; k-=segment){ 
+        seg = k;
+        if(seg < 0)
+            seg = 0;
+        //cout << "segment: " << seg << endl;
+        hidden_temporal_gradient.clear();
+        //for(int i=hidden_list.length()-2; i>=0; i--){  
+        for(int i=hidden_list.length()-2; i>=seg; i--){     
+            
+            //visible_layer->expectation << input_prediction_list(i);
+            //visible_layer->activation << ?????;
+            visible_layer->setExpectation(input_prediction_list(i));
+            
+            visible_layer->bpropNLL(input_list(i+1),nll_list[i],visi_bias_gradient);
+            
+            visible_layer->update(visi_bias_gradient);
+            
+            
+            connections_transpose->bpropUpdate(
+                hidden2_list(i),input_prediction_activations_list(i),
+                hidden_gradient, visi_bias_gradient);
+            
+            
+            //hidden_layer->setExpectation(hidden_list(i+1));//////////////////////////////
+            //hidden_layer->bpropNLL(hidden2_list(i),nll_list[i], hidden_gradient2);////////////////////////////////
+            
+            
+            
             hidden_layer->bpropUpdate(
-                hidden_activations_list(i), hidden_list(i),
-                hidden_temporal_gradient, hidden_gradient);
+                hidden2_activations_list(i), hidden2_list(i),
+                bias_gradient, hidden_gradient);
             
-            dynamic_connections->bpropUpdate(
-                hidden_list(i-1),
-                hidden_activations_list(i), // Here, it should be cond_bias, but doesn't matter
-                hidden_gradient, hidden_temporal_gradient);
-            connections->bpropUpdate(
-                input_list(i),
-                hidden_activations_list(i), 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+            //hidden_layer->update(hidden_gradient2);/////////////////////////////////////
+            
+            
+            //bias_gradient += hidden_gradient2;///////////////////////////////////
+            
+            
+            
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->bpropUpdate(
+                    hidden_list(i),
+                    hidden2_activations_list(i), 
+                    hidden_gradient, bias_gradient);        
+            else
+                dynamic_connections->bpropUpdate(
+                    hidden_list(i),
+                    hidden2_activations_list(i), 
+                    hidden_gradient, bias_gradient);
+            
+            if(i!=0)
+            {
+                
+                hidden_gradient += hidden_temporal_gradient;
+                
+                hidden_layer->bpropUpdate(
+                    hidden_activations_list(i), hidden_list(i),
+                    hidden_temporal_gradient, hidden_gradient);
+                
+                dynamic_connections->bpropUpdate(
+                    hidden_list(i-1),
+                    hidden_activations_list(i), // Here, it should be cond_bias, but doesn't matter
+                    hidden_gradient, hidden_temporal_gradient);
+                
+                hidden_temporal_gradient << hidden_gradient;
+                
+                connections->bpropUpdate(
+                    input_list(i),
+                    hidden_activations_list(i), 
+                    visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+            }
+            else
+            {
+                // Could learn initial value for h_{-1}
+            }
         }
-        else
-        {
-            // Could learn initial value for h_{-1}
-        }
     }
-
 }
 
 
@@ -1285,12 +1382,12 @@
             dynamic_connections->fprop(previous_hidden_layer, cond_bias);
             hidden_layer->setAllBias(cond_bias); //**************************
 
-            if (visible_connections_option){
+            /* if (visible_connections_option){
                 //v*_{t-1} VISIBLE DYNAMIC CONNECTION
                 //////////////////////////////////
                 visible_connections->fprop(previous_input, visi_cond_bias);
                 visible_layer->setAllBias(visi_cond_bias); //**************************
-            }
+                }*/
 
             //up phase
             connections->setAsDownInput( previous_input );
@@ -1336,7 +1433,7 @@
             bias_tempo << hidden_layer->bias;
 
 
-            if (visible_connections_option){
+            /*  if (visible_connections_option){
 
                 /////////VISIBLE DYNAMIC CONNECTION
                 previous_visible_layer.clear();//v_{t-1}
@@ -1347,7 +1444,7 @@
                 
                 visi_bias_tempo.resize(visible_layer->bias.length());
                 visi_bias_tempo << visible_layer->bias;
-            }
+                }*/
             
             begin++;
         }
@@ -1361,13 +1458,14 @@
 
 
      
+        connections_transpose->setAsDownInput( hidden_layer->expectation );
+        visible_layer->getAllActivations( connections_idem_t );
 
+        //connections->setAsUpInput( hidden_layer->expectation );
+        //visible_layer->getAllActivations( connections_idem );
 
 
-        connections->setAsUpInput( hidden_layer->expectation );
-
-        visible_layer->getAllActivations( connections_idem );
-
+       
         visible_layer->computeExpectation();
 
        
@@ -1378,9 +1476,9 @@
         hidden_layer->setAllBias(bias_tempo); 
 
         /////////VISIBLE DYNAMIC CONNECTION
-        if (visible_connections_option){
+        /* if (visible_connections_option){
             visible_layer->setAllBias(visi_bias_tempo); 
-        }
+            }*/
 
         // costs[0] = 0; //nll/nb_de_temps_par_mesure
 
@@ -1416,6 +1514,126 @@
     return getTestCostNames();
 }
 
+void DynamicallyLinkedRBMsModel::generate(int nbNotes)
+{
+    
+    previous_hidden_layer.resize(hidden_layer->size);
+    connections_idem = connections;
+
+    for (int ith_sample = 0; ith_sample < nbNotes ; ith_sample++ ){
+        
+        input_prediction_list.resize(
+            ith_sample+1,visible_layer->size);
+        if(ith_sample > 0)
+        {
+            
+            //input_list(ith_sample_in_sequence) << previous_input;
+            //h*_{t-1}
+            //////////////////////////////////
+            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+            hidden_layer->setAllBias(cond_bias); //**************************
+            
+            
+            
+            //up phase
+            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+            hidden_layer->getAllActivations( connections_idem );
+            hidden_layer->computeExpectation();
+            //////////////////////////////////
+            
+            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
+            //previous_hidden_layer_activation << hidden_layer->activation;
+            
+            
+            //h*_{t}
+            ////////////
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            ///////////
+            
+            //previous_input << visible_layer->expectation;//v_{t-1}
+            
+        }
+        else
+        {
+            
+            previous_hidden_layer.clear();//h_{t-1}
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( previous_hidden_layer ,
+                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop(previous_hidden_layer,
+                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            
+            
+        }
+        
+        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+        //visible_layer->getAllActivations( connections_idem_t );
+        
+        connections->setAsUpInput( hidden_layer->expectation );
+        visible_layer->getAllActivations( connections_idem );
+        
+        visible_layer->computeExpectation();
+        visible_layer->generateSample();
+        
+        input_prediction_list(ith_sample) << visible_layer->sample;
+        
+    }
+    
+    //Vec tempo;
+    TVec<int> tempo;
+    tempo.resize(visible_layer->size);
+    int theNote;
+    int nbNoteVisiLayer = input_prediction_list(1).length()/13;
+    ofstream myfile;
+    int theLayer;
+    myfile.open ("/u/laulysta/recherche_maitrise/projet_GenerationDeMusique/data/generate/test.txt");
+    
+    for (int i = 0; i < nbNotes ; i++ ){
+        tempo << input_prediction_list(i);
+        
+        //cout << tempo[2] << endl;
+       
+        for (int j = 0; j < tempo.length() ; j++ ){
+            
+            if (tempo[j] == 1){
+                theLayer = (j/13);
+                
+                theNote = j - (13*theLayer);
+               
+
+                if (theNote<=11){
+                    //print theNote
+                    //cout << theNote+50 << " ";
+                    myfile << theNote << " ";
+                }
+                else{
+                    //print #
+                    //cout << "# ";
+                    myfile << "# ";
+                    
+                }
+     
+            }
+           
+        }
+        myfile << "\n";
+    }
+     myfile << "<oov> <oov> \n";
+
+     myfile.close();
+
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-02-13 20:41:46 UTC (rev 8508)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-02-14 05:19:32 UTC (rev 8509)
@@ -121,6 +121,7 @@
     //! The weights of the connections between the RBM visible and hidden layers
     PP<RBMMatrixConnection> connections;
     PP<RBMConnection> connections_idem;
+    PP<RBMConnection> connections_idem_t;
 
     //! The weights of the connections between the RBM hidden and input layers.
     //! It is the transpose "connections".
@@ -172,6 +173,10 @@
     //! thus the test method).
     virtual TVec<std::string> getTestCostNames() const;
 
+
+    //! Generate music in a folder
+    void generate(int nbNotes);
+
     //! Returns the names of the objective costs that the train method computes
     //! and  for which it updates the VecStatsCollector train_stats.
     virtual TVec<std::string> getTrainCostNames() const;
@@ -259,6 +264,10 @@
     //! Stores hidden gradient of dynamic connections
     mutable Vec hidden_gradient;
     
+    //! Stores hidden gradient of dynamic connections
+    mutable Vec hidden_gradient2;
+
+    
     //! Stores hidden gradient of dynamic connections coming from time t+1
     mutable Vec hidden_temporal_gradient;
     
@@ -309,7 +318,9 @@
     //! List of the nll of the input samples in a sequence
     Vec nll_list;
 
-    
+    //! Temporary variable to clamp visible units (i.e. set the expectation
+    //! field of visible_layer)
+    mutable Vec input_expectation;
 
 protected:
     //#####  Protected Member Functions  ######################################



From tihocan at mail.berlios.de  Thu Feb 14 17:32:05 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 14 Feb 2008 17:32:05 +0100
Subject: [Plearn-commits] r8510 - in trunk/plearn/math: .
	test/pl_math/.pytest/pl_math/expected_results
Message-ID: <200802141632.m1EGW56P027468@sheep.berlios.de>

Author: tihocan
Date: 2008-02-14 17:32:04 +0100 (Thu, 14 Feb 2008)
New Revision: 8510

Modified:
   trunk/plearn/math/pl_math.cc
   trunk/plearn/math/test/pl_math/.pytest/pl_math/expected_results/PLMathTest.psave
Log:
Fixed logadd problems with INFINITY

Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2008-02-14 05:19:32 UTC (rev 8509)
+++ trunk/plearn/math/pl_math.cc	2008-02-14 16:32:04 UTC (rev 8510)
@@ -162,6 +162,11 @@
         double tmp = log_a;
         log_a = log_b;
         log_b = tmp;
+    } else if (fast_exact_is_equal(log_a, log_b)) {
+        // Special case when log_a == log_b. In particular this works when both
+        // log_a and log_b are (+-) INFINITY: it will return (+-) INFINITY
+        // instead of NaN.
+        return LOG_2 + log_a;
     }
     double negative_absolute_difference = log_b - log_a;
     if (negative_absolute_difference < MINUS_LOG_THRESHOLD)

Modified: trunk/plearn/math/test/pl_math/.pytest/pl_math/expected_results/PLMathTest.psave
===================================================================
--- trunk/plearn/math/test/pl_math/.pytest/pl_math/expected_results/PLMathTest.psave	2008-02-14 05:19:32 UTC (rev 8509)
+++ trunk/plearn/math/test/pl_math/.pytest/pl_math/expected_results/PLMathTest.psave	2008-02-14 16:32:04 UTC (rev 8510)
@@ -1,4 +1,4 @@
 *1 ->PLMathTest(
-results = {"DOUBLE_TO_INT" : 86 [ 0 0 1 0 0 1 1 -1 0 1 0 0 1 0 -1 0 -1 -1 0 1 -10 -8 8 9 3 -8 -2 4 -8 8 7 9 -2 1 10 -10 -2 -4 0 -7 -29 35 -2 3 -7 25 -7 -41 -30 29 28 -22 4 -15 -43 15 -46 -40 -7 -1 -441 -502 -214 -35 -686 660 221 -869 -830 -782 894 -275 391 814 -964 -260 77 -323 155 -60 0 1 -1 -2147483648 -2147483648 -2147483648 ] , "FABS" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 0.0229738191701471806 0.281510354019701481 0.547011177986860275 0.638541524298489094 0.522267847321927547 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 0.966943142935633659 0.412394006736576557 0.932065241038799286 0.547248289920389652 0.204638117458671331 0.8511194814927876 9.8411181615665555 7.68696950748562813 7.95039555057883263 8.72947671916335821 3.1281573697924614 7.90857966523617506 2.06657044589519501 3.92067671287804842 8.342577307485044 7.84265012014657259 6.87006874475628138 8.87!
 609967496246099 1.98038394097238779 1.43916808068752289 9.70257001463323832 9.55282651819288731 1.54959974810481071 4.28370018023997545 0.311001515947282314 6.62803614977747202 29.2189291212707758 35.1463320199400187 2.19451428856700659 3.34315712098032236 6.5985801862552762 24.6784927789121866 7.40931213367730379 41.4044293342158198 30.3642636863514781 29.3665965553373098 28.1087801558896899 22.2120927879586816 3.71552517171949148 15.1306831743568182 43.4476080350577831 15.3994822409003973 46.1658099899068475 39.8597451858222485 7.17367979232221842 0.566293741576373577 441.263623535633087 501.513385679572821 213.578812312334776 35.0282588042318821 686.374915298074484 659.782795235514641 220.642273314297199 869.162584189325571 830.341589637100697 782.309267669916153 893.529789987951517 274.512093048542738 391.405788715928793 813.523761462420225 963.540622033178806 259.774779435247183 77.3236751556396484 323.050507809966803 154.615687672048807 59.5321925356984138 0 1 1 nan i!
 nf inf ] , "d_hinge_loss_1" : 86 [ -1 -1 -1 -1 -1 -1 -1 -1 -1 !
 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 -1 -1 0 -1 0 0 0 -1 0 0 -1 -1 -1 -1 -1 -1 0 -1 0 -1 0 -1 -1 -1 0 0 -1 0 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 -1 -1 -1 0 -1 0 0 -1 -1 0 -1 0 -1 -1 0 -1 0 0 -1 ] , "d_soft_slope" : 86 [ 0.242285698073777811 0.244037175721172089 0.24142634460912743 0.22980825628581214 0.242190808539238855 0.24479150158679186 0.243817139803235633 0.193194278139123626 0.236124157782237692 0.244771407051106199 0.231694215146674098 0.243770194618311398 0.244822030971784232 0.233689426402807737 0.152772615215307883 0.202481489173009677 0.155983744438359717 0.191019006099864802 0.218367110090888522 0.237953458248121696 3.3637598930311098e-05 0.000289813753533217796 0.000604940196398962623 0.000277759986236062796 0.0644294275677806255 0.000232235751034840995 0.0678812607067606349 0.0316986737378326874 0.000150495377013869832 0.000673660341962691858 0.00177740568165485292 0.000239898620697587484 0.0729579454382622145 0.200265007724166055 0.000105008167115428126 4.!
 48763880318066733e-05 0.102690705248289535 0.00855590646865983691 0.21055099650659162 0.000834631976874788376 1.29190603527506997e-13 9.35910420343799565e-16 0.0608715080726522778 0.0534912609033074832 0.000859536091819745429 3.29125219374233613e-11 0.000382487172532957958 1.02999206386122921e-18 4.11383709215951132e-14 3.02930884587063187e-13 1.06562962664050187e-12 1.42629916015900304e-10 0.0382992724382222455 1.69678785018432133e-07 1.08420217248550443e-19 3.52520116187084306e-07 0 4.87890977618476995e-18 0.000484010924694119556 0.189347073486123174 0 0 0 3.90909093289648624e-16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.231058578630004896 0.231058578630004868 0.149738499347877557 nan 0 0 ] , "data" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 -0.0229738191701471806 0.281510354019701481 0.547011177986860275 0.638541524298489094 -0.522267847321927547 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.!
 0515793473459780216 -0.966943142935633659 -0.41239400673657655!
 7 -0.932
065241038799286 -0.547248289920389652 -0.204638117458671331 0.8511194814927876 -9.8411181615665555 -7.68696950748562813 7.95039555057883263 8.72947671916335821 3.1281573697924614 -7.90857966523617506 -2.06657044589519501 3.92067671287804842 -8.342577307485044 7.84265012014657259 6.87006874475628138 8.87609967496246099 -1.98038394097238779 1.43916808068752289 9.70257001463323832 -9.55282651819288731 -1.54959974810481071 -4.28370018023997545 -0.311001515947282314 -6.62803614977747202 -29.2189291212707758 35.1463320199400187 -2.19451428856700659 3.34315712098032236 -6.5985801862552762 24.6784927789121866 -7.40931213367730379 -41.4044293342158198 -30.3642636863514781 29.3665965553373098 28.1087801558896899 -22.2120927879586816 3.71552517171949148 -15.1306831743568182 -43.4476080350577831 15.3994822409003973 -46.1658099899068475 -39.8597451858222485 -7.17367979232221842 -0.566293741576373577 -441.263623535633087 -501.513385679572821 -213.578812312334776 -35.0282588042318821 -686.!
 374915298074484 659.782795235514641 220.642273314297199 -869.162584189325571 -830.341589637100697 -782.309267669916153 893.529789987951517 -274.512093048542738 391.405788715928793 813.523761462420225 -963.540622033178806 -259.774779435247183 77.3236751556396484 -323.050507809966803 154.615687672048807 -59.5321925356984138 0 1 -1 nan inf -inf ] , "dilogarithm" : 86 [ 0.308821818354089395 0.41903704230712352 0.973798967287272776 -0.0228432001973329982 0.304281881734638049 0.648824602623850222 0.787699775851718575 -0.466400827029289367 0.107341308626911297 0.654015615338651601 0.0120659611819196235 0.397134765289189673 0.640117836209777247 0.0522601591501091617 -0.799447406247214221 -0.376233907889713892 -0.774925280735139221 -0.486411798592015221 -0.195024079455506927 1.18308297983425881 -4.15999032198697005 -3.59868099598513069 1.01077088538761317 0.824548197471537292 2.28988133524398574 -3.66048124080796899 -1.47307718868669069 2.08304670132001624 -3.77858968703090792 1.037!
 11900312929273 1.28163866538727045 0.79032778092548428 -1.4259!
 50302883
68396 2.34305326156256566 0.602137860104727918 -4.08952892376395472 -1.17750789516368459 -2.48210963560146025 -0.289674519093430338 -3.28792255444474835 -7.3056922057630107 -3.07388156712283678 -1.54161331652594757 2.23646375382612383 -3.27888129522408311 -1.89007143561711377 -3.51974757294980201 -8.55273493924368466 -7.43746156312112205 -2.45619809006538148 -2.31075097580080335 -6.40738483907422562 2.13869972767813676 -5.27020076123721637 -8.73436551257760385 -0.514445817259908789 -8.96641891805910873 -8.41096639555507153 -3.45138929493309643 -0.501552347925561137 -20.1845419136930957 -20.9724043585797766 -16.026537118848843 -7.93970655085588639 -22.9732274156849599 -17.7841002475601435 -11.2760058846354703 -24.5435163769182303 -24.2352775749799072 -23.8364447076462653 -19.7984847430303148 -17.4053807448869833 -14.5316151482080045 -19.1655758157211444 -25.246569316814675 -17.0968598199599668 -6.17565937565364909 -18.3333775503610212 -9.42216116979470186 -9.97801793442999241!
  0 1.64493406684822641 -0.822467033424113203 0 0 0 ] , "exp" : 86 [ 1.33025574313894634 1.45659549955455336 2.11164717790577328 0.977288069651014557 1.32512971666968449 1.7280803670102578 1.89371692409480796 0.593173793831244023 1.11013065042966019 1.73427732247392696 1.0121022352004676 1.43122150540332815 1.71768844225102679 1.05293273039523827 0.380243613719554452 0.66206336722885184 0.393739702707275152 0.578539595299591092 0.814942176401687535 2.34226750989173427 5.32177717703872648e-05 0.000458766355238190472 2836.69678564414198 6182.49208496940901 22.831870064781544 0.000367576291390422309 0.126619285608298465 50.4345629509076332 0.000238157745533740906 2546.94560982139183 963.014765751905088 7158.81450960804796 0.138016237048799978 4.21718599877950684 16359.5976221839119 7.10002955544502217e-05 0.212332943484016129 0.0137915363872410657 0.732712765114565512 0.00132275823599962788 2.04352669570150596e-13 1835938188118301 0.111412662769134918 28.3083586863385328 0.0013!
 6230087859793496 52207540682.0305481 0.000605587114279446245 1!
 .0430012
1369638302e-18 6.50081540245363351e-14 5672190867026.25781 1612456898891.82178 2.25637183826867542e-10 41.0801555562517464 2.68427984169624881e-07 1.35189691961941112e-19 4874276.49881367292 8.92161481222496892e-21 4.88801596521792746e-18 0.000766496985614515541 0.5676253110515741 2.29955199510165949e-192 1.56857034014490309e-218 1.75347816119370231e-93 6.12943565971134027e-16 8.15007807649707632e-299 3.46758587720560441e+286 6.66379725087312634e+95 0 0 0 inf 6.03827340355772552e-120 9.6688366768244734e+169 inf 0 1.51791255677237637e-113 3.81281242774195972e+33 5.0228138381392881e-141 1.40844525705415359e+67 1.39796817955469167e-26 1 2.71828182845904509 0.367879441171442334 nan inf 0 ] , "fastsigmoid" : 86 [ 0.570540525019168854 0.592926144599914551 0.678775385022163391 0.4939990877173841 0.569559954106807709 0.633696898818016052 0.654782399535179138 0.371892884373664856 0.525981778278946877 0.634625092148780823 0.503000564174726605 0.589057870209217072 0.631837546825408936 !
 0.512999670580029488 0.275240689516067505 0.398412905633449554 0.282481342554092407 0.366303101181983948 0.449166037142276764 0.701022237539291382 5.31673431396484375e-05 0.00045922398567199707 0.999647319316864014 0.999838322401046753 0.958058208227157593 0.000367075204849243164 0.112204968929290771 0.980559855699539185 0.000238329172134399414 0.999607115983963013 0.998961955308914185 0.999860554933547974 0.121276617050170898 0.808499246835708618 0.999938845634460449 7.09295272827148438e-05 0.175330549478530884 0.0135883986949920654 0.422611407935619354 0.00131931900978088379 0 1 0.100070685148239136 0.96592983603477478 0.00136217474937438965 1 0.000605106353759765625 0 0 1 1 0 0.976264059543609619 2.68220901489257812e-07 0 0.999999791383743286 0 0 0.000766098499298095703 0.361672207713127136 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.5 0.731097906827926636 0.268902093172073364 0 0 0 ] , "fasttanh" : 86 [ 0.278501838445663452 0.35929417610168457 0.634043753147125244 -0.0220!
 008492469787598 0.274807274341583252 0.497598469257354736 0.56!
 36233091
35437012 -0.479322582483291626 0.103647239506244659 0.500602662563323975 0.0120018245652318001 0.343514323234558105 0.493069738149642944 0.0519635565578937531 -0.747026681900024414 -0.390239417552947998 -0.731611669063568115 -0.499102085828781128 -0.201255589723587036 0.691158294677734375 -1 -0.999999582767486572 0.999999761581420898 0.999999940395355225 0.996174335479736328 -0.999999701976776123 -0.968431293964385986 0.999214231967926025 -0.999999880790710449 0.999999701976776123 0.999997854232788086 0.999999940395355225 -0.962616026401519775 0.893352508544921875 1 -1 -0.913836658000946045 -0.999619007110595703 -0.300493508577346802 -0.999996483325958252 -1 1 -0.975475132465362549 0.997504949569702148 -0.999996304512023926 1 -0.999999284744262695 -1 -1 1 1 -1 0.998813748359680176 -1 -1 1 -1 -1 -0.999998807907104492 -0.512499094009399414 -1 -1 -1 -1 -1 1 1 -1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 -0 0.761678159236907959 -0.761678159236907959 0 0 0 ] , "hard_slope" : 86 [ 0.28537121!
 182307601 0.376101863104850054 0.747468295972794294 0 0.281510354019701481 0.547011177986860275 0.638541524298489094 0 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 0 0 0 0 0 0.8511194814927876 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 nan 1 0 ] , "hard_slope_integral" : 86 [ 0.357314394088461995 0.311949068447574973 0.126265852013602853 0.511228937994123833 0.35924482299014926 0.226494411006569862 0.180729237850755453 0.671542691465478625 0.447761144256219268 0.224704601103439927 0.493985205655917525 0.320735860383138061 0.229510270990431309 0.474210326327010989 0.745798447811888754 0.64599113447438028 0.741209567161837279 0.676845659964680602 0.584937590174541389 0.0744402592536062002 0.953879296162219004 0.94244252848255694 0 0 0 0.943874330276110896 0.83695140587124528 0 0.946481577455140499 0 0 0 0.8322363!
 79640111545 0 0 0.952619329130635406 0.803890787025820841 0.90!
 53693466
80588771 0.618612187767976662 0.934452329514119273 0.983454079461470565 0 0.843481683024717488 0 0.934198233387807497 0 0.940542104746282637 0.988208778944784538 0.984058289874103376 0 0 0.978459503648917983 0 0.969003172736362606 0.988750800726877599 0 0.989399100744649651 0.987763017176781277 0.938828041628243737 0.680775076393536294 0 0 0 0.986122004876314739 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.991739932438344574 0.5 0 0.75 0 0 0 ] , "hinge_loss_1" : 86 [ 0.71462878817692399 0.623898136895149946 0.252531704027205706 1.02297381917014718 0.718489645980298519 0.452988822013139725 0.361458475701510906 1.52226784732192755 0.895522288512438536 0.449409202206879854 0.987970411311835051 0.641471720766276121 0.459020541980862617 0.948420652654021978 1.96694314293563366 1.41239400673657656 1.93206524103879929 1.54724828992038965 1.20463811745867133 0.1488805185072124 10.8411181615665555 8.68696950748562813 0 0 0 8.90857966523617506 3.06657044589519501 0 9.342577307485044 0 0 0 2.980383!
 94097238779 0 0 10.5528265181928873 2.54959974810481071 5.28370018023997545 1.31100151594728231 7.62803614977747202 30.2189291212707758 0 3.19451428856700659 0 7.5985801862552762 0 8.40931213367730379 42.4044293342158198 31.3642636863514781 0 0 23.2120927879586816 0 16.1306831743568182 44.4476080350577831 0 47.1658099899068475 40.8597451858222485 8.17367979232221842 1.56629374157637358 442.263623535633087 502.513385679572821 214.578812312334776 36.0282588042318821 687.374915298074484 0 0 870.162584189325571 831.341589637100697 783.309267669916153 0 275.512093048542738 0 0 964.540622033178806 260.774779435247183 0 324.050507809966803 0 60.5321925356984138 1 0 2 0 0 inf ] , "inverse_sigmoid" : 86 [ -0.917972400120546594 -0.50612709367614328 1.08515508961390594 0 -0.936982066309396178 0.188601787578807994 0.569039539159469254 0 -2.14843334919660789 0.203058043323619486 -4.40828341034237159 -0.581757561069722984 0.164286344905755943 -2.91167678313082545 0 0 0 0 0 1.743408424955!
 17578 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 !
 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -88 14.5 0 0 0 0 ] , "inverse_softplus" : 86 [ -1.10788794545401115 -0.783957401363940654 0.105842859426561048 nan -1.12353104806846371 -0.317343842633587725 -0.112366193664542063 nan -2.2060878868056526 -0.308868498190999374 -4.41436511612805838 -0.841133387246643771 -0.331719728547833659 -2.93873340942025241 nan nan nan nan nan 0.294360355414619246 nan nan 7.95004296573161007 8.7293149590076613 3.08337083179349092 nan nan 3.90064983442618063 nan 7.84225741589677661 6.86902979956236592 8.87595997727241404 nan 1.16850706404884619 9.70250888656716803 nan nan nan nan nan nan 35.1463320199400187 nan 3.30719283431445277 nan 24.6784927788930339 nan nan nan 29.3665965553371322 28.1087801558890682 nan 3.69088133791308337 nan nan 15.3994820357417233 nan nan nan nan nan nan nan nan nan 659.782795235514641 220.642273314297199 nan nan nan 893.529789987951517 nan 391.405788715928793 813.523761462420225 nan nan 77.3236751556396484 n!
 an 154.615687672048807 nan -30 0.541324854612918016 nan nan inf nan ] , "ipow_int" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 823543 16777216 27 0 0 27 0 823543 46656 16777216 0 1 387420489 0 0 0 1 0 0 618402555 0 27 0 0 0 0 0 -1266004979 0 0 27 0 0 1500973039 0 0 0 1 0 0 0 0 0 399033611 0 0 0 0 -215727763 0 -1726603401 -2029503715 0 0 827793981 0 0 0 1 1 0 0 0 0 ] , "ipow_real" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2007803.25723359827 33721407.1343812943 30.6101726838393446 0 0 60.2674893079957812 0 1824903.41336422157 105139.64271598027 38528079.7583845481 0 1.43916808068752289 762045789.769375205 0 0 0 1 0 0 1.2758518835962983e+54 0 37.3654626413696818 0 2.60403709650791697e+33 0 0 0 3.69619511108423411e+42 3.69467266177917788e+40 0 51.2932979881883213 0 0 649518894868470784 0 0 0 1 0 0 0 0 0 inf inf 0 0 0 inf 0 inf inf 0 0 2.51224244468082715e+145 0 inf 0 1 1 0 0 0 0 ] , "is_equal_1.00000001" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 !
 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 !
 1 1 1 1 
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 ] , "is_equal_1.001" : 86 [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 ] , "is_integer" : 86 [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 ] , "is_missing" : 86 [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ] , "is_positive" : 86 [ 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 ] , "log_a_b" : 86 [ -0.948575255825366948 -1.2442250198512399 -4.53880682090604548 nan -0.937454139500691785 -2.09868161979432655 -2.87934332301557!
 605 nan -0.501529467806405727 -2.12331053259617475 -0.249438431040839265 -1.18109240576515173 -2.05803538416069509 -0.376322777464899449 nan nan nan nan nan -8.36439704244876303 nan nan 1.1544233785890905 1.13633584885232586 1.58963868773401318 nan nan 1.41591462103840904 nan 1.15727048340743877 1.18801243922671063 1.13335657752354435 nan 4.09395593905715138 1.11855950830527684 nan nan nan nan nan nan 1.02301134429357532 nan 1.53065931323398274 nan 1.03578477225373367 nan nan nan 1.02877903975952645 1.03039736769492962 nan 1.45096607078732331 nan nan 1.06509396594772721 nan nan nan nan nan nan nan nan nan 1.00069881503428326 1.00250253997976535 nan nan nan 1.00049326832254581 nan 1.00127902539700497 1.00054927210614442 nan nan 1.0087544215103883 nan 1.00381220565070706 nan -0 inf nan nan nan nan ] , "logadd" : 86 [ 2.4122992228660487 2.50302987414782274 2.87439630701576698 2.10395419187282551 2.40843836506267417 2.67393918902983296 2.76546953534146178 1.60466016372104492 2.!
 23140572253053415 2.67751880883609283 2.13895759973113764 2.48!
 54562902
7669657 2.66790746906211007 2.17850735838895071 1.15998486810733881 1.71453400430639591 1.19486277000417318 1.57967972112258281 1.92228989358430113 2.97804749253576029 -7.71419015052358326 -5.56004149644265588 10.0773235616218049 10.8564047302063305 5.25508538083543364 -5.78165165419320282 0.0603575651477775005 6.04760472392102066 -6.21564929644207176 9.96957813118954483 8.99699675579925362 11.0030276860054332 0.146544070070584703 3.56609609173049558 11.8294980256762106 -7.42589850714991506 0.577328262938161751 -2.15677216919700276 1.81592649509569015 -4.50110813873449978 -27.0920011102278018 37.2732600309829891 -0.0675862775240340802 5.47008513202329461 -4.47165217521230396 26.8054207899551606 -5.28238412263433155 -39.2775013231728494 -28.2373356753085041 31.4935245663802839 30.2357081669326639 -20.0851647769157076 5.84245318276246373 -13.003755163313846 -41.3206800240148127 17.5264102519433713 -44.038881978863877 -37.732817174779278 -5.04675178127924617 1.56063426946659889!
  -439.136695524590095 -499.386457668529829 -211.451884301291813 -32.9013307931889116 -684.247987287031492 661.909723246557633 222.769201325340163 -867.035656178282579 -828.214661626057705 -780.182339658873161 895.656717998994509 -272.385165037499746 393.532716726971785 815.650689473463217 -961.413694022135815 -257.647851424204191 79.450603166682626 -320.923579798923811 156.742615683091771 -57.4052645246554434 2.12692801104297269 3.12692801104297269 1.12692801104297247 nan nan nan ] , "logsub" : 86 [ -0.173303933564005891 -0.0825732822822318607 0.288793150585712366 -0.481648964557229109 -0.17716479136738042 0.0883360325997783608 0.179866378911407193 -0.98094299270900942 -0.354197433899520464 0.0919156524060382313 -0.446645556698916979 -0.100146866153358036 0.082304312632055468 -0.407095798041103907 -1.42561828832271553 -0.87106915212365843 -1.39074038642588116 -1.00592343530747153 -0.663313262845753204 0.392444336105705671 -10.2997933069536369 -8.14564465287270956 7.49172040!
 519175031 8.27080157377627678 2.66948222440537952 -8.367254810!
 62325649
 -2.52524559128227688 3.46200156749096655 -8.80125245287212543 7.38397497475949027 6.41139359936919906 8.41742452957537957 -2.43905908635946966 0.980492935300441015 9.24389486924615689 -10.0115016635799687 -2.00827489349189259 -4.74237532562705777 -0.769676661334364187 -7.08671129516455434 -29.677604266657859 34.6876568745529354 -2.65318943395408846 2.88448197559324049 -7.05725533164235852 24.2198176335251034 -7.86798727906438611 -41.863104479602903 -30.8229388317385613 28.9079214099502266 27.6501050105026067 -22.6707679333457648 3.25685002633240961 -15.5893583197438996 -43.9062831804448663 14.9408070955133159 -46.6244851352939307 -40.3184203312093317 -7.63235493770930074 -1.02496888696345545 -441.722298681020163 -501.972060824959897 -214.037487457721852 -35.4869339496189653 -686.833590443461617 659.324120090127508 220.183598168910123 -869.621259334712704 -830.80026478248783 -782.767942815303286 893.071114842564384 -274.970768193929814 390.947113570541717 813.065086317033092!
  -963.999297178565939 -260.233454580634259 76.8650000102525723 -323.509182955353879 154.157012526661731 -59.9908676810854971 -0.458675145387081928 0.541324854612918127 -1.45867514538708187 0 0 0 ] , "logtwo" : 86 [ -1.8090882914503903 -1.41080464177830023 -0.419915706496796548 nan -1.82874010892939931 -0.870357780587190377 -0.647147653578636572 nan -3.25873289378726039 -0.860947596993796194 -6.37726887458319869 -1.47984117763044831 -0.886354281568315638 -4.27706267152697173 nan nan nan nan nan -0.232566420958034648 nan nan 2.99102663959982085 3.1258951753559181 1.64531309285890126 nan nan 1.97110268649713172 nan 2.97134123969121644 2.78032453532034429 3.14992586850007106 nan 0.525235094519928336 3.2783669386677432 nan nan nan nan nan nan 5.13530222791871438 nan 1.7412111600203406 nan 4.62518238050964925 nan nan nan 4.87610416895135579 4.8129489409487487 nan 1.89356614322330929 nan nan 3.94480994054504297 nan nan nan nan nan nan nan nan nan 9.36584734786547379 7.785565416144!
 47717 nan nan nan 9.80337201881927278 nan 8.61252128270931472 !
 9.668040
67463179351 nan nan 6.27283830464047298 nan 7.27254289570278267 nan -inf 0 nan nan inf nan ] , "mypow" : 86 [ 0.699181192024650344 0.692263767473617508 0.804478808041997451 nan 0.69988545165551086 0.718920707970550854 0.750939898245551518 nan 0.789786736548211965 0.71995077583443301 0.948213657792794651 0.692283605952475067 0.71722642160772021 0.858203624272328769 nan nan nan nan nan 0.871793400969397392 nan nan 14402789.1630946752 163807839.241403729 35.4275127512796928 nan nan 212.019977983952145 nan 10350446.9438332785 562315.656379642198 260924283.575454772 nan 1.68868657287043855 3761301361.61926746 nan nan nan nan nan nan 2.14788983749796281e+54 nan 56.5377470079284592 nan 2.29258425016952159e+34 nan nan nan 1.27604199653943385e+43 5.31106021811161782e+40 nan 131.197159012290399 nan nan 1.93633009340701389e+18 nan nan nan nan nan nan nan nan nan inf inf nan nan nan inf nan inf inf nan nan 1.02626941720702529e+146 nan inf nan 0 1 -1 nan inf 0 ] , "n_choose" : 86 [ 1 1 1!
  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ] , "negative" : 86 [ 0 0 0 -0.0229738191701471806 0 0 0 -0.522267847321927547 0 0 0 0 0 0 -0.966943142935633659 -0.412394006736576557 -0.932065241038799286 -0.547248289920389652 -0.204638117458671331 0 -9.8411181615665555 -7.68696950748562813 0 0 0 -7.90857966523617506 -2.06657044589519501 0 -8.342577307485044 0 0 0 -1.98038394097238779 0 0 -9.55282651819288731 -1.54959974810481071 -4.28370018023997545 -0.311001515947282314 -6.62803614977747202 -29.2189291212707758 0 -2.19451428856700659 0 -6.5985801862552762 0 -7.40931213367730379 -41.4044293342158198 -30.3642636863514781 0 0 -22.2120927879586816 0 -15.1306831743568182 -43.4476080350577831 0 -46.1658099899068475 -39.8597451858222485 -7.17367979232221842 -0.566293741576373577 -441.263623535633087 -501.513385679572821 -213.578812312334776 -35.0282588042318821 !
 -686.374915298074484 0 0 -869.162584189325571 -830.34158963710!
 0697 -78
2.309267669916153 0 -274.512093048542738 0 0 -963.540622033178806 -259.774779435247183 0 -323.050507809966803 0 -59.5321925356984138 0 0 -1 0 0 -inf ] , "pl_log" : 86 [ -1.25396444860284606 -0.97789525976951186 -0.291063388031091885 nan -1.26758605048129969 -0.60328604169242217 -0.44856857148401591 nan -2.25878151752658995 -0.596763399466109568 -4.42038594009003738 -1.02574773995105395 -0.614373971246313544 -2.96463393184710622 nan nan nan nan nan -0.161202758979979016 nan nan 2.0732216822183025 2.1667054275238895 1.14044413145351031 nan nan 1.3662642697396199 nan 2.05957680277345823 1.92717411269893568 2.18336223472366031 nan 0.364065224897624495 2.27239080037848407 nan nan nan nan nan nan 3.55952026060506066 nan 1.2069156063276103 nan 3.20593212662579763 nan nan nan 3.37985785682522577 3.33608198859759808 nan 1.31252003337900569 nan nan 2.73433388813364076 nan nan nan nan nan nan nan nan nan 6.49191068272779059 5.39654271726555912 nan nan nan 6.79517967482483431 nan 5.9697!
 4484462248256 6.70137513515989625 nan nan 4.34800018496996898 nan 5.04094260365764146 nan -inf 0 nan nan inf nan ] , "positive" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 0 0.281510354019701481 0.547011177986860275 0.638541524298489094 0 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 0 0 0 0 0 0.8511194814927876 0 0 7.95039555057883263 8.72947671916335821 3.1281573697924614 0 0 3.92067671287804842 0 7.84265012014657259 6.87006874475628138 8.87609967496246099 0 1.43916808068752289 9.70257001463323832 0 0 0 0 0 0 35.1463320199400187 0 3.34315712098032236 0 24.6784927789121866 0 0 0 29.3665965553373098 28.1087801558896899 0 3.71552517171949148 0 0 15.3994822409003973 0 0 0 0 0 0 0 0 0 659.782795235514641 220.642273314297199 0 0 0 893.529789987951517 0 391.405788715928793 813.523761462420225 0 0 77.3236751556396484 0 154.615687672048807 0 0 1 0 0 inf 0 ] , "safeexp" : 86 [!
  1.33025574313894634 1.45659549955455336 2.11164717790577328 0!
 .9772880
69651014557 1.32512971666968449 1.7280803670102578 1.89371692409480796 0.593173793831244023 1.11013065042966019 1.73427732247392696 1.0121022352004676 1.43122150540332815 1.71768844225102679 1.05293273039523827 0.380243613719554452 0.66206336722885184 0.393739702707275152 0.578539595299591092 0.814942176401687535 2.34226750989173427 5.32177717703872648e-05 0.000458766355238190472 2836.69678564414198 6182.49208496940901 22.831870064781544 0.000367576291390422309 0.126619285608298465 50.4345629509076332 0.000238157745533740906 2546.94560982139183 963.014765751905088 7158.81450960804796 0.138016237048799978 4.21718599877950684 16359.5976221839119 7.10002955544502217e-05 0.212332943484016129 0.0137915363872410657 0.732712765114565512 0.00132275823599962788 2.04352669570150596e-13 1835938188118301 0.111412662769134918 28.3083586863385328 0.00136230087859793496 52207540682.0305481 0.000605587114279446245 1.04300121369638302e-18 6.50081540245363351e-14 5672190867026.25781 161245689!
 8891.82178 2.25637183826867542e-10 41.0801555562517464 2.68427984169624881e-07 1.35189691961941112e-19 4874276.49881367292 8.92161481222496892e-21 4.88801596521792746e-18 0.000766496985614515541 0.5676253110515741 0 0 1.75347816119370231e-93 6.12943565971134027e-16 0 9.99999999999999977e+37 6.66379725087312634e+95 0 0 0 9.99999999999999977e+37 6.03827340355772552e-120 9.99999999999999977e+37 9.99999999999999977e+37 0 1.51791255677237637e-113 3.81281242774195972e+33 0 1.40844525705415359e+67 1.39796817955469167e-26 1 2.71828182845904509 0.367879441171442334 nan 9.99999999999999977e+37 0 ] , "safeflog" : 172 [ -1.25396444860284606 -0.948575255825366948 -0.97789525976951186 -1.2442250198512399 -0.291063388031091885 -4.53880682090604548 0 0 -1.26758605048129969 -0.937454139500691785 -0.60328604169242217 -2.09868161979432655 -0.44856857148401591 -2.87934332301557605 0 0 -2.25878151752658995 -0.501529467806405727 -0.596763399466109568 -2.12331053259617475 -4.42038594009003738 -0.!
 249438431040839265 -1.02574773995105395 -1.18109240576515173 -!
 0.614373
971246313544 -2.05803538416069509 -2.96463393184710622 -0.376322777464899449 0 0 0 0 0 0 0 0 0 0 -0.161202758979979016 -8.36439704244876303 0 0 0 0 2.0732216822183025 1.1544233785890905 2.1667054275238895 1.13633584885232586 1.14044413145351031 1.58963868773401318 0 0 0 0 1.3662642697396199 1.41591462103840904 0 0 2.05957680277345823 1.15727048340743877 1.92717411269893568 1.18801243922671063 2.18336223472366031 1.13335657752354435 0 0 0.364065224897624495 4.09395593905715138 2.27239080037848407 1.11855950830527684 0 0 0 0 0 0 0 0 0 0 0 0 3.55952026060506066 1.02301134429357532 0 0 1.2069156063276103 1.53065931323398274 0 0 3.20593212662579763 1.03578477225373367 0 0 0 0 0 0 3.37985785682522577 1.02877903975952645 3.33608198859759808 1.03039736769492962 0 0 1.31252003337900569 1.45096607078732331 0 0 0 0 2.73433388813364076 1.06509396594772721 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.49191068272779059 1.00069881503428326 5.39654271726555912 1.00250253997976535 0 0 0 0 0 0 6.795!
 17967482483431 1.00049326832254581 0 0 5.96974484462248256 1.00127902539700497 6.70137513515989625 1.00054927210614442 0 0 0 0 4.34800018496996898 1.0087544215103883 0 0 5.04094260365764146 1.00381220565070706 0 0 -57.5 -0.0191063006724888663 0 inf 0 0 0 0 inf nan 0 0 ] , "safeflog2" : 86 [ -1.8090882914503903 -1.41080464177830023 -0.419915706496796548 0 -1.82874010892939931 -0.870357780587190377 -0.647147653578636572 0 -3.25873289378726039 -0.860947596993796194 -6.37726887458319869 -1.47984117763044831 -0.886354281568315638 -4.27706267152697173 0 0 0 0 0 -0.232566420958034648 0 0 2.99102663959982085 3.1258951753559181 1.64531309285890126 0 0 1.97110268649713172 0 2.97134123969121644 2.78032453532034429 3.14992586850007106 0 0.525235094519928336 3.2783669386677432 0 0 0 0 0 0 5.13530222791871438 0 1.7412111600203406 0 4.62518238050964925 0 0 0 4.87610416895135579 4.8129489409487487 0 1.89356614322330929 0 0 3.94480994054504297 0 0 0 0 0 0 0 0 0 9.36584734786547379 7.7855654!
 1614447717 0 0 0 9.80337201881927278 0 8.61252128270931472 9.6!
 68040674
63179351 0 0 6.27283830464047298 0 7.27254289570278267 0 -82.9549648511154345 0 0 0 inf 0 ] , "safelog" : 86 [ -1.25396444860284606 -0.97789525976951186 -0.291063388031091885 0 -1.26758605048129969 -0.60328604169242217 -0.44856857148401591 0 -2.25878151752658995 -0.596763399466109568 -4.42038594009003738 -1.02574773995105395 -0.614373971246313544 -2.96463393184710622 0 0 0 0 0 -0.161202758979979016 0 0 2.0732216822183025 2.1667054275238895 1.14044413145351031 0 0 1.3662642697396199 0 2.05957680277345823 1.92717411269893568 2.18336223472366031 0 0.364065224897624495 2.27239080037848407 0 0 0 0 0 0 3.55952026060506066 0 1.2069156063276103 0 3.20593212662579763 0 0 0 3.37985785682522577 3.33608198859759808 0 1.31252003337900569 0 0 2.73433388813364076 0 0 0 0 0 0 0 0 0 6.49191068272779059 5.39654271726555912 0 0 0 6.79517967482483431 0 5.96974484462248256 6.70137513515989625 0 0 4.34800018496996898 0 5.04094260365764146 0 -57.5 0 0 0 inf 0 ] , "sigmoid" : 86 [ 0.570862553200722!
 656 0.592932576738284034 0.678626803481933272 0.494256797808678927 0.569916468388562025 0.633441883863592237 0.654423695810255168 0.372322088229173787 0.526095694692467419 0.634272649748922612 0.503007360905610712 0.588684125334723585 0.63204023520381436 0.51289197878029047 0.275490217770219259 0.398338222406469433 0.282505909778170139 0.366503062084920339 0.44901825909263704 0.700801926524309504 5.32149397898667455e-05 0.000458555985180196675 0.999647601531968077 0.999838279084656611 0.958039381832742176 0.000367441228706270537 0.11238870772563829 0.980557820604902153 0.000238101039926862681 0.999607526944003344 0.998962671490596921 0.999860331577213657 0.121277915512625165 0.808325790908367647 0.999938877538394832 7.09952548703714648e-05 0.175144084490363922 0.0136039174645201117 0.422870299028540508 0.0013210108580073536 2.04352675189575783e-13 0.999999999999999445 0.100244190570535005 0.965880040888603908 0.00136044753971928888 0.999999999980845655 0.0006052206004824975!
 57 1.02999206386122921e-18 6.50081659510359788e-14 0.999999999!
 99982369
7 0.999999999999379829 2.256371837690372e-10 0.976235829293377377 2.68427912116051427e-07 1.35525271560688054e-19 0.999999794841389233 0 4.87890977618476995e-18 0.000765909917971138395 0.362092463709205503 0 0 0 6.12953698214679932e-16 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.5 0.731058578630004896 0.268941421369995104 nan 1 0 ] , "sign" : 86 [ 1 1 1 -1 1 1 1 -1 1 1 1 1 1 1 -1 -1 -1 -1 -1 1 -1 -1 1 1 1 -1 -1 1 -1 1 1 1 -1 1 1 -1 -1 -1 -1 -1 -1 1 -1 1 -1 1 -1 -1 -1 1 1 -1 1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 1 -1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 0 1 -1 0 1 -1 ] , "soft_slope" : 86 [ 0.447622300082229474 0.469691472780372044 0.56032045609854042 0.37459146662094156 0.446687052102823889 0.511511921903472166 0.533880476740231313 0.268340265664307132 0.404299644314462414 0.512388146884607543 0.382668869186828631 0.465405159680174485 0.510035313941699653 0.391872257281055902 0.191333680756239355 0.290081989617738345 0.196718042959454609 0.263541313460694759 0.33384283852681107 0.585174429!
 522600459 3.36388232575757229e-05 0.0002899046839095476 0.999394663369197023 0.999722156491966163 0.930380501690790451 0.000232294133018839943 0.0736930565715367969 0.967138750476113862 0.000150519889967171139 0.99932584797127344 0.998219163902456352 0.999760039080041185 0.0797601999539536699 0.715309665486769997 0.999894979899716874 4.48785672109153211e-05 0.117334334971604282 0.00863650337162358284 0.311025775199932686 0.000835386960323702965 2.06057393370429054e-13 1 0.0654631231305664585 0.943026547853804464 0.000860336842694486847 0.999999999967087438 0.000382645583675511602 0 0 0.99999999999969702 0.999999999998934408 1.42630796062803711e-10 0.959978124503771957 1.69678814998519556e-07 0 0.999999647479749343 0 0 0.000484264642557796776 0.259919173799199266 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.379885493041722477 0.620114506958277634 0.186333676475250121 nan 1 nan ] , "soft_slope_integral" : 86 [ 0.499999999999999889 0.500000000000000111 0.500000000000000777 0.4999!
 99999999883538 0.499999999999997224 0.500000000000000555 0.499!
 99999999
9999889 0.499999999999999889 0.500000000000025757 0.500000000000000666 0.499999999999808931 0.500000000000001554 0.499999999999999889 0.500000000000035527 0.500000000000000111 0.500000000000002109 0.499999999999999889 0.500000000000000555 0.50000000000000977 0.499999999999999889 0.5 0.5 0.499999999999999944 0.500000000000000111 0.5 0.499999999999999889 0.5 0.5 0.499999999999999944 0.499999999999999944 0.499999999999999944 0.5 0.5 0.5 0.500000000000000111 0.5 0.5 0.5 0.499999999999999334 0.5 0.499999999999999944 0.499999999999999944 0.5 0.500000000000000111 0.500000000000000111 0.5 0.5 0.5 0.499999999999999944 0.5 0.500000000000000111 0.499999999999999944 0.499999999999999944 0.5 0.500000000000000111 0.500000000000000111 0.500000000000000111 0.499999999999999944 0.499999999999999944 0.499999999999999611 0 0 0 0.5 0 0 0 0 0 0 0 0 0 0 0 0 0.499999999999999889 0 0 0.5 0.5 0.500000000000000111 0.500000000000000111 0 0 0 ] , "softplus" : 86 [ 0.845978022558150577 0.898776448139803!
 508 1.13515222517980541 0.681726244069950305 0.843775829598990779 1.00359819959502539 1.06254180866865799 0.4657281234273557 0.746749865205943664 1.00586716785911578 0.699180063670460483 0.88839380817334801 0.999781681583664095 0.719269371009577663 0.322260015270066258 0.508059822799609262 0.331990568006909459 0.45650011283155234 0.596053608477461361 1.20664947182069171 5.3216355755009063e-05 0.000458661154127764769 7.95074801115379604 8.72963845315693909 3.17102376326375612 0.00036750875177556005 0.119221365748697392 3.94031037743223589 0.000238129390479756531 7.84304267024027624 6.87110661166326508 8.8762393531397894 0.129286603659293398 1.65195817584472771 9.70263113896289653 7.09977751527646195e-05 0.192546555767278554 0.0136972986164907422 0.54968825268855348 0.00132188416203163112 2.04352669570129719e-13 35.1463320199400187 0.105631875328681707 3.37787275473581117 0.00136137378864414275 24.6784927789313393 0.000605403820399496835 0 0 29.3665965553374875 28.10878015589!
 03116 2.25637183801411472e-10 3.73957626510160823 2.6842794814!
 283998e-
07 0 15.3994824460590287 0 0 0.000766203376823959971 0.449561933584239737 0 0 0 0 0 659.782795235514641 220.642273314297199 0 0 0 893.529789987951517 0 391.405788715928793 813.523761462420225 0 0 77.3236751556396484 0 154.615687672048807 0 0.693147180559945286 1.31326168751822281 0.313261687518222864 nan inf 0 ] , "softplus_primitive" : 86 [ 1.04159682337948811 1.12073303702115323 1.49741371583688987 0.806674239309017826 1.03833487488175935 1.28320118418907958 1.37774379020373239 0.522753777310472456 0.897661867832517224 1.28679774431664851 0.830841559191260459 1.10502965316349422 1.27715927522248474 0.858889937133157866 0.349151575093653965 0.576224513081076806 0.360560307723318207 0.51123524515272678 0.690735196245947369 1.61875912257232724 5.32170637543253415e-05 0.000458713749321579311 33.2489762805467848 39.7466542215225829 6.49429036858313413 0.000367542518825015663 0.122821864260914376 9.31105676531765702 0.000238143567256499951 32.3981219317464735 25.2428182094568356!
  41.037367103727675 0.133525378681091961 2.45615892843594308 48.7148053860145254 7.09990353337248129e-05 0.202013318136428949 0.0137442739979328071 0.629827350748239145 0.00132232107062805852 2.0435266957014017e-13 619.277261294778782 0.108454135233594207 7.19826574490231685 0.00136183719337798189 306.158936986239553 0.000605495455009514498 1.04300121369638302e-18 6.50081540245352875e-14 432.84343068882265 396.696694992916548 2.25637183814139507e-10 8.52330162407799818 2.68427966156231371e-07 1.35189691961941112e-19 120.216960505592937 8.92161481222496892e-21 4.88801596521792746e-18 0.000766350156222434789 0.502607198022952728 0 0 0 6.12943565971133928e-16 0 0 0 0 0 0 0 0 0 0 0 0 2991.1203038542908 0 0 1.39796817955469167e-26 0.822467033424113203 1.80628607044477407 0.338647996403452167 0 0 0 ] , "sqrt" : 86 [ 0.534201471191418586 0.61327144324911298 0.864562488182776923 nan 0.530575493233245266 0.739602040280352591 0.799087932769910525 nan 0.323230121565985018 0.7420180575!
 92347402 0.109679481618782965 0.598772310009175301 0.735513057!
 68092067
9 0.22711087016252221 nan nan nan nan nan 0.922561370041466433 nan nan 2.81964457876854269 2.9545687873467017 1.76865976654427848 nan nan 1.98006987575642102 nan 2.80047319575577669 2.62108159826364062 2.97927838158210045 nan 1.19965331687430554 3.11489486413799188 nan nan nan nan nan nan 5.92843419630681367 nan 1.82843023410255445 nan 4.96774524094303604 nan nan nan 5.41909554772171465 5.30177141678983421 nan 1.92756975793860486 nan nan 3.92421740489749071 nan nan nan nan nan nan nan nan nan 25.6862374674749638 14.8540322240897673 nan nan nan 29.8919686536024507 nan 19.7839780811627683 28.5223379382269471 nan nan 8.79338814994764384 nan 12.4344556644852293 nan 0 1 nan nan inf nan ] , "square" : 86 [ 0.0814367285373709116 0.141452611430939373 0.558708853484472856 0.000527796367262622088 0.0792480794202976568 0.299221228842572518 0.407735278253437916 0.272763704346280222 0.0109155921976781325 0.303150226614464502 0.000144711004006426109 0.128542527010295088 0.2926587739986796!
 22 0.00266042907263704977 0.934979041670241262 0.170068816792247557 0.868745613552714957 0.299480690820790874 0.0418767591170289624 0.724404371776551592 96.8476066699150948 59.0895002090138419 63.208789410663698 76.2037637904150671 9.78536853018689001 62.5456323213871315 4.27071340784746489 15.3717058869042198 69.5985961313644026 61.5071609070350505 47.1978445576771506 78.7851454398686997 3.92192055366132575 2.07120476446980817 94.139864888860032 91.2564944866892489 2.40125937932649292 18.350087234187999 0.0967219429215076948 43.9308632027569743 853.745818993845432 1235.26465445586109 4.81589296272475487 11.176699535561438 43.541260474440719 609.028005838820945 54.8979062942577229 1714.32676849207155 921.988509214282999 862.396993243949169 790.103521852137987 493.377066020886048 13.8051273016811571 228.937573322764507 1887.69464396801754 237.144053287806713 2131.28201202418268 1588.7992862786798 51.4616817627721446 0.320688601748568558 194713.585455796914 251515.67601578796!
 2 45615.9090687475255 1226.97891485624837 471110.524350438907 !
 435313.3
36888789025 48683.0127733010231 755443.597754666465 689467.15548106737 612007.790282240487 798395.485595912789 75356.8892298917926 153198.491440338286 661820.910463964799 928410.530308085144 67482.9360306313174 5978.95073957488421 104361.63059627742 23906.0108743005439 3544.08194810746591 0 1 1 nan inf inf ] , "tabulated_soft_slope" : 86 [ 0.447621404203703555 0.469692899179227519 0.560320485166853732 0.374591603479243029 0.446686362278421178 0.511511156107323006 0.533882109570459118 0.268338928018172385 0.404297138095832786 0.5123874758889293 0.382668238596417298 0.465405011476318964 0.510032518078911701 0.391883780134454462 0.191331608191728453 0.290081913274965464 0.196716281785152214 0.263540057762600144 0.333843797093716588 0.585171838749248163 5.48957275636752229e-05 0.000288902506006216697 0.999394663863645949 0.999722156610895585 0.930380122015928346 0.000231294709898577366 0.0736919910627160668 0.967138610827208978 0.00014951985957267766 0.999325852107521895 0.99821!
 9175529783898 0.99976003850884998 0.079758661733816183 0.715309946144477515 0.999894980750903883 6.4032815409120758e-05 0.117334249343037156 0.00863555886385647398 0.311023135722213184 0.000834387040924866596 0 1 0.0654622899685772808 0.943026258163342179 0.000859340420745979827 1 0.00038164380009853005 0 0 1 1 0 0.959978389663021736 0 0 1 0 0 0.000483266193000098099 0.259918947787489918 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.379897072596339647 0.620111927435283761 0.18633414334189724 0 0 0 ] , "tabulated_soft_slope_integral" : 86 [ 0.501200202315716936 0.502825971905470626 0.500526423501853834 0.454225859616143923 0.498158770737271495 0.50174719671819179 0.499181579037731638 0.500412810368937921 0.494887444900521278 0.497336760480590945 0.50235711061287236 0.504154397035909141 0.501709235018028776 0.511071295830558658 0.5008750904563114 0.503275225714697405 0.498896391340021117 0.502178869483597801 0.503014018679248331 0.498864701217245798 0.500013622663900148 0.5000261!
 25769797658 0.499949746026605024 0.499840027086333905 0.499630!
 52532233
9702 0.499973292717558582 0.499692063375528517 0.4998175081341098 0.499849306681267647 0.499982893610532886 0.499909993364298644 0.500136558115257923 0.500093876969957218 0.499321646562090937 0.499958743853308851 0.499986519361270865 0.500932020644628273 0.499829977711322482 0.49668364396040815 0.500056332370027001 0.500000053176752668 0.500000036752796362 0.500589918492246544 0.500246110564004143 0.500139477896798801 0.500000074544076578 0.49980712403609312 0.500000026482366744 0.500000049240776678 0.500000052643307713 0.500000057460103675 0.500000092017715003 0.500310469017433146 0.50000019713215238 0.500000024050198166 0.500000190578207104 0.50000002130146548 0.5000000285746794 0.499994761473438742 0.498651798516594413 0 0 0 0.500000037000986497 0 0 0 0 0 0 0 0 0 0 0 0 0.500000007593210283 0 0 0.500000012809926786 0.5 0.500899684928312938 0.500899684928312938 0 0 0 ] , "tabulated_softplus" : 86 [ 0.845977493653015222 0.898781495757737248 1.13515388879937884 0.681728120345!
 184485 0.843775788246689973 1.00359779987715259 1.06254777322850757 0.465727127467436441 0.746745863168179369 1.00586701103415876 0.699180276589834282 0.888395032193885381 0.999776045628391841 0.719264603266264735 0.322257859822930259 0.508061254547034769 0.331988960246080045 0.456499285495705731 0.596057161467600682 1.20664342360582921 5.32162663774293104e-05 0.000458657403353306386 7.95075041010623451 8.72964046309820851 3.17101969956045071 0.000367509397216493063 0.11922118359809053 3.94030763947186768 0.000238129144007791352 7.84305038969908708 6.87111472852820793 8.87623855433310283 0.129285628653368845 1.65196089055317907 9.70264082623048729 7.09982696617108351e-05 0.192547991685139919 0.0136973788320747959 0.549684533506272777 0.00132188353015401145 0 35.1463320199400187 0.105632085241214008 3.37786910576301969 0.00136137866940675627 24.6784927789121866 0.000605400627436921616 0 0 29.3665965553373098 28.1087801558896899 0 3.7395846058362312 0 0 15.3994822409003973 0 !
 0 0.000766205382416965336 0.449563083314658163 0 0 0 0 0 659.7!
 82795235
514641 220.642273314297199 0 0 0 893.529789987951517 0 391.405788715928793 813.523761462420225 0 0 77.3236751556396484 0 154.615687672048807 0 0.693152180552878971 1.31325510795653932 0.313264107988162732 0 0 0 ] , "tabulated_softplus_primitive" : 86 [ 1.04130693264135488 1.11977675170634616 1.49696698553295171 0.805973290009951882 1.03877189960413552 1.28224145949816681 1.37829893231127398 0.522854216558236162 0.898059803041828753 1.28827142418577645 0.830821710672135061 1.10370431378053935 1.27623429527458732 0.858476983529323112 0.349424477650137399 0.5769122331961275 0.360219148003618039 0.511780423420464481 0.691103473425540349 1.61992474340452652 5.32241985851542452e-05 0.000458805871772160383 33.2521529141563974 39.7588449853292403 6.49795532121366382 0.000367464904081049679 0.122746029458622771 9.31387601807132981 0.000237844408575308672 32.3991741498544457 25.2470669701957 41.0266081833356395 0.133549417893795569 2.45777151507910396 48.7186892585501425 7.09898930179!
 188434e-05 0.202291717844318819 0.0137343023742180926 0.629261567935465838 0.0013228147324585869 0 619.277215895346671 0.108591009071907066 7.19548646245595691 0.00136309080586243664 306.158891586826599 0.00060463098661042334 0 0 432.84338528939071 396.696649593485176 0 8.5189877953670603 2.68473635822432436e-07 0 120.216915311319511 0 0 0.000766321363287731207 0.502264303781404298 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2991.12025845485823 0 0 0 0.822467033423196048 1.80510431283294093 0.338930069085138286 0 0 0 ] , "tanh" : 86 [ 0.277868935752042134 0.359317380720308888 0.633636144316777838 -0.0229697781905040706 0.274302370668007334 0.498276800922946128 0.563905675377289639 -0.479448445686924629 0.104099218627074108 0.500962871086776396 0.0120290084504660229 0.343917044339332234 0.493729021959674386 0.0515336549048859283 -0.747357898660468201 -0.390503512380018014 -0.731555326686537555 -0.498455021700417333 -0.201828641736480657 0.691653860347550276 -0.999999994335737497 -0.9999!
 9957906695125 0.999999751455527774 0.99999994767576994 0.99617!
 07412166
20304 -0.999999729775376545 -0.968441079631438106 0.999214035792840849 -0.999999886561782958 0.999999691687887271 0.9999978434296245 0.999999960974563673 -0.962615161042118417 0.893530133644261038 0.999999992527175863 -0.999999989917916166 -0.913719425965825338 -0.999619659391405624 -0.301347939442288659 -0.999996500627420959 -1 1 -0.975478813280421408 0.997507363160468619 -0.999996288279520806 1 -0.999999266528763031 -1 -1 1 1 -1 0.9988155722733848 -0.999999999999855893 -1 0.999999999999915845 -1 -1 -0.999998824965432465 -0.512632187571988918 -1 -1 -1 -1 -1 1 1 -1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 0 0.761594155955764851 -0.761594155955764851 nan 1 -1 ] , "ultrafastsigmoid" : 86 [ 0.59365148548299651 0.618713932979307768 0.704042835653942989 0.491482655774221289 0.592540787791205892 0.661074433844617282 0.681504114607858535 0.344702898660289969 0.537234076268872984 0.66190095984904207 0.504484124670356415 0.614010169728664956 0.659676455562789199 0.518855966043686112 0.25557086!
 1231883661 0.371788976349332223 0.261584796615427995 0.338870745712378185 0.430383772793096198 0.723890866469534711 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.957497275954168781 0.00247262315663498988 0.11886118658391942 0.973676865725655483 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.99752737684336501 0.126848068488980392 0.813848010679333989 0.99752737684336501 0.00247262315663498988 0.172582856222275316 0.0221591376039232957 0.399069241040782341 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.10760993926961987 0.969267098826247619 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.00247262315663498988 0.971323711147181967 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.334500509703386073 0.00247262!
 315663498988 0.00247262315663498988 0.00247262315663498988 0.0!
 02472623
15663498988 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.5 0.75 0.25 0 0 0 ] , "ultrafasttanh" : 86 [ 0.333021942453098574 0.409964414541520217 0.641615328039489063 -0.0336868139823713816 0.32950614070735823 0.53038838933797261 0.584551732283869363 -0.514628074396435986 0.141892014299020208 0.532626788360361481 0.0178298967084918013 0.395863985366522453 0.526593124136654378 0.0735741161275507705 -0.737395343435666373 -0.437973403423140784 -0.723628701485511949 -0.530536979894041916 -0.25481277052362411 0.689679534467238398 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.952227797252091501 0.995054753!
 68673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.948273448822313347 0.885036229410972597 0.99505475368673002 -0.99505475368673002 -0.911672361077462412 -0.99505475368673002 -0.355836563303930098 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.958098026400172165 0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.542325229180608992 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.!
 99505475368673002 0.99505475368673002 -0.99505475368673002 0.9!
 95054753
68673002 -0.99505475368673002 0 0.75 -0.75 0 0 0 ] };
+results = {"DOUBLE_TO_INT" : 86 [ 0 0 1 0 0 1 1 -1 0 1 0 0 1 0 -1 0 -1 -1 0 1 -10 -8 8 9 3 -8 -2 4 -8 8 7 9 -2 1 10 -10 -2 -4 0 -7 -29 35 -2 3 -7 25 -7 -41 -30 29 28 -22 4 -15 -43 15 -46 -40 -7 -1 -441 -502 -214 -35 -686 660 221 -869 -830 -782 894 -275 391 814 -964 -260 77 -323 155 -60 0 1 -1 -2147483648 -2147483648 -2147483648 ] , "FABS" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 0.0229738191701471806 0.281510354019701481 0.547011177986860275 0.638541524298489094 0.522267847321927547 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 0.966943142935633659 0.412394006736576557 0.932065241038799286 0.547248289920389652 0.204638117458671331 0.8511194814927876 9.8411181615665555 7.68696950748562813 7.95039555057883263 8.72947671916335821 3.1281573697924614 7.90857966523617506 2.06657044589519501 3.92067671287804842 8.342577307485044 7.84265012014657259 6.87006874475628138 8.87!
 609967496246099 1.98038394097238779 1.43916808068752289 9.70257001463323832 9.55282651819288731 1.54959974810481071 4.28370018023997545 0.311001515947282314 6.62803614977747202 29.2189291212707758 35.1463320199400187 2.19451428856700659 3.34315712098032236 6.5985801862552762 24.6784927789121866 7.40931213367730379 41.4044293342158198 30.3642636863514781 29.3665965553373098 28.1087801558896899 22.2120927879586816 3.71552517171949148 15.1306831743568182 43.4476080350577831 15.3994822409003973 46.1658099899068475 39.8597451858222485 7.17367979232221842 0.566293741576373577 441.263623535633087 501.513385679572821 213.578812312334776 35.0282588042318821 686.374915298074484 659.782795235514641 220.642273314297199 869.162584189325571 830.341589637100697 782.309267669916153 893.529789987951517 274.512093048542738 391.405788715928793 813.523761462420225 963.540622033178806 259.774779435247183 77.3236751556396484 323.050507809966803 154.615687672048807 59.5321925356984138 0 1 1 nan i!
 nf inf ] , "d_hinge_loss_1" : 86 [ -1 -1 -1 -1 -1 -1 -1 -1 -1 !
 -1 -1 -1
 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 0 -1 -1 0 -1 0 0 0 -1 0 0 -1 -1 -1 -1 -1 -1 0 -1 0 -1 0 -1 -1 -1 0 0 -1 0 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 0 -1 -1 -1 0 -1 0 0 -1 -1 0 -1 0 -1 -1 0 -1 0 0 -1 ] , "d_soft_slope" : 86 [ 0.242285698073777922 0.244037175721172117 0.24142634460912743 0.22980825628581214 0.242190808539238855 0.24479150158679186 0.243817139803235661 0.193194278139123599 0.236124157782237831 0.244771407051106227 0.231694215146674098 0.243770194618311398 0.244822030971784232 0.233689426402807876 0.152772615215307939 0.202481489173009677 0.155983744438359606 0.191019006099864885 0.218367110090888494 0.237953458248121696 3.36375989302695189e-05 0.00028981375353320793 0.000604940196399006425 0.000277759986236081335 0.0644294275677805839 0.000232235751034814975 0.0678812607067604823 0.0316986737378326944 0.000150495377013837306 0.000673660341962700748 0.00177740568165485335 0.000239898620697565068 0.0729579454382620618 0.200265007724166055 0.000105008167115394624 4.!
 48763880317448738e-05 0.102690705248289493 0.00855590646865989068 0.210550996506591592 0.00083463197687483337 1.29118937763905706e-13 9.43689570931383059e-16 0.0608715080726522917 0.0534912609033074693 0.000859536091819768089 3.29125060538615344e-11 0.000382487172532974817 0 4.1078251911130792e-14 3.02924352268973962e-13 1.06564757018645651e-12 1.42629907884384011e-10 0.0382992724382222316 1.69678785022497891e-07 0 3.52520116209120715e-07 0 0 0.000484010924693967226 0.18934707348612323 0 0 0 4.44089209850062616e-16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.231058578630004896 0.231058578630004896 0.149738499347877529 nan 0 0 ] , "data" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 -0.0229738191701471806 0.281510354019701481 0.547011177986860275 0.638541524298489094 -0.522267847321927547 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 -0.966943142935633659 -0.412394006736576557 -0.9!
 32065241038799286 -0.547248289920389652 -0.204638117458671331 !
 0.851119
4814927876 -9.8411181615665555 -7.68696950748562813 7.95039555057883263 8.72947671916335821 3.1281573697924614 -7.90857966523617506 -2.06657044589519501 3.92067671287804842 -8.342577307485044 7.84265012014657259 6.87006874475628138 8.87609967496246099 -1.98038394097238779 1.43916808068752289 9.70257001463323832 -9.55282651819288731 -1.54959974810481071 -4.28370018023997545 -0.311001515947282314 -6.62803614977747202 -29.2189291212707758 35.1463320199400187 -2.19451428856700659 3.34315712098032236 -6.5985801862552762 24.6784927789121866 -7.40931213367730379 -41.4044293342158198 -30.3642636863514781 29.3665965553373098 28.1087801558896899 -22.2120927879586816 3.71552517171949148 -15.1306831743568182 -43.4476080350577831 15.3994822409003973 -46.1658099899068475 -39.8597451858222485 -7.17367979232221842 -0.566293741576373577 -441.263623535633087 -501.513385679572821 -213.578812312334776 -35.0282588042318821 -686.374915298074484 659.782795235514641 220.642273314297199 -869.1625841!
 89325571 -830.341589637100697 -782.309267669916153 893.529789987951517 -274.512093048542738 391.405788715928793 813.523761462420225 -963.540622033178806 -259.774779435247183 77.3236751556396484 -323.050507809966803 154.615687672048807 -59.5321925356984138 0 1 -1 nan inf -inf ] , "dilogarithm" : 86 [ 0.308821818354089395 0.41903704230712352 0.973798967287272776 -0.0228432001973329982 0.304281881734638049 0.648824602623850222 0.787699775851718575 -0.466400827029289367 0.107341308626911297 0.654015615338651601 0.0120659611819196235 0.397134765289189673 0.640117836209777247 0.0522601591501091617 -0.79944740624721411 -0.376233907889713892 -0.774925280735139221 -0.486411798592015221 -0.195024079455506927 1.18308297983425881 -4.15999032198696916 -3.59868099598513069 1.01077088538761295 0.824548197471537403 2.28988133524398574 -3.66048124080796899 -1.47307718868669113 2.08304670132001624 -3.77858968703090747 1.03711900312929295 1.28163866538727045 0.79032778092548428 -1.42595030288!
 368396 2.34305326156256566 0.602137860104728251 -4.08952892376!
 395472 -
1.17750789516368459 -2.48210963560146025 -0.289674519093430338 -3.2879225544447479 -7.30569220576300982 -3.07388156712283678 -1.54161331652594735 2.23646375382612383 -3.27888129522408311 -1.89007143561711377 -3.51974757294980201 -8.55273493924368466 -7.43746156312112294 -2.45619809006538192 -2.31075097580080291 -6.40738483907422562 2.13869972767813632 -5.27020076123721637 -8.73436551257760385 -0.5144458172599089 -8.96641891805910873 -8.41096639555507153 -3.45138929493309643 -0.501552347925561137 -20.1845419136930957 -20.9724043585797766 -16.0265371188488501 -7.9397065508558855 -22.9732274156849599 -17.7841002475601435 -11.2760058846354703 -24.5435163769182338 -24.2352775749799072 -23.8364447076462653 -19.7984847430303148 -17.4053807448869833 -14.5316151482080045 -19.1655758157211444 -25.246569316814675 -17.0968598199599668 -6.17565937565364997 -18.3333775503610212 -9.42216116979470186 -9.97801793442999418 0 1.64493406684822641 -0.822467033424113203 0 0 0 ] , "exp" : 86 [ 1.3!
 3025574313894634 1.45659549955455336 2.11164717790577328 0.977288069651014557 1.32512971666968449 1.7280803670102578 1.89371692409480796 0.593173793831244023 1.11013065042966019 1.73427732247392696 1.0121022352004676 1.43122150540332815 1.71768844225102679 1.05293273039523827 0.380243613719554452 0.66206336722885184 0.393739702707275152 0.578539595299591092 0.814942176401687535 2.34226750989173427 5.32177717703872648e-05 0.000458766355238190472 2836.69678564414198 6182.49208496940901 22.831870064781544 0.000367576291390422309 0.126619285608298465 50.4345629509076332 0.000238157745533740906 2546.94560982139183 963.014765751905088 7158.81450960804796 0.138016237048799978 4.21718599877950684 16359.5976221839119 7.10002955544502217e-05 0.212332943484016129 0.0137915363872410657 0.732712765114565512 0.00132275823599962788 2.04352669570150596e-13 1835938188118301 0.111412662769134918 28.3083586863385328 0.00136230087859793496 52207540682.0305481 0.000605587114279446245 1.04300121!
 369638302e-18 6.50081540245363351e-14 5672190867026.25781 1612!
 45689889
1.82178 2.25637183826867542e-10 41.0801555562517464 2.68427984169624881e-07 1.35189691961941112e-19 4874276.49881367292 8.92161481222496892e-21 4.88801596521792746e-18 0.000766496985614515541 0.5676253110515741 2.29955199510165949e-192 1.56857034014490309e-218 1.75347816119370231e-93 6.12943565971134027e-16 8.15007807649707632e-299 3.46758587720560441e+286 6.66379725087312634e+95 0 0 0 inf 6.03827340355772552e-120 9.66883667682447206e+169 inf 0 1.51791255677237637e-113 3.81281242774195972e+33 5.0228138381392881e-141 1.40844525705415359e+67 1.39796817955469167e-26 1 2.71828182845904509 0.367879441171442334 nan inf 0 ] , "fastsigmoid" : 86 [ 0.570540525019168854 0.592926144599914551 0.678775385022163391 0.4939990877173841 0.569559954106807709 0.633696898818016052 0.654782399535179138 0.371892884373664856 0.525981778278946877 0.634625092148780823 0.503000564174726605 0.589057870209217072 0.631837546825408936 0.512999670580029488 0.275240689516067505 0.398412905633449554 0.28248!
 1342554092407 0.366303101181983948 0.449166037142276764 0.701022237539291382 5.31673431396484375e-05 0.00045922398567199707 0.999647319316864014 0.999838322401046753 0.958058208227157593 0.000367075204849243164 0.112204968929290771 0.980559855699539185 0.000238329172134399414 0.999607115983963013 0.998961955308914185 0.999860554933547974 0.121276617050170898 0.808499246835708618 0.999938845634460449 7.09295272827148438e-05 0.175330549478530884 0.0135883986949920654 0.422611407935619354 0.00131931900978088379 0 1 0.100070685148239136 0.96592983603477478 0.00136217474937438965 1 0.000605106353759765625 0 0 1 1 0 0.976264059543609619 2.68220901489257812e-07 0 0.999999791383743286 0 0 0.000766098499298095703 0.361672207713127136 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.5 0.731097906827926636 0.268902093172073364 0 0 0 ] , "fasttanh" : 86 [ 0.278501838445663452 0.35929417610168457 0.634043753147125244 -0.0220008492469787598 0.274807274341583252 0.497598469257354736 0.5636233091!
 35437012 -0.479322582483291626 0.103647239506244659 0.50060266!
 25633239
75 0.0120018245652318001 0.343514323234558105 0.493069738149642944 0.0519635565578937531 -0.747026681900024414 -0.390239417552947998 -0.731611669063568115 -0.499102085828781128 -0.201255589723587036 0.691158294677734375 -1 -0.999999582767486572 0.999999761581420898 0.999999940395355225 0.996174335479736328 -0.999999701976776123 -0.968431293964385986 0.999214231967926025 -0.999999880790710449 0.999999701976776123 0.999997854232788086 0.999999940395355225 -0.962616026401519775 0.893352508544921875 1 -1 -0.913836658000946045 -0.999619007110595703 -0.300493508577346802 -0.999996483325958252 -1 1 -0.975475132465362549 0.997504949569702148 -0.999996304512023926 1 -0.999999284744262695 -1 -1 1 1 -1 0.998813748359680176 -1 -1 1 -1 -1 -0.999998807907104492 -0.512499094009399414 -1 -1 -1 -1 -1 1 1 -1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 -0 0.761678159236907959 -0.761678159236907959 0 0 0 ] , "hard_slope" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 0 0.28151035401970!
 1481 0.547011177986860275 0.638541524298489094 0 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 0 0 0 0 0 0.8511194814927876 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 nan 1 0 ] , "hard_slope_integral" : 86 [ 0.357314394088461995 0.311949068447574973 0.126265852013602853 0.511228937994123833 0.35924482299014926 0.226494411006569862 0.180729237850755453 0.671542691465478625 0.447761144256219268 0.224704601103439927 0.493985205655917525 0.320735860383138061 0.229510270990431309 0.474210326327010989 0.745798447811888754 0.64599113447438028 0.741209567161837279 0.676845659964680602 0.584937590174541389 0.0744402592536062002 0.953879296162218893 0.942442528482557051 0 0 0 0.943874330276111007 0.83695140587124528 0 0.946481577455140943 0 0 0 0.832236379640111545 0 0 0.952619329130635073 0.80389078702582073 0.9053693466!
 80588549 0.618612187767976662 0.934452329514119384 0.983454079!
 46147034
3 0 0.843481683024717488 0 0.934198233387807164 0 0.940542104746282526 0.988208778944784427 0.984058289874104375 0 0 0.978459503648918982 0 0.969003172736362828 0.98875080072687993 0 0.989399100744652316 0.987763017176781943 0.938828041628243959 0.680775076393536294 0 0 0 0.986122004876314406 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.991739932438346017 0.5 0 0.75 0 0 0 ] , "hinge_loss_1" : 86 [ 0.71462878817692399 0.623898136895149946 0.252531704027205706 1.02297381917014718 0.718489645980298519 0.452988822013139725 0.361458475701510906 1.52226784732192755 0.895522288512438536 0.449409202206879854 0.987970411311835051 0.641471720766276121 0.459020541980862617 0.948420652654021978 1.96694314293563366 1.41239400673657656 1.93206524103879929 1.54724828992038965 1.20463811745867133 0.1488805185072124 10.8411181615665555 8.68696950748562813 0 0 0 8.90857966523617506 3.06657044589519501 0 9.342577307485044 0 0 0 2.98038394097238779 0 0 10.5528265181928873 2.54959974810481071 5.2837001802399!
 7545 1.31100151594728231 7.62803614977747202 30.2189291212707758 0 3.19451428856700659 0 7.5985801862552762 0 8.40931213367730379 42.4044293342158198 31.3642636863514781 0 0 23.2120927879586816 0 16.1306831743568182 44.4476080350577831 0 47.1658099899068475 40.8597451858222485 8.17367979232221842 1.56629374157637358 442.263623535633087 502.513385679572821 214.578812312334776 36.0282588042318821 687.374915298074484 0 0 870.162584189325571 831.341589637100697 783.309267669916153 0 275.512093048542738 0 0 964.540622033178806 260.774779435247183 0 324.050507809966803 0 60.5321925356984138 1 0 2 0 0 inf ] , "inverse_sigmoid" : 86 [ -0.917972400120546594 -0.50612709367614328 1.08515508961390594 0 -0.936982066309396178 0.188601787578807856 0.569039539159469254 0 -2.14843334919660789 0.203058043323619347 -4.40828341034237159 -0.581757561069722984 0.164286344905755805 -2.91167678313082545 0 0 0 0 0 1.74340842495517556 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0!
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -88 14.!
 5 0 0 0 
0 ] , "inverse_softplus" : 86 [ -1.10788794545401115 -0.783957401363940654 0.105842859426561048 nan -1.12353104806846371 -0.317343842633587725 -0.112366193664542063 nan -2.2060878868056526 -0.308868498190999374 -4.41436511612805838 -0.841133387246643771 -0.331719728547833659 -2.93873340942025241 nan nan nan nan nan 0.294360355414619246 nan nan 7.95004296573161007 8.7293149590076613 3.08337083179349092 nan nan 3.90064983442618063 nan 7.84225741589677661 6.86902979956236592 8.87595997727241404 nan 1.16850706404884619 9.70250888656716803 nan nan nan nan nan nan 35.1463320199400187 nan 3.30719283431445277 nan 24.6784927788930339 nan nan nan 29.3665965553371322 28.1087801558890682 nan 3.69088133791308337 nan nan 15.3994820357417233 nan nan nan nan nan nan nan nan nan 659.782795235514641 220.642273314297199 nan nan nan 893.529789987951517 nan 391.405788715928793 813.523761462420225 nan nan 77.3236751556396484 nan 154.615687672048807 nan -30 0.541324854612918016 nan nan inf nan ] ,!
  "ipow_int" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 823543 16777216 27 0 0 27 0 823543 46656 16777216 0 1 387420489 0 0 0 1 0 0 618402555 0 27 0 0 0 0 0 -1266004979 0 0 27 0 0 1500973039 0 0 0 1 0 0 0 0 0 399033611 0 0 0 0 -215727763 0 -1726603401 -2029503715 0 0 827793981 0 0 0 1 1 0 0 0 0 ] , "ipow_real" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2007803.25723359827 33721407.1343812943 30.6101726838393446 0 0 60.2674893079957812 0 1824903.41336422157 105139.64271598027 38528079.7583845481 0 1.43916808068752289 762045789.769375205 0 0 0 1 0 0 1.2758518835962983e+54 0 37.3654626413696818 0 2.60403709650791697e+33 0 0 0 3.69619511108423411e+42 3.69467266177917788e+40 0 51.2932979881883213 0 0 649518894868470784 0 0 0 1 0 0 0 0 0 inf inf 0 0 0 inf 0 inf inf 0 0 2.51224244468082715e+145 0 inf 0 1 1 0 0 0 0 ] , "is_equal_1.00000001" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1!
  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 ] , "is_equ!
 al_1.001
" : 86 [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 ] , "is_integer" : 86 [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 ] , "is_missing" : 86 [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 ] , "is_positive" : 86 [ 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 1 0 ] , "log_a_b" : 86 [ -0.948575255825366948 -1.2442250198512399 -4.53880682090604548 nan -0.937454139500691785 -2.09868161979432655 -2.87934332301557605 nan -0.501529467806405727 -2.12331053259617475 -0.24943843104083926!
 5 -1.18109240576515173 -2.05803538416069509 -0.376322777464899449 nan nan nan nan nan -8.36439704244876303 nan nan 1.1544233785890905 1.13633584885232586 1.58963868773401318 nan nan 1.41591462103840904 nan 1.15727048340743877 1.18801243922671063 1.13335657752354435 nan 4.09395593905715138 1.11855950830527684 nan nan nan nan nan nan 1.02301134429357532 nan 1.53065931323398274 nan 1.03578477225373367 nan nan nan 1.02877903975952645 1.03039736769492962 nan 1.45096607078732331 nan nan 1.06509396594772721 nan nan nan nan nan nan nan nan nan 1.00069881503428326 1.00250253997976535 nan nan nan 1.00049326832254581 nan 1.00127902539700497 1.00054927210614442 nan nan 1.0087544215103883 nan 1.00381220565070706 nan -0 inf nan nan nan nan ] , "logadd" : 86 [ 2.4122992228660487 2.50302987414782274 2.87439630701576698 2.10395419187282551 2.40843836506267417 2.67393918902983296 2.76546953534146178 1.60466016372104492 2.23140572253053415 2.67751880883609283 2.13895759973113764 2.48545629027!
 669657 2.66790746906211007 2.17850735838895071 1.1599848681073!
 3881 1.7
1453400430639591 1.19486277000417318 1.57967972112258281 1.92228989358430113 2.97804749253576029 -7.71419015052358326 -5.56004149644265588 10.0773235616218049 10.8564047302063305 5.25508538083543364 -5.78165165419320282 0.0603575651477774866 6.04760472392102066 -6.21564929644207176 9.96957813118954483 8.99699675579925362 11.0030276860054332 0.146544070070584703 3.56609609173049558 11.8294980256762106 -7.42589850714991506 0.577328262938161751 -2.15677216919700276 1.81592649509569015 -4.50110813873449978 -27.0920011102278018 37.2732600309829891 -0.0675862775240340941 5.47008513202329461 -4.47165217521230396 26.8054207899551606 -5.28238412263433155 -39.2775013231728494 -28.2373356753085041 31.4935245663802839 30.2357081669326639 -20.0851647769157076 5.84245318276246373 -13.003755163313846 -41.3206800240148127 17.5264102519433713 -44.038881978863877 -37.732817174779278 -5.04675178127924617 1.56063426946659889 -439.136695524590095 -499.386457668529829 -211.451884301291813 -32.901!
 3307931889116 -684.247987287031492 661.909723246557633 222.769201325340163 -867.035656178282579 -828.214661626057705 -780.182339658873161 895.656717998994509 -272.385165037499746 393.532716726971785 815.650689473463217 -961.413694022135815 -257.647851424204191 79.450603166682626 -320.923579798923811 156.742615683091771 -57.4052645246554434 2.12692801104297269 3.12692801104297269 1.12692801104297247 nan inf -inf ] , "logsub" : 86 [ -0.173303933564005919 -0.0825732822822318746 0.288793150585712366 -0.481648964557229109 -0.177164791367380448 0.0883360325997783469 0.179866378911407165 -0.98094299270900942 -0.354197433899520464 0.0919156524060382174 -0.446645556698916979 -0.10014686615335805 0.0823043126320554541 -0.407095798041103907 -1.42561828832271553 -0.87106915212365843 -1.39074038642588116 -1.00592343530747153 -0.663313262845753204 0.392444336105705671 -10.2997933069536369 -8.14564465287270956 7.49172040519175031 8.27080157377627678 2.66948222440537952 -8.3672548106232564!
 9 -2.52524559128227688 3.46200156749096655 -8.8012524528721254!
 3 7.3839
7497475949027 6.41139359936919906 8.41742452957537957 -2.43905908635946966 0.980492935300441015 9.24389486924615689 -10.0115016635799687 -2.00827489349189259 -4.74237532562705777 -0.769676661334364187 -7.08671129516455434 -29.677604266657859 34.6876568745529354 -2.65318943395408846 2.88448197559324049 -7.05725533164235852 24.2198176335251034 -7.86798727906438611 -41.863104479602903 -30.8229388317385613 28.9079214099502266 27.6501050105026067 -22.6707679333457648 3.25685002633240961 -15.5893583197438996 -43.9062831804448663 14.9408070955133159 -46.6244851352939307 -40.3184203312093317 -7.63235493770930074 -1.02496888696345545 -441.722298681020163 -501.972060824959897 -214.037487457721852 -35.4869339496189653 -686.833590443461617 659.324120090127508 220.183598168910123 -869.621259334712704 -830.80026478248783 -782.767942815303286 893.071114842564384 -274.970768193929814 390.947113570541717 813.065086317033092 -963.999297178565939 -260.233454580634259 76.8650000102525723 -323.5!
 09182955353879 154.157012526661731 -59.9908676810854971 -0.458675145387081928 0.541324854612918127 -1.45867514538708187 0 0 0 ] , "logtwo" : 86 [ -1.8090882914503903 -1.41080464177830023 -0.419915706496796548 nan -1.82874010892939931 -0.870357780587190377 -0.647147653578636572 nan -3.25873289378726039 -0.860947596993796194 -6.37726887458319869 -1.47984117763044831 -0.886354281568315638 -4.27706267152697173 nan nan nan nan nan -0.232566420958034648 nan nan 2.99102663959982085 3.1258951753559181 1.64531309285890126 nan nan 1.97110268649713172 nan 2.97134123969121644 2.78032453532034429 3.14992586850007106 nan 0.525235094519928336 3.2783669386677432 nan nan nan nan nan nan 5.13530222791871438 nan 1.7412111600203406 nan 4.62518238050964925 nan nan nan 4.87610416895135579 4.8129489409487487 nan 1.89356614322330929 nan nan 3.94480994054504297 nan nan nan nan nan nan nan nan nan 9.36584734786547379 7.78556541614447717 nan nan nan 9.80337201881927278 nan 8.61252128270931472 9.66804!
 067463179351 nan nan 6.27283830464047298 nan 7.272542895702782!
 67 nan -
inf 0 nan nan inf nan ] , "mypow" : 86 [ 0.699181192024650344 0.692263767473617508 0.804478808041997451 nan 0.69988545165551086 0.718920707970550854 0.750939898245551518 nan 0.789786736548211965 0.71995077583443301 0.948213657792794651 0.692283605952475067 0.71722642160772021 0.858203624272328769 nan nan nan nan nan 0.871793400969397392 nan nan 14402789.1630946752 163807839.241403729 35.4275127512796928 nan nan 212.019977983952145 nan 10350446.9438332785 562315.656379642198 260924283.575454772 nan 1.68868657287043855 3761301361.61926746 nan nan nan nan nan nan 2.14788983749796281e+54 nan 56.5377470079284592 nan 2.29258425016952159e+34 nan nan nan 1.27604199653943385e+43 5.31106021811161782e+40 nan 131.197159012290399 nan nan 1.93633009340701389e+18 nan nan nan nan nan nan nan nan nan inf inf nan nan nan inf nan inf inf nan nan 1.02626941720702529e+146 nan inf nan 0 1 -1 nan inf 0 ] , "n_choose" : 86 [ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 !
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ] , "negative" : 86 [ 0 0 0 -0.0229738191701471806 0 0 0 -0.522267847321927547 0 0 0 0 0 0 -0.966943142935633659 -0.412394006736576557 -0.932065241038799286 -0.547248289920389652 -0.204638117458671331 0 -9.8411181615665555 -7.68696950748562813 0 0 0 -7.90857966523617506 -2.06657044589519501 0 -8.342577307485044 0 0 0 -1.98038394097238779 0 0 -9.55282651819288731 -1.54959974810481071 -4.28370018023997545 -0.311001515947282314 -6.62803614977747202 -29.2189291212707758 0 -2.19451428856700659 0 -6.5985801862552762 0 -7.40931213367730379 -41.4044293342158198 -30.3642636863514781 0 0 -22.2120927879586816 0 -15.1306831743568182 -43.4476080350577831 0 -46.1658099899068475 -39.8597451858222485 -7.17367979232221842 -0.566293741576373577 -441.263623535633087 -501.513385679572821 -213.578812312334776 -35.0282588042318821 -686.374915298074484 0 0 -869.162584189325571 -830.341589637100697 -7!
 82.309267669916153 0 -274.512093048542738 0 0 -963.54062203317!
 8806 -25
9.774779435247183 0 -323.050507809966803 0 -59.5321925356984138 0 0 -1 0 0 -inf ] , "pl_log" : 86 [ -1.25396444860284606 -0.97789525976951186 -0.291063388031091885 nan -1.26758605048129969 -0.60328604169242217 -0.44856857148401591 nan -2.25878151752658995 -0.596763399466109568 -4.42038594009003738 -1.02574773995105395 -0.614373971246313544 -2.96463393184710622 nan nan nan nan nan -0.161202758979979016 nan nan 2.0732216822183025 2.1667054275238895 1.14044413145351031 nan nan 1.3662642697396199 nan 2.05957680277345823 1.92717411269893568 2.18336223472366031 nan 0.364065224897624495 2.27239080037848407 nan nan nan nan nan nan 3.55952026060506066 nan 1.2069156063276103 nan 3.20593212662579763 nan nan nan 3.37985785682522577 3.33608198859759808 nan 1.31252003337900569 nan nan 2.73433388813364076 nan nan nan nan nan nan nan nan nan 6.49191068272779059 5.39654271726555912 nan nan nan 6.79517967482483431 nan 5.96974484462248256 6.70137513515989625 nan nan 4.34800018496996898 nan 5.0!
 4094260365764146 nan -inf 0 nan nan inf nan ] , "positive" : 86 [ 0.28537121182307601 0.376101863104850054 0.747468295972794294 0 0.281510354019701481 0.547011177986860275 0.638541524298489094 0 0.104477711487561464 0.550590797793120146 0.0120295886881649494 0.358528279233723879 0.540979458019137383 0.0515793473459780216 0 0 0 0 0 0.8511194814927876 0 0 7.95039555057883263 8.72947671916335821 3.1281573697924614 0 0 3.92067671287804842 0 7.84265012014657259 6.87006874475628138 8.87609967496246099 0 1.43916808068752289 9.70257001463323832 0 0 0 0 0 0 35.1463320199400187 0 3.34315712098032236 0 24.6784927789121866 0 0 0 29.3665965553373098 28.1087801558896899 0 3.71552517171949148 0 0 15.3994822409003973 0 0 0 0 0 0 0 0 0 659.782795235514641 220.642273314297199 0 0 0 893.529789987951517 0 391.405788715928793 813.523761462420225 0 0 77.3236751556396484 0 154.615687672048807 0 0 1 0 0 inf 0 ] , "safeexp" : 86 [ 1.33025574313894634 1.45659549955455336 2.11164717790577328 0.977288!
 069651014557 1.32512971666968449 1.7280803670102578 1.89371692!
 40948079
6 0.593173793831244023 1.11013065042966019 1.73427732247392696 1.0121022352004676 1.43122150540332815 1.71768844225102679 1.05293273039523827 0.380243613719554452 0.66206336722885184 0.393739702707275152 0.578539595299591092 0.814942176401687535 2.34226750989173427 5.32177717703872648e-05 0.000458766355238190472 2836.69678564414198 6182.49208496940901 22.831870064781544 0.000367576291390422309 0.126619285608298465 50.4345629509076332 0.000238157745533740906 2546.94560982139183 963.014765751905088 7158.81450960804796 0.138016237048799978 4.21718599877950684 16359.5976221839119 7.10002955544502217e-05 0.212332943484016129 0.0137915363872410657 0.732712765114565512 0.00132275823599962788 2.04352669570150596e-13 1835938188118301 0.111412662769134918 28.3083586863385328 0.00136230087859793496 52207540682.0305481 0.000605587114279446245 1.04300121369638302e-18 6.50081540245363351e-14 5672190867026.25781 1612456898891.82178 2.25637183826867542e-10 41.0801555562517464 2.684279841696!
 24881e-07 1.35189691961941112e-19 4874276.49881367292 8.92161481222496892e-21 4.88801596521792746e-18 0.000766496985614515541 0.5676253110515741 0 0 1.75347816119370231e-93 6.12943565971134027e-16 0 9.99999999999999977e+37 6.66379725087312634e+95 0 0 0 9.99999999999999977e+37 6.03827340355772552e-120 9.99999999999999977e+37 9.99999999999999977e+37 0 1.51791255677237637e-113 3.81281242774195972e+33 0 1.40844525705415359e+67 1.39796817955469167e-26 1 2.71828182845904509 0.367879441171442334 nan 9.99999999999999977e+37 0 ] , "safeflog" : 172 [ -1.25396444860284606 -0.948575255825366948 -0.97789525976951186 -1.2442250198512399 -0.291063388031091885 -4.53880682090604548 0 0 -1.26758605048129969 -0.937454139500691785 -0.60328604169242217 -2.09868161979432655 -0.44856857148401591 -2.87934332301557605 0 0 -2.25878151752658995 -0.501529467806405727 -0.596763399466109568 -2.12331053259617475 -4.42038594009003738 -0.249438431040839265 -1.02574773995105395 -1.18109240576515173 -0.61437!
 3971246313544 -2.05803538416069509 -2.96463393184710622 -0.376!
 32277746
4899449 0 0 0 0 0 0 0 0 0 0 -0.161202758979979016 -8.36439704244876303 0 0 0 0 2.0732216822183025 1.1544233785890905 2.1667054275238895 1.13633584885232586 1.14044413145351031 1.58963868773401318 0 0 0 0 1.3662642697396199 1.41591462103840904 0 0 2.05957680277345823 1.15727048340743877 1.92717411269893568 1.18801243922671063 2.18336223472366031 1.13335657752354435 0 0 0.364065224897624495 4.09395593905715138 2.27239080037848407 1.11855950830527684 0 0 0 0 0 0 0 0 0 0 0 0 3.55952026060506066 1.02301134429357532 0 0 1.2069156063276103 1.53065931323398274 0 0 3.20593212662579763 1.03578477225373367 0 0 0 0 0 0 3.37985785682522577 1.02877903975952645 3.33608198859759808 1.03039736769492962 0 0 1.31252003337900569 1.45096607078732331 0 0 0 0 2.73433388813364076 1.06509396594772721 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.49191068272779059 1.00069881503428326 5.39654271726555912 1.00250253997976535 0 0 0 0 0 0 6.79517967482483431 1.00049326832254581 0 0 5.96974484462248256 1.00127902!
 539700497 6.70137513515989625 1.00054927210614442 0 0 0 0 4.34800018496996898 1.0087544215103883 0 0 5.04094260365764146 1.00381220565070706 0 0 -57.5 -0.0191063006724888663 0 inf 0 0 0 0 inf nan 0 0 ] , "safeflog2" : 86 [ -1.8090882914503903 -1.41080464177830023 -0.419915706496796548 0 -1.82874010892939931 -0.870357780587190377 -0.647147653578636572 0 -3.25873289378726039 -0.860947596993796194 -6.37726887458319869 -1.47984117763044831 -0.886354281568315638 -4.27706267152697173 0 0 0 0 0 -0.232566420958034648 0 0 2.99102663959982085 3.1258951753559181 1.64531309285890126 0 0 1.97110268649713172 0 2.97134123969121644 2.78032453532034429 3.14992586850007106 0 0.525235094519928336 3.2783669386677432 0 0 0 0 0 0 5.13530222791871438 0 1.7412111600203406 0 4.62518238050964925 0 0 0 4.87610416895135579 4.8129489409487487 0 1.89356614322330929 0 0 3.94480994054504297 0 0 0 0 0 0 0 0 0 9.36584734786547379 7.78556541614447717 0 0 0 9.80337201881927278 0 8.61252128270931472 9.66804067!
 463179351 0 0 6.27283830464047298 0 7.27254289570278267 0 -82.!
 95496485
11154345 0 0 0 inf 0 ] , "safelog" : 86 [ -1.25396444860284606 -0.97789525976951186 -0.291063388031091885 0 -1.26758605048129969 -0.60328604169242217 -0.44856857148401591 0 -2.25878151752658995 -0.596763399466109568 -4.42038594009003738 -1.02574773995105395 -0.614373971246313544 -2.96463393184710622 0 0 0 0 0 -0.161202758979979016 0 0 2.0732216822183025 2.1667054275238895 1.14044413145351031 0 0 1.3662642697396199 0 2.05957680277345823 1.92717411269893568 2.18336223472366031 0 0.364065224897624495 2.27239080037848407 0 0 0 0 0 0 3.55952026060506066 0 1.2069156063276103 0 3.20593212662579763 0 0 0 3.37985785682522577 3.33608198859759808 0 1.31252003337900569 0 0 2.73433388813364076 0 0 0 0 0 0 0 0 0 6.49191068272779059 5.39654271726555912 0 0 0 6.79517967482483431 0 5.96974484462248256 6.70137513515989625 0 0 4.34800018496996898 0 5.04094260365764146 0 -57.5 0 0 0 inf 0 ] , "sigmoid" : 86 [ 0.570862553200722656 0.592932576738284034 0.678626803481933161 0.494256797808678927 0.!
 569916468388562025 0.633441883863592237 0.654423695810255168 0.372322088229173787 0.526095694692467419 0.634272649748922612 0.503007360905610712 0.588684125334723585 0.63204023520381436 0.51289197878029047 0.275490217770219203 0.398338222406469433 0.282505909778170139 0.366503062084920339 0.44901825909263704 0.700801926524309504 5.32149397898806775e-05 0.000458555985180186809 0.999647601531968188 0.999838279084656611 0.958039381832742176 0.000367441228706244516 0.112388707725638304 0.980557820604902153 0.000238101039926885694 0.999607526944003233 0.998962671490596921 0.999860331577213657 0.121277915512625178 0.808325790908367647 0.999938877538394832 7.09952548703651765e-05 0.175144084490363949 0.01360391746452011 0.422870299028540453 0.00132101085800734319 2.04336547682260061e-13 0.999999999999999445 0.100244190570535019 0.965880040888603908 0.00136044753971931165 0.999999999980845655 0.00060522060048251447 0 6.50035580918029154e-14 0.999999999999823697 0.999999999999379829!
  2.25637175610415852e-10 0.976235829293377377 2.68427912120117!
 185e-07 
0 0.999999794841389233 0 0 0.000765909917971152598 0.362092463709205503 0 0 0 6.10622663543836097e-16 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.5 0.731058578630004896 0.268941421369995104 nan 1 0 ] , "sign" : 86 [ 1 1 1 -1 1 1 1 -1 1 1 1 1 1 1 -1 -1 -1 -1 -1 1 -1 -1 1 1 1 -1 -1 1 -1 1 1 1 -1 1 1 -1 -1 -1 -1 -1 -1 1 -1 1 -1 1 -1 -1 -1 1 1 -1 1 -1 -1 1 -1 -1 -1 -1 -1 -1 -1 -1 -1 1 1 -1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 0 1 -1 0 1 -1 ] , "soft_slope" : 86 [ 0.447622300082229474 0.469691472780372044 0.56032045609854042 0.37459146662094156 0.446687052102823667 0.511511921903472166 0.533880476740231313 0.268340265664307132 0.404299644314462414 0.512388146884607654 0.382668869186828631 0.465405159680174485 0.510035313941699653 0.391872257281055902 0.191333680756239355 0.290081989617738345 0.196718042959454609 0.263541313460694537 0.33384283852681107 0.585174429522600459 3.36388232575757229e-05 0.0002899046839095476 0.999394663369197023 0.999722156491966163 0.930380501690790451 0.0002322941330!
 18839943 0.0736930565715367969 0.967138750476113862 0.000150519889967171139 0.99932584797127344 0.998219163902456352 0.999760039080041185 0.0797601999539536699 0.715309665486769997 0.999894979899716874 4.48785672109153211e-05 0.117334334971604282 0.00863650337162358284 0.311025775199932797 0.000835386960323702965 2.06057393370429054e-13 1 0.0654631231305664585 0.943026547853804464 0.000860336842694486847 0.999999999967087438 0.000382645583675511602 0 0 0.99999999999969702 0.999999999998934408 1.42630796062803711e-10 0.959978124503771957 1.69678814998519556e-07 0 0.999999647479749343 0 0 0.000484264642557796776 0.259919173799199266 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.379885493041722477 0.620114506958277634 0.186333676475250121 nan 1 nan ] , "soft_slope_integral" : 86 [ 0.499999999999999778 0.500000000000000111 0.500000000000000777 0.499999999999883538 0.500000000000002776 0.499999999999999778 0.499999999999999889 0.499999999999999445 0.500000000000025757 0.500000000000!
 000666 0.499999999999808931 0.500000000000001554 0.49999999999!
 9999889 
0.500000000000035527 0.499999999999999889 0.500000000000002109 0.499999999999999889 0.500000000000001665 0.50000000000000977 0.499999999999999556 0.5 0.500000000000000111 0.499999999999999889 0.500000000000000111 0.500000000000000111 0.499999999999999889 0.500000000000000111 0.499999999999999889 0.499999999999999889 0.5 0.5 0.5 0.5 0.500000000000000222 0.5 0.499999999999999889 0.500000000000000111 0.5 0.499999999999997113 0.5 0.5 0.5 0.500000000000000111 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.499999999999999889 0.5 0.5 0.5 0.5 0.5 0.5 0.499999999999999556 0 0 0 0.5 0 0 0 0 0 0 0 0 0 0 0 0 0.499999999999999889 0 0 0.5 0.5 0.499999999999999889 0.499999999999999889 0 0 0 ] , "softplus" : 86 [ 0.845978022558150577 0.898776448139803508 1.13515222517980519 0.681726244069950305 0.843775829598990779 1.00359819959502539 1.06254180866865799 0.4657281234273557 0.746749865205943664 1.00586716785911578 0.699180063670460483 0.88839380817334801 0.999781681583664095 0.719269371009577663 0.32!
 2260015270066258 0.508059822799609262 0.331990568006909459 0.45650011283155234 0.596053608477461361 1.20664947182069171 5.3216355755009063e-05 0.000458661154127764769 7.95074801115379604 8.72963845315693909 3.17102376326375612 0.00036750875177556005 0.119221365748697392 3.94031037743223589 0.000238129390479756531 7.84304267024027624 6.87110661166326508 8.8762393531397894 0.129286603659293398 1.65195817584472771 9.70263113896289653 7.09977751527646195e-05 0.192546555767278554 0.0136972986164907422 0.54968825268855348 0.00132188416203163112 2.04352669570129719e-13 35.1463320199400187 0.105631875328681707 3.37787275473581117 0.00136137378864414275 24.6784927789313393 0.000605403820399496835 0 0 29.3665965553374875 28.1087801558903116 2.25637183801411472e-10 3.73957626510160823 2.6842794814283998e-07 0 15.3994824460590287 0 0 0.000766203376823959971 0.449561933584239737 0 0 0 0 0 659.782795235514641 220.642273314297199 0 0 0 893.529789987951517 0 391.405788715928793 813.5237614!
 62420225 0 0 77.3236751556396484 0 154.615687672048807 0 0.693!
 14718055
9945286 1.31326168751822281 0.313261687518222864 nan inf 0 ] , "softplus_primitive" : 86 [ 1.04159682337948811 1.12073303702115323 1.49741371583688987 0.806674239309017826 1.03833487488175891 1.28320118418908002 1.37774379020373239 0.522753777310472567 0.897661867832517224 1.28679774431664851 0.830841559191260459 1.10502965316349422 1.27715927522248474 0.858889937133157866 0.349151575093653965 0.576224513081076806 0.360560307723318207 0.511235245152726892 0.690735196245947369 1.61875912257232746 5.32170637543253415e-05 0.000458713749321579311 33.2489762805467848 39.7466542215225829 6.49429036858313324 0.000367542518825015663 0.122821864260914376 9.3110567653176588 0.000238143567256499951 32.3981219317464735 25.2428182094568321 41.0373671037276679 0.133525378681091961 2.45615892843594263 48.7148053860145325 7.09990353337248129e-05 0.202013318136428949 0.0137442739979328071 0.629827350748239145 0.00132232107062805852 2.0435266957014017e-13 619.277261294778782 0.108454135233594!
 207 7.19826574490231685 0.00136183719337798189 306.158936986239553 0.000605495455009514498 1.04300121369638302e-18 6.50081540245352875e-14 432.84343068882265 396.696694992916605 2.25637183814139507e-10 8.52330162407799818 2.68427966156231371e-07 1.35189691961941112e-19 120.216960505592937 8.92161481222496892e-21 4.88801596521792746e-18 0.000766350156222434789 0.502607198022952728 0 0 0 6.12943565971133928e-16 0 0 0 0 0 0 0 0 0 0 0 0 2991.1203038542908 0 0 1.39796817955469167e-26 0.822467033424113203 1.80628607044477429 0.338647996403452167 0 0 0 ] , "sqrt" : 86 [ 0.534201471191418586 0.61327144324911298 0.864562488182776923 nan 0.530575493233245266 0.739602040280352591 0.799087932769910525 nan 0.323230121565985018 0.742018057592347402 0.109679481618782965 0.598772310009175301 0.735513057680920679 0.22711087016252221 nan nan nan nan nan 0.922561370041466433 nan nan 2.81964457876854269 2.9545687873467017 1.76865976654427848 nan nan 1.98006987575642102 nan 2.80047319575577669 !
 2.62108159826364062 2.97927838158210045 nan 1.1996533168743055!
 4 3.1148
9486413799188 nan nan nan nan nan nan 5.92843419630681367 nan 1.82843023410255445 nan 4.96774524094303604 nan nan nan 5.41909554772171465 5.30177141678983421 nan 1.92756975793860486 nan nan 3.92421740489749071 nan nan nan nan nan nan nan nan nan 25.6862374674749638 14.8540322240897673 nan nan nan 29.8919686536024507 nan 19.7839780811627683 28.5223379382269471 nan nan 8.79338814994764384 nan 12.4344556644852293 nan 0 1 nan nan inf nan ] , "square" : 86 [ 0.0814367285373709116 0.141452611430939373 0.558708853484472856 0.000527796367262622088 0.0792480794202976568 0.299221228842572518 0.407735278253437916 0.272763704346280222 0.0109155921976781325 0.303150226614464502 0.000144711004006426109 0.128542527010295088 0.292658773998679622 0.00266042907263704977 0.934979041670241262 0.170068816792247557 0.868745613552714957 0.299480690820790874 0.0418767591170289624 0.724404371776551592 96.8476066699150948 59.0895002090138419 63.208789410663698 76.2037637904150671 9.78536853018689001 !
 62.5456323213871315 4.27071340784746489 15.3717058869042198 69.5985961313644026 61.5071609070350505 47.1978445576771506 78.7851454398686997 3.92192055366132575 2.07120476446980817 94.139864888860032 91.2564944866892489 2.40125937932649292 18.350087234187999 0.0967219429215076948 43.9308632027569743 853.745818993845432 1235.26465445586109 4.81589296272475487 11.176699535561438 43.541260474440719 609.028005838820945 54.8979062942577229 1714.32676849207155 921.988509214282999 862.396993243949169 790.103521852137987 493.377066020886048 13.8051273016811571 228.937573322764507 1887.69464396801754 237.144053287806713 2131.28201202418268 1588.7992862786798 51.4616817627721446 0.320688601748568558 194713.585455796914 251515.676015787962 45615.9090687475255 1226.97891485624837 471110.524350438907 435313.336888789025 48683.0127733010231 755443.597754666465 689467.15548106737 612007.790282240487 798395.485595912789 75356.8892298917926 153198.491440338286 661820.910463964799 928410.5303!
 08085144 67482.9360306313174 5978.95073957488421 104361.630596!
 27742 23
906.0108743005439 3544.08194810746591 0 1 1 nan inf inf ] , "tabulated_soft_slope" : 86 [ 0.447621404203703555 0.469692899179227519 0.560320485166853732 0.374591603479243029 0.446686362278421178 0.511511156107323117 0.533882109570459118 0.268338928018172496 0.404297138095832786 0.5123874758889293 0.382668238596417298 0.465405011476318742 0.510032518078911701 0.39188378013445424 0.191331608191728453 0.290081913274965464 0.196716281785152436 0.263540057762600144 0.333843797093716588 0.585171838749248163 5.48957275636752229e-05 0.000288902506006216697 0.999394663863645949 0.999722156610895585 0.930380122015928346 0.000231294709898577366 0.0736919910627165109 0.967138610827208978 0.00014951985957267766 0.999325852107521895 0.998219175529783898 0.99976003850884998 0.079758661733816183 0.715309946144477515 0.999894980750903883 6.4032815409120758e-05 0.117334249343037156 0.00863555886385647398 0.311023135722213184 0.000834387040924866596 0 1 0.0654622899685772808 0.9430262581633421!
 79 0.000859340420745979827 1 0.00038164380009853005 0 0 1 1 0 0.959978389663021736 0 0 1 0 0 0.000483266193000098099 0.25991894778749014 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0.379897072596339647 0.620111927435283761 0.18633414334189724 0 0 0 ] , "tabulated_soft_slope_integral" : 86 [ 0.501200202315710053 0.502825971905468294 0.500526423501852613 0.454225859614881822 0.498158770737265888 0.501747196718190347 0.499181579037731082 0.50041281036893559 0.494887444900490747 0.497336760480589057 0.502357110609803481 0.504154397035905699 0.501709235018026556 0.511071295830475059 0.500875090456310956 0.503275225714697405 0.498896391340020506 0.502178869483596468 0.503014018679237784 0.498864701217245465 0.500013622663900148 0.500026125769797658 0.499949746026605024 0.499840027086334016 0.499630525322339758 0.499973292717558748 0.499692063375528406 0.499817508134109856 0.499849306681267591 0.499982893610532941 0.499909993364298755 0.500136558115257812 0.500093876969956996 0.499321!
 64656209066 0.49995874385330874 0.499986519361270809 0.5009320!
 20644628
051 0.499829977711322648 0.496683643960403542 0.50005633237002689 0.500000053176752779 0.500000036752796362 0.500589918492246433 0.500246110564004254 0.500139477896798912 0.500000074544076578 0.499807124036093064 0.500000026482366744 0.500000049240776567 0.500000052643307713 0.500000057460103564 0.500000092017715003 0.500310469017433035 0.50000019713215238 0.500000024050198166 0.500000190578207104 0.50000002130146548 0.500000028574679511 0.499994761473438798 0.498651798516593026 0 0 0 0.500000037000986497 0 0 0 0 0 0 0 0 0 0 0 0 0.500000007593210283 0 0 0.500000012809926786 0.5 0.50089968492831205 0.50089968492831205 0 0 0 ] , "tabulated_softplus" : 86 [ 0.845977493653015222 0.898781495757737137 1.13515388879937884 0.681728120345184485 0.843775788246690084 1.00359779987715259 1.06254777322850757 0.465727127467436441 0.746745863168179369 1.00586701103415876 0.699180276589834282 0.888395032193885381 0.999776045628391952 0.719264603266264735 0.322257859822930259 0.5080612545470!
 3488 0.331988960246080045 0.456499285495705731 0.596057161467600682 1.20664342360582921 5.32162663774293104e-05 0.000458657403353306386 7.95075041010623451 8.72964046309820851 3.17101969956045071 0.000367509397216493063 0.11922118359809053 3.94030763947186768 0.000238129144007791352 7.84305038969908708 6.87111472852820793 8.87623855433310283 0.129285628653368845 1.65196089055317907 9.70264082623048729 7.09982696617108351e-05 0.192547991685139919 0.0136973788320747959 0.549684533506272777 0.00132188353015401145 0 35.1463320199400187 0.105632085241214008 3.37786910576301969 0.00136137866940675627 24.6784927789121866 0.000605400627436921616 0 0 29.3665965553373098 28.1087801558896899 0 3.7395846058362312 0 0 15.3994822409003973 0 0 0.000766205382416965336 0.449563083314658163 0 0 0 0 0 659.782795235514641 220.642273314297199 0 0 0 893.529789987951517 0 391.405788715928793 813.523761462420225 0 0 77.3236751556396484 0 154.615687672048807 0 0.693152180552878971 1.313255107956539!
 32 0.313264107988162732 0 0 0 ] , "tabulated_softplus_primitiv!
 e" : 86 
[ 1.04130693264135488 1.11977675170634616 1.49696698553295193 0.805973290009951882 1.03877189960413552 1.28224145949816681 1.37829893231127398 0.522854216558236162 0.898059803041828753 1.28827142418577645 0.830821710672135061 1.10370431378053935 1.27623429527458754 0.85847698352932289 0.349424477650137399 0.5769122331961275 0.360219148003618039 0.511780423420464481 0.691103473425540349 1.6199247434045263 5.32241985851542452e-05 0.000458805871772160383 33.2521529141563974 39.7588449853292332 6.49795532121366382 0.000367464904081049679 0.122746029458622771 9.31387601807132981 0.000237844408575308672 32.3991741498544386 25.2470669701956965 41.0266081833356395 0.133549417893795569 2.45777151507910396 48.7186892585501496 7.09898930179188434e-05 0.202291717844318819 0.0137343023742180926 0.629261567935465838 0.0013228147324585869 0 619.277215895346671 0.108591009071907066 7.19548646245595513 0.00136309080586243664 306.158891586826599 0.00060463098661042334 0 0 432.843385289390767 !
 396.696649593485176 0 8.5189877953670603 2.68473635822432436e-07 0 120.216915311319497 0 0 0.000766321363287731207 0.502264303781404298 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2991.12025845485823 0 0 0 0.822467033423195826 1.80510431283294137 0.338930069085138286 0 0 0 ] , "tanh" : 86 [ 0.277868935752042134 0.359317380720308832 0.633636144316777949 -0.0229697781905040706 0.274302370668007278 0.498276800922946184 0.563905675377289528 -0.479448445686924574 0.104099218627074094 0.500962871086776507 0.0120290084504660211 0.34391704433933229 0.493729021959674386 0.0515336549048859283 -0.747357898660468312 -0.390503512380018014 -0.731555326686537444 -0.498455021700417389 -0.201828641736480657 0.691653860347550165 -0.999999994335737497 -0.99999957906695125 0.999999751455527774 0.99999994767576994 0.996170741216620304 -0.999999729775376545 -0.968441079631438106 0.999214035792840849 -0.999999886561782958 0.999999691687887271 0.9999978434296245 0.999999960974563673 -0.962615161042118417 0.89!
 3530133644261038 0.999999992527175863 -0.999999989917916166 -0!
 .9137194
25965825338 -0.999619659391405624 -0.301347939442288659 -0.999996500627420959 -1 1 -0.975478813280421408 0.997507363160468619 -0.999996288279520806 1 -0.999999266528763031 -1 -1 1 1 -1 0.9988155722733848 -0.999999999999855893 -1 0.999999999999915845 -1 -1 -0.999998824965432465 -0.512632187571988918 -1 -1 -1 -1 -1 1 1 -1 -1 -1 1 -1 1 1 -1 -1 1 -1 1 -1 0 0.761594155955764851 -0.761594155955764851 nan 1 -1 ] , "ultrafastsigmoid" : 86 [ 0.59365148548299651 0.618713932979307768 0.704042835653942989 0.491482655774221289 0.592540787791205892 0.661074433844617282 0.681504114607858535 0.344702898660289969 0.537234076268872984 0.66190095984904207 0.504484124670356415 0.614010169728664956 0.659676455562789199 0.518855966043686112 0.255570861231883661 0.371788976349332223 0.261584796615427995 0.338870745712378185 0.430383772793096198 0.723890866469534711 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.957497275954168781 0.00247262315663498988 0.11!
 886118658391942 0.973676865725655483 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.99752737684336501 0.126848068488980392 0.813848010679333989 0.99752737684336501 0.00247262315663498988 0.172582856222275316 0.0221591376039232957 0.399069241040782341 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.10760993926961987 0.969267098826247619 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.00247262315663498988 0.971323711147181967 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.334500509703386073 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.99752737684336501 0.00247262315663498988 0.00247262315663498988 0.00247262315663498988 0.99752737684336501 0.00247262315663!
 498988 0.99752737684336501 0.99752737684336501 0.0024726231566!
 3498988 
0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.99752737684336501 0.00247262315663498988 0.5 0.75 0.25 0 0 0 ] , "ultrafasttanh" : 86 [ 0.333021942453098574 0.409964414541520217 0.641615328039489063 -0.0336868139823713816 0.32950614070735823 0.53038838933797261 0.584551732283869363 -0.514628074396435986 0.141892014299020208 0.532626788360361481 0.0178298967084918013 0.395863985366522453 0.526593124136654378 0.0735741161275507705 -0.737395343435666373 -0.437973403423140784 -0.723628701485511949 -0.530536979894041916 -0.25481277052362411 0.689679534467238398 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.952227797252091501 0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.948273448822313347 0.885036229410972597 0.99505475368673002 -0.99505475368673002 -0.911672361077462412 -0.99505475368673002 -0.355836563303930098 -0.99505475!
 368673002 -0.99505475368673002 0.99505475368673002 -0.958098026400172165 0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.542325229180608992 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 0.99505475368673002 0.99505475368673002 -0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 0.99505475368673002 -0.99505475368673002 0 0.75 -0.75 0 0 0 ] };
 save = 1 ;
 save_path = ""  )



From tihocan at mail.berlios.de  Thu Feb 14 17:56:58 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 14 Feb 2008 17:56:58 +0100
Subject: [Plearn-commits] r8511 - trunk/plearn/vmat
Message-ID: <200802141656.m1EGuw2X030207@sheep.berlios.de>

Author: tihocan
Date: 2008-02-14 17:56:58 +0100 (Thu, 14 Feb 2008)
New Revision: 8511

Modified:
   trunk/plearn/vmat/RandomSamplesVMatrix.cc
Log:
Clarified help for an option

Modified: trunk/plearn/vmat/RandomSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RandomSamplesVMatrix.cc	2008-02-14 16:32:04 UTC (rev 8510)
+++ trunk/plearn/vmat/RandomSamplesVMatrix.cc	2008-02-14 16:56:58 UTC (rev 8511)
@@ -96,8 +96,10 @@
         "If given a non-negative value, it indicates the total number of\n"
         "non-preserved examples that are added to this VMat, and overrides\n"
         "the 'length' option. Two special negative values can be used:\n"
-        " -1: this option is ignored, and 'length' is used instead (or the\n"
-        "     source's length if 'length' is not set)\n"
+        " -1: this option is ignored, and the number of non-preserved\n"
+        "     samples is set so that this VMat has either the desired length\n"
+        "     (if the 'length' option is provided), or the same length as\n"
+        "     its source otherwise.\n"
         " -2: the number of non-preserved examples is set exactly to match\n"
         "     the number of preserved examples.");
 



From chrish at mail.berlios.de  Thu Feb 14 18:00:42 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 14 Feb 2008 18:00:42 +0100
Subject: [Plearn-commits] r8512 - trunk/python_modules/plearn/pyplearn
Message-ID: <200802141700.m1EH0gD4030751@sheep.berlios.de>

Author: chrish
Date: 2008-02-14 18:00:42 +0100 (Thu, 14 Feb 2008)
New Revision: 8512

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Move imports out of functions and to the top of the file.

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2008-02-14 16:56:58 UTC (rev 8511)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2008-02-14 17:00:42 UTC (rev 8512)
@@ -202,7 +202,7 @@
     
 for instance.
 """
-import copy, inspect, logging, new, re, sys
+import copy, inspect, logging, new, re, sys, os, warnings
 from plearn.pyplearn.context import *
 from plearn.utilities.Bindings import Bindings
 
@@ -248,11 +248,9 @@
         raise ValueError, "Cannot cast '%s' into a list", str(slist)
 
 def warn(message, category=UserWarning, stacklevel=0):
-    import os
     pytest_state = os.environ.get("PYTEST_STATE", "")
     if pytest_state!="Active":        
-        from warnings import warn
-        warn(message, category, stacklevel=stacklevel+3)
+        warnings.warn(message, category, stacklevel=stacklevel+3)
 
 #######  Classes to Manage Command-Line Arguments  ############################
 



From tihocan at mail.berlios.de  Thu Feb 14 22:15:53 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 14 Feb 2008 22:15:53 +0100
Subject: [Plearn-commits] r8513 - in trunk: commands plearn/vmat
Message-ID: <200802142115.m1ELFrKN016950@sheep.berlios.de>

Author: tihocan
Date: 2008-02-14 22:15:53 +0100 (Thu, 14 Feb 2008)
New Revision: 8513

Added:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
   trunk/plearn/vmat/ReplicateSamplesVMatrix.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added new VMat to replicate samples so as to rebalance classes in a dataset

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-02-14 17:00:42 UTC (rev 8512)
+++ trunk/commands/plearn_noblas_inc.h	2008-02-14 21:15:53 UTC (rev 8513)
@@ -335,6 +335,7 @@
 #include <plearn/vmat/RegularGridVMatrix.h>
 #include <plearn/vmat/RemoveDuplicateVMatrix.h>
 #include <plearn/vmat/ReorderByMissingVMatrix.h>
+#include <plearn/vmat/ReplicateSamplesVMatrix.h>
 //#include <plearn/vmat/SelectAttributsSequenceVMatrix.h>
 #include <plearn/vmat/SelectRowsMultiInstanceVMatrix.h>
 #include <plearn/vmat/ShuffleColumnsVMatrix.h>

Added: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-02-14 17:00:42 UTC (rev 8512)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-02-14 21:15:53 UTC (rev 8513)
@@ -0,0 +1,170 @@
+// -*- C++ -*-
+
+// ReplicateSamplesVMatrix.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file ReplicateSamplesVMatrix.cc */
+
+
+#include "ReplicateSamplesVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ReplicateSamplesVMatrix,
+    "VMat that replicates samples in order to reweight classes evenly.",
+    "If the class with most samples in the source VMat has n samples, then\n"
+    "the first 'n-n_j' samples of class j (having n_j samples) will be\n"
+    "replicated so that each class also has n samples. If required, samples\n"
+    "will be replicated more than once.\n"
+    "All samples are also shuffled so as to mix classes together."
+);
+
+/////////////////////////////
+// ReplicateSamplesVMatrix //
+/////////////////////////////
+ReplicateSamplesVMatrix::ReplicateSamplesVMatrix():
+    seed(1827),
+    random_gen(new PRandom())
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void ReplicateSamplesVMatrix::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "seed", &ReplicateSamplesVMatrix::seed,
+                  OptionBase::buildoption,
+        "Seed for the random number generator (to shuffle data).");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void ReplicateSamplesVMatrix::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void ReplicateSamplesVMatrix::build_()
+{
+    if (!source)
+        return;
+    PLCHECK_MSG(source->targetsize() == 1,
+                "In ReplicateSamplesVMatrix::build_ - The source VMat must "
+                "have a targetsize equal to 1");
+    
+    // Build the vector of indices.
+    indices.resize(0);
+    Vec input, target;
+    real weight;
+    TVec< TVec<int>  > class_indices;  // Indices of samples in each class.
+    for (int i = 0; i < source->length(); i++) {
+        source->getExample(i, input, target, weight);
+        int c = int(round(target[0]));
+        if (c >= class_indices.length()) {
+            int n_to_add = c - class_indices.length() + 1;
+            for (int j = 0; j < n_to_add; j++)
+                class_indices.append(TVec<int>());
+        }
+        class_indices[c].append(i);
+        indices.append(i);
+    }
+    int max_n = -1;
+    for (int c = 0; c < class_indices.length(); c++)
+        if (class_indices[c].length() > max_n)
+            max_n = class_indices[c].length();
+    for (int c = 0; c < class_indices.length(); c++) {
+        int n_replicated = max_n - class_indices[c].length();
+        for (int i = 0; i < n_replicated; i++) {
+            indices.append(class_indices[c][i % class_indices[c].length()]);
+        }
+    }
+
+    // Shuffle data.
+    random_gen->manual_seed(seed);
+    random_gen->shuffleElements(indices);
+    
+    // Re-build since indices have changed.
+    inherited::build();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void ReplicateSamplesVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("ReplicateSamplesVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/ReplicateSamplesVMatrix.h
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.h	2008-02-14 17:00:42 UTC (rev 8512)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.h	2008-02-14 21:15:53 UTC (rev 8513)
@@ -0,0 +1,132 @@
+// -*- C++ -*-
+
+// ReplicateSamplesVMatrix.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file ReplicateSamplesVMatrix.h */
+
+
+#ifndef ReplicateSamplesVMatrix_INC
+#define ReplicateSamplesVMatrix_INC
+
+#include <plearn/math/PRandom.h>
+#include <plearn/vmat/SelectRowsVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ReplicateSamplesVMatrix : public SelectRowsVMatrix
+{
+    typedef SelectRowsVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int32_t seed;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ReplicateSamplesVMatrix();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(ReplicateSamplesVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    PP<PRandom> random_gen;
+    
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ReplicateSamplesVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Fri Feb 15 16:38:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Feb 2008 16:38:23 +0100
Subject: [Plearn-commits] r8514 - trunk/plearn/math
Message-ID: <200802151538.m1FFcNGX023046@sheep.berlios.de>

Author: nouiz
Date: 2008-02-15 16:38:21 +0100 (Fri, 15 Feb 2008)
New Revision: 8514

Modified:
   trunk/plearn/math/TMat_maths_impl.h
   trunk/plearn/math/stats_utils.cc
Log:
-bugfix in max_cdf_diff, now work if one or both vector are empty
-bugfix in max_cdf_diff, now work correctly if both vector are the same.
-made the Kologorov Smirnov test with matrix work with missing value.


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-02-14 21:15:53 UTC (rev 8513)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-02-15 15:38:21 UTC (rev 8514)
@@ -1825,6 +1825,8 @@
     return result;
 }
 
+//! @ return a new array that contain only the non-missing value
+//! @ see remove_missing_inplace for inplace version
 template<class T>
 TVec<T> remove_missing(const TVec<T>& vec)
 {
@@ -1843,6 +1845,35 @@
     return result;
 }
 
+//! remove all missing value inplace while keeping the order
+template<class T>
+void remove_missing_inplace(TVec<T>& v)
+{   
+    int n_non_missing=0;
+    int next_non_missing=1;
+    T* d = v.data();
+    for(;;)
+    {
+        while(n_non_missing<v.length()&&!is_missing(d[n_non_missing]))
+        {
+            n_non_missing++;next_non_missing++;
+        }
+        if(n_non_missing>=v.length())
+            return;
+        while(next_non_missing<v.length()&&is_missing(d[next_non_missing]))
+            next_non_missing++;
+        if(next_non_missing>=v.length())
+        {
+            v.resize(n_non_missing);
+            return;
+        }
+        else
+        {
+            pl_swap(d[n_non_missing],d[next_non_missing]);
+        }
+    }
+}
+
 //! Transform a vector of T into a vector of U through a unary function.
 //! Note: output type need not be specified in this case
 template<class T, class U, class V>

Modified: trunk/plearn/math/stats_utils.cc
===================================================================
--- trunk/plearn/math/stats_utils.cc	2008-02-14 21:15:53 UTC (rev 8513)
+++ trunk/plearn/math/stats_utils.cc	2008-02-15 15:38:21 UTC (rev 8514)
@@ -183,6 +183,14 @@
     int i2=0;
     real maxdiff = 0;
 
+    if(n1==0 && n2==0)
+    {
+        PLWARNING("In max_cdf_diff(Vec, Vec) - both vector are empty!");
+        return 0;
+    }
+    else if (n1==0 || n2==0)
+        return 1;
+
     for(;;)
     {
 
@@ -192,6 +200,15 @@
             if(i1+1==n1)
                 break;
         }
+        else if(fast_exact_is_equal(v1[i1],v2[i2]))
+        {
+            i1++;i2++;
+            if(i2==n2)
+                break;
+            else if(i1+1==n1)
+                break;
+            continue;
+        }
         else
         {
             i2++;
@@ -278,6 +295,7 @@
     p_value = KS_test(D,N,conv);
 }
 
+//! This version work with nan value
 void KS_test(VMat& m1, VMat& m2, int conv, Vec& Ds, Vec& p_values)
 {
     m1->compatibleSizeError(m2);
@@ -289,6 +307,8 @@
         Vec row2(m2->length());
         m1->getColumn(col,row1);
         m2->getColumn(col,row2);
+        remove_missing_inplace(row1);
+        remove_missing_inplace(row2);
         real D;
         real p_value;
         KS_test(row1,row2,conv,D,p_value);



From nouiz at mail.berlios.de  Fri Feb 15 17:26:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Feb 2008 17:26:52 +0100
Subject: [Plearn-commits] r8515 - in trunk/plearn: misc vmat
Message-ID: <200802151626.m1FGQqXR027871@sheep.berlios.de>

Author: nouiz
Date: 2008-02-15 17:26:50 +0100 (Fri, 15 Feb 2008)
New Revision: 8515

Modified:
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
-added function max_fieldnames_size in VMatrix
-VMatrix::compareStats now return the result instead of printing to pout
-in vmatmain.cc
  -use the new version of VMatrix::compareStats 
  -added an option compare_stats_ks, that do the Kologorov Smirnov two sample test
  -added an optino compare_stats_desjardins that will be used in the prototype for desjardins


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-15 15:38:21 UTC (rev 8514)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-15 16:26:50 UTC (rev 8515)
@@ -41,6 +41,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/base/lexical_cast.h>
 #include <plearn/math/StatsCollector.h>
+#include <plearn/math/stats_utils.h>
 #include <plearn/vmat/ConcatColumnsVMatrix.h>
 #include <plearn/vmat/SelectColumnsVMatrix.h>
 #include <plearn/vmat/SubVMatrix.h>
@@ -495,6 +496,10 @@
             "       A column separator can be provided. By default, \"\t\" is used.\n"
             "   or: vmat compare_stats <dataset1> <dataset2> [stdev threshold] [missing threshold]\n"
             "       Will compare stats from dataset1 to dataset2\n\n"
+            "   or: vmat compare_stats_ks <dataset1> <dataset2> [--mat_to_mem]"
+            "       Will compare stats from dataset2 to dataset2 with "
+            "Kolmogorov-Smirnov 2 samples statistic\n\n"
+
             "<dataset> is a parameter understandable by getDataSet. This includes \n"
             "all matrix file formats. Type 'vmat help dataset' to see what other\n"
             "<dataset> strings are available." << endl;
@@ -954,7 +959,7 @@
     else if(command=="compare_stats")
     {
         if(!(argc==4||argc==5||argc==6))
-            PLERROR("vmat compare_stats must be used that way: vmat compare_stats <dataset1> <dataset2> [stderror threshold] [missing threshold]");
+            PLERROR("vmat compare_stats must be used that way: vmat compare_stats <dataset1> <dataset2> [[stderror threshold [missing threshold]]");
 
         VMat m1 = getVMat(argv[2], indexf);
         VMat m2 = getVMat(argv[3], indexf);
@@ -967,17 +972,298 @@
             stderror_threshold=toreal(argv[4]);
         if(argc>5)
             missing_threshold=toreal(argv[5]);
-        real sumdiff_stderr = 0;
-        real sumdiff_missing = 0;
+        Vec missing(m1->width());
+        Vec stderr(m1->width());
+
         pout << "Test of difference that suppose gaussiane variable"<<endl;
-        int diff = m1->compareStats(m2, stderror_threshold, missing_threshold,
-                                    &sumdiff_stderr, &sumdiff_missing);
-        pout<<"There are "<<diff<<"/"<<m1.width()
+        m1->compareStats(m2, stderror_threshold, missing_threshold,
+                         stderr, missing);
+
+        Mat score(m1->width(),3);
+
+        for(int col = 0;col<m1->width();col++)
+        {
+            score(col,0)=col;
+            score(col,1)=stderr[col];
+            score(col,2)=missing[col];
+        }
+        
+        int nbdiff = 0;
+
+        pout<<"Print the field that do not pass the threshold sorted by the stderror"<<endl;
+        sortRows(score,1,false);
+        for(int i=0;i<score.length();i++)
+        {
+            if(score(i,1)>stderror_threshold)
+            {
+                const StatsCollector tstats = m1->getStats(i);
+                const StatsCollector lstats = m2->getStats(i);
+                real tmean = tstats.mean();
+                real lmean = lstats.mean();
+                real tstderror = sqrt(pow(tstats.stderror(), 2) + 
+                                      pow(lstats.stderror(), 2));
+
+                pout<<i<<"("<<m1->fieldName(int(round(score(i,0))))<<")"
+                    <<" differ by "<<score(i,1)<<" stderror."
+                    <<" The mean is "<<lmean<<" while the target mean is "<<tmean
+                    <<" and the used stderror is "<<tstderror<<endl;
+                nbdiff++;
+            }
+        }
+
+        cout<<"Print the field that do not pass the threshold sorted by the missing error"<<endl;
+        sortRows(score,2,false);
+        for(int i=0;i<score.length();i++)
+        {
+            if(score(i,2)>missing_threshold)
+            {
+                const StatsCollector tstats = m1->getStats(i);
+                const StatsCollector lstats = m2->getStats(i);
+                real tmissing = tstats.nmissing()/tstats.n();
+                real lmissing = lstats.nmissing()/lstats.n();
+                pout<<i<<"("<<m1->fieldName(int(round(score(i,0))))<<")"
+                    <<" The missing stats difference is "<< score(i,2)
+                    <<". Their is "<<lmissing<<" missing while target have "
+                    <<tmissing<<" missing."<<endl;
+                nbdiff++;
+            }
+        }
+
+        pout<<"There are "<<nbdiff<<"/"<<m1.width()
             <<" fields that have different stats"<<endl;
-        pout <<"The sum of stderror difference is "<<sumdiff_stderr<<endl;
-        pout <<"The sum of missing difference is "<<sumdiff_missing<<endl;
 
     }
+    else if(command=="compare_stats_ks")
+    {
+        bool err = false;
+        real threashold = REAL_MAX;
+        bool mat_to_mem = false;
+        if(argc<4||argc>6)
+            err = true;
+        if(argc==5)
+        {
+            if(argv[4]==string("--mat_to_mem"))
+                mat_to_mem=true;
+            else if(!pl_isnumber(string(argv[4]),&threashold))
+                err = true;
+        }
+        else if(argc==6)
+        {
+             if(argv[5]!=string("--mat_to_mem"))
+                 err = true;
+             else if(!pl_isnumber(string(argv[4]),&threashold))
+                 err = true;
+        }
+        if(err)
+            PLERROR("vmat compare_stats_ks must be used that way:"
+                    " vmat compare_stats_ks <dataset1> <dataset2> [threashold]"
+                    " [--mat_to_mem]");
+
+        VMat m1 = getVMat(argv[2], indexf);
+        VMat m2 = getVMat(argv[3], indexf);
+        if(mat_to_mem)
+        {
+            m1.precompute();
+            m2.precompute();
+        }
+
+        m1->compatibleSizeError(m2);
+        int pc_value_99=0;
+        int pc_value_95=0;
+        int pc_value_90=0;
+        int pc_value_0=0;
+
+        uint size_fieldnames=m1->max_fieldnames_size();
+
+        Vec Ds(m1->width());
+        Vec p_values(m1->width());
+        KS_test(m1,m2,10,Ds,p_values);
+        Mat score(m1->width(),3);
+            
+        for(int col = 0;col<m1->width();col++)
+        {
+            score(col,0)=col;
+            score(col,1)=Ds[col];
+            real p_value = p_values[col];
+            score(col,2)=p_value;
+            if(p_value>0.99)
+                pc_value_99++;
+            if(p_value>0.95)
+                pc_value_95++;
+            if(p_value>0.90)
+                pc_value_90++;
+            else
+                pc_value_0++;
+        }
+
+        sortRows(score,2,false);
+        pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
+        if(threashold==REAL_MAX)
+            pout<<"Variables that are under the threashold"<<endl;
+        pout<<"Sorted by p_value"<<endl;
+        cout << std::left << setw(8) << "# "
+             << setw(size_fieldnames) << " fieldname " << std::right
+             << setw(15) << " D"
+             << setw(15) << " p_value"
+             <<endl;
+        for(int col=0;col<score.length();col++)
+        {
+            if(threashold>=score(col,2))
+                cout << std::left << setw(8) << tostring(col)+"/"+tostring(score(col,0))
+                     << setw(size_fieldnames) << m1->fieldName(int(round(score(col,0))))
+                     << std::right
+                     << setw(15) << score(col,1)
+                     << setw(15) << score(col,2)
+                     <<endl;
+        }
+        if(threashold==REAL_MAX)
+        {
+            pout << "99% cutoff: "<<pc_value_99<<endl;
+            pout << "95% cutoff: "<<pc_value_95<<endl;
+            pout << "90% cutoff: "<<pc_value_90<<endl;
+            pout << "0-90% cutoff: "<<pc_value_0<<endl;
+        }
+        pout <<"Kolmogorow Smirnow two sample test end"<<endl<<endl;
+    }
+    else if(command=="compare_stats_desjardins")
+    {      
+        if(argc!=8)
+            PLERROR("vmat compare_stats_desjardins must be used that way:"
+                    " vmat compare_stats_desjardins <orig dataset1> <orig dataset2> <new dataset3> <ks_threashold> <stderror_threashold> <missing_threashold>");
+        VMat m1 = getVMat(argv[2], indexf);
+        VMat m2 = getVMat(argv[3], indexf);
+        VMat m3 = getVMat(argv[4], indexf);
+        real ks_threashold = toreal(argv[5]);
+
+        m3->compatibleSizeError(m1);
+        m3->compatibleSizeError(m2);
+
+        Vec Ds(m1->width());
+        Vec p_values(m1->width());
+        KS_test(m1,m3,10,Ds,p_values);
+        Mat score(m1->width(),3);
+
+        uint size_fieldnames=m1->max_fieldnames_size();
+
+        for(int col = 0;col<m1->width();col++)
+        {
+            score(col,0)=col;
+            score(col,1)=Ds[col];
+            real p_value = p_values[col];
+            score(col,2)=p_value;
+        }
+
+        KS_test(m2,m3,10,Ds,p_values);
+
+        for(int col = 0;col<m1->width();col++)
+        {
+            if(p_values[col]>score(col,2))
+            {
+                score(col,1)=Ds[col];
+                score(col,2)=p_values[col];
+            }
+        }
+
+        sortRows(score,2,false);
+        pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
+        pout<<"Variables that are under the kd_threashold"<<endl;
+        pout<<"Sorted by p_value"<<endl;
+        cout << std::left << setw(8) << "# "
+             << setw(size_fieldnames) << " fieldname " << std::right
+             << setw(15) << " D"
+             << setw(15) << " p_value"
+             <<endl;
+        int threashold_fail = 0;
+        for(int col=0;col<score.length();col++)
+        {
+            if(ks_threashold>=score(col,2))
+            {
+                cout << std::left << setw(8) << tostring(col)+"/"+tostring(score(col,0))
+                     << setw(size_fieldnames) << m1->fieldName(int(round(score(col,0))))
+                     << std::right
+                     << setw(15) << score(col,1)
+                     << setw(15) << score(col,2)
+                     <<endl;
+            }
+            threashold_fail++;
+        }
+        pout << "Their is "<<threashold_fail<<" variable that are under the threashold"<<endl;
+        pout <<"Kolmogorow Smirnow two sample test end"<<endl<<endl;
+
+
+//         real stderror_threshold = 1;
+//         real missing_threshold = 10;
+//         stderror_threshold=toreal(argv[6]);
+//         missing_threshold=toreal(argv[7]);
+
+//         pout << "Test of difference that suppose gaussiane variable"<<endl;
+//         pout << "Comparing with dataset1"<<endl;
+
+//         m3->compareStats(m1, stderror_threshold, missing_threshold,
+//                          stderr, missing);
+//         pout << "Comparing with dataset2"<<endl;
+//         m3->compareStats(m2, stderror_threshold, missing_threshold,
+//                          stderr, missing);
+//         pout<<"There are "<<diff<<"/"<<m1.width()
+//             <<" fields that have different stats"<<endl;
+    }
+    else if(command=="characterize")
+    {
+        if(argc!=3)
+            PLERROR("The command 'vmat characterize' must be used that way: vmat caracterize <dataset1>");
+        VMat m1 = getVMat(argv[2], indexf);
+        TVec<StatsCollector> stats = 
+            m1->getStats();//"stats_all.psave",-1,true);
+        TVec<string> caracs;
+        uint size_fieldnames=m1->max_fieldnames_size();
+
+        for(int i=0;i<stats.size();i++)
+        {
+            StatsCollector& stat=stats[i];
+            string carac = tostring(i)+"\t"+left(m1->fieldName(i),size_fieldnames);
+            if(stat.isbinary())
+                carac += "\tBinary";
+            else if(stat.isinteger())
+                carac+="\tInteger";
+            else
+                carac+="\tReal";
+
+            //find is normal or not
+            int m=min(100,int(round(sqrt(stat.nnonmissing()))));
+            int nelem=int(stat.nnonmissing()/m);
+            int row=0;
+            real gsum=0;
+            for(int bloc=0;bloc<m;bloc++)
+            {
+                real bloc_sum=0;
+                for(int bloc_elem=0;bloc_elem<nelem;)
+                {
+                    real v = m1->get(row,i);
+                    if(is_missing(v))
+                        continue;
+                    else
+                    {
+                        bloc_elem++;
+                        row++;
+                        bloc_sum+=v;
+                    }
+                    
+                }
+                gsum+=pow((bloc_sum/nelem)-stat.mean(),2);
+            }
+            real s2=(stat.variance()/nelem);
+            real mu_square = stat.mean()*stat.mean();
+            real th = ((1./(m-1))*gsum)/s2;
+            real th2= mu_square+s2-12*mu_square*s2+mu_square*mu_square
+                * s2*s2;
+            bool b = th>(s2+2*th2)/s2;
+                carac+="\tnormal test value: "+tostring(th)+" "+tostring(th2)
+                + " "+tostring(b);
+            caracs.append(carac);
+            pout<<carac<<endl;
+        }
+
+    }
     else
         PLERROR("Unknown command : %s",command.c_str());
     return 0;

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-15 15:38:21 UTC (rev 8514)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-15 16:26:50 UTC (rev 8515)
@@ -1934,22 +1934,17 @@
 //////////////////
 // compareStats //
 //////////////////
-int VMatrix::compareStats(const VMat& target,
+void VMatrix::compareStats(const VMat& target,
                           const real stderror_threshold,
                           const real missing_threshold,
-                          real* sumdiff_stderr,
-                          real* sumdiff_missing) const
+                          Vec& stderr,
+                          Vec& missing) const
 {
     if(target->width()!=width())
         PLERROR("In VecStatsCollector:: compareStats() - This VMatrix has "
                 "width %d which differs from the target width of %d",
                 width(), target->width());
 
-    int nbdiff            = 0;
-    real sumdiff_stderr_  = 0;
-    real sumdiff_missing_ = 0;
-    Mat score(width(),3);
-
     for(int i=0;i<width();i++)
     {
         const StatsCollector tstats = target->getStats(i);
@@ -1970,8 +1965,6 @@
         }
         else if(isnan(th_missing))
             PLWARNING("In VMatrix::compareStats - should not happen!");
-        else
-            sumdiff_missing_ += th_missing;
         
         real tmean = tstats.mean();
         real lmean = lstats.mean();
@@ -1982,57 +1975,24 @@
             PLWARNING("In VMatrix::compareStats - field %d(%s) have a"
                       " stderror of 0 for both matrice.",
                       i, fieldName(i).c_str());
-        else
-            sumdiff_stderr_+=th_stderror;
-        score(i,0)=i;
-        score(i,1)=th_stderror;
-        score(i,2)=th_missing;
-
+        stderr[i]=th_stderror;
+        missing[i]=th_missing;
     }
-    pout<<"Print the field that do not pass the threshold sorted by the stderror"<<endl;
-    sortRows(score,1,false);
-    for(int i=0;i<score.length();i++)
-    {
-        if(score(i,1)>stderror_threshold)
-        {
-            const StatsCollector tstats = target->getStats(i);
-            const StatsCollector lstats = getStats(i);
-            real tmean = tstats.mean();
-            real lmean = lstats.mean();
-            real tstderror = sqrt(pow(tstats.stderror(), 2) + 
-                                  pow(lstats.stderror(), 2));
+    return;
+}
 
-            pout<<i<<"("<<fieldName(int(round(score(i,0))))<<")"
-                <<" differ by "<<score(i,1)<<" stderror."
-                <<" The mean is "<<lmean<<" while the target mean is "<<tmean
-                <<" and the used stderror is "<<tstderror<<endl;
-            nbdiff++;
-        }
-    }
-
-    cout<<"Print the field that do not pass the threshold sorted by the missing error"<<endl;
-    sortRows(score,2,false);
-    for(int i=0;i<score.length();i++)
-    {
-        if(score(i,2)>missing_threshold)
-        {
-            const StatsCollector tstats = target->getStats(i);
-            const StatsCollector lstats = getStats(i);
-            real tmissing = tstats.nmissing()/tstats.n();
-            real lmissing = lstats.nmissing()/lstats.n();
-            pout<<i<<"("<<fieldName(int(round(score(i,0))))<<")"
-                <<" The missing stats difference is "<< score(i,2)
-                <<". Their is "<<lmissing<<" missing while target have "
-                <<tmissing<<" missing."<<endl;
-            nbdiff++;
-        }
-    }
-    if(sumdiff_stderr!=NULL)
-        *sumdiff_stderr = sumdiff_stderr_;
-    if(sumdiff_missing!=NULL)
-        *sumdiff_missing = sumdiff_missing_;
-    return nbdiff;
+/////////////////////////
+// max_fieldnames_size //
+/////////////////////////
+int VMatrix::max_fieldnames_size() const
+{
+    uint size_fieldnames=0;
+    for(int i=0;i<width();i++)
+        if(fieldName(i).size()>size_fieldnames)
+            size_fieldnames=fieldName(i).size();
+    return size_fieldnames;
 }
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-15 15:38:21 UTC (rev 8514)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-15 16:26:50 UTC (rev 8515)
@@ -618,12 +618,16 @@
      * @param sumdiff_missing The sum of all variable differences of missing
      * @return The number of differences that were found
      */
-    int compareStats(const VMat& target,
-                     const real stderror_threshold = 2,
-                     const real missing_threshold = 2,
-                     real* sumdiff_stderr = NULL,
-                     real* sumdiff_missing = NULL) const;
+    void compareStats(const VMat& target,
+                     const real stderror_threshold ,
+                     const real missing_threshold,
+                     Vec& stderr,
+                     Vec& missing) const;
 
+    /**
+     * @return The size of the longuest fieldname
+     */
+    int max_fieldnames_size() const;
 
     /**
      *  Returns the bounding box of the data, as a vector of min:max pairs.  If



From nouiz at mail.berlios.de  Fri Feb 15 17:30:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Feb 2008 17:30:39 +0100
Subject: [Plearn-commits] r8516 - trunk
Message-ID: <200802151630.m1FGUdCc028353@sheep.berlios.de>

Author: nouiz
Date: 2008-02-15 17:30:39 +0100 (Fri, 15 Feb 2008)
New Revision: 8516

Modified:
   trunk/pymake.config.model
Log:
-corrected linking flag for openmp
-added an gcc option ignore unknow pragma so that old version of gcc won't generate warning for openmp pragma


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-02-15 16:26:50 UTC (rev 8515)
+++ trunk/pymake.config.model	2008-02-15 16:30:39 UTC (rev 8516)
@@ -523,7 +523,7 @@
               description = 'compiling with g++, with no MPI support',
               compiler = 'g++',
               compileroptions = '-Wno-deprecated '+pedantic_mode+'-Wno-long-long -ftemplate-depth-100 ' \
-                      + gcc_opt_options,
+                      + gcc_opt_options+ " -Wno-unknown-pragmas",
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++' )
 
@@ -647,7 +647,7 @@
 pymakeOption( name = 'openmpgcc',
               description = 'vectorized with gcc compiler in opt mode',
               compileroptions = '-Wall -O3 -fopenmp -msse2',
-              linkeroptions = '-lgomp',
+              linkeroptions = '-fopenmp',
               cpp_definitions = ['NDEBUG'] )
 
 pymakeOption( name = 'pintel',



From tihocan at mail.berlios.de  Fri Feb 15 18:54:50 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Feb 2008 18:54:50 +0100
Subject: [Plearn-commits] r8517 - trunk/plearn_learners/distributions
Message-ID: <200802151754.m1FHsoMe006534@sheep.berlios.de>

Author: tihocan
Date: 2008-02-15 18:54:50 +0100 (Fri, 15 Feb 2008)
New Revision: 8517

Modified:
   trunk/plearn_learners/distributions/PDistribution.h
Log:
Minor modifications to fix some issues when writing subclasses

Modified: trunk/plearn_learners/distributions/PDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/PDistribution.h	2008-02-15 16:30:39 UTC (rev 8516)
+++ trunk/plearn_learners/distributions/PDistribution.h	2008-02-15 17:54:50 UTC (rev 8517)
@@ -129,8 +129,8 @@
     //! Learnt sizes of the 'predictor' and 'predicted' sizes. These are the
     //! options to use in PDistribution subclasses code. They always verify:
     //!     n_predictor + n_predicted == inputsize()
-    int n_predictor;
-    int n_predicted;
+    mutable int n_predictor;
+    mutable int n_predicted;
 
 public:
 
@@ -331,8 +331,12 @@
     virtual void generatePredictorGivenPredicted(Vec& x, const Vec& y);
 
     //! 'Get' accessor for n_predicted.
-    int getNPredicted() { return n_predicted; }
+    int getNPredicted() const { return n_predicted; }
 
+    //! 'Get' accessor for n_predictor.
+    int getNPredictor() const { return n_predictor; }
+
+
 };
 
 // Declares a few other classes and functions related to this class



From tihocan at mail.berlios.de  Fri Feb 15 19:04:28 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Feb 2008 19:04:28 +0100
Subject: [Plearn-commits] r8518 - in trunk: commands
	plearn_learners/distributions
Message-ID: <200802151804.m1FI4SRW015137@sheep.berlios.de>

Author: tihocan
Date: 2008-02-15 19:04:28 +0100 (Fri, 15 Feb 2008)
New Revision: 8518

Added:
   trunk/plearn_learners/distributions/MixtureDistribution.cc
   trunk/plearn_learners/distributions/MixtureDistribution.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added MixtureDistribution to represent mixtures of distributions (yep, that's revolutionary!)

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-02-15 17:54:50 UTC (rev 8517)
+++ trunk/commands/plearn_noblas_inc.h	2008-02-15 18:04:28 UTC (rev 8518)
@@ -185,6 +185,7 @@
 #include <plearn_learners/unsupervised/UniformizeLearner.h>
 
 // PDistribution
+#include <plearn_learners/distributions/MixtureDistribution.h>
 #include <plearn_learners/distributions/SpiralDistribution.h>
 #include <plearn_learners/distributions/UniformDistribution.h>
 

Added: trunk/plearn_learners/distributions/MixtureDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/MixtureDistribution.cc	2008-02-15 17:54:50 UTC (rev 8517)
+++ trunk/plearn_learners/distributions/MixtureDistribution.cc	2008-02-15 18:04:28 UTC (rev 8518)
@@ -0,0 +1,293 @@
+// -*- C++ -*-
+
+// MixtureDistribution.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file MixtureDistribution.cc */
+
+
+#include "MixtureDistribution.h"
+#include <plearn/math/TMat_maths.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    MixtureDistribution,
+    "Weighted mixture of n distributions.",
+    "Note that the weights are fixed and not learnt."
+);
+
+//////////////////////////
+// MixtureDistribution //
+//////////////////////////
+MixtureDistribution::MixtureDistribution()
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void MixtureDistribution::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "distributions", &MixtureDistribution::distributions,
+                  OptionBase::buildoption,
+        "Underlying distributions being mixed.");
+
+    declareOption(ol, "weights", &MixtureDistribution::weights,
+                  OptionBase::buildoption,
+        "Weights of the distributions (must sum to 1). If left empty, then\n"
+        "each distribution will be given a weight 1/number_of_distributions.");
+
+    // Now call the parent class' declareOptions().
+    inherited::declareOptions(ol);
+
+    // Hide unused options.
+
+    redeclareOption(ol, "predicted_size",
+                    &MixtureDistribution::predicted_size,
+                    OptionBase::nosave,
+        "Unused");
+
+    redeclareOption(ol, "predictor_part",
+                    &MixtureDistribution::predictor_part,
+                    OptionBase::nosave,
+        "Unused");
+
+    redeclareOption(ol, "predictor_size",
+                    &MixtureDistribution::predictor_size,
+                    OptionBase::nosave,
+        "Unused");
+
+
+}
+
+///////////
+// build //
+///////////
+void MixtureDistribution::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void MixtureDistribution::build_()
+{
+    if (distributions.isEmpty())
+        return;
+    if (weights.isEmpty()) {
+        int n = distributions.length();
+        weights.resize(n);
+        weights.fill(1 / real(n));
+    }
+    PLCHECK_MSG(weights.length() == distributions.length() &&
+                 is_equal(PLearn::sum(weights), 1),
+                 "There must be one weight for each distribution, and the "
+                 "weights must sum to 1");
+    getSizes();
+}
+
+/////////
+// cdf //
+/////////
+real MixtureDistribution::cdf(const Vec& y) const
+{
+    PLERROR("cdf not implemented for MixtureDistribution"); return 0;
+}
+
+/////////////////
+// expectation //
+/////////////////
+void MixtureDistribution::expectation(Vec& mu) const
+{
+    PLASSERT( !distributions.isEmpty() );
+    mu.resize(distributions[0]->getNPredicted());
+    mu.fill(0);
+    for (int i = 0; i < distributions.length(); i++) {
+        distributions[i]->expectation(work);
+        multiplyAcc(mu, work, weights[i]);
+    }
+}
+
+////////////
+// forget //
+////////////
+void MixtureDistribution::forget()
+{
+    for (int i = 0; i < distributions.length(); i++)
+        distributions[i]->forget();
+    inherited::forget();
+    getSizes();
+}
+
+//////////////
+// generate //
+//////////////
+void MixtureDistribution::generate(Vec& y) const
+{
+    int j = random_gen->multinomial_sample(weights);
+    distributions[j]->generate(y);
+}
+
+//////////////
+// getSizes //
+//////////////
+void MixtureDistribution::getSizes() const {
+    PLASSERT( !distributions.isEmpty() );
+    n_predicted = distributions[0]->getNPredicted();
+    n_predictor = distributions[0]->getNPredictor();
+}
+
+/////////////////
+// log_density //
+/////////////////
+real MixtureDistribution::log_density(const Vec& y) const
+{
+    int n = distributions.length();
+    work.resize(n);
+    for (int i = 0; i < n; i++) {
+        work[i] = distributions[i]->log_density(y) + pl_log(weights[i]);
+    }
+    return logadd(work);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void MixtureDistribution::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("MixtureDistribution::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+////////////////////
+// resetGenerator //
+////////////////////
+void MixtureDistribution::resetGenerator(long g_seed)
+{
+    for (int i = 0; i < distributions.length(); i++)
+        distributions[i]->resetGenerator(g_seed);
+    inherited::resetGenerator(g_seed);
+}
+
+//////////////////
+// setPredictor //
+//////////////////
+void MixtureDistribution::setPredictor(const Vec& predictor, bool call_parent) const
+{
+    if (call_parent)
+        inherited::setPredictor(predictor, true);
+    for (int i = 0; i < distributions.length(); i++)
+        distributions[i]->setPredictor(predictor, call_parent);
+    getSizes();
+}
+
+////////////////////////////////
+// setPredictorPredictedSizes //
+////////////////////////////////
+bool MixtureDistribution::setPredictorPredictedSizes(int the_predictor_size,
+                                               int the_predicted_size,
+                                               bool call_parent)
+{
+    bool sizes_have_changed = false;
+    if (call_parent)
+        sizes_have_changed = inherited::setPredictorPredictedSizes(
+                the_predictor_size, the_predicted_size, true);
+    for (int i = 0; i < distributions.length(); i++)
+        distributions[i]->setPredictorPredictedSizes(the_predictor_size,
+                                                     the_predicted_size,
+                                                     call_parent);
+    getSizes();
+    // Returned value.
+    return sizes_have_changed;
+}
+
+/////////////////
+// survival_fn //
+/////////////////
+real MixtureDistribution::survival_fn(const Vec& y) const
+{
+    PLERROR("survival_fn not implemented for MixtureDistribution"); return 0;
+}
+
+///////////
+// train //
+///////////
+void MixtureDistribution::train()
+{
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    PLCHECK( nstages == 1 && stage == 0 );
+    for (int i = 0; i < distributions.length(); i++)
+        distributions[i]->train();
+    stage = 1;
+    getSizes();
+}
+
+//////////////
+// variance //
+//////////////
+void MixtureDistribution::variance(Mat& covar) const
+{
+    PLERROR("variance not implemented for MixtureDistribution");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/distributions/MixtureDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/MixtureDistribution.h	2008-02-15 17:54:50 UTC (rev 8517)
+++ trunk/plearn_learners/distributions/MixtureDistribution.h	2008-02-15 18:04:28 UTC (rev 8518)
@@ -0,0 +1,234 @@
+// -*- C++ -*-
+
+// MixtureDistribution.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file MixtureDistribution.h */
+
+
+#ifndef MixtureDistribution_INC
+#define MixtureDistribution_INC
+
+#include <plearn_learners/distributions/PDistribution.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class MixtureDistribution : public PDistribution
+{
+    typedef PDistribution inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    TVec< PP<PDistribution> > distributions;
+    Vec weights;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    MixtureDistribution();
+
+    //#####  PDistribution Member Functions  ##################################
+
+    //! Return log of probability density log(p(y | x)).
+    virtual real log_density(const Vec& y) const;
+
+    //! Return survival function: P(Y>y | x).
+    virtual real survival_fn(const Vec& y) const;
+
+    //! Return cdf: P(Y<y | x).
+    virtual real cdf(const Vec& y) const;
+
+    //! Return E[Y | x].
+    virtual void expectation(Vec& mu) const;
+
+    //! Return Var[Y | x].
+    virtual void variance(Mat& cov) const;
+
+    //! Return a pseudo-random sample generated from the conditional
+    //! distribution, of density p(y | x).
+    virtual void generate(Vec& y) const;
+
+    //### Override this method if you need it (and if your distribution can
+    //### handle it. Default version calls PLERROR.
+    //! Generates a pseudo-random sample x from the reversed conditional
+    //! distribution, of density p(x | y) (and NOT p(y | x)).
+    //! i.e., generates a "predictor" part given a "predicted" part, regardless
+    //! of any previously set predictor.
+    // virtual void generatePredictorGivenPredicted(Vec& x, const Vec& y);
+
+    //! Reset the random number generator used by generate() using the
+    //! given seed.
+    virtual void resetGenerator(long g_seed);
+
+    //! Set the 'predictor' and 'predicted' sizes for this distribution.
+    //### See help in PDistribution.h.
+    virtual bool setPredictorPredictedSizes(int the_predictor_size,
+                                            int the_predicted_size,
+                                            bool call_parent = true);
+
+    //! Set the value for the predictor part of a conditional probability.
+    //### See help in PDistribution.h.
+    virtual void setPredictor(const Vec& predictor, bool call_parent = true)
+                              const;
+
+    // ### These methods may be overridden for efficiency purpose:
+    /*
+    //### Default version calls exp(log_density(y))
+    //! Return probability density p(y | x)
+    virtual real density(const Vec& y) const;
+
+    //### Default version calls setPredictorPredictedSises(0,-1) and generate
+    //! Generates a pseudo-random sample (x,y) from the JOINT distribution,
+    //! of density p(x, y)
+    //! i.e., generates a predictor and a predicted part, regardless of any
+    //! previously set predictor.
+    virtual void generateJoint(Vec& xy);
+
+    //### Default version calls generateJoint and discards y
+    //! Generates a pseudo-random sample x from the marginal distribution of
+    //! predictors, of density p(x),
+    //! i.e., generates a predictor part, regardless of any previously set
+    //! predictor.
+    virtual void generatePredictor(Vec& x);
+
+    //### Default version calls generateJoint and discards x
+    //! Generates a pseudo-random sample y from the marginal distribution of
+    //! predicted parts, of density p(y) (and NOT p(y | x)).
+    //! i.e., generates a predicted part, regardless of any previously set
+    //! predictor.
+    virtual void generatePredicted(Vec& y);
+    */
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    // ### Default version of inputsize returns learner->inputsize()
+    // ### If this is not appropriate, you should uncomment this and define
+    // ### it properly in the .cc
+    // virtual int inputsize() const;
+
+    /**
+     * (Re-)initializes the PDistribution in its fresh state (that state may
+     * depend on the 'seed' option).  And sets 'stage' back to 0 (this is the
+     * stage of a fresh learner!).
+     * ### You may remove this method if your distribution does not
+     * ### implement it.
+     */
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage == nstages, updating the train_stats collector with training
+    //! costs measured on-line in the process.
+    // ### You may remove this method if your distribution does not
+    // ### implement it.
+    virtual void train();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(MixtureDistribution);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Re-obtain the sizes of the predictor and predicted parts from the first
+    //! distribution in the 'distributions' vector.
+    //! This method is used to re-obtain sizes after things may have changed
+    //! (e.g. after a build(), forget() or train()).
+    void getSizes() const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    //! Vector to store temporary data.
+    mutable Vec work;
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(MixtureDistribution);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Fri Feb 15 19:04:58 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 15 Feb 2008 19:04:58 +0100
Subject: [Plearn-commits] r8519 - trunk/scripts/Skeletons
Message-ID: <200802151804.m1FI4wZo015598@sheep.berlios.de>

Author: tihocan
Date: 2008-02-15 19:04:58 +0100 (Fri, 15 Feb 2008)
New Revision: 8519

Modified:
   trunk/scripts/Skeletons/PDistribution.cc
   trunk/scripts/Skeletons/PDistribution.h
   trunk/scripts/Skeletons/UnconditionalDistribution.cc
   trunk/scripts/Skeletons/UnconditionalDistribution.h
Log:
Fixed skeletons to match the true PLearn class

Modified: trunk/scripts/Skeletons/PDistribution.cc
===================================================================
--- trunk/scripts/Skeletons/PDistribution.cc	2008-02-15 18:04:28 UTC (rev 8518)
+++ trunk/scripts/Skeletons/PDistribution.cc	2008-02-15 18:04:58 UTC (rev 8519)
@@ -151,7 +151,7 @@
 ////////////////////
 // resetGenerator //
 ////////////////////
-void DERIVEDCLASS::resetGenerator(long g_seed) const
+void DERIVEDCLASS::resetGenerator(long g_seed)
 {
     PLERROR("resetGenerator not implemented for DERIVEDCLASS");
 }

Modified: trunk/scripts/Skeletons/PDistribution.h
===================================================================
--- trunk/scripts/Skeletons/PDistribution.h	2008-02-15 18:04:28 UTC (rev 8518)
+++ trunk/scripts/Skeletons/PDistribution.h	2008-02-15 18:04:58 UTC (rev 8519)
@@ -67,7 +67,7 @@
     //### random_gen->manual_seed(g_seed) if g_seed !=0
     //! Reset the random number generator used by generate() using the
     //! given seed.
-    // virtual void resetGenerator(long g_seed) const;
+    // virtual void resetGenerator(long g_seed);
 
     //! Set the 'predictor' and 'predicted' sizes for this distribution.
     //### See help in PDistribution.h.

Modified: trunk/scripts/Skeletons/UnconditionalDistribution.cc
===================================================================
--- trunk/scripts/Skeletons/UnconditionalDistribution.cc	2008-02-15 18:04:28 UTC (rev 8518)
+++ trunk/scripts/Skeletons/UnconditionalDistribution.cc	2008-02-15 18:04:58 UTC (rev 8519)
@@ -144,7 +144,7 @@
 ////////////////////
 // resetGenerator //
 ////////////////////
-void DERIVEDCLASS::resetGenerator(long g_seed) const
+void DERIVEDCLASS::resetGenerator(long g_seed)
 {
     PLERROR("resetGenerator not implemented for DERIVEDCLASS");
 }

Modified: trunk/scripts/Skeletons/UnconditionalDistribution.h
===================================================================
--- trunk/scripts/Skeletons/UnconditionalDistribution.h	2008-02-15 18:04:28 UTC (rev 8518)
+++ trunk/scripts/Skeletons/UnconditionalDistribution.h	2008-02-15 18:04:58 UTC (rev 8519)
@@ -56,7 +56,7 @@
 
     //! Reset the random number generator used by generate() using the
     //! given seed.
-    virtual void resetGenerator(long g_seed) const;
+    virtual void resetGenerator(long g_seed);
 
     // ### These methods may be overridden for efficiency purpose:
     /*



From nouiz at mail.berlios.de  Fri Feb 15 19:47:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Feb 2008 19:47:19 +0100
Subject: [Plearn-commits] r8520 - trunk/plearn/base
Message-ID: <200802151847.m1FIlJDv024334@sheep.berlios.de>

Author: nouiz
Date: 2008-02-15 19:47:18 +0100 (Fri, 15 Feb 2008)
New Revision: 8520

Modified:
   trunk/plearn/base/ProgressBar.h
Log:
-Added a function that update by one value. Usefull for parallelisation


Modified: trunk/plearn/base/ProgressBar.h
===================================================================
--- trunk/plearn/base/ProgressBar.h	2008-02-15 18:04:58 UTC (rev 8519)
+++ trunk/plearn/base/ProgressBar.h	2008-02-15 18:47:18 UTC (rev 8520)
@@ -168,7 +168,7 @@
     void operator()(uint32_t newpos){plugin->update(this,newpos);}
 
     void update(uint32_t newpos){plugin->update(this,newpos);}
-
+    void updateone(){plugin->update(this,currentpos+1);}
     // this function assumes plugin is always a valid object (it is created statically in the .cc)
     static void setPlugin(PP<ProgressBarPlugin> plugin_) { plugin = plugin_; }
     static PP<ProgressBarPlugin> getCurrentPlugin();



From nouiz at mail.berlios.de  Fri Feb 15 20:14:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 15 Feb 2008 20:14:56 +0100
Subject: [Plearn-commits] r8521 - in trunk/plearn: misc vmat
Message-ID: <200802151914.m1FJEuEZ027011@sheep.berlios.de>

Author: nouiz
Date: 2008-02-15 20:14:55 +0100 (Fri, 15 Feb 2008)
New Revision: 8521

Modified:
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
now plearn vmat stats print progress_report


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-15 18:47:18 UTC (rev 8520)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-15 19:14:55 UTC (rev 8521)
@@ -258,7 +258,7 @@
 void displayBasicStats(VMat vm)
 {
     int nfields = vm.width();
-    TVec<StatsCollector> stats = vm->getStats();        
+    TVec<StatsCollector> stats = vm->getStats(true);        
 
     // find longest field name
     size_t fieldlen = 0;

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-15 18:47:18 UTC (rev 8520)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-15 19:14:55 UTC (rev 8521)
@@ -1408,10 +1408,11 @@
 //////////////
 // getStats //
 //////////////
-TVec<StatsCollector> VMatrix::getStats() const
+TVec<StatsCollector> VMatrix::getStats(bool progress_bar) const
 {
     if(!field_stats)
-        field_stats = getPrecomputedStatsFromFile("stats.psave", 2000, false);
+        field_stats = getPrecomputedStatsFromFile("stats.psave", 2000, 
+                                                  progress_bar);
     return field_stats;
 }
 

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-15 18:47:18 UTC (rev 8520)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-15 19:14:55 UTC (rev 8521)
@@ -595,7 +595,7 @@
      * stats.psave file (if the file does not exist, a default version is
      * automatically created).
      */
-    TVec<StatsCollector> getStats() const;
+    TVec<StatsCollector> getStats(bool progress_bar=false) const;
 
     //! Generic function to obtain the statistics from a given file in the
     //! metadatadir. If this file does not exist, statistics are computed and



From tihocan at mail.berlios.de  Mon Feb 18 16:28:55 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 18 Feb 2008 16:28:55 +0100
Subject: [Plearn-commits] r8522 - trunk/scripts/Skeletons
Message-ID: <200802181528.m1IFStaU013924@sheep.berlios.de>

Author: tihocan
Date: 2008-02-18 16:28:55 +0100 (Mon, 18 Feb 2008)
New Revision: 8522

Modified:
   trunk/scripts/Skeletons/UnconditionalDistribution.h
Log:
Fixed include

Modified: trunk/scripts/Skeletons/UnconditionalDistribution.h
===================================================================
--- trunk/scripts/Skeletons/UnconditionalDistribution.h	2008-02-15 19:14:55 UTC (rev 8521)
+++ trunk/scripts/Skeletons/UnconditionalDistribution.h	2008-02-18 15:28:55 UTC (rev 8522)
@@ -1,7 +1,7 @@
 #ifndef DERIVEDCLASS_INC
 #define DERIVEDCLASS_INC
 
-#include <plearn_learners/distributions/PDistribution.h>
+#include <plearn_learners/distributions/UnconditionalDistribution.h>
 
 namespace PLearn {
 



From nouiz at mail.berlios.de  Mon Feb 18 17:37:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 18 Feb 2008 17:37:07 +0100
Subject: [Plearn-commits] r8523 - in trunk/plearn: math misc
Message-ID: <200802181637.m1IGb783023827@sheep.berlios.de>

Author: nouiz
Date: 2008-02-18 17:37:07 +0100 (Mon, 18 Feb 2008)
New Revision: 8523

Modified:
   trunk/plearn/math/stats_utils.cc
   trunk/plearn/math/stats_utils.h
   trunk/plearn/misc/vmatmain.cc
Log:
-Added progress bar to kologorov smirnov test on matrix
-parallelized this function with openmp


Modified: trunk/plearn/math/stats_utils.cc
===================================================================
--- trunk/plearn/math/stats_utils.cc	2008-02-18 15:28:55 UTC (rev 8522)
+++ trunk/plearn/math/stats_utils.cc	2008-02-18 16:37:07 UTC (rev 8523)
@@ -296,15 +296,26 @@
 }
 
 //! This version work with nan value
-void KS_test(VMat& m1, VMat& m2, int conv, Vec& Ds, Vec& p_values)
+void KS_test(const VMat& m1, const VMat& m2, const int conv, Vec& Ds, Vec& p_values,
+             const bool report_progress)
 {
     m1->compatibleSizeError(m2);
     Ds.resize(m1->width());
     p_values.resize(m1->width());
-    for(int col = 0;col<m1->width();col++)
+    PP<ProgressBar> pbar;
+    if (report_progress)
+        pbar = new ProgressBar("Computing Kologorov Smirnow two sample test",
+                               m1->width());
+
+#pragma omp parallel default(none) shared(pbar)
     {
         Vec row1(m1->length());
         Vec row2(m2->length());
+#pragma omp for
+    for(int col = 0;col<m1->width();col++)
+    {
+        row1->resize(m1->length());
+        row2->resize(m2->length());
         m1->getColumn(col,row1);
         m2->getColumn(col,row2);
         remove_missing_inplace(row1);
@@ -314,7 +325,12 @@
         KS_test(row1,row2,conv,D,p_value);
         Ds[col]=D;
         p_values[col]=p_value;
+        if (report_progress)
+#pragma omp critical
+            pbar->updateone();
+
     }
+    }
 }
 real KS_test(Vec& v1, Vec& v2, int conv)
 {

Modified: trunk/plearn/math/stats_utils.h
===================================================================
--- trunk/plearn/math/stats_utils.h	2008-02-18 15:28:55 UTC (rev 8522)
+++ trunk/plearn/math/stats_utils.h	2008-02-18 16:37:07 UTC (rev 8523)
@@ -147,7 +147,7 @@
  * Returns result of Kolmogorov-Smirnov test for each pair of variable
  * between the two VMat
  */
-void KS_test(VMat& m1, VMat& m2, int conv, Vec& Ds, Vec& p_values);
+void KS_test(const VMat& m1, const VMat& m2, const int conv, Vec& Ds, Vec& p_values, const bool report_progress = false);
 
 /**
  * Given two paired sets u and v of n measured values, the paired t-test 

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-18 15:28:55 UTC (rev 8522)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-18 16:37:07 UTC (rev 8523)
@@ -1077,7 +1077,7 @@
 
         Vec Ds(m1->width());
         Vec p_values(m1->width());
-        KS_test(m1,m2,10,Ds,p_values);
+        KS_test(m1,m2,10,Ds,p_values,true);
         Mat score(m1->width(),3);
             
         for(int col = 0;col<m1->width();col++)



From nouiz at mail.berlios.de  Mon Feb 18 18:25:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 18 Feb 2008 18:25:13 +0100
Subject: [Plearn-commits] r8524 - trunk/commands
Message-ID: <200802181725.m1IHPDJv028012@sheep.berlios.de>

Author: nouiz
Date: 2008-02-18 18:25:13 +0100 (Mon, 18 Feb 2008)
New Revision: 8524

Added:
   trunk/commands/plearn_desjardins.cc
Log:
Main file for the profile of cgi-desjardins


Added: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-02-18 16:37:07 UTC (rev 8523)
+++ trunk/commands/plearn_desjardins.cc	2008-02-18 17:25:13 UTC (rev 8524)
@@ -0,0 +1,121 @@
+// -*- C++ -*-
+
+// plearn.cc
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: plearn_light.cc 3995 2005-08-25 13:58:23Z chapados $
+ ******************************************************* */
+
+//! All includes should go into plearn_inc.h.
+#include "plearn_version.h"
+//#include "plearn_noblas_inc.h"
+/*****************
+ * Miscellaneous *
+ *****************/
+#include <plearn_learners/testers/PTester.h>
+
+/***********
+ * Command *
+ ***********/
+#include <commands/PLearnCommands/VMatCommand.h>
+//#include <commands/PLearnCommands/AutoRunCommand.h>
+//#include <commands/PLearnCommands/DiffCommand.h>
+//#include <commands/PLearnCommands/FieldConvertCommand.h>
+#include <commands/PLearnCommands/HelpCommand.h>
+//#include <commands/PLearnCommands/JulianDateCommand.h>
+//#include <commands/PLearnCommands/KolmogorovSmirnovCommand.h>
+#include <commands/PLearnCommands/LearnerCommand.h>
+//#include <commands/PLearnCommands/PairwiseDiffsCommand.h>
+#include <commands/PLearnCommands/ReadAndWriteCommand.h>
+#include <commands/PLearnCommands/RunCommand.h>
+//#include <commands/PLearnCommands/ServerCommand.h>
+//#include <commands/PLearnCommands/TestDependenciesCommand.h>
+//#include <commands/PLearnCommands/TestDependencyCommand.h>
+
+/************
+ * PLearner *
+ ************/
+#include <plearn_learners/generic/AddCostToLearner.h>
+
+/************
+ * Splitter *
+ ************/
+#include <plearn/vmat/FractionSplitter.h>
+
+/***********
+ * VMatrix *
+ ***********/
+#include <plearn/vmat/AddMissingVMatrix.h>
+#include <plearn/vmat/AutoVMatrix.h>
+#include <plearn/vmat/BootstrapVMatrix.h>
+#include <plearn/vmat/ConcatColumnsVMatrix.h>
+#include <plearn/vmat/DichotomizeVMatrix.h>
+#include <plearn/vmat/FilteredVMatrix.h>
+#include <plearn/vmat/GaussianizeVMatrix.h>
+#include <plearn/vmat/MemoryVMatrixNoSave.h>
+#include <plearn/vmat/MissingInstructionVMatrix.h>
+#include <plearn/vmat/ProcessingVMatrix.h>
+#include <plearn/vmat/TextFilesVMatrix.h>
+#include <plearn/vmat/TransposeVMatrix.h>
+#include <plearn/vmat/VariableDeletionVMatrix.h>
+#include <plearn/vmat/MeanMedianModeImputationVMatrix.h>
+
+#include <plearn_learners/meta/MultiClassAdaBoost.h>
+#include <plearn/vmat/MissingIndicatorVMatrix.h>
+
+
+#include "PLearnCommands/plearn_main.h"
+
+using namespace PLearn;
+
+int main(int argc, char** argv)
+{
+    cout<<"main begin"<<endl;
+    return plearn_main( argc, argv, 
+                        PLEARN_MAJOR_VERSION, 
+                        PLEARN_MINOR_VERSION, 
+                        PLEARN_FIXLEVEL       );
+}
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :


Property changes on: trunk/commands/plearn_desjardins.cc
___________________________________________________________________
Name: svn:executable
   + *



From saintmlx at mail.berlios.de  Mon Feb 18 21:25:03 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 18 Feb 2008 21:25:03 +0100
Subject: [Plearn-commits] r8525 - trunk/python_modules/plearn/pytest
Message-ID: <200802182025.m1IKP3Nn000713@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-18 21:25:02 +0100 (Mon, 18 Feb 2008)
New Revision: 8525

Modified:
   trunk/python_modules/plearn/pytest/tests.py
Log:
- keep env until after pldiff



Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2008-02-18 17:25:13 UTC (rev 8524)
+++ trunk/python_modules/plearn/pytest/tests.py	2008-02-18 20:25:02 UTC (rev 8525)
@@ -765,11 +765,12 @@
         self.clean_cwd()
         popd()
 
+        ## Set the status and quit
+        self.status_hook()
+
         ## Unlink all resources.
         self.test.unlinkResources(test_results)
 
-        ## Set the status and quit
-        self.status_hook()
 
     def status_hook(self):
         """Overriden by subclasses to set the test's status."""



From saintmlx at mail.berlios.de  Mon Feb 18 21:25:52 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 18 Feb 2008 21:25:52 +0100
Subject: [Plearn-commits] r8526 - trunk/python_modules/plearn/pyext
Message-ID: <200802182025.m1IKPqXE000793@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-18 21:25:52 +0100 (Mon, 18 Feb 2008)
New Revision: 8526

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
- don't print version string when running under pytest



Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-02-18 20:25:02 UTC (rev 8525)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-02-18 20:25:52 UTC (rev 8526)
@@ -34,6 +34,7 @@
 from plearn.pyext import plext as pl
 
 from plearn.pyplearn.plargs import *
+import os
 import cgitb
 cgitb.enable(format='PLearn')
 
@@ -45,7 +46,8 @@
         ramassePoubelles()
 atexit.register(cleanupWrappedObjects)
 
-print versionString()
+if os.getenv('PYTEST_STATE') != 'Active':
+    print versionString()
 
 
 # Redefines function TMat to emulate pyplearn behaviour



From ducharme at mail.berlios.de  Mon Feb 18 22:43:30 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Mon, 18 Feb 2008 22:43:30 +0100
Subject: [Plearn-commits] r8527 - trunk/plearn_learners/distributions
Message-ID: <200802182143.m1ILhUmc009830@sheep.berlios.de>

Author: ducharme
Date: 2008-02-18 22:43:30 +0100 (Mon, 18 Feb 2008)
New Revision: 8527

Modified:
   trunk/plearn_learners/distributions/GaussianDistribution.cc
   trunk/plearn_learners/distributions/GaussianDistribution.h
Log:
- On ajoute une methode remote, "computeEigenDecomposition" (utilisee par train)
- La variable "covarmat" devient une variable de classe


Modified: trunk/plearn_learners/distributions/GaussianDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussianDistribution.cc	2008-02-18 20:25:52 UTC (rev 8526)
+++ trunk/plearn_learners/distributions/GaussianDistribution.cc	2008-02-18 21:43:30 UTC (rev 8527)
@@ -80,6 +80,7 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(mu, copies);
+    deepCopyField(covarmat, copies);
     deepCopyField(eigenvalues, copies);
     deepCopyField(eigenvectors, copies);
     deepCopyField(given_mu, copies);
@@ -122,12 +123,27 @@
 
     // Learnt options
     declareOption(ol, "mu", &GaussianDistribution::mu, OptionBase::learntoption, "");
+    declareOption(ol, "covarmat", &GaussianDistribution::covarmat, OptionBase::learntoption, "");
     declareOption(ol, "eigenvalues", &GaussianDistribution::eigenvalues, OptionBase::learntoption, "");
     declareOption(ol, "eigenvectors", &GaussianDistribution::eigenvectors, OptionBase::learntoption, "");
 
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void GaussianDistribution::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "computeEigenDecomposition", &GaussianDistribution::computeEigenDecomposition,
+        (BodyDoc("Compute eigenvectors and corresponding eigenvalues.\n")));
+}
+
 ///////////
 // build //
 ///////////
@@ -159,27 +175,16 @@
 void GaussianDistribution::train()
 {
     VMat training_set = getTrainingSet();
-    int l = training_set.length();
     int d = training_set.width();
     int ws = training_set->weightsize();
 
-    if(d!=inputsize()+ws)
+    if(d != inputsize()+ws)
         PLERROR("In GaussianDistribution::train width of training_set should be equal to inputsize()+weightsize()");
 
-    // these are used in SVD
-    static Mat trainmat;
-    static Mat U;
-
-    // The maximum number of eigenvalues we want.
-    int maxneigval = min(k, min(l,d));
-
     // First get mean and covariance
-    // (declared static to avoid repeated dynamic memory allocation)
-    static Mat covarmat;
-
     if(given_mu.length()>0)
     { // we have a fixed given_mu
-        int d = given_mu.length();
+        d = given_mu.length();
         mu.resize(d);
         mu << given_mu;
         if(ws==0)
@@ -199,15 +204,24 @@
             PLERROR("In GaussianDistribution, weightsize can only be 0 or 1");
     }
 
-    // cerr << "maxneigval: " << maxneigval << " ";
-    // cerr << eigenvalues.length() << endl;
-    // cerr << "eig V: \n" << V << endl;
+    computeEigenDecomposition();
+}
 
+void GaussianDistribution::computeEigenDecomposition()
+{
+    VMat training_set = getTrainingSet();
+    int l = training_set.length();
+    int d = training_set.width();
+    int maxneigval = min(k, min(l,d));  // The maximum number of eigenvalues we want.
+
     // Compute eigendecomposition only if there is a training set...
-    // Otherwise, just fill the eigen-* matrices to all NaN...
+    // Otherwise, just empty the eigen-* matrices
+    static Mat covarmat_tmp;
     if (l>0 && maxneigval>0)
     {
-        eigenVecOfSymmMat(covarmat, maxneigval, eigenvalues, eigenvectors, (verbosity>=4));
+        // On copie covarmat car cette matrice est detruite par la fonction eigenVecOfSymmMat
+        covarmat_tmp = covarmat.copy();
+        eigenVecOfSymmMat(covarmat_tmp, maxneigval, eigenvalues, eigenvectors, (verbosity>=4));
         int neig = 0;
         while(neig<eigenvalues.length() && eigenvalues[neig]>0.)
             neig++;
@@ -218,12 +232,6 @@
     {
         eigenvalues.resize(0);
         eigenvectors.resize(0, mu.length());
-        /*
-          eigenvalues.resize(maxneigval);
-          eigenvectors.resize(maxneigval, mu.size());
-          eigenvalues.fill(0);
-          eigenvectors.fill(0);
-        */
     }
 }
 

Modified: trunk/plearn_learners/distributions/GaussianDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/GaussianDistribution.h	2008-02-18 20:25:52 UTC (rev 8526)
+++ trunk/plearn_learners/distributions/GaussianDistribution.h	2008-02-18 21:43:30 UTC (rev 8527)
@@ -63,6 +63,7 @@
 
     // Possibly "Learned" parameters
     Vec mu;
+    Mat covarmat;
     Vec eigenvalues;
     Mat eigenvectors;
 
@@ -82,6 +83,7 @@
 
     virtual void forget();
     virtual void train();
+    virtual void computeEigenDecomposition();
     virtual real log_density(const Vec& x) const;
 
     //! return a pseudo-random sample generated from the distribution.
@@ -96,6 +98,9 @@
 protected:
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
 private:
 
     //! Internal build.



From nouiz at mail.berlios.de  Mon Feb 18 23:46:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 18 Feb 2008 23:46:24 +0100
Subject: [Plearn-commits] r8528 - in trunk/plearn: misc vmat
Message-ID: <200802182246.m1IMkO9l016596@sheep.berlios.de>

Author: nouiz
Date: 2008-02-18 23:46:24 +0100 (Mon, 18 Feb 2008)
New Revision: 8528

Modified:
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
changed the variable name stderr to stderror as the first one cause compilation bug in wnidows


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-18 21:43:30 UTC (rev 8527)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-18 22:46:24 UTC (rev 8528)
@@ -973,18 +973,18 @@
         if(argc>5)
             missing_threshold=toreal(argv[5]);
         Vec missing(m1->width());
-        Vec stderr(m1->width());
+        Vec stderror(m1->width());
 
         pout << "Test of difference that suppose gaussiane variable"<<endl;
         m1->compareStats(m2, stderror_threshold, missing_threshold,
-                         stderr, missing);
+                         stderror, missing);
 
         Mat score(m1->width(),3);
 
         for(int col = 0;col<m1->width();col++)
         {
             score(col,0)=col;
-            score(col,1)=stderr[col];
+            score(col,1)=stderror[col];
             score(col,2)=missing[col];
         }
         

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-18 21:43:30 UTC (rev 8527)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-18 22:46:24 UTC (rev 8528)
@@ -1935,11 +1935,11 @@
 //////////////////
 // compareStats //
 //////////////////
-void VMatrix::compareStats(const VMat& target,
-                          const real stderror_threshold,
-                          const real missing_threshold,
-                          Vec& stderr,
-                          Vec& missing) const
+void VMatrix::compareStats(VMat target,
+                           real stderror_threshold,
+                           real missing_threshold,
+                           Vec stderror,
+                           Vec missing)
 {
     if(target->width()!=width())
         PLERROR("In VecStatsCollector:: compareStats() - This VMatrix has "
@@ -1976,7 +1976,7 @@
             PLWARNING("In VMatrix::compareStats - field %d(%s) have a"
                       " stderror of 0 for both matrice.",
                       i, fieldName(i).c_str());
-        stderr[i]=th_stderror;
+        stderror[i]=th_stderror;
         missing[i]=th_missing;
     }
     return;

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-18 21:43:30 UTC (rev 8527)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-18 22:46:24 UTC (rev 8528)
@@ -618,12 +618,13 @@
      * @param sumdiff_missing The sum of all variable differences of missing
      * @return The number of differences that were found
      */
-    void compareStats(const VMat& target,
-                     const real stderror_threshold ,
-                     const real missing_threshold,
-                     Vec& stderr,
-                     Vec& missing) const;
-
+    void compareStats(VMat target,
+                      real stderror_threshold ,
+                      real missing_threshold,
+                      Vec stderror,
+                      Vec missing
+                      );
+                      
     /**
      * @return The size of the longuest fieldname
      */



From nouiz at mail.berlios.de  Mon Feb 18 23:52:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 18 Feb 2008 23:52:16 +0100
Subject: [Plearn-commits] r8529 - trunk/commands
Message-ID: <200802182252.m1IMqGA8018195@sheep.berlios.de>

Author: nouiz
Date: 2008-02-18 23:52:16 +0100 (Mon, 18 Feb 2008)
New Revision: 8529

Modified:
   trunk/commands/plearn_desjardins.cc
Log:


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-02-18 22:46:24 UTC (rev 8528)
+++ trunk/commands/plearn_desjardins.cc	2008-02-18 22:52:16 UTC (rev 8529)
@@ -100,7 +100,6 @@
 
 int main(int argc, char** argv)
 {
-    cout<<"main begin"<<endl;
     return plearn_main( argc, argv, 
                         PLEARN_MAJOR_VERSION, 
                         PLEARN_MINOR_VERSION, 



From chapados at mail.berlios.de  Tue Feb 19 04:55:48 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 19 Feb 2008 04:55:48 +0100
Subject: [Plearn-commits] r8530 - trunk/plearn/base
Message-ID: <200802190355.m1J3tm9K001666@sheep.berlios.de>

Author: chapados
Date: 2008-02-19 04:55:47 +0100 (Tue, 19 Feb 2008)
New Revision: 8530

Modified:
   trunk/plearn/base/ParentableObject.cc
   trunk/plearn/base/ParentableObject.h
Log:
getParent/setParent now remote-callable

Modified: trunk/plearn/base/ParentableObject.cc
===================================================================
--- trunk/plearn/base/ParentableObject.cc	2008-02-18 22:52:16 UTC (rev 8529)
+++ trunk/plearn/base/ParentableObject.cc	2008-02-19 03:55:47 UTC (rev 8530)
@@ -100,6 +100,28 @@
     deepCopyField(m_parent, copies);
 }
 
+void ParentableObject::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "getParent", (const Object* (ParentableObject::*)() const)&ParentableObject::parent,
+        (BodyDoc("Return the parent object")));
+
+    declareMethod(
+        rmm, "setParent", &ParentableObject::setParent,
+        (BodyDoc("Setter for the parent object"),
+         ArgDoc ("parent", "Pointer to the new parent of this object")));
+
+    declareMethod(
+        rmm, "checkParent", &ParentableObject::checkParent,
+        (BodyDoc("After the m_parent field of a child has been set, this function is\n"
+                 "called so that the child has a chance to check the parent (e.g. dynamic\n"
+                 "type checking).  By default it just asserts that there is a parent.")));
+}
+
 void ParentableObject::build_()
 {
     updateChildrensParent(this);

Modified: trunk/plearn/base/ParentableObject.h
===================================================================
--- trunk/plearn/base/ParentableObject.h	2008-02-18 22:52:16 UTC (rev 8529)
+++ trunk/plearn/base/ParentableObject.h	2008-02-19 03:55:47 UTC (rev 8530)
@@ -138,6 +138,9 @@
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+    
 protected:
     //! Backpointer to parent
     Object* m_parent;



From ducharme at mail.berlios.de  Tue Feb 19 18:00:26 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Tue, 19 Feb 2008 18:00:26 +0100
Subject: [Plearn-commits] r8531 - in
	trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456:
	. Split0
Message-ID: <200802191700.m1JH0QJB013300@sheep.berlios.de>

Author: ducharme
Date: 2008-02-19 18:00:25 +0100 (Tue, 19 Feb 2008)
New Revision: 8531

Modified:
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/global_stats.pmat
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/split_stats.pmat
Log:
Nouveaux resultats pour tenir compte de la nouvelle version de GaussianDistribution.


Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave	2008-02-19 03:55:47 UTC (rev 8530)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/final_learner.psave	2008-02-19 17:00:25 UTC (rev 8531)
@@ -6,10 +6,17 @@
 given_mu = []
 ;
 mu = 4 [ -0.0868066666666664488 -0.0626033333333333025 -1.50779199999991498 5.35029800000000844 ] ;
-eigenvalues = 2 [ 12666.4930233106425 458.781023436976511 ] ;
+covarmat = 4  4  [ 
+0.715700285995525709 	-0.00151849129753913966 	7.56589645354362883 	12.7390914968322093 	
+-0.00151849129753913966 	1.0095320630089486 	100.696078851570505 	48.1153474461677817 	
+7.56589645354362883 	100.696078851570505 	10149.543581877524 	4937.80362099732884 	
+12.7390914968322093 	48.1153474461677817 	4937.80362099732884 	2974.58090941429145 	
+]
+;
+eigenvalues = 2 [ 12666.4930233106388 458.781023436978387 ] ;
 eigenvectors = 2  4  [ 
--0.00098882171295527465 	-0.00880822163979269081 	-0.89097087080375037 	-0.453973947316771431 	
-0.0172789682695909386 	-0.00620286016779083085 	-0.453881809086822574 	0.890872754753085139 	
+-0.0009888217129552753 	-0.00880822163979269081 	-0.890970870803750703 	-0.453973947316771431 	
+0.0172789682695909455 	-0.00620286016779082999 	-0.453881809086822519 	0.890872754753085139 	
 ]
 ;
 outputs_def = "l" ;

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2008-02-19 03:55:47 UTC (rev 8530)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2008-02-19 17:00:25 UTC (rev 8531)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 48.8719180204967216 ;
-sumsquare_ = 104.363929133442653 ;
-sumcube_ = 318.950374924761547 ;
-sumfourth_ = 1177.77083991773657 ;
-min_ = 17.6042426640295062 ;
-max_ = 22.4027835362180525 ;
+sum_ = 48.8719180204962811 ;
+sumsquare_ = 104.363929133440806 ;
+sumcube_ = 318.950374924751884 ;
+sumfourth_ = 1177.77083991768404 ;
+min_ = 17.6042426640295098 ;
+max_ = 22.4027835362179744 ;
 agmemin_ = 36 ;
 agemax_ = 18 ;
-first_ = 17.6199119453043878 ;
-last_ = 17.6479526969470193 ;
+first_ = 17.6199119453043913 ;
+last_ = 17.6479526969470228 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2008-02-19 03:55:47 UTC (rev 8530)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2008-02-19 17:00:25 UTC (rev 8531)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 9.41750512464414413 ;
-sumsquare_ = 139.406100961585082 ;
-sumcube_ = 289.545736167180053 ;
-sumfourth_ = 1182.45789565908922 ;
-min_ = 17.6005077694121894 ;
-max_ = 23.6898552948462715 ;
+sum_ = 9.41750512464426492 ;
+sumsquare_ = 139.406100961582524 ;
+sumcube_ = 289.545736167174312 ;
+sumfourth_ = 1182.4578956590592 ;
+min_ = 17.600507769412193 ;
+max_ = 23.689855294846236 ;
 agmemin_ = 95 ;
 agemax_ = 6 ;
-first_ = 18.5231445696105403 ;
-last_ = 19.4342267319090887 ;
+first_ = 18.5231445696105332 ;
+last_ = 19.4342267319090674 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/split_stats.pmat
===================================================================
(Binary files differ)



From nouiz at mail.berlios.de  Tue Feb 19 18:19:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Feb 2008 18:19:34 +0100
Subject: [Plearn-commits] r8532 - trunk/plearn_learners/cgi
Message-ID: <200802191719.m1JHJYK9015445@sheep.berlios.de>

Author: nouiz
Date: 2008-02-19 18:19:29 +0100 (Tue, 19 Feb 2008)
New Revision: 8532

Modified:
   trunk/plearn_learners/cgi/ComputeDond2Target.cc
   trunk/plearn_learners/cgi/ComputeDond2Target.h
Log:
code cleanup


Modified: trunk/plearn_learners/cgi/ComputeDond2Target.cc
===================================================================
--- trunk/plearn_learners/cgi/ComputeDond2Target.cc	2008-02-19 17:00:25 UTC (rev 8531)
+++ trunk/plearn_learners/cgi/ComputeDond2Target.cc	2008-02-19 17:19:29 UTC (rev 8532)
@@ -141,8 +141,7 @@
 void ComputeDond2Target::computeTarget()
 {    
     // initialize primary dataset
-    main_row = 0;
-    main_col = 0;
+    int main_col = 0;
     main_length = train_set->length();
     main_width = train_set->width();
     main_input.resize(main_width);
@@ -155,7 +154,7 @@
     output_names.resize(output_width);
     output_vec.resize(output_width);
     main_names << train_set->fieldNames();
-    for (ins_col = 0; ins_col < ins_width; ins_col++)
+    for (int ins_col = 0; ins_col < ins_width; ins_col++)
     {
         for (main_col = 0; main_col < main_width; main_col++)
         {
@@ -175,10 +174,10 @@
     //Now, we can group the input and compute the class target
     ProgressBar* pb = 0;
     pb = new ProgressBar( "Computing target classes", main_length);
-    for (main_row = 0; main_row < main_length; main_row++)
+    for (int main_row = 0; main_row < main_length; main_row++)
     {
         train_set->getRow(main_row, main_input);
-        for (ins_col = 0; ins_col < ins_width; ins_col++)
+        for (int ins_col = 0; ins_col < ins_width; ins_col++)
         {
             output_vec[ins_col] = main_input[output_variable_src[ins_col]];
         }

Modified: trunk/plearn_learners/cgi/ComputeDond2Target.h
===================================================================
--- trunk/plearn_learners/cgi/ComputeDond2Target.h	2008-02-19 17:00:25 UTC (rev 8531)
+++ trunk/plearn_learners/cgi/ComputeDond2Target.h	2008-02-19 17:19:29 UTC (rev 8532)
@@ -133,13 +133,9 @@
     
     // input instructions variables
     int ins_width;
-    int ins_col;
     
     // primary dataset variables
     int main_length;
-    int main_width;
-    int main_row;
-    int main_col;
     Vec main_input;
     TVec<string> main_names;
     



From nouiz at mail.berlios.de  Tue Feb 19 18:25:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Feb 2008 18:25:30 +0100
Subject: [Plearn-commits] r8533 - trunk/plearn/math
Message-ID: <200802191725.m1JHPUmS015887@sheep.berlios.de>

Author: nouiz
Date: 2008-02-19 18:25:29 +0100 (Tue, 19 Feb 2008)
New Revision: 8533

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
changed code to allow the for loop to be vectorized by gcc


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-02-19 17:19:29 UTC (rev 8532)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-02-19 17:25:29 UTC (rev 8533)
@@ -1287,7 +1287,8 @@
     if (vec1.size() > 0 && vec2.size() > 0) {
         T* v1 = vec1.data();
         T* v2 = vec2.data();
-        for(int i=0; i<vec1.length(); i++)
+        int l=vec1.length();
+        for(int i=0; i<l; i++)
             v1[i] /= v2[i];
     }
 }
@@ -3442,7 +3443,8 @@
     T s = 0;
     T* rowi = mat.rowdata(i);
     T* v_=v.data();
-    for (int j=0;j<mat.width();j++)
+    int w=mat.width();
+    for (int j=0;j<w;j++)
         s += rowi[j] * v_[j];
     return s;
 }
@@ -3459,7 +3461,8 @@
     T s = 0;
     T* colj = mat.data()+j;
     T* v_=v.data();
-    for (int i=0;i<mat.length();i++, colj+=mat.mod())
+    int l=mat.length();
+    for (int i=0;i<l;i++, colj+=mat.mod())
         s += *colj * v_[i];
     return s;
 }
@@ -3545,8 +3548,9 @@
     T dif;
     T value;
     bool warning_flag = false;
+    int w=mat.width();
     for (int i=0; i<mat.length()-1 ; i++)
-        for (int j=i+1; j<mat.width(); j++)
+        for (int j=i+1; j<w; j++)
         {
             dif = std::abs(mat[i][j] - mat[j][i]);
             if (dif > max_dif)
@@ -3793,11 +3797,12 @@
 #endif
     const T* v_1=v1.data();
     const T* v_2=v2.data();
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T* mi = mat[i];
         T v1i = v_1[i];
-        for (int j=0;j<mat.width();j++)
+        for (int j=0;j<w;j++)
             mi[j] = v1i * v_2[j];
     }
 }
@@ -3853,11 +3858,12 @@
 #endif
     const T* v_1=v1.data();
     const T* v_2=v2.data();
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T* mi = mat[i];
         T v1i = v_1[i];
-        for (int j=0;j<mat.width();j++)
+        for (int j=0;j<w;j++)
             mi[j] += gamma * v1i * v_2[j];
     }
 }
@@ -3873,11 +3879,12 @@
 #endif
     const T* v_1=v1.data();
     const T* v_2=v2.data();
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T* mi = mat[i];
         T v1i = v_1[i];
-        for (int j=0;j<mat.width();j++)
+        for (int j=0;j<w;j++)
             mi[j] = alpha*mi[j] + gamma * v1i * v_2[j];
     }
 }
@@ -4545,6 +4552,7 @@
                  !transpose && x.length()==y.length(),
                  "multiply matrix rows or columns by vector: incompatible dimensions");
     result.resize(x.length(),x.width());
+    int w=x.width();
     if(result.isCompact() && x.isCompact())
     {
         typename TMat<T>::compact_iterator itm = result.compact_begin();
@@ -4554,12 +4562,12 @@
             for (int i=0;i<x.length();i++)
             {
                 ity = y.begin();
-                for (int j=0;j<x.width();j++,++itx,++itm,++ity)
+                for (int j=0;j<w;j++,++itx,++itm,++ity)
                     *itm = *itx * *ity;
             }
         else
             for (int i=0;i<x.length();i++,++ity)
-                for (int j=0;j<x.width();j++,++itx,++itm)
+                for (int j=0;j<w;j++,++itx,++itm)
                     *itm = *itx * *ity;
     }
     else // use non-compact iterators
@@ -4571,12 +4579,12 @@
             for (int i=0;i<x.length();i++)
             {
                 ity = y.begin();
-                for (int j=0;j<x.width();j++,++itx,++itm,++ity)
+                for (int j=0;j<w;j++,++itx,++itm,++ity)
                     *itm = *itx * *ity;
             }
         else
             for (int i=0;i<x.length();i++,++ity)
-                for (int j=0;j<x.width();j++,++itx,++itm)
+                for (int j=0;j<w;j++,++itx,++itm)
                     *itm = *itx * *ity;
     }
 }
@@ -4735,7 +4743,8 @@
 void addToDiagonal(const TMat<T>& mat, T lambda)
 {
     T *d = mat.data();
-    for (int i=0;i<mat.length();i++,d+=mat.mod()+1) *d+=lambda;
+    int l=mat.length();
+    for (int i=0;i<l;i++,d+=mat.mod()+1) *d+=lambda;
 }
 
 
@@ -4752,7 +4761,8 @@
 #endif
     T *l = lambda.data();
     T *d = mat.data();
-    for (int i=0;i<mat.length();i++,d+=mat.mod()+1,l++) *d += *l;
+    int le= mat.length();
+    for (int i=0;i<le;i++,d+=mat.mod()+1,l++) *d += *l;
 }
 
 
@@ -4760,7 +4770,8 @@
 void diag(const TMat<T>& mat, const TVec<T>& d)
 {
     T* d_ = d.data();
-    for (int i=0;i<mat.length();i++)
+    int l=mat.length();
+    for (int i=0;i<l;i++)
         d_[i] = mat(i,i);
 }
 
@@ -4800,11 +4811,12 @@
     avg_across_rows.clear();
     avg_across_columns.clear();
     T* row_i=mat.data();
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T& avg_cols_i=avg_across_columns[i];
         T* avg_rows = avg_across_rows.data();
-        for (int j=0;j<mat.width();j++)
+        for (int j=0;j<w;j++)
         {
             T row_ij=row_i[j];
             avg_cols_i += row_ij;
@@ -4820,7 +4832,8 @@
 template<class T>
 void addToRows(const TMat<T>& mat, const TVec<T> row, bool ignored)
 {
-    for (int i=0;i<mat.length();i++)
+    int l=mat.length();
+    for (int i=0;i<l;i++)
     {
         TVec<T> row_i = mat(i);
         row_i += row;
@@ -4832,10 +4845,11 @@
 void addToColumns(const TMat<T>& mat, const TVec<T> col, bool ignored)
 {
     T* row_i=mat.data();
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T col_i=col[i];
-        for (int j=0;j<mat.width();j++)
+        for (int j=0;j<w;j++)
             row_i[j] += col_i;
         row_i+=mat.mod();
     }
@@ -4858,10 +4872,11 @@
 void substractFromColumns(const TMat<T>& mat, const TVec<T> col, bool ignored)
 {
     T* row_i=mat.data();
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T col_i=col[i];
-        for (int j=0;j<mat.width();j++)
+        for (int j=0;j<w;j++)
             row_i[j] -= col_i;
         row_i+=mat.mod();
     }
@@ -4882,9 +4897,10 @@
 {
     double res = 0.0;
     T* m_i = mat.data();
+    int w=mat.width();
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
     {
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
         {
             if (!is_missing(m_i[j])) res += m_i[j];
             else if (!ignore_missing) return MISSING_VALUE;
@@ -4900,8 +4916,10 @@
 {
     T res = T(0);
     T* m_i = mat.data();
+    int w=mat.width();
+
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
             res += m_i[j];
     return res;
 }
@@ -4911,8 +4929,10 @@
 {
     double res = 1.0;
     T* m_i = mat.data();
+    int w=mat.width();
+
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
             res *= m_i[j];
     return T(res);
 }
@@ -4922,8 +4942,9 @@
 {
     double res = 0.0;
     T* m_i = mat.data();
+    int w=mat.width();
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
         {
             T v = m_i[j];
             res += v*v;
@@ -4940,8 +4961,9 @@
 #endif
     double res = 0.0;
     T* m_i = mat.data();
+    int w=mat.width();
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
             res += m_i[j];
     return T(res/(mat.length()*mat.width()));
 }
@@ -5123,8 +5145,9 @@
 #endif
     T* m_i = mat.data();
     double minval = fabs(m_i[0]);
+    int w=mat.width();
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
         {
             T a=fabs(m_i[j]);
             if(a<minval)
@@ -5143,8 +5166,9 @@
     double minval = fabs(m_i[0]);
     min_i = 0;
     min_j = 0;
+    int w=mat.width();
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
         {
             T a = fabs(m_i[j]);
             if(a<minval)
@@ -5167,8 +5191,9 @@
 #endif
     T* m_i = mat.data();
     double maxval = fabs(m_i[0]);
+    int w=mat.width();
     for(int i=0; i<mat.length(); i++, m_i+=mat.mod())
-        for(int j=0; j<mat.width(); j++)
+        for(int j=0; j<w; j++)
         {
             T a=fabs(m_i[j]);
             if(a>maxval)
@@ -5333,11 +5358,12 @@
     if(singlecolumn.length()!=mat.length() || singlecolumn.width()!=1)
         PLERROR("IN void rowSumOfSquares(const TMat<T>& mat, TMat<T>& singlecolumn) singlecolumn must be a mat.length() x 1 matrix");
 #endif
+    int w=mat.width();
     for (int i=0;i<mat.length();i++)
     {
         T ss=0;
         T* mi=mat[i];
-        for (int j=0;j<mat.width();j++) { T mij=mi[j]; ss+=mij*mij; }
+        for (int j=0;j<w;j++) { T mij=mi[j]; ss+=mij*mij; }
         singlecolumn(i,0)=ss;
     }
 }
@@ -5655,7 +5681,8 @@
 {
     columnMean(m,meanvec);
     columnVariance(m,stddevvec,meanvec);
-    for(int i=0; i<stddevvec.length(); i++)
+    int l=stddevvec.length();
+    for(int i=0; i<l; i++)
         stddevvec[i] = sqrt(stddevvec[i]);
 }
 
@@ -5667,7 +5694,8 @@
 {
     rowMean(m,meanvec);
     rowVariance(m,stddevvec,meanvec);
-    for(int i=0; i<stddevvec.length(); i++)
+    int l=stddevvec.length();
+    for(int i=0; i<l; i++)
         stddevvec[i][0] = sqrt(stddevvec[i][0]);
 }
 
@@ -5721,8 +5749,9 @@
 void operator+=(const TMat<T>& m, T scalar)
 {
     T* m_i = m.data();
+    int w = m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] += scalar;
 }
 
@@ -5730,8 +5759,9 @@
 void operator*=(const TMat<T>& m, T scalar)
 {
     T* m_i = m.data();
+    int w = m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] *= scalar;
 }
 
@@ -5755,8 +5785,9 @@
 #endif
     T* m_i = m.data();
     T* vv = v.data();
+    int w=m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] += vv[j];
 }
 
@@ -5770,8 +5801,9 @@
 #endif
     T* m_i = m.data();
     T* vv = v.data();
+    int w=m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] -= vv[j];
 }
 
@@ -5785,8 +5817,9 @@
 #endif
     T* m_i = m.data();
     T* vv = v.data();
+    int w=m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] *= vv[j];
 }
 
@@ -5818,8 +5851,9 @@
 #endif
     T* m_i = m.data();
     T* vv = v.data();
+    int w=m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] /= vv[j];
 }
 
@@ -5942,8 +5976,9 @@
     TMat<T> opposite(m.length(),m.width());
     T *m_i=m.data();
     T *o_i=opposite.data();
+    int w=m.width();
     for (int i=0;i<m.length();i++,m_i+=m.mod(),o_i+=opposite.mod())
-        for (int j=0;j<m.width();j++)
+        for (int j=0;j<w;j++)
             o_i[j] = - m_i[j];
     return opposite;
 }
@@ -5953,8 +5988,9 @@
 void negateElements(const TMat<T>& m)
 {
     T* m_i = m.data();
+    int w=m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] = -m_i[j];
 }
 
@@ -5963,8 +5999,9 @@
 void invertElements(const TMat<T>& m)
 {
     T* m_i = m.data();
+    int w=m.width();
     for(int i=0; i<m.length(); i++, m_i+=m.mod())
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             m_i[j] = 1.0/m_i[j];
 }
 
@@ -6744,7 +6781,8 @@
     if (dest.length()!=m.length())
         PLERROR("apply: m.length_=%d, dest.length_=%d",
                 m.length(),dest.length());
-    for (int i=0;i<m.length();i++)
+    int l=m.length();
+    for (int i=0;i<l;i++)
         dest(i,0)=func(m(i));
 }
 
@@ -6955,8 +6993,9 @@
 TMat<T> square(const TMat<T>& m)
 {
     TMat<T> res(m.length(), m.width());
+    int w=m.width();
     for(int i=0; i<m.length(); i++)
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             res(i,j) = square(m(i,j));
     return res;
 }
@@ -6965,8 +7004,9 @@
 TMat<T> sqrt(const TMat<T>& m)
 {
     TMat<T> res(m.length(), m.width());
+    int w=m.width();
     for(int i=0; i<m.length(); i++)
-        for(int j=0; j<m.width(); j++)
+        for(int j=0; j<w; j++)
             res(i,j) = sqrt(m(i,j));
     return res;
 }
@@ -7010,8 +7050,10 @@
             T* maskptr = mask.data();
             T* mptr = m[i]+j;
             sum = 0.0;
+            int w=mask.width();
+
             for(int l=0; l<mask.length(); l++, maskptr += mask.mod(), mptr += m.mod())
-                for(int c=0; c<mask.width(); c++)
+                for(int c=0; c<w; c++)
                     sum += maskptr[c] * mptr[c];
             result(i,j) = sum;
         }



From nouiz at mail.berlios.de  Tue Feb 19 18:34:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Feb 2008 18:34:48 +0100
Subject: [Plearn-commits] r8534 - trunk/plearn/vmat
Message-ID: <200802191734.m1JHYmWE025423@sheep.berlios.de>

Author: nouiz
Date: 2008-02-19 18:34:45 +0100 (Tue, 19 Feb 2008)
New Revision: 8534

Modified:
   trunk/plearn/vmat/BinaryNumbersVMatrix.cc
   trunk/plearn/vmat/JoinVMatrix.cc
Log:
modif for auto-vectorisation


Modified: trunk/plearn/vmat/BinaryNumbersVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2008-02-19 17:25:29 UTC (rev 8533)
+++ trunk/plearn/vmat/BinaryNumbersVMatrix.cc	2008-02-19 17:34:45 UTC (rev 8534)
@@ -76,31 +76,32 @@
         swap_endian=true;
 #endif
     
+    int l=v.length();
     if (format=="u1") 
-        for (int i=0;i<v.length();i++)
+        for (int i=0;i<l;i++)
             v[i] = (real)((unsigned char*)buffer)[i];
     else if (format=="u2") {
         if (swap_endian)
             endianswap2(buffer,width_);
-        for (int i=0;i<v.length();i++)
+        for (int i=0;i<l;i++)
             v[i] = (real)((unsigned short*)buffer)[i];
     }
     else if (format=="i4") {
         if (swap_endian)
             endianswap4(buffer,width_);
-        for (int i=0;i<v.length();i++)
+        for (int i=0;i<l;i++)
             v[i] = (real)((int*)buffer)[i];
     }
     else if (format=="f4") {
         if (swap_endian)
             endianswap4(buffer,width_);
-        for (int i=0;i<v.length();i++)
+        for (int i=0;i<l;i++)
             v[i] = (real)((float*)buffer)[i];
     }
     else if (format=="f8") {
         if (swap_endian)
             endianswap8(buffer,width_);
-        for (int i=0;i<v.length();i++)
+        for (int i=0;i<l;i++)
             v[i] = (real)((double*)buffer)[i];
     }
     else

Modified: trunk/plearn/vmat/JoinVMatrix.cc
===================================================================
--- trunk/plearn/vmat/JoinVMatrix.cc	2008-02-19 17:25:29 UTC (rev 8533)
+++ trunk/plearn/vmat/JoinVMatrix.cc	2008-02-19 17:34:45 UTC (rev 8534)
@@ -75,7 +75,8 @@
 
         for(int i=0;i<slave.length();i++) {
             slave->getRow(i,temp);
-            for(int j=0;j<slave_idx.size();j++)
+            int s=slave_idx.size();
+            for(int j=0;j<s;j++)
                 tempkey[j]=temp[slave_idx[j]];
             mp.insert(make_pair(tempkey,i));
         }
@@ -131,7 +132,8 @@
     master->getRow(idx,v.subVec(0,master.width()));
 
     // build a key used to search in the slave matrix
-    for(int j=0;j<master_idx.size();j++)
+    int s=master_idx.size();
+    for(int j=0;j<s;j++)
         tempkey[j]=v[master_idx[j]];
     Maptype::const_iterator it,low,upp;
     pair<Maptype::const_iterator,Maptype::const_iterator> tit=mp.equal_range(tempkey);



From nouiz at mail.berlios.de  Tue Feb 19 19:20:38 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Feb 2008 19:20:38 +0100
Subject: [Plearn-commits] r8535 - trunk/plearn_learners/cgi
Message-ID: <200802191820.m1JIKcjH012766@sheep.berlios.de>

Author: nouiz
Date: 2008-02-19 19:20:29 +0100 (Tue, 19 Feb 2008)
New Revision: 8535

Modified:
   trunk/plearn_learners/cgi/ComputeDond2Target.cc
Log:
bugfix


Modified: trunk/plearn_learners/cgi/ComputeDond2Target.cc
===================================================================
--- trunk/plearn_learners/cgi/ComputeDond2Target.cc	2008-02-19 17:34:45 UTC (rev 8534)
+++ trunk/plearn_learners/cgi/ComputeDond2Target.cc	2008-02-19 18:20:29 UTC (rev 8535)
@@ -143,7 +143,7 @@
     // initialize primary dataset
     int main_col = 0;
     main_length = train_set->length();
-    main_width = train_set->width();
+    int main_width = train_set->width();
     main_input.resize(main_width);
     main_names.resize(main_width);
     ins_width = input_vector.size();



From nouiz at mail.berlios.de  Tue Feb 19 21:42:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Feb 2008 21:42:45 +0100
Subject: [Plearn-commits] r8536 - trunk/plearn_learners/online
Message-ID: <200802192042.m1JKgjRp024293@sheep.berlios.de>

Author: nouiz
Date: 2008-02-19 21:42:45 +0100 (Tue, 19 Feb 2008)
New Revision: 8536

Modified:
   trunk/plearn_learners/online/BinarizeModule.cc
Log:
for auto-vectorizer


Modified: trunk/plearn_learners/online/BinarizeModule.cc
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.cc	2008-02-19 18:20:29 UTC (rev 8535)
+++ trunk/plearn_learners/online/BinarizeModule.cc	2008-02-19 20:42:45 UTC (rev 8536)
@@ -184,11 +184,12 @@
     {
         real* xt = (*input)[t];
         real* yt = (*output)[t];
+        int w=input->width();
         if (stochastic)
-            for (int i=0;i<input->width();i++)
+            for (int i=0;i<w;i++)
                 yt[i]=random_gen->binomial_sample(xt[i]);
         else
-            for (int i=0;i<input->width();i++)
+            for (int i=0;i<w;i++)
                 yt[i]=xt[i]>=0.5?1:0;
     }
 }



From nouiz at mail.berlios.de  Tue Feb 19 21:48:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 19 Feb 2008 21:48:21 +0100
Subject: [Plearn-commits] r8537 - trunk/plearn/vmat
Message-ID: <200802192048.m1JKmL4E025251@sheep.berlios.de>

Author: nouiz
Date: 2008-02-19 21:48:21 +0100 (Tue, 19 Feb 2008)
New Revision: 8537

Modified:
   trunk/plearn/vmat/DichotomizeVMatrix.cc
   trunk/plearn/vmat/DichotomizeVMatrix.h
Log:
make the warning optional with a verbose option


Modified: trunk/plearn/vmat/DichotomizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-02-19 20:42:45 UTC (rev 8536)
+++ trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-02-19 20:48:21 UTC (rev 8537)
@@ -51,8 +51,8 @@
     "Variables with no specification will be kept as_is\n"
     );
 
-DichotomizeVMatrix::DichotomizeVMatrix()
-//    discrete_variable_instructions(NULL)
+DichotomizeVMatrix::DichotomizeVMatrix():
+    verbose(3)
 /* ### Initialize all fields to their default value */
 {
     // ...
@@ -96,7 +96,7 @@
                    }
                    output_col += 1;
                }
-               if(found_range==false)
+               if(found_range==false && verbose>2)
                {
                    PLWARNING("DichotomizeVMatrix::getNewRow() - "
                              "row %d, fields %s, have a value (%f) that are outside all dichotomize range",

Modified: trunk/plearn/vmat/DichotomizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.h	2008-02-19 20:42:45 UTC (rev 8536)
+++ trunk/plearn/vmat/DichotomizeVMatrix.h	2008-02-19 20:48:21 UTC (rev 8537)
@@ -64,6 +64,8 @@
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!
     TVec< pair<string, TVec< pair<real, real> > > > discrete_variable_instructions;
+    
+    int verbose;
 
 public:
     //#####  Public Member Functions  #########################################



From nouiz at mail.berlios.de  Wed Feb 20 15:31:51 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 20 Feb 2008 15:31:51 +0100
Subject: [Plearn-commits] r8538 - trunk/plearn/vmat
Message-ID: <200802201431.m1KEVp2j014065@sheep.berlios.de>

Author: nouiz
Date: 2008-02-20 15:31:50 +0100 (Wed, 20 Feb 2008)
New Revision: 8538

Modified:
   trunk/plearn/vmat/TransposeVMatrix.cc
   trunk/plearn/vmat/TransposeVMatrix.h
Log:
implemented getColumn. Otherwise if the source was a pmat file, we would read all the file each time we call the function getColum. It was VMatrix::getColumn version that was called


Modified: trunk/plearn/vmat/TransposeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TransposeVMatrix.cc	2008-02-19 20:48:21 UTC (rev 8537)
+++ trunk/plearn/vmat/TransposeVMatrix.cc	2008-02-20 14:31:50 UTC (rev 8538)
@@ -119,6 +119,15 @@
     source->getColumn(i, v);
 }
 
+///////////////
+// getColumn //
+///////////////
+void TransposeVMatrix::getColumn(int j, Vec v) const
+{
+    //We do not buffer the column...
+    source->getRow(j, v);
+}
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////

Modified: trunk/plearn/vmat/TransposeVMatrix.h
===================================================================
--- trunk/plearn/vmat/TransposeVMatrix.h	2008-02-19 20:48:21 UTC (rev 8537)
+++ trunk/plearn/vmat/TransposeVMatrix.h	2008-02-20 14:31:50 UTC (rev 8538)
@@ -100,6 +100,7 @@
     //! Fill the vector 'v' with the content of the i-th row.
     //! v is assumed to be the right size.
     virtual void getNewRow(int i, const Vec& v) const;
+    virtual void getColumn(int j, Vec v) const;
 
 public:
 



From nouiz at mail.berlios.de  Wed Feb 20 15:37:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 20 Feb 2008 15:37:30 +0100
Subject: [Plearn-commits] r8539 - trunk/python_modules/plearn/pymake
Message-ID: <200802201437.m1KEbU2T014413@sheep.berlios.de>

Author: nouiz
Date: 2008-02-20 15:37:30 +0100 (Wed, 20 Feb 2008)
New Revision: 8539

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
print the number of file to compile


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-02-20 14:31:50 UTC (rev 8538)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-02-20 14:37:30 UTC (rev 8539)
@@ -2801,7 +2801,7 @@
             generate_vcproj_files(target, ccfiles_to_compile, executables_to_link, linkname)
 
         else:
-            print '++++ Compiling...'
+            print '++++ Compiling',len(ccfiles_to_compile),'files...'
             if platform=='win32':
                 win32_parallel_compile(ccfiles_to_compile.keys())
             else:



From nouiz at mail.berlios.de  Wed Feb 20 20:12:43 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 20 Feb 2008 20:12:43 +0100
Subject: [Plearn-commits] r8540 - trunk/plearn/vmat
Message-ID: <200802201912.m1KJChnS029365@sheep.berlios.de>

Author: nouiz
Date: 2008-02-20 20:12:43 +0100 (Wed, 20 Feb 2008)
New Revision: 8540

Modified:
   trunk/plearn/vmat/FileVMatrix.cc
   trunk/plearn/vmat/FileVMatrix.h
Log:
code refactoring:
-moved seek to the existing moveto function
-added function openfile and always use it for opening file
   -will be usefull later for changing the buffer size


Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-02-20 14:37:30 UTC (rev 8539)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-02-20 19:12:43 UTC (rev 8540)
@@ -158,11 +158,8 @@
         if (!writable)
             PLERROR("In FileVMatrix::build_ - You asked to create a new file (%s), but 'writable' is set to 0 !", filename_.c_str());
 
-#ifdef USE_NSPR_FILE
-        f = PR_Open(filename_.c_str(), PR_RDWR | PR_CREATE_FILE | PR_TRUNCATE, 0666);
-#else
-        f = fopen(filename_.c_str(),"w+b");
-#endif
+        openfile(filename_.c_str(),"w+b", 
+                 PR_RDWR | PR_CREATE_FILE | PR_TRUNCATE, 0666);
         if (!f)
             PLERROR("In FileVMatrix constructor, could not open file %s",filename_.c_str());
 
@@ -206,21 +203,9 @@
     else
     {
         if (writable)
-        {
-#ifdef USE_NSPR_FILE
-            f = PR_Open(filename_.c_str(), PR_RDWR | PR_CREATE_FILE, 0666);
-#else
-            f = fopen(filename_.c_str(), "r+b");
-#endif
-        }
+            openfile(filename_.c_str(), "r+b", PR_RDWR | PR_CREATE_FILE, 0666);
         else
-        {
-#ifdef USE_NSPR_FILE
-            f = PR_Open(filename_.c_str(), PR_RDONLY, 0666);
-#else
-            f = fopen(filename_.c_str(), "rb");
-#endif
-        }
+            openfile(filename_.c_str(), "rb", PR_RDONLY, 0666);
 
         if (! f)
             PLERROR("FileVMatrix::build: could not open file %s", filename_.c_str());
@@ -376,23 +361,17 @@
         PLERROR("In FileVMatrix::getNewRow length of v (%d) differs from matrix width (%d)",v.length(),width_);
 #endif
 
-#ifdef USE_NSPR_FILE
     moveto(i);
+#ifdef USE_NSPR_FILE
     if(file_is_float)
         PR_Read_float(f, v.data(), v.length(), file_is_bigendian);
     else
         PR_Read_double(f, v.data(), v.length(), file_is_bigendian);
 #else
     if(file_is_float)
-    {
-        fseek(f, DATAFILE_HEADERLENGTH+(i*width_)*sizeof(float), SEEK_SET);
         fread_float(f, v.data(), v.length(), file_is_bigendian);
-    }
     else
-    {
-        fseek(f, DATAFILE_HEADERLENGTH+(i*width_)*sizeof(double), SEEK_SET);
         fread_double(f, v.data(), v.length(), file_is_bigendian);
-    }
 #endif
 }
 
@@ -440,23 +419,17 @@
 /////////
 void FileVMatrix::put(int i, int j, real value)
 {
-#ifdef USE_NSPR_FILE
     moveto(i,j);
+#ifdef USE_NSPR_FILE
     if(file_is_float)
         PR_Write_float(f,float(value),file_is_bigendian);
     else
         PR_Write_double(f,double(value),file_is_bigendian);
 #else
     if(file_is_float)
-    {
-        fseek(f, DATAFILE_HEADERLENGTH+(i*width_+j)*sizeof(float), SEEK_SET);
         fwrite_float(f,float(value),file_is_bigendian);
-    }
     else
-    {
-        fseek(f, DATAFILE_HEADERLENGTH+(i*width_+j)*sizeof(double), SEEK_SET);
         fwrite_double(f,double(value),file_is_bigendian);
-    }
 #endif
     invalidateBuffer();
 }
@@ -541,6 +514,18 @@
 #endif
 }
 
+//////////////////
+// updateHeader //
+//////////////////
+void FileVMatrix::openfile(const char *path, const char *mode,
+                           PRIntn flags, PRIntn mode2) {
+#ifdef USE_NSPR_FILE
+        f = PR_Open(path, flags, mode2);
+#else
+        f = fopen(path, mode);
+#endif
+
+}
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/FileVMatrix.h
===================================================================
--- trunk/plearn/vmat/FileVMatrix.h	2008-02-20 14:37:30 UTC (rev 8539)
+++ trunk/plearn/vmat/FileVMatrix.h	2008-02-20 19:12:43 UTC (rev 8540)
@@ -78,6 +78,8 @@
 private:
 
     bool build_new_file;
+    void openfile(const char *path, const char *mode, PRIntn flags,
+                  PRIntn mode2);
 
 public:
 



From tihocan at mail.berlios.de  Wed Feb 20 20:53:03 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 20:53:03 +0100
Subject: [Plearn-commits] r8541 - trunk/plearn/vmat
Message-ID: <200802201953.m1KJr3Vx032612@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 20:53:02 +0100 (Wed, 20 Feb 2008)
New Revision: 8541

Modified:
   trunk/plearn/vmat/VMatrix.h
Log:
Typo fix in comment

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-20 19:12:43 UTC (rev 8540)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-20 19:53:02 UTC (rev 8541)
@@ -626,7 +626,7 @@
                       );
                       
     /**
-     * @return The size of the longuest fieldname
+     * @return The size of the longest fieldname
      */
     int max_fieldnames_size() const;
 



From tihocan at mail.berlios.de  Wed Feb 20 20:53:40 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 20:53:40 +0100
Subject: [Plearn-commits] r8542 - trunk/plearn_learners/online
Message-ID: <200802201953.m1KJreos032687@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 20:53:39 +0100 (Wed, 20 Feb 2008)
New Revision: 8542

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
Log:
Added a verbosity option that may be used in subclasses

Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2008-02-20 19:53:02 UTC (rev 8541)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2008-02-20 19:53:39 UTC (rev 8542)
@@ -74,7 +74,8 @@
     output_size(-1),
     name(the_name),
     estimate_simpler_diag_hessian(false),
-    use_fast_approximations(true)
+    use_fast_approximations(true),
+    verbosity(1)
 {
     if (call_build_) {
         if (the_name.empty())
@@ -321,6 +322,11 @@
                   " operation\n"
                   "required by the module.\n");
 
+    declareOption(ol, "verbosity", &OnlineLearningModule::verbosity,
+                  OptionBase::buildoption,
+                  "Controls the level of verbosity of the module.",
+                  OptionBase::advanced_level);
+
     inherited::declareOptions(ol);
 }
 

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2008-02-20 19:53:02 UTC (rev 8541)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2008-02-20 19:53:39 UTC (rev 8542)
@@ -97,6 +97,8 @@
     //! use tables to approximate nonlinearities such as sigmoid, tanh, and softplus
     bool use_fast_approximations;
 
+    int verbosity;
+
 public:
     //#####  Public Member Fun ctions  #########################################
 



From tihocan at mail.berlios.de  Wed Feb 20 20:54:08 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 20:54:08 +0100
Subject: [Plearn-commits] r8543 - trunk/plearn_learners/distributions
Message-ID: <200802201954.m1KJs8i2032728@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 20:54:07 +0100 (Wed, 20 Feb 2008)
New Revision: 8543

Added:
   trunk/plearn_learners/distributions/RBMDistribution.cc
   trunk/plearn_learners/distributions/RBMDistribution.h
Log:
PDistribution interface for a RBMModule

Added: trunk/plearn_learners/distributions/RBMDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/RBMDistribution.cc	2008-02-20 19:53:39 UTC (rev 8542)
+++ trunk/plearn_learners/distributions/RBMDistribution.cc	2008-02-20 19:54:07 UTC (rev 8543)
@@ -0,0 +1,250 @@
+// -*- C++ -*-
+
+// RBMDistribution.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file RBMDistribution.cc */
+
+
+#include "RBMDistribution.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMDistribution,
+    "Distribution learnt by a Restricted Boltzmann Machine.",
+    "The RBM is train by standard Contrastive Divergence in online mode."
+);
+
+/////////////////////
+// RBMDistribution //
+/////////////////////
+RBMDistribution::RBMDistribution()
+{}
+
+////////////////////
+// declareOptions //
+////////////////////
+void RBMDistribution::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "rbm", &RBMDistribution::rbm,
+                  OptionBase::buildoption,
+        "Underlying RBM modeling the distribution.");
+
+    // Now call the parent class' declareOptions().
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void RBMDistribution::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////
+// build_ //
+////////////
+void RBMDistribution::build_()
+{
+    if (!rbm)
+        return;
+    int n_ports = rbm->nPorts();
+    ports_val.resize(n_ports);
+    predicted_size = rbm->visible_layer->size;
+    // Rebuild the PDistribution object to take size into account.
+    inherited::build();
+}
+
+/////////
+// cdf //
+/////////
+real RBMDistribution::cdf(const Vec& y) const
+{
+    PLERROR("cdf not implemented for RBMDistribution"); return 0;
+}
+
+/////////////////
+// expectation //
+/////////////////
+void RBMDistribution::expectation(Vec& mu) const
+{
+    PLERROR("In RBMDistribution::expectation - Not implemeted");
+}
+
+////////////
+// forget //
+////////////
+void RBMDistribution::forget()
+{
+    rbm->forget();
+    learner = NULL;
+    inherited::forget();
+    n_predicted = rbm->visible_layer->size;
+}
+
+//////////////
+// generate //
+//////////////
+void RBMDistribution::generate(Vec& y) const
+{
+    work1.resize(1, 0);
+    ports_val.fill(NULL);
+    ports_val[rbm->getPortIndex("visible_sample")] = &work1;
+    rbm->fprop(ports_val);
+    y.resize(work1.width());
+    y << work1(0);
+}
+
+///////////////
+// generateN //
+///////////////
+void RBMDistribution::generateN(const Mat& Y) const
+{
+    work1.resize(Y.length(), 0);
+    ports_val.fill(NULL);
+    ports_val[rbm->getPortIndex("visible_sample")] = &work1;
+    rbm->fprop(ports_val);
+    Y << work1;
+}
+
+/////////////////
+// log_density //
+/////////////////
+real RBMDistribution::log_density(const Vec& y) const
+{
+    ports_val.fill(NULL);
+    work1.resize(1, 0);
+    work2.resize(1, y.length());
+    work2 << y;
+    ports_val[rbm->getPortIndex("neg_log_likelihood")] = &work1;
+    ports_val[rbm->getPortIndex("visible")] = &work2;
+    rbm->fprop(ports_val);
+    return -work1(0, 0);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void RBMDistribution::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("RBMDistribution::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+////////////////////
+// resetGenerator //
+////////////////////
+void RBMDistribution::resetGenerator(long g_seed)
+{
+    if (!rbm->random_gen)
+        PLERROR("In RBMDistribution::resetGenerator - The underlying RBM "
+                "must have a random number generator");
+    if (g_seed != 0)
+        rbm->random_gen->manual_seed(g_seed);
+    inherited::resetGenerator(g_seed);
+}
+
+/////////////////
+// survival_fn //
+/////////////////
+real RBMDistribution::survival_fn(const Vec& y) const
+{
+    PLERROR("survival_fn not implemented for RBMDistribution"); return 0;
+}
+
+///////////
+// train //
+///////////
+void RBMDistribution::train()
+{
+    if (!learner) {
+        // First build the learner that will train a RBM.
+        learner = new ModuleLearner();
+        learner->module = rbm;
+        learner->seed_ = this->seed_;
+        learner->use_a_separate_random_generator_for_testing =
+            this->use_a_separate_random_generator_for_testing;
+        learner->input_ports = TVec<string>(1, "visible");
+        learner->target_ports.resize(0);
+        learner->cost_ports.resize(0);
+        learner->build();
+        learner->setTrainingSet(this->train_set);
+    }
+    learner->nstages = this->nstages;
+    learner->train();
+    this->stage = learner->stage;
+}
+
+//////////////
+// variance //
+//////////////
+void RBMDistribution::variance(Mat& covar) const
+{
+    PLERROR("variance not implemented for RBMDistribution");
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/distributions/RBMDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/RBMDistribution.h	2008-02-20 19:53:39 UTC (rev 8542)
+++ trunk/plearn_learners/distributions/RBMDistribution.h	2008-02-20 19:54:07 UTC (rev 8543)
@@ -0,0 +1,196 @@
+// -*- C++ -*-
+
+// RBMDistribution.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file RBMDistribution.h */
+
+
+#ifndef RBMDistribution_INC
+#define RBMDistribution_INC
+
+#include <plearn_learners/distributions/UnconditionalDistribution.h>
+#include <plearn_learners/online/ModuleLearner.h>
+#include <plearn_learners/online/RBMModule.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class RBMDistribution : public UnconditionalDistribution
+{
+    typedef UnconditionalDistribution inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    PP<RBMModule> rbm;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMDistribution();
+
+
+    //#####  UnconditionalDistribution Member Functions  ######################
+
+    //! Return log of probability density log(p(y)).
+    virtual real log_density(const Vec& x) const;
+
+    //! Return survival function: P(Y>y).
+    virtual real survival_fn(const Vec& y) const;
+
+    //! Return cdf: P(Y<y).
+    virtual real cdf(const Vec& y) const;
+
+    //! Return E[Y].
+    virtual void expectation(Vec& mu) const;
+
+    //! Return Var[Y].
+    virtual void variance(Mat& cov) const;
+
+    //! Return a pseudo-random sample generated from the distribution.
+    virtual void generate(Vec& y) const;
+
+    //! Overridden for efficiency purpose.
+    void generateN(const Mat& Y) const;
+
+    //! Reset the random number generator used by generate() using the
+    //! given seed.
+    virtual void resetGenerator(long g_seed);
+
+    // ### These methods may be overridden for efficiency purpose:
+    /*
+    //! Return probability density p(y)
+    virtual real density(const Vec& y) const;
+    */
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    // ### Default version of inputsize returns learner->inputsize()
+    // ### If this is not appropriate, you should uncomment this and define
+    // ### it properly in the .cc
+    // virtual int inputsize() const;
+
+    //! (Re-)initializes the PDistribution in its fresh state (that state may
+    //! depend on the 'seed' option) and sets 'stage' back to 0 (this is the
+    //! stage of a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage == nstages, updating the train_stats collector with training
+    //! costs measured on-line in the process.
+    // ### You may remove this method if your distribution does not
+    // ### implement it.
+    virtual void train();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(RBMDistribution);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    
+    //! Associated learner to train the RBM.
+    PP<ModuleLearner> learner;
+
+    //! Vector of data passed to the fprop(..) method of the RBM.
+    TVec<Mat*> ports_val;
+
+    //! Temporary storage.
+    mutable Mat work1, work2;
+    
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMDistribution);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Wed Feb 20 20:54:56 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 20:54:56 +0100
Subject: [Plearn-commits] r8544 - trunk/plearn_learners/distributions
Message-ID: <200802201954.m1KJsu01000044@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 20:54:56 +0100 (Wed, 20 Feb 2008)
New Revision: 8544

Modified:
   trunk/plearn_learners/distributions/UnconditionalDistribution.cc
   trunk/plearn_learners/distributions/UnconditionalDistribution.h
Log:
'n_predictor' is now properly (re)set to 0 in forget()

Modified: trunk/plearn_learners/distributions/UnconditionalDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/UnconditionalDistribution.cc	2008-02-20 19:54:07 UTC (rev 8543)
+++ trunk/plearn_learners/distributions/UnconditionalDistribution.cc	2008-02-20 19:54:56 UTC (rev 8544)
@@ -116,6 +116,14 @@
 {
 }
 
+////////////
+// forget //
+////////////
+void UnconditionalDistribution::forget() {
+    inherited::forget();
+    n_predictor = 0;
+}
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////

Modified: trunk/plearn_learners/distributions/UnconditionalDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/UnconditionalDistribution.h	2008-02-20 19:54:07 UTC (rev 8543)
+++ trunk/plearn_learners/distributions/UnconditionalDistribution.h	2008-02-20 19:54:56 UTC (rev 8544)
@@ -109,6 +109,9 @@
     // **** PDistribution methods ****
     // *******************************
 
+    //! Resets the distribution.
+    virtual void forget();
+ 
     //! Return an error (not used in unconditional distributions).
     void setPredictor(const Vec& predictor, bool call_parent = true) const;
 



From tihocan at mail.berlios.de  Wed Feb 20 20:56:51 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 20:56:51 +0100
Subject: [Plearn-commits] r8545 - trunk/plearn_learners/distributions
Message-ID: <200802201956.m1KJupRr000217@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 20:56:50 +0100 (Wed, 20 Feb 2008)
New Revision: 8545

Modified:
   trunk/plearn_learners/distributions/PDistribution.cc
   trunk/plearn_learners/distributions/PDistribution.h
Log:
- Added remote interface for generate() and log_density()
- Added progress bar in generateN
- generateN is now virtual so it can be overridden in subclasses for efficiency reasons


Modified: trunk/plearn_learners/distributions/PDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/PDistribution.cc	2008-02-20 19:54:56 UTC (rev 8544)
+++ trunk/plearn_learners/distributions/PDistribution.cc	2008-02-20 19:56:50 UTC (rev 8545)
@@ -192,6 +192,27 @@
 
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void PDistribution::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different from declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "log_density", &PDistribution::log_density,
+        (BodyDoc("Compute the log density of a data point"),
+         ArgDoc("sample", "The data point"),
+         RetDoc("The log density")));
+
+    declareMethod(
+        rmm, "generate", &PDistribution::remote_generate,
+        (BodyDoc("Generate a sample"),
+         RetDoc("The generated sample")));
+}
+
 ///////////
 // build //
 ///////////
@@ -378,10 +399,15 @@
         PLERROR("In PDistribution::generateN - Matrix width (%d) differs from "
                 "n_predicted (%d)", Y.width(), n_predicted);
     int N = Y.length();
+    PP<ProgressBar> pb =
+        report_progress ? new ProgressBar("Generating samples", N)
+                        : NULL;
     for(int i=0; i<N; i++)
     {
         v = Y(i);
         generate(v);
+        if (pb)
+            pb->updateone();
     }
 }
 
@@ -418,6 +444,17 @@
     return l;
 }
 
+
+/////////////////////
+// remote_generate //
+/////////////////////
+Vec PDistribution::remote_generate()
+{
+    Vec sample;
+    generate(sample);
+    return sample;
+}
+
 ////////////////////
 // resetGenerator //
 ////////////////////

Modified: trunk/plearn_learners/distributions/PDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/PDistribution.h	2008-02-20 19:54:56 UTC (rev 8544)
+++ trunk/plearn_learners/distributions/PDistribution.h	2008-02-20 19:56:50 UTC (rev 8545)
@@ -162,6 +162,9 @@
 
     //! Declares this class' options.
     static void declareOptions(OptionList& ol);
+    
+    //! Declare the methods that are remote-callable.
+    static void declareMethods(RemoteMethodMap& rmm);
 
 public:
 
@@ -296,9 +299,12 @@
     //! distribution, of density p(y | x).
     virtual void generate(Vec& y) const;
 
+    //! Remote interface for 'generate'.
+    Vec remote_generate();
+
     //! X must be a N x n_predicted matrix. that will be filled.
     //! This will call generate N times to fill the N rows of the matrix.
-    void generateN(const Mat& Y) const;
+    virtual void generateN(const Mat& Y) const;
 
     //! Generates a pseudo-random sample (x,y) from the JOINT distribution,
     //! of density p(x, y)



From tihocan at mail.berlios.de  Wed Feb 20 20:57:42 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 20:57:42 +0100
Subject: [Plearn-commits] r8546 - trunk/plearn_learners/generic
Message-ID: <200802201957.m1KJvg1S000278@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 20:57:42 +0100 (Wed, 20 Feb 2008)
New Revision: 8546

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Typo fix in comments

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2008-02-20 19:56:50 UTC (rev 8545)
+++ trunk/plearn_learners/generic/PLearner.cc	2008-02-20 19:57:42 UTC (rev 8546)
@@ -270,8 +270,8 @@
 ////////////////////
 void PLearner::declareMethods(RemoteMethodMap& rmm)
 {
-    // Insert a backpointer to remote methods; note that this
-    // different than for declareOptions()
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
     rmm.inherited(inherited::_getRemoteMethodMap_());
 
     declareMethod(



From tihocan at mail.berlios.de  Wed Feb 20 21:01:05 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 21:01:05 +0100
Subject: [Plearn-commits] r8547 - trunk/plearn_learners/online
Message-ID: <200802202001.m1KK15wP000694@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 21:01:04 +0100 (Wed, 20 Feb 2008)
New Revision: 8547

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
- Can now ask for more than one sample at a time
- Can now obtain samples even if there is nothing readily available to initialize the Gibbs chain (in which case we start from zero)
- Added progress bar when during Gibbs sampling, for verbosity >= 2
- Gibbs_step is now properly reset in forget() and when we restart the Gibbs chain
- Added some safety asserts


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-02-20 19:57:42 UTC (rev 8546)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-02-20 20:01:04 UTC (rev 8547)
@@ -1102,27 +1102,65 @@
         }
         else // sample unconditionally: Gibbs sample after k steps
         {
+            // Find out how many samples we want.
+            int n_samples = -1;
+            if (visible_sample_is_output)
+                n_samples = visible_sample->length();
+            if (visible_expectation_is_output) {
+                PLASSERT( n_samples == -1 ||
+                          n_samples == visible_expectation->length() );
+                n_samples = visible_expectation->length();
+            }
+            if (hidden_sample_is_output) {
+                PLASSERT( n_samples == -1 ||
+                          n_samples == hidden_sample->length() );
+                n_samples = hidden_sample->length();
+            }
+            PLCHECK( n_samples > 0 );
+            
             // the visible_layer->expectations contain the "state" from which we
             // start or continue the chain
-            if (visible_layer->samples.isEmpty())
+            if (visible_layer->samples.length() != n_samples)
             {
-                if (visible && !visible_is_output)
+                // There are no samples already available to continue the
+                // chain: we restart it.
+                Gibbs_step = 0;
+                if (visible && !visible_is_output
+                            && visible->length() == n_samples)
                     visible_layer->samples << *visible;
-                else if (!visible_layer->getExpectations().isEmpty())
+                else if (visible_layer->getExpectations().length() ==
+                                                                    n_samples)
                     visible_layer->samples << visible_layer->getExpectations();
-                else if (!hidden_layer->samples.isEmpty())
+                else if (hidden_layer->samples.length() == n_samples)
                     sampleVisibleGivenHidden(hidden_layer->samples);
-                else if (!hidden_layer->getExpectations().isEmpty())
+                else if (hidden_layer->getExpectations().length() == n_samples)
                     sampleVisibleGivenHidden(hidden_layer->getExpectations());
+                else {
+                    // There is no available data to initialize the chain: we
+                    // initialize it with a zero vector.
+                    Mat& zero_vector = visible_layer->samples;
+                    PLASSERT( zero_vector.width() > 0 );
+                    zero_vector.resize(n_samples, zero_vector.width());
+                    zero_vector.clear();
+                }
+                PLASSERT( visible_layer->samples.length() == n_samples );
             }
             int min_n = max(Gibbs_step+n_Gibbs_steps_per_generated_sample,
                             min_n_Gibbs_steps);
             //cout << "Gibbs sampling " << Gibbs_step+1;
+            PP<ProgressBar> pb =
+                verbosity >= 2 ? new ProgressBar("Gibbs sampling",
+                                                 min_n - Gibbs_step)
+                               : NULL;
             for (;Gibbs_step<min_n;Gibbs_step++)
             {
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
+                if (pb)
+                    pb->updateone();
             }
+            if (pb)
+                pb = NULL;
             hidden_activations_are_computed = false;
             //cout << " -> " << Gibbs_step << endl;
         }
@@ -1137,6 +1175,8 @@
         if (visible_sample && visible_sample_is_output)
             // provide sample of the visible units
         {
+            PLASSERT( visible_sample->length() ==
+                      visible_layer->samples.length() );
             visible_sample->resize(visible_layer->samples.length(),
                                    visible_layer->samples.width());
             *visible_sample << visible_layer->samples;
@@ -1144,6 +1184,8 @@
         if (hidden_sample && hidden_sample_is_output)
             // provide sample of the hidden units
         {
+            PLASSERT( hidden_sample->length() ==
+                      hidden_layer->samples.length() );
             hidden_sample->resize(hidden_layer->samples.length(),
                                   hidden_layer->samples.width());
             *hidden_sample << hidden_layer->samples;
@@ -1152,18 +1194,23 @@
             // provide expectation of the visible units
         {
             const Mat& to_store = visible_layer->getExpectations();
+            PLASSERT( visible_expectation->length() == to_store.length() );
             visible_expectation->resize(to_store.length(),
                                         to_store.width());
             *visible_expectation << to_store;
         }
         if (hidden && hidden_is_output)
         {
+            PLASSERT( hidden->length() ==
+                      hidden_layer->getExpectations().length() );
             hidden->resize(hidden_layer->getExpectations().length(),
                            hidden_layer->getExpectations().width());
             *hidden << hidden_layer->getExpectations();
         }
         if (hidden_act && hidden_act_is_output)
         {
+            PLASSERT( hidden_act->length() ==
+                      hidden_layer->activations.length() );
             hidden_act->resize(hidden_layer->activations.length(),
                                hidden_layer->activations.width());
             *hidden_act << hidden_layer->activations;
@@ -1966,6 +2013,7 @@
     if (reconstruction_connection && reconstruction_connection != connection)
         // We avoid to call forget() twice if the connections are the same.
         reconstruction_connection->forget();
+    Gibbs_step = 0;
 }
 
 //////////////////



From larocheh at mail.berlios.de  Wed Feb 20 21:09:16 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 20 Feb 2008 21:09:16 +0100
Subject: [Plearn-commits] r8548 - trunk/plearn_learners/online
Message-ID: <200802202009.m1KK9GGT002525@sheep.berlios.de>

Author: larocheh
Date: 2008-02-20 21:09:16 +0100 (Wed, 20 Feb 2008)
New Revision: 8548

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Added an option to use a sample for the top layer statistic.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-02-20 20:01:04 UTC (rev 8547)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-02-20 20:09:16 UTC (rev 8548)
@@ -66,6 +66,7 @@
     use_classification_cost( true ),
     reconstruct_layerwise( false ),
     i_output_layer( -1 ),
+    use_sample_for_up_layer( false ),
     online ( false ),
     background_gibbs_update_ratio(0),
     gibbs_chain_reinit_freq( INT_MAX ),
@@ -228,6 +229,11 @@
                   "(except the first one) of the RBM. These costs are not\n"
                   "back-propagated to previous layers.\n");
 
+    declareOption(ol, "use_sample_for_up_layer", &DeepBeliefNet::use_sample_for_up_layer,
+                  OptionBase::buildoption,
+                  "Indication that the update of the top layer during CD uses\n"
+                  "a sample, not the expectation.\n");
+
     declareOption(ol, "online", &DeepBeliefNet::online,
                   OptionBase::buildoption,
                   "If true then all unsupervised training stages (as well as\n"
@@ -1996,13 +2002,16 @@
         pos_up_vals.resize(minibatch_size, up_layer->size);
 
         pos_down_vals << down_layer->getExpectations();
-        pos_up_vals << up_layer->getExpectations();
+        if( !use_sample_for_up_layer )
+            pos_up_vals << up_layer->getExpectations();
 
         // down propagation, starting from a sample of up_layer
         if (background_gibbs_update_ratio<1)
             // then do some contrastive divergence, o/w only background Gibbs
         {
             up_layer->generateSamples();
+            if( use_sample_for_up_layer )
+                pos_up_vals << up_layer->samples;
             connection->setAsUpInputs( up_layer->samples );
             down_layer->getAllActivations( connection, 0, true );
             down_layer->computeExpectations();
@@ -2015,7 +2024,14 @@
             // accumulate negative stats
             // no need to deep-copy because the values won't change before update
             Mat neg_down_vals = down_layer->samples;
-            Mat neg_up_vals = up_layer->getExpectations();
+            Mat neg_up_vals;
+            if( use_sample_for_up_layer)
+            {
+                up_layer->generateSamples();
+                neg_up_vals = up_layer->samples;
+            }
+            else
+                neg_up_vals = up_layer->getExpectations();
 
             if (background_gibbs_update_ratio==0)
             // update here only if there is ONLY contrastive divergence
@@ -2097,7 +2113,10 @@
         pos_up_val.resize( up_layer->size );
 
         pos_down_val << down_layer->expectation;
-        pos_up_val << up_layer->expectation;
+        if( use_sample_for_up_layer )
+            pos_up_val << up_layer->sample;
+        else
+            pos_up_val << up_layer->expectation;
 
         // down propagation, starting from a sample of up_layer
         connection->setAsUpInput( up_layer->sample );
@@ -2113,8 +2132,16 @@
         // accumulate negative stats
         // no need to deep-copy because the values won't change before update
         Vec neg_down_val = down_layer->sample;
-        Vec neg_up_val = up_layer->expectation;
+        Vec neg_up_val;
+        if( use_sample_for_up_layer )
+        {
+            up_layer->generateSample();
+            neg_up_val = up_layer->sample;
+        }
+        else
+            neg_up_val = up_layer->expectation;
 
+
         // update
         down_layer->update( pos_down_val, neg_down_val );
         connection->update( pos_down_val, pos_up_val,

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2008-02-20 20:01:04 UTC (rev 8547)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2008-02-20 20:09:16 UTC (rev 8548)
@@ -144,6 +144,10 @@
     //! costs are not back-propagated to previous layers.
     TVec< PP<CostModule> > partial_costs;
 
+    //! Indication that the update of the top layer during CD uses
+    //! a sample, not the expectation.
+    bool use_sample_for_up_layer;
+
     //#####  Public Learnt Options  ###########################################
     //! The module computing the probabilities of the different classes.
     PP<RBMClassificationModule> classification_module;



From larocheh at mail.berlios.de  Wed Feb 20 21:10:34 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 20 Feb 2008 21:10:34 +0100
Subject: [Plearn-commits] r8549 - trunk/plearn_learners/online
Message-ID: <200802202010.m1KKAYAp002959@sheep.berlios.de>

Author: larocheh
Date: 2008-02-20 21:10:33 +0100 (Wed, 20 Feb 2008)
New Revision: 8549

Added:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
RBM layer with tree structured groups of neurons.


Added: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-02-20 20:09:16 UTC (rev 8548)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-02-20 20:10:33 UTC (rev 8549)
@@ -0,0 +1,1193 @@
+// -*- C++ -*-
+
+// RBMWoodsLayer.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMWoodsLayer.cc */
+
+
+
+#include "RBMWoodsLayer.h"
+#include <plearn/math/TMat_maths.h>
+#include "RBMConnection.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMWoodsLayer,
+    "RBM layer with tree-structured groups of units.",
+    "");
+
+RBMWoodsLayer::RBMWoodsLayer( real the_learning_rate ) :
+    inherited( the_learning_rate ),
+    n_trees( 10 ),
+    tree_depth( 3 )
+{
+}
+
+////////////////////
+// generateSample //
+////////////////////
+void RBMWoodsLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectation_is_up_to_date, "Expectation should be computed "
+            "before calling generateSample()");
+
+    sample.clear();
+
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, node_sample, sub_tree_size;
+    int offset = 0;
+
+    for( int t=0; t<n_trees; t++ )
+    {
+        depth = 0;
+        node = n_nodes_per_tree / 2;
+        sub_tree_size = node;
+        while( depth < tree_depth )
+        {
+            node_sample = random_gen->binomial_sample( 
+                local_node_expectation[ node + offset ] );
+            sample[node + offset] = node_sample;
+
+            // Descending in the tree
+            sub_tree_size /= 2;
+            if ( node_sample > 0.5 )
+                node -= sub_tree_size+1;
+            else
+                node += sub_tree_size+1;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+}
+
+/////////////////////
+// generateSamples //
+/////////////////////
+void RBMWoodsLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectations_are_up_to_date, "Expectations should be computed "
+            "before calling generateSamples()");
+
+    PLASSERT( samples.width() == size && samples.length() == batch_size );
+
+    PLERROR( "RBMWoodsLayer::generateSamples(): not implemented yet" );
+}
+
+void RBMWoodsLayer::computeProbabilisticClustering(Vec& prob_clusters)
+{
+    int n_nodes_per_tree = size / n_trees;    
+    int n_leaves = n_nodes_per_tree+1;
+    prob_clusters.resize( n_trees * n_leaves );
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+
+    // Get local expectations at every node
+    
+    // Divide and conquer computation of local (conditional) free energies
+    for( int t=0; t<n_trees; t++ )
+    {
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        // Initialize last level
+        for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
+            //on_free_energy[ n + offset ] = safeexp(activation[n+offset]);
+            //off_free_energy[ n + offset ] = 1;
+            // Now working in log-domain
+            on_free_energy[ n + offset ] = activation[n+offset];
+            off_free_energy[ n + offset ] = 0;
+            
+        }
+
+        depth = tree_depth-2;
+        sub_tree_size = 1;
+
+        while( depth >= 0 )
+        {
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
+                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
+                //off_free_energy[ n + offset ] = 
+                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
+                // Now working in log-domain
+                on_free_energy[ n + offset ] = activation[n+offset] + 
+                    logadd( on_free_energy[n + offset - sub_tree_size],
+                            off_free_energy[n + offset - sub_tree_size] ) ;
+                off_free_energy[ n + offset ] = 
+                    logadd( on_free_energy[n + offset + sub_tree_size],
+                            off_free_energy[n + offset + sub_tree_size] ) ;
+
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+        offset += n_nodes_per_tree;
+    }    
+    
+    for( int i=0 ; i<size ; i++ )
+        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
+        // Now working in log-domain
+        local_node_expectation[i] = safeexp(on_free_energy[i] 
+                                            - logadd(on_free_energy[i], off_free_energy[i]));
+
+    // Compute marginal expectations over clustering
+    offset = 0;
+    for( int t=0; t<n_trees; t++ )
+    {
+        // Initialize root        
+        node = n_nodes_per_tree / 2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
+        sub_tree_size = node;
+
+        // First level nodes
+        depth = 1;
+        sub_tree_size /= 2;
+
+        // Left child
+        node = sub_tree_size;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        
+        // Right child
+        node = 3*sub_tree_size+2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+
+        // Set other nodes, level-wise
+        depth = 2;
+        sub_tree_size /= 2;
+        while( depth < tree_depth )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+
+    offset = 0;
+    for( int t=0; t<n_trees; t++ )
+    {
+        for( int i=0; i<n_nodes_per_tree; i = i+2)
+            prob_clusters[i+offset+t] = expectation[i+offset];
+        for( int i=0; i<n_nodes_per_tree; i = i+2)
+            prob_clusters[i+1+offset+t] = off_expectation[i+offset];
+        offset += n_nodes_per_tree;
+    }
+}
+
+////////////////////////
+// computeExpectation //
+////////////////////////
+void RBMWoodsLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+
+    // Get local expectations at every node
+    
+    // Divide and conquer computation of local (conditional) free energies
+    for( int t=0; t<n_trees; t++ )
+    {
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        // Initialize last level
+        for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
+            //on_free_energy[ n + offset ] = safeexp(activation[n+offset]);
+            //off_free_energy[ n + offset ] = 1;
+            // Now working in log-domain
+            on_free_energy[ n + offset ] = activation[n+offset];
+            off_free_energy[ n + offset ] = 0;
+            
+        }
+
+        depth = tree_depth-2;
+        sub_tree_size = 1;
+
+        while( depth >= 0 )
+        {
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energy[ n + offset ] = safeexp(activation[n+offset]) * 
+                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
+                //off_free_energy[ n + offset ] = 
+                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
+                // Now working in log-domain
+                on_free_energy[ n + offset ] = activation[n+offset] + 
+                    logadd( on_free_energy[n + offset - sub_tree_size],
+                            off_free_energy[n + offset - sub_tree_size] ) ;
+                off_free_energy[ n + offset ] = 
+                    logadd( on_free_energy[n + offset + sub_tree_size],
+                            off_free_energy[n + offset + sub_tree_size] ) ;
+
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+        offset += n_nodes_per_tree;
+    }    
+    
+    for( int i=0 ; i<size ; i++ )
+        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
+        // Now working in log-domain
+        local_node_expectation[i] = safeexp(on_free_energy[i] 
+                                            - logadd(on_free_energy[i], off_free_energy[i]));
+
+    // Compute marginal expectations
+    offset = 0;
+    for( int t=0; t<n_trees; t++ )
+    {
+        // Initialize root        
+        node = n_nodes_per_tree / 2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
+        sub_tree_size = node;
+
+        // First level nodes
+        depth = 1;
+        sub_tree_size /= 2;
+
+        // Left child
+        node = sub_tree_size;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+
+        // Right child
+        node = 3*sub_tree_size+2;
+        expectation[ node + offset ] = local_node_expectation[ node + offset ]
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+
+        // Set other nodes, level-wise
+        depth = 2;
+        sub_tree_size /= 2;
+        while( depth < tree_depth )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = expectation[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                expectation[ n + offset ] = local_node_expectation[ n + offset ]
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+
+    expectation_is_up_to_date = true;
+}
+
+/////////////////////////
+// computeExpectations //
+/////////////////////////
+void RBMWoodsLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLERROR( "RBMWoodsLayer::computeExpectations(): not implemented yet" );
+
+    expectations_are_up_to_date = true;
+}
+
+///////////
+// fprop //
+///////////
+void RBMWoodsLayer::fprop( const Vec& input, Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;    
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+
+    // Get local expectations at every node
+    
+    // Divide and conquer computation of local (conditional) free energies
+    for( int t=0; t<n_trees; t++ )
+    {
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        // Initialize last level
+        for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+        {
+            //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]);
+            //off_free_energy[ n + offset ] = 1;
+            // Now working in log-domain
+            on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset];
+            off_free_energy[ n + offset ] = 0;            
+        }
+
+        depth = tree_depth-2;
+        sub_tree_size = 1;
+
+        while( depth >= 0 )
+        {
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energy[ n + offset ] = safeexp(input[n+offset] + bias[n+offset]) * 
+                //    ( on_free_energy[n + offset - sub_tree_size] + off_free_energy[n + offset - sub_tree_size] ) ;
+                //off_free_energy[ n + offset ] = 
+                //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
+                // Now working in the log-domain
+                on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset] +
+                    logadd( on_free_energy[n + offset - sub_tree_size], 
+                            off_free_energy[n + offset - sub_tree_size] ) ;
+                off_free_energy[ n + offset ] = 
+                    logadd( on_free_energy[n + offset + sub_tree_size], 
+                            off_free_energy[n + offset + sub_tree_size] ) ;
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+        offset += n_nodes_per_tree;
+    }    
+    
+    for( int i=0 ; i<size ; i++ )
+        //local_node_expectation[i] = on_free_energy[i] / ( on_free_energy[i] + off_free_energy[i] );
+        // Now working in log-domain
+        local_node_expectation[i] = safeexp(on_free_energy[i] 
+                                            - logadd(on_free_energy[i], off_free_energy[i]));
+
+    // Compute marginal expectations
+    offset = 0;    
+    for( int t=0; t<n_trees; t++ )
+    {
+        // Initialize root        
+        node = n_nodes_per_tree / 2;
+        output[ node + offset ] = local_node_expectation[ node + offset ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ]);
+        sub_tree_size = node;
+
+        // First level nodes
+        depth = 1;
+        sub_tree_size /= 2;
+
+        // Left child
+        node = sub_tree_size;
+        output[ node + offset ] = local_node_expectation[ node + offset ]
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * local_node_expectation[ node + offset + sub_tree_size + 1 ];
+
+        // Right child
+        node = 3*sub_tree_size+2;
+        output[ node + offset ] = local_node_expectation[ node + offset ]
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+        off_expectation[ node + offset ] = (1 - local_node_expectation[ node + offset ])
+            * (1 - local_node_expectation[ node + offset - sub_tree_size - 1 ]);
+
+        // Set other nodes, level-wise
+        depth = 2;
+        sub_tree_size /= 2;
+        while( depth < tree_depth )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = output[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                output[ n + offset ] = local_node_expectation[ n + offset ]
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * local_node_expectation[ n + offset + sub_tree_size + 1 ]
+                    * grand_parent_prob;
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = output[ grand_parent ];
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    left_of_grand_parent = true;
+                }
+
+                output[ n + offset ] = local_node_expectation[ n + offset ]
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+                off_expectation[ n + offset ] = (1 - local_node_expectation[ n + offset ])
+                    * (1 - local_node_expectation[ n + offset - sub_tree_size - 1 ])
+                    * grand_parent_prob;
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+        offset += n_nodes_per_tree;
+    }
+}
+
+void RBMWoodsLayer::fprop( const Mat& inputs, Mat& outputs ) const
+{
+    int mbatch_size = inputs.length();
+    PLASSERT( inputs.width() == size );
+    outputs.resize( mbatch_size, size );
+
+    PLERROR( "RBMWoodsLayer::fprop(): not implemented yet" );
+}
+
+void RBMWoodsLayer::fprop( const Vec& input, const Vec& rbm_bias,
+                              Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( rbm_bias.size() == input_size );
+    output.resize( output_size );
+
+    PLERROR( "RBMWoodsLayer::fprop(): not implemented yet" );
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMWoodsLayer::bpropUpdate(const Vec& input, const Vec& output,
+                                   Vec& input_gradient,
+                                   const Vec& output_gradient,
+                                   bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    // Compute gradient on marginal expectations
+    int n_nodes_per_tree = size / n_trees;    
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+    real node_exp, parent_exp, out_grad, off_grad;
+    local_node_expectation_gradient.clear();
+    on_tree_gradient.clear();
+    off_tree_gradient.clear();
+
+    for( int t=0; t<n_trees; t++ )
+    {
+        // Set other nodes, level-wise
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+        while( depth > 1 )
+        {
+            // Left child
+            left_of_grand_parent = true;
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                out_grad = output_gradient[ n + offset ] + 
+                    on_tree_gradient[ n + offset ] ;
+                off_grad = off_tree_gradient[ n + offset ] ;
+                node_exp = local_node_expectation[ n + offset ];
+                parent_exp = local_node_expectation[ n + offset + sub_tree_size + 1 ];
+
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + 3*sub_tree_size + 3;
+                    grand_parent_prob = output[ grand_parent ];
+                    // Gradient for rest of the tree
+                    on_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) ) 
+                        * parent_exp;
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - sub_tree_size - 1;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    // Gradient for rest of the tree
+                    off_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) )
+                        * parent_exp;
+                    left_of_grand_parent = true;
+                }
+
+                // Gradient w/r current node
+                local_node_expectation_gradient[ n + offset ] += 
+                    ( out_grad - off_grad ) * parent_exp * grand_parent_prob
+                    * node_exp * ( 1 - node_exp );
+
+                // Gradient w/r parent node
+                local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] += 
+                        ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob
+                        * parent_exp * (1-parent_exp) ;
+
+            }
+
+            // Right child
+            left_of_grand_parent = true;
+            for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+            {
+                out_grad = output_gradient[ n + offset ] + 
+                    on_tree_gradient[ n + offset ] ;
+                off_grad = off_tree_gradient[ n + offset ] ;
+                node_exp = local_node_expectation[ n + offset ];
+                parent_exp = local_node_expectation[ n + offset - sub_tree_size - 1 ];
+
+                if( left_of_grand_parent )
+                {
+                    grand_parent = n + offset + sub_tree_size + 1;
+                    grand_parent_prob = output[ grand_parent ];
+                    // Gradient for rest of the tree
+                    on_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) ) 
+                        * ( 1 - parent_exp );
+                    left_of_grand_parent = false;
+                }
+                else
+                {
+                    grand_parent = n + offset - 3*sub_tree_size - 3;
+                    grand_parent_prob = off_expectation[ grand_parent ];
+                    // Gradient for rest of the tree
+                    off_tree_gradient[ grand_parent ] += 
+                        ( out_grad * node_exp 
+                          + off_grad * (1 - node_exp) ) 
+                        * ( 1 - parent_exp );
+                    left_of_grand_parent = true;
+                }
+
+                // Gradient w/r current node
+                local_node_expectation_gradient[ n + offset ] += 
+                    ( out_grad - off_grad ) * ( 1 - parent_exp ) * grand_parent_prob
+                    * node_exp * ( 1 - node_exp );
+
+                // Gradient w/r parent node
+                local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -= 
+                    ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob
+                    * parent_exp * (1-parent_exp) ;
+            }
+            sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+            depth--;
+        }
+
+        ////// First level nodes
+        depth = 1;
+
+        //// Left child
+        node = sub_tree_size;
+        out_grad = output_gradient[ node + offset ] + 
+            on_tree_gradient[ node + offset ] ;
+        off_grad = off_tree_gradient[ node + offset ] ;
+        node_exp = local_node_expectation[ node + offset ];
+        parent_exp = local_node_expectation[ node + offset + sub_tree_size + 1 ];
+        
+        // Gradient w/r current node
+        local_node_expectation_gradient[ node + offset ] += 
+            ( out_grad - off_grad ) * parent_exp
+            * node_exp * ( 1 - node_exp );
+        
+        // Gradient w/r parent node
+        local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] += 
+            ( out_grad * node_exp  + off_grad * (1 - node_exp) )
+            * parent_exp * (1-parent_exp) ;
+
+        //// Right child
+        node = 3*sub_tree_size+2;
+        out_grad = output_gradient[ node + offset ] + 
+            on_tree_gradient[ node + offset ] ;
+        off_grad = off_tree_gradient[ node + offset ] ;
+        node_exp = local_node_expectation[ node + offset ];
+        parent_exp = local_node_expectation[ node + offset - sub_tree_size - 1 ];
+
+        // Gradient w/r current node
+        local_node_expectation_gradient[ node + offset ] += 
+            ( out_grad - off_grad ) * ( 1 - parent_exp ) 
+            * node_exp * ( 1 - node_exp );
+        
+        // Gradient w/r parent node
+        local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -= 
+            ( out_grad * node_exp + off_grad * (1 - node_exp) ) 
+            * parent_exp * (1-parent_exp) ;
+        
+        ////// Root
+        node = n_nodes_per_tree / 2;
+        sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+
+        out_grad = output_gradient[ node + offset ] + 
+            on_tree_gradient[ node + offset ] ;
+        off_grad = off_tree_gradient[ node + offset ] ;
+        node_exp = local_node_expectation[ node + offset ];
+        local_node_expectation_gradient[ node + offset ] += 
+            ( out_grad - off_grad ) * node_exp * ( 1 - node_exp );
+
+        offset += n_nodes_per_tree;
+    }
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        node_exp = local_node_expectation[i];
+        out_grad = local_node_expectation_gradient[i];
+        on_free_energy_gradient[i] = out_grad * node_exp * ( 1 - node_exp );
+        off_free_energy_gradient[i] = -out_grad * node_exp * ( 1 - node_exp );
+    }
+
+    offset = 0;
+    for( int t=0; t<n_trees; t++ )
+    {
+        depth = 0;
+        sub_tree_size = n_nodes_per_tree / 2;
+
+        while( depth < tree_depth-1 )
+        {
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                out_grad = on_free_energy_gradient[ n + offset ];
+                node_exp = local_node_expectation[n + offset - sub_tree_size];
+                input_gradient[n+offset] += out_grad;
+                on_free_energy[n + offset - sub_tree_size] += out_grad * node_exp; 
+                off_free_energy[n + offset - sub_tree_size] += out_grad * (1 - node_exp); 
+
+                out_grad = off_free_energy_gradient[ n + offset ];
+                node_exp = local_node_expectation[n + offset + sub_tree_size];
+                on_free_energy[n + offset + sub_tree_size] += out_grad * node_exp; 
+                off_free_energy[n + offset + sub_tree_size] += out_grad * (1 - node_exp); 
+            }
+            sub_tree_size /= 2;
+            depth++;
+        }
+
+        depth = tree_depth-1;
+        sub_tree_size = 0;
+
+        for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            input_gradient[n+offset] += on_free_energy_gradient[ n + offset ];
+
+        offset += n_nodes_per_tree;
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        real in_grad_i = input_gradient[i];
+        input_gradient[i] += in_grad_i;
+
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
+    }
+
+    applyBiasDecay();
+}
+
+void RBMWoodsLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                                   Mat& input_gradients,
+                                   const Mat& output_gradients,
+                                   bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    int mbatch_size = inputs.length();
+    PLASSERT( outputs.length() == mbatch_size );
+    PLASSERT( output_gradients.length() == mbatch_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &&
+                input_gradients.length() == mbatch_size,
+                "Cannot resize input_gradients and accumulate into it" );
+    }
+    else
+    {
+        input_gradients.resize(mbatch_size, size);
+        input_gradients.clear();
+    }
+
+    PLERROR( "RBMWoodsLayer::bpropUpdate(): not implemeted yet" );
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    // TODO Can we do this more efficiently? (using BLAS)
+
+    // We use the average gradient over the mini-batch.
+    real avg_lr = learning_rate / inputs.length();
+
+    for (int j = 0; j < mbatch_size; j++)
+    {
+        for( int i=0 ; i<size ; i++ )
+        {
+            real output_i = outputs(j, i);
+            real in_grad_i = output_i * (1-output_i) * output_gradients(j, i);
+            input_gradients(j, i) += in_grad_i;
+
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= avg_lr * in_grad_i;
+            }
+            else
+            {
+                PLERROR("In RBMWoodsLayer:bpropUpdate - Not implemented for "
+                        "momentum with mini-batches");
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                bias[i] += bias_inc[i];
+            }
+        }
+    }
+
+    applyBiasDecay();
+}
+
+
+//! TODO: add "accumulate" here
+void RBMWoodsLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
+                                   const Vec& output,
+                                   Vec& input_gradient, Vec& rbm_bias_gradient,
+                                   const Vec& output_gradient)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( rbm_bias.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+    input_gradient.resize( size );
+    rbm_bias_gradient.resize( size );
+
+    PLERROR( "RBMWoodsLayer::bpropUpdate(): not implemeted yet" );
+
+    for( int i=0 ; i<size ; i++ )
+    {
+        real output_i = output[i];
+        input_gradient[i] = output_i * (1-output_i) * output_gradient[i];
+    }
+
+    rbm_bias_gradient << input_gradient;
+    addBiasDecay(rbm_bias_gradient);
+}
+
+real RBMWoodsLayer::fpropNLL(const Vec& target)
+{
+    PLASSERT( target.size() == input_size );
+
+    PLERROR( "RBMWoodsLayer::fpropNLL(): not implemeted yet" );
+
+    real ret = 0;
+    real target_i, activation_i;
+    if(use_fast_approximations){
+        for( int i=0 ; i<size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += tabulated_softplus(activation_i) - target_i * activation_i;
+            // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+            // but it is numerically unstable, so use instead the following identity:
+            //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+            //     = act + softplus(-act) - target*act 
+            //     = softplus(act) - target*act
+        }
+    } else {
+        for( int i=0 ; i<size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += softplus(activation_i) - target_i * activation_i;
+        }
+    }
+    return ret;
+}
+
+void RBMWoodsLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
+{
+    // computeExpectations(); // why?
+
+    PLERROR( "RBMWoodsLayer::fpropNLL(): not implemeted yet" );
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    for (int k=0;k<batch_size;k++) // loop over minibatch
+    {
+        real nll = 0;
+        real* activation = activations[k];
+        real* target = targets[k];
+        if(use_fast_approximations){
+            for( int i=0 ; i<size ; i++ ) // loop over outputs
+            {
+                if(!fast_exact_is_equal(target[i],0.0))
+                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // but it is numerically unstable, so use instead
+                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                    nll += target[i] * tabulated_softplus(-activation[i]);
+                if(!fast_exact_is_equal(target[i],1.0))
+                    // nll -= (1-target[i]) * pl_log(1-output[i]);
+                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                    //                         = log(1/(1+exp(x)))
+                    //                         = -log(1+exp(x))
+                    //                         = -softplus(x)
+                    nll += (1-target[i]) * tabulated_softplus(activation[i]);
+            }
+        } else {
+            for( int i=0 ; i<size ; i++ ) // loop over outputs
+            {
+                if(!fast_exact_is_equal(target[i],0.0))
+                    // nll -= target[i] * pl_log(expectations[i]); 
+                    // but it is numerically unstable, so use instead
+                    // log (1/(1+exp(-x))) = -log(1+exp(-x)) = -softplus(-x)
+                    nll += target[i] * softplus(-activation[i]);
+                if(!fast_exact_is_equal(target[i],1.0))
+                    // nll -= (1-target[i]) * pl_log(1-output[i]);
+                    // log (1 - 1/(1+exp(-x))) = log(exp(-x)/(1+exp(-x)))
+                    //                         = log(1/(1+exp(x)))
+                    //                         = -log(1+exp(x))
+                    //                         = -softplus(x)
+                    nll += (1-target[i]) * softplus(activation[i]);
+            }
+        }
+        costs_column(k,0) = nll;
+    }
+}
+
+void RBMWoodsLayer::bpropNLL(const Vec& target, real nll, Vec& bias_gradient)
+{
+    PLERROR( "RBMWoodsLayer::bpropNLL(): not implemeted yet" );
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
+    addBiasDecay(bias_gradient);
+}
+
+void RBMWoodsLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
+                                Mat& bias_gradients)
+{
+    PLERROR( "RBMWoodsLayer::bpropNLL(): not implemeted yet" );
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
+
+    addBiasDecay(bias_gradients);
+}
+
+void RBMWoodsLayer::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "n_trees", &RBMWoodsLayer::n_trees,
+                  OptionBase::buildoption,
+                  "Number of trees in the woods.");
+
+    declareOption(ol, "tree_depth", &RBMWoodsLayer::tree_depth,
+                  OptionBase::buildoption,
+                  "Depth of the trees in the woods (1 gives the ordinary "
+                  "RBMBinomialLayer).");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMWoodsLayer::build_()
+{
+    PLASSERT( n_trees > 0 );
+    PLASSERT( tree_depth > 0 );
+
+    if ( tree_depth < 2 )
+        PLERROR("RBMWoodsLayer::build_(): tree_depth < 2 not supported, use "
+                "RBMBinomialLayer instead.");
+
+    size = n_trees * ( ipow( 2, tree_depth ) - 1 );
+    local_node_expectation.resize( size );
+    on_free_energy.resize( size );
+    off_free_energy.resize( size );
+    off_expectation.resize( size );
+    local_node_expectation_gradient.resize( size );
+    on_tree_gradient.resize( size );
+    off_tree_gradient.resize( size );
+    on_free_energy_gradient.resize( size );
+    off_free_energy_gradient.resize( size );
+
+    // Must call parent's build, since size was just set
+    inherited::build();
+}
+
+void RBMWoodsLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMWoodsLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField( off_expectation, copies );
+    deepCopyField( local_node_expectation, copies );
+    deepCopyField( on_free_energy, copies );
+    deepCopyField( off_free_energy, copies );
+    deepCopyField( local_node_expectation_gradient, copies );
+    deepCopyField( on_tree_gradient, copies );
+    deepCopyField( off_tree_gradient, copies );
+    deepCopyField( on_free_energy_gradient, copies );
+    deepCopyField( off_free_energy_gradient, copies );
+}
+
+real RBMWoodsLayer::energy(const Vec& unit_values) const
+{
+    PLERROR( "RBMWoodsLayer::energy(): not implemeted yet" );
+    return -dot(unit_values, bias);
+}
+
+real RBMWoodsLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    PLERROR( "RBMWoodsLayer::freeEnergyContribution(): not implemeted yet" );
+    PLASSERT( unit_activations.size() == size );
+
+    // result = -\sum_{i=0}^{size-1} softplus(a_i)
+    real result = 0;
+    real* a = unit_activations.data();
+    for (int i=0; i<size; i++)
+    {
+        if (use_fast_approximations)
+            result -= tabulated_softplus(a[i]);
+        else
+            result -= softplus(a[i]);
+    }
+    return result;
+}
+
+int RBMWoodsLayer::getConfigurationCount()
+{
+    PLERROR( "RBMWoodsLayer::getConfigurationCount(): not implemeted yet" );
+    return size < 31 ? 1<<size : INFINITE_CONFIGURATIONS;
+}
+
+void RBMWoodsLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLERROR( "RBMWoodsLayer::getConfigurationCount(): not implemeted yet" );
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
+
+    for ( int i = 0; i < size; ++i ) {
+        output[i] = conf_index & 1;
+        conf_index >>= 1;
+    }
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-02-20 20:09:16 UTC (rev 8548)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-02-20 20:10:33 UTC (rev 8549)
@@ -0,0 +1,203 @@
+// -*- C++ -*-
+
+// RBMWoodsLayer.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMWoodsLayer.h */
+
+
+#ifndef RBMWoodsLayer_INC
+#define RBMWoodsLayer_INC
+
+#include "RBMLayer.h"
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * RBM layer with tree-structured groups of units
+ *
+ */
+class RBMWoodsLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    // Number of trees in the woods
+    int n_trees;
+
+    // Depth of the trees in the woods (1 gives the ordinary RBMBinomialLayer)
+    int tree_depth;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMWoodsLayer( real the_learning_rate=0. );
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample() ;
+
+    //! Inherited.
+    virtual void generateSamples();
+
+    virtual void computeProbabilisticClustering(Vec& prob_clusters);
+
+    //! Compute marginal expectations of all units
+    virtual void computeExpectation() ;
+
+    //! Compute marginal mini-batch expectations of all units.
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec& input, Vec& output ) const;
+
+    //! Batch forward propagation
+    virtual void fprop( const Mat& inputs, Mat& outputs ) const;
+
+    //! forward propagation with provided bias
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
+                        Vec& output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! back-propagates the output gradient to the input and the bias
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
+                             const Vec& output,
+                             Vec& input_gradient, Vec& rbm_bias_gradient,
+                             const Vec& output_gradient) ;
+
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec& target);
+    virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+    virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
+                          Mat& bias_gradients);
+
+    //! compute -bias' unit_values
+    virtual real energy(const Vec& unit_values) const;
+
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMWoodsLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    // Probability that sampling reaches a node but samples h=0 (expectation is for h=1)
+    Vec off_expectation;
+    // Ordinary RBMBinomialLayer expectation
+    Vec local_node_expectation;
+
+    // Computations of the local_node_expectation free energies for h = 1
+    Vec on_free_energy;
+    // Computations of the local_node_expectation free energies for h = 0
+    Vec off_free_energy;
+
+    // Gradient through the local_node_expectations (after sigmoid)
+    Vec local_node_expectation_gradient;
+    // Gradient through the tree structure
+    Vec on_tree_gradient;
+    Vec off_tree_gradient;
+    Vec on_free_energy_gradient;
+    Vec off_free_energy_gradient;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMWoodsLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Wed Feb 20 23:12:10 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 20 Feb 2008 23:12:10 +0100
Subject: [Plearn-commits] r8550 - trunk/plearn_learners/distributions
Message-ID: <200802202212.m1KMCAoi019975@sheep.berlios.de>

Author: tihocan
Date: 2008-02-20 23:12:10 +0100 (Wed, 20 Feb 2008)
New Revision: 8550

Modified:
   trunk/plearn_learners/distributions/RBMDistribution.cc
   trunk/plearn_learners/distributions/RBMDistribution.h
Log:
Added an option to control the number of Gibbs chains being run simultaneously in generateN

Modified: trunk/plearn_learners/distributions/RBMDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/RBMDistribution.cc	2008-02-20 20:10:33 UTC (rev 8549)
+++ trunk/plearn_learners/distributions/RBMDistribution.cc	2008-02-20 22:12:10 UTC (rev 8550)
@@ -51,7 +51,8 @@
 /////////////////////
 // RBMDistribution //
 /////////////////////
-RBMDistribution::RBMDistribution()
+RBMDistribution::RBMDistribution():
+    n_gibbs_chains(-1)
 {}
 
 ////////////////////
@@ -71,6 +72,15 @@
                   OptionBase::buildoption,
         "Underlying RBM modeling the distribution.");
 
+    declareOption(ol, "n_gibbs_chains", &RBMDistribution::n_gibbs_chains,
+                  OptionBase::buildoption,
+        "Number of Gibbs chains ran in parallel when generating multiple\n"
+        "samples with generateN(). If <0, then there are as many chains as\n"
+        "samples. If in the (0,1) interval, then it is the given fraction of\n"
+        "the number of generated samples. If an integer >= 1, it is the\n"
+        "absolute number of chains that are run simultaneously. Each chain\n"
+        "will sample about N/n_chains samples, so as to obtain N samples.");
+
     // Now call the parent class' declareOptions().
     inherited::declareOptions(ol);
 }
@@ -143,11 +153,36 @@
 ///////////////
 void RBMDistribution::generateN(const Mat& Y) const
 {
-    work1.resize(Y.length(), 0);
-    ports_val.fill(NULL);
-    ports_val[rbm->getPortIndex("visible_sample")] = &work1;
-    rbm->fprop(ports_val);
-    Y << work1;
+    int n = Y.length(); // Number of samples to obtain.
+    int n_chains = Y.length();
+    if (n_gibbs_chains > 0 && n_gibbs_chains < 1) {
+        // Fraction.
+        n_chains = min(1, int(round(n_gibbs_chains * n)));
+    } else if (n_gibbs_chains > 0) {
+        n_chains = int(round(n_gibbs_chains));
+        PLCHECK( is_equal(real(n_chains), n_gibbs_chains) );
+    }
+    int n_gibbs_samples = n / n_chains;
+    if (n % n_chains > 0)
+        n_gibbs_samples += 1;
+    work2.resize(n_chains * n_gibbs_samples, Y.width());
+    PP<ProgressBar> pb = verbosity && n_gibbs_samples > 10
+        ? new ProgressBar("Gibbs sampling", n_gibbs_samples)
+        : NULL;
+    for (int i = 0; i < n_gibbs_samples; i++) {
+        work1.resize(n_chains, 0);
+        ports_val.fill(NULL);
+        ports_val[rbm->getPortIndex("visible_sample")] = &work1;
+        rbm->fprop(ports_val);
+        work2.subMatRows(i * n_chains, n_chains) << work1;
+        if (pb)
+            pb->updateone();
+    }
+    if (n_gibbs_samples > 1)
+        // We shuffle rows to add more "randomness" since consecutive samples
+        // in the same Gibbs chain may be similar.
+        random_gen->shuffleRows(work2);
+    Y << work2.subMatRows(0, Y.length());
 }
 
 /////////////////

Modified: trunk/plearn_learners/distributions/RBMDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/RBMDistribution.h	2008-02-20 20:10:33 UTC (rev 8549)
+++ trunk/plearn_learners/distributions/RBMDistribution.h	2008-02-20 22:12:10 UTC (rev 8550)
@@ -65,6 +65,8 @@
 
     PP<RBMModule> rbm;
 
+    real n_gibbs_chains;
+
 public:
     //#####  Public Member Functions  #########################################
 



From lamblin at mail.berlios.de  Thu Feb 21 02:41:26 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 21 Feb 2008 02:41:26 +0100
Subject: [Plearn-commits] r8552 - in trunk: commands plearn/vmat
Message-ID: <200802210141.m1L1fQHK002843@sheep.berlios.de>

Author: lamblin
Date: 2008-02-21 02:41:23 +0100 (Thu, 21 Feb 2008)
New Revision: 8552

Added:
   trunk/plearn/vmat/MixtureVMatrix.cc
   trunk/plearn/vmat/MixtureVMatrix.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
New VMat, used for mixing sources with different proportions.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-02-21 01:37:30 UTC (rev 8551)
+++ trunk/commands/plearn_noblas_inc.h	2008-02-21 01:41:23 UTC (rev 8552)
@@ -319,6 +319,7 @@
 #include <plearn/vmat/MeanImputationVMatrix.h>
 #include <plearn/vmat/MemoryVMatrixNoSave.h>
 #include <plearn/vmat/MissingInstructionVMatrix.h>
+#include <plearn/vmat/MixtureVMatrix.h>
 //#include <plearn/vmat/MixUnlabeledNeighbourVMatrix.h>
 #include <plearn/vmat/MultiInstanceVMatrix.h>
 #include <plearn/vmat/MultiTargetOneHotVMatrix.h>

Added: trunk/plearn/vmat/MixtureVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MixtureVMatrix.cc	2008-02-21 01:37:30 UTC (rev 8551)
+++ trunk/plearn/vmat/MixtureVMatrix.cc	2008-02-21 01:41:23 UTC (rev 8552)
@@ -0,0 +1,217 @@
+// -*- C++ -*-
+
+// MixtureVMatrix.cc
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file MixtureVMatrix.cc */
+
+
+#include "MixtureVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    MixtureVMatrix,
+    "Mixes several underlying source VMat, with ponderation.",
+    ""
+    );
+
+MixtureVMatrix::MixtureVMatrix():
+    n_sources(0),
+    period_length(0)
+{
+}
+
+void MixtureVMatrix::getNewRow(int i, const Vec& v) const
+{
+    // The source it comes from
+    int source = period[i % period_length];
+    // The number of previous samples of the same source in the same period
+    int occurrence = occurrences[i % period_length];
+    // The index in this source
+    int index = (i/period_length)*weights[source] + occurrence;
+
+    sources[source]->getRow(index % sources[source].length(), v);
+}
+
+void MixtureVMatrix::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "myoption", &MixtureVMatrix::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+
+    declareOption(ol, "sources", &MixtureVMatrix::sources,
+                  OptionBase::buildoption,
+                  "The VMat to mix");
+
+    declareOption(ol, "weights", &MixtureVMatrix::weights,
+                  OptionBase::buildoption,
+                  "Weights of the different sources.\n"
+                  "If weights[0]==2 and weights[1]==2, then there will be\n"
+                  "twice as many exambles coming from sources[0] than from\n"
+                  "sources[1], regardless of the sources' length."
+                  );
+
+    declareOption(ol, "n_sources", &MixtureVMatrix::n_sources,
+                  OptionBase::learntoption,
+                  "Number of sources");
+
+    declareOption(ol, "period_length", &MixtureVMatrix::period_length,
+                  OptionBase::learntoption,
+                  "sum(weights)");
+
+    declareOption(ol, "period", &MixtureVMatrix::period,
+                  OptionBase::learntoption,
+                  "Sequence of sources to select, ensuring the proportion of\n"
+                  "sources and their homogeneity ."
+                  );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void MixtureVMatrix::build_()
+{
+    n_sources = sources.size();
+    if (n_sources == 0)
+        return;
+
+    PLCHECK_MSG(sources.size() == weights.size(),
+                "You should provide as many weights as sources");
+
+    PLCHECK_MSG(length_ >= 0,
+                "You should provide a length greater than 0");
+
+    width_ = sources[0]->width();
+    for (int i=1; i<n_sources; i++)
+        PLCHECK_MSG(sources[i]->width() == width_,
+                    "All sources should have the same width");
+
+    if (inputsize_<0 || targetsize_<0 || weightsize_<0 || extrasize_<0
+        || inputsize_ + targetsize_ + weightsize_ + extrasize_ != width_)
+    {
+        inputsize_ = sources[0]->inputsize();
+        targetsize_ = sources[0]->targetsize();
+        weightsize_ = sources[0]->weightsize();
+        extrasize_ = sources[0]->extrasize();
+    }
+
+    period_length = sum(weights);
+    period.resize(period_length);
+    occurrences.resize(period_length);
+
+    bool incorrect_period = false;
+    for (int i=0; i<n_sources; i++)
+        if (period.count(i) != weights[i])
+        {
+            incorrect_period = true;
+            break;
+        }
+
+    if (incorrect_period)
+        buildPeriod();
+}
+
+void MixtureVMatrix::buildPeriod()
+{
+    TVec<int> ideal_count(n_sources);
+    TVec<int> actual_count(n_sources);
+    TVec<int> sources_count(n_sources);
+
+    for (int i=0; i<period_length; i++)
+    {
+        // Find the source that is the most underrepresented
+        ideal_count += weights;
+
+        int max = 0;
+        int argmax = -1;
+        for (int j=0; j<n_sources; j++)
+        {
+            if (ideal_count[j] - actual_count[j] > max)
+            {
+                argmax = j;
+                max = ideal_count[j] - actual_count[j];
+            }
+        }
+        PLASSERT(argmax >= 0);
+
+        period[i] = argmax;
+        actual_count[argmax] += period_length;
+        occurrences[i] = sources_count[argmax];
+        sources_count[argmax]++;
+    }
+
+#ifdef BOUNDCHECK
+    for (int i=0; i<n_sources; i++)
+    {
+        PLASSERT(period.count(i) == weights[i]);
+        PLASSERT( i==0 && occurrences[0]==0
+                  || period.subVec(0,i-1).count(period[i]) == occurrences[i]);
+    }
+#endif
+}
+
+// ### Nothing to add here, simply calls build_
+void MixtureVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+void MixtureVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(sources, copies);
+    deepCopyField(weights, copies);
+    deepCopyField(period, copies);
+    deepCopyField(occurrences, copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/MixtureVMatrix.h
===================================================================
--- trunk/plearn/vmat/MixtureVMatrix.h	2008-02-21 01:37:30 UTC (rev 8551)
+++ trunk/plearn/vmat/MixtureVMatrix.h	2008-02-21 01:41:23 UTC (rev 8552)
@@ -0,0 +1,146 @@
+// -*- C++ -*-
+
+// MixtureVMatrix.h
+//
+// Copyright (C) 2008 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file MixtureVMatrix.h */
+
+
+#ifndef MixtureVMatrix_INC
+#define MixtureVMatrix_INC
+
+#include <plearn/vmat/RowBufferedVMatrix.h>
+#include <plearn/vmat/VMat.h>
+
+namespace PLearn {
+
+/**
+ * Mixes several underlying source VMat, with ponderation.
+ */
+class MixtureVMatrix : public RowBufferedVMatrix
+{
+    typedef RowBufferedVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The sources to mix (with repetitions if needed)
+    TVec<VMat> sources;
+
+    //! Weights of the different sources. If weights[0]==2 and weights[1]==2,
+    //! then there will be twice as many exambles coming from sources[0] than
+    //! from sources[1], regardless of the sources' length.
+    TVec<int> weights;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    MixtureVMatrix();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(MixtureVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    //! sources.size()
+    int n_sources;
+
+    //! sum(weights)
+    int period_length;
+
+    //! Sequence of sources to select, ensuring the proportion of sources and
+    //! their homogeneity
+    TVec<int> period;
+
+    //! occurrences[i] is the count of element period[i] in period[0..i-1]
+    TVec<int> occurrences;
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! 'v' is assumed to be the right size.
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    //! build period and occurrences
+    void buildPeriod();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(MixtureVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Thu Feb 21 02:37:31 2008
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 21 Feb 2008 02:37:31 +0100
Subject: [Plearn-commits] r8551 - trunk/plearn/math
Message-ID: <200802210137.m1L1bVf3002539@sheep.berlios.de>

Author: lamblin
Date: 2008-02-21 02:37:30 +0100 (Thu, 21 Feb 2008)
New Revision: 8551

Modified:
   trunk/plearn/math/TVec_decl.h
Log:
Useful function


Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2008-02-20 22:12:10 UTC (rev 8550)
+++ trunk/plearn/math/TVec_decl.h	2008-02-21 01:37:30 UTC (rev 8551)
@@ -746,7 +746,7 @@
         }
         return indices;
     }
- 
+
     TVec<int> findIndices(const TVec<T>& elements)
     {
         TVec<int> indices(0);
@@ -763,7 +763,7 @@
         }
         return indices;
     }
- 
+
     //!  Returns the position of the first occurence of element
     //!  in the vector or -1 if it never occurs
     int find(const T& element, int start=0) const
@@ -791,6 +791,38 @@
         return indices;
     }
 
+    //! Returns the number of occurrences of "element"
+    int count(const T& element)
+    {
+        int result = 0;
+        if (!isEmpty())
+        {
+            T *v = data();
+            for (int i=0; i<length(); i++)
+                if (v[i]==element)
+                    result++;
+        }
+        return result;
+    }
+
+    int count(const TVec<T>& elements)
+    {
+        int result = 0;
+        if (!isEmpty())
+        {
+            T *v = data();
+            for (int i=0; i<length(); i++)
+                for (int j=0, m=elements.length(); j<m; j++)
+                    if (v[i]==elements[j])
+                    {
+                        result++;
+                        break;
+                    }
+        }
+        return result;
+    }
+
+
     //!  C++ stream output
     void print(ostream& out = cout) const; //!<  the data is printed on a single row, no newline
     void println(ostream& out = cout) const { print(out); out<<endl; } //!<  same with newline



From nouiz at mail.berlios.de  Thu Feb 21 17:08:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 21 Feb 2008 17:08:14 +0100
Subject: [Plearn-commits] r8553 - trunk/plearn/vmat
Message-ID: <200802211608.m1LG8EDV017370@sheep.berlios.de>

Author: nouiz
Date: 2008-02-21 17:08:13 +0100 (Thu, 21 Feb 2008)
New Revision: 8553

Modified:
   trunk/plearn/vmat/FileVMatrix.cc
Log:
code reuse


Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-02-21 01:41:23 UTC (rev 8552)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-02-21 16:08:13 UTC (rev 8553)
@@ -439,23 +439,17 @@
 ///////////////
 void FileVMatrix::appendRow(Vec v)
 {
-#ifdef USE_NSPR_FILE
     moveto(length_,0);
+#ifdef USE_NSPR_FILE
     if(file_is_float)
         PR_Write_float(f, v.data(), v.length(), file_is_bigendian);
     else
         PR_Write_double(f, v.data(), v.length(), file_is_bigendian);
 #else
     if(file_is_float)
-    {
-        fseek(f,DATAFILE_HEADERLENGTH+length_*width_*sizeof(float), SEEK_SET);
         fwrite_float(f, v.data(), v.length(), file_is_bigendian);
-    }
     else
-    {
-        fseek(f,DATAFILE_HEADERLENGTH+length_*width_*sizeof(double), SEEK_SET);
         fwrite_double(f, v.data(), v.length(), file_is_bigendian);
-    }
 #endif
 
     length_++;



From tihocan at mail.berlios.de  Thu Feb 21 17:34:43 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 21 Feb 2008 17:34:43 +0100
Subject: [Plearn-commits] r8554 - trunk/plearn/vmat
Message-ID: <200802211634.m1LGYhbc020642@sheep.berlios.de>

Author: tihocan
Date: 2008-02-21 17:34:43 +0100 (Thu, 21 Feb 2008)
New Revision: 8554

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
Log:
More explicit error message

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-02-21 16:08:13 UTC (rev 8553)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-02-21 16:34:43 UTC (rev 8554)
@@ -100,7 +100,8 @@
         return;
     PLCHECK_MSG(source->targetsize() == 1,
                 "In ReplicateSamplesVMatrix::build_ - The source VMat must "
-                "have a targetsize equal to 1");
+                "have a targetsize equal to 1, but its targetsize is " +
+                tostring(source->targetsize()));
     
     // Build the vector of indices.
     indices.resize(0);



From tihocan at mail.berlios.de  Thu Feb 21 18:18:17 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 21 Feb 2008 18:18:17 +0100
Subject: [Plearn-commits] r8555 - trunk/plearn/vmat
Message-ID: <200802211718.m1LHIHt5027600@sheep.berlios.de>

Author: tihocan
Date: 2008-02-21 18:18:16 +0100 (Thu, 21 Feb 2008)
New Revision: 8555

Modified:
   trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
   trunk/plearn/vmat/ReplicateSamplesVMatrix.h
Log:
Can now operate on bags

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-02-21 16:34:43 UTC (rev 8554)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.cc	2008-02-21 17:18:16 UTC (rev 8555)
@@ -38,6 +38,7 @@
 
 
 #include "ReplicateSamplesVMatrix.h"
+#include <plearn/var/SumOverBagsVariable.h>
 
 namespace PLearn {
 using namespace std;
@@ -56,6 +57,7 @@
 // ReplicateSamplesVMatrix //
 /////////////////////////////
 ReplicateSamplesVMatrix::ReplicateSamplesVMatrix():
+    operate_on_bags(false),
     seed(1827),
     random_gen(new PRandom())
 {}
@@ -73,6 +75,13 @@
     // ### You can also combine flags, for example with OptionBase::nosave:
     // ### (OptionBase::buildoption | OptionBase::nosave)
 
+    declareOption(ol, "operate_on_bags",
+                  &ReplicateSamplesVMatrix::operate_on_bags,
+                  OptionBase::buildoption,
+        "If set to 1, then bags in the source VMat will be taken into\n"
+        "account so as to preserve their integrity. The classes will also be\n"
+        "reweighted so that they have the same number of bags.");
+
     declareOption(ol, "seed", &ReplicateSamplesVMatrix::seed,
                   OptionBase::buildoption,
         "Seed for the random number generator (to shuffle data).");
@@ -98,16 +107,23 @@
 {
     if (!source)
         return;
-    PLCHECK_MSG(source->targetsize() == 1,
-                "In ReplicateSamplesVMatrix::build_ - The source VMat must "
-                "have a targetsize equal to 1, but its targetsize is " +
-                tostring(source->targetsize()));
+    PLCHECK_MSG(operate_on_bags || source->targetsize() == 1,
+            "In ReplicateSamplesVMatrix::build_ - The source VMat must have a "
+            "targetsize equal to 1 when not operating on bags, but its "
+            "targetsize is " + tostring(source->targetsize()));
+
+    PLCHECK_MSG(!operate_on_bags || source->targetsize() == 2,
+            "In ReplicateSamplesVMatrix::build_ - The source VMat must have a "
+            "targetsize equal to 2 when operating on bags, but its targetsize "
+            "is " + tostring(source->targetsize()));
     
     // Build the vector of indices.
     indices.resize(0);
     Vec input, target;
     real weight;
     TVec< TVec<int>  > class_indices;  // Indices of samples in each class.
+    map<int, int> bag_sizes; // Map a source index to the size of its bag.
+    int bag_start_idx = -1;
     for (int i = 0; i < source->length(); i++) {
         source->getExample(i, input, target, weight);
         int c = int(round(target[0]));
@@ -116,8 +132,15 @@
             for (int j = 0; j < n_to_add; j++)
                 class_indices.append(TVec<int>());
         }
-        class_indices[c].append(i);
-        indices.append(i);
+        if (!operate_on_bags || int(round(target[1])) &
+                                SumOverBagsVariable::TARGET_COLUMN_FIRST) {
+            class_indices[c].append(i);
+            indices.append(i);
+            bag_sizes[i] = 0;
+            bag_start_idx = i;
+        }
+        if (operate_on_bags)
+            bag_sizes[bag_start_idx]++;
     }
     int max_n = -1;
     for (int c = 0; c < class_indices.length(); c++)
@@ -133,6 +156,18 @@
     // Shuffle data.
     random_gen->manual_seed(seed);
     random_gen->shuffleElements(indices);
+
+    if (operate_on_bags) {
+        // We now need to convert the list of start indices to the list of all
+        // indices within each bag.
+        TVec<int> start_idx = indices.copy();
+        indices.resize(0);
+        for (int i = 0; i < start_idx.length(); i++) {
+            int start_i = start_idx[i];
+            for (int j = 0; j < bag_sizes[start_i]; j++)
+                indices.append(start_i + j);
+        }
+    }
     
     // Re-build since indices have changed.
     inherited::build();

Modified: trunk/plearn/vmat/ReplicateSamplesVMatrix.h
===================================================================
--- trunk/plearn/vmat/ReplicateSamplesVMatrix.h	2008-02-21 16:34:43 UTC (rev 8554)
+++ trunk/plearn/vmat/ReplicateSamplesVMatrix.h	2008-02-21 17:18:16 UTC (rev 8555)
@@ -62,6 +62,7 @@
 public:
     //#####  Public Build Options  ############################################
 
+    bool operate_on_bags;
     int32_t seed;
 
 public:



From tihocan at mail.berlios.de  Thu Feb 21 22:16:56 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 21 Feb 2008 22:16:56 +0100
Subject: [Plearn-commits] r8556 - trunk/python_modules/plearn/pytest
Message-ID: <200802212116.m1LLGu9i006235@sheep.berlios.de>

Author: tihocan
Date: 2008-02-21 22:16:56 +0100 (Thu, 21 Feb 2008)
New Revision: 8556

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:
Automatically add PSAVEDIFF to the ignore list when confirming a new test

Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2008-02-21 17:18:16 UTC (rev 8555)
+++ trunk/python_modules/plearn/pytest/modes.py	2008-02-21 21:16:56 UTC (rev 8556)
@@ -431,7 +431,7 @@
                     else:
                         version_control.add( test.resultsDirectory() )
                         version_control.recursive_add( test.resultsDirectory(tests.EXPECTED_RESULTS) )
-                        version_control.ignore( test.resultsDirectory(), [ '.plearn', tests.RUN_RESULTS ] )
+                        version_control.ignore( test.resultsDirectory(), [ '.plearn', tests.RUN_RESULTS, 'PSAVEDIFF' ] )
 
             except version_control.VersionControlError:
                 raise core.PyTestUsageError(



From tihocan at mail.berlios.de  Thu Feb 21 22:17:30 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 21 Feb 2008 22:17:30 +0100
Subject: [Plearn-commits] r8557 - trunk/examples/data/test_suite
Message-ID: <200802212117.m1LLHUoo006300@sheep.berlios.de>

Author: tihocan
Date: 2008-02-21 22:17:30 +0100 (Thu, 21 Feb 2008)
New Revision: 8557

Added:
   trunk/examples/data/test_suite/linear_4x_2y_multi_class.vmat
Log:
New multiclass classification dataset that can be used in the test suite

Added: trunk/examples/data/test_suite/linear_4x_2y_multi_class.vmat
===================================================================
--- trunk/examples/data/test_suite/linear_4x_2y_multi_class.vmat	2008-02-21 21:16:56 UTC (rev 8556)
+++ trunk/examples/data/test_suite/linear_4x_2y_multi_class.vmat	2008-02-21 21:17:30 UTC (rev 8557)
@@ -0,0 +1,23 @@
+# Dummy multi-class classification problem derived from the 'linear_4x_2y' dataset.
+# The target is one of:
+#   0 if y2 < -50
+#   1 if -50 <= y2 < 0
+#   2 if 0 <= y2 < 50
+#   3 if y2 >= 50
+# The first five columns (x1 -> x4 and y1) are the inputs.
+# These five inputs are processed through a sigmoid to ensure they are
+# between 0 and 1.
+
+MemoryVMatrix(
+    source =
+        ProcessingVMatrix(
+            source =
+                AutoVMatrix(
+                    filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.amat"
+                )
+            prg = "[@x1:@y1:sigmoid] @y2 0 < @y2 -50 < 0 1 ifelse @y2 50 < 2 3 ifelse ifelse :target"
+            inputsize = 5
+            targetsize = 1
+            weightsize = 0
+        )
+)



From tihocan at mail.berlios.de  Thu Feb 21 22:17:55 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 21 Feb 2008 22:17:55 +0100
Subject: [Plearn-commits] r8558 - in trunk/plearn_learners/generic/test: .
	NNet NNet/.pytest NNet/.pytest/PL_NNet
	NNet/.pytest/PL_NNet/expected_results
	NNet/.pytest/PL_NNet/expected_results/expdir-nnet
	NNet/.pytest/PL_NNet/expected_results/expdir-nnet/Split0
	NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata
	NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata
Message-ID: <200802212117.m1LLHtFO006360@sheep.berlios.de>

Author: tihocan
Date: 2008-02-21 22:17:55 +0100 (Thu, 21 Feb 2008)
New Revision: 8558

Added:
   trunk/plearn_learners/generic/test/NNet/
   trunk/plearn_learners/generic/test/NNet/.pytest/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/RUN.log
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/Split0/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/Split0/final_learner.psave
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/generic/test/NNet/nnet.pyplearn
   trunk/plearn_learners/generic/test/NNet/pytest.config
Log:
It would be time to have a test for NNet


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest
___________________________________________________________________
Name: svn:ignore
   + *.compilation_log



Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/RUN.log
===================================================================

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/Split0/final_learner.psave	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/Split0/final_learner.psave	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1,62 @@
+*1 ->NNet(
+nhidden = 10 ;
+nhidden2 = 20 ;
+noutputs = 4 ;
+weight_decay = 0.0100000000000000002 ;
+bias_decay = 0 ;
+layer1_weight_decay = 0 ;
+layer1_bias_decay = 0 ;
+layer2_weight_decay = 0 ;
+layer2_bias_decay = 0 ;
+output_layer_weight_decay = 0 ;
+output_layer_bias_decay = 0 ;
+direct_in_to_out_weight_decay = 0 ;
+penalty_type = "L2_square" ;
+L1_penalty = 0 ;
+fixed_output_weights = 0 ;
+input_reconstruction_penalty = 0 ;
+direct_in_to_out = 1 ;
+rbf_layer_size = 0 ;
+first_class_is_junk = 1 ;
+output_transfer_func = "softmax" ;
+hidden_transfer_func = "tanh" ;
+cost_funcs = 1 [ "NLL" ] ;
+classification_regularizer = 0 ;
+first_hidden_layer = *0 ;
+first_hidden_layer_is_output = 0 ;
+n_non_params_in_first_hidden_layer = 0 ;
+transpose_first_hidden_layer = 0 ;
+margin = 1 ;
+do_not_change_params = 0 ;
+optimizer = *2 ->GradientOptimizer(
+start_learning_rate = 0.0100000000000000002 ;
+learning_rate = 0.000625039064941558834 ;
+decrease_constant = 0.00100000000000000002 ;
+lr_schedule = 0  0  [ 
+]
+;
+use_stochastic_hack = 0 ;
+verbosity = 0 ;
+nstages = 150  )
+;
+batch_size = 1 ;
+initialization_method = "uniform_linear" ;
+paramsvalues = 384 [ 0.0153604489130646105 -0.00650847977560525467 -0.014519138828815191 -0.0166967717386671682 0.0204639771006939475 0.0178865056111326033 0.0482476262635132477 0.0104418541071218708 0.0326103867362739219 0.026304870484164234 0.0973261733951203828 0.10802848586698767 -0.0274307616052095471 0.083663979361160476 0.0740142528610545636 -0.0411403186468525459 -0.0833769352127151503 0.0535296195007013975 0.00992948581983363679 -0.0972897708885692991 0.021984788633963491 0.0190139780939338233 0.0476187533971595064 -0.101109717614203273 -0.0913198362373354305 -0.0159905626483526145 0.0465384142386058713 0.0852707994482205522 -0.0388084245671419109 -0.100159700478036434 0.00892746094597828346 0.0331487078825705578 -0.0708607598257290777 0.0478983108090662371 -0.0426523703788639907 0.0153184136237460248 -0.128442367273464253 0.0214726348380649759 0.0309519528115176981 -0.120998805311630514 0.00785655969843160502 0.0448534312606049743 0.0726988013926748583 -0.10364201!
 8817208983 0.00356624601948190862 -0.091084770585205671 -0.0246145235003415661 -0.0682506219630721611 -0.0757629202862246942 0.0972451025432798655 -0.124381134089005918 -0.0954871509368727528 0.116799970715033952 0.0642292276258020767 -0.131690008826822663 -0.0593335844716629743 -0.122040898618734697 0.0848146771736280058 -0.13376286584977537 -0.0504468720157150233 0.0661001919373456343 -0.00609728684157972035 -0.145269827848282185 0.00621171996350913629 0.0341533515601188178 0.0730538654257211068 -0.0955168525689681081 -0.0537865364880622462 0.0340906971664668504 -0.0794412298413800649 0.0388757665634782423 0.0506678306721699834 -0.0297961593816686819 -0.0266637015278070773 0.0857694155342698383 0.0624125219149128385 0.0698547320628382651 -0.0161009373793744789 0.0417297178519203393 0.0416145180506312592 -0.03242241458058822 0.0361295839355311524 -0.0124195347106517626 -0.0451992801903453173 0.0595067710537599617 0.0323331621733981833 -0.0464700060609991999 -0.050271805151!
 047351 0.0209593241338327518 -0.0135892575181334879 0.02313452!
 38375060
72 0.0466938558244173671 -0.0193613912076341516 0.0282213806441183879 0.025847977369007423 0.0333243250449671846 -0.0217895152285282542 0.0416879283821098567 -0.0204947985915433159 -0.0111805777837461848 -0.0332498721640030886 -0.0155152093414014994 0.0487476640444102702 0.0415960685471562439 0.0170662948505605483 0.0431680053599029639 0.0109069887576893603 -0.0509965055137841378 0.0486494716033246677 0.0373718048120225063 -0.0289186362821605356 0.0493272212937663351 0.0259800473790725607 0.034610340996432877 -0.0212210272051457198 0.0365160229624920327 -0.0373962736597032927 0.0447666351819308028 0.0490909993363092823 0.0567217678883305126 -0.046479275206843855 0.0149394039968663277 0.0658894018553795036 0.0390592888642987829 0.0141387534142476384 -0.0176300139857917086 -0.00480365766651212785 -0.0198503593010424841 -0.0168118041300782846 -0.0142912902552615548 -0.0158633634156708607 0.0370523727180357212 0.0133816985410629662 -0.0559120154402104677 0.0267795360045070541 -0!
 .028941075205845098 -0.0462100779362940028 0.0545919536646444492 -0.0598698313612570712 0.014164217848514786 -0.0420407143123923271 -0.0262067203054225234 0.0326642763068345846 -0.00499863550692940944 0.0434023342113425276 -0.0284197115276186608 0.029405741434624922 -0.042618191964541266 -0.00156717386326725395 0.03537416172986875 -0.0301944775685968822 -0.0197616468433883113 0.0192704911833745315 0.0297124132805940896 -0.0180282349926145849 0.00494636703387872634 0.021013973833511855 -0.00171055337667006414 0.039852832919314865 -0.0044903985958031226 0.0396143663876108373 0.0448436553063974799 -0.0663745791554343906 0.0214814390924981807 0.0510897688115148282 -0.00329725348303335679 0.0371712505091681966 -0.0666792272060133251 0.0178363325059021464 -0.00955530942441461945 -0.00327040830985138092 0.0683068869578800086 -0.0649554617181689609 0.031670556555222662 0.0620091099006936305 -0.0449889647095759529 0.0350337236264316523 0.0572361731136034543 0.0468645763479468708 -0.!
 0324158103035152925 0.0332817481143390104 -0.02433646686034219!
 44 -0.00
779547956407612361 -0.00567498937625502201 0.0239489691841704398 -0.00905683798658328022 -0.0603492537558075262 -0.0210582211336544922 0.0531040327174752969 0.0240734247079828594 0.0381847250735914689 -0.0233996519871360158 0.0363200182238816102 -0.0273819917940599962 0.0631344003804491655 -0.00830576412901244329 0.0356946785743484998 -0.0412368305976175339 -0.0370766548927538506 -0.00111850240324453168 0.0548123926356271565 0.00705741926891998751 -0.0926797490206229951 0.0120221407147714319 0.0526257981830655364 0.0735896975226796174 0.00573719398386036365 0.0315988529247082581 0.0564528503176723248 -0.0327093247796945708 0.0130741825808066373 -0.0130309602855029072 -0.0281918188659743482 -0.0215999208481949222 0.0751028480119925179 0.043910198901858627 0.0330233057508047198 -0.00452706606443805532 0.0732816962486857043 0.0475923209693682112 -0.0230879262737447369 -0.0327240463749929392 -0.01437268331457312 0.0141138161707113109 0.0193189933208380675 0.00645572856728411464 !
 0.0421243078387911335 -0.0293062828024117164 -0.0471001010688314736 0.0312424282737094709 0.0420543507016808635 0.0345846457719920924 0.032217153738724226 0.0297759701879173576 0.0358875198523024783 0.0429438246348492828 -0.00478926841661099847 -0.0316706492846649049 0.0216667084962309728 -0.00750200472867330447 0.0683023589934727143 -0.0362052846523643404 -0.0568202829240478449 -0.0384218578927435495 0.0346285726050679851 0.0347386121846796 -0.0676403466505690609 -0.0353039925291051479 0.0370505926986620937 0.0387677066571026144 0.0137563424338028716 0.06533566937164835 -0.0357297918707107295 0.0309568298773064098 0.0598924244157644642 -0.0219084590291019725 -0.0294506026463165933 -0.0308655178357122237 -0.000898473223240684837 0.0225594845465858784 0.0117851176061533492 -0.0481894806748069532 -0.00792682562017802544 0.0279598729532173317 0.0109002831481884097 -0.0228666705170885061 -0.0654574419996027251 0.0490000865961048374 -0.00283446428013791598 -0.0497010833679140049!
  0.0393536197217953868 0.0606678301808660364 -0.01665298839881!
 69175 0.
0145250345676479278 0.0609501681253591943 0.0610452331595505091 -0.00791998559328273982 0.0378650699861208806 0.0580199522938379675 0.0371828042987912555 0.285461688794165835 0.852338111128345899 -0.0857372186388937751 -1.05206258128361263 0.0742975400026906346 0.0485009690693549295 -0.00572140700022712948 -0.042962474857500578 0.0111928518118286375 -0.0208054931520309377 0.00342249750277377404 -0.00816707105899603693 -0.0873391489959279427 -0.0853968117054837855 0.0340678710527509698 0.110632994527130477 -0.0119882379782621229 -0.00668929230362060988 -0.0050026907078874203 -0.0148254458666847062 0.0294200278428331241 0.0385963867839825628 -0.0202670632021797505 -0.026695577528572878 0.0413515355004380053 0.041001637255915914 -0.0191942034988579133 -0.0574204423163219707 -0.0627189136644920459 -0.0424338356005514866 -0.0044663530733389965 0.0751056165680022009 -0.015880680243673638 -0.0341292679221286976 -0.00161445585182248765 0.0363973354699752838 0.0459577272488584446 0.0!
 320213425603930249 0.00038330732861000281 -0.0284038298043329807 -0.0212890872225538312 -0.0621802781143980729 -0.00340061145728448285 0.0468764732419207839 0.00113301799015747364 0.0233509260450682626 -0.0101563813775918058 -0.0359485307695455586 0.0308234363430395404 0.0179686423882101411 -0.0305692934019089133 -0.053407782485077708 -0.0149427368282414406 -0.0201773798225915527 0.043697213821257623 0.0327628837511904497 0.0230876533186801236 -0.0361192860791738068 -0.0162496315864929762 0.00385448908661355748 0.0576536433136958446 0.0605875519097519993 -0.022948931395600794 -0.0723850512397770096 -0.0066015892365497604 0.0424620005387502733 -0.0312988666543407568 -0.0468269779565594138 0.040634288686666728 0.0193992596670531335 -0.0253673356776732452 -0.0601462063349458287 0.0118228262025526758 -0.000181194943870755887 -0.0209629235388078712 0.0137705816101042905 0.0385879141599069844 0.0258402208935279813 -0.0440446763259086799 -0.0388428282577284387 0.009283284644072199!
 37 0.0287416126755615127 -0.0102889907783085596 -0.03309674761!
 83897588
 -0.0968514036732316119 0.174033111275561092 0.108117833073812036 0.012203178965919742 -0.273359001955773728 0.0797753706734724705 -0.0680019068076809369 0.304928557561435276 -0.647879503330761497 -0.958293261636531057 0.676639224342663992 0.88293670403160307 -0.550582823839710134 -0.142968389978786997 0.0249467040087432909 0.560905209785120928 -0.715374871566240467 -0.459962856042415769 0.432692774650989043 0.698248860461753384 ] ;
+random_gen = *3 ->PRandom(
+seed = 1827 ;
+fixed_seed = 0  )
+;
+seed = 1827 ;
+stage = 100 ;
+n_examples = 150 ;
+inputsize = 5 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 100 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1 @@
+E[test1.E[NLL]]	0

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1,2 @@
+splitnum	0
+test1.E[NLL]	0

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/generic/test/NNet/nnet.pyplearn
===================================================================
--- trunk/plearn_learners/generic/test/NNet/nnet.pyplearn	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/nnet.pyplearn	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1,35 @@
+# Simple basic NNet experiment.
+
+from plearn.pyplearn import pl
+
+nnet = pl.NNet(
+        cost_funcs = [ 'NLL' ],
+        direct_in_to_out = True,
+        nhidden = 10,
+        nhidden2 = 20,
+        noutputs = 4,
+        nstages = 100,
+        optimizer = pl.GradientOptimizer(
+            start_learning_rate = 1e-2,
+            decrease_constant = 1e-3,
+            ),
+        output_transfer_func = 'softmax',
+        weight_decay = 1e-2,
+        )
+
+tester = pl.PTester(
+        expdir = 'expdir-nnet',
+        learner = nnet,
+        dataset = pl.AutoVMatrix( filename = "PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat" ),
+        statnames = [ 'E[test1.E[NLL]]' ],
+        splitter = pl.FractionSplitter(
+            splits = TMat(1, 2, [ (0, 0.75), (0.75, 1) ]),
+            ),
+        save_initial_tester = False,
+        save_stat_collectors = False,
+        save_test_names = False,
+        )
+
+def main():
+    return tester
+

Added: trunk/plearn_learners/generic/test/NNet/pytest.config
===================================================================
--- trunk/plearn_learners/generic/test/NNet/pytest.config	2008-02-21 21:17:30 UTC (rev 8557)
+++ trunk/plearn_learners/generic/test/NNet/pytest.config	2008-02-21 21:17:55 UTC (rev 8558)
@@ -0,0 +1,91 @@
+"""Pytest config file.
+
+Test is a class regrouping the elements that define a test for PyTest.
+    
+    For each Test instance you declare in a config file, a test will be ran
+    by PyTest.
+    
+    @ivar name: The name of the Test must uniquely determine the
+    test. Among others, it will be used to identify the test's results
+    (.PyTest/I{name}/*_results/) and to report test informations.
+    @type name: String
+    
+    @ivar description: The description must provide other users an
+    insight of what exactly is the Test testing. You are encouraged
+    to used triple quoted strings for indented multi-lines
+    descriptions.
+    @type description: String
+    
+    @ivar category: The category to which this test belongs. By default, a
+    test is considered a 'General' test.
+    
+    It is not desirable to let an extensive and lengthy test as 'General',
+    while one shall refrain abusive use of categories since it is likely
+    that only 'General' tests will be ran before most commits...
+    
+    @type category: string
+    
+    @ivar program: The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner::
+    
+    1) Look for a local program named PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named PRGNAME
+    3) Call 'which PRGNAME'
+    4) Fail
+    
+    Compilable program should provide the keyword argument 'compiler'
+    mapping to a string interpreted as the compiler name (e.g.
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument.  @type program:
+    L{Program}
+    
+    @ivar arguments: The command line arguments to be passed to the program
+    for the test to proceed.
+    @type arguments: String
+    
+    @ivar resources: A list of resources that are used by your program
+    either in the command line or directly in the code (plearn or pyplearn
+    files, databases, ...).  The elements of the list must be string
+    representations of the path, absolute or relative, to the resource.
+    @type resources: List of Strings
+    
+    @ivar precision: The precision (absolute and relative) used when comparing
+    floating numbers in the test output (default = 1e-6)
+    @type precision: float
+    
+    @ivar pfileprg: The program to be used for comparing files of psave &
+    vmat formats. It can be either::
+    - "__program__": maps to this test's program if its compilable;
+    maps to 'plearn_tests' otherwise (default);
+    - "__plearn__": always maps to 'plearn_tests' (for when the program
+    under test is not a version of PLearn);
+    - A Program (see 'program' option) instance
+    - None: if you are sure no files are to be compared.
+    
+    @ivar ignored_files_re: Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+    @type ignored_files_re: list of regular expressions
+    
+    @ivar disabled: If true, the test will not be ran.
+    @type disabled: bool
+    
+"""
+Test(
+    name = "PL_NNet",
+    description = "Basic neural network for classification",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "nnet.pyplearn",
+    resources = [ 'nnet.pyplearn' ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )



From tihocan at mail.berlios.de  Fri Feb 22 16:54:39 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 22 Feb 2008 16:54:39 +0100
Subject: [Plearn-commits] r8559 - trunk/plearn/var
Message-ID: <200802221554.m1MFsdxU021129@sheep.berlios.de>

Author: tihocan
Date: 2008-02-22 16:54:38 +0100 (Fri, 22 Feb 2008)
New Revision: 8559

Modified:
   trunk/plearn/var/LogAddVariable.cc
Log:
Better help for the class

Modified: trunk/plearn/var/LogAddVariable.cc
===================================================================
--- trunk/plearn/var/LogAddVariable.cc	2008-02-21 21:17:55 UTC (rev 8558)
+++ trunk/plearn/var/LogAddVariable.cc	2008-02-22 15:54:38 UTC (rev 8559)
@@ -55,10 +55,10 @@
 /** LogAddVariable **/
 
 
-PLEARN_IMPLEMENT_OBJECT(LogAddVariable,
-                        "output = log(exp(input1)+exp(input2)) but it is "
-                        "computed in such a way as to preserve precision",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        LogAddVariable,
+        "Stable computation of log(exp(input1) + exp(input2)).",
+        "");
 
 LogAddVariable::LogAddVariable(Variable* input1, Variable* input2)
     : inherited(input1, input2, input1->length(), input1->width())
@@ -107,6 +107,7 @@
     grad1 = input1->value - value;
     apply(grad1, grad1, safeexp);
     input1->gradient += grad1%gradient;
+    // TODO Note that the '%' operator is inefficient.
 
     Vec grad2(nelems());
     grad2 = input2->value - value;



From nouiz at mail.berlios.de  Fri Feb 22 18:47:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 22 Feb 2008 18:47:10 +0100
Subject: [Plearn-commits] r8560 - trunk/plearn/misc
Message-ID: <200802221747.m1MHlAZp027499@sheep.berlios.de>

Author: nouiz
Date: 2008-02-22 18:47:09 +0100 (Fri, 22 Feb 2008)
New Revision: 8560

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
-added option --mat_to_mem
-in vmat compare_stats_ks print the number of value under the threashold


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-22 15:54:38 UTC (rev 8559)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-22 17:47:09 UTC (rev 8560)
@@ -545,9 +545,10 @@
     {
         string source = argv[2];
         string destination = argv[3];
+        bool mat_to_mem=false;
         if(argc<4)
             PLERROR("Usage: vmat convert <source> <destination> "
-                    "[--cols=col1,col2,col3,...]");
+                    "[--mat_to_mem] [--cols=col1,col2,col3,...]");
 
         VMat vm = getVMat(source, indexf);
 
@@ -569,6 +570,8 @@
          *
          *     --delimiter=CHAR
          *           :: conversion to CSV uses specified character as field delimiter
+         *     --mat_to_mem
+         *           :: load the source vmat in memory before saving
          */
         TVec<string> columns;
         bool skip_missings = false;
@@ -593,6 +596,8 @@
             }
             else if (curopt == "--convert-date")
                 convert_date = true;
+            else if (curopt =="--mat_to_mem")
+                mat_to_mem = true;
             else
                 PLWARNING("VMat convert: unrecognized option '%s'; ignoring it...",
                           curopt.c_str());
@@ -607,7 +612,8 @@
         if (ext != ".csv" && skip_missings)
             PLWARNING("Option '--skip-missings' not supported for extension '%s'; ignoring it...",
                       ext.c_str());
-
+        if(mat_to_mem)
+            vm.precompute();
         if(ext==".amat")
             // Save strings as strings so they are not lost.
             vm->saveAMAT(destination, true, false, true);
@@ -1098,7 +1104,7 @@
 
         sortRows(score,2,false);
         pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
-        if(threashold==REAL_MAX)
+        if(threashold<REAL_MAX)
             pout<<"Variables that are under the threashold"<<endl;
         pout<<"Sorted by p_value"<<endl;
         cout << std::left << setw(8) << "# "
@@ -1106,16 +1112,22 @@
              << setw(15) << " D"
              << setw(15) << " p_value"
              <<endl;
+        int threashold_fail=0;
         for(int col=0;col<score.length();col++)
         {
             if(threashold>=score(col,2))
+            {
                 cout << std::left << setw(8) << tostring(col)+"/"+tostring(score(col,0))
                      << setw(size_fieldnames) << m1->fieldName(int(round(score(col,0))))
                      << std::right
                      << setw(15) << score(col,1)
                      << setw(15) << score(col,2)
                      <<endl;
+                threashold_fail++;
+            }
         }
+        if(threashold<REAL_MAX)
+            pout << "Their is "<<threashold_fail<<" variable that are under the threashold"<<endl;
         if(threashold==REAL_MAX)
         {
             pout << "99% cutoff: "<<pc_value_99<<endl;
@@ -1127,9 +1139,21 @@
     }
     else if(command=="compare_stats_desjardins")
     {      
-        if(argc!=8)
+        bool err=false;
+        bool mat_to_mem=false;
+        if(!(argc==8||argc==9))
+            err=true;
+        if(argc==9)
+        {
+            if(argv[8]!=string("--mat_to_mem"))
+                 err = true;
+            else
+                mat_to_mem=true;
+        }
+        if(err)
             PLERROR("vmat compare_stats_desjardins must be used that way:"
-                    " vmat compare_stats_desjardins <orig dataset1> <orig dataset2> <new dataset3> <ks_threashold> <stderror_threashold> <missing_threashold>");
+                    " vmat compare_stats_desjardins <orig dataset1> <orig dataset2> <new dataset3> <ks_threashold> <stderror_threashold> <missing_threashold> [--mat_to_mem]");
+
         VMat m1 = getVMat(argv[2], indexf);
         VMat m2 = getVMat(argv[3], indexf);
         VMat m3 = getVMat(argv[4], indexf);
@@ -1140,11 +1164,16 @@
 
         Vec Ds(m1->width());
         Vec p_values(m1->width());
-        KS_test(m1,m3,10,Ds,p_values);
         Mat score(m1->width(),3);
-
         uint size_fieldnames=m1->max_fieldnames_size();
+        if(mat_to_mem==true)
+        {
+            m1.precompute();
+            m2.precompute();
+            m3.precompute();
+        }
 
+        KS_test(m1,m3,10,Ds,p_values,true);
         for(int col = 0;col<m1->width();col++)
         {
             score(col,0)=col;
@@ -1153,8 +1182,7 @@
             score(col,2)=p_value;
         }
 
-        KS_test(m2,m3,10,Ds,p_values);
-
+        KS_test(m2,m3,10,Ds,p_values,true);
         for(int col = 0;col<m1->width();col++)
         {
             if(p_values[col]>score(col,2))
@@ -1166,7 +1194,7 @@
 
         sortRows(score,2,false);
         pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
-        pout<<"Variables that are under the kd_threashold"<<endl;
+        pout<<"Variables that are under the ks_threashold"<<endl;
         pout<<"Sorted by p_value"<<endl;
         cout << std::left << setw(8) << "# "
              << setw(size_fieldnames) << " fieldname " << std::right
@@ -1184,8 +1212,8 @@
                      << setw(15) << score(col,1)
                      << setw(15) << score(col,2)
                      <<endl;
+                threashold_fail++;
             }
-            threashold_fail++;
         }
         pout << "Their is "<<threashold_fail<<" variable that are under the threashold"<<endl;
         pout <<"Kolmogorow Smirnow two sample test end"<<endl<<endl;



From nouiz at mail.berlios.de  Fri Feb 22 18:49:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 22 Feb 2008 18:49:24 +0100
Subject: [Plearn-commits] r8561 - trunk/plearn/vmat
Message-ID: <200802221749.m1MHnO3h030919@sheep.berlios.de>

Author: nouiz
Date: 2008-02-22 18:49:24 +0100 (Fri, 22 Feb 2008)
New Revision: 8561

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
Added warning


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-22 17:47:09 UTC (rev 8560)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-22 17:49:24 UTC (rev 8561)
@@ -213,7 +213,8 @@
             }
         }
     }
-
+    if(features_to_gaussianize.size()==0)
+        PLWARNING("GaussianizeVMatrix::build_() 0 variable was gaussianized");
     // Obtain meta information from source.
     setMetaInfoFromSource();
 }



From tihocan at mail.berlios.de  Fri Feb 22 20:23:11 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 22 Feb 2008 20:23:11 +0100
Subject: [Plearn-commits] r8562 - trunk/plearn_learners/generic
Message-ID: <200802221923.m1MJNBwx029401@sheep.berlios.de>

Author: tihocan
Date: 2008-02-22 20:23:10 +0100 (Fri, 22 Feb 2008)
New Revision: 8562

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/NNet.h
Log:
- Renamed obscure function 'f' into 'input_to_output'
- Factorized some code into a new method 'applyTransferFunc'
- Started to write code to operate on bags rather than single examples


Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-02-22 17:49:24 UTC (rev 8561)
+++ trunk/plearn_learners/generic/NNet.cc	2008-02-22 19:23:10 UTC (rev 8562)
@@ -93,6 +93,8 @@
 nhidden(0),
 nhidden2(0),
 noutputs(0),
+operate_on_bags(false),
+max_bag_size(20),
 weight_decay(0),
 bias_decay(0),
 layer1_weight_decay(0),
@@ -346,6 +348,32 @@
         " - \"zero\"           = all weights are set to 0\n");
 
     declareOption(
+        ol, "operate_on_bags", &NNet::operate_on_bags, OptionBase::buildoption,
+        "If True, then samples are no longer considered as unique entities.\n"
+        "Instead, each sample belongs to a so-called 'bag', that may contain\n"
+        "1 or more samples. The last column of the target is assumed to\n"
+        "provide information about bags (see help of SumOverBagsVariable for\n"
+        "details on the coding of bags).\n"
+        "When operating on bags, each bag is considered a training sample.\n"
+        "The activations a_ci of output units c for each bag sample i are\n"
+        "combined within each bag, yielding bag activation a_c given by:\n"
+        "   a_c = logadd(a_c1, ..., acn)\n"
+        "In particular, when using the 'softmax' output transfer function,\n"
+        "this corresponds to computing:\n"
+        "   P(class = c | x_1, ..., x_i, ..., x_n) =\n"
+        "                          (\\sum_i exp(a_ci)) / \\sum_c,i exp(a_ci)\n"
+        "where a_ci is the activation of output node c for the i-th sample\n"
+        "x_i in the bag.",
+        OptionBase::advanced_level);
+
+    declareOption(
+        ol, "max_bag_size", &NNet::max_bag_size, OptionBase::buildoption,
+        "Maximum number of samples in a bag (used with 'operate_on_bags').",
+        OptionBase::advanced_level);
+
+    // Learnt options.
+    
+    declareOption(
         ol, "paramsvalues", &NNet::paramsvalues, OptionBase::learntoption, 
         "The learned parameter vector\n");
 
@@ -362,6 +390,26 @@
     build_();
 }
 
+Var to_define(const Var& bag_inputs, const Var& bag_size, const Func& in_to_out, int max_bag_size)
+{
+    return NULL;
+}
+
+/////////////////////////////////
+// buildBagOutputFromBagInputs //
+/////////////////////////////////
+void NNet::buildBagOutputFromBagInputs(
+        const Var& input, Var& before_transfer_func,
+        const Var& bag_inputs, const Var& bag_size, Var& bag_output,
+        int max_bag_size)
+{
+    Func in_to_out = Func(input, before_transfer_func);
+    before_transfer_func = to_define(bag_inputs, bag_size, in_to_out, max_bag_size);
+    // TODO Note that 'to_define' should basically be a logadd over the outputs
+    // before the transfer function.
+    applyTransferFunc(before_transfer_func, bag_output);
+}
+
 ////////////
 // build_ //
 ////////////
@@ -389,6 +437,13 @@
         // Build main network graph.
         buildOutputFromInput(input, hidden_layer, before_transfer_func);
 
+        // When operating on bags, use this network to compute the output on a
+        // whole bag, which also becomes the output of the network.
+        if (operate_on_bags)
+            buildBagOutputFromBagInputs(input, before_transfer_func,
+                                        bag_inputs, bag_size, output,
+                                        max_bag_size);
+
         // Build target and weight variables.
         buildTargetAndWeight();
 
@@ -432,7 +487,9 @@
         }
 
         // Build functions.
-        buildFuncs(input, output, target, sampleweight);
+        buildFuncs(operate_on_bags ? bag_inputs : input,
+                   output, target, sampleweight,
+                   operate_on_bags ? bag_size : NULL);
 
     }
 }
@@ -454,7 +511,6 @@
     //cout << "name = " << name << endl << "targetsize = " << targetsize_ << endl << "weightsize = " << weightsize_ << endl;
 }
 
-
 ////////////////
 // buildCosts //
 ////////////////
@@ -583,7 +639,8 @@
 ////////////////
 // buildFuncs //
 ////////////////
-void NNet::buildFuncs(const Var& the_input, const Var& the_output, const Var& the_target, const Var& the_sampleweight) {
+void NNet::buildFuncs(const Var& the_input, const Var& the_output, const Var& the_target, const Var& the_sampleweight,
+        const Var& the_bag_size) {
     invars.resize(0);
     VarArray outvars;
     VarArray testinvars;
@@ -592,6 +649,10 @@
         invars.push_back(the_input);
         testinvars.push_back(the_input);
     }
+    if (the_bag_size) {
+        invars.append(the_bag_size);
+        testinvars.append(the_bag_size);
+    }
     if (the_output)
         outvars.push_back(the_output);
     if(the_target)
@@ -604,17 +665,19 @@
     {
         invars.push_back(the_sampleweight);
     }
-    f = Func(the_input, the_output);
+    input_to_output = Func(the_input, the_output);
     test_costf = Func(testinvars, the_output&test_costs);
     test_costf->recomputeParents();
     output_and_target_to_cost = Func(outvars, test_costs); 
     // Since there will be a fprop() in the network, we need to make sure the
     // input is valid.
-    if (train_set && train_set->length() >= 1) {
+    if (train_set && train_set->length() >= the_input->width()) {
         Vec input, target;
         real weight;
-        train_set->getExample(0, input, target, weight);
-        the_input->matValue << input;
+        for (int i = 0; i < the_input->width(); i++) {
+            train_set->getExample(i, input, target, weight);
+            the_input->matValue.column(i) << input;
+        }
     }
     output_and_target_to_cost->recomputeParents();
 }
@@ -725,35 +788,41 @@
     }
 
     before_transfer_func = output;
+    applyTransferFunc(before_transfer_func, output);
+}
 
-    /*
-     * output_transfer_func
-     */
+///////////////////////
+// applyTransferFunc //
+///////////////////////
+void NNet::applyTransferFunc(const Var& before_transfer_func, Var& output)
+{
     size_t p=0;
     if(output_transfer_func!="" && output_transfer_func!="none")
     {
         if(output_transfer_func=="tanh")
-            output = tanh(output);
+            output = tanh(before_transfer_func);
         else if(output_transfer_func=="sigmoid")
-            output = sigmoid(output);
+            output = sigmoid(before_transfer_func);
         else if(output_transfer_func=="softplus")
-            output = softplus(output);
+            output = softplus(before_transfer_func);
         else if(output_transfer_func=="exp")
-            output = exp(output);
+            output = exp(before_transfer_func);
         else if(output_transfer_func=="softmax")
-            output = softmax(output);
+            output = softmax(before_transfer_func);
         else if (output_transfer_func == "log_softmax")
-            output = log_softmax(output);
+            output = log_softmax(before_transfer_func);
         else if ((p=output_transfer_func.find("interval"))!=string::npos)
         {
             size_t q = output_transfer_func.find(",");
             interval_minval = atof(output_transfer_func.substr(p+1,q-(p+1)).c_str());
             size_t r = output_transfer_func.find(")");
             interval_maxval = atof(output_transfer_func.substr(q+1,r-(q+1)).c_str());
-            output = interval_minval + (interval_maxval - interval_minval)*sigmoid(output);
+            output = interval_minval + (interval_maxval - interval_minval)*sigmoid(before_transfer_func);
         }
         else
-            PLERROR("In NNet::build_()  unknown output_transfer_func option: %s",output_transfer_func.c_str());
+            PLERROR("In NNet::applyTransferFunc() -Unknown value for the "
+                   "'output_transfer_func' option: %s",
+                   output_transfer_func.c_str());
     }
 }
 
@@ -827,7 +896,7 @@
 void NNet::computeOutput(const Vec& inputv, Vec& outputv) const
 {
     outputv.resize(outputsize());
-    f->fprop(inputv,outputv);
+    input_to_output->fprop(inputv,outputv);
 }
 
 ///////////////////////////
@@ -994,6 +1063,8 @@
     varDeepCopyField(test_costs, copies);
     deepCopyField(invars, copies);
     deepCopyField(params, copies);
+    varDeepCopyField(bag_inputs, copies);
+    varDeepCopyField(bag_size, copies);
     // public:
     deepCopyField(paramsvalues, copies);
     varDeepCopyField(input, copies);
@@ -1006,7 +1077,7 @@
     varDeepCopyField(wdirect, copies);
     varDeepCopyField(wrec, copies);
     varDeepCopyField(hidden_layer, copies);
-    deepCopyField(f, copies);
+    deepCopyField(input_to_output, copies);
     deepCopyField(test_costf, copies);
     deepCopyField(output_and_target_to_cost, copies);
     varDeepCopyField(first_hidden_layer, copies);
@@ -1028,10 +1099,11 @@
 {
     // NNet nstages is number of epochs (whole passages through the training set)
     // while optimizer nstages is number of weight updates.
-    // So relationship between the 2 depends whether we are in stochastic, batch or minibatch mode
+    // So relationship between the 2 depends on whether we are in stochastic,
+    // batch or minibatch mode.
 
     if(!train_set)
-        PLERROR("In NNet::train, you did not setTrainingSet");
+        PLERROR("In NNet::train - No training set available");
     
     if(!train_stats)
         setTrainStatsCollector(new VecStatsCollector());
@@ -1039,7 +1111,7 @@
 
     int l = train_set->length();  
     
-    if(f.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
+    if(input_to_output.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
 
     // number of samples seen by optimizer before each optimizer update

Modified: trunk/plearn_learners/generic/NNet.h
===================================================================
--- trunk/plearn_learners/generic/NNet.h	2008-02-22 17:49:24 UTC (rev 8561)
+++ trunk/plearn_learners/generic/NNet.h	2008-02-22 19:23:10 UTC (rev 8562)
@@ -70,6 +70,12 @@
     VarArray invars;
     VarArray params;  // all arameter input vars
 
+    //! Used to store the inputs in a bag when 'operate_on_bags' is true.
+    Var bag_inputs;
+
+    //! Used to store the size of a bag when 'operate_on_bags' is true.
+    Var bag_size;
+
 // to put back later -- blip  Vec paramsvalues; // values of all parameters
 
 public: // to set these values instead of getting them by training
@@ -90,7 +96,7 @@
 
 public:
 
-    mutable Func f; // input -> output
+    mutable Func input_to_output; // input -> output
     mutable Func test_costf; // input & target -> output & test_costs
     mutable Func output_and_target_to_cost; // output & target -> cost
 
@@ -125,6 +131,9 @@
      */
     int noutputs;
 
+    bool operate_on_bags;
+    int max_bag_size;
+
     real weight_decay; // default: 0
     real bias_decay;   // default: 0 
     real layer1_weight_decay; // default: MISSING_VALUE
@@ -229,6 +238,15 @@
     //! available in the 'before_transfer_func' parameter.
     void buildOutputFromInput(const Var& the_input, Var& hidden_layer, Var& before_transfer_func);
 
+    //! Build the output for a whole bag, from the network defined by the
+    //! 'input' to 'before_transfer_func' variables.
+    //! 'before_transfer_func' is modified so as to hold the activations for
+    //! the bag rather than for individual samples.
+    void buildBagOutputFromBagInputs(
+        const Var& input, Var& before_transfer_func,
+        const Var& bag_inputs, const Var& bag_size, Var& bag_output,
+        int max_bag_size);
+
     //! Builds the target and sampleweight variables.
     void buildTargetAndWeight();
 
@@ -236,8 +254,11 @@
     void buildCosts(const Var& output, const Var& target, const Var& hidden_layer, const Var& before_transfer_func);
 
     //! Build the various functions used in the network.
-    void buildFuncs(const Var& the_input, const Var& the_output, const Var& the_target, const Var& the_sampleweight);
+    void buildFuncs(const Var& the_input, const Var& the_output, const Var& the_target, const Var& the_sampleweight, const Var& the_bag_size);
 
+    //! Compute the final output from the activations of the output units.
+    void applyTransferFunc(const Var& before_transfer_func, Var& output);
+
     //! Fill a matrix of weights according to the 'initialization_method' specified.
     //! The 'clear_first_row' boolean indicates whether we should fill the first
     //! row with zeros.



From tihocan at mail.berlios.de  Fri Feb 22 22:14:49 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 22 Feb 2008 22:14:49 +0100
Subject: [Plearn-commits] r8563 - trunk/plearn_learners/generic
Message-ID: <200802222114.m1MLEnjb009325@sheep.berlios.de>

Author: tihocan
Date: 2008-02-22 22:14:48 +0100 (Fri, 22 Feb 2008)
New Revision: 8563

Modified:
   trunk/plearn_learners/generic/AddLayersNNet.cc
Log:
Fixed to compile with new change to NNet

Modified: trunk/plearn_learners/generic/AddLayersNNet.cc
===================================================================
--- trunk/plearn_learners/generic/AddLayersNNet.cc	2008-02-22 19:23:10 UTC (rev 8562)
+++ trunk/plearn_learners/generic/AddLayersNNet.cc	2008-02-22 21:14:48 UTC (rev 8563)
@@ -231,7 +231,7 @@
     }
 
     // Build functions.
-    buildFuncs(input, output, target, sampleweight);
+    buildFuncs(input, output, target, sampleweight, NULL);
 
 }
 



From nouiz at mail.berlios.de  Fri Feb 22 22:45:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 22 Feb 2008 22:45:32 +0100
Subject: [Plearn-commits] r8564 - trunk/plearn/math
Message-ID: <200802222145.m1MLjWuo011988@sheep.berlios.de>

Author: nouiz
Date: 2008-02-22 22:45:32 +0100 (Fri, 22 Feb 2008)
New Revision: 8564

Modified:
   trunk/plearn/math/pl_erf.cc
Log:
the glibc version of erf is 8.5 times faster then the plearn version. This is probabli caused by pl_gcf which take more then half the execution time of pl_erf. So if glibc is their, we use the faster version.


Modified: trunk/plearn/math/pl_erf.cc
===================================================================
--- trunk/plearn/math/pl_erf.cc	2008-02-22 21:14:48 UTC (rev 8563)
+++ trunk/plearn/math/pl_erf.cc	2008-02-22 21:45:32 UTC (rev 8564)
@@ -35,7 +35,7 @@
 
 #include <plearn/base/general.h>
 // #include <iostream>
-// #include <cmath>
+#include <cmath>
 
 namespace PLearn {
 using namespace std;
@@ -152,7 +152,13 @@
 
 // returns the error function "erf"
 real pl_erf(real x) {
+#ifdef __GNUC__
+    //8.5 time faster in my test then plearn version.
+    return erf(x);
+#else
+    //it is pl_gcf that take too much time...optimise?
     return (x<0?-1:1)*(1-pl_gammq(0.5,x*x));
+#endif
 }
 
 



From tihocan at mail.berlios.de  Mon Feb 25 17:46:25 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 17:46:25 +0100
Subject: [Plearn-commits] r8565 - trunk/plearn/var
Message-ID: <200802251646.m1PGkP1p025872@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 17:46:24 +0100 (Mon, 25 Feb 2008)
New Revision: 8565

Modified:
   trunk/plearn/var/AffineTransformVariable.cc
   trunk/plearn/var/AffineTransformVariable.h
Log:
Better help and a few cosmetic changes

Modified: trunk/plearn/var/AffineTransformVariable.cc
===================================================================
--- trunk/plearn/var/AffineTransformVariable.cc	2008-02-22 21:45:32 UTC (rev 8564)
+++ trunk/plearn/var/AffineTransformVariable.cc	2008-02-25 16:46:24 UTC (rev 8565)
@@ -46,33 +46,54 @@
 using namespace std;
 
 
-PLEARN_IMPLEMENT_OBJECT(AffineTransformVariable,
-                        "Affine transformation of a vector variable.",
-                        "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        AffineTransformVariable,
+        "Affine transformation of a vector variable.",
+        "The first input is the vector variable.\n"
+        "The second input is the matrix of biases (on the first row) and\n"
+        "weights (in other rows).");
 
-AffineTransformVariable::AffineTransformVariable(Variable* vec, Variable* transformation)
-    : inherited(vec, transformation, 
-                (vec->size() == 1) ? transformation->width() : (vec->isRowVec() ? 1 : transformation->width()),
-                (vec->size() == 1) ? 1 : (vec->isRowVec() ? transformation->width() : 1))
+/////////////////////////////
+// AffineTransformVariable //
+/////////////////////////////
+AffineTransformVariable::AffineTransformVariable(Variable* vec,
+                                                 Variable* transformation,
+                                                 bool call_build_):
+    inherited(vec, transformation, 
+            (vec->size() == 1) ? transformation->width()
+                               : (vec->isRowVec() ? 1
+                                                  : transformation->width()),
+            (vec->size() == 1) ? 1
+                               : (vec->isRowVec() ? transformation->width()
+                                                  : 1),
+            call_build_)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-AffineTransformVariable::build()
+///////////
+// build //
+///////////
+void AffineTransformVariable::build()
 {
     inherited::build();
     build_();
 }
 
-void
-AffineTransformVariable::build_()
+////////////
+// build_ //
+////////////
+void AffineTransformVariable::build_()
 {
     // input1 is vec from constructor
     if (input1 && !input1->isVec())
         PLERROR("In AffineTransformVariable: expecting a vector Var (row or column) as first argument");
 }
 
+///////////////////
+// recomputeSize //
+///////////////////
 void AffineTransformVariable::recomputeSize(int& l, int& w) const
 { 
     if (input1 && input2) {

Modified: trunk/plearn/var/AffineTransformVariable.h
===================================================================
--- trunk/plearn/var/AffineTransformVariable.h	2008-02-22 21:45:32 UTC (rev 8564)
+++ trunk/plearn/var/AffineTransformVariable.h	2008-02-25 16:46:24 UTC (rev 8565)
@@ -63,7 +63,8 @@
 public:
     //!  Default constructor for persistence
     AffineTransformVariable() {}
-    AffineTransformVariable(Variable* vec, Variable* transformation);
+    AffineTransformVariable(Variable* vec, Variable* transformation,
+                            bool call_build_ = true);
 
     PLEARN_DECLARE_OBJECT(AffineTransformVariable);
 



From tihocan at mail.berlios.de  Mon Feb 25 17:47:59 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 17:47:59 +0100
Subject: [Plearn-commits] r8566 - trunk/plearn/var
Message-ID: <200802251647.m1PGlxPT026049@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 17:47:57 +0100 (Mon, 25 Feb 2008)
New Revision: 8566

Modified:
   trunk/plearn/var/Variable.cc
Log:
Can now use the [] operator on a column vector Variable

Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2008-02-25 16:46:24 UTC (rev 8565)
+++ trunk/plearn/var/Variable.cc	2008-02-25 16:47:57 UTC (rev 8566)
@@ -99,12 +99,21 @@
     :PP<Variable>(new SourceVariable(m))
 {}
 
+////////////
+// length //
+////////////
 int Var::length() const
 { return (*this)->length(); }
 
+///////////
+// width //
+///////////
 int Var::width() const
 { return (*this)->width(); }
 
+////////
+// [] // 
+////////
 Var Var::operator[](int i) const
 {
     if(width()==1)
@@ -117,11 +126,16 @@
 
 Var Var::operator[](Var index) const
 { 
-    if(width()!=1)
-        PLERROR("You shouldnt use operator[](Var index) to get the row of a matrix var but operator()(Var index)");
-    return new VarElementVariable(*this,index); 
+    if (!ptr->isVec())
+        PLERROR("In Var::operator[](Var index) - You should not use this "
+                "operator to get the row of a matrix Var, but "
+                "operator()(Var index)");
+    return new VarElementVariable(*this, index); 
 }
 
+////////////
+// subMat //
+////////////
 Var Var::subMat(int i, int j, int sublength, int subwidth, bool do_transpose) const
 { 
     if(do_transpose)



From tihocan at mail.berlios.de  Mon Feb 25 17:49:07 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 17:49:07 +0100
Subject: [Plearn-commits] r8567 - trunk/plearn/misc
Message-ID: <200802251649.m1PGn7OL026232@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 17:49:06 +0100 (Mon, 25 Feb 2008)
New Revision: 8567

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
Fixed typo in spelling of 'threshold'

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-25 16:47:57 UTC (rev 8566)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-25 16:49:06 UTC (rev 8567)
@@ -1042,7 +1042,7 @@
     else if(command=="compare_stats_ks")
     {
         bool err = false;
-        real threashold = REAL_MAX;
+        real threshold = REAL_MAX;
         bool mat_to_mem = false;
         if(argc<4||argc>6)
             err = true;
@@ -1050,19 +1050,19 @@
         {
             if(argv[4]==string("--mat_to_mem"))
                 mat_to_mem=true;
-            else if(!pl_isnumber(string(argv[4]),&threashold))
+            else if(!pl_isnumber(string(argv[4]),&threshold))
                 err = true;
         }
         else if(argc==6)
         {
              if(argv[5]!=string("--mat_to_mem"))
                  err = true;
-             else if(!pl_isnumber(string(argv[4]),&threashold))
+             else if(!pl_isnumber(string(argv[4]),&threshold))
                  err = true;
         }
         if(err)
             PLERROR("vmat compare_stats_ks must be used that way:"
-                    " vmat compare_stats_ks <dataset1> <dataset2> [threashold]"
+                    " vmat compare_stats_ks <dataset1> <dataset2> [threshold]"
                     " [--mat_to_mem]");
 
         VMat m1 = getVMat(argv[2], indexf);
@@ -1104,18 +1104,18 @@
 
         sortRows(score,2,false);
         pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
-        if(threashold<REAL_MAX)
-            pout<<"Variables that are under the threashold"<<endl;
+        if(threshold<REAL_MAX)
+            pout<<"Variables that are under the threshold"<<endl;
         pout<<"Sorted by p_value"<<endl;
         cout << std::left << setw(8) << "# "
              << setw(size_fieldnames) << " fieldname " << std::right
              << setw(15) << " D"
              << setw(15) << " p_value"
              <<endl;
-        int threashold_fail=0;
+        int threshold_fail=0;
         for(int col=0;col<score.length();col++)
         {
-            if(threashold>=score(col,2))
+            if(threshold>=score(col,2))
             {
                 cout << std::left << setw(8) << tostring(col)+"/"+tostring(score(col,0))
                      << setw(size_fieldnames) << m1->fieldName(int(round(score(col,0))))
@@ -1123,12 +1123,12 @@
                      << setw(15) << score(col,1)
                      << setw(15) << score(col,2)
                      <<endl;
-                threashold_fail++;
+                threshold_fail++;
             }
         }
-        if(threashold<REAL_MAX)
-            pout << "Their is "<<threashold_fail<<" variable that are under the threashold"<<endl;
-        if(threashold==REAL_MAX)
+        if(threshold<REAL_MAX)
+            pout << "Their is "<<threshold_fail<<" variable that are under the threshold"<<endl;
+        if(threshold==REAL_MAX)
         {
             pout << "99% cutoff: "<<pc_value_99<<endl;
             pout << "95% cutoff: "<<pc_value_95<<endl;
@@ -1152,12 +1152,12 @@
         }
         if(err)
             PLERROR("vmat compare_stats_desjardins must be used that way:"
-                    " vmat compare_stats_desjardins <orig dataset1> <orig dataset2> <new dataset3> <ks_threashold> <stderror_threashold> <missing_threashold> [--mat_to_mem]");
+                    " vmat compare_stats_desjardins <orig dataset1> <orig dataset2> <new dataset3> <ks_threshold> <stderror_threshold> <missing_threshold> [--mat_to_mem]");
 
         VMat m1 = getVMat(argv[2], indexf);
         VMat m2 = getVMat(argv[3], indexf);
         VMat m3 = getVMat(argv[4], indexf);
-        real ks_threashold = toreal(argv[5]);
+        real ks_threshold = toreal(argv[5]);
 
         m3->compatibleSizeError(m1);
         m3->compatibleSizeError(m2);
@@ -1194,17 +1194,17 @@
 
         sortRows(score,2,false);
         pout <<"Kolmogorow Smirnow two sample test"<<endl<<endl;
-        pout<<"Variables that are under the ks_threashold"<<endl;
+        pout<<"Variables that are under the ks_threshold"<<endl;
         pout<<"Sorted by p_value"<<endl;
         cout << std::left << setw(8) << "# "
              << setw(size_fieldnames) << " fieldname " << std::right
              << setw(15) << " D"
              << setw(15) << " p_value"
              <<endl;
-        int threashold_fail = 0;
+        int threshold_fail = 0;
         for(int col=0;col<score.length();col++)
         {
-            if(ks_threashold>=score(col,2))
+            if(ks_threshold>=score(col,2))
             {
                 cout << std::left << setw(8) << tostring(col)+"/"+tostring(score(col,0))
                      << setw(size_fieldnames) << m1->fieldName(int(round(score(col,0))))
@@ -1212,10 +1212,10 @@
                      << setw(15) << score(col,1)
                      << setw(15) << score(col,2)
                      <<endl;
-                threashold_fail++;
+                threshold_fail++;
             }
         }
-        pout << "Their is "<<threashold_fail<<" variable that are under the threashold"<<endl;
+        pout << "Their is "<<threshold_fail<<" variable that are under the threshold"<<endl;
         pout <<"Kolmogorow Smirnow two sample test end"<<endl<<endl;
 
 



From tihocan at mail.berlios.de  Mon Feb 25 17:50:17 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 17:50:17 +0100
Subject: [Plearn-commits] r8568 - trunk/plearn/vmat
Message-ID: <200802251650.m1PGoH5c026428@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 17:50:14 +0100 (Mon, 25 Feb 2008)
New Revision: 8568

Modified:
   trunk/plearn/vmat/FileVMatrix.cc
   trunk/plearn/vmat/FileVMatrix.h
Log:
Added missing call to absolute() when opening file, which could have been dangerous in some situations

Modified: trunk/plearn/vmat/FileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FileVMatrix.cc	2008-02-25 16:49:06 UTC (rev 8567)
+++ trunk/plearn/vmat/FileVMatrix.cc	2008-02-25 16:50:14 UTC (rev 8568)
@@ -158,7 +158,7 @@
         if (!writable)
             PLERROR("In FileVMatrix::build_ - You asked to create a new file (%s), but 'writable' is set to 0 !", filename_.c_str());
 
-        openfile(filename_.c_str(),"w+b", 
+        openfile(filename_,"w+b", 
                  PR_RDWR | PR_CREATE_FILE | PR_TRUNCATE, 0666);
         if (!f)
             PLERROR("In FileVMatrix constructor, could not open file %s",filename_.c_str());
@@ -203,9 +203,9 @@
     else
     {
         if (writable)
-            openfile(filename_.c_str(), "r+b", PR_RDWR | PR_CREATE_FILE, 0666);
+            openfile(filename_, "r+b", PR_RDWR | PR_CREATE_FILE, 0666);
         else
-            openfile(filename_.c_str(), "rb", PR_RDONLY, 0666);
+            openfile(filename_, "rb", PR_RDONLY, 0666);
 
         if (! f)
             PLERROR("FileVMatrix::build: could not open file %s", filename_.c_str());
@@ -511,12 +511,12 @@
 //////////////////
 // updateHeader //
 //////////////////
-void FileVMatrix::openfile(const char *path, const char *mode,
+void FileVMatrix::openfile(const PPath& path, const char *mode,
                            PRIntn flags, PRIntn mode2) {
 #ifdef USE_NSPR_FILE
-        f = PR_Open(path, flags, mode2);
+        f = PR_Open(path.absolute().c_str(), flags, mode2);
 #else
-        f = fopen(path, mode);
+        f = fopen(path.absolute().c_str(), mode);
 #endif
 
 }

Modified: trunk/plearn/vmat/FileVMatrix.h
===================================================================
--- trunk/plearn/vmat/FileVMatrix.h	2008-02-25 16:49:06 UTC (rev 8567)
+++ trunk/plearn/vmat/FileVMatrix.h	2008-02-25 16:50:14 UTC (rev 8568)
@@ -78,7 +78,7 @@
 private:
 
     bool build_new_file;
-    void openfile(const char *path, const char *mode, PRIntn flags,
+    void openfile(const PPath& path, const char *mode, PRIntn flags,
                   PRIntn mode2);
 
 public:



From tihocan at mail.berlios.de  Mon Feb 25 17:53:14 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 17:53:14 +0100
Subject: [Plearn-commits] r8569 - trunk/plearn_learners/generic
Message-ID: <200802251653.m1PGrEKg026977@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 17:53:13 +0100 (Mon, 25 Feb 2008)
New Revision: 8569

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
The data is now stored in row vectors rather than column ones: this is to make it easier in the future to use bags of multiple inputs stored as row vectors of a matrix

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-02-25 16:50:14 UTC (rev 8568)
+++ trunk/plearn_learners/generic/NNet.cc	2008-02-25 16:53:13 UTC (rev 8569)
@@ -55,6 +55,7 @@
 #include <plearn/var/NegCrossEntropySigmoidVariable.h>
 #include <plearn/var/NegLogPoissonVariable.h>
 #include <plearn/var/OneHotSquaredLoss.h>
+#include <plearn/var/ProductVariable.h>
 // #include "RBFLayerVariable.h" //TODO Put it back when the file exists.
 #include <plearn/var/SigmoidVariable.h>
 #include <plearn/var/SoftmaxVariable.h>
@@ -65,7 +66,6 @@
 #include <plearn/var/SumSquareVariable.h>
 #include <plearn/var/TanhVariable.h>
 #include <plearn/var/TransposeVariable.h>
-#include <plearn/var/TransposeProductVariable.h>
 #include <plearn/var/UnaryHardSlopeVariable.h>
 #include <plearn/var/Var_operators.h>
 #include <plearn/var/Var_utils.h>
@@ -429,7 +429,7 @@
             PLERROR("NNet: the option 'noutputs' must be specified");
 
         // Initialize the input.
-        input = Var(inputsize(), "input");
+        input = Var(1, inputsize(), "input");
 
         params.resize(0);
         Var before_transfer_func;
@@ -529,7 +529,7 @@
             costs[k] = onehot_squared_loss(the_output, the_target);
         else if (cost_funcs[k]=="NLL") 
         {
-            if (the_output->size() == 1) {
+            if (the_output->width() == 1) {
                 // Assume sigmoid output here!
                 costs[k] = stable_cross_entropy(before_transfer_func, the_target);
             } else {
@@ -541,7 +541,7 @@
         } 
         else if (cost_funcs[k]=="class_error")
         {
-            if (the_output->size()==1)
+            if (the_output->width()==1)
                 costs[k] = binary_classification_loss(the_output, the_target);
             else
                 costs[k] = classification_loss(the_output, the_target);
@@ -674,9 +674,9 @@
     if (train_set && train_set->length() >= the_input->width()) {
         Vec input, target;
         real weight;
-        for (int i = 0; i < the_input->width(); i++) {
+        for (int i = 0; i < the_input->length(); i++) {
             train_set->getExample(i, input, target, weight);
-            the_input->matValue.column(i) << input;
+            the_input->matValue(i) << input;
         }
     }
     output_and_target_to_cost->recomputeParents();
@@ -714,7 +714,7 @@
     }
     else if(nhidden>0)
     {
-        w1 = Var(1 + the_input->size(), nhidden, "w1");      
+        w1 = Var(1 + the_input->width(), nhidden, "w1");      
         params.append(w1);
         hidden_layer = hiddenLayer(output, w1);
         output = hidden_layer;
@@ -725,7 +725,7 @@
     if(nhidden2>0)
     {
         PLASSERT( !first_hidden_layer_is_output );
-        w2 = Var(1 + output.length(), nhidden2, "w2");
+        w2 = Var(1 + output.width(), nhidden2, "w2");
         params.append(w2);
         output = hiddenLayer(output, w2);
     }
@@ -757,13 +757,14 @@
 
     // Output layer before transfer function.
     if (!first_hidden_layer_is_output) {
-        wout = Var(1 + output->size(), outputsize(), "wout");
+        wout = Var(1 + output->width(), outputsize(), "wout");
         output = affine_transform(output, wout);
+        output->setName("output_activations");
         if (!fixed_output_weights)
             params.append(wout);
         else
         {
-            outbias = Var(output->size(), "outbias");
+            outbias = Var(1, output->width(), "outbias");
             output = output + outbias;
             params.append(outbias);
         }
@@ -780,8 +781,8 @@
     // Direct in-to-out layer.
     if(direct_in_to_out)
     {
-        wdirect = Var(the_input->size(), outputsize(), "wdirect");
-        output += transposeProduct(wdirect, the_input);
+        wdirect = Var(the_input->width(), outputsize(), "wdirect");
+        output += product(the_input, wdirect);
         params.append(wdirect);
         if (nhidden <= 0)
             PLERROR("In NNet::buildOutputFromInput - It seems weird to use direct in-to-out connections if there is no hidden layer anyway");
@@ -853,7 +854,7 @@
     }
     if (input_reconstruction_penalty>0)
     {
-        wrec = Var(1 + hidden_layer->size(),input->size(),"wrec");
+        wrec = Var(1 + hidden_layer->width(),input->width(),"wrec");
         predicted_input = affine_transform(hidden_layer, wrec);
         params.append(wrec);
         penalties.append(input_reconstruction_penalty*sumsquare(predicted_input - input));
@@ -864,7 +865,7 @@
 // buildTargetAndWeight //
 //////////////////////////
 void NNet::buildTargetAndWeight() {
-    target = Var(targetsize(), "target");
+    target = Var(1, targetsize(), "target");
     if(weightsize_>0)
     {
         if (weightsize_!=1)
@@ -971,6 +972,7 @@
 /////////////////
 Var NNet::hiddenLayer(const Var& input, const Var& weights, string transfer_func) {
     Var hidden = affine_transform(input, weights); 
+    hidden->setName("hidden_layer_activations");
     Var result;
     if (transfer_func == "default")
         transfer_func = hidden_transfer_func;



From nouiz at mail.berlios.de  Mon Feb 25 18:14:53 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 25 Feb 2008 18:14:53 +0100
Subject: [Plearn-commits] r8570 - trunk/plearn/vmat
Message-ID: <200802251714.m1PHErJl029036@sheep.berlios.de>

Author: nouiz
Date: 2008-02-25 18:14:52 +0100 (Mon, 25 Feb 2008)
New Revision: 8570

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
give warning when we recompute old stats


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-25 16:53:13 UTC (rev 8569)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-25 17:14:52 UTC (rev 8570)
@@ -1426,7 +1426,11 @@
     PPath metadatadir = getMetaDataDir();
     PPath statsfile =  metadatadir / filename;
     lockMetaDataDir();
-    if (isfile(statsfile) && getMtime()<mtime(statsfile))
+    bool file = isfile(statsfile);
+    bool uptodate = false;
+    if(file)
+        uptodate = getMtime()<mtime(statsfile);
+    if (file && uptodate)
     {
         if(getMtime() == 0)
             PLWARNING("Warning: using a saved stat file (%s) but mtime is 0"
@@ -1436,6 +1440,10 @@
     }
     else
     {
+        if(file && !uptodate)
+            PLWARNING("Warning: recomputing stat file (%s) as it is older"
+                      "the source.",
+                      statsfile.absolute().c_str());
         VMat vm = const_cast<VMatrix*>(this);
         stats = PLearn::computeStats(vm, maxnvalues, progress_bar);
         if(!metadatadir.isEmpty())



From tihocan at mail.berlios.de  Mon Feb 25 22:15:02 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 22:15:02 +0100
Subject: [Plearn-commits] r8571 - in trunk: commands plearn/var/test
	plearn/var/test/.pytest plearn/var/test/.pytest/PL_Variables
	plearn/var/test/.pytest/PL_Variables/expected_results
Message-ID: <200802252115.m1PLF2sV007970@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 22:15:01 +0100 (Mon, 25 Feb 2008)
New Revision: 8571

Added:
   trunk/plearn/var/test/.pytest/PL_Variables/
   trunk/plearn/var/test/.pytest/PL_Variables/expected_results/
   trunk/plearn/var/test/.pytest/PL_Variables/expected_results/RUN.log
   trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave
   trunk/plearn/var/test/VariablesTest.cc
   trunk/plearn/var/test/VariablesTest.h
Modified:
   trunk/commands/plearn_tests_inc.h
   trunk/plearn/var/test/pytest.config
Log:
Added new test for Variables - Currently only testing UnfoldedFuncVariable

Modified: trunk/commands/plearn_tests_inc.h
===================================================================
--- trunk/commands/plearn_tests_inc.h	2008-02-25 17:14:52 UTC (rev 8570)
+++ trunk/commands/plearn_tests_inc.h	2008-02-25 21:15:01 UTC (rev 8571)
@@ -71,6 +71,7 @@
 #include <plearn/python/test/InjectionTest.h>
 #include <plearn/python/test/InterfunctionXchgTest.h>
 #include <plearn/python/test/MemoryStressTest.h>
+#include <plearn/var/test/VariablesTest.h>
 #include <plearn/var/test/VarUtilsTest.h>
 #include <plearn/vmat/test/AutoVMatrixTest.h>
 #include <plearn/vmat/test/FileVMatrixTest.h>


Property changes on: trunk/plearn/var/test/.pytest/PL_Variables
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn/var/test/.pytest/PL_Variables/expected_results/RUN.log
===================================================================

Added: trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave
===================================================================
--- trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave	2008-02-25 17:14:52 UTC (rev 8570)
+++ trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave	2008-02-25 21:15:01 UTC (rev 8571)
@@ -0,0 +1,9 @@
+*1 ->VariablesTest(
+unfolded_argmin = 3  1  [ 
+3 	
+2 	
+4 	
+]
+;
+save = 1 ;
+save_path = ""  )

Added: trunk/plearn/var/test/VariablesTest.cc
===================================================================
--- trunk/plearn/var/test/VariablesTest.cc	2008-02-25 17:14:52 UTC (rev 8570)
+++ trunk/plearn/var/test/VariablesTest.cc	2008-02-25 21:15:01 UTC (rev 8571)
@@ -0,0 +1,160 @@
+// -*- C++ -*-
+
+// VariablesTest.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file VariablesTest.cc */
+
+
+#include "VariablesTest.h"
+#include <plearn/math/PRandom.h>
+#include <plearn/var/ArgminVariable.h>
+#include <plearn/var/Func.h>
+#include <plearn/var/UnfoldedFuncVariable.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    VariablesTest,
+    "Tests various Variable objects.",
+    "Feel free to add more Variable tests in there."
+);
+
+//////////////////
+// VariablesTest //
+//////////////////
+VariablesTest::VariablesTest()
+{
+}
+
+///////////
+// build //
+///////////
+void VariablesTest::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void VariablesTest::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("VariablesTest::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void VariablesTest::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "unfolded_argmin", &VariablesTest::unfolded_argmin,
+                   OptionBase::buildoption,
+        "Result of the unfolded argmin computation.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void VariablesTest::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+/////////////
+// perform //
+/////////////
+void VariablesTest::perform()
+{
+    // Test UnfoldedFuncVariable to compute an argmin over a set of row vectors.
+    int is1 = 5;
+    int n_input2 = 3;
+    Var input1(1, is1, "input1");
+    Var argmin1 = argmin(input1);
+    Func input1_to_argmin1(input1, argmin1);
+    Var input2(n_input2, is1, "input2");
+    PRandom::common(false)->fill_random_uniform(input2->matValue, -1, 1);
+    //pout << input2->matValue << endl;
+    Var unfolded_argmin1 = new UnfoldedFuncVariable(input2, input1_to_argmin1,
+                                                    false);
+    unfolded_argmin1->fprop();
+    unfolded_argmin = unfolded_argmin1->matValue.copy();
+    //pout << unfolded_argmin << endl;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/test/VariablesTest.h
===================================================================
--- trunk/plearn/var/test/VariablesTest.h	2008-02-25 17:14:52 UTC (rev 8570)
+++ trunk/plearn/var/test/VariablesTest.h	2008-02-25 21:15:01 UTC (rev 8571)
@@ -0,0 +1,139 @@
+// -*- C++ -*-
+
+// VariablesTest.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file VariablesTest.h */
+
+
+#ifndef VariablesTest_INC
+#define VariablesTest_INC
+
+#include <plearn/misc/PTest.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class VariablesTest : public PTest
+{
+    typedef PTest inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    Mat unfolded_argmin;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    VariablesTest();
+
+    // Your other public member functions go here
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(VariablesTest);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    //#####  PLearn::PTest Protocol  ##########################################
+
+    //! The method performing the test. A typical test consists in some output
+    //! (to pout and / or perr), and updates of this object's options.
+    virtual void perform();
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(VariablesTest);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/var/test/pytest.config
===================================================================
--- trunk/plearn/var/test/pytest.config	2008-02-25 17:14:52 UTC (rev 8570)
+++ trunk/plearn/var/test/pytest.config	2008-02-25 21:15:01 UTC (rev 8571)
@@ -93,6 +93,23 @@
     
 """
 Test(
+    name = "PL_Variables",
+    description = "Perform various tests on Variable objects",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn \"object=VariablesTest()\"",
+    resources = [ ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+Test(
     name = "PL_Var_util",
     description = "Test various functions in Var_utils",
     category = "General",
@@ -104,5 +121,7 @@
     resources = [ ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = False,
+    runtime = None,
+    difftime = None
     )



From tihocan at mail.berlios.de  Mon Feb 25 22:46:11 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 22:46:11 +0100
Subject: [Plearn-commits] r8572 - trunk/plearn/var
Message-ID: <200802252146.m1PLkBRR010272@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 22:46:11 +0100 (Mon, 25 Feb 2008)
New Revision: 8572

Modified:
   trunk/plearn/var/NaryVariable.cc
   trunk/plearn/var/NaryVariable.h
Log:
Added the proper build mechanism

Modified: trunk/plearn/var/NaryVariable.cc
===================================================================
--- trunk/plearn/var/NaryVariable.cc	2008-02-25 21:15:01 UTC (rev 8571)
+++ trunk/plearn/var/NaryVariable.cc	2008-02-25 21:46:11 UTC (rev 8572)
@@ -52,35 +52,56 @@
     ""
 );
 
-NaryVariable::NaryVariable(const VarArray& the_varray, int thelength, int thewidth)
-    :Variable(thelength,thewidth), varray(the_varray) {}
+//////////////////
+// NaryVariable //
+//////////////////
+NaryVariable::NaryVariable(const VarArray& the_varray, int thelength,
+                           int thewidth, bool call_build_):
+    inherited(thelength, thewidth, call_build_),
+    varray(the_varray)
+{
+    if (call_build_)
+        build_();
+}
 
-
-
+////////////////////
+// declareOptions //
+////////////////////
 void NaryVariable::declareOptions(OptionList& ol)
 {
     declareOption(ol, "varray", &NaryVariable::varray, OptionBase::buildoption, 
-                  "The array of parent variables that this one depends on\n");
+                  "The array of parent variables that this one depends on.");
 
     inherited::declareOptions(ol);
 }
 
+///////////
+// build //
+///////////
+void NaryVariable::build()
+{
+    inherited::build();
+    build_();
+}
 
+////////////
+// build_ //
+////////////
+void NaryVariable::build_()
+{
+    // Nothing to do here.
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void NaryVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(varray, copies);
-    //for(int i=0; i<varray.size(); i++)
-    //  deepCopyField(varray[i], copies);
 }
 
 
-
-
-
-
-
-
 bool NaryVariable::markPath()
 {
     if(!marked)

Modified: trunk/plearn/var/NaryVariable.h
===================================================================
--- trunk/plearn/var/NaryVariable.h	2008-02-25 21:15:01 UTC (rev 8571)
+++ trunk/plearn/var/NaryVariable.h	2008-02-25 21:46:11 UTC (rev 8572)
@@ -69,11 +69,13 @@
     //#####  PLearn::Object Interface  ########################################
 
     NaryVariable() {}
-    NaryVariable(const VarArray& the_varray, int thelength, int thewidth=1);
+    NaryVariable(const VarArray& the_varray, int thelength, int thewidth = 1,
+                 bool call_build_ = true);
 
     PLEARN_DECLARE_ABSTRACT_OBJECT(NaryVariable);
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
+    virtual void build();
     
     //#####  PLearn::Variable Interface  ######################################
 
@@ -97,6 +99,10 @@
 protected:
     //!  Default constructor for persistence
     static void declareOptions(OptionList & ol);
+
+private:
+
+    void build_();
 };
 
 



From tihocan at mail.berlios.de  Mon Feb 25 22:48:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 25 Feb 2008 22:48:01 +0100
Subject: [Plearn-commits] r8573 - trunk/plearn/var
Message-ID: <200802252148.m1PLm1dq010392@sheep.berlios.de>

Author: tihocan
Date: 2008-02-25 22:48:01 +0100 (Mon, 25 Feb 2008)
New Revision: 8573

Modified:
   trunk/plearn/var/UnfoldedFuncVariable.cc
   trunk/plearn/var/UnfoldedFuncVariable.h
Log:
Fixed the build mechanism to be coherent with usual PLearn object building


Modified: trunk/plearn/var/UnfoldedFuncVariable.cc
===================================================================
--- trunk/plearn/var/UnfoldedFuncVariable.cc	2008-02-25 21:46:11 UTC (rev 8572)
+++ trunk/plearn/var/UnfoldedFuncVariable.cc	2008-02-25 21:48:01 UTC (rev 8573)
@@ -51,36 +51,60 @@
 
 /** UnfoldedFuncVariable **/
 
-PLEARN_IMPLEMENT_OBJECT(UnfoldedFuncVariable, "Variable that puts in the rows of its output matrix the value\n"
-                        "of a Func evaluated on each row of an input matrix.\n",
-                        "The input_matrix and output matrix have n_unfold rows. A separate propagation path\n"
-                        "is created that maps (using the Func as a template) each input row to each output row.\n"
-                        "The parents of this variable include the non-input parents of the Func.\n");
+PLEARN_IMPLEMENT_OBJECT(
+        UnfoldedFuncVariable,
+        "Computes the output of a Func over all elements on an input matrix.",
+        "By default the function 'f' is applied over all rows of the input\n"
+        "provided in 'input_matrix', but it may be applied on columns\n"
+        "instead using the 'transpose' option.\n"
+        "A separate propagation path is created (using the Func as a\n"
+        "template) that maps each input to the corresponding output.\n"
+        "The parents of this variable include the non-input parents of 'f'.");
 
-UnfoldedFuncVariable::UnfoldedFuncVariable()
-    : transpose(0)
+//////////////////////////
+// UnfoldedFuncVariable //
+//////////////////////////
+UnfoldedFuncVariable::UnfoldedFuncVariable():
+    transpose(false)
 {}
 
-UnfoldedFuncVariable::UnfoldedFuncVariable(Var inputmatrix, Func the_f, bool the_transpose)
-    : inherited(nonInputParentsOfPath(the_f->inputs,the_f->outputs) & inputmatrix, 
-                the_transpose ? the_f->outputs[0]->length()*the_f->outputs[0]->width() : inputmatrix->length(),
-                the_transpose ? inputmatrix->width() : the_f->outputs[0]->length()*the_f->outputs[0]->width()),
+UnfoldedFuncVariable::UnfoldedFuncVariable(
+        Var inputmatrix, Func the_f, bool the_transpose, bool call_build_):
+    inherited(VarArray(),
+            the_transpose ? the_f->outputs[0]->length()
+                                                * the_f->outputs[0]->width()
+                          : inputmatrix->length(),
+            the_transpose ? inputmatrix->width()
+                          : the_f->outputs[0]->length()
+                                                * the_f->outputs[0]->width(),
+            call_build_),
       input_matrix(inputmatrix), 
       f(the_f),
       transpose(the_transpose)
 {
-    build();
+    if (call_build_)
+        build_();
 }
 
+///////////
+// build //
+///////////
 void UnfoldedFuncVariable::build()
 {
     inherited::build();
     build_();
 }
 
+////////////
+// build_ //
+////////////
 void UnfoldedFuncVariable::build_()
 {
     if (f) {
+        VarArray f_parents = nonInputParentsOfPath(f->inputs, f->outputs);
+        varray.resize(f_parents.length() + 1);
+        varray << (f_parents & input_matrix);
+
         if(f->outputs.size()!=1)
             PLERROR("In UnfoldedFuncVariable: function must have a single variable output (maybe you can vconcat the vars into a single one prior to calling sumOf, if this is really what you want)");
         f->inputs.setDontBpropHere(true);
@@ -100,22 +124,40 @@
     }
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void UnfoldedFuncVariable::declareOptions(OptionList& ol)
 {
     declareOption(ol, "f", &UnfoldedFuncVariable::f, OptionBase::buildoption, 
                   "    Func that is replicated for each element of the 'bag' taken from the VMat.");
 
     declareOption(ol, "input_matrix", &UnfoldedFuncVariable::input_matrix, OptionBase::buildoption, 
-                  "    Var that contains the data, with multiple consecutive rows forming one bag.\n");
+        "Var containing the data: multiple consecutive rows form one bag.");
 
+    /*
+    declareOption(ol, "bag_size", &UnfoldedFuncVariable::bag_size,
+                  OptionBase::buildoption, 
+        "Optional Var that contains the size of the bag being presented.\n"
+        "If provided, then only the corresponding number of function values\n"
+        "will be computed, while the rest of the output data matrix will be\n"
+        "left untouched.");
+    */
+
     declareOption(ol, "transpose", &UnfoldedFuncVariable::transpose, OptionBase::buildoption, 
                   "    If set to 1, then instead puts in the columns of the output matrix the values\n"
                   "    of f at the columns of the input matrix.");
 
     inherited::declareOptions(ol);
+
+    redeclareOption(ol, "varray", &UnfoldedFuncVariable::varray,
+                    OptionBase::nosave,
+            "This option is set at build time.");
 }
 
-
+///////////////////
+// recomputeSize //
+///////////////////
 void UnfoldedFuncVariable::recomputeSize(int& l, int& w) const
 {
     if (f && f->outputs.size()) {
@@ -130,7 +172,9 @@
         l = w = 0;
 }
 
-
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void UnfoldedFuncVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -141,7 +185,9 @@
     deepCopyField(f_paths, copies);
 }
 
-
+///////////
+// fprop //
+///////////
 void UnfoldedFuncVariable::fprop()
 {
     int n_unfold = transpose ? input_matrix->width() : input_matrix->length();

Modified: trunk/plearn/var/UnfoldedFuncVariable.h
===================================================================
--- trunk/plearn/var/UnfoldedFuncVariable.h	2008-02-25 21:46:11 UTC (rev 8572)
+++ trunk/plearn/var/UnfoldedFuncVariable.h	2008-02-25 21:48:01 UTC (rev 8573)
@@ -64,10 +64,13 @@
     TVec<VarArray> f_paths; // the duplicates of f prop. path for each input/output pair: inputs[i]->outputs[i]
 
 public:
-    //!  protected default constructor for persistence
+    
+    //! Default constructor.
     UnfoldedFuncVariable();
-    //! concatenate_{i=0 to n_unfold} f(i-th row of input_matrix)
-    UnfoldedFuncVariable(Var inputmatrix, Func the_f, bool transpose);
+
+    //! Concatenate_{i=0 to n_unfold} f(i-th row of input_matrix).
+    UnfoldedFuncVariable(Var inputmatrix, Func the_f, bool transpose,
+                         bool call_build_ = true);
     
     PLEARN_DECLARE_OBJECT(UnfoldedFuncVariable);
     static void declareOptions(OptionList& ol);
@@ -81,7 +84,7 @@
     
     void printInfo(bool print_gradient);
 
-protected:
+private:
     void build_();
 };
 



From nouiz at mail.berlios.de  Mon Feb 25 23:38:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 25 Feb 2008 23:38:24 +0100
Subject: [Plearn-commits] r8574 - trunk/plearn/math
Message-ID: <200802252238.m1PMcOgb015023@sheep.berlios.de>

Author: nouiz
Date: 2008-02-25 23:38:23 +0100 (Mon, 25 Feb 2008)
New Revision: 8574

Modified:
   trunk/plearn/math/pl_erf.cc
   trunk/plearn/math/pl_erf.h
Log:
Added function fast_gauss_01_quantile() that use precalculated data in a table.


Modified: trunk/plearn/math/pl_erf.cc
===================================================================
--- trunk/plearn/math/pl_erf.cc	2008-02-25 21:48:01 UTC (rev 8573)
+++ trunk/plearn/math/pl_erf.cc	2008-02-25 22:38:23 UTC (rev 8574)
@@ -34,6 +34,7 @@
 // library, go to the PLearn Web site at www.plearn.org
 
 #include <plearn/base/general.h>
+#include <plearn/math/pl_erf.h>
 // #include <iostream>
 #include <cmath>
 
@@ -172,6 +173,12 @@
 // i.e. approximately gauss_01_quantile(gauss_01_cum(x)) ~=~ x
 // (the inverse is computed with a binary search, the bisection method)
 real gauss_01_quantile(real q) {
+#ifdef BOUNDCHECK
+    if(q<0||q>1)
+        PLERROR("gauss_01_quantile(q=%f) - q is less then 0 or more then 1",q);
+    PLASSERT(!is_missing(q));
+#endif
+
     // first find a reasonable interval (a,b) s.t. cum(a)<q<cum(b)
     real a=-2;
     real b=2;
@@ -232,6 +239,38 @@
 }
 
 
+float gaussQuantiletable[GAUSSQUANTILETABLESIZE];
+
+PLGaussQuantileInitializer::PLGaussQuantileInitializer()
+{
+    //! Fill the table
+    real scaling = 1./(GAUSSQUANTILETABLESIZE-1);
+    for(int i=0; i<GAUSSQUANTILETABLESIZE; i++)
+        gaussQuantiletable[i] = (float) gauss_01_quantile(i*scaling);
+}
+
+PLGaussQuantileInitializer::~PLGaussQuantileInitializer() {}
+
+PLGaussQuantileInitializer pl_gauss_quantile_initializer;
+
+real fast_gauss_01_quantile(real q)
+{
+#ifdef BOUNDCHECK
+    if(q<0||q>1)
+        PLERROR("fast_gauss_01_quantile(q=%f) - "
+                "q is less then 0 or more then 1",q);
+    PLASSERT(!is_missing(q));
+#endif
+
+    if(q>GAUSSQUANTILETABLESIZE*0.001&&q<GAUSSQUANTILETABLESIZE*0.999)
+    {
+        int i;
+        DOUBLE_TO_INT( double(q*((GAUSSQUANTILETABLESIZE-1))), i);
+        return real(gaussQuantiletable[i]);
+    }
+    else
+        return gauss_01_quantile(q);
+}
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/math/pl_erf.h
===================================================================
--- trunk/plearn/math/pl_erf.h	2008-02-25 21:48:01 UTC (rev 8573)
+++ trunk/plearn/math/pl_erf.h	2008-02-25 22:38:23 UTC (rev 8574)
@@ -96,7 +96,24 @@
 // the number of observations.
 real p_value(real mu, real vn);
 
+#define GAUSSQUANTILETABLESIZE 10000
 
+class PLGaussQuantileInitializer
+{
+public:
+    PLGaussQuantileInitializer();
+    ~PLGaussQuantileInitializer();
+};
+
+extern float gaussQuantiletable[GAUSSQUANTILETABLESIZE];
+extern PLGaussQuantileInitializer pl_gauss_quantile_initializer;
+
+//! Use precomputed value in a table of size GAUSSQUANTILETABLESIZE.
+//! If value is in the first or last 1/1000 we return
+//! the result of the non-fast version as the quality is too low.
+real fast_gauss_01_quantile(real x);
+
+
 } // end of namespace PLearn
 
 #endif



From nouiz at mail.berlios.de  Mon Feb 25 23:52:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 25 Feb 2008 23:52:59 +0100
Subject: [Plearn-commits] r8575 - trunk/plearn/misc
Message-ID: <200802252252.m1PMqxen016764@sheep.berlios.de>

Author: nouiz
Date: 2008-02-25 23:52:59 +0100 (Mon, 25 Feb 2008)
New Revision: 8575

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
load the matrix after checking the parameter. So if the parameter are bad we know it earlier


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-02-25 22:38:23 UTC (rev 8574)
+++ trunk/plearn/misc/vmatmain.cc	2008-02-25 22:52:59 UTC (rev 8575)
@@ -550,8 +550,6 @@
             PLERROR("Usage: vmat convert <source> <destination> "
                     "[--mat_to_mem] [--cols=col1,col2,col3,...]");
 
-        VMat vm = getVMat(source, indexf);
-
         /**
          * Interpret the following options:
          *
@@ -603,6 +601,8 @@
                           curopt.c_str());
         }
 
+        VMat vm = getVMat(source, indexf);
+
         // If columns specified, select them.  Note: SelectColumnsVMatrix is very
         // powerful and allows ranges, etc.
         if (columns.size() > 0)



From tihocan at mail.berlios.de  Tue Feb 26 17:11:54 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 17:11:54 +0100
Subject: [Plearn-commits] r8576 - in trunk/plearn/var/test: .
	.pytest/PL_Variables/expected_results
Message-ID: <200802261611.m1QGBsjK020014@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 17:11:54 +0100 (Tue, 26 Feb 2008)
New Revision: 8576

Modified:
   trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave
   trunk/plearn/var/test/VariablesTest.cc
   trunk/plearn/var/test/VariablesTest.h
Log:
Added a test of LogAddVariable

Modified: trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave
===================================================================
--- trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave	2008-02-25 22:52:59 UTC (rev 8575)
+++ trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave	2008-02-26 16:11:54 UTC (rev 8576)
@@ -5,5 +5,11 @@
 4 	
 ]
 ;
+logadd_binary = 3  5  [ 
+0.689299342300172868 	0.615366813089032516 	0.989610623621780094 	0.583460791819662261 	1.29948220789154911 	
+0.742818586786261004 	0.857531198358037261 	1.03236649219508791 	1.25393020526635834 	1.13190098479682311 	
+0.382235930293690018 	0.808491704438245362 	1.16244002798912671 	0.396847673154189673 	0.944310390523003362 	
+]
+;
 save = 1 ;
 save_path = ""  )

Modified: trunk/plearn/var/test/VariablesTest.cc
===================================================================
--- trunk/plearn/var/test/VariablesTest.cc	2008-02-25 22:52:59 UTC (rev 8575)
+++ trunk/plearn/var/test/VariablesTest.cc	2008-02-26 16:11:54 UTC (rev 8576)
@@ -41,6 +41,7 @@
 #include <plearn/math/PRandom.h>
 #include <plearn/var/ArgminVariable.h>
 #include <plearn/var/Func.h>
+#include <plearn/var/LogAddVariable.h>
 #include <plearn/var/UnfoldedFuncVariable.h>
 
 namespace PLearn {
@@ -102,6 +103,10 @@
                    OptionBase::buildoption,
         "Result of the unfolded argmin computation.");
 
+    declareOption(ol, "logadd_binary", &VariablesTest::logadd_binary,
+                   OptionBase::buildoption,
+        "Result of the logadd on two variables.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -128,20 +133,27 @@
 /////////////
 void VariablesTest::perform()
 {
-    // Test UnfoldedFuncVariable to compute an argmin over a set of row vectors.
+    // Declare and initialize variables used in tests.
     int is1 = 5;
     int n_input2 = 3;
     Var input1(1, is1, "input1");
+    Var input2(n_input2, is1, "input2");
+    PRandom::common(false)->fill_random_uniform(input2->matValue, -1, 1);
+    Var input3(input2->length(), input2->width(), "input3");
+    PRandom::common(false)->fill_random_uniform(input3->matValue, -1, 1);
+
+    // Test UnfoldedFuncVariable to compute an argmin over a set of row vectors.
     Var argmin1 = argmin(input1);
     Func input1_to_argmin1(input1, argmin1);
-    Var input2(n_input2, is1, "input2");
-    PRandom::common(false)->fill_random_uniform(input2->matValue, -1, 1);
-    //pout << input2->matValue << endl;
     Var unfolded_argmin1 = new UnfoldedFuncVariable(input2, input1_to_argmin1,
                                                     false);
     unfolded_argmin1->fprop();
     unfolded_argmin = unfolded_argmin1->matValue.copy();
-    //pout << unfolded_argmin << endl;
+    
+    // Test LogAddVariable to compute a logadd over two input matrices.
+    Var logadd1 = logadd(input2, input3);
+    logadd1->fprop();
+    logadd_binary = logadd1->matValue.copy();
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/var/test/VariablesTest.h
===================================================================
--- trunk/plearn/var/test/VariablesTest.h	2008-02-25 22:52:59 UTC (rev 8575)
+++ trunk/plearn/var/test/VariablesTest.h	2008-02-26 16:11:54 UTC (rev 8576)
@@ -61,6 +61,7 @@
 public:
     //#####  Public Build Options  ############################################
 
+    Mat logadd_binary;
     Mat unfolded_argmin;
 
 public:



From tihocan at mail.berlios.de  Tue Feb 26 17:44:21 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 17:44:21 +0100
Subject: [Plearn-commits] r8577 - trunk/plearn/var
Message-ID: <200802261644.m1QGiLYh026580@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 17:44:21 +0100 (Tue, 26 Feb 2008)
New Revision: 8577

Modified:
   trunk/plearn/var/Func.cc
Log:
Cosmetic changes

Modified: trunk/plearn/var/Func.cc
===================================================================
--- trunk/plearn/var/Func.cc	2008-02-26 16:11:54 UTC (rev 8576)
+++ trunk/plearn/var/Func.cc	2008-02-26 16:44:21 UTC (rev 8577)
@@ -123,8 +123,15 @@
   }
 */
 
-PLEARN_IMPLEMENT_OBJECT(Function, "Implements a function defined as a var graph", "NO HELP");
+PLEARN_IMPLEMENT_OBJECT(
+        Function,
+        "Implements a function defined as a graph of Variables.",
+        ""
+);
 
+////////////////////
+// declareOptions //
+////////////////////
 void Function::declareOptions(OptionList& ol)
 {
     declareOption(ol, "inputs", &Function::inputs, OptionBase::buildoption,
@@ -138,6 +145,9 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void Function::build_()
 {
     if(parameters.isEmpty())



From tihocan at mail.berlios.de  Tue Feb 26 17:45:01 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 17:45:01 +0100
Subject: [Plearn-commits] r8578 - trunk/plearn/var
Message-ID: <200802261645.m1QGj1Tk026712@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 17:45:00 +0100 (Tue, 26 Feb 2008)
New Revision: 8578

Modified:
   trunk/plearn/var/BinaryVariable.cc
Log:
Cosmetic changes

Modified: trunk/plearn/var/BinaryVariable.cc
===================================================================
--- trunk/plearn/var/BinaryVariable.cc	2008-02-26 16:44:21 UTC (rev 8577)
+++ trunk/plearn/var/BinaryVariable.cc	2008-02-26 16:45:00 UTC (rev 8578)
@@ -95,6 +95,9 @@
 ////////////
 void BinaryVariable::build_() {}
 
+////////////////
+// setParents //
+////////////////
 void BinaryVariable::setParents(const VarArray& parents)
 {
     if(parents.length() != 2)
@@ -116,16 +119,19 @@
 #pragma warning(default:1419)
 #endif
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void BinaryVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    //deepCopyField(input1, copies);
     varDeepCopyField(input1, copies);
-    //deepCopyField(input2, copies);
     varDeepCopyField(input2, copies);
 }
 
-
+//////////////
+// markPath //
+//////////////
 bool BinaryVariable::markPath()
 {
     if(!marked)



From tihocan at mail.berlios.de  Tue Feb 26 17:47:20 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 17:47:20 +0100
Subject: [Plearn-commits] r8579 - trunk/plearn/var
Message-ID: <200802261647.m1QGlKuH026962@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 17:47:20 +0100 (Tue, 26 Feb 2008)
New Revision: 8579

Modified:
   trunk/plearn/var/LogAddVariable.cc
   trunk/plearn/var/LogAddVariable.h
Log:
- Implemented proper Object methods (build, deep copy, options, etc.)
- Added a new option to perform a logadd over the rows or columns of the first input, possibly controlling the range with the second input


Modified: trunk/plearn/var/LogAddVariable.cc
===================================================================
--- trunk/plearn/var/LogAddVariable.cc	2008-02-26 16:45:00 UTC (rev 8578)
+++ trunk/plearn/var/LogAddVariable.cc	2008-02-26 16:47:20 UTC (rev 8579)
@@ -50,75 +50,203 @@
 namespace PLearn {
 using namespace std;
 
-
-
-/** LogAddVariable **/
-
-
 PLEARN_IMPLEMENT_OBJECT(
         LogAddVariable,
         "Stable computation of log(exp(input1) + exp(input2)).",
-        "");
+        "This Variable may be used:\n"
+        "   - with two inputs of the same sizes, to compute an element-wize\n"
+        "     logadd over both input matrices, or\n"
+        "   - with one matrix input (input1) and one scalar input (input2)\n"
+        "     to compute a vector logadd (i.e. log(exp(a1) + ... + exp(an)))\n"
+        "     over the first 'n' rows (or columns, depending on the option\n"
+        "     'vector_logadd'), where 'n' is an integer value provided by\n"
+        "     the input2 Variable. If input2 is not provided, then the\n"
+        "     logadd is performed over all rows/columns of input1.\n"
+        "Note that in order to use the second mechanism, one must change the\n"
+        "value of the 'vector_logadd' option, to remove any ambiguity, e.g\n"
+        "in the case of two scalar inputs."
+);
 
-LogAddVariable::LogAddVariable(Variable* input1, Variable* input2)
-    : inherited(input1, input2, input1->length(), input1->width())
+////////////////////
+// LogAddVariable //
+////////////////////
+LogAddVariable::LogAddVariable(Variable* input1, Variable* input2,
+                               const string& vl,
+                               bool call_build_):
+    inherited(input1, input2,
+              vl == "none" || vl == "per_row" ? input1->length()
+                                              : 1,
+              vl == "none" || vl == "per_column" ? input1->width()
+                                                 : 1,
+              call_build_),
+    vector_logadd(vl),
+    vector_logadd_id(0)
 {
-    build_();
+    if (call_build_)
+        build_();
 }
 
-void
-LogAddVariable::build()
+////////////////////
+// declareOptions //
+////////////////////
+void LogAddVariable::declareOptions(OptionList& ol)
 {
+    declareOption(ol, "vector_logadd", &LogAddVariable::vector_logadd,
+                  OptionBase::buildoption, 
+        "Must be one of:\n"
+        "   - 'none'      : element-wize logadd over the two input matrices\n"
+        "   - 'per_column': vector logadd on each column of input1, using\n"
+        "                   the first 'n' rows as given by input2\n"
+        "   - 'per_row'   : vector logadd on each row of input1, using the\n"
+        "                   first 'n' columns as given by input2.");
+
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void LogAddVariable::build()
+{
     inherited::build();
     build_();
 }
 
-void
-LogAddVariable::build_()
+////////////
+// build_ //
+////////////
+void LogAddVariable::build_()
 {
-    if (input1 && input2) {
-        if (input1->length() != input2->length()  ||  input1->width() != input2->width())
-            PLERROR("PLogPVariable LogAddVariable input1 and input2 must have the same size");
+    // Transform the string 'vector_logadd' into an integer for faster
+    // computations.
+    if (vector_logadd == "none")
+        vector_logadd_id = 0;
+    else if (vector_logadd == "per_row")
+        vector_logadd_id = 1;
+    else if (vector_logadd == "per_column")
+        vector_logadd_id = -1;
+    else
+        PLERROR("In LogAddVariable::build_ - Invalid value for "
+                "'vector_logadd': %s", vector_logadd.c_str());
+    
+    if (!vector_logadd_id && input1 && input2) {
+        if (input1->length() != input2->length() ||
+            input1->width() != input2->width())
+            PLERROR("In LogAddVariable::build_ - input1 and input2 must "
+                    "have the same size");
     }
 }
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void LogAddVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(work,     copies);
+    deepCopyField(work_ptr, copies);
+}
+///////////////////
+// recomputeSize //
+///////////////////
 void LogAddVariable::recomputeSize(int& l, int& w) const
 {
     if (input1) {
-        l = input1->length();
-        w = input1->width();
+        l = vector_logadd_id >= 0 ? input1->length()
+                                  : 1;
+        w = vector_logadd_id <= 0 ? input1->width()
+                                  : 1;
     } else
         l = w = 0;
 }
 
+///////////
+// fprop //
+///////////
 void LogAddVariable::fprop()
 {
-    // Ugly hack to make it compile with ICC.
+    if (!vector_logadd_id) {
+        // Ugly hack to make it compile with ICC.
 #ifdef __INTEL_COMPILER
-    PLearn::apply(input1->value,input2->value,value, logadd_for_icc);
+        PLearn::apply(input1->value, input2->value, value, logadd_for_icc);
 #else
-    PLearn::apply(input1->value,input2->value,value, logadd);
+        PLearn::apply(input1->value, input2->value, value, logadd);
 #endif
+    } else if (vector_logadd_id > 0) {
+        int n = input2 ? int(round(input2->value[0]))
+                       : width();
+        for (int i = 0; i < length(); i++) {
+            work_ptr = input1->matValue(i);
+            if (input2)
+                work_ptr = work_ptr.subVec(0, n);
+            value[i] = logadd(work_ptr);
+        }
+    } else {
+        int n = input2 ? int(round(input2->value[0]))
+                       : length();
+        work.resize(n);
+        for (int i = 0; i < width(); i++) {
+            if (input2)
+                work << input1->matValue.subMat(0, i, n, 1);
+            else
+                work << input1->matValue.column(i);
+            value[i] = logadd(work);
+        }
+    }
 }
 
+///////////
+// bprop //
+///////////
 void LogAddVariable::bprop()
 {
-    Vec grad1(nelems());
-    grad1 = input1->value - value;
-    apply(grad1, grad1, safeexp);
-    input1->gradient += grad1%gradient;
-    // TODO Note that the '%' operator is inefficient.
+    if (!vector_logadd_id) {
+        // TODO Note that these computations are not efficient at all.
+        Vec grad1(nelems());
+        grad1 = input1->value - value;
+        apply(grad1, grad1, safeexp);
+        input1->gradient += grad1%gradient;
 
-    Vec grad2(nelems());
-    grad2 = input2->value - value;
-    apply(grad2, grad2, safeexp);
-    input2->gradient += grad2%gradient;
+        Vec grad2(nelems());
+        grad2 = input2->value - value;
+        apply(grad2, grad2, safeexp);
+        input2->gradient += grad2%gradient;
+    } else if (vector_logadd_id > 0) {
+        int n = input2 ? int(round(input2->value[0]))
+                       : width();
+        work.resize(n);
+        for (int i = 0; i < length(); i++) {
+            work << input1->matValue.subMat(i, 0, 1, n);
+            work -= value[i];
+            apply(work, work, safeexp);
+            multiplyAcc(input1->matGradient.subMat(i, 0, 1, n).toVec(),
+                        work, gradient[i]);
+        }
+    } else {
+        int n = input2 ? int(round(input2->value[0]))
+                       : length();
+        work.resize(n);
+        for (int i = 0; i < width(); i++) {
+            work << input1->matValue.subMat(0, i, n, 1);
+            work -= value[i];
+            apply(work, work, safeexp);
+            work *= gradient[i];
+            input1->matGradient.subMat(0, i, n, 1) += work;
+        }
+    }
 }
 
+///////////////////
+// symbolicBprop //
+///////////////////
 void LogAddVariable::symbolicBprop()
 {
-    input1->accg(g * (exp(input1)/(exp(input1)+exp(input2))));
-    input2->accg(g * (exp(input2)/(exp(input1)+exp(input2))));
+    if (!vector_logadd_id) {
+        input1->accg(g * (exp(input1)/(exp(input1)+exp(input2))));
+        input2->accg(g * (exp(input2)/(exp(input1)+exp(input2))));
+    } else {
+        PLERROR("In LogAddVariable::symbolicBprop - Not implemented");
+    }
 }
 
 

Modified: trunk/plearn/var/LogAddVariable.h
===================================================================
--- trunk/plearn/var/LogAddVariable.h	2008-02-26 16:45:00 UTC (rev 8578)
+++ trunk/plearn/var/LogAddVariable.h	2008-02-26 16:47:20 UTC (rev 8579)
@@ -62,20 +62,46 @@
     typedef BinaryVariable inherited;
 
 public:
-    //!  Default constructor for persistence
+
+    string vector_logadd;
+
+    //! Default constructor.
     LogAddVariable() {}
-    LogAddVariable(Variable* input1, Variable* input2);
 
+    //! Convenience constructor.
+    LogAddVariable(Variable* input1, Variable* input2,
+                   const string& the_vector_logadd = "none",
+                   bool call_build_ = true);
+
     PLEARN_DECLARE_OBJECT(LogAddVariable);
 
     virtual void build();
 
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
     virtual void recomputeSize(int& l, int& w) const;  
     virtual void fprop();
     virtual void bprop();
     virtual void symbolicBprop();
 
 protected:
+
+    //! Integer coding for 'vector_logadd':
+    //!     0 <-> 'none'
+    //!    -1 <-> 'per_column'
+    //!    +1 <-> 'per_row'
+    int vector_logadd_id;
+
+    //! Temporary work vector.
+    Vec work;
+
+    //! Temporary work vector whose content must not be modified: it can only
+    //! be used to point to other data in memory.
+    Vec work_ptr;
+
+    static void declareOptions(OptionList & ol);
+
+private:
     void build_();
 };
 



From tihocan at mail.berlios.de  Tue Feb 26 17:48:41 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 17:48:41 +0100
Subject: [Plearn-commits] r8580 - trunk/plearn/var
Message-ID: <200802261648.m1QGmf65027312@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 17:48:40 +0100 (Tue, 26 Feb 2008)
New Revision: 8580

Modified:
   trunk/plearn/var/UnfoldedFuncVariable.cc
   trunk/plearn/var/UnfoldedFuncVariable.h
Log:
Added a new parent Variable, 'bag_size', that can control the range over which the function is applied

Modified: trunk/plearn/var/UnfoldedFuncVariable.cc
===================================================================
--- trunk/plearn/var/UnfoldedFuncVariable.cc	2008-02-26 16:47:20 UTC (rev 8579)
+++ trunk/plearn/var/UnfoldedFuncVariable.cc	2008-02-26 16:48:40 UTC (rev 8580)
@@ -69,7 +69,8 @@
 {}
 
 UnfoldedFuncVariable::UnfoldedFuncVariable(
-        Var inputmatrix, Func the_f, bool the_transpose, bool call_build_):
+        Var inputmatrix, Func the_f, bool the_transpose,
+        Var bagsize, bool call_build_):
     inherited(VarArray(),
             the_transpose ? the_f->outputs[0]->length()
                                                 * the_f->outputs[0]->width()
@@ -79,6 +80,7 @@
                                                 * the_f->outputs[0]->width(),
             call_build_),
       input_matrix(inputmatrix), 
+      bag_size(bagsize),
       f(the_f),
       transpose(the_transpose)
 {
@@ -102,8 +104,8 @@
 {
     if (f) {
         VarArray f_parents = nonInputParentsOfPath(f->inputs, f->outputs);
-        varray.resize(f_parents.length() + 1);
-        varray << (f_parents & input_matrix);
+        varray.resize(f_parents.length() + 2);
+        varray << (f_parents & input_matrix & bag_size);
 
         if(f->outputs.size()!=1)
             PLERROR("In UnfoldedFuncVariable: function must have a single variable output (maybe you can vconcat the vars into a single one prior to calling sumOf, if this is really what you want)");
@@ -121,7 +123,11 @@
             outputs[i] = f(inputs[i])[0];
             f_paths[i] = propagationPath(inputs[i],outputs[i]);
         }
+        inherited::build(); // Re-build since varray has changed.
     }
+
+    if (bag_size)
+        PLASSERT( bag_size->isScalar() );
 }
 
 ////////////////////
@@ -135,14 +141,12 @@
     declareOption(ol, "input_matrix", &UnfoldedFuncVariable::input_matrix, OptionBase::buildoption, 
         "Var containing the data: multiple consecutive rows form one bag.");
 
-    /*
     declareOption(ol, "bag_size", &UnfoldedFuncVariable::bag_size,
                   OptionBase::buildoption, 
         "Optional Var that contains the size of the bag being presented.\n"
         "If provided, then only the corresponding number of function values\n"
         "will be computed, while the rest of the output data matrix will be\n"
         "left untouched.");
-    */
 
     declareOption(ol, "transpose", &UnfoldedFuncVariable::transpose, OptionBase::buildoption, 
                   "    If set to 1, then instead puts in the columns of the output matrix the values\n"
@@ -152,7 +156,7 @@
 
     redeclareOption(ol, "varray", &UnfoldedFuncVariable::varray,
                     OptionBase::nosave,
-            "This option is set at build time.");
+            "This option is set at build time from other options.");
 }
 
 ///////////////////
@@ -160,7 +164,7 @@
 ///////////////////
 void UnfoldedFuncVariable::recomputeSize(int& l, int& w) const
 {
-    if (f && f->outputs.size()) {
+    if (f && f->outputs.size() > 0) {
         w = f->outputs[0]->length()*f->outputs[0]->width();
         if (transpose) {
             l = w;
@@ -179,10 +183,11 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(input_matrix, copies);
-    deepCopyField(f, copies);
-    deepCopyField(inputs, copies);
-    deepCopyField(outputs, copies);
-    deepCopyField(f_paths, copies);
+    deepCopyField(bag_size,     copies);
+    deepCopyField(f,            copies);
+    deepCopyField(inputs,       copies);
+    deepCopyField(outputs,      copies);
+    deepCopyField(f_paths,      copies);
 }
 
 ///////////
@@ -190,7 +195,11 @@
 ///////////
 void UnfoldedFuncVariable::fprop()
 {
-    int n_unfold = transpose ? input_matrix->width() : input_matrix->length();
+    int n_unfold = bag_size ? int(round(bag_size->value[0]))
+                            : transpose ? input_matrix->width()
+                                        : input_matrix->length();
+    PLASSERT( !bag_size || is_equal(bag_size->value[0],
+                                    round(bag_size->value[0])) );
     for (int i=0;i<n_unfold;i++) {
         if (transpose) {
             Vec tmp = input_matrix->matValue.column(i).toVecCopy(); // TODO something more efficient
@@ -207,10 +216,14 @@
     }
 }
 
-
+///////////
+// bprop //
+///////////
 void UnfoldedFuncVariable::bprop()
 { 
-    int n_unfold = transpose ? input_matrix->width() : input_matrix->length();
+    int n_unfold = bag_size ? int(round(bag_size->value[0]))
+                            : transpose ? input_matrix->width()
+                                        : input_matrix->length();
     for (int i=0;i<n_unfold;i++)
     {
         f_paths[i].clearGradient();
@@ -224,21 +237,26 @@
     }
 }
 
-
+///////////////
+// printInfo //
+///////////////
 void UnfoldedFuncVariable::printInfo(bool print_gradient)
 {
-    int n_unfold = transpose ? input_matrix->width() : input_matrix->length();
+    int n_unfold = bag_size ? int(round(bag_size->value[0]))
+                            : transpose ? input_matrix->width()
+                                        : input_matrix->length();
     for (int i=0;i<n_unfold;i++)
         f_paths[i].printInfo(print_gradient);
-    cout << info() << " : " << getName() << "[" << (void*)this << "]" 
+    pout << info() << " : " << getName() << "[" << (void*)this << "]" 
          << "(input_matrix=" << (void*)input_matrix << " ";
-    for(int i=0; i<n_unfold; i++) cout << (void*)outputs[i] << " ";
-    cout << ") = " << value;
-    if (print_gradient) cout << " gradient=" << gradient;
-    cout << endl; 
+    for(int i=0; i<n_unfold; i++)
+        pout << (void*)outputs[i] << " ";
+    pout << ") = " << value;
+    if (print_gradient)
+        pout << " gradient=" << gradient;
+    pout << endl; 
 }
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/var/UnfoldedFuncVariable.h
===================================================================
--- trunk/plearn/var/UnfoldedFuncVariable.h	2008-02-26 16:47:20 UTC (rev 8579)
+++ trunk/plearn/var/UnfoldedFuncVariable.h	2008-02-26 16:48:40 UTC (rev 8580)
@@ -56,6 +56,7 @@
 public:
 //protected:
     Var input_matrix;
+    Var bag_size;
     Func f;
     bool transpose;
 
@@ -70,6 +71,7 @@
 
     //! Concatenate_{i=0 to n_unfold} f(i-th row of input_matrix).
     UnfoldedFuncVariable(Var inputmatrix, Func the_f, bool transpose,
+                         Var bagsize = (Variable*) NULL,
                          bool call_build_ = true);
     
     PLEARN_DECLARE_OBJECT(UnfoldedFuncVariable);



From nouiz at mail.berlios.de  Tue Feb 26 18:03:25 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Feb 2008 18:03:25 +0100
Subject: [Plearn-commits] r8581 - trunk/plearn/math
Message-ID: <200802261703.m1QH3PLD029671@sheep.berlios.de>

Author: nouiz
Date: 2008-02-26 18:03:24 +0100 (Tue, 26 Feb 2008)
New Revision: 8581

Modified:
   trunk/plearn/math/pl_erf.cc
Log:
bugfix, fast_gauss_01_quantile was always executing the slow version


Modified: trunk/plearn/math/pl_erf.cc
===================================================================
--- trunk/plearn/math/pl_erf.cc	2008-02-26 16:48:40 UTC (rev 8580)
+++ trunk/plearn/math/pl_erf.cc	2008-02-26 17:03:24 UTC (rev 8581)
@@ -262,7 +262,7 @@
     PLASSERT(!is_missing(q));
 #endif
 
-    if(q>GAUSSQUANTILETABLESIZE*0.001&&q<GAUSSQUANTILETABLESIZE*0.999)
+    if(q>0.005&&q<0.995)
     {
         int i;
         DOUBLE_TO_INT( double(q*((GAUSSQUANTILETABLESIZE-1))), i);



From nouiz at mail.berlios.de  Tue Feb 26 18:08:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Feb 2008 18:08:22 +0100
Subject: [Plearn-commits] r8582 - trunk/plearn/vmat
Message-ID: <200802261708.m1QH8MMG030276@sheep.berlios.de>

Author: nouiz
Date: 2008-02-26 18:08:21 +0100 (Tue, 26 Feb 2008)
New Revision: 8582

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
use the fast version of gauss_01_quantile in GaussianizeVMatrix as precission is not too much important


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-26 17:03:24 UTC (rev 8581)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-26 17:08:21 UTC (rev 8582)
@@ -272,7 +272,7 @@
         // inverse of the Gaussian cumulative function.
         real n = values_j.length();
         interpol = (n - 1) / (n + 1) * interpol + 1 / (n + 1);
-        v[j] = gauss_01_quantile(interpol);
+        v[j] = fast_gauss_01_quantile(interpol);
     }
 }
 



From nouiz at mail.berlios.de  Tue Feb 26 19:22:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 26 Feb 2008 19:22:47 +0100
Subject: [Plearn-commits] r8583 - trunk/plearn/math
Message-ID: <200802261822.m1QIMlRM000093@sheep.berlios.de>

Author: nouiz
Date: 2008-02-26 19:22:39 +0100 (Tue, 26 Feb 2008)
New Revision: 8583

Modified:
   trunk/plearn/math/pl_erf.cc
Log:
optimisation


Modified: trunk/plearn/math/pl_erf.cc
===================================================================
--- trunk/plearn/math/pl_erf.cc	2008-02-26 17:08:21 UTC (rev 8582)
+++ trunk/plearn/math/pl_erf.cc	2008-02-26 18:22:39 UTC (rev 8583)
@@ -166,7 +166,7 @@
 //returns the gaussian cumulative function
 // For X ~ Normal(0,1), cumulative probability function P(X<x)
 real gauss_01_cum(real x) {
-    return 0.5*(1+pl_erf(x/1.414214));
+    return 0.5*(1+pl_erf(x*0.707106781187));
 }
 
 // For X ~ Normal(0,1), inverse of cumulative probability function P(X<x)



From tihocan at mail.berlios.de  Tue Feb 26 20:32:06 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 20:32:06 +0100
Subject: [Plearn-commits] r8584 - in trunk/plearn/var/test: .
	.pytest/PL_Variables/expected_results
Message-ID: <200802261932.m1QJW63L015743@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 20:32:06 +0100 (Tue, 26 Feb 2008)
New Revision: 8584

Modified:
   trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave
   trunk/plearn/var/test/VariablesTest.cc
   trunk/plearn/var/test/VariablesTest.h
Log:
Added more tests of the new LogAddVariable functionalities

Modified: trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave
===================================================================
--- trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave	2008-02-26 18:22:39 UTC (rev 8583)
+++ trunk/plearn/var/test/.pytest/PL_Variables/expected_results/VariablesTest.psave	2008-02-26 19:32:06 UTC (rev 8584)
@@ -1,15 +1,23 @@
 *1 ->VariablesTest(
-unfolded_argmin = 3  1  [ 
+results_mat = {"logadd_binary" : 3  5  [ 
+0.689299342300172868 	0.615366813089032516 	0.989610623621780094 	0.583460791819662261 	1.29948220789154911 	
+0.742818586786261004 	0.857531198358037261 	1.03236649219508791 	1.25393020526635834 	1.13190098479682311 	
+0.382235930293690018 	0.808491704438245362 	1.16244002798912671 	0.396847673154189673 	0.944310390523003362 	
+]
+, "logadd_per_column" : 1  5  [ 
+1.07497258332875179 	1.66872910693274878 	0.887466306383460801 	1.16105977069747945 	1.39386381390672609 	
+]
+, "logadd_per_row" : 4  1  [ 
+1.65582747027099719 	
+0.638016599439115151 	
+1.26247734209561657 	
+1.07749345732512336 	
+]
+, "unfolded_argmin" : 3  1  [ 
 3 	
 2 	
 4 	
 ]
-;
-logadd_binary = 3  5  [ 
-0.689299342300172868 	0.615366813089032516 	0.989610623621780094 	0.583460791819662261 	1.29948220789154911 	
-0.742818586786261004 	0.857531198358037261 	1.03236649219508791 	1.25393020526635834 	1.13190098479682311 	
-0.382235930293690018 	0.808491704438245362 	1.16244002798912671 	0.396847673154189673 	0.944310390523003362 	
-]
-;
+};
 save = 1 ;
 save_path = ""  )

Modified: trunk/plearn/var/test/VariablesTest.cc
===================================================================
--- trunk/plearn/var/test/VariablesTest.cc	2008-02-26 18:22:39 UTC (rev 8583)
+++ trunk/plearn/var/test/VariablesTest.cc	2008-02-26 19:32:06 UTC (rev 8584)
@@ -99,14 +99,10 @@
     // ### You can also combine flags, for example with OptionBase::nosave:
     // ### (OptionBase::buildoption | OptionBase::nosave)
 
-    declareOption(ol, "unfolded_argmin", &VariablesTest::unfolded_argmin,
+    declareOption(ol, "results_mat", &VariablesTest::results_mat,
                    OptionBase::buildoption,
-        "Result of the unfolded argmin computation.");
+        "Matrix test results.");
 
-    declareOption(ol, "logadd_binary", &VariablesTest::logadd_binary,
-                   OptionBase::buildoption,
-        "Result of the logadd on two variables.");
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -136,11 +132,16 @@
     // Declare and initialize variables used in tests.
     int is1 = 5;
     int n_input2 = 3;
+    int n_input5 = 4;
     Var input1(1, is1, "input1");
     Var input2(n_input2, is1, "input2");
     PRandom::common(false)->fill_random_uniform(input2->matValue, -1, 1);
     Var input3(input2->length(), input2->width(), "input3");
     PRandom::common(false)->fill_random_uniform(input3->matValue, -1, 1);
+    Var input4(1, 1, "input4");
+    input4->matValue(0, 0) = 3;
+    Var input5(n_input5, is1, "input5");
+    PRandom::common(false)->fill_random_uniform(input5->matValue, -1, 1);
 
     // Test UnfoldedFuncVariable to compute an argmin over a set of row vectors.
     Var argmin1 = argmin(input1);
@@ -148,12 +149,24 @@
     Var unfolded_argmin1 = new UnfoldedFuncVariable(input2, input1_to_argmin1,
                                                     false);
     unfolded_argmin1->fprop();
-    unfolded_argmin = unfolded_argmin1->matValue.copy();
+    results_mat["unfolded_argmin"] = unfolded_argmin1->matValue.copy();
     
     // Test LogAddVariable to compute a logadd over two input matrices.
     Var logadd1 = logadd(input2, input3);
     logadd1->fprop();
-    logadd_binary = logadd1->matValue.copy();
+    results_mat["logadd_binary"] = logadd1->matValue.copy();
+
+    // Test LogAddVariable to compute a logadd over sub-columns of its first
+    // input.
+    Var logadd2 = new LogAddVariable(input5, input4, "per_row");
+    logadd2->fprop();
+    results_mat["logadd_per_row"] = logadd2->matValue.copy();
+    
+    // Test LogAddVariable to compute a logadd over sub-rows of its first
+    // input.
+    Var logadd3 = new LogAddVariable(input5, input4, "per_column");
+    logadd3->fprop();
+    results_mat["logadd_per_column"] = logadd3->matValue.copy();
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn/var/test/VariablesTest.h
===================================================================
--- trunk/plearn/var/test/VariablesTest.h	2008-02-26 18:22:39 UTC (rev 8583)
+++ trunk/plearn/var/test/VariablesTest.h	2008-02-26 19:32:06 UTC (rev 8584)
@@ -61,8 +61,7 @@
 public:
     //#####  Public Build Options  ############################################
 
-    Mat logadd_binary;
-    Mat unfolded_argmin;
+    map<string, Mat> results_mat;
 
 public:
     //#####  Public Member Functions  #########################################



From tihocan at mail.berlios.de  Tue Feb 26 20:33:42 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 26 Feb 2008 20:33:42 +0100
Subject: [Plearn-commits] r8585 - trunk/plearn_learners/generic
Message-ID: <200802261933.m1QJXg3d016139@sheep.berlios.de>

Author: tihocan
Date: 2008-02-26 20:33:42 +0100 (Tue, 26 Feb 2008)
New Revision: 8585

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/NNet.h
Log:
Full (but not tested yet) implementation of the 'operate_on_bags' feature

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-02-26 19:32:06 UTC (rev 8584)
+++ trunk/plearn_learners/generic/NNet.cc	2008-02-26 19:33:42 UTC (rev 8585)
@@ -51,6 +51,7 @@
 #include <plearn/var/MarginPerceptronCostVariable.h>
 #include <plearn/var/ConfRatedAdaboostCostVariable.h>
 #include <plearn/var/GradientAdaboostCostVariable.h>
+#include <plearn/var/LogAddVariable.h>
 #include <plearn/var/MulticlassLossVariable.h>
 #include <plearn/var/NegCrossEntropySigmoidVariable.h>
 #include <plearn/var/NegLogPoissonVariable.h>
@@ -63,10 +64,12 @@
 #include <plearn/var/SumVariable.h>
 #include <plearn/var/SumAbsVariable.h>
 #include <plearn/var/SumOfVariable.h>
+#include <plearn/var/SumOverBagsVariable.h>
 #include <plearn/var/SumSquareVariable.h>
 #include <plearn/var/TanhVariable.h>
 #include <plearn/var/TransposeVariable.h>
 #include <plearn/var/UnaryHardSlopeVariable.h>
+#include <plearn/var/UnfoldedFuncVariable.h>
 #include <plearn/var/Var_operators.h>
 #include <plearn/var/Var_utils.h>
 #include <plearn/var/FNetLayerVariable.h>
@@ -88,8 +91,8 @@
 //////////
 // NNet //
 //////////
-NNet::NNet()
-    :
+NNet::NNet():
+n_training_bags(-1),
 nhidden(0),
 nhidden2(0),
 noutputs(0),
@@ -390,23 +393,17 @@
     build_();
 }
 
-Var to_define(const Var& bag_inputs, const Var& bag_size, const Func& in_to_out, int max_bag_size)
-{
-    return NULL;
-}
-
 /////////////////////////////////
 // buildBagOutputFromBagInputs //
 /////////////////////////////////
 void NNet::buildBagOutputFromBagInputs(
         const Var& input, Var& before_transfer_func,
-        const Var& bag_inputs, const Var& bag_size, Var& bag_output,
-        int max_bag_size)
+        const Var& bag_inputs, const Var& bag_size, Var& bag_output)
 {
     Func in_to_out = Func(input, before_transfer_func);
-    before_transfer_func = to_define(bag_inputs, bag_size, in_to_out, max_bag_size);
-    // TODO Note that 'to_define' should basically be a logadd over the outputs
-    // before the transfer function.
+    Var tmp_out = new UnfoldedFuncVariable(bag_inputs, in_to_out, false,
+                                           bag_size);
+    before_transfer_func = new LogAddVariable(tmp_out, bag_size, "per_column");
     applyTransferFunc(before_transfer_func, bag_output);
 }
 
@@ -439,10 +436,11 @@
 
         // When operating on bags, use this network to compute the output on a
         // whole bag, which also becomes the output of the network.
-        if (operate_on_bags)
+        if (operate_on_bags) {
+            bag_inputs = Var(max_bag_size, inputsize(), "bag_inputs");
             buildBagOutputFromBagInputs(input, before_transfer_func,
-                                        bag_inputs, bag_size, output,
-                                        max_bag_size);
+                                        bag_inputs, bag_size, output);
+        }
 
         // Build target and weight variables.
         buildTargetAndWeight();
@@ -498,7 +496,6 @@
 ////////////////////
 // setTrainingSet //
 ////////////////////
-
 void NNet::setTrainingSet(VMat training_set, bool call_forget)
 {
     PLASSERT( training_set );
@@ -509,6 +506,25 @@
 
     inherited::setTrainingSet(training_set, call_forget);
     //cout << "name = " << name << endl << "targetsize = " << targetsize_ << endl << "weightsize = " << weightsize_ << endl;
+    
+    if (operate_on_bags) {
+        // Compute the number of bags in the training set.
+        int n_train = training_set->length();
+        PP<ProgressBar> pb = 
+            report_progress ? new ProgressBar("Counting bags", n_train)
+                            : NULL;
+        Vec input, target;
+        real weight;
+        n_training_bags = 0;
+        for (int i = 0; i < n_train; i++) {
+            training_set->getExample(i, input, target, weight);
+            if (int(round(target.lastElement()))
+                & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+                n_training_bags++;
+            if (pb)
+                pb->updateone();
+        }
+    }
 }
 
 ////////////////
@@ -944,6 +960,7 @@
     if(optimizer)
         optimizer->reset();
     stage = 0;
+    n_training_bags = -1;
 }
 
 ///////////////////////
@@ -1111,15 +1128,19 @@
         setTrainStatsCollector(new VecStatsCollector());
     // PLERROR("In NNet::train, you did not setTrainStatsCollector");
 
-    int l = train_set->length();  
+    int n_train = operate_on_bags ? n_training_bags
+                                  : train_set->length();  
     
     if(input_to_output.isNull()) // Net has not been properly built yet (because build was called before the learner had a proper training set)
         build();
 
     // number of samples seen by optimizer before each optimizer update
-    int nsamples = batch_size>0 ? batch_size : l;
+    int nsamples = batch_size>0 ? batch_size : n_train;
     Func paramf = Func(invars, training_cost); // parameterized function to optimize
-    Var totalcost = meanOf(train_set, paramf, nsamples);
+    Var totalcost =
+        operate_on_bags ? sumOverBags(train_set, paramf, max_bag_size,
+                                      nsamples, true)
+                        : meanOf(train_set, paramf, nsamples);
     if(optimizer)
     {
         optimizer->setToOptimize(params, totalcost);  
@@ -1128,7 +1149,7 @@
     else PLERROR("NNet::train can't train without setting an optimizer first!");
 
     // number of optimizer stages corresponding to one learner stage (one epoch)
-    int optstage_per_lstage = l/nsamples;
+    int optstage_per_lstage = n_train / nsamples;
 
     PP<ProgressBar> pb;
     if(report_progress)

Modified: trunk/plearn_learners/generic/NNet.h
===================================================================
--- trunk/plearn_learners/generic/NNet.h	2008-02-26 19:32:06 UTC (rev 8584)
+++ trunk/plearn_learners/generic/NNet.h	2008-02-26 19:33:42 UTC (rev 8585)
@@ -76,6 +76,9 @@
     //! Used to store the size of a bag when 'operate_on_bags' is true.
     Var bag_size;
 
+    //! Number of bags in the training set.
+    int n_training_bags;
+
 // to put back later -- blip  Vec paramsvalues; // values of all parameters
 
 public: // to set these values instead of getting them by training
@@ -244,8 +247,7 @@
     //! the bag rather than for individual samples.
     void buildBagOutputFromBagInputs(
         const Var& input, Var& before_transfer_func,
-        const Var& bag_inputs, const Var& bag_size, Var& bag_output,
-        int max_bag_size);
+        const Var& bag_inputs, const Var& bag_size, Var& bag_output);
 
     //! Builds the target and sampleweight variables.
     void buildTargetAndWeight();



From larocheh at mail.berlios.de  Wed Feb 27 03:00:25 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Wed, 27 Feb 2008 03:00:25 +0100
Subject: [Plearn-commits] r8586 - trunk/plearn_learners/online
Message-ID: <200802270200.m1R20PVg023060@sheep.berlios.de>

Author: larocheh
Date: 2008-02-27 03:00:19 +0100 (Wed, 27 Feb 2008)
New Revision: 8586

Modified:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
Log:
Corrected many bugs...


Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-02-26 19:33:42 UTC (rev 8585)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-02-27 02:00:19 UTC (rev 8586)
@@ -319,11 +319,11 @@
                 //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
                 // Now working in log-domain
                 on_free_energy[ n + offset ] = activation[n+offset] + 
-                    logadd( on_free_energy[n + offset - sub_tree_size],
-                            off_free_energy[n + offset - sub_tree_size] ) ;
+                    logadd( on_free_energy[n + offset - (sub_tree_size/2+1)],
+                            off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
                 off_free_energy[ n + offset ] = 
-                    logadd( on_free_energy[n + offset + sub_tree_size],
-                            off_free_energy[n + offset + sub_tree_size] ) ;
+                    logadd( on_free_energy[n + offset + (sub_tree_size/2+1)],
+                            off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
 
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
@@ -488,11 +488,11 @@
                 //    ( on_free_energy[n + offset + sub_tree_size] + off_free_energy[n + offset + sub_tree_size] ) ;
                 // Now working in the log-domain
                 on_free_energy[ n + offset ] = input[n+offset] + bias[n+offset] +
-                    logadd( on_free_energy[n + offset - sub_tree_size], 
-                            off_free_energy[n + offset - sub_tree_size] ) ;
+                    logadd( on_free_energy[n + offset - (sub_tree_size/2+1)], 
+                            off_free_energy[n + offset - (sub_tree_size/2+1)] ) ;
                 off_free_energy[ n + offset ] = 
-                    logadd( on_free_energy[n + offset + sub_tree_size], 
-                            off_free_energy[n + offset + sub_tree_size] ) ;
+                    logadd( on_free_energy[n + offset + (sub_tree_size/2+1)], 
+                            off_free_energy[n + offset + (sub_tree_size/2+1)] ) ;
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
@@ -690,13 +690,13 @@
 
                 // Gradient w/r current node
                 local_node_expectation_gradient[ n + offset ] += 
-                    ( out_grad - off_grad ) * parent_exp * grand_parent_prob
-                    * node_exp * ( 1 - node_exp );
+                    ( out_grad - off_grad ) * parent_exp * grand_parent_prob;
+                    //* node_exp * ( 1 - node_exp );
 
                 // Gradient w/r parent node
                 local_node_expectation_gradient[ n + offset + sub_tree_size + 1 ] += 
-                        ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob
-                        * parent_exp * (1-parent_exp) ;
+                    ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob;
+                    //* parent_exp * (1-parent_exp) ;
 
             }
 
@@ -735,13 +735,13 @@
 
                 // Gradient w/r current node
                 local_node_expectation_gradient[ n + offset ] += 
-                    ( out_grad - off_grad ) * ( 1 - parent_exp ) * grand_parent_prob
-                    * node_exp * ( 1 - node_exp );
+                    ( out_grad - off_grad ) * ( 1 - parent_exp ) * grand_parent_prob;
+                    //* node_exp * ( 1 - node_exp );
 
                 // Gradient w/r parent node
                 local_node_expectation_gradient[ n + offset - sub_tree_size - 1 ] -= 
-                    ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob
-                    * parent_exp * (1-parent_exp) ;
+                    ( out_grad * node_exp + off_grad * (1 - node_exp) )  * grand_parent_prob;
+                    //* parent_exp * (1-parent_exp) ;
             }
             sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
             depth--;
@@ -760,13 +760,13 @@
         
         // Gradient w/r current node
         local_node_expectation_gradient[ node + offset ] += 
-            ( out_grad - off_grad ) * parent_exp
-            * node_exp * ( 1 - node_exp );
+            ( out_grad - off_grad ) * parent_exp;
+            //* node_exp * ( 1 - node_exp );
         
         // Gradient w/r parent node
         local_node_expectation_gradient[ node + offset + sub_tree_size + 1 ] += 
-            ( out_grad * node_exp  + off_grad * (1 - node_exp) )
-            * parent_exp * (1-parent_exp) ;
+            ( out_grad * node_exp  + off_grad * (1 - node_exp) );
+            //* parent_exp * (1-parent_exp) ;
 
         //// Right child
         node = 3*sub_tree_size+2;
@@ -778,13 +778,13 @@
 
         // Gradient w/r current node
         local_node_expectation_gradient[ node + offset ] += 
-            ( out_grad - off_grad ) * ( 1 - parent_exp ) 
-            * node_exp * ( 1 - node_exp );
+            ( out_grad - off_grad ) * ( 1 - parent_exp ) ;
+            //* node_exp * ( 1 - node_exp );
         
         // Gradient w/r parent node
         local_node_expectation_gradient[ node + offset - sub_tree_size - 1 ] -= 
-            ( out_grad * node_exp + off_grad * (1 - node_exp) ) 
-            * parent_exp * (1-parent_exp) ;
+            ( out_grad * node_exp + off_grad * (1 - node_exp) ) ;
+            //* parent_exp * (1-parent_exp) ;
         
         ////// Root
         node = n_nodes_per_tree / 2;
@@ -795,7 +795,7 @@
         off_grad = off_tree_gradient[ node + offset ] ;
         node_exp = local_node_expectation[ node + offset ];
         local_node_expectation_gradient[ node + offset ] += 
-            ( out_grad - off_grad ) * node_exp * ( 1 - node_exp );
+            ( out_grad - off_grad );// * node_exp * ( 1 - node_exp );
 
         offset += n_nodes_per_tree;
     }
@@ -819,15 +819,16 @@
             for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
             {
                 out_grad = on_free_energy_gradient[ n + offset ];
-                node_exp = local_node_expectation[n + offset - sub_tree_size];
+                node_exp = local_node_expectation[n + offset - (sub_tree_size/2+1)];
                 input_gradient[n+offset] += out_grad;
-                on_free_energy[n + offset - sub_tree_size] += out_grad * node_exp; 
-                off_free_energy[n + offset - sub_tree_size] += out_grad * (1 - node_exp); 
+                on_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * node_exp; 
+                off_free_energy_gradient[n + offset - (sub_tree_size/2+1)] += out_grad * (1 - node_exp); 
 
                 out_grad = off_free_energy_gradient[ n + offset ];
-                node_exp = local_node_expectation[n + offset + sub_tree_size];
-                on_free_energy[n + offset + sub_tree_size] += out_grad * node_exp; 
-                off_free_energy[n + offset + sub_tree_size] += out_grad * (1 - node_exp); 
+                node_exp = local_node_expectation[n + offset + (sub_tree_size/2+1)];
+                on_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += out_grad * node_exp; 
+                off_free_energy_gradient[n + offset + (sub_tree_size/2+1)] += 
+                    out_grad * (1 - node_exp); 
             }
             sub_tree_size /= 2;
             depth++;
@@ -848,7 +849,6 @@
     for( int i=0 ; i<size ; i++ )
     {
         real in_grad_i = input_gradient[i];
-        input_gradient[i] += in_grad_i;
 
         if( momentum == 0. )
         {



From nouiz at mail.berlios.de  Wed Feb 27 13:24:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Feb 2008 13:24:07 +0100
Subject: [Plearn-commits] r8587 - trunk/plearn/vmat
Message-ID: <200802271224.m1RCO7Wb013576@sheep.berlios.de>

Author: nouiz
Date: 2008-02-27 13:23:54 +0100 (Wed, 27 Feb 2008)
New Revision: 8587

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
added function VMatrix::is_file_uptodate() that check if a file is more recent then this VMatrix and print warning as needed.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-27 02:00:19 UTC (rev 8586)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-27 12:23:54 UTC (rev 8587)
@@ -1416,6 +1416,28 @@
     return field_stats;
 }
 
+//////////////////////
+// is_file_uptodate //
+//////////////////////
+bool VMatrix::is_file_uptodate(PPath& path, bool warning_recalcul,
+                               bool warning_mtime) const
+{
+    bool exist = isfile(path);
+    bool uptodate = false;
+    if(exist)
+        uptodate = getMtime()<mtime(path);
+    if (warning_mtime && exist && uptodate && getMtime()==0)
+        PLWARNING("Warning: using a saved file (%s) but mtime is 0"
+                  "(cannot be sure file is up to date).",
+                  path.absolute().c_str());
+    if(warning_recalcul && exist && !uptodate)
+        PLWARNING("Warning: recomputing saved file (%s) as it is older"
+                  "then the source.",
+                  path.absolute().c_str());
+
+    return exist && uptodate;
+ 
+}
 /////////////////////////////////
 // getPrecomputedStatsFromFile //
 /////////////////////////////////
@@ -1426,24 +1448,11 @@
     PPath metadatadir = getMetaDataDir();
     PPath statsfile =  metadatadir / filename;
     lockMetaDataDir();
-    bool file = isfile(statsfile);
-    bool uptodate = false;
-    if(file)
-        uptodate = getMtime()<mtime(statsfile);
-    if (file && uptodate)
-    {
-        if(getMtime() == 0)
-            PLWARNING("Warning: using a saved stat file (%s) but mtime is 0"
-                      "(cannot be sure file is up to date).",
-                      statsfile.absolute().c_str());
+    bool uptodate=is_file_uptodate(statsfile, true, true);
+    if (uptodate)
         PLearn::load(statsfile, stats);
-    }
     else
     {
-        if(file && !uptodate)
-            PLWARNING("Warning: recomputing stat file (%s) as it is older"
-                      "the source.",
-                      statsfile.absolute().c_str());
         VMat vm = const_cast<VMatrix*>(this);
         stats = PLearn::computeStats(vm, maxnvalues, progress_bar);
         if(!metadatadir.isEmpty())

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-27 02:00:19 UTC (rev 8586)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-27 12:23:54 UTC (rev 8587)
@@ -376,6 +376,9 @@
     inline void setMtime(time_t t) { mtime_ = t; }
 
 
+    bool is_file_uptodate(PPath& file, bool warning_recalcul=false,
+                          bool warning_mtime=true) const;
+
     //#####  Matrix Sizes  ####################################################
 
     /// Return the number of columns in the VMatrix



From nouiz at mail.berlios.de  Wed Feb 27 14:27:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Feb 2008 14:27:30 +0100
Subject: [Plearn-commits] r8588 - in trunk/plearn: db vmat
Message-ID: <200802271327.m1RDRUPW019590@sheep.berlios.de>

Author: nouiz
Date: 2008-02-27 14:27:29 +0100 (Wed, 27 Feb 2008)
New Revision: 8588

Modified:
   trunk/plearn/db/databases.cc
   trunk/plearn/vmat/AutoVMatrix.cc
Log:
removed old include

Modified: trunk/plearn/db/databases.cc
===================================================================
--- trunk/plearn/db/databases.cc	2008-02-27 12:23:54 UTC (rev 8587)
+++ trunk/plearn/db/databases.cc	2008-02-27 13:27:29 UTC (rev 8588)
@@ -43,7 +43,6 @@
 
 #include "databases.h"
 #include <plearn/vmat/ConcatRowsVMatrix.h>
-#include "NistDB.h"
 #include <plearn/math/random.h>
 #include <plearn/vmat/RemapLastColumnVMatrix.h>
 #include <plearn/vmat/ShiftAndRescaleVMatrix.h>

Modified: trunk/plearn/vmat/AutoVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AutoVMatrix.cc	2008-02-27 12:23:54 UTC (rev 8587)
+++ trunk/plearn/vmat/AutoVMatrix.cc	2008-02-27 13:27:29 UTC (rev 8588)
@@ -44,7 +44,6 @@
 #include "AutoVMatrix.h"
 #include "MemoryVMatrix.h"
 #include <plearn/db/getDataSet.h>
-#include <plearn/db/databases.h>
 
 namespace PLearn {
 using namespace std;



From saintmlx at mail.berlios.de  Wed Feb 27 21:04:26 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 27 Feb 2008 21:04:26 +0100
Subject: [Plearn-commits] r8589 - trunk/python_modules/plearn/utilities
Message-ID: <200802272004.m1RK4Qms010203@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-27 21:04:25 +0100 (Wed, 27 Feb 2008)
New Revision: 8589

Modified:
   trunk/python_modules/plearn/utilities/ppath.py
Log:
- removed useless semicolumn;



Modified: trunk/python_modules/plearn/utilities/ppath.py
===================================================================
--- trunk/python_modules/plearn/utilities/ppath.py	2008-02-27 13:27:29 UTC (rev 8588)
+++ trunk/python_modules/plearn/utilities/ppath.py	2008-02-27 20:04:25 UTC (rev 8589)
@@ -129,7 +129,7 @@
         begvar = string.find(expanded, "${")
         endvar = string.find(expanded,  "}")
 
-    return expanded;
+    return expanded
 
 ########################################
 ##  Binding Functions  #################



From saintmlx at mail.berlios.de  Wed Feb 27 21:32:14 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 27 Feb 2008 21:32:14 +0100
Subject: [Plearn-commits] r8590 - trunk/python_modules/plearn/pytest
Message-ID: <200802272032.m1RKWE3e013151@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-27 21:32:13 +0100 (Wed, 27 Feb 2008)
New Revision: 8590

Modified:
   trunk/python_modules/plearn/pytest/programs.py
Log:
- allow compilation of python extension modules (WIP)



Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2008-02-27 20:04:25 UTC (rev 8589)
+++ trunk/python_modules/plearn/pytest/programs.py	2008-02-27 20:32:13 UTC (rev 8590)
@@ -49,6 +49,10 @@
     return program_path
 
 def find_local_program(name):
+    name= ppath.expandEnvVariables(name)
+    name2, ext= os.path.splitext(name)
+    if ext=='.so':
+        name= name2
     if os.path.exists( name ) or \
            os.path.exists( name+'.cc' ):
         return os.path.abspath(name)
@@ -123,7 +127,7 @@
         ##  initialize the '_program_path' and '__is_global' members
         logging.debug("\nProgram: "+self.getProgramPath())
 
-        internal_exec_path = self.getInternalExecPath(self.__signature())
+        internal_exec_path = self.getInternalExecPath(self._signature())
         logging.debug("Internal Exec Path: %s"%internal_exec_path)
         if self.isCompilable():
             if self.compiler is None:
@@ -152,7 +156,7 @@
                           self.name, self.compile_options, options)
             self.compile_options = options
             
-    def __signature(self):
+    def _signature(self):
         if self.compiler is None:
             signature = self.name
         else:
@@ -303,7 +307,7 @@
         
     def getInternalExecPath(self, candidate=None):
         if hasattr(self, '_internal_exec_path'):
-            assert candidate is None
+            assert candidate is None, 'candidate is not None:'+repr(candidate)
             return self._internal_exec_path
 
         logging.debug("Parsing for internal exec path; candidate=%s"%candidate)
@@ -393,6 +397,172 @@
     def isPLearnCommand(self):
         return self.isGlobal() and self.isCompilable()
 
+
+class PythonSharedLibrary(Program):
+    """
+    Version of Program for shared libraries used as python extension modules.
+    WARNING: the compilation produces an .so file in the same directory as
+    the source (instead of ~/.plearn/pytest/compiled_programs)
+    """
+    def __init__(self, **overrides):
+        Program.__init__(self, **overrides)
+
+    def getProgramPath(self):
+        if hasattr(self, '_program_path'):
+            return self._program_path        
+        pp= super(PythonSharedLibrary, self).getProgramPath()
+        ppd, ext= os.path.splitext(pp)
+        if ext == '':
+            pp+= '.cc'
+        self._program_path= pp
+        return self._program_path
+
+    def _signature(self):
+        return self.getProgramPath()[:-2]+'so'
+
+        
+class PythonSharedLibrary2(Program):
+    """
+    DO NOT USE (unless you fix it).
+    Old, incomplete version of PythonSharedLibrary;
+    should produce executable in ~/.plearn/pytest/compiled_programs
+    """
+
+    def __init__(self, **overrides):
+        Program.__init__(self, **overrides)
+        internal_exec_path= self.getInternalExecPath()
+        internal_exec_dir= os.path.dirname(internal_exec_path)
+        module_path= self.getPythonModulePath()
+
+        base_exec_dir= internal_exec_dir[:-len(module_path)]
+        base_exec_dir= os.path.dirname(base_exec_dir)
+
+        def mkInit(d):
+            logging.debug("** "+d)
+            try:
+                os.mkdir(d)
+            except:
+                pass
+            try:
+                os.mknod(os.path.join(d, '__init__.py'))
+                logging.debug("\t**! "+d)
+            except:
+                pass
+
+        to_init= base_exec_dir
+        mkInit(to_init)
+        subdirs= module_path.split('/')
+        for d in subdirs:
+            to_init= os.path.join(to_init, d)
+            mkInit(to_init)
+            
+        os.environ['PYTHONPATH']= base_exec_dir+':'+os.getenv('PYTHONPATH','')
+        
+        logging.debug("PYTHONPATH: "+os.environ['PYTHONPATH']+"\tinternal_exec_path="+internal_exec_path)
+        logging.debug("NAME: "+self.name)
+        p= ppath.expandEnvVariables(self.name)
+        logging.debug("path: "+p)
+
+
+    def getPythonModulePath(self):
+        if hasattr(self, '_python_module_path'):
+            return self._python_module_path
+        pythonpath= os.getenv('PYTHONPATH','').split(':')
+        module_path= None
+        common_prefix= None
+        name= ppath.expandEnvVariables(self.name)
+        for p in pythonpath:
+            c= os.path.commonprefix([p, name])
+            if c == p:
+                logging.debug("Common Prefix Match: "+c+' -- '+repr(name[len(p):].split('/')))
+                module_path= name[len(p):]
+                while module_path[0] == '/':
+                    module_path= module_path[1:]
+                while module_path[-1] == '/':
+                    module_path= module_path[:-1]
+                module_path= os.path.dirname(module_path)
+                common_prefix= c
+                break
+            else:
+                logging.debug("NO Common Prefix: "+p+' -- '+name)
+        self._python_module_path= module_path
+        return module_path
+
+    def _signature(self):
+        """
+        
+        """
+        if self.compiler is None:
+            signature = self.name
+        else:
+            name= os.path.basename(self.name)
+            if self.compile_options == "":
+                self.compile_options = None
+            module_path= self.getPythonModulePath()
+            signature = "SHARED_OBJS__compiler_%s__options_%s"%(
+               self.compiler, self.compile_options)
+            signature= os.path.join(signature, module_path, name)
+        signature = signature.replace('-', '') # Compile options...
+        return signature.replace(' ', '_')
+
+#      def getInternalExecPath(self, candidate=None):
+#         if hasattr(self, '_internal_exec_path'):
+#             return self._internal_exec_path
+#         iep= super(PythonSharedLibrary, self).getInternalExecPath(self, candidate)
+#         if iep.endswith('.so'):
+#             return iep
+#         self._internal_exec_path= iep+'.so'
+#         return self._internal_exec_path
+
+
+
+         
+#         if hasattr(self, '_internal_exec_path'):
+#             assert candidate is None
+#             return self._internal_exec_path
+
+#         logging.debug("Parsing for internal exec path; candidate=%s"%candidate)
+
+#         if candidate == self.name:
+#             self._internal_exec_path = self.getProgramPath()
+#         else:
+#             self._internal_exec_path = \
+#                 os.path.join(core.pytest_config_path(), "compiled_programs", candidate)
+#         if sys.platform == 'win32':
+#             # Cygwin has trouble with the \ characters.
+#             self._internal_exec_path = \
+#                 self._internal_exec_path.replace('\\', '/')
+#             # In addition, we need to add the '.exe' extension as otherwise it
+#             # may not be able to run it.
+#             self._internal_exec_path += '.exe'
+#         return self._internal_exec_path
+
+    def getProgramPath(self):
+        if hasattr(self, '_program_path'):
+            return self._program_path        
+        pp= super(PythonSharedLibrary, self).getProgramPath()
+        ppd, ext= os.path.splitext(pp)
+        if ext == '':
+            pp+= '.cc'
+        self._program_path= pp
+        return self._program_path
+         
+#         if hasattr(self, '_program_path'):
+#             return self._program_path        
+
+#         try:
+#             self._program_path = find_local_program(self.name)
+#             self.__is_global = False
+#         except core.PyTestUsageError, not_local:
+#             try:
+#                 self._program_path = find_global_program(self.name)
+#                 self.__is_global = True                
+#             except core.PyTestUsageError, neither_global:
+#                 raise core.PyTestUsageError("%s %s"%(not_local, neither_global))
+#         return self._program_path
+
+
+
 if __name__ == "__main__":
     # In Python2.4
     # logging.basicConfig(level=logging.DEBUG)



From saintmlx at mail.berlios.de  Wed Feb 27 21:33:13 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 27 Feb 2008 21:33:13 +0100
Subject: [Plearn-commits] r8591 - trunk/python_modules/plearn/pyplearn
Message-ID: <200802272033.m1RKXDWU013274@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-27 21:33:13 +0100 (Wed, 27 Feb 2008)
New Revision: 8591

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
- added bindersHelp() function to print options help



Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2008-02-27 20:32:13 UTC (rev 8590)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2008-02-27 20:33:13 UTC (rev 8591)
@@ -1074,6 +1074,12 @@
         for opt in plopt.iterator(namespace):
             print >>out, '   ',opt.getName()+':', opt
 
+def bindersHelp():
+    s= ''
+    for binder in plargs.getBinders():
+        for opt in plopt.iterator(binder):
+            s+= opt.getName()+': '+opt.getDoc()+'\n\t'+str(opt)+'\n'
+    return s
 
 def currentNamespacesHelp():
     s= ''



From nouiz at mail.berlios.de  Wed Feb 27 21:54:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Feb 2008 21:54:22 +0100
Subject: [Plearn-commits] r8592 - trunk/plearn/vmat
Message-ID: <200802272054.m1RKsMeW014599@sheep.berlios.de>

Author: nouiz
Date: 2008-02-27 21:54:22 +0100 (Wed, 27 Feb 2008)
New Revision: 8592

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
fix typo


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-27 20:33:13 UTC (rev 8591)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-27 20:54:22 UTC (rev 8592)
@@ -1432,7 +1432,7 @@
                   path.absolute().c_str());
     if(warning_recalcul && exist && !uptodate)
         PLWARNING("Warning: recomputing saved file (%s) as it is older"
-                  "then the source.",
+                  " then the source.",
                   path.absolute().c_str());
 
     return exist && uptodate;



From nouiz at mail.berlios.de  Wed Feb 27 22:16:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 27 Feb 2008 22:16:24 +0100
Subject: [Plearn-commits] r8593 - trunk/plearn/db
Message-ID: <200802272116.m1RLGOgB016197@sheep.berlios.de>

Author: nouiz
Date: 2008-02-27 22:16:24 +0100 (Wed, 27 Feb 2008)
New Revision: 8593

Modified:
   trunk/plearn/db/databases.cc
Log:
reverted last modif...


Modified: trunk/plearn/db/databases.cc
===================================================================
--- trunk/plearn/db/databases.cc	2008-02-27 20:54:22 UTC (rev 8592)
+++ trunk/plearn/db/databases.cc	2008-02-27 21:16:24 UTC (rev 8593)
@@ -43,6 +43,7 @@
 
 #include "databases.h"
 #include <plearn/vmat/ConcatRowsVMatrix.h>
+#include <plearn/db/NistDB.h>
 #include <plearn/math/random.h>
 #include <plearn/vmat/RemapLastColumnVMatrix.h>
 #include <plearn/vmat/ShiftAndRescaleVMatrix.h>



From saintmlx at mail.berlios.de  Thu Feb 28 00:14:20 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 28 Feb 2008 00:14:20 +0100
Subject: [Plearn-commits] r8594 - trunk/python_modules/plearn/pytest
Message-ID: <200802272314.m1RNEKUu026504@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-28 00:14:20 +0100 (Thu, 28 Feb 2008)
New Revision: 8594

Modified:
   trunk/python_modules/plearn/pytest/programs.py
   trunk/python_modules/plearn/pytest/tests.py
Log:
- fixed compilation of python extension (still WIP)



Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2008-02-27 21:16:24 UTC (rev 8593)
+++ trunk/python_modules/plearn/pytest/programs.py	2008-02-27 23:14:20 UTC (rev 8594)
@@ -189,7 +189,8 @@
 
         # Account for dependencies
         success = no_need_to_compile or (self.__attempted_to_compile and exec_exists)
-        #print "+++ compilationSucceeded():", self.name, success, no_need_to_compile, self.__attempted_to_compile, exec_exists
+        #logging.debug("compilationSucceeded(): %s %s %s %s %s", self.getInternalExecPath(),#self.name,
+        #              success, no_need_to_compile, self.__attempted_to_compile, exec_exists)
         for dep in self.dependencies:            
             success = (success and dep.compilationSucceeded())
             #print "+++ DEP compilationSucceeded():", success
@@ -231,7 +232,7 @@
         #print "+++ Success", self.name, succeeded        
         for dep in self.dependencies:
             succeeded = (succeeded and dep.compile(publish_dirpath))        
-            #print "+++ DEP", succeeded
+            #print "+++ DEPcompile", succeeded
             
         return succeeded
         
@@ -281,6 +282,7 @@
                               self.getInternalExecPath(),
                               redirection, log_fname )
 
+        compile_cmd= self.getCompileCommand(the_compiler, compile_options, redirection, log_fname)
 
         logging.debug(compile_cmd)
         if sys.platform == 'win32':
@@ -302,6 +304,15 @@
             
         return compile_exit_code==0
 
+    def getCompileCommand(self, the_compiler, compile_options, redirection, log_fname):
+        compile_cmd   = "%s %s %s -link-target %s %s %s" \
+                        % ( the_compiler, compile_options,
+                            self.getProgramPath(),
+                            self.getInternalExecPath(),
+                            redirection, log_fname )
+        return compile_cmd
+
+
     def getName(self):
         return self.name
         
@@ -402,7 +413,7 @@
     """
     Version of Program for shared libraries used as python extension modules.
     WARNING: the compilation produces an .so file in the same directory as
-    the source (instead of ~/.plearn/pytest/compiled_programs)
+    the source (instead of e.g. ~/.plearn/pytest/compiled_programs)
     """
     def __init__(self, **overrides):
         Program.__init__(self, **overrides)
@@ -420,12 +431,23 @@
     def _signature(self):
         return self.getProgramPath()[:-2]+'so'
 
+    def getCompileCommand(self, the_compiler, compile_options, redirection, log_fname):
+        """
+        WARNING: this replaces the shared object in-place
+        """
+        compile_cmd   = "%s %s %s %s %s" \
+                        % ( the_compiler, compile_options,
+                            self.getProgramPath(),
+                            redirection, log_fname )
+        return compile_cmd
+
         
 class PythonSharedLibrary2(Program):
     """
     DO NOT USE (unless you fix it).
-    Old, incomplete version of PythonSharedLibrary;
+    Incomplete version of PythonSharedLibrary;
     should produce executable in ~/.plearn/pytest/compiled_programs
+    should replace PythonSharedLibrary once fixed
     """
 
     def __init__(self, **overrides):

Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2008-02-27 21:16:24 UTC (rev 8593)
+++ trunk/python_modules/plearn/pytest/tests.py	2008-02-27 23:14:20 UTC (rev 8594)
@@ -52,7 +52,6 @@
 from core import *
 from programs import *
 
-
 # Eventually remove Test's static methods
 EXPECTED_RESULTS = "expected_results"
 RUN_RESULTS      = "run_results"
@@ -707,7 +706,7 @@
 
         logging.debug("\nCompilation:")
         logging.debug("------------")
-        
+
         self.test.compile()
         if not self.test.compilationSucceeded():
             logging.debug("Compilation failed.")



From tihocan at mail.berlios.de  Thu Feb 28 18:13:37 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Feb 2008 18:13:37 +0100
Subject: [Plearn-commits] r8595 - trunk/plearn/var
Message-ID: <200802281713.m1SHDb82012750@sheep.berlios.de>

Author: tihocan
Date: 2008-02-28 18:13:34 +0100 (Thu, 28 Feb 2008)
New Revision: 8595

Modified:
   trunk/plearn/var/LogAddVariable.cc
Log:
Bug fix in newly introduced feature

Modified: trunk/plearn/var/LogAddVariable.cc
===================================================================
--- trunk/plearn/var/LogAddVariable.cc	2008-02-27 23:14:20 UTC (rev 8594)
+++ trunk/plearn/var/LogAddVariable.cc	2008-02-28 17:13:34 UTC (rev 8595)
@@ -231,7 +231,7 @@
             work -= value[i];
             apply(work, work, safeexp);
             work *= gradient[i];
-            input1->matGradient.subMat(0, i, n, 1) += work;
+            input1->matGradient.subMat(0, i, n, 1) += work.toMat(n, 1);
         }
     }
 }



From tihocan at mail.berlios.de  Thu Feb 28 18:15:18 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Feb 2008 18:15:18 +0100
Subject: [Plearn-commits] r8596 - trunk/plearn/var
Message-ID: <200802281715.m1SHFIkt012892@sheep.berlios.de>

Author: tihocan
Date: 2008-02-28 18:15:18 +0100 (Thu, 28 Feb 2008)
New Revision: 8596

Modified:
   trunk/plearn/var/BinaryVariable.cc
Log:
Robustified code so that it does not crash when one of the two inputs is NULL

Modified: trunk/plearn/var/BinaryVariable.cc
===================================================================
--- trunk/plearn/var/BinaryVariable.cc	2008-02-28 17:13:34 UTC (rev 8595)
+++ trunk/plearn/var/BinaryVariable.cc	2008-02-28 17:15:18 UTC (rev 8596)
@@ -61,8 +61,10 @@
     input1(v1),
     input2(v2) 
 {
-    input1->disallowPartialUpdates();
-    input2->disallowPartialUpdates();
+    if (input1)
+        input1->disallowPartialUpdates();
+    if (input2)
+        input2->disallowPartialUpdates();
     if (call_build_)
         build_();
 }
@@ -135,33 +137,47 @@
 bool BinaryVariable::markPath()
 {
     if(!marked)
-        marked = input1->markPath() | input2->markPath();
+        marked = (input1 ? input1->markPath() : false) |
+                 (input2 ? input2->markPath() : false);
     return marked;
 }
 
-
+///////////////
+// buildPath //
+///////////////
 void BinaryVariable::buildPath(VarArray& proppath)
 {
     if(marked)
     {
-        input1->buildPath(proppath);
-        input2->buildPath(proppath);
+        if (input1)
+            input1->buildPath(proppath);
+        if (input2)
+            input2->buildPath(proppath);
         proppath &= Var(this);
         //cout<<"proppath="<<this->getName()<<endl;
         clearMark();
     }
 }
 
-
+/////////////
+// sources //
+/////////////
 VarArray BinaryVariable::sources() 
 { 
     if (marked)
         return VarArray(0,0);
     marked = true;
-    return input1->sources() & input2->sources(); 
+    if (!input1)
+        return input2->sources();
+    if (!input2)
+        return input1->sources();
+    return (input1 ? input1->sources() : VarArray(0, 0)) &
+           (input2 ? input2->sources() : VarArray(0, 0));
 }
 
-
+////////////////////
+// random_sources //
+////////////////////
 VarArray BinaryVariable::random_sources() 
 { 
     if (marked)
@@ -170,47 +186,58 @@
     return input1->random_sources() & input2->random_sources(); 
 }
 
-
+///////////////
+// ancestors //
+///////////////
 VarArray BinaryVariable::ancestors() 
 { 
     if (marked)
         return VarArray(0,0);
     marked = true;
-    return input1->ancestors() & input2->ancestors() & Var(this) ;
+    return (input1 ? input1->ancestors() : VarArray(0, 0)) &
+           (input2 ? input2->ancestors() : VarArray(0, 0)) & Var(this);
 }
 
-
+/////////////////////
+// unmarkAncestors //
+/////////////////////
 void BinaryVariable::unmarkAncestors()
 { 
     if (marked)
     {
         marked = false;
-        input1->unmarkAncestors();
-        input2->unmarkAncestors();
+        if (input1)
+            input1->unmarkAncestors();
+        if (input2)
+            input2->unmarkAncestors();
     }
 }
 
-
+/////////////
+// parents //
+/////////////
 VarArray BinaryVariable::parents() 
 { 
     VarArray unmarked_parents;
-    if (!input1->marked)
+    if (input1 && !input1->marked)
         unmarked_parents.append(input1);
-    if (!input2->marked)
+    if (input2 && !input2->marked)
         unmarked_parents.append(input2);
     return unmarked_parents;
 }
 
-
+//////////////////
+// resizeRValue //
+//////////////////
 void BinaryVariable::resizeRValue()
 {
     inherited::resizeRValue();
-    if (!input1->rvaluedata) input1->resizeRValue();
-    if (!input2->rvaluedata) input2->resizeRValue();
+    if (input1 && !input1->rvaluedata)
+        input1->resizeRValue();
+    if (input2 && !input2->rvaluedata)
+        input2->resizeRValue();
 }
 
-
-
 } // end of namespace PLearn
 
 



From tihocan at mail.berlios.de  Thu Feb 28 18:17:04 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Feb 2008 18:17:04 +0100
Subject: [Plearn-commits] r8597 - trunk/plearn_learners/generic
Message-ID: <200802281717.m1SHH4TG013079@sheep.berlios.de>

Author: tihocan
Date: 2008-02-28 18:17:04 +0100 (Thu, 28 Feb 2008)
New Revision: 8597

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/generic/NNet.h
Log:
Bunch of fixes to the 'operate_on_bags' mode


Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-02-28 17:15:18 UTC (rev 8596)
+++ trunk/plearn_learners/generic/NNet.cc	2008-02-28 17:17:04 UTC (rev 8597)
@@ -425,9 +425,16 @@
         if (noutputs == 0)
             PLERROR("NNet: the option 'noutputs' must be specified");
 
-        // Initialize the input.
+        // Initialize input.
         input = Var(1, inputsize(), "input");
 
+        // Initialize bag stuff.
+        if (operate_on_bags) {
+            bag_size = Var(1, 1, "bag_size");
+            store_bag_size.resize(1);
+            store_bag_inputs.resize(max_bag_size, inputsize());
+        }
+
         params.resize(0);
         Var before_transfer_func;
 
@@ -687,7 +694,7 @@
     output_and_target_to_cost = Func(outvars, test_costs); 
     // Since there will be a fprop() in the network, we need to make sure the
     // input is valid.
-    if (train_set && train_set->length() >= the_input->width()) {
+    if (train_set && train_set->length() >= the_input->length()) {
         Vec input, target;
         real weight;
         for (int i = 0; i < the_input->length(); i++) {
@@ -881,7 +888,9 @@
 // buildTargetAndWeight //
 //////////////////////////
 void NNet::buildTargetAndWeight() {
-    target = Var(1, targetsize(), "target");
+    int ts = operate_on_bags ? targetsize() - 1 // Remove bag information.
+                             : targetsize();
+    target = Var(1, ts, "target");
     if(weightsize_>0)
     {
         if (weightsize_!=1)
@@ -896,6 +905,7 @@
 void NNet::computeCostsFromOutputs(const Vec& inputv, const Vec& outputv, 
                                    const Vec& targetv, Vec& costsv) const
 {
+    PLASSERT_MSG( !operate_on_bags, "Not implemented" );
 #ifdef BOUNDCHECK
     // Stable cross entropy needs the value *before* the transfer function.
     if (cost_funcs.contains("stable_cross_entropy") or
@@ -912,6 +922,9 @@
 ///////////////////
 void NNet::computeOutput(const Vec& inputv, Vec& outputv) const
 {
+    if (operate_on_bags)
+        PLERROR("In NNet::computeOutput - Cannot compute output without bag "
+                "information");
     outputv.resize(outputsize());
     input_to_output->fprop(inputv,outputv);
 }
@@ -923,7 +936,27 @@
                                  Vec& outputv, Vec& costsv) const
 {
     outputv.resize(outputsize());
-    test_costf->fprop(inputv&targetv, outputv&costsv);
+    costsv.resize(nTestCosts());
+    if (!operate_on_bags)
+        test_costf->fprop(inputv&targetv, outputv&costsv);
+    else {
+        // We can only compute the output once the whole bag has been seen.
+        int last_target_idx = targetv.length() - 1;
+        int bag_info = int(round(targetv[last_target_idx]));
+        if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
+            store_bag_size[0] = 0;
+        store_bag_inputs(int(round(store_bag_size[0]))) << inputv;
+        store_bag_size[0]++;
+        if (bag_info & SumOverBagsVariable::TARGET_COLUMN_LAST)
+            test_costf->fprop(store_bag_inputs.toVec()
+                                    & store_bag_size
+                                    & targetv.subVec(0, last_target_idx),
+                              outputv & costsv);
+        else {
+            outputv.fill(MISSING_VALUE);
+            costsv.fill(MISSING_VALUE);
+        }
+    }
 }
 
 /////////////////
@@ -1069,6 +1102,7 @@
 {
 
     inherited::makeDeepCopyFromShallowCopy(copies);
+
     // protected:
     varDeepCopyField(rbf_centers, copies);
     varDeepCopyField(rbf_sigmas, copies);
@@ -1083,7 +1117,10 @@
     deepCopyField(invars, copies);
     deepCopyField(params, copies);
     varDeepCopyField(bag_inputs, copies);
+    deepCopyField(store_bag_inputs, copies);
     varDeepCopyField(bag_size, copies);
+    deepCopyField(store_bag_size, copies);
+
     // public:
     deepCopyField(paramsvalues, copies);
     varDeepCopyField(input, copies);

Modified: trunk/plearn_learners/generic/NNet.h
===================================================================
--- trunk/plearn_learners/generic/NNet.h	2008-02-28 17:15:18 UTC (rev 8596)
+++ trunk/plearn_learners/generic/NNet.h	2008-02-28 17:17:04 UTC (rev 8597)
@@ -73,9 +73,15 @@
     //! Used to store the inputs in a bag when 'operate_on_bags' is true.
     Var bag_inputs;
 
+    //! Used to store test inputs in a bag when 'operate_on_bags' is true.
+    Mat store_bag_inputs;
+
     //! Used to store the size of a bag when 'operate_on_bags' is true.
     Var bag_size;
 
+    //! Used to remember the size of a test bag.
+    Vec store_bag_size;
+
     //! Number of bags in the training set.
     int n_training_bags;
 



From tihocan at mail.berlios.de  Thu Feb 28 20:14:17 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Feb 2008 20:14:17 +0100
Subject: [Plearn-commits] r8598 - trunk/examples/data/test_suite
Message-ID: <200802281914.m1SJEHsU010021@sheep.berlios.de>

Author: tihocan
Date: 2008-02-28 20:14:17 +0100 (Thu, 28 Feb 2008)
New Revision: 8598

Added:
   trunk/examples/data/test_suite/toy_multi_class_bags.amat
Log:
New dataset for the test suite

Added: trunk/examples/data/test_suite/toy_multi_class_bags.amat
===================================================================
--- trunk/examples/data/test_suite/toy_multi_class_bags.amat	2008-02-28 17:17:04 UTC (rev 8597)
+++ trunk/examples/data/test_suite/toy_multi_class_bags.amat	2008-02-28 19:14:17 UTC (rev 8598)
@@ -0,0 +1,9 @@
+# 3-class classification toy problem with bag information.
+#sizes: 3 2 0
+#: x1 x2 x3 class bag_info
+0 0 1 0 1
+0 1 0 0 0
+1 0 0 0 2
+0 1 1 1 1
+1 1 0 1 2
+1 1 1 2 3



From tihocan at mail.berlios.de  Thu Feb 28 20:14:47 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Feb 2008 20:14:47 +0100
Subject: [Plearn-commits] r8599 - in
	trunk/plearn_learners/generic/test/NNet: . .pytest
	.pytest/PL_NNet_Bags .pytest/PL_NNet_Bags/expected_results
	.pytest/PL_NNet_Bags/expected_results/expdir-nnet
	.pytest/PL_NNet_Bags/expected_results/expdir-nnet/Split0
	.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata
	.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata
Message-ID: <200802281914.m1SJElR8010057@sheep.berlios.de>

Author: tihocan
Date: 2008-02-28 20:14:47 +0100 (Thu, 28 Feb 2008)
New Revision: 8599

Added:
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/RUN.log
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/Split0/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/Split0/final_learner.psave
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames
   trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes
   trunk/plearn_learners/generic/test/NNet/nnet_bags.pyplearn
Modified:
   trunk/plearn_learners/generic/test/NNet/pytest.config
Log:
Added new test of a NNet operating on bags


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/RUN.log
===================================================================

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/Split0/final_learner.psave	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/Split0/final_learner.psave	2008-02-28 19:14:47 UTC (rev 8599)
@@ -0,0 +1,64 @@
+*1 ->NNet(
+nhidden = 10 ;
+nhidden2 = 0 ;
+noutputs = 3 ;
+weight_decay = 0 ;
+bias_decay = 0 ;
+layer1_weight_decay = 0 ;
+layer1_bias_decay = 0 ;
+layer2_weight_decay = 0 ;
+layer2_bias_decay = 0 ;
+output_layer_weight_decay = 0 ;
+output_layer_bias_decay = 0 ;
+direct_in_to_out_weight_decay = 0 ;
+penalty_type = "L2_square" ;
+L1_penalty = 0 ;
+fixed_output_weights = 0 ;
+input_reconstruction_penalty = 0 ;
+direct_in_to_out = 0 ;
+rbf_layer_size = 0 ;
+first_class_is_junk = 1 ;
+output_transfer_func = "softmax" ;
+hidden_transfer_func = "tanh" ;
+cost_funcs = 1 [ "NLL" ] ;
+classification_regularizer = 0 ;
+first_hidden_layer = *0 ;
+first_hidden_layer_is_output = 0 ;
+n_non_params_in_first_hidden_layer = 0 ;
+transpose_first_hidden_layer = 0 ;
+margin = 1 ;
+do_not_change_params = 0 ;
+optimizer = *2 ->GradientOptimizer(
+start_learning_rate = 0.100000000000000006 ;
+learning_rate = 0.100000000000000006 ;
+decrease_constant = 0 ;
+lr_schedule = 0  0  [ 
+]
+;
+use_stochastic_hack = 0 ;
+verbosity = 0 ;
+nstages = 1  )
+;
+batch_size = 0 ;
+initialization_method = "uniform_linear" ;
+operate_on_bags = 1 ;
+max_bag_size = 20 ;
+paramsvalues = 73 [ -0.264371813933822586 -0.205331393475229718 0.294763050648583658 0.105289000631594784 -0.0600026001346709026 -0.0127392290430181788 0.185606746681020646 -0.146029102499062757 -0.035236628856851554 0.494463642020458116 0.393139162922157281 0.440366736932692793 -0.267097090638493184 0.145480678130172292 0.380941902198827065 -0.107676958597727371 -0.408938431273406844 0.177410084672891211 0.0851483841469526936 -0.385202883323724099 0.248204980519073803 0.0796150189345870285 -0.048167412239538715 -0.570596016554755447 -0.326360881269845637 0.0034805846719268012 0.227583204665000316 0.39565382264075305 -0.135264412864009614 -0.341423445754042321 0.191578842355264789 0.189870922901048761 -0.487345364775797374 0.0139964824590312533 0.142618001875924688 0.0938651429187691011 -0.383816497606948581 0.135487639409365007 0.251265360744742317 -0.382071708616923356 0.420885891542839952 0.00468512346816874967 -0.425571015011008635 -0.470708339942575216 0.00854415616643!
 394021 0.298523787303021149 -0.311283749677584431 -0.135230854662720279 0.327952925466075185 0.358211698449398486 0.0436491958188225671 -0.504830864988569328 0.366310796329157806 -0.285490748564436347 -0.0400430791403840053 0.0343099477420112861 -0.34074858124809787 0.266057385700989746 -0.0659685760370418844 -0.0248485353448619009 -0.041971485702691845 0.195540425483182012 0.356120514577813407 -0.364622987168045753 -0.329868801987559246 0.178574560024803763 0.184561957317535025 -0.0293825910408284166 -0.173932379519760538 0.136426789459912712 0.570971748588152583 -0.0287276061742493087 -0.565704503389098168 ] ;
+random_gen = *3 ->PRandom(
+seed = 1827 ;
+fixed_seed = 0  )
+;
+seed = 1827 ;
+stage = 100 ;
+n_examples = 6 ;
+inputsize = 3 ;
+targetsize = 2 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 100 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827  )

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/fieldnames	2008-02-28 19:14:47 UTC (rev 8599)
@@ -0,0 +1 @@
+E[test1.E[NLL]]	0

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/global_stats.pmat.metadata/sizes	2008-02-28 19:14:47 UTC (rev 8599)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/fieldnames	2008-02-28 19:14:47 UTC (rev 8599)
@@ -0,0 +1,2 @@
+splitnum	0
+test1.E[NLL]	0

Added: trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/.pytest/PL_NNet_Bags/expected_results/expdir-nnet/split_stats.pmat.metadata/sizes	2008-02-28 19:14:47 UTC (rev 8599)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/plearn_learners/generic/test/NNet/nnet_bags.pyplearn
===================================================================
--- trunk/plearn_learners/generic/test/NNet/nnet_bags.pyplearn	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/nnet_bags.pyplearn	2008-02-28 19:14:47 UTC (rev 8599)
@@ -0,0 +1,42 @@
+# NNet operating on bags.
+
+from plearn.pyplearn import pl
+import plearn
+
+plarg_defaults.operate_on_bags = True
+plarg_defaults.nstages = 100
+plarg_defaults.lr = 0.1
+plarg_defaults.datafile = 'PLEARNDIR:examples/data/test_suite/toy_multi_class_bags.amat'
+
+nnet = pl.NNet(
+        cost_funcs = [ 'NLL' ],
+        operate_on_bags = plargs.operate_on_bags,
+        nhidden = 10,
+        noutputs = 3,
+        nstages = plargs.nstages,
+        batch_size = 0,
+        optimizer = pl.GradientOptimizer(
+            start_learning_rate = plargs.lr,
+            ),
+        output_transfer_func = 'softmax',
+        verbosity = 1,
+        )
+expdir = 'expdir-nnet'# % plearn.utilities.toolkit.date_time_string()
+tester = pl.PTester(
+        expdir = expdir,
+        learner = nnet,
+        dataset = pl.AutoVMatrix( filename = plargs.datafile ),
+        statnames = [ 'E[test1.E[NLL]]' ],
+        splitter = pl.FractionSplitter(
+            splits = TMat(1, 2, [ (0, 1), (0, 1) ]),
+            ),
+        save_initial_tester = False,
+        save_stat_collectors = False,
+        save_test_names = False,
+        save_test_outputs = False,
+        #final_commands = [ 'plearn vmat cat %s/Split0/test1_outputs.pmat' % expdir ],
+        )
+
+def main():
+    return tester
+

Modified: trunk/plearn_learners/generic/test/NNet/pytest.config
===================================================================
--- trunk/plearn_learners/generic/test/NNet/pytest.config	2008-02-28 19:14:17 UTC (rev 8598)
+++ trunk/plearn_learners/generic/test/NNet/pytest.config	2008-02-28 19:14:47 UTC (rev 8599)
@@ -5,31 +5,39 @@
     For each Test instance you declare in a config file, a test will be ran
     by PyTest.
     
-    @ivar name: The name of the Test must uniquely determine the
+      @ivar(name):
+    The name of the Test must uniquely determine the
     test. Among others, it will be used to identify the test's results
-    (.PyTest/I{name}/*_results/) and to report test informations.
-    @type name: String
+    (.PyTest/name/*_results/) and to report test informations.
+      @type(name):
+    String
     
-    @ivar description: The description must provide other users an
+      @ivar(description):
+    The description must provide other users an
     insight of what exactly is the Test testing. You are encouraged
     to used triple quoted strings for indented multi-lines
     descriptions.
-    @type description: String
+      @type(description):
+    String
     
-    @ivar category: The category to which this test belongs. By default, a
+      @ivar(category):
+    The category to which this test belongs. By default, a
     test is considered a 'General' test.
     
     It is not desirable to let an extensive and lengthy test as 'General',
     while one shall refrain abusive use of categories since it is likely
     that only 'General' tests will be ran before most commits...
     
-    @type category: string
+      @type(category):
+    string
     
-    @ivar program: The program to be run by the Test. The program's name
-    PRGNAME is used to lookup for the program in the following manner::
+      @ivar(program):
+    The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner:
     
     1) Look for a local program named PRGNAME
-    2) Look for a plearn-like command (plearn, plearn_tests, ...) named PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named 
+PRGNAME
     3) Call 'which PRGNAME'
     4) Fail
     
@@ -38,39 +46,50 @@
     "compiler = 'pymake'"). If no compiler is provided while the program is
     believed to be compilable, 'pymake' will be assigned by
     default. Arguments to be forwarded to the compiler can be provided as a
-    string through the 'compile_options' keyword argument.  @type program:
-    L{Program}
+    string through the 'compile_options' keyword argument. @type program:
+    Program
     
-    @ivar arguments: The command line arguments to be passed to the program
+      @ivar(arguments):
+    The command line arguments to be passed to the program
     for the test to proceed.
-    @type arguments: String
+      @type(arguments):
+    String
     
-    @ivar resources: A list of resources that are used by your program
+      @ivar(resources):
+    A list of resources that are used by your program
     either in the command line or directly in the code (plearn or pyplearn
-    files, databases, ...).  The elements of the list must be string
+    files, databases, ...). The elements of the list must be string
     representations of the path, absolute or relative, to the resource.
-    @type resources: List of Strings
+      @type(resources):
+    List of Strings
     
-    @ivar precision: The precision (absolute and relative) used when comparing
+      @ivar(precision):
+    The precision (absolute and relative) used when comparing
     floating numbers in the test output (default = 1e-6)
-    @type precision: float
+      @type(precision):
+    float
     
-    @ivar pfileprg: The program to be used for comparing files of psave &
-    vmat formats. It can be either::
-    - "__program__": maps to this test's program if its compilable;
+      @ivar(pfileprg):
+    The program to be used for comparing files of psave &
+    vmat formats. It can be either:
+      - "__program__": maps to this test's program if its compilable;
     maps to 'plearn_tests' otherwise (default);
-    - "__plearn__": always maps to 'plearn_tests' (for when the program
+      - "__plearn__": always maps to 'plearn_tests' (for when the program
     under test is not a version of PLearn);
-    - A Program (see 'program' option) instance
-    - None: if you are sure no files are to be compared.
+      - A Program (see 'program' option) instance
+      - None: if you are sure no files are to be compared.
     
-    @ivar ignored_files_re: Default behaviour of a test is to compare all
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
     files created by running the test. In some case, one may prefer some of
     these files to be ignored.
-    @type ignored_files_re: list of regular expressions
+      @type(ignored_files_re):
+    list of regular expressions
     
-    @ivar disabled: If true, the test will not be ran.
-    @type disabled: bool
+      @ivar(disabled):
+    If true, the test will not be ran.
+      @type(disabled):
+    bool
     
 """
 Test(
@@ -82,10 +101,28 @@
         compiler = "pymake"
         ),
     arguments = "nnet.pyplearn",
-    resources = [ 'nnet.pyplearn' ],
+    resources = [ "nnet.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
     disabled = False,
     runtime = None,
     difftime = None
     )
+
+Test(
+    name = "PL_NNet_Bags",
+    description = "Neural network operating on bags",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "nnet_bags.pyplearn",
+    resources = [ "nnet_bags.pyplearn" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+



From tihocan at mail.berlios.de  Thu Feb 28 20:37:18 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 28 Feb 2008 20:37:18 +0100
Subject: [Plearn-commits] r8600 - trunk/plearn/var
Message-ID: <200802281937.m1SJbIHU012646@sheep.berlios.de>

Author: tihocan
Date: 2008-02-28 20:37:18 +0100 (Thu, 28 Feb 2008)
New Revision: 8600

Modified:
   trunk/plearn/var/SumOverBagsVariable.cc
Log:
Added check in debug to ensure all samples in a bag share the same target

Modified: trunk/plearn/var/SumOverBagsVariable.cc
===================================================================
--- trunk/plearn/var/SumOverBagsVariable.cc	2008-02-28 19:14:47 UTC (rev 8599)
+++ trunk/plearn/var/SumOverBagsVariable.cc	2008-02-28 19:37:18 UTC (rev 8600)
@@ -244,10 +244,24 @@
             vmat->getExample(curpos,input_value,bag_target_and_bag_signal,weight);
         }
         else
-            vmat->getExample(curpos,input_value,bag_target_and_bag_signal,dummy_weight);
+            vmat->getExample(curpos, input_value,
+                             bag_target_and_bag_signal, dummy_weight);
         if (bag_size == 0) {
             // It's the first element: we copy the good target.
-            bag_target << bag_target_and_bag_signal.subVec(0, bag_target_and_bag_signal.length() - 1);
+            bag_target << bag_target_and_bag_signal.subVec(
+                                    0, bag_target_and_bag_signal.length() - 1);
+        } else {
+#ifdef BOUNDCHECK
+            // Safety check: make sure the target is the same for all elements
+            // in the bag.
+            Vec targ = bag_target_and_bag_signal.subVec(
+                    0, bag_target_and_bag_signal.length() - 1);
+            PLASSERT( targ.length() == bag_target.length() );
+            for (int i = 0; i < targ.length(); i++)
+                if (!is_equal(bag_target[i], targ[i]))
+                    PLERROR("In SumOverBagsVariable::fpropOneBag - A bag must "
+                            "have the same target across all elements in it");
+#endif
         }
         if (transpose) {
             // Need to put input_value into input_values, because it uses a separate



From larocheh at mail.berlios.de  Thu Feb 28 23:14:05 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 28 Feb 2008 23:14:05 +0100
Subject: [Plearn-commits] r8601 - trunk/plearn_learners/online
Message-ID: <200802282214.m1SME5Qx027472@sheep.berlios.de>

Author: larocheh
Date: 2008-02-28 23:14:04 +0100 (Thu, 28 Feb 2008)
New Revision: 8601

Modified:
   trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
   trunk/plearn_learners/online/RBMLateralBinomialLayer.h
Log:
Added an option to train a parametric mean-field approximator...


Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-02-28 19:37:18 UTC (rev 8600)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.cc	2008-02-28 22:14:04 UTC (rev 8601)
@@ -60,7 +60,8 @@
     topographic_patch_vradius( 5 ),
     topographic_patch_hradius( 5 ),
     topographic_lateral_weights_init_value( 0. ),
-    do_not_learn_topographic_lateral_weights( false )
+    do_not_learn_topographic_lateral_weights( false ),
+    use_parametric_mean_field( false )
 {
 }
 
@@ -100,6 +101,13 @@
     for( int i=0; i<topographic_lateral_weights.length(); i++ )
         //topographic_lateral_weights[i].clear();
         topographic_lateral_weights[i].fill( topographic_lateral_weights_init_value );
+
+    mean_field_output_weights.clear();
+    for( int i=0; i<mean_field_output_weights.length(); i++ )
+        mean_field_output_weights(i,i) = 1;
+    for( int i=0; i<mean_field_output_bias.length(); i++ )
+        mean_field_output_bias[i] = -0.5;
+    
 }
 
 ////////////////////
@@ -114,8 +122,7 @@
             "before calling generateSample()");
 
     for( int i=0 ; i<size ; i++ )
-        sample[i] = random_gen->binomial_sample( expectation[i] );
-    
+        sample[i] = random_gen->binomial_sample( expectation[i] );    
 }
 
 /////////////////////
@@ -145,73 +152,115 @@
     if( expectation_is_up_to_date )
         return;
 
-    if( temp_output.length() != n_lateral_connections_passes+1 )
+    if( use_parametric_mean_field )
     {
-        temp_output.resize(n_lateral_connections_passes+1);
-        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
-            temp_output[i].resize(size);
-    }       
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                mean_field_input[i] = fastsigmoid( activation[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                mean_field_input[i] = sigmoid( activation[i] );
+        
+        product(pre_sigmoid_mean_field_output, mean_field_output_weights, mean_field_input);
+        pre_sigmoid_mean_field_output += mean_field_output_bias;
 
-    current_temp_output = temp_output[0];
-    temp_output.last() = expectation;
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                expectation[i] = fastsigmoid( pre_sigmoid_mean_field_output[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                expectation[i] = sigmoid( pre_sigmoid_mean_field_output[i] );
 
-    if (use_fast_approximations)
+        // Update mean-field predictor, using KL-divergence gradient:
+        //   dKL/dp_i = -activation[i] - \sum_{j \neq i} p_j + V_i h
+        // where - V_i is the ith row of mean_field_output_weights
+        //       - h is sigmoid(activation)
+
+        real mean_field_i;
+        product(temp_mean_field_gradient, lateral_weights, expectation);
+        temp_mean_field_gradient += activation;
         for( int i=0 ; i<size ; i++ )
-            current_temp_output[i] = fastsigmoid( activation[i] );
+        {
+            mean_field_i = expectation[i];
+            temp_mean_field_gradient[i] = (pre_sigmoid_mean_field_output[i] 
+                                           - temp_mean_field_gradient[i]) 
+                * mean_field_i * (1 - mean_field_i);
+        }
+
+        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient, 
+                                 mean_field_input, -learning_rate );
+        multiplyScaledAdd( temp_mean_field_gradient, 1.0, -learning_rate, mean_field_output_bias);
+    }
     else
-        for( int i=0 ; i<size ; i++ )
-            current_temp_output[i] = sigmoid( activation[i] );
-
-    for( int t=0; t<n_lateral_connections_passes; t++ )
-    {
-        previous_temp_output = current_temp_output;
-        current_temp_output = temp_output[t+1];
-        if( topographic_lateral_weights.length() == 0 )
-            product(dampening_expectation, lateral_weights, previous_temp_output);
+    {        
+        if( temp_output.length() != n_lateral_connections_passes+1 )
+        {
+            temp_output.resize(n_lateral_connections_passes+1);
+            for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+                temp_output[i].resize(size);
+        }       
+        
+        current_temp_output = temp_output[0];
+        temp_output.last() = expectation;
+        
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                current_temp_output[i] = fastsigmoid( activation[i] );
         else
-            productTopoLateralWeights( dampening_expectation, previous_temp_output );
-        dampening_expectation += activation;
-        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                current_temp_output[i] = sigmoid( activation[i] );
+        
+        for( int t=0; t<n_lateral_connections_passes; t++ )
         {
-            if( fast_exact_is_equal( dampening_factor, 0) )
+            previous_temp_output = current_temp_output;
+            current_temp_output = temp_output[t+1];
+            if( topographic_lateral_weights.length() == 0 )
+                product(dampening_expectation, lateral_weights, previous_temp_output);
+            else
+                productTopoLateralWeights( dampening_expectation, previous_temp_output );
+            dampening_expectation += activation;
+            if (use_fast_approximations)
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+                }
+                else
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = 
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                            + dampening_factor * previous_temp_output[i];
+                }
             }
             else
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = 
-                        (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
-                        + dampening_factor * previous_temp_output[i];
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = sigmoid( dampening_expectation[i] );
+                }
+                else
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = 
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                            + dampening_factor * previous_temp_output[i];
+                }
             }
-        }
-        else
-        {
-            if( fast_exact_is_equal( dampening_factor, 0) )
+            if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) && 
+                dist(current_temp_output, previous_temp_output,2)/size < mean_field_precision_threshold )
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = sigmoid( dampening_expectation[i] );
+                expectation << current_temp_output;
+                break;
             }
-            else
-            {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = 
-                        (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
-                        + dampening_factor * previous_temp_output[i];
-            }
+            //cout << sqrt(max(square(current_temp_output-previous_temp_output))) << " ";
+            //cout << dist(current_temp_output, previous_temp_output,2)/current_temp_output.length() << " ";
         }
-        if( !fast_exact_is_equal(mean_field_precision_threshold, 0.) && 
-            dist(current_temp_output, previous_temp_output,2)/size < mean_field_precision_threshold )
-        {
-            expectation << current_temp_output;
-            break;
-        }
-        //cout << sqrt(max(square(current_temp_output-previous_temp_output))) << " ";
-        //cout << dist(current_temp_output, previous_temp_output,2)/current_temp_output.length() << " ";
+        //cout << endl;
+        //expectation << current_temp_output;
     }
-    //cout << endl;
-    //expectation << current_temp_output;
     expectation_is_up_to_date = true;
 }
 
@@ -225,78 +274,87 @@
 
     PLASSERT( expectations.width() == size
               && expectations.length() == batch_size );
-    dampening_expectations.resize( batch_size, size );
 
-    if( temp_outputs.length() != n_lateral_connections_passes+1 )
+    if( use_parametric_mean_field )
     {
-        temp_outputs.resize(n_lateral_connections_passes+1);
-        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
-            temp_outputs[i].resize( batch_size, size);
-    }       
-
-    current_temp_outputs = temp_outputs[0];
-    temp_outputs.last() = expectations;
-
-    if (use_fast_approximations)
-        for (int k = 0; k < batch_size; k++)
-            for (int i = 0 ; i < size ; i++)
-                current_temp_outputs(k, i) = fastsigmoid(activations(k, i));
+        PLERROR("RBMLateralBinomialLayer::computeExpectations(): use_parametric_mean_field=true "
+            "not implemented yet.");
+    }
     else
-        for (int k = 0; k < batch_size; k++)
-            for (int i = 0 ; i < size ; i++)
-                current_temp_outputs(k, i) = sigmoid(activations(k, i));
-
-    for( int t=0; t<n_lateral_connections_passes; t++ )
     {
-        previous_temp_outputs = current_temp_outputs;
-        current_temp_outputs = temp_outputs[t+1];
-        if( topographic_lateral_weights.length() == 0 )
-            productTranspose(dampening_expectations, previous_temp_outputs, 
-                             lateral_weights);
+        dampening_expectations.resize( batch_size, size );
+        
+        if( temp_outputs.length() != n_lateral_connections_passes+1 )
+        {
+            temp_outputs.resize(n_lateral_connections_passes+1);
+            for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+                temp_outputs[i].resize( batch_size, size);
+        }       
+        
+        current_temp_outputs = temp_outputs[0];
+        temp_outputs.last() = expectations;
+        
+        if (use_fast_approximations)
+            for (int k = 0; k < batch_size; k++)
+                for (int i = 0 ; i < size ; i++)
+                    current_temp_outputs(k, i) = fastsigmoid(activations(k, i));
         else
-            for( int b = 0; b<dampening_expectations.length(); b++)
-                productTopoLateralWeights( dampening_expectations(b), 
-                                           previous_temp_outputs(b) );
+            for (int k = 0; k < batch_size; k++)
+                for (int i = 0 ; i < size ; i++)
+                    current_temp_outputs(k, i) = sigmoid(activations(k, i));
 
-        dampening_expectations += activations;
-        if (use_fast_approximations)
+        for( int t=0; t<n_lateral_connections_passes; t++ )
         {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = 
-                            fastsigmoid( dampening_expectations(k, i) );
-            }
+            previous_temp_outputs = current_temp_outputs;
+            current_temp_outputs = temp_outputs[t+1];
+            if( topographic_lateral_weights.length() == 0 )
+                productTranspose(dampening_expectations, previous_temp_outputs, 
+                                 lateral_weights);
             else
+                for( int b = 0; b<dampening_expectations.length(); b++)
+                    productTopoLateralWeights( dampening_expectations(b), 
+                                               previous_temp_outputs(b) );
+
+            dampening_expectations += activations;
+            if (use_fast_approximations)
             {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = (1-dampening_factor)
-                            * fastsigmoid( dampening_expectations(k, i) ) 
-                            + dampening_factor * previous_temp_outputs(k, i);
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = 
+                                fastsigmoid( dampening_expectations(k, i) );
+                }
+                else
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = (1-dampening_factor)
+                                * fastsigmoid( dampening_expectations(k, i) ) 
+                                + dampening_factor * previous_temp_outputs(k, i);
+                }
             }
-        }
-        else
-        {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = 
-                            sigmoid( dampening_expectations(k, i) );
-            }
             else
             {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = (1-dampening_factor) 
-                            * sigmoid( dampening_expectations(k, i) ) 
-                            + dampening_factor * previous_temp_outputs(k, i);
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = 
+                                sigmoid( dampening_expectations(k, i) );
+                }
+                else
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = (1-dampening_factor) 
+                                * sigmoid( dampening_expectations(k, i) ) 
+                                + dampening_factor * previous_temp_outputs(k, i);
+                }
             }
         }
+        //expectations << current_temp_outputs;
     }
-    //expectations << current_temp_outputs;
     expectations_are_up_to_date = true;
 }
 
@@ -310,60 +368,83 @@
 
     add(bias, input, bias_plus_input);
 
-    if( temp_output.length() != n_lateral_connections_passes+1 )
+    if( use_parametric_mean_field )
     {
-        temp_output.resize(n_lateral_connections_passes+1);
-        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
-            temp_output[i].resize(size);
-    }       
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                mean_field_input[i] = fastsigmoid( bias_plus_input[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                mean_field_input[i] = sigmoid( bias_plus_input[i] );
+        
+        product(pre_sigmoid_mean_field_output, mean_field_output_weights, mean_field_input);
+        pre_sigmoid_mean_field_output += mean_field_output_bias;
 
-    temp_output.last() = output;
-    current_temp_output = temp_output[0];
-
-    if (use_fast_approximations)
-        for( int i=0 ; i<size ; i++ )
-            current_temp_output[i] = fastsigmoid( bias_plus_input[i] );
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                output[i] = fastsigmoid( pre_sigmoid_mean_field_output[i] );
+        else
+            for( int i=0 ; i<size ; i++ )
+                output[i] = sigmoid( pre_sigmoid_mean_field_output[i] );
+    }
     else
-        for( int i=0 ; i<size ; i++ )
-            current_temp_output[i] = sigmoid( bias_plus_input[i] );
+    {        
 
-    for( int t=0; t<n_lateral_connections_passes; t++ )
-    {
-        previous_temp_output = current_temp_output;
-        current_temp_output = temp_output[t+1];
-        if( topographic_lateral_weights.length() == 0 )
-            product(dampening_expectation, lateral_weights, previous_temp_output);
+        if( temp_output.length() != n_lateral_connections_passes+1 )
+        {
+            temp_output.resize(n_lateral_connections_passes+1);
+            for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+                temp_output[i].resize(size);
+        }       
+
+        temp_output.last() = output;
+        current_temp_output = temp_output[0];
+
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                current_temp_output[i] = fastsigmoid( bias_plus_input[i] );
         else
-            productTopoLateralWeights( dampening_expectation, previous_temp_output );
-        dampening_expectation += bias_plus_input;
-        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                current_temp_output[i] = sigmoid( bias_plus_input[i] );
+
+        for( int t=0; t<n_lateral_connections_passes; t++ )
         {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
-            }
+            previous_temp_output = current_temp_output;
+            current_temp_output = temp_output[t+1];
+            if( topographic_lateral_weights.length() == 0 )
+                product(dampening_expectation, lateral_weights, previous_temp_output);
             else
+                productTopoLateralWeights( dampening_expectation, previous_temp_output );
+            dampening_expectation += bias_plus_input;
+            if (use_fast_approximations)
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = 
-                        (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
-                        + dampening_factor * previous_temp_output[i];
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+                }
+                else
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = 
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                            + dampening_factor * previous_temp_output[i];
+                }
             }
-        }
-        else
-        {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = sigmoid( dampening_expectation[i] );
-            }
             else
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = 
-                        (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
-                        + dampening_factor * previous_temp_output[i];
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = sigmoid( dampening_expectation[i] );
+                }
+                else
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = 
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                            + dampening_factor * previous_temp_output[i];
+                }
             }
         }
     }
@@ -374,80 +455,89 @@
     int mbatch_size = inputs.length();
     PLASSERT( inputs.width() == size );
     outputs.resize( mbatch_size, size );
+
     dampening_expectations.resize( mbatch_size, size );
 
-    if(bias_plus_inputs.length() != inputs.length() ||
-       bias_plus_inputs.width() != inputs.width())
-        bias_plus_inputs.resize(inputs.length(), inputs.width());
-    bias_plus_inputs << inputs;
-    bias_plus_inputs += bias;
-
-    if( temp_outputs.length() != n_lateral_connections_passes+1 )
+    if( use_parametric_mean_field )
     {
-        temp_outputs.resize(n_lateral_connections_passes+1);
-        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
-            temp_outputs[i].resize(mbatch_size,size);
-    }       
+        PLERROR("RBMLateralBinomialLayer::fprop: use_parametric_mean_field = true "
+            "not implemented yet for batch mode.");
+    }
+    else
+    {
+        if(bias_plus_inputs.length() != inputs.length() ||
+           bias_plus_inputs.width() != inputs.width())
+            bias_plus_inputs.resize(inputs.length(), inputs.width());
+        bias_plus_inputs << inputs;
+        bias_plus_inputs += bias;
 
-    temp_outputs.last() = outputs;
-    current_temp_outputs = temp_outputs[0];
+        if( temp_outputs.length() != n_lateral_connections_passes+1 )
+        {
+            temp_outputs.resize(n_lateral_connections_passes+1);
+            for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+                temp_outputs[i].resize(mbatch_size,size);
+        }       
 
-    if (use_fast_approximations)
-        for( int k = 0; k < mbatch_size; k++ )
-            for( int i = 0; i < size; i++ )
-                current_temp_outputs(k,i) = fastsigmoid( bias_plus_inputs(k,i) );
-    else
-        for( int k = 0; k < mbatch_size; k++ )
-            for( int i = 0; i < size; i++ )
-                current_temp_outputs(k,i) = sigmoid( bias_plus_inputs(k,i) );
+        temp_outputs.last() = outputs;
+        current_temp_outputs = temp_outputs[0];
 
-    for( int t=0; t<n_lateral_connections_passes; t++ )
-    {
-        previous_temp_outputs = current_temp_outputs;
-        current_temp_outputs = temp_outputs[t+1];
-        if( topographic_lateral_weights.length() == 0 )
-            productTranspose(dampening_expectations, previous_temp_outputs, 
-                             lateral_weights);
+        if (use_fast_approximations)
+            for( int k = 0; k < mbatch_size; k++ )
+                for( int i = 0; i < size; i++ )
+                    current_temp_outputs(k,i) = fastsigmoid( bias_plus_inputs(k,i) );
         else
-            for( int b = 0; b<dampening_expectations.length(); b++)
-                productTopoLateralWeights( dampening_expectations(b), 
-                                           previous_temp_outputs(b) );
+            for( int k = 0; k < mbatch_size; k++ )
+                for( int i = 0; i < size; i++ )
+                    current_temp_outputs(k,i) = sigmoid( bias_plus_inputs(k,i) );
 
-        dampening_expectations += bias_plus_inputs;
-        if (use_fast_approximations)
+        for( int t=0; t<n_lateral_connections_passes; t++ )
         {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = 
-                            fastsigmoid( dampening_expectations(k, i) );
-            }
+            previous_temp_outputs = current_temp_outputs;
+            current_temp_outputs = temp_outputs[t+1];
+            if( topographic_lateral_weights.length() == 0 )
+                productTranspose(dampening_expectations, previous_temp_outputs, 
+                                 lateral_weights);
             else
+                for( int b = 0; b<dampening_expectations.length(); b++)
+                    productTopoLateralWeights( dampening_expectations(b), 
+                                               previous_temp_outputs(b) );
+
+            dampening_expectations += bias_plus_inputs;
+            if (use_fast_approximations)
             {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = (1-dampening_factor)
-                            * fastsigmoid( dampening_expectations(k, i) ) 
-                            + dampening_factor * previous_temp_outputs(k, i);
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = 
+                                fastsigmoid( dampening_expectations(k, i) );
+                }
+                else
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = (1-dampening_factor)
+                                * fastsigmoid( dampening_expectations(k, i) ) 
+                                + dampening_factor * previous_temp_outputs(k, i);
+                }
             }
-        }
-        else
-        {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = 
-                            sigmoid( dampening_expectations(k, i) );
-            }
             else
             {
-                for(int k = 0; k < batch_size; k++)
-                    for( int i=0 ; i<size ; i++ )
-                        current_temp_outputs(k, i) = (1-dampening_factor)
-                            * sigmoid( dampening_expectations(k, i) ) 
-                            + dampening_factor * previous_temp_outputs(k, i);
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = 
+                                sigmoid( dampening_expectations(k, i) );
+                }
+                else
+                {
+                    for(int k = 0; k < batch_size; k++)
+                        for( int i=0 ; i<size ; i++ )
+                            current_temp_outputs(k, i) = (1-dampening_factor)
+                                * sigmoid( dampening_expectations(k, i) ) 
+                                + dampening_factor * previous_temp_outputs(k, i);
+                }
             }
         }
     }
@@ -462,60 +552,69 @@
 
     add(rbm_bias, input, bias_plus_input);
 
-        if( temp_output.length() != n_lateral_connections_passes+1 )
+    if( use_parametric_mean_field )
     {
-        temp_output.resize(n_lateral_connections_passes+1);
-        for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
-            temp_output[i].resize(size);
-    }       
+        PLERROR("RBMLateralBinomialLayer::fprop: use_parametric_mean_field = true "
+            "not implemented yet for rbm_bias input.");
+    }
+    else
+    {
 
-    temp_output.last() = output;
-    current_temp_output = temp_output[0];
+        if( temp_output.length() != n_lateral_connections_passes+1 )
+        {
+            temp_output.resize(n_lateral_connections_passes+1);
+            for( int i=0 ; i<n_lateral_connections_passes+1 ; i++ )
+                temp_output[i].resize(size);
+        }       
 
-    if (use_fast_approximations)
-        for( int i=0 ; i<size ; i++ )
-            current_temp_output[i] = fastsigmoid( bias_plus_input[i] );
-    else
-        for( int i=0 ; i<size ; i++ )
-            current_temp_output[i] = sigmoid( bias_plus_input[i] );
+        temp_output.last() = output;
+        current_temp_output = temp_output[0];
 
-    for( int t=0; t<n_lateral_connections_passes; t++ )
-    {
-        previous_temp_output = current_temp_output;
-        current_temp_output = temp_output[t+1];
-        if( topographic_lateral_weights.length() == 0 )
-            product(dampening_expectation, lateral_weights, previous_temp_output);
+        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                current_temp_output[i] = fastsigmoid( bias_plus_input[i] );
         else
-            productTopoLateralWeights( dampening_expectation, previous_temp_output );
-        dampening_expectation += bias_plus_input;
-        if (use_fast_approximations)
+            for( int i=0 ; i<size ; i++ )
+                current_temp_output[i] = sigmoid( bias_plus_input[i] );
+
+        for( int t=0; t<n_lateral_connections_passes; t++ )
         {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
-            }
+            previous_temp_output = current_temp_output;
+            current_temp_output = temp_output[t+1];
+            if( topographic_lateral_weights.length() == 0 )
+                product(dampening_expectation, lateral_weights, previous_temp_output);
             else
+                productTopoLateralWeights( dampening_expectation, previous_temp_output );
+            dampening_expectation += bias_plus_input;
+            if (use_fast_approximations)
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = 
-                        (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
-                        + dampening_factor * previous_temp_output[i];
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = fastsigmoid( dampening_expectation[i] );
+                }
+                else
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = 
+                            (1-dampening_factor) * fastsigmoid( dampening_expectation[i] ) 
+                            + dampening_factor * previous_temp_output[i];
+                }
             }
-        }
-        else
-        {
-            if( fast_exact_is_equal( dampening_factor, 0) )
-            {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = sigmoid( dampening_expectation[i] );
-            }
             else
             {
-                for( int i=0 ; i<size ; i++ )
-                    current_temp_output[i] = 
-                        (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
-                        + dampening_factor * previous_temp_output[i];
+                if( fast_exact_is_equal( dampening_factor, 0) )
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = sigmoid( dampening_expectation[i] );
+                }
+                else
+                {
+                    for( int i=0 ; i<size ; i++ )
+                        current_temp_output[i] = 
+                            (1-dampening_factor) * sigmoid( dampening_expectation[i] ) 
+                            + dampening_factor * previous_temp_output[i];
+                }
             }
         }
     }
@@ -763,120 +862,145 @@
     //if( momentum != 0. )
     //    bias_inc.resize( size );
 
-    temp_input_gradient.clear();
-    temp_mean_field_gradient << output_gradient;
-    current_temp_output = output;
-    lateral_weights_gradient.clear();
-    for( int i=0; i<topographic_lateral_weights_gradient.length(); i++)
-        topographic_lateral_weights_gradient[i].clear();
-
-    real output_i;
-    for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+    if( use_parametric_mean_field )
     {
+        real mean_field_i;
         for( int i=0 ; i<size ; i++ )
         {
-            output_i = current_temp_output[i];
-
-            // Contribution from the mean field approximation
-            temp_mean_field_gradient2[i] =  (1-dampening_factor)*
-                output_i * (1-output_i) * temp_mean_field_gradient[i];
-            
-            // Contribution from the dampening
-            temp_mean_field_gradient[i] *= dampening_factor;
+            mean_field_i = output[i];
+            temp_mean_field_gradient[i] = output_gradient[i] * mean_field_i * (1 - mean_field_i);
         }
 
-        // Input gradient contribution
-        temp_input_gradient += temp_mean_field_gradient2;
+        transposeProductAcc( input_gradient, mean_field_output_weights, temp_mean_field_gradient );
 
-        // Lateral weights gradient contribution
-        if( topographic_lateral_weights.length() == 0)
+        externalProductScaleAcc( mean_field_output_weights, temp_mean_field_gradient, 
+                                 mean_field_input, -learning_rate );
+        multiplyScaledAdd( temp_mean_field_gradient, 1.0, -learning_rate, mean_field_output_bias);
+
+        real input_mean_field_i;
+        for( int i=0 ; i<size ; i++ )
         {
-            externalSymetricProductAcc( lateral_weights_gradient, 
-                                        temp_mean_field_gradient2,
-                                        temp_output[t] );
-            
-            transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
-                                temp_mean_field_gradient2);
+            input_mean_field_i = mean_field_input[i];
+            input_gradient[i] = input_gradient[i] * input_mean_field_i * (1 - input_mean_field_i);
         }
-        else
-        {
-            productTopoLateralWeightsGradients( 
-                temp_output[t],
-                temp_mean_field_gradient,
-                temp_mean_field_gradient2,
-                topographic_lateral_weights_gradient);
-        }
-
-        current_temp_output = temp_output[t];
     }
-    
-    for( int i=0 ; i<size ; i++ )
+    else
     {
-        output_i = current_temp_output[i];
-        temp_mean_field_gradient[i] *= output_i * (1-output_i);
-    }
+        temp_input_gradient.clear();
+        temp_mean_field_gradient << output_gradient;
+        current_temp_output = output;
+        lateral_weights_gradient.clear();
+        for( int i=0; i<topographic_lateral_weights_gradient.length(); i++)
+            topographic_lateral_weights_gradient[i].clear();
 
-    temp_input_gradient += temp_mean_field_gradient;
+        real output_i;
+        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+        {
+            for( int i=0 ; i<size ; i++ )
+            {
+                output_i = current_temp_output[i];
 
-    input_gradient += temp_input_gradient;
+                // Contribution from the mean field approximation
+                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                    output_i * (1-output_i) * temp_mean_field_gradient[i];
+            
+                // Contribution from the dampening
+                temp_mean_field_gradient[i] *= dampening_factor;
+            }
 
-    // Update bias
-    real in_grad_i;
-    for( int i=0 ; i<size ; i++ )
-    {
-        in_grad_i = temp_input_gradient[i];
-        if( momentum == 0. )
+            // Input gradient contribution
+            temp_input_gradient += temp_mean_field_gradient2;
+
+            // Lateral weights gradient contribution
+            if( topographic_lateral_weights.length() == 0)
+            {
+                externalSymetricProductAcc( lateral_weights_gradient, 
+                                            temp_mean_field_gradient2,
+                                            temp_output[t] );
+            
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                    temp_mean_field_gradient2);
+            }
+            else
+            {
+                productTopoLateralWeightsGradients( 
+                    temp_output[t],
+                    temp_mean_field_gradient,
+                    temp_mean_field_gradient2,
+                    topographic_lateral_weights_gradient);
+            }
+
+            current_temp_output = temp_output[t];
+        }
+    
+        for( int i=0 ; i<size ; i++ )
         {
-            // update the bias: bias -= learning_rate * input_gradient
-            bias[i] -= learning_rate * in_grad_i;
+            output_i = current_temp_output[i];
+            temp_mean_field_gradient[i] *= output_i * (1-output_i);
         }
-        else
+
+        temp_input_gradient += temp_mean_field_gradient;
+
+        input_gradient += temp_input_gradient;
+
+        // Update bias
+        real in_grad_i;
+        for( int i=0 ; i<size ; i++ )
         {
-            // The update rule becomes:
-            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
-            // bias += bias_inc
-            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
-            bias[i] += bias_inc[i];
+            in_grad_i = temp_input_gradient[i];
+            if( momentum == 0. )
+            {
+                // update the bias: bias -= learning_rate * input_gradient
+                bias[i] -= learning_rate * in_grad_i;
+            }
+            else
+            {
+                // The update rule becomes:
+                // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+                // bias += bias_inc
+                bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+                bias[i] += bias_inc[i];
+            }
         }
-    }
 
-    if( topographic_lateral_weights.length() == 0)
-    {
-        if( momentum == 0. )
+        if( topographic_lateral_weights.length() == 0)
         {
-            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
-                               lateral_weights);
+            if( momentum == 0. )
+            {
+                multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                                   lateral_weights);
+            }
+            else
+            {
+                multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
+                                   lateral_weights_inc);
+                lateral_weights += lateral_weights_inc;
+            }
         }
         else
         {
-            multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
-                               lateral_weights_inc);
-            lateral_weights += lateral_weights_inc;
+            if( !do_not_learn_topographic_lateral_weights )
+            {
+                if( momentum == 0. )
+                    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                           -learning_rate,
+                                           topographic_lateral_weights[i]);
+            
+                else
+                    PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                            "topographic weights");
+            }
         }
-    }
-    else
-    {
-        if( !do_not_learn_topographic_lateral_weights )
+
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
         {
-            if( momentum == 0. )
-                for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
-                                       -learning_rate,
-                                       topographic_lateral_weights[i]);
-            
-            else
-                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
-                        "topographic weights");
+            real *d = lateral_weights.data();        
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
         }
     }
-
-    // Set diagonal to 0
-    if( lateral_weights.length() != 0 )
-    {
-        real *d = lateral_weights.data();        
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
-            *d = 0;
-    }
 }
 
 void RBMLateralBinomialLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
@@ -911,114 +1035,123 @@
 
     // We use the average gradient over the mini-batch.
     real avg_lr = learning_rate / inputs.length();
-    lateral_weights_gradient.clear();
-    real output_i;
-    for (int j = 0; j < mbatch_size; j++)
+
+    if( use_parametric_mean_field )
     {
-        temp_input_gradient.clear();
-        temp_mean_field_gradient << output_gradients(j);
-        current_temp_output = outputs(j);
-
-        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+        PLERROR("RBMLateralBinomialLayer::bpropUpdate: use_parametric_mean_field=true "
+            "not implemented yet for batch mode.");
+    }
+    else
+    {
+        lateral_weights_gradient.clear();
+        real output_i;
+        for (int j = 0; j < mbatch_size; j++)
         {
+            temp_input_gradient.clear();
+            temp_mean_field_gradient << output_gradients(j);
+            current_temp_output = outputs(j);
 
-            for( int i=0 ; i<size ; i++ )
+            for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
             {
-                output_i = current_temp_output[i];
+
+                for( int i=0 ; i<size ; i++ )
+                {
+                    output_i = current_temp_output[i];
                 
-                // Contribution from the mean field approximation
-                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
-                    output_i * (1-output_i) * temp_mean_field_gradient[i];
+                    // Contribution from the mean field approximation
+                    temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                        output_i * (1-output_i) * temp_mean_field_gradient[i];
                 
-                // Contribution from the dampening
-                temp_mean_field_gradient[i] *= dampening_factor;
-            }
+                    // Contribution from the dampening
+                    temp_mean_field_gradient[i] *= dampening_factor;
+                }
             
-            // Input gradient contribution
-            temp_input_gradient += temp_mean_field_gradient2;
+                // Input gradient contribution
+                temp_input_gradient += temp_mean_field_gradient2;
             
-            // Lateral weights gradient contribution
-            if( topographic_lateral_weights.length() == 0)
-            {
+                // Lateral weights gradient contribution
+                if( topographic_lateral_weights.length() == 0)
+                {
                 
-                externalSymetricProductAcc( lateral_weights_gradient, 
-                                            temp_mean_field_gradient2,
-                                            temp_outputs[t](j) );
+                    externalSymetricProductAcc( lateral_weights_gradient, 
+                                                temp_mean_field_gradient2,
+                                                temp_outputs[t](j) );
                 
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
-                                    temp_mean_field_gradient2);
+                    transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                        temp_mean_field_gradient2);
+                }
+                else
+                {
+                    productTopoLateralWeightsGradients( 
+                        temp_outputs[t](j),
+                        temp_mean_field_gradient,
+                        temp_mean_field_gradient2,
+                        topographic_lateral_weights_gradient);
+                }
+
+                current_temp_output = temp_outputs[t](j);
             }
-            else
+    
+            for( int i=0 ; i<size ; i++ )
             {
-                productTopoLateralWeightsGradients( 
-                    temp_outputs[t](j),
-                    temp_mean_field_gradient,
-                    temp_mean_field_gradient2,
-                    topographic_lateral_weights_gradient);
+                output_i = current_temp_output[i];
+                temp_mean_field_gradient[i] *= output_i * (1-output_i);
             }
 
-            current_temp_output = temp_outputs[t](j);
+            temp_input_gradient += temp_mean_field_gradient;
+        
+            input_gradients(j) += temp_input_gradient;
+
+            // Update bias
+            real in_grad_i;
+            for( int i=0 ; i<size ; i++ )
+            {
+                in_grad_i = temp_input_gradient[i];
+                if( momentum == 0. )
+                {
+                    // update the bias: bias -= learning_rate * input_gradient
+                    bias[i] -= avg_lr * in_grad_i;
+                }
+                else
+                    PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                            "momentum with mini-batches");
+            }        
         }
-    
-        for( int i=0 ; i<size ; i++ )
-        {
-            output_i = current_temp_output[i];
-            temp_mean_field_gradient[i] *= output_i * (1-output_i);
-        }
 
-        temp_input_gradient += temp_mean_field_gradient;
-        
-        input_gradients(j) += temp_input_gradient;
-
-        // Update bias
-        real in_grad_i;
-        for( int i=0 ; i<size ; i++ )
+        if( topographic_lateral_weights.length() == 0)
         {
-            in_grad_i = temp_input_gradient[i];
             if( momentum == 0. )
-            {
-                // update the bias: bias -= learning_rate * input_gradient
-                bias[i] -= avg_lr * in_grad_i;
-            }
+                multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                                   lateral_weights);
             else
                 PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
                         "momentum with mini-batches");
-        }        
-    }
-
-    if( topographic_lateral_weights.length() == 0)
-    {
-        if( momentum == 0. )
-            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
-                               lateral_weights);
+        }
         else
-            PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
-                    "momentum with mini-batches");
-    }
-    else
-    {
-        if( !do_not_learn_topographic_lateral_weights )
         {
-            if( momentum == 0. )
-                for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
-                                       -learning_rate,
-                                       topographic_lateral_weights[i]);
+            if( !do_not_learn_topographic_lateral_weights )
+            {
+                if( momentum == 0. )
+                    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                           -learning_rate,
+                                           topographic_lateral_weights[i]);
             
-            else
-                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
-                        "topographic weights");
+                else
+                    PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                            "topographic weights");
+            }
+
         }
 
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
+        {
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
+        }
     }
-
-    // Set diagonal to 0
-    if( lateral_weights.length() != 0 )
-    {
-        real *d = lateral_weights.data();
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
-            *d = 0;
-    }
 }
 
 
@@ -1035,101 +1168,109 @@
     input_gradient.resize( size );
     rbm_bias_gradient.resize( size );
 
-    temp_input_gradient.clear();
-    temp_mean_field_gradient << output_gradient;
-    current_temp_output = output;
-    lateral_weights_gradient.clear();
-
-    real output_i;
-    for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+    if( use_parametric_mean_field )
     {
+        PLERROR("RBMLateralBinomialLayer::bpropUpdate: use_parametric_mean_field=true "
+                "not implemented yet for bias input.");
+    }
+    else
+    {
+        temp_input_gradient.clear();
+        temp_mean_field_gradient << output_gradient;
+        current_temp_output = output;
+        lateral_weights_gradient.clear();
 
-        for( int i=0 ; i<size ; i++ )
+        real output_i;
+        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
         {
-            output_i = current_temp_output[i];
 
-            // Contribution from the mean field approximation
-            temp_mean_field_gradient2[i] =  (1-dampening_factor)*
-                output_i * (1-output_i) * temp_mean_field_gradient[i];
+            for( int i=0 ; i<size ; i++ )
+            {
+                output_i = current_temp_output[i];
+
+                // Contribution from the mean field approximation
+                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                    output_i * (1-output_i) * temp_mean_field_gradient[i];
             
-            // Contribution from the dampening
-            temp_mean_field_gradient[i] *= dampening_factor;
-        }
+                // Contribution from the dampening
+                temp_mean_field_gradient[i] *= dampening_factor;
+            }
 
-        // Input gradient contribution
-        temp_input_gradient += temp_mean_field_gradient2;
+            // Input gradient contribution
+            temp_input_gradient += temp_mean_field_gradient2;
 
-        // Lateral weights gradient contribution
-        if( topographic_lateral_weights.length() == 0)
-        {
+            // Lateral weights gradient contribution
+            if( topographic_lateral_weights.length() == 0)
+            {
 
-            externalSymetricProductAcc( lateral_weights_gradient, 
-                                        temp_mean_field_gradient2,
-                                        temp_output[t] );
+                externalSymetricProductAcc( lateral_weights_gradient, 
+                                            temp_mean_field_gradient2,
+                                            temp_output[t] );
             
-            transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
-                                temp_mean_field_gradient2);
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                    temp_mean_field_gradient2);
+            }
+            else
+            {
+                productTopoLateralWeightsGradients( 
+                    temp_output[t],
+                    temp_mean_field_gradient,
+                    temp_mean_field_gradient2,
+                    topographic_lateral_weights_gradient);
+            }
+
+            current_temp_output = temp_output[t];
         }
-        else
+    
+        for( int i=0 ; i<size ; i++ )
         {
-            productTopoLateralWeightsGradients( 
-                temp_output[t],
-                temp_mean_field_gradient,
-                temp_mean_field_gradient2,
-                topographic_lateral_weights_gradient);
+            output_i = current_temp_output[i];
+            temp_mean_field_gradient[i] *= output_i * (1-output_i);
         }
 
-        current_temp_output = temp_output[t];
-    }
-    
-    for( int i=0 ; i<size ; i++ )
-    {
-        output_i = current_temp_output[i];
-        temp_mean_field_gradient[i] *= output_i * (1-output_i);
-    }
+        temp_input_gradient += temp_mean_field_gradient;
 
-    temp_input_gradient += temp_mean_field_gradient;
+        input_gradient << temp_input_gradient;
+        rbm_bias_gradient << temp_input_gradient;
 
-    input_gradient << temp_input_gradient;
-    rbm_bias_gradient << temp_input_gradient;
-
-    if( topographic_lateral_weights.length() == 0)
-    {
-        if( momentum == 0. )
+        if( topographic_lateral_weights.length() == 0)
         {
-            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
-                               lateral_weights);
+            if( momentum == 0. )
+            {
+                multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                                   lateral_weights);
+            }
+            else
+            {
+                multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
+                                   lateral_weights_inc);
+                lateral_weights += lateral_weights_inc;
+            }
         }
         else
         {
-            multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
-                               lateral_weights_inc);
-            lateral_weights += lateral_weights_inc;
+            if( !do_not_learn_topographic_lateral_weights )
+            {
+                if( momentum == 0. )
+                    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                           -learning_rate,
+                                           topographic_lateral_weights[i]);
+            
+                else
+                    PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                            "topographic weights");
+            }
         }
-    }
-    else
-    {
-        if( !do_not_learn_topographic_lateral_weights )
+        
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
         {
-            if( momentum == 0. )
-                for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
-                                       -learning_rate,
-                                       topographic_lateral_weights[i]);
-            
-            else
-                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
-                        "topographic weights");
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
         }
     }
-        
-    // Set diagonal to 0
-    if( lateral_weights.length() != 0 )
-    {
-        real *d = lateral_weights.data();
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
-            *d = 0;
-    }
 }
 
 real RBMLateralBinomialLayer::fpropNLL(const Vec& target)
@@ -1186,97 +1327,105 @@
     bias_gradient.resize( size );
     bias_gradient.clear();
 
-    // bias_gradient = expectation - target
-    substract(expectation, target, temp_mean_field_gradient);
+    if( use_parametric_mean_field )
+    {
+        PLERROR("RBMLateralBinomialLayer::bpropNLL: use_parametric_mean_field=true "
+                "not implemented yet.");
+    }
+    else
+    {
+        // bias_gradient = expectation - target
+        substract(expectation, target, temp_mean_field_gradient);
 
-    current_temp_output = expectation;
-    lateral_weights_gradient.clear();
+        current_temp_output = expectation;
+        lateral_weights_gradient.clear();
 
-    real output_i;
-    for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
-    {
-        for( int i=0 ; i<size ; i++ )
+        real output_i;
+        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
         {
-            output_i = current_temp_output[i];
+            for( int i=0 ; i<size ; i++ )
+            {
+                output_i = current_temp_output[i];
 
-            // Contribution from the mean field approximation
-            temp_mean_field_gradient2[i] =  (1-dampening_factor)*
-                output_i * (1-output_i) * temp_mean_field_gradient[i];
+                // Contribution from the mean field approximation
+                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                    output_i * (1-output_i) * temp_mean_field_gradient[i];
             
-            // Contribution from the dampening
-            temp_mean_field_gradient[i] *= dampening_factor;
-        }
+                // Contribution from the dampening
+                temp_mean_field_gradient[i] *= dampening_factor;
+            }
 
-        // Input gradient contribution
-        bias_gradient += temp_mean_field_gradient2;
+            // Input gradient contribution
+            bias_gradient += temp_mean_field_gradient2;
 
-        // Lateral weights gradient contribution
-        if( topographic_lateral_weights.length() == 0)
-        {
-            externalSymetricProductAcc( lateral_weights_gradient, 
-                                        temp_mean_field_gradient2,
-                                        temp_output[t] );
+            // Lateral weights gradient contribution
+            if( topographic_lateral_weights.length() == 0)
+            {
+                externalSymetricProductAcc( lateral_weights_gradient, 
+                                            temp_mean_field_gradient2,
+                                            temp_output[t] );
             
-            transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
-                                temp_mean_field_gradient2);
+                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                    temp_mean_field_gradient2);
+            }
+            else
+            {
+                productTopoLateralWeightsGradients( 
+                    temp_output[t],
+                    temp_mean_field_gradient,
+                    temp_mean_field_gradient2,
+                    topographic_lateral_weights_gradient);
+            }
+
+            current_temp_output = temp_output[t];
         }
-        else
+    
+        for( int i=0 ; i<size ; i++ )
         {
-            productTopoLateralWeightsGradients( 
-                temp_output[t],
-                temp_mean_field_gradient,
-                temp_mean_field_gradient2,
-                topographic_lateral_weights_gradient);
+            output_i = current_temp_output[i];
+            temp_mean_field_gradient[i] *= output_i * (1-output_i);
         }
 
-        current_temp_output = temp_output[t];
-    }
-    
-    for( int i=0 ; i<size ; i++ )
-    {
-        output_i = current_temp_output[i];
-        temp_mean_field_gradient[i] *= output_i * (1-output_i);
-    }
+        bias_gradient += temp_mean_field_gradient;
 
-    bias_gradient += temp_mean_field_gradient;
-
-    if( topographic_lateral_weights.length() == 0)
-    {
-        // Update lateral connections
-        if( momentum == 0. )
+        if( topographic_lateral_weights.length() == 0)
         {
-            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
-                               lateral_weights);
+            // Update lateral connections
+            if( momentum == 0. )
+            {
+                multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                                   lateral_weights);
+            }
+            else
+            {
+                multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
+                                   lateral_weights_inc);
+                lateral_weights += lateral_weights_inc;
+            }
         }
         else
         {
-            multiplyScaledAdd( lateral_weights_gradient, momentum, -learning_rate,
-                               lateral_weights_inc);
-            lateral_weights += lateral_weights_inc;
+            if( !do_not_learn_topographic_lateral_weights )
+            {
+                if( momentum == 0. )
+                    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                           -learning_rate,
+                                           topographic_lateral_weights[i]);
+            
+                else
+                    PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
+                            "topographic weights");
+            }
         }
-    }
-    else
-    {
-        if( !do_not_learn_topographic_lateral_weights )
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
         {
-            if( momentum == 0. )
-                for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
-                                       -learning_rate,
-                                       topographic_lateral_weights[i]);
-            
-            else
-                PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
-                        "topographic weights");
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
         }
     }
-    // Set diagonal to 0
-    if( lateral_weights.length() != 0 )
-    {
-        real *d = lateral_weights.data();
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
-            *d = 0;
-    }
 }
 
 void RBMLateralBinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
@@ -1294,95 +1443,104 @@
 
     // TODO Can we do this more efficiently? (using BLAS)
 
-    // We use the average gradient over the mini-batch.
-    lateral_weights_gradient.clear();
-    real output_i;
-    for (int j = 0; j < batch_size; j++)
+    if( use_parametric_mean_field )
     {
-        // top_gradient = expectations(j) - targets(j)
-        substract(expectations(j), targets(j), temp_mean_field_gradient);
-        current_temp_output = expectations(j);
+        PLERROR("RBMLateralBinomialLayer::bpropNLL: use_parametric_mean_field=true "
+                "not implemented yet.");
+    }
+    else
+    {
 
-        for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
+        // We use the average gradient over the mini-batch.
+        lateral_weights_gradient.clear();
+        real output_i;
+        for (int j = 0; j < batch_size; j++)
         {
-            for( int i=0 ; i<size ; i++ )
+            // top_gradient = expectations(j) - targets(j)
+            substract(expectations(j), targets(j), temp_mean_field_gradient);
+            current_temp_output = expectations(j);
+
+            for( int t=n_lateral_connections_passes-1 ; t>=0 ; t-- )
             {
-                output_i = current_temp_output[i];
+                for( int i=0 ; i<size ; i++ )
+                {
+                    output_i = current_temp_output[i];
                 
-                // Contribution from the mean field approximation
-                temp_mean_field_gradient2[i] =  (1-dampening_factor)*
-                    output_i * (1-output_i) * temp_mean_field_gradient[i];
+                    // Contribution from the mean field approximation
+                    temp_mean_field_gradient2[i] =  (1-dampening_factor)*
+                        output_i * (1-output_i) * temp_mean_field_gradient[i];
                 
-                // Contribution from the dampening
-                temp_mean_field_gradient[i] *= dampening_factor;
-            }
+                    // Contribution from the dampening
+                    temp_mean_field_gradient[i] *= dampening_factor;
+                }
             
-            // Input gradient contribution
-            bias_gradients(j) += temp_mean_field_gradient2;
+                // Input gradient contribution
+                bias_gradients(j) += temp_mean_field_gradient2;
             
-            // Lateral weights gradient contribution
-            if( topographic_lateral_weights.length() == 0)
-            {
+                // Lateral weights gradient contribution
+                if( topographic_lateral_weights.length() == 0)
+                {
 
-                externalSymetricProductAcc( lateral_weights_gradient, 
-                                            temp_mean_field_gradient2,
-                                            temp_outputs[t](j) );
+                    externalSymetricProductAcc( lateral_weights_gradient, 
+                                                temp_mean_field_gradient2,
+                                                temp_outputs[t](j) );
                 
-                transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
-                                    temp_mean_field_gradient2);
+                    transposeProductAcc(temp_mean_field_gradient, lateral_weights, 
+                                        temp_mean_field_gradient2);
+                }
+                else
+                {
+                    productTopoLateralWeightsGradients( 
+                        temp_outputs[t](j),
+                        temp_mean_field_gradient,
+                        temp_mean_field_gradient2,
+                        topographic_lateral_weights_gradient);
+                }
+                current_temp_output = temp_outputs[t](j);
             }
-            else
+    
+            for( int i=0 ; i<size ; i++ )
             {
-                productTopoLateralWeightsGradients( 
-                    temp_outputs[t](j),
-                    temp_mean_field_gradient,
-                    temp_mean_field_gradient2,
-                    topographic_lateral_weights_gradient);
+                output_i = current_temp_output[i];
+                temp_mean_field_gradient[i] *= output_i * (1-output_i);
             }
-            current_temp_output = temp_outputs[t](j);
+
+            bias_gradients(j) += temp_mean_field_gradient;
         }
-    
-        for( int i=0 ; i<size ; i++ )
+
+        // Update lateral connections
+        if( topographic_lateral_weights.length() == 0 )
         {
-            output_i = current_temp_output[i];
-            temp_mean_field_gradient[i] *= output_i * (1-output_i);
+            if( momentum == 0. )
+                multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
+                                   lateral_weights);
+            else
+                PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
+                        "momentum with mini-batches");
         }
-
-        bias_gradients(j) += temp_mean_field_gradient;
-    }
-
-    // Update lateral connections
-    if( topographic_lateral_weights.length() == 0 )
-    {
-        if( momentum == 0. )
-            multiplyScaledAdd( lateral_weights_gradient, 1.0, -learning_rate,
-                               lateral_weights);
         else
-            PLERROR("In RBMLateralBinomialLayer:bpropUpdate - Not implemented for "
-                    "momentum with mini-batches");
-    }
-    else
-    {
-        if( !do_not_learn_topographic_lateral_weights )
         {
-            if( momentum == 0. )
-                for( int i=0; i<topographic_lateral_weights.length(); i++ )
-                    multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
-                                       -learning_rate,
-                                       topographic_lateral_weights[i]);
+            if( !do_not_learn_topographic_lateral_weights )
+            {
+                if( momentum == 0. )
+                    for( int i=0; i<topographic_lateral_weights.length(); i++ )
+                        multiplyScaledAdd( topographic_lateral_weights_gradient[i], 1.0, 
+                                           -learning_rate,
+                                           topographic_lateral_weights[i]);
             
-            else
-                PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
-                        "topographic weights");
+                else
+                    PLERROR("In RBMLateralBinomialLayer:bpropNLL - Not implemented for "
+                            "topographic weights");
+            }
         }
-    }
 
-    // Set diagonal to 0
-    if( lateral_weights.length() != 0 )
-    {
-        real *d = lateral_weights.data();
-        for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
-            *d = 0;
+        // Set diagonal to 0
+        if( lateral_weights.length() != 0 )
+        {
+            real *d = lateral_weights.data();
+            for (int i=0; i<lateral_weights.length(); i++,d+=lateral_weights.mod()+1) 
+                *d = 0;
+        }
     }
 }
 
@@ -1635,6 +1793,22 @@
                   OptionBase::learntoption,
                   "Local topographic lateral connections.\n");
 
+    declareOption(ol, "use_parametric_mean_field", 
+                  &RBMLateralBinomialLayer::use_parametric_mean_field,
+                  OptionBase::buildoption,
+                  "Indication that a parametric predictor of the mean-field\n"
+                  "approximation of the hidden layer conditional distribution.\n");
+
+    declareOption(ol, "mean_field_output_weights", 
+                  &RBMLateralBinomialLayer::mean_field_output_weights,
+                  OptionBase::learntoption,
+                  "Output weights of the mean field predictor.\n");
+
+    declareOption(ol, "mean_field_output_bias", 
+                  &RBMLateralBinomialLayer::mean_field_output_bias,
+                  OptionBase::learntoption,
+                  "Output bias of the mean field predictor.\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -1653,7 +1827,19 @@
     if( n_lateral_connections_passes < 0 )
         PLERROR("In RBMLateralBinomialLayer::build_(): n_lateral_connections_passes\n"
                 " should be >= 0.");
+ 
+    if( use_parametric_mean_field && topographic_length > 0 && topographic_width > 0 )
+        PLERROR("RBMLateralBinomialLayer::build_(): can't use parametric mean field "
+            "and topographic lateral connections.");
     
+    if( use_parametric_mean_field )
+    {
+        mean_field_output_weights.resize(size,size);
+        mean_field_output_bias.resize(size);
+        mean_field_input.resize(size);
+        pre_sigmoid_mean_field_output.resize(size);
+    }
+
     if( topographic_length <= 0 || topographic_width <= 0)
     {
         lateral_weights.resize(size,size);
@@ -1719,6 +1905,8 @@
     deepCopyField(lateral_weights_neg_stats,copies);
     deepCopyField(dampening_expectation,copies);
     deepCopyField(dampening_expectations,copies);
+    deepCopyField(mean_field_input,copies);
+    deepCopyField(pre_sigmoid_mean_field_output,copies);
     deepCopyField(temp_output,copies);
     deepCopyField(temp_outputs,copies);
     deepCopyField(current_temp_output,copies);
@@ -1733,6 +1921,8 @@
     deepCopyField(lateral_weights_gradient,copies);
     deepCopyField(lateral_weights_inc,copies);
     deepCopyField(topographic_lateral_weights_gradient,copies);
+    deepCopyField(mean_field_output_weights,copies);
+    deepCopyField(mean_field_output_bias,copies);
 }
 
 real RBMLateralBinomialLayer::energy(const Vec& unit_values) const

Modified: trunk/plearn_learners/online/RBMLateralBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-02-28 19:37:18 UTC (rev 8600)
+++ trunk/plearn_learners/online/RBMLateralBinomialLayer.h	2008-02-28 22:14:04 UTC (rev 8601)
@@ -100,6 +100,16 @@
     //! Accumulates negative contribution to the gradient of lateral weights
     Mat lateral_weights_neg_stats;
 
+    //! Indication that a parametric predictor of the mean-field 
+    //! approximation of the hidden layer conditional distribution.
+    bool use_parametric_mean_field;
+
+    //! Output weights of the mean field predictor
+    Mat mean_field_output_weights;
+
+    //! Output bias of the mean field predictor
+    Vec mean_field_output_bias;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -237,6 +247,9 @@
     mutable Vec dampening_expectation;
     mutable Mat dampening_expectations;
 
+    mutable Vec mean_field_input;
+    mutable Vec pre_sigmoid_mean_field_output;
+
     mutable TVec<Vec> temp_output;
     mutable TVec<Mat> temp_outputs;
 



From larocheh at mail.berlios.de  Thu Feb 28 23:16:39 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 28 Feb 2008 23:16:39 +0100
Subject: [Plearn-commits] r8602 - trunk/plearn_learners_experimental
Message-ID: <200802282216.m1SMGdVX027648@sheep.berlios.de>

Author: larocheh
Date: 2008-02-28 23:16:38 +0100 (Thu, 28 Feb 2008)
New Revision: 8602

Added:
   trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc
   trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h
Log:
Deep Network with distinct layers for bottom up and top down inference...


Added: trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc
===================================================================
--- trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc	2008-02-28 22:14:04 UTC (rev 8601)
+++ trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.cc	2008-02-28 22:16:38 UTC (rev 8602)
@@ -0,0 +1,896 @@
+// -*- C++ -*-
+
+// TopDownAsymetricDeepNetwork.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file TopDownAsymetricDeepNetwork.cc */
+
+
+#define PL_LOG_MODULE_NAME "TopDownAsymetricDeepNetwork"
+#include <plearn/io/pl_log.h>
+
+#include "TopDownAsymetricDeepNetwork.h"
+#include <plearn/vmat/VMat_computeNearestNeighbors.h>
+#include <plearn/vmat/GetInputVMatrix.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMMixedConnection.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    TopDownAsymetricDeepNetwork,
+    "Neural net, trained layer-wise in a greedy but focused fashion using autoassociators/RBMs and a supervised non-parametric gradient.",
+    "It is highly inspired by the StackedFocusedAutoassociators class,\n"
+    "and can use use the same RBMLayer and RBMConnection components.\n"
+    );
+
+TopDownAsymetricDeepNetwork::TopDownAsymetricDeepNetwork() :
+    cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
+    greedy_learning_rate( 0. ),
+    greedy_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    n_classes( -1 ),
+    output_weights_l1_penalty_factor(0),
+    output_weights_l2_penalty_factor(0),
+    n_layers( 0 ),
+    currently_trained_layer( 0 )
+{
+    // random_gen will be initialized in PLearner::build_()
+    random_gen = new PRandom();
+    nstages = 0;
+}
+
+void TopDownAsymetricDeepNetwork::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "cd_learning_rate", 
+                  &TopDownAsymetricDeepNetwork::cd_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the RBM "
+                  "contrastive divergence training.\n");
+
+    declareOption(ol, "cd_decrease_ct", 
+                  &TopDownAsymetricDeepNetwork::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "the RBMs contrastive\n"
+                  "divergence training. When a hidden layer has finished "
+                  "its training,\n"
+                  "the learning rate is reset to it's initial value.\n");
+
+    declareOption(ol, "greedy_learning_rate", 
+                  &TopDownAsymetricDeepNetwork::greedy_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the autoassociator "
+                  "gradient descent training.\n");
+
+    declareOption(ol, "greedy_decrease_ct", 
+                  &TopDownAsymetricDeepNetwork::greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "the autoassociator\n"
+                  "gradient descent training. When a hidden layer has finished "
+                  "its training,\n"
+                  "the learning rate is reset to it's initial value.\n");
+
+    declareOption(ol, "fine_tuning_learning_rate", 
+                  &TopDownAsymetricDeepNetwork::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the fine tuning "
+                  "gradient descent.\n");
+
+    declareOption(ol, "fine_tuning_decrease_ct", 
+                  &TopDownAsymetricDeepNetwork::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "fine tuning\n"
+                  "gradient descent.\n");
+
+    declareOption(ol, "training_schedule", 
+                  &TopDownAsymetricDeepNetwork::training_schedule,
+                  OptionBase::buildoption,
+                  "Number of examples to use during each phase "
+                  "of greedy pre-training.\n"
+                  "The number of fine-tunig steps is defined by nstages.\n"
+        );
+
+    declareOption(ol, "layers", &TopDownAsymetricDeepNetwork::layers,
+                  OptionBase::buildoption,
+                  "The layers of units in the network. The first element\n"
+                  "of this vector should be the input layer and the\n"
+                  "subsequent elements should be the hidden layers. The\n"
+                  "output layer should not be included in layers.\n"
+                  "These layers will be used only for bottom up inference.\n");
+
+    declareOption(ol, "top_down_layers", 
+                  &TopDownAsymetricDeepNetwork::top_down_layers,
+                  OptionBase::buildoption,
+                  "The layers of units used for top down inference during\n"
+                  "greedy training of an RBM/autoencoder.");
+
+    declareOption(ol, "connections", &TopDownAsymetricDeepNetwork::connections,
+                  OptionBase::buildoption,
+                  "The weights of the connections between the layers");
+
+    declareOption(ol, "reconstruction_connections", 
+                  &TopDownAsymetricDeepNetwork::reconstruction_connections,
+                  OptionBase::buildoption,
+                  "The reconstruction weights of the autoassociators");
+
+    declareOption(ol, "n_classes", 
+                  &TopDownAsymetricDeepNetwork::n_classes,
+                  OptionBase::buildoption,
+                  "Number of classes.");
+
+    declareOption(ol, "greedy_stages", 
+                  &TopDownAsymetricDeepNetwork::greedy_stages,
+                  OptionBase::learntoption,
+                  "Number of training samples seen in the different greedy "
+                  "phases.\n"
+        );
+
+    declareOption(ol, "n_layers", &TopDownAsymetricDeepNetwork::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers"
+        );
+
+    declareOption(ol, "final_module", 
+                  &TopDownAsymetricDeepNetwork::final_module,
+                  OptionBase::learntoption,
+                  "Output layer of neural net"
+        );
+
+    declareOption(ol, "final_cost", 
+                  &TopDownAsymetricDeepNetwork::final_cost,
+                  OptionBase::learntoption,
+                  "Cost on output layer of neural net"
+        );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void TopDownAsymetricDeepNetwork::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(inputsize_ > 0 && targetsize_ > 0)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        if( n_classes <= 0 )
+            PLERROR("TopDownAsymetricDeepNetwork::build_() - \n"
+                    "n_classes should be > 0.\n");
+
+        if( weightsize_ > 0 )
+            PLERROR("TopDownAsymetricDeepNetwork::build_() - \n"
+                    "usage of weighted samples (weight size > 0) is not\n"
+                    "implemented yet.\n");
+
+        if( training_schedule.length() != n_layers-1 )        
+            PLERROR("TopDownAsymetricDeepNetwork::build_() - \n"
+                    "training_schedule should have %d elements.\n",
+                    n_layers-1);
+        
+        if(greedy_stages.length() == 0)
+        {
+            greedy_stages.resize(n_layers-1);
+            greedy_stages.clear();
+        }
+
+        if(stage > 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer>1
+                  && greedy_stages[currently_trained_layer-1] <= 0)
+                currently_trained_layer--;
+        }
+
+        build_layers_and_connections();
+
+        if( !final_module || !final_cost )
+            build_output_layer_and_cost();
+    }
+}
+
+void TopDownAsymetricDeepNetwork::build_output_layer_and_cost()
+{
+    GradNNetLayerModule* gnl = new GradNNetLayerModule();
+    gnl->input_size = layers[n_layers-1]->size;
+    gnl->output_size = n_classes;
+    gnl->L1_penalty_factor = output_weights_l1_penalty_factor;
+    gnl->L2_penalty_factor = output_weights_l2_penalty_factor;
+    gnl->random_gen = random_gen;
+    gnl->build();
+
+    SoftmaxModule* sm = new SoftmaxModule();
+    sm->input_size = n_classes;
+    sm->random_gen = random_gen;
+    sm->build();
+
+    ModuleStackModule* msm = new ModuleStackModule();
+    msm->modules.resize(2);
+    msm->modules[0] = gnl;
+    msm->modules[1] = sm;
+    msm->random_gen = random_gen;
+    msm->build();
+    final_module = msm;
+
+    final_module->forget();
+
+    NLLCostModule* nll = new NLLCostModule();
+    nll->input_size = n_classes;
+    nll->random_gen = random_gen;
+    nll->build();
+    
+    ClassErrorCostModule* class_error = new ClassErrorCostModule();
+    class_error->input_size = n_classes;
+    class_error->random_gen = random_gen;
+    class_error->build();
+
+    CombiningCostsModule* comb_costs = new CombiningCostsModule();
+    comb_costs->cost_weights.resize(2);
+    comb_costs->cost_weights[0] = 1;
+    comb_costs->cost_weights[1] = 0;
+    comb_costs->sub_costs.resize(2);
+    comb_costs->sub_costs[0] = nll;
+    comb_costs->sub_costs[1] = class_error;
+    comb_costs->build();
+
+    final_cost = comb_costs;
+    final_cost->forget();
+}
+
+void TopDownAsymetricDeepNetwork::build_layers_and_connections()
+{
+    MODULE_LOG << "build_layers_and_connections() called" << endl;
+
+    if( connections.length() != n_layers-1 )
+        PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() - \n"
+                "there should be %d connections.\n",
+                n_layers-1);
+
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) 
+        && reconstruction_connections.length() != n_layers-1 )
+        PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() - \n"
+                "there should be %d reconstruction connections.\n",
+                n_layers-1);
+    
+    if(  !( reconstruction_connections.length() == 0
+            || reconstruction_connections.length() == n_layers-1 ) )
+        PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() - \n"
+                "there should be either 0 or %d reconstruction connections.\n",
+                n_layers-1);
+    
+    
+    if(top_down_layers.length() != n_layers 
+       && top_down_layers.length() != 0)
+        PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() - \n"
+                "there should be either 0 of %d top_down_layers.\n",
+                n_layers);
+        
+    if(layers[0]->size != inputsize_)
+        PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() - \n"
+                "layers[0] should have a size of %d.\n",
+                inputsize_);
+    
+    if(top_down_layers[0]->size != inputsize_)
+        PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() - \n"
+                "top_down_layers[0] should have a size of %d.\n",
+                inputsize_);
+    
+
+    activations.resize( n_layers );
+    expectations.resize( n_layers );
+    activation_gradients.resize( n_layers );
+    expectation_gradients.resize( n_layers );
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        if( layers[i]->size != connections[i]->down_size )
+            PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a down_size of %d.\n",
+                    i, layers[i]->size);
+
+        if( top_down_layers[i]->size != connections[i]->down_size )
+            PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() "
+                    "- \n"
+                    "top_down_layers[%i] should have a size of %d.\n",
+                    i, connections[i]->down_size);
+
+        if( connections[i]->up_size != layers[i+1]->size )
+            PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a up_size of %d.\n",
+                    i, layers[i+1]->size);
+
+        if( connections[i]->up_size != top_down_layers[i+1]->size )
+            PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() "
+                    "- \n"
+                    "top_down_layers[%i] should have a up_size of %d.\n",
+                    i, connections[i]->up_size);
+
+        if( reconstruction_connections.length() != 0 )
+        {
+            if( layers[i+1]->size != reconstruction_connections[i]->down_size )
+                PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() "
+                        "- \n"
+                        "recontruction_connections[%i] should have a down_size of "
+                            "%d.\n",
+                        i, layers[i+1]->size);
+            
+            if( reconstruction_connections[i]->up_size != layers[i]->size )
+                PLERROR("TopDownAsymetricDeepNetwork::build_layers_and_connections() "
+                        "- \n"
+                        "recontruction_connections[%i] should have a up_size of "
+                        "%d.\n",
+                        i, layers[i]->size);
+        }
+        
+        if( !(layers[i]->random_gen) )
+        {
+            layers[i]->random_gen = random_gen;
+            layers[i]->forget();
+        }
+
+        if( !(top_down_layers[i]->random_gen) )
+        {
+            top_down_layers[i]->random_gen = random_gen;
+            top_down_layers[i]->forget();
+        }
+
+        if( !(connections[i]->random_gen) )
+        {
+            connections[i]->random_gen = random_gen;
+            connections[i]->forget();
+        }
+
+        if( reconstruction_connections.length() != 0
+            && !(reconstruction_connections[i]->random_gen) )
+        {
+            reconstruction_connections[i]->random_gen = random_gen;
+            reconstruction_connections[i]->forget();
+        }        
+
+        activations[i].resize( layers[i]->size );
+        expectations[i].resize( layers[i]->size );
+        activation_gradients[i].resize( layers[i]->size );
+        expectation_gradients[i].resize( layers[i]->size );
+    }
+
+    if( !(layers[n_layers-1]->random_gen) )
+    {
+        layers[n_layers-1]->random_gen = random_gen;
+        layers[n_layers-1]->forget();
+    }
+    if( !(top_down_layers[n_layers-1]->random_gen) )
+    {
+        top_down_layers[n_layers-1]->random_gen = random_gen;
+        top_down_layers[n_layers-1]->forget();
+    }
+    activations[n_layers-1].resize( layers[n_layers-1]->size );
+    expectations[n_layers-1].resize( layers[n_layers-1]->size );
+    activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+    expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+}
+
+// ### Nothing to add here, simply calls build_
+void TopDownAsymetricDeepNetwork::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void TopDownAsymetricDeepNetwork::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    // Public options
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(top_down_layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(reconstruction_connections, copies);
+
+    // Protected options
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(input_representation, copies);
+    deepCopyField(pos_down_val, copies);
+    deepCopyField(pos_up_val, copies);
+    deepCopyField(neg_down_val, copies);
+    deepCopyField(neg_up_val, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_gradient, copies);
+    deepCopyField(greedy_stages, copies);
+    deepCopyField(final_module, copies);
+    deepCopyField(final_cost, copies);
+}
+
+
+int TopDownAsymetricDeepNetwork::outputsize() const
+{
+//    if(currently_trained_layer < n_layers)
+//        return layers[currently_trained_layer]->size;
+    return n_classes;
+}
+
+void TopDownAsymetricDeepNetwork::forget()
+{
+    inherited::forget();
+
+    for( int i=0 ; i<n_layers ; i++ )
+        layers[i]->forget();
+    
+    for( int i=0 ; i<n_layers ; i++ )
+        top_down_layers[i]->forget();
+    
+    for( int i=0 ; i<n_layers-1 ; i++ )
+        connections[i]->forget();
+    
+    for( int i=0; i<reconstruction_connections.length(); i++)
+        reconstruction_connections[i]->forget();
+
+    build_output_layer_and_cost();
+
+    stage = 0;
+    greedy_stages.clear();
+}
+
+void TopDownAsymetricDeepNetwork::train()
+{
+    MODULE_LOG << "train() called " << endl;
+    MODULE_LOG << "  training_schedule = " << training_schedule << endl;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight; // unused
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int sample;
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    int init_stage;
+
+    /***** initial greedy training *****/
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        MODULE_LOG << "Training connection weights between layers " << i
+            << " and " << i+1 << endl;
+
+        int end_stage = training_schedule[i];
+        int* this_stage = greedy_stages.subVec(i,1).data();
+        init_stage = *this_stage;
+
+        MODULE_LOG << "  stage = " << *this_stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+        MODULE_LOG << "  greedy_learning_rate = " << greedy_learning_rate << endl;
+        MODULE_LOG << "  cd_learning_rate = " << cd_learning_rate << endl;
+
+        if( report_progress && *this_stage < end_stage )
+            pb = new ProgressBar( "Training layer "+tostring(i)
+                                  +" of "+classname(),
+                                  end_stage - init_stage );
+
+        train_costs.fill(MISSING_VALUE);
+        reconstruction_activations.resize(layers[i]->size);
+        reconstruction_activation_gradients.resize(layers[i]->size);
+        reconstruction_expectation_gradients.resize(layers[i]->size);
+
+        input_representation.resize(layers[i]->size);
+        pos_down_val.resize(layers[i]->size);
+        pos_up_val.resize(layers[i+1]->size);
+        neg_down_val.resize(layers[i]->size);
+        neg_up_val.resize(layers[i+1]->size);
+
+        for( ; *this_stage<end_stage ; (*this_stage)++ )
+        {
+            
+            sample = *this_stage % nsamples;
+            train_set->getExample(sample, input, target, weight);
+            greedyStep( input, target, i, train_costs, *this_stage);
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( *this_stage - init_stage + 1 );
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage < nstages )
+    {
+
+        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  nstages = " << nstages << endl;
+        MODULE_LOG << "  fine_tuning_learning_rate = " << 
+            fine_tuning_learning_rate << endl;
+
+        init_stage = stage;
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+
+        final_cost_input.resize(n_classes);
+        final_cost_value.resize(2); // Should be resized anyways
+        final_cost_gradient.resize(n_classes);
+        input_representation.resize(layers.last()->size);
+        for( ; stage<nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set->getExample( sample, input, target, weight );
+
+            fineTuningStep( input, target, train_costs);
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( stage - init_stage + 1 );
+        }
+    }
+    
+    train_stats->finalize();
+    MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
+
+
+    // Update currently_trained_layer
+    if(stage > 0)
+        currently_trained_layer = n_layers;
+    else
+    {            
+        currently_trained_layer = n_layers-1;
+        while(currently_trained_layer>1 
+              && greedy_stages[currently_trained_layer-1] <= 0)
+            currently_trained_layer--;
+    }
+}
+
+void TopDownAsymetricDeepNetwork::greedyStep( 
+    const Vec& input, const Vec& target, int index, 
+    Vec train_costs, int this_stage)
+{
+    PLASSERT( index < n_layers );
+    real lr;
+
+    // Get example representation
+    computeRepresentation(input, input_representation, 
+                          index);
+    // Autoassociator learning
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) )
+    {
+        if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                       * this_stage); 
+        else
+            lr = greedy_learning_rate;
+
+        top_down_layers[index]->setLearningRate( lr );
+        connections[index]->setLearningRate( lr );
+        reconstruction_connections[index]->setLearningRate( lr );
+        layers[index+1]->setLearningRate( lr );
+
+        connections[index]->fprop(input_representation,
+                              activations[index+1]);
+        layers[index+1]->fprop(activations[index+1], expectations[index+1]);
+
+        reconstruction_connections[ index ]->fprop( expectations[index+1],
+                                                    reconstruction_activations);
+        top_down_layers[ index ]->fprop( reconstruction_activations,
+                                top_down_layers[ index ]->expectation);
+        
+        top_down_layers[ index ]->activation << reconstruction_activations;
+        top_down_layers[ index ]->setExpectationByRef(
+            top_down_layers[ index ]->expectation);
+        real rec_err = top_down_layers[ index ]->fpropNLL(
+            input_representation);
+        train_costs[index] = rec_err;
+        
+        top_down_layers[ index ]->bpropNLL(
+            input_representation, rec_err,
+            reconstruction_activation_gradients);
+    }
+
+    // RBM learning
+    if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
+    {
+        connections[index]->setAsDownInput( input_representation );
+        layers[index+1]->getAllActivations( connections[index] );
+        layers[index+1]->computeExpectation();
+        layers[index+1]->generateSample();
+        
+        // accumulate positive stats using the expectation
+        // we deep-copy because the value will change during negative phase
+        pos_down_val = expectations[index];
+        pos_up_val << layers[index+1]->expectation;
+        
+        // down propagation, starting from a sample of layers[index+1]
+        connections[index]->setAsUpInput( layers[index+1]->sample );
+        
+        top_down_layers[index]->getAllActivations( connections[index] );
+        top_down_layers[index]->computeExpectation();
+        top_down_layers[index]->generateSample();
+        
+        // negative phase
+        connections[index]->setAsDownInput( top_down_layers[index]->sample );
+        layers[index+1]->getAllActivations( connections[index] );
+        layers[index+1]->computeExpectation();
+        // accumulate negative stats
+        // no need to deep-copy because the values won't change before update
+        neg_down_val = top_down_layers[index]->sample;
+        neg_up_val = layers[index+1]->expectation;
+    }
+    
+    // Update hidden layer bias and weights
+
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) )
+    {
+        top_down_layers[ index ]->update(reconstruction_activation_gradients);
+    
+        reconstruction_connections[ index ]->bpropUpdate( 
+            expectations[index+1],
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+        layers[ index+1 ]->bpropUpdate( 
+            activations[index+1],
+            expectations[index+1],
+            // reused
+            reconstruction_activation_gradients,
+            reconstruction_expectation_gradients);
+        
+        connections[ index ]->bpropUpdate( 
+            input_representation,
+            activations[index+1],
+            reconstruction_expectation_gradients, //reused
+            reconstruction_activation_gradients);
+    }
+     
+
+    // RBM updates
+    if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
+    {
+        if( !fast_exact_is_equal( cd_decrease_ct , 0 ) )
+            lr = cd_learning_rate/(1 + cd_decrease_ct 
+                                       * this_stage); 
+        else
+            lr = cd_learning_rate;
+
+        top_down_layers[index]->setLearningRate( lr );
+        connections[index]->setLearningRate( lr );
+        layers[index+1]->setLearningRate( lr );
+
+        top_down_layers[index]->update( pos_down_val, neg_down_val );
+        connections[index]->update( pos_down_val, pos_up_val,
+                                    neg_down_val, neg_up_val );
+        layers[index+1]->update( pos_up_val, neg_up_val );
+    }
+}
+
+void TopDownAsymetricDeepNetwork::fineTuningStep( 
+    const Vec& input, const Vec& target,
+    Vec& train_costs)
+{
+    // Get example representation
+    computeRepresentation(input, input_representation, 
+                          n_layers-1);
+
+    final_module->fprop( input_representation, final_cost_input );
+    final_cost->fprop( final_cost_input, target, final_cost_value );
+        
+    final_cost->bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module->bpropUpdate( input_representation,
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+    train_costs.last() = final_cost_value[0];
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        layers[i]->bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+        
+        
+        connections[i-1]->bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }        
+}
+
+void TopDownAsymetricDeepNetwork::computeRepresentation(
+    const Vec& input,
+    Vec& representation,
+    int layer) const
+{
+    if(layer == 0)
+    {
+        representation.resize(input.length());
+        expectations[0] << input;
+        representation << input;
+        return;
+    }
+
+    expectations[0] << input;
+    for( int i=0 ; i<layer; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+    representation.resize(expectations[layer].length());
+    representation << expectations[layer];
+}
+
+void TopDownAsymetricDeepNetwork::computeOutput(
+    const Vec& input, Vec& output) const
+{
+    computeRepresentation(input,input_representation, 
+                          min(currently_trained_layer,n_layers-1));
+    final_module->fprop( input_representation, final_cost_input );
+    output[0] = argmax(final_cost_input);
+}
+
+void TopDownAsymetricDeepNetwork::computeCostsFromOutputs(
+    const Vec& input, const Vec& output,
+    const Vec& target, Vec& costs) const
+{
+
+    //Assumes that computeOutput has been called
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if( currently_trained_layer<n_layers 
+        && reconstruction_connections.length() != 0 )
+    {
+        reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
+            expectations[currently_trained_layer],
+            reconstruction_activations);
+        top_down_layers[ currently_trained_layer-1 ]->fprop( 
+            reconstruction_activations,
+            top_down_layers[ currently_trained_layer-1 ]->expectation);
+        
+        top_down_layers[ currently_trained_layer-1 ]->activation << 
+            reconstruction_activations;
+        top_down_layers[ currently_trained_layer-1 ]->setExpectationByRef( 
+            top_down_layers[ currently_trained_layer-1 ]->expectation);
+        costs[ currently_trained_layer-1 ]  = 
+            top_down_layers[ currently_trained_layer-1 ]->fpropNLL(
+                expectations[currently_trained_layer-1]);
+    }
+
+    if( ((int)round(output[0])) == ((int)round(target[0])) )
+        costs[n_layers-1] = 0;
+    else
+        costs[n_layers-1] = 1;
+}
+
+TVec<string> TopDownAsymetricDeepNetwork::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec<string> cost_names(0);
+
+    for( int i=0; i<layers.size()-1; i++)
+        cost_names.push_back("reconstruction_error_" + tostring(i+1));
+        
+    cost_names.append( "class_error" );
+
+    return cost_names;
+}
+
+TVec<string> TopDownAsymetricDeepNetwork::getTrainCostNames() const
+{
+    TVec<string> cost_names = getTestCostNames();
+    cost_names.append( "NLL" );
+    return cost_names;    
+}
+
+//#####  Helper functions  ##################################################
+
+void TopDownAsymetricDeepNetwork::setLearningRate( real the_learning_rate )
+{
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( the_learning_rate );
+        top_down_layers[i]->setLearningRate( the_learning_rate );
+        connections[i]->setLearningRate( the_learning_rate );
+    }
+    layers[n_layers-1]->setLearningRate( the_learning_rate );
+    top_down_layers[n_layers-1]->setLearningRate( the_learning_rate );
+
+    final_module->setLearningRate( the_learning_rate );
+    final_cost->setLearningRate( the_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h
===================================================================
--- trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h	2008-02-28 22:14:04 UTC (rev 8601)
+++ trunk/plearn_learners_experimental/TopDownAsymetricDeepNetwork.h	2008-02-28 22:16:38 UTC (rev 8602)
@@ -0,0 +1,289 @@
+// -*- C++ -*-
+
+// TopDownAsymetricDeepNetwork.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file TopDownAsymetricDeepNetwork.h */
+
+
+#ifndef TopDownAsymetricDeepNetwork_INC
+#define TopDownAsymetricDeepNetwork_INC
+
+#include <plearn/vmat/ClassSubsetVMatrix.h>
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/ModuleStackModule.h>
+#include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/ClassErrorCostModule.h>
+#include <plearn_learners/online/CombiningCostsModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/SoftmaxModule.h>
+#include <plearn/misc/PTimer.h>
+
+namespace PLearn {
+
+/**
+ * Neural net, trained layer-wise in a greedy but focused fashion 
+ * using autoassociators/RBMs and a supervised non-parametric gradient.
+ * It is highly inspired by the StackedAutoassociators class, 
+ * and can use use the same RBMLayer and RBMConnection components.
+ */
+class TopDownAsymetricDeepNetwork : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Contrastive divergence learning rate
+    real cd_learning_rate;
+    
+    //! Contrastive divergence decrease constant
+    real cd_decrease_ct;
+
+    //! The learning rate used during the autoassociator gradient descent training
+    real greedy_learning_rate;
+
+    //! The decrease constant of the learning rate used during the autoassociator
+    //! gradient descent training. When a hidden layer has finished its training,
+    //! the learning rate is reset to it's initial value.
+    real greedy_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! Number of examples to use during each phase of greedy pre-training.
+    //! The number of fine-tunig steps is defined by nstages.
+    TVec<int> training_schedule;
+
+    //! The layers of units in the network for bottom up inference
+    TVec< PP<RBMLayer> > layers;
+
+    //! The layers of units in the network for top down inference
+    TVec< PP<RBMLayer> > top_down_layers;
+
+    //! The weights of the connections between the layers
+    TVec< PP<RBMConnection> > connections;
+
+    //! The reconstruction weights of the autoassociators
+    TVec< PP<RBMConnection> > reconstruction_connections;
+
+    //! Number of classes
+    int n_classes;
+
+    //! Output weights l1_penalty_factor
+    real output_weights_l1_penalty_factor;
+
+    //! Output weights l2_penalty_factor
+    real output_weights_l2_penalty_factor;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    TopDownAsymetricDeepNetwork();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+    void greedyStep( const Vec& input, const Vec& target, int index, 
+                     Vec train_costs, int stage);
+
+    void fineTuningStep( const Vec& input, const Vec& target,
+                         Vec& train_costs);
+
+    void computeRepresentation( const Vec& input, 
+                                Vec& representation, int layer) const;
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(TopDownAsymetricDeepNetwork);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectation_gradients;
+
+    //! Reconstruction activations
+    mutable Vec reconstruction_activations;
+    
+    //! Reconstruction activation gradients
+    mutable Vec reconstruction_activation_gradients;
+
+    //! Reconstruction expectation gradients
+    mutable Vec reconstruction_expectation_gradients;
+
+    //! Example representation
+    mutable Vec input_representation;
+
+    //! Positive down statistic
+    Vec pos_down_val;
+    //! Positive up statistic
+    Vec pos_up_val;
+    //! Negative down statistic
+    Vec neg_down_val;
+    //! Negative up statistic
+    Vec neg_up_val;
+
+    //! Input of cost function
+    mutable Vec final_cost_input;
+    //! Cost value
+    mutable Vec final_cost_value;
+    //! Cost gradient on output layer
+    mutable Vec final_cost_gradient;
+
+    //! Stages of the different greedy phases
+    TVec<int> greedy_stages;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+    //! Output layer of neural net
+    PP<OnlineLearningModule> final_module;
+
+    //! Cost on output layer of neural net
+    PP<CostModule> final_cost;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_output_layer_and_cost();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(TopDownAsymetricDeepNetwork);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Fri Feb 29 16:31:00 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 29 Feb 2008 16:31:00 +0100
Subject: [Plearn-commits] r8603 - trunk/plearn/vmat
Message-ID: <200802291531.m1TFV0RY003880@sheep.berlios.de>

Author: tihocan
Date: 2008-02-29 16:31:00 +0100 (Fri, 29 Feb 2008)
New Revision: 8603

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Renamed is_file_uptodate into isFileUpToDate to match PLearn's naming conventions, and added documentation of this method

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-28 22:16:38 UTC (rev 8602)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-29 15:31:00 UTC (rev 8603)
@@ -1416,28 +1416,29 @@
     return field_stats;
 }
 
-//////////////////////
-// is_file_uptodate //
-//////////////////////
-bool VMatrix::is_file_uptodate(PPath& path, bool warning_recalcul,
-                               bool warning_mtime) const
+////////////////////
+// isFileUpToDate //
+////////////////////
+bool VMatrix::isFileUpToDate(const PPath& path, bool warning_reuse,
+                             bool warning_older) const
 {
     bool exist = isfile(path);
     bool uptodate = false;
     if(exist)
-        uptodate = getMtime()<mtime(path);
-    if (warning_mtime && exist && uptodate && getMtime()==0)
-        PLWARNING("Warning: using a saved file (%s) but mtime is 0"
-                  "(cannot be sure file is up to date).",
+        uptodate = getMtime() < mtime(path);
+    if (warning_reuse && exist && uptodate && getMtime()==0)
+        PLWARNING("In VMatrix::isFileUpToDate - File '%s' will be used, but "
+                  "this VMat's last modification time is undefined: we cannot "
+                  "be sure the file is up-to-date",
                   path.absolute().c_str());
-    if(warning_recalcul && exist && !uptodate)
-        PLWARNING("Warning: recomputing saved file (%s) as it is older"
-                  " then the source.",
+    if(warning_older && exist && !uptodate)
+        PLWARNING("In VMatrix::isFileUpToDate - File '%s' is older than this "
+                  "VMat's last modification time, and cannot be re-used",
                   path.absolute().c_str());
 
     return exist && uptodate;
- 
 }
+
 /////////////////////////////////
 // getPrecomputedStatsFromFile //
 /////////////////////////////////
@@ -1448,7 +1449,7 @@
     PPath metadatadir = getMetaDataDir();
     PPath statsfile =  metadatadir / filename;
     lockMetaDataDir();
-    bool uptodate=is_file_uptodate(statsfile, true, true);
+    bool uptodate= isFileUpToDate(statsfile, true, true);
     if (uptodate)
         PLearn::load(statsfile, stats);
     else

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-28 22:16:38 UTC (rev 8602)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-29 15:31:00 UTC (rev 8603)
@@ -375,10 +375,15 @@
      */
     inline void setMtime(time_t t) { mtime_ = t; }
 
+    //! Return 'true' iff 'file' was last modified after this VMat, or this
+    //! VMat's last modification time is undefined (set to 0).
+    //! If 'warning_older' is 'true', then a warning will be issued when the
+    //! file exists and it is older than this VMat's last modification time.
+    //! If 'warning_reuse' is 'true', then a warning will be issued when the
+    //! file exists and this VMat's last modification time is undefined.
+    bool isFileUpToDate(const PPath& file, bool warning_older = false,
+                        bool warning_reuse = true) const;
 
-    bool is_file_uptodate(PPath& file, bool warning_recalcul=false,
-                          bool warning_mtime=true) const;
-
     //#####  Matrix Sizes  ####################################################
 
     /// Return the number of columns in the VMatrix



From nouiz at mail.berlios.de  Fri Feb 29 16:59:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Feb 2008 16:59:26 +0100
Subject: [Plearn-commits] r8604 - trunk/plearn/vmat
Message-ID: <200802291559.m1TFxQdO008358@sheep.berlios.de>

Author: nouiz
Date: 2008-02-29 16:59:25 +0100 (Fri, 29 Feb 2008)
New Revision: 8604

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
updated function name to isFileUpToDate


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-02-29 15:31:00 UTC (rev 8603)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-02-29 15:59:25 UTC (rev 8604)
@@ -324,7 +324,7 @@
     setColumnNamesAndWidth();
 
     // open the index file
-    if(!isfile(idxfname))
+    if(!isFileUpToDate(idxfname))
         buildIdx(); // (re)build it first!
     idxfile = fopen(idxfname.c_str(),"rb");
     if(fgetc(idxfile) != byte_order())



From tihocan at mail.berlios.de  Fri Feb 29 17:32:44 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 29 Feb 2008 17:32:44 +0100
Subject: [Plearn-commits] r8605 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200802291632.m1TGWilN012325@sheep.berlios.de>

Author: tihocan
Date: 2008-02-29 17:32:43 +0100 (Fri, 29 Feb 2008)
New Revision: 8605

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Added TODO comments

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-02-29 15:59:25 UTC (rev 8604)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2008-02-29 16:32:43 UTC (rev 8605)
@@ -878,6 +878,12 @@
         ptimer->startTimer("big_loop");
     }
 
+    // TODO Maybe...
+    // - see if it has anything to do with accessing shared memory
+    // - try to mix in data with a lower or higher measure_every, just to see
+    // if the difference in behaviors in speedup_whilefalse is due to having
+    // less examples to process.
+
     //pout << "CPU " << iam << ": my_stage_incr = " << my_stage_incr << endl;
     for(int i = 0; i < my_stage_incr; i++)
     {



From nouiz at mail.berlios.de  Fri Feb 29 17:57:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Feb 2008 17:57:08 +0100
Subject: [Plearn-commits] r8606 - trunk/plearn/vmat
Message-ID: <200802291657.m1TGv8Ix014333@sheep.berlios.de>

Author: nouiz
Date: 2008-02-29 17:57:07 +0100 (Fri, 29 Feb 2008)
New Revision: 8606

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Added function VMatrix::updateMtime() that will update the mtime_ to the more recent value. A value of 0 is unknow. If it is called with a value of 0, we assume that a dependence have an mtime of 0, thus we should have an mtime of 0 too. We remember this and subsequent call to updateMtime() won't change the mtime.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-29 16:32:43 UTC (rev 8605)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
@@ -46,7 +46,7 @@
 #include <plearn/base/tostring.h>
 #include <plearn/base/lexical_cast.h>
 #include <plearn/base/stringutils.h> //!< For pgetline()
-#include <plearn/io/fileutils.h>     //!< For isfile()
+#include <plearn/io/fileutils.h>     //!< For isfile() mtime()
 #include <plearn/io/load_and_save.h>
 #include <plearn/math/random.h>      //!< For uniform_multinomial_sample()
 #include <plearn/base/RemoteDeclareMethod.h>
@@ -74,9 +74,9 @@
 
 VMatrix::VMatrix(bool call_build_):
     inherited   (call_build_),
+    mtime_      (0),
     length_     (-1),
     width_      (-1),
-    mtime_      (0),
     inputsize_  (-1),
     targetsize_ (-1),
     weightsize_ (-1),
@@ -90,9 +90,9 @@
 
 VMatrix::VMatrix(int the_length, int the_width, bool call_build_):
     inherited                       (call_build_),
+    mtime_                          (0),
     length_                         (the_length),
     width_                          (the_width),
-    mtime_                          (0),
     inputsize_                      (-1),
     targetsize_                     (-1),
     weightsize_                     (-1),
@@ -1156,7 +1156,7 @@
 /////////////////////
 void VMatrix::setMetaInfoFrom(const VMatrix* vm)
 {
-    setMtime(max(getMtime(),vm->getMtime()));
+    updateMtime(vm->getMtime());
 
     // copy length and width from vm if not set
     if(length_<0)
@@ -1416,6 +1416,20 @@
     return field_stats;
 }
 
+/////////////////
+// updateMtime //
+/////////////////
+void VMatrix::updateMtime(time_t t)
+{
+    if(t>mtime_ && mtime_!=numeric_limits<time_t>::max())
+        mtime_=t;
+    else if(t==0)
+        mtime_=numeric_limits<time_t>::max();
+}
+void VMatrix::updateMtime(const PPath& p){updateMtime(mtime(p));}
+
+void VMatrix::updateMtime(VMat v){updateMtime(v->getMtime());}
+
 ////////////////////
 // isFileUpToDate //
 ////////////////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-29 16:32:43 UTC (rev 8605)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-29 16:57:07 UTC (rev 8606)
@@ -89,12 +89,12 @@
     /// Used in the default dot(i,j) method to store the i-th and j-th rows.
     mutable Vec dotrow_1;
     mutable Vec dotrow_2;
+    time_t mtime_;          ///< Time of "last modification" of data files.
 
 protected:
 
     mutable int length_;    ///< Length of the VMatrix.
     int width_;             ///< Width of the VMatrix.
-    time_t mtime_;          ///< Time of "last modification" of data files.
 
     /// For training/testing data sets we assume each row is composed of 4
     /// parts: an input part, a target part, and a weight part.  These fields
@@ -364,14 +364,32 @@
      *  result returned is typically based on mtime of the files contianing
      *  this matrix's data when the object is constructed.  mtime_ defaults to
      *  0, so that's what will be returned by default, if the time was never
-     *  set by a call to setMtime(t) (see below).
+     *  set by a call to updateMtime(t) or setMtime(t)(see below).
      */
-    inline time_t getMtime() const { return mtime_; }
+    inline time_t getMtime() const { return mtime_==numeric_limits<time_t>::max()? 0: mtime_; }
 
     /**
-     *  Sets the "last modification" time for this matrix For matrices on disk,
+     *  Update the "last modification" time for this matrix.
+     *  this should be called by the constructor for all dependence,
+     *  file or other VMatrix, VMat
+     *  This fonction remember the time that is the more recent. If
+     *  a dependence have a mtime of 0, getMtime() will always return 0
+     *  as if a dependence have an unknow mtime, we should have an unknow mtime
+     */
+    void updateMtime(time_t t);
+
+    void updateMtime(const PPath& p);
+
+    void updateMtime(const VMatrix& v){updateMtime(v.getMtime());}
+
+    void updateMtime(VMat v);
+
+    /**
+     *  Preferably use updateMtime()!
+     *  Sets the "last modification" time for this matrix,
      *  this should be called by the constructor to reflect the mtime of the
      *  disk files.
+     *  @see updateMtime 
      */
     inline void setMtime(time_t t) { mtime_ = t; }
 



From nouiz at mail.berlios.de  Fri Feb 29 18:11:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Feb 2008 18:11:48 +0100
Subject: [Plearn-commits] r8607 - trunk/plearn/vmat
Message-ID: <200802291711.m1THBmnU015849@sheep.berlios.de>

Author: nouiz
Date: 2008-02-29 18:11:46 +0100 (Fri, 29 Feb 2008)
New Revision: 8607

Modified:
   trunk/plearn/vmat/AsciiVMatrix.cc
   trunk/plearn/vmat/CompactFileVMatrix.cc
   trunk/plearn/vmat/CumVMatrix.cc
   trunk/plearn/vmat/DiskVMatrix.cc
   trunk/plearn/vmat/MovingAverageVMatrix.cc
   trunk/plearn/vmat/MultiInstanceVMatrix.cc
   trunk/plearn/vmat/SourceVMatrix.cc
   trunk/plearn/vmat/VMat.cc
   trunk/plearn/vmat/VVMatrix.cc
Log:
updated to use updateMtime() instead of setMtime()


Modified: trunk/plearn/vmat/AsciiVMatrix.cc
===================================================================
--- trunk/plearn/vmat/AsciiVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/AsciiVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -119,7 +119,7 @@
 
 void AsciiVMatrix::build_()
 {
-    //setMtime(mtime(filename));
+    //updateMtime(filename);
 
     if(!newfile)  // open old file
     {

Modified: trunk/plearn/vmat/CompactFileVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CompactFileVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/CompactFileVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -231,7 +231,7 @@
         cache_index = TVec<unsigned char>((length_ + 7) / 8);
     }
 
-    setMtime(mtime(filename_));
+    updateMtime(filename_);
 }
 
 

Modified: trunk/plearn/vmat/CumVMatrix.cc
===================================================================
--- trunk/plearn/vmat/CumVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/CumVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -128,7 +128,7 @@
             if ((columns[j] = source->fieldIndex(columns_to_accumulate[j])) == -1)
                 PLERROR("CumVMatrix: provided field name %s not found in source VMatrix",columns_to_accumulate[j].c_str());
 
-        setMtime(max(getMtime(),source->getMtime()));
+        updateMtime(source);
 
         // copy length and width from source if not set
         if(length_<0)

Modified: trunk/plearn/vmat/DiskVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DiskVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/DiskVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -147,7 +147,7 @@
         if(!isdir(dirname))
             PLERROR("In DiskVMatrix constructor, directory %s could not be found",dirname.c_str());
         setMetaDataDir(dirname + ".metadata");
-        setMtime( mtime( dirname/"indexfile" ) );
+        updateMtime( dirname/"indexfile" );
         string omode;
         if(writable)
             omode = "r+b";
@@ -217,7 +217,7 @@
         if(isdir(dirname))
             PLERROR("In DiskVMatrix constructor (with specified width), directory %s already exists",dirname.c_str());
         setMetaDataDir(dirname + ".metadata");
-        setMtime(mtime( dirname/"indexfile" ));
+        updateMtime( dirname/"indexfile" );
 
         if(isfile(dirname)) // patch for running mkstemp (TmpFilenames)
             unlink(dirname.c_str());

Modified: trunk/plearn/vmat/MovingAverageVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MovingAverageVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/MovingAverageVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -169,7 +169,7 @@
                 max_window_size=window_sizes[j];
         }
 
-        setMtime(max(getMtime(),source->getMtime()));
+        updateMtime(source);
 
         // copy length and width from source if not set
         if(length_<0)

Modified: trunk/plearn/vmat/MultiInstanceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiInstanceVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/MultiInstanceVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -241,7 +241,7 @@
     //test << data_;
     //test.close();
 
-    this->setMtime(mtime(filename_));
+    this->updateMtime(filename_);
 }
 
 // ### Nothing to add here, simply calls build_

Modified: trunk/plearn/vmat/SourceVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SourceVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/SourceVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -127,10 +127,8 @@
 void SourceVMatrix::build_()
 {
     /*
-      time_t mt = getMtime();
       for(int k=0; k<dependencies.size(); k++)
-      mt = max(mt, mtime(dependencies[k]));
-      setMtime(mt);
+      updateMtime(dependencies[k]);
     */
 }
 

Modified: trunk/plearn/vmat/VMat.cc
===================================================================
--- trunk/plearn/vmat/VMat.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/VMat.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -140,7 +140,7 @@
     else
         vm->defineSizes(m.width(),0,0);
 
-    vm->setMtime(mtime(filename));
+    vm->updateMtime(filename);
     // Set the discovered string -> real mappings.
     for (int i = 0; i < map_sr.length(); i++) {
         vm->setStringMapping(i, map_sr[i]);

Modified: trunk/plearn/vmat/VVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VVMatrix.cc	2008-02-29 16:57:07 UTC (rev 8606)
+++ trunk/plearn/vmat/VVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
@@ -403,7 +403,7 @@
         }
         source = new FileVMatrix(meta_data_dir / "precomputed.pmat");
         source->setMetaDataDir(meta_data_dir);
-        source->setMtime(date_of_code);
+        source->updateMtime(date_of_code);
         source->defineSizes(inputsize, targetsize, weightsize);
         return source;
     }
@@ -416,7 +416,7 @@
         {
             source = new DiskVMatrix(meta_data_dir/"precomputed.dmat");
             source->setMetaDataDir(meta_data_dir);
-            source->setMtime(date_of_code);
+            source->updateMtime(date_of_code);
             source->defineSizes(inputsize, targetsize, weightsize);
             return source;
         }
@@ -594,7 +594,7 @@
 
     VMatLanguage::output_preproc=olddebugval;
     source->setMetaDataDir(meta_data_dir);
-    source->setMtime(date_of_code);
+    source->updateMtime(date_of_code);
     if (sizes_spec) {
         source->defineSizes(inputsize, targetsize, weightsize);
     }
@@ -624,7 +624,7 @@
             the_mat->setMetaDataDir(getMetaDataDir());
         }
 
-        setMtime(the_mat->getMtime());
+        updateMtime(the_mat->getMtime());
         length_ = the_mat.length();
         width_ = the_mat.width();
 



From nouiz at mail.berlios.de  Fri Feb 29 19:14:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Feb 2008 19:14:52 +0100
Subject: [Plearn-commits] r8608 - trunk/plearn/vmat
Message-ID: <200802291814.m1TIEq3L014023@sheep.berlios.de>

Author: nouiz
Date: 2008-02-29 19:14:51 +0100 (Fri, 29 Feb 2008)
New Revision: 8608

Modified:
   trunk/plearn/vmat/ConcatColumnsVMatrix.cc
   trunk/plearn/vmat/DichotomizeVMatrix.cc
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
   trunk/plearn/vmat/ProcessingVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SubVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
set mtime with updateMtime()


Modified: trunk/plearn/vmat/ConcatColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -102,6 +102,7 @@
 
     for(int i=0; i<sources.size(); i++)
     {
+        updateMtime(sources[i]);
         if(sources[i]->length()!=length_)
             PLERROR("ConcatColumnsVMatrix: Problem concatenating two VMat with"
                     " different lengths");

Modified: trunk/plearn/vmat/DichotomizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/DichotomizeVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -145,6 +145,8 @@
     // ### You should assume that the parent class' build_() has already been
     // ### called.
 
+    updateMtime(source);
+
     instruction_index.fill(-1);
     TVec<string> source_names = source->fieldNames();
 

Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -40,6 +40,7 @@
 #include "GaussianizeVMatrix.h"
 #include <plearn/math/pl_erf.h>
 #include <plearn/vmat/VMat_computeStats.h>
+#include <plearn/io/fileutils.h>
 
 namespace PLearn {
 using namespace std;
@@ -146,11 +147,7 @@
         return;
 
     if (train_source) {
-        PLCHECK( train_source->width() == source->width() );
-        PLCHECK( train_source->inputsize()  == source->inputsize() &&
-                train_source->targetsize() == source->targetsize() &&
-                train_source->weightsize() == source->weightsize() &&
-                train_source->extrasize()  == source->extrasize() );
+        source->compatibleSizeError(train_source);
     }
 
     VMat the_source = train_source ? train_source : source;
@@ -158,6 +155,11 @@
     PLCHECK( the_source->inputsize() >= 0 && the_source->targetsize() >= 0 &&
             the_source->weightsize() >= 0 && the_source->extrasize() >= 0 );
 
+    // We set the mtime to remove the warning of Mtime=0
+    if(train_source)
+        updateMtime(train_source);
+    updateMtime(source);
+
     // Find which dimensions to Gaussianize.
     features_to_gaussianize.resize(0);
     int col = 0;
@@ -217,6 +219,7 @@
         PLWARNING("GaussianizeVMatrix::build_() 0 variable was gaussianized");
     // Obtain meta information from source.
     setMetaInfoFromSource();
+    
 }
 
 ///////////////

Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -49,7 +49,7 @@
 
 PLEARN_IMPLEMENT_OBJECT(
   MissingIndicatorVMatrix,
-  "VMat class to add a missing indicator for each variable.",
+  "VMatrix class to add a missing indicator for each variable.",
   "For each variable with a missing value in the referenced train set, an indicator is added.\n"
   "It is set to 1 if the value of the corresponding variable`in the underlying dataset is missing.\n"
   "It is set to 0 otherwise.\n"
@@ -152,6 +152,9 @@
     if (!source) PLERROR("In MissingIndicatorVMatrix:: source vmat must be supplied");
     if(!train_set && !fields)
       PLERROR("In MissingIndicatorVMatrix:: train_set or fields must be supplied");
+    updateMtime(source);
+    if(train_set)
+      updateMtime(train_set);
     buildNewRecordFormat(); 
 }
 

Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -132,6 +132,7 @@
     // ###    options have been modified.
     // ### You should assume that the parent class' build_() has already been
     // ### called.
+    updateMtime(source);
     length_ = source->length();
     ins.resize(source->width());
     tmp2.resize(source->width());

Modified: trunk/plearn/vmat/ProcessingVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessingVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/ProcessingVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -134,6 +134,7 @@
     if (!source)
         return;
 
+    updateMtime(source);
     length_ = source->length();
 
     TVec<string> fieldnames;

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -187,6 +187,7 @@
 void SelectColumnsVMatrix::build_()
 {
     if (source) {
+        updateMtime(source);
         if (fields.isNotEmpty()) {
             // Find out the indices from the fields.
             indices.resize(0);

Modified: trunk/plearn/vmat/SubVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SubVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/SubVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -131,6 +131,7 @@
 ////////////
 void SubVMatrix::build_()
 {
+    updateMtime(source);
     int sl = source->length();
     int sw = source->width();
 

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-02-29 17:11:46 UTC (rev 8607)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
@@ -305,6 +305,9 @@
         PLERROR("In TextFilesVMatrix::build_: could not create directory '%s'",
                 getMetaDataDir().absolute().c_str());
     
+    for(int i=0;i<txtfilenames.length();i++)
+        updateMtime(txtfilenames[i]);
+
     PPath metadir = getMetaDataDir();
     PPath idxfname = metadir/"txtmat.idx";
 



From nouiz at mail.berlios.de  Fri Feb 29 20:27:18 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Feb 2008 20:27:18 +0100
Subject: [Plearn-commits] r8609 - trunk/plearn/vmat
Message-ID: <200802291927.m1TJRIbE014226@sheep.berlios.de>

Author: nouiz
Date: 2008-02-29 20:27:18 +0100 (Fri, 29 Feb 2008)
New Revision: 8609

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
removed useless include, added today


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-29 18:14:51 UTC (rev 8608)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-02-29 19:27:18 UTC (rev 8609)
@@ -40,7 +40,6 @@
 #include "GaussianizeVMatrix.h"
 #include <plearn/math/pl_erf.h>
 #include <plearn/vmat/VMat_computeStats.h>
-#include <plearn/io/fileutils.h>
 
 namespace PLearn {
 using namespace std;



From saintmlx at mail.berlios.de  Fri Feb 29 20:32:27 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 29 Feb 2008 20:32:27 +0100
Subject: [Plearn-commits] r8610 - in trunk/python_modules/plearn: pyext
	utilities
Message-ID: <200802291932.m1TJWRuK015340@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-29 20:32:26 +0100 (Fri, 29 Feb 2008)
New Revision: 8610

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
   trunk/python_modules/plearn/utilities/toolkit.py
Log:
- new function to split dirs from path
- don't print version string when loading pyext



Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2008-02-29 19:27:18 UTC (rev 8609)
+++ trunk/python_modules/plearn/pyext/__init__.py	2008-02-29 19:32:26 UTC (rev 8610)
@@ -46,10 +46,8 @@
         ramassePoubelles()
 atexit.register(cleanupWrappedObjects)
 
-if os.getenv('PYTEST_STATE') != 'Active':
-    print versionString()
+__VERSION__= versionString()
 
-
 # Redefines function TMat to emulate pyplearn behaviour
 def TMat( *args ):
     """Returns a list of lists, each inner list being a row of the TMat"""

Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2008-02-29 19:27:18 UTC (rev 8609)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2008-02-29 19:32:26 UTC (rev 8610)
@@ -361,6 +361,21 @@
             dirs_list.extend( os.listdir(dirc) )
     return dirs_list
 
+def splitDirs(path):
+    """
+    returns a list of path elements, by successive calls to os.path.split
+    """
+    dirs= []
+    h= 123
+    while h not in ['','/']:
+        h,t= os.path.split(path)
+        dirs= [t]+dirs
+        path= h
+    if h=='/':
+        dirs= [h]+dirs
+
+    return dirs
+
 def no_none( orig ):
     """Parses the I{None} elements out of a list.  
 



From louradou at mail.berlios.de  Fri Feb 29 21:55:30 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 29 Feb 2008 21:55:30 +0100
Subject: [Plearn-commits] r8611 - trunk/plearn/math
Message-ID: <200802292055.m1TKtUMm026614@sheep.berlios.de>

Author: louradou
Date: 2008-02-29 21:55:30 +0100 (Fri, 29 Feb 2008)
New Revision: 8611

Modified:
   trunk/plearn/math/pl_math.cc
   trunk/plearn/math/pl_math.h
Log:
fixed: problem of compilation in -float caused by plearn/var/LogAddVariable
(because of a function missing in pl_math)



Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2008-02-29 19:32:26 UTC (rev 8610)
+++ trunk/plearn/math/pl_math.cc	2008-02-29 20:55:30 UTC (rev 8611)
@@ -174,6 +174,13 @@
     return (real)(log_a + log1p(exp(negative_absolute_difference)));
 }
 
+#ifdef USEFLOAT
+real logadd(real log_a, real log_b)
+{
+    return logadd(double(log_a), double(log_b));
+}
+#endif
+
 real square_f(real x)
 { return x*x; }
 

Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2008-02-29 19:32:26 UTC (rev 8610)
+++ trunk/plearn/math/pl_math.h	2008-02-29 20:55:30 UTC (rev 8611)
@@ -569,6 +569,11 @@
 //! (doing the computation in double precision)
 real logadd(double log_a, double log_b);
 
+#ifdef USEFLOAT
+//! Required in some template functions when compiling in float mode.
+real logadd(real log_a, real log_b);
+#endif
+
 //!  compute log(exp(log_a)-exp(log_b)) without losing too much precision
 real logsub(real log_a, real log_b);
 



From louradou at mail.berlios.de  Fri Feb 29 22:00:54 2008
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Fri, 29 Feb 2008 22:00:54 +0100
Subject: [Plearn-commits] r8612 - trunk/plearn/math
Message-ID: <200802292100.m1TL0sTA027112@sheep.berlios.de>

Author: louradou
Date: 2008-02-29 22:00:53 +0100 (Fri, 29 Feb 2008)
New Revision: 8612

Modified:
   trunk/plearn/math/pl_math.cc
   trunk/plearn/math/pl_math.h
Log:
fixed: again the bug with logadd in pl_math (problem with Grapher.cc)



Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2008-02-29 20:55:30 UTC (rev 8611)
+++ trunk/plearn/math/pl_math.cc	2008-02-29 21:00:53 UTC (rev 8612)
@@ -179,6 +179,10 @@
 {
     return logadd(double(log_a), double(log_b));
 }
+real logadd(double log_a, real log_b)
+{
+    return logadd(log_a, double(log_b));
+}
 #endif
 
 real square_f(real x)

Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2008-02-29 20:55:30 UTC (rev 8611)
+++ trunk/plearn/math/pl_math.h	2008-02-29 21:00:53 UTC (rev 8612)
@@ -572,6 +572,7 @@
 #ifdef USEFLOAT
 //! Required in some template functions when compiling in float mode.
 real logadd(real log_a, real log_b);
+real logadd(double log_a, real log_b);
 #endif
 
 //!  compute log(exp(log_a)-exp(log_b)) without losing too much precision



From nouiz at mail.berlios.de  Fri Feb 29 22:35:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 29 Feb 2008 22:35:26 +0100
Subject: [Plearn-commits] r8613 - in trunk/plearn: base db io vmat
Message-ID: <200802292135.m1TLZQv8030189@sheep.berlios.de>

Author: nouiz
Date: 2008-02-29 22:35:25 +0100 (Fri, 29 Feb 2008)
New Revision: 8613

Modified:
   trunk/plearn/base/HelpSystem.cc
   trunk/plearn/base/Object.cc
   trunk/plearn/db/getDataSet.cc
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
   trunk/plearn/vmat/VVMatrix.cc
Log:
big modif to get the $INCLUDE{} macro preprocessing set the mtime valid in VMatrix.



Modified: trunk/plearn/base/HelpSystem.cc
===================================================================
--- trunk/plearn/base/HelpSystem.cc	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/base/HelpSystem.cc	2008-02-29 21:35:25 UTC (rev 8613)
@@ -873,7 +873,8 @@
     map<string, string> dic;
     dic["PAGE_TITLE"]= title;
     PStream stm= openString(the_prologue, PStream::raw_ascii);
-    return readAndMacroProcess(stm, dic, false);
+    time_t latest = 0;
+    return readAndMacroProcess(stm, dic, latest, false);
 }
 
 string HelpSystem::helpEpilogueHTML()

Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/base/Object.cc	2008-02-29 21:35:25 UTC (rev 8613)
@@ -827,7 +827,8 @@
 
 Object* macroLoadObject(const PPath &filename, map<string, string>& vars)
 {
-    string script = readFileAndMacroProcess(filename, vars);
+    time_t date = 0;
+    string script = readFileAndMacroProcess(filename, vars, date);
     PStream sin = openString(script,PStream::plearn_ascii);
     Object* o = readObject(sin);
     o->build();

Modified: trunk/plearn/db/getDataSet.cc
===================================================================
--- trunk/plearn/db/getDataSet.cc	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/db/getDataSet.cc	2008-02-29 21:35:25 UTC (rev 8613)
@@ -105,7 +105,9 @@
         //            "use a standard .vmat or .pymat script"); } 
         else if (ext == "vmat" || ext == "txtmat") {
             use_params = true;
-            const string code = readFileAndMacroProcess(dataset, params);
+            time_t date = 0;
+            const string code = readFileAndMacroProcess(dataset, params, date);
+
             if (removeblanks(code)[0] == '<') {
                 // Old XML-like format.
                 PLDEPRECATED("In getDataSet - File %s is using the old XML-like VMat format, " 
@@ -118,6 +120,7 @@
                     PLERROR("In getDataSet - Object described in %s is not a VMatrix subclass",
                             dataset.absolute().c_str());
             }
+            vm->updateMtime(date);
         } else if (ext == "pymat" || ext == "py") {
             use_params = true;
             if (ext == "py")

Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/io/PyPLearnScript.cc	2008-02-29 21:35:25 UTC (rev 8613)
@@ -48,6 +48,7 @@
 #include <plearn/io/fileutils.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/base/pl_repository_revision.h>
+#include <plearn/vmat/VMatrix.h>
 
 
 namespace PLearn {
@@ -304,7 +305,8 @@
     else
     {
         PStream is     = openString( plearn_script, PStream::raw_ascii );
-        plearn_script  = readAndMacroProcess( is, vars );
+        time_t latest = 0 ;
+        plearn_script  = readAndMacroProcess( is, vars, latest );
     }
 }
 
@@ -327,6 +329,7 @@
     
     const string extension = extract_extension(filepath);
     string script;
+    time_t date = 0;
 
     PP<PyPLearnScript> pyplearn_script;
     PStream in;
@@ -363,8 +366,7 @@
                 vars[name_val.first] = name_val.second;
             }
         }
-
-        script = readFileAndMacroProcess(filepath, vars);
+        script = readFileAndMacroProcess(filepath, vars, date);
         in = openString( script, PStream::plearn_ascii );
     }
     else if(extension==".psave") // do not perform plearn macro expansion
@@ -375,7 +377,9 @@
         PLERROR("In smartLoadObject: unsupported file extension. Must be one of .pyplearn .pymat .plearn .vmat .psave");
 
     Object* o = readObject(in);
-    
+    if(extension==".vmat")
+        ((VMatrix*)o)->updateMtime(date);
+
     if ( pyplearn_script.isNotNull() )
         pyplearn_script->close();
 

Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/io/fileutils.cc	2008-02-29 21:35:25 UTC (rev 8613)
@@ -628,7 +628,7 @@
 // readFileAndMacroProcess //
 /////////////////////////////
 string readFileAndMacroProcess(const PPath& filepath, map<string, string>& variables,
-                               bool change_dir)
+                               time_t& latest, bool change_dir)
 {
     // pout << "Processing file: " << filepath.absolute() << endl;
     // Save old variables (to allow recursive calls)
@@ -658,12 +658,14 @@
         file = file.basename();
     }
 
+    latest=max(latest,mtime(file.absolute()));
+
     // Add the new file and date variables
     addFileAndDateVariables(file, variables);
 
     // Perform actual parsing and macro processing...
     PStream in = openFile(file, PStream::plearn_ascii, "r");
-    string text = readAndMacroProcess(in, variables);
+    string text = readAndMacroProcess(in, variables, latest);
 
     // Restore previous variables
     if (added)
@@ -691,7 +693,8 @@
 /////////////////////////
 // readAndMacroProcess //
 /////////////////////////
-string readAndMacroProcess(PStream& in, map<string, string>& variables, bool skip_comments)
+string readAndMacroProcess(PStream& in, map<string, string>& variables, 
+                           time_t& latest, bool skip_comments)
 {
     string text; // the processed text to return
     bool inside_a_quoted_string=false; // inside a quoted string we don't skip characters following a #
@@ -724,13 +727,13 @@
                 in.smartReadUntilNext("}", varname, true);
                 // Maybe there are macros to process to obtain the real name of the variable.
                 PStream varname_stream = openString(varname, PStream::raw_ascii);
-                varname = readAndMacroProcess(varname_stream, variables);
+                varname = readAndMacroProcess(varname_stream, variables, latest);
                 varname = removeblanks(varname);
                 map<string, string>::const_iterator it = variables.find(varname);
                 if(it==variables.end())
                     PLERROR("Macro variable ${%s} undefined", varname.c_str());
                 PStream varin = openString(it->second, PStream::raw_ascii);
-                text += readAndMacroProcess(varin, variables);
+                text += readAndMacroProcess(varin, variables, latest);
             }
             break;
 
@@ -747,7 +750,7 @@
                 if (!syntax_ok)
                     PLERROR("$CHAR syntax is: $CHAR{expr}");
                 PStream expr_stream = openString(expr, PStream::raw_ascii);
-                char ch = (char) toint(readAndMacroProcess(expr_stream, variables));
+                char ch = (char) toint(readAndMacroProcess(expr_stream, variables, latest));
                 text += ch;
             }
             break;
@@ -801,8 +804,8 @@
                         PLERROR("$DIVIDE syntax is: $DIVIDE{expr1}{expr2}");
                     PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                     PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                    string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                    string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                    string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                    string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                     real e1, e2;
                     if (!pl_isnumber(expr1_eval, &e1) || !pl_isnumber(expr2_eval, &e2)) {
                         PLERROR("In $DIVIDE{expr1}{expr2}, either 'expr1' or 'expr2' is not a number");
@@ -834,7 +837,7 @@
                     if (!syntax_ok)
                         PLERROR("$ECHO syntax is: $ECHO{expr}");
                     PStream expr_stream = openString(expr, PStream::raw_ascii);
-                    pout << readAndMacroProcess(expr_stream, variables) << endl;
+                    pout << readAndMacroProcess(expr_stream, variables, latest) << endl;
                 }
                 break;
 
@@ -851,10 +854,10 @@
                     if (!syntax_ok)
                         PLERROR("$EVALUATE syntax is: $EVALUATE{varname}");
                     PStream expr_stream = openString(expr, PStream::raw_ascii);
-                    string varname = readAndMacroProcess(expr_stream, variables);
+                    string varname = readAndMacroProcess(expr_stream, variables, latest);
                     string to_evaluate = variables[varname];
                     PStream to_evaluate_stream = openString(to_evaluate, PStream::raw_ascii);
-                    string evaluated = readAndMacroProcess(to_evaluate_stream, variables);
+                    string evaluated = readAndMacroProcess(to_evaluate_stream, variables, latest);
                     variables[varname] = evaluated;
                 }
                 break;
@@ -875,7 +878,7 @@
                 if (!syntax_ok)
                     PLERROR("$GETENV syntax is: $GETENV{expr}");
                 PStream expr_stream = openString(expr, PStream::raw_ascii);
-                string var_name = readAndMacroProcess(expr_stream, variables);
+                string var_name = readAndMacroProcess(expr_stream, variables, latest);
                 const char* var = PR_GetEnv(var_name.c_str());
 
                 if (!var)
@@ -918,7 +921,7 @@
                         PLERROR("$IF syntax is: $IF{cond}{expr_cond_true}{expr_cond_false}");
 
                     PStream cond_stream = openString(cond, PStream::raw_ascii);
-                    string evaluate_cond = readAndMacroProcess(cond_stream, variables);
+                    string evaluate_cond = readAndMacroProcess(cond_stream, variables, latest);
                     if (evaluate_cond == "1" ) {
                         expr_evaluated = expr_cond_true;
                     } else if (evaluate_cond == "0") {
@@ -927,7 +930,7 @@
                         PLERROR("$IF condition should be 0 or 1, but is %s", evaluate_cond.c_str());
                     }
                     PStream expr_stream = openString(expr_evaluated, PStream::raw_ascii);
-                    text += readAndMacroProcess(expr_stream, variables);
+                    text += readAndMacroProcess(expr_stream, variables, latest);
                 }
                 break;
 
@@ -949,11 +952,12 @@
                         else
                             PLERROR("$INCLUDE must be followed immediately by a { or <");
                         PStream pathin = openString(raw_includefilepath, PStream::raw_ascii);
-                        raw_includefilepath = readAndMacroProcess(pathin,variables);
+                        raw_includefilepath = readAndMacroProcess(pathin, variables, latest);
                         raw_includefilepath = removeblanks(raw_includefilepath);
+                        PPath p = PPath(raw_includefilepath);
                         // Read file with appropriate variable definitions.
                         text += readFileAndMacroProcess
-                            (PPath(raw_includefilepath), variables, true);
+                            (p, variables, latest);
                     }
                     break;
 
@@ -970,7 +974,7 @@
                         if (!syntax_ok)
                             PLERROR("$INT syntax is: $INT{expr}");
                         PStream expr_stream = openString(expr, PStream::raw_ascii);
-                        string expr_eval = readAndMacroProcess(expr_stream, variables);
+                        string expr_eval = readAndMacroProcess(expr_stream, variables, latest);
                         real e;
                         if (!pl_isnumber(expr_eval, &e)) {
                             PLERROR("In $INT{expr}, 'expr' is not a number");
@@ -1001,7 +1005,7 @@
                         if (!syntax_ok)
                             PLERROR("$ISDEFINED syntax is: $ISDEFINED{expr}");
                         PStream expr_stream = openString(expr, PStream::raw_ascii);
-                        string expr_eval = readAndMacroProcess(expr_stream, variables);
+                        string expr_eval = readAndMacroProcess(expr_stream, variables, latest);
                         map<string, string>::const_iterator it = variables.find(expr_eval);
                         if(it==variables.end()) {
                             // The variable is not defined.
@@ -1033,8 +1037,8 @@
                             PLERROR("$ISEQUAL syntax is: $ISEQUAL{expr1}{expr2}");
                         PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                         PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                        string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                        string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                        string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                        string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                         if (expr1_eval == expr2_eval) {
                             text += "1";
                         } else {
@@ -1064,8 +1068,8 @@
                             PLERROR("$ISHIGHER syntax is: $ISHIGHER{expr1}{expr2}");
                         PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                         PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                        string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                        string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                        string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                        string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                         real e1, e2;
                         if (!pl_isnumber(expr1_eval, &e1) || !pl_isnumber(expr2_eval, &e2)) {
                             PLERROR("In $ISHIGHER{expr1}{expr2}, either 'expr1' or 'expr2' is not a number");
@@ -1107,8 +1111,8 @@
                     PLERROR("$MINUS syntax is: $MINUS{expr1}{expr2}");
                 PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                 PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                 real e1, e2;
                 if (!pl_isnumber(expr1_eval, &e1) || !pl_isnumber(expr2_eval, &e2)) {
                     PLERROR("In $MINUS{expr1}{expr2}, either 'expr1' or 'expr2' is not a number");
@@ -1140,8 +1144,8 @@
                     PLERROR("$OR syntax is: $OR{expr1}{expr2}");
                 PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                 PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                 real e1, e2;
                 if (!pl_isnumber(expr1_eval, &e1) || !pl_isnumber(expr2_eval, &e2)) {
                     PLERROR("In $OR{expr1}{expr2}, either 'expr1' or 'expr2' is not a number");
@@ -1176,8 +1180,8 @@
                     PLERROR("$PLUS syntax is: $PLUS{expr1}{expr2}");
                 PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                 PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                 real e1, e2;
                 if (!pl_isnumber(expr1_eval, &e1) || !pl_isnumber(expr2_eval, &e2)) {
                     PLERROR("In $PLUS{expr1}{expr2}, either 'expr1' or 'expr2' is not a number");
@@ -1230,21 +1234,21 @@
                 if (!syntax_ok)
                     PLERROR("$SWITCH syntax is: $SWITCH{expr}{comp1}{val1}{comp2}{val2}...{valdef}");
                 PStream expr_stream = openString(expr, PStream::raw_ascii);
-                string expr_eval =  readAndMacroProcess(expr_stream, variables);
+                string expr_eval =  readAndMacroProcess(expr_stream, variables, latest);
                 bool not_done = true;
                 for (size_t i = 0; i < comp.size() && not_done; i++) {
                     PStream comp_stream = openString(comp[i], PStream::raw_ascii);
-                    string comp_eval = readAndMacroProcess(comp_stream, variables);
+                    string comp_eval = readAndMacroProcess(comp_stream, variables, latest);
                     if (expr_eval == comp_eval) {
                         not_done = false;
                         PStream val_stream = openString(val[i], PStream::raw_ascii);
-                        text += readAndMacroProcess(val_stream, variables);
+                        text += readAndMacroProcess(val_stream, variables, latest);
                     }
                 }
                 if (not_done) {
                     // Default value needed.
                     PStream val_stream = openString(valdef, PStream::raw_ascii);
-                    text += readAndMacroProcess(val_stream, variables);
+                    text += readAndMacroProcess(val_stream, variables, latest);
                 }
             }
             break;
@@ -1272,8 +1276,8 @@
                     PLERROR("$TIMES syntax is: $TIMES{expr1}{expr2}");
                 PStream expr1_stream = openString(expr1, PStream::raw_ascii);
                 PStream expr2_stream = openString(expr2, PStream::raw_ascii);
-                string expr1_eval = readAndMacroProcess(expr1_stream, variables);
-                string expr2_eval = readAndMacroProcess(expr2_stream, variables);
+                string expr1_eval = readAndMacroProcess(expr1_stream, variables, latest);
+                string expr2_eval = readAndMacroProcess(expr2_stream, variables, latest);
                 real e1, e2;
                 if (!pl_isnumber(expr1_eval, &e1) || !pl_isnumber(expr2_eval, &e2)) {
                     PLERROR("In $TIMES{expr1}{expr2}, either 'expr1' or 'expr2' is not a number");
@@ -1295,7 +1299,7 @@
                 if (!syntax_ok)
                     PLERROR("$UNDEFINE syntax is: $UNDEFINE{expr}");
                 PStream expr_stream = openString(expr, PStream::raw_ascii);
-                string varname = readAndMacroProcess(expr_stream, variables);
+                string varname = readAndMacroProcess(expr_stream, variables, latest);
                 while (variables.count(varname) > 0) {
                     // This loop is probably not necessary, but just in case...
                     variables.erase(varname);

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/io/fileutils.h	2008-02-29 21:35:25 UTC (rev 8613)
@@ -203,9 +203,9 @@
 //! Also every $DEFINE{varname=... } in the text will add a new varname
 //! entry in the map (the DEFINE macro will be discarded).
 //! Also every $INCLUDE{filepath} will be replaced in place by the text of
-//! the file it includes
+//! the file it includes and set latest, to max(latest,mtime(filepath))
 string readAndMacroProcess(PStream& in, map<string, string>& variables, 
-                           bool skip_comments= true);
+                           time_t& latest, bool skip_comments= true);
 
 /*! Given a filename, generates the standard PLearn variables FILEPATH,
   DIRPATH, FILENAME, FILEBASE, FILEEXT, DATE, TIME and DATETIME and
@@ -235,12 +235,27 @@
 */
 string readFileAndMacroProcess(const PPath& filepath,
                                map<string, string>& variables,
-                               bool change_dir = false);
+                               time_t& latest, bool change_dir = false);
 
+inline string readFileAndMacroProcess(const PPath& filepath,
+                               map<string, string>& variables,
+                               bool change_dir = false)
+{
+    time_t latest = 0;
+    return readFileAndMacroProcess(filepath, variables, latest, change_dir);
+}
+
+inline string readFileAndMacroProcess(const PPath& filepath, time_t& latest)
+{
+    map<string, string> variables;
+    return readFileAndMacroProcess(filepath, variables, latest);
+}
+
 inline string readFileAndMacroProcess(const PPath& filepath)
 {
     map<string, string> variables;
-    return readFileAndMacroProcess(filepath, variables);
+    time_t latest = 0;
+    return readFileAndMacroProcess(filepath, variables, latest);
 }
 
 //! Increase by one the number of references to a file.

Modified: trunk/plearn/vmat/VVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VVMatrix.cc	2008-02-29 21:00:53 UTC (rev 8612)
+++ trunk/plearn/vmat/VVMatrix.cc	2008-02-29 21:35:25 UTC (rev 8613)
@@ -142,12 +142,12 @@
 
 time_t VVMatrix::getDateOfVMat(const string& filename)
 {
-    string in=readFileAndMacroProcess(filename);
+    time_t latest = 0;
+    string in=readFileAndMacroProcess(filename, latest);
     size_t idx_source = in.find("<SOURCES>");
     size_t cidx_source;
+    latest=max(getDateOfCode(filename),latest);
 
-    time_t latest = getDateOfCode(filename),tmp;
-
     if(idx_source!=string::npos)
     {
         idx_source += strlen("<SOURCES>");     // skip beyond
@@ -167,11 +167,9 @@
                     // also check for date of possible IntVecFile
                     if(vecstr[1][0]!=slash_char)
                         vecstr[1]=extract_directory(filename)+vecstr[1];
-                    if((tmp=mtime(vecstr[1])) > latest)
-                        latest=tmp;
+                    latest=max(mtime(vecstr[1]),latest);
                 }
-                if((tmp=getDataSetDate(vecstr[0])) > latest)
-                    latest=tmp;
+                latest=max(getDataSetDate(vecstr[0]),latest);
             }
     }
     return latest;
@@ -344,7 +342,8 @@
 
 VMat VVMatrix::createPreproVMat(const string & filename)
 {
-    string in=readFileAndMacroProcess(filename);
+    time_t date_of_code = 0;
+    string in=readFileAndMacroProcess(filename,date_of_code);
     size_t idx_source     = in.find("<SOURCES>");
     size_t idx_prefilter  = in.find("<PREFILTER>");
     size_t idx_postfilter = in.find("<POSTFILTER>");
@@ -367,7 +366,7 @@
 
     PPath meta_data_dir = remove_trailing_slash(filename)+".metadata";
     force_mkdir(meta_data_dir);
-    time_t date_of_code = getDateOfVMat(filename);
+    date_of_code=max(date_of_code,getDateOfVMat(filename));
 
     // remove pollution : all stuff that has possibly been interrupted during computation
     rm ( meta_data_dir / "incomplete.*" );
@@ -612,8 +611,8 @@
     if (the_filename != "") {
         setMetaDataDir( PPath(the_filename+".metadata").absolute() );
         force_mkdir( getMetaDataDir() );
-
-        code = readFileAndMacroProcess(the_filename);
+        time_t latest=0;
+        code = readFileAndMacroProcess(the_filename,latest);
         if(removeblanks(code)[0]=='<') // old xml-like format
             the_mat = createPreproVMat(the_filename);
         else  // New standard PLearn object description format
@@ -625,6 +624,7 @@
         }
 
         updateMtime(the_mat->getMtime());
+        updateMtime(latest);
         length_ = the_mat.length();
         width_ = the_mat.width();
 



From saintmlx at mail.berlios.de  Fri Feb 29 23:19:23 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 29 Feb 2008 23:19:23 +0100
Subject: [Plearn-commits] r8614 - trunk/plearn/base
Message-ID: <200802292219.m1TMJNaH001781@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-29 23:19:23 +0100 (Fri, 29 Feb 2008)
New Revision: 8614

Modified:
   trunk/plearn/base/ProgressBar.cc
   trunk/plearn/base/ProgressBar.h
Log:
- remote function to disable progress bars



Modified: trunk/plearn/base/ProgressBar.cc
===================================================================
--- trunk/plearn/base/ProgressBar.cc	2008-02-29 21:35:25 UTC (rev 8613)
+++ trunk/plearn/base/ProgressBar.cc	2008-02-29 22:19:23 UTC (rev 8614)
@@ -44,6 +44,7 @@
  ******************************************************* */
 
 //#include <iomanip>
+#include "RemoteDeclareMethod.h"
 #include "ProgressBar.h"
 #include "stringutils.h"
 #include <math.h>
@@ -269,6 +270,26 @@
 }
 
 
+void setProgressBarPlugin(string pb_type)
+{
+    if(pb_type == "none")
+        ProgressBar::setPlugin(new NullProgressBarPlugin);
+    else if(pb_type == "text")
+        ProgressBar::setPlugin(new TextProgressBarPlugin(cerr));
+    else
+        PLERROR("Invalid ProgressBar type: %s", pb_type.c_str());
+}
+
+BEGIN_DECLARE_REMOTE_FUNCTIONS
+
+    declareFunction("setProgressBarPlugin", &setProgressBarPlugin,
+                    (BodyDoc("Sets the progress bar plugin.\n"),
+                     ArgDoc ("pb_type", "one of: 'none','text'.")));
+
+END_DECLARE_REMOTE_FUNCTIONS
+
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/base/ProgressBar.h
===================================================================
--- trunk/plearn/base/ProgressBar.h	2008-02-29 21:35:25 UTC (rev 8613)
+++ trunk/plearn/base/ProgressBar.h	2008-02-29 22:19:23 UTC (rev 8614)
@@ -184,6 +184,9 @@
 };
 
 
+void setProgressBarPlugin(string pb_type);
+
+
 } // end of namespace PLearn
 
 #endif



From saintmlx at mail.berlios.de  Fri Feb 29 23:23:14 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 29 Feb 2008 23:23:14 +0100
Subject: [Plearn-commits] r8615 - trunk/plearn/vmat
Message-ID: <200802292223.m1TMNErZ002041@sheep.berlios.de>

Author: saintmlx
Date: 2008-02-29 23:23:14 +0100 (Fri, 29 Feb 2008)
New Revision: 8615

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
- added remote getColumn method



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-02-29 22:19:23 UTC (rev 8614)
+++ trunk/plearn/vmat/VMatrix.cc	2008-02-29 22:23:14 UTC (rev 8615)
@@ -167,6 +167,13 @@
          RetDoc ("row i vector")));
 
     declareMethod(
+        rmm, "getColumn", &VMatrix::remote_getColumn,
+        (BodyDoc("Returns a row of a matrix \n"),
+         ArgDoc ("i", "Position of the row to get.\n"),
+         RetDoc ("row i vector")));
+
+
+    declareMethod(
         rmm, "getString", &VMatrix::getString,
         (BodyDoc("Returns an element of a matrix as a string\n"),
          ArgDoc ("i", "Position of the row to get.\n"),
@@ -1548,6 +1555,14 @@
         v[i] = get(i,j);
 }
 
+Vec VMatrix::remote_getColumn(int i) const
+{
+    Vec v(length());
+    getColumn(i,v);
+    return v;
+}
+
+
 ///////////////
 // getSubRow //
 ///////////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-02-29 22:19:23 UTC (rev 8614)
+++ trunk/plearn/vmat/VMatrix.h	2008-02-29 22:23:14 UTC (rev 8615)
@@ -546,6 +546,9 @@
     /// VMat's length).
     virtual void getColumn(int i, Vec v) const;
 
+    //! remote version of getColumn: return newly alloc'd vec
+    Vec remote_getColumn(int i) const;
+
     /**
      *  Return true iff the input vector is in this VMat (we compare only the
      *  input part).  If the parameter 'i' is provided, it will be filled with



