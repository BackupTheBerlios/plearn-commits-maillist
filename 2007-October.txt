From nouiz at mail.berlios.de  Mon Oct  1 20:04:05 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 1 Oct 2007 20:04:05 +0200
Subject: [Plearn-commits] r8125 - trunk
Message-ID: <200710011804.l91I451Y007005@sheep.berlios.de>

Author: nouiz
Date: 2007-10-01 20:04:04 +0200 (Mon, 01 Oct 2007)
New Revision: 8125

Modified:
   trunk/pymake.config.model
Log:
-ssh timeout of 20s
-a not working first version to compile for condor


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-09-28 18:50:08 UTC (rev 8124)
+++ trunk/pymake.config.model	2007-10-01 18:04:04 UTC (rev 8125)
@@ -63,7 +63,7 @@
 # Force use of ssh @ Apstat & Lisa
 domain_name = socket.getfqdn();
 if domain_name.endswith('apstat.com') or domain_name.endswith('iro.umontreal.ca') or domain_name.endswith('public'):
-    rshcommand= 'ssh -x'
+    rshcommand= 'ssh -x -o ConnecTimeout=30'
 
 default_nice_value = 1
 
@@ -254,7 +254,7 @@
 # do not specify any option from that group.
 options_choices = [
   [ 'g++', 'g++3', 'g++no-cygwin', 'icc', 'icc8', 'icc9', 'mpi',
-    'purify', 'quantify', 'vc++' ],
+    'purify', 'quantify', 'vc++', 'condor' ],
   
   [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
     'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg' ],
@@ -529,6 +529,15 @@
               cpp_definitions = ['USING_MPI=0'],
               linker = 'g++' )
 
+pymakeOption( name = 'condor',
+              description = 'compiling with g++, with no MPI support',
+              compiler = 'g++',
+              compileroptions = '-Wno-deprecated '+pedantic_mode+'-Wno-long-long -ftemplate-depth-100 ' \
+                      + gcc_opt_options,
+              cpp_definitions = ['USING_MPI=0'],
+              linker = 'condor_compile g++',
+              linkeroptions = '-static')
+
 pymakeOption( name = 'g++3',
               description = 'compiling with g++3 (version 3.0), with no MPI support',
               compiler = 'g++3',



From nouiz at mail.berlios.de  Mon Oct  1 20:34:36 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 1 Oct 2007 20:34:36 +0200
Subject: [Plearn-commits] r8126 - trunk
Message-ID: <200710011834.l91IYa1X008853@sheep.berlios.de>

Author: nouiz
Date: 2007-10-01 20:34:35 +0200 (Mon, 01 Oct 2007)
New Revision: 8126

Modified:
   trunk/pymake.config.model
Log:
fix typo


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-10-01 18:04:04 UTC (rev 8125)
+++ trunk/pymake.config.model	2007-10-01 18:34:35 UTC (rev 8126)
@@ -63,7 +63,7 @@
 # Force use of ssh @ Apstat & Lisa
 domain_name = socket.getfqdn();
 if domain_name.endswith('apstat.com') or domain_name.endswith('iro.umontreal.ca') or domain_name.endswith('public'):
-    rshcommand= 'ssh -x -o ConnecTimeout=30'
+    rshcommand= 'ssh -x -o ConnectTimeout=30'
 
 default_nice_value = 1
 



From manzagop at mail.berlios.de  Mon Oct  1 21:01:51 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 1 Oct 2007 21:01:51 +0200
Subject: [Plearn-commits] r8127 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710011901.l91J1pI6010346@sheep.berlios.de>

Author: manzagop
Date: 2007-10-01 21:01:51 +0200 (Mon, 01 Oct 2007)
New Revision: 8127

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h
Log:
mNNet stands for (minimal?,minibatch?) NNet. It's a lean concentrate
of the current NatGradNNet implementation of a neural net removing
all bells and whistles. Think about subclassing it for your needs.


Added: trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc	2007-10-01 18:34:35 UTC (rev 8126)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc	2007-10-01 19:01:51 UTC (rev 8127)
@@ -0,0 +1,569 @@
+// -*- C++ -*-
+
+// mNNet.cc
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio, PAM
+
+/*! \file mNNet.cc */
+
+#include "mNNet.h"
+//#include <plearn/math/pl_erf.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    mNNet,
+    "Multi-layer neural network based on matrix-matrix multiplications",
+    "This is a LEAN neural network. No bells, no whistles.\n"
+    );
+
+mNNet::mNNet()
+    : noutputs(-1),
+      init_lrate(0.0),
+      lrate_decay(0.0),
+      minibatch_size(1),
+      output_type("NLL"),
+      output_layer_L1_penalty_factor(0.0),
+      n_layers(-1),
+      cumulative_training_time(0.0)
+{
+    random_gen = new PRandom();
+}
+
+void mNNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "noutputs", &mNNet::noutputs,
+                  OptionBase::buildoption,
+                  "Number of outputs of the neural network, which can be derived from output_type and targetsize_\n");
+
+    declareOption(ol, "hidden_layer_sizes", &mNNet::hidden_layer_sizes,
+                  OptionBase::buildoption,
+                  "Defines the architecture of the multi-layer neural network by\n"
+                  "specifying the number of hidden units in each hidden layer.\n");
+
+    declareOption(ol, "init_lrate", &mNNet::init_lrate,
+                  OptionBase::buildoption,
+                  "Initial learning rate\n");
+
+    declareOption(ol, "lrate_decay", &mNNet::lrate_decay,
+                  OptionBase::buildoption,
+                  "Learning rate decay factor\n");
+
+    // TODO Why this dependance on test_minibatch_size?
+    declareOption(ol, "minibatch_size", &mNNet::minibatch_size,
+                  OptionBase::buildoption,
+                  "Update the parameters only so often (number of examples).\n"
+                  "Must be greater or equal to test_minibatch_size\n");
+
+    declareOption(ol, "output_type", 
+                  &mNNet::output_type,
+                  OptionBase::buildoption,
+                  "type of output cost: 'cross_entropy' for binary classification,\n"
+                  "'NLL' for classification problems, or 'MSE' for regression.\n");
+
+    declareOption(ol, "output_layer_L1_penalty_factor",
+                  &mNNet::output_layer_L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L1 regularization term, i.e.\n"
+                  "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| during training.\n"
+                  "Gets multiplied by the learning rate. Only on output layer!!");
+
+    declareOption(ol, "n_layers", &mNNet::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers of weights plus 1 (ie. 3 for a neural net with one hidden layer).\n"
+                  "Needs not be specified explicitly (derived from hidden_layer_sizes).\n");
+
+    declareOption(ol, "layer_sizes", &mNNet::layer_sizes,
+                  OptionBase::learntoption,
+                  "Derived from hidden_layer_sizes, inputsize_ and noutputs\n");
+
+    declareOption(ol, "layer_params", &mNNet::layer_params,
+                  OptionBase::learntoption,
+                  "Parameters used while training, for each layer, organized as follows: layer_params[i] \n"
+                  "is a matrix of dimension layer_sizes[i+1] x (layer_sizes[i]+1)\n"
+                  "containing the neuron biases in its first column.\n");
+
+    declareOption(ol, "cumulative_training_time", &mNNet::cumulative_training_time,
+                  OptionBase::learntoption,
+                  "Cumulative training time since age=0, in seconds.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+// TODO - reloading an object will not work! layer_params will juste get lost.
+void mNNet::build_()
+{
+    // *** Sanity checks ***
+
+    if (!train_set)
+        return;
+    if (output_type=="MSE")
+    {
+        if (noutputs<0) noutputs = targetsize_;
+        else PLASSERT_MSG(noutputs==targetsize_,"mNNet: noutputs should be -1 or match data's targetsize");
+    }
+    else if (output_type=="NLL")
+    {
+        // TODO add a check on noutput's value
+        if (noutputs<0)
+            PLERROR("mNNet: if output_type=NLL (classification), one \n"
+                    "should provide noutputs = number of classes, or possibly\n"
+                    "1 when 2 classes\n");
+    }
+    else if (output_type=="cross_entropy")
+    {
+        if(noutputs!=1)
+            PLERROR("mNNet: if output_type=cross_entropy, then \n"
+                    "noutputs should be 1.\n");
+    }
+    else PLERROR("mNNet: output_type should be cross_entropy, NLL or MSE\n");
+
+    if( output_layer_L1_penalty_factor < 0. )
+        PLWARNING("mNNet::build_ - output_layer_L1_penalty_factor is negative!\n");
+
+    // *** Determine topology ***
+    inputsize_ = train_set->inputsize();
+    while (hidden_layer_sizes.length()>0 && hidden_layer_sizes[hidden_layer_sizes.length()-1]==0)
+        hidden_layer_sizes.resize(hidden_layer_sizes.length()-1);
+    n_layers = hidden_layer_sizes.length()+2; 
+    layer_sizes.resize(n_layers);
+    layer_sizes.subVec(1,n_layers-2) << hidden_layer_sizes;
+    layer_sizes[0]=inputsize_;
+    layer_sizes[n_layers-1]=noutputs;
+
+    // *** Allocate memory for params and gradients ***
+    int n_params=0;
+    int n_neurons=0;
+    for (int i=0;i<n_layers-1;i++)    {
+        n_neurons+=layer_sizes[i+1];
+        n_params+=layer_sizes[i+1]*(1+layer_sizes[i]);
+    }
+    all_params.resize(n_params);
+    all_params_gradient.resize(n_params);
+
+    // *** Set handles ***
+    layer_params.resize(n_layers-1);
+    layer_params_gradient.resize(n_layers-1);
+    biases.resize(n_layers-1);
+    weights.resize(n_layers-1);
+
+    for (int i=0,p=0;i<n_layers-1;i++)    {
+        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        layer_params[i]=all_params.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        biases[i]=layer_params[i].subMatColumns(0,1);
+        weights[i]=layer_params[i].subMatColumns(1,layer_sizes[i]); // weights[0] from layer 0 to layer 1
+        layer_params_gradient[i]=all_params_gradient.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        p+=np;
+    }
+
+    // *** Allocate memory for outputs and gradients on neurons ***
+    neuron_extended_outputs.resize(minibatch_size,layer_sizes[0]+1+n_neurons+n_layers);
+    neuron_gradients.resize(minibatch_size,n_neurons);
+
+    // *** Set handles and biases ***
+    neuron_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_extended_outputs_per_layer.resize(n_layers); // layer 0 = input, layer n_layers-1 = output
+    neuron_gradients_per_layer.resize(n_layers); // layer 0 not used
+
+    int k=0, kk=0;
+    for (int i=0;i<n_layers;i++)
+    {
+        neuron_extended_outputs_per_layer[i] = neuron_extended_outputs.subMatColumns(k,1+layer_sizes[i]);
+        neuron_extended_outputs_per_layer[i].column(0).fill(1.0); // for biases
+        neuron_outputs_per_layer[i]=neuron_extended_outputs_per_layer[i].subMatColumns(1,layer_sizes[i]);
+        k+=1+layer_sizes[i];
+        if(i>0) {
+            neuron_gradients_per_layer[i] = neuron_gradients.subMatColumns(kk,layer_sizes[i]);
+            kk+=layer_sizes[i];
+        }
+    }
+
+}
+
+// ### Nothing to add here, simply calls build_
+void mNNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void mNNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(hidden_layer_sizes, copies);
+    deepCopyField(layer_sizes, copies);
+    deepCopyField(all_params, copies);
+    deepCopyField(biases, copies);
+    deepCopyField(weights, copies);
+    deepCopyField(layer_params, copies);
+    deepCopyField(all_params_gradient, copies);
+    deepCopyField(layer_params_gradient, copies);
+    deepCopyField(neuron_gradients, copies);
+    deepCopyField(neuron_gradients_per_layer, copies);
+    deepCopyField(neuron_extended_outputs, copies);
+    deepCopyField(neuron_extended_outputs_per_layer, copies);
+    deepCopyField(neuron_outputs_per_layer, copies);
+    deepCopyField(targets, copies);
+    deepCopyField(example_weights, copies);
+    deepCopyField(train_costs, copies);
+}
+
+
+int mNNet::outputsize() const
+{
+    return noutputs;
+}
+
+void mNNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    inherited::forget();
+    for (int i=0;i<n_layers-1;i++)
+    {
+        real delta = 1/sqrt(real(layer_sizes[i]));
+        random_gen->fill_random_uniform(weights[i],-delta,delta);
+        biases[i].clear();
+    }
+    stage = 0;
+    cumulative_training_time=0.0;
+}
+
+void mNNet::train()
+{
+
+    if (inputsize_<0)
+        build();
+    if(!train_set)
+        PLERROR("In NNet::train, you did not setTrainingSet");
+    if(!train_stats)
+        setTrainStatsCollector(new VecStatsCollector());
+
+    targets.resize(minibatch_size,targetsize());  // the train_set's targetsize()
+    example_weights.resize(minibatch_size);
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    train_costs.resize(minibatch_size,train_cost_names.length()-2); 
+    train_costs.fill(MISSING_VALUE) ;
+    Vec costs_plus_time(train_costs.width()+2);
+    costs_plus_time[train_costs.width()] = MISSING_VALUE;
+    costs_plus_time[train_costs.width()+1] = MISSING_VALUE;
+    Vec costs = costs_plus_time.subVec(0,train_costs.width());
+
+    train_stats->forget();
+
+    int b, sample, nsamples;
+    nsamples = train_set->length();
+    Vec input,target;   // TODO discard these variables.
+
+    Profiler::reset("training");
+    Profiler::start("training");
+
+    for( ; stage<nstages; stage++)
+    {
+        sample = stage % nsamples;
+        b = stage % minibatch_size;
+        input = neuron_outputs_per_layer[0](b);
+        target = targets(b);
+        train_set->getExample(sample, input, target, example_weights[b]);
+        if (b+1==minibatch_size) // TODO do also special end-case || stage+1==nstages)
+        {
+            onlineStep(stage, targets, train_costs, example_weights );
+            for (int i=0;i<minibatch_size;i++)  {
+                costs << train_costs(b);    // TODO Is the copy necessary? Might be
+                                            // better to waste some memory in
+                                            // train_costs instead
+                train_stats->update( costs_plus_time );
+            }
+        }
+    }
+
+    Profiler::end("training");
+    if (verbosity>0)
+        Profiler::report(cout);
+    // Take care of the timing stats.
+    const Profiler::Stats& stats = Profiler::getStats("training");
+    costs.fill(MISSING_VALUE);
+    real ticksPerSec = Profiler::ticksPerSecond();
+    real cpu_time = (stats.user_duration+stats.system_duration)/ticksPerSec;
+    cumulative_training_time += cpu_time;
+    costs_plus_time[train_costs.width()] = cpu_time;
+    costs_plus_time[train_costs.width()+1] = cumulative_training_time;
+    train_stats->update( costs_plus_time );
+    train_stats->finalize(); // finalize statistics for this epoch
+}
+
+void mNNet::onlineStep(int t, const Mat& targets,
+                             Mat& train_costs, Vec example_weights)
+{
+    PLASSERT(targets.length()==minibatch_size && train_costs.length()==minibatch_size && example_weights.length()==minibatch_size);
+
+    fpropNet(minibatch_size);
+    fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);
+
+    // mean gradient over minibatch_size examples has less variance
+    // can afford larger learning rate (divide by sqrt(minibatch) 
+    // instead of minibatch)
+    real lrate = init_lrate/(1 + t*lrate_decay);
+    lrate /= sqrt(real(minibatch_size));
+
+    for (int i=n_layers-1;i>0;i--)
+    {
+        // here neuron_gradients_per_layer[i] contains the gradient on activations (weighted sums)
+        //      (minibatch_size x layer_size[i])
+        Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
+        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
+        Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
+
+        if (i>1) // if not first hidden layer then compute gradient on previous layer
+        {
+            // propagate gradients
+            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
+                            weights[i-1],false,1,0);
+            // propagate through tanh non-linearity
+            // TODO IN NEED OF OPTIMIZATION
+            for (int j=0;j<previous_neurons_gradient.length();j++)
+            {
+                real* grad = previous_neurons_gradient[j];
+                real* out = previous_neurons_output[j];
+                for (int k=0;k<previous_neurons_gradient.width();k++,out++)
+                    grad[k] *= (1 - *out * *out); // gradient through tanh derivative
+            }
+        }
+        // compute gradient on parameters and update them in one go (more efficient)
+        productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            -lrate,1);
+    }
+
+    // Output layer L1 regularization
+    if( output_layer_L1_penalty_factor != 0. )    {
+        real L1_delta = lrate * output_layer_L1_penalty_factor;
+        real* m_i = layer_params[n_layers-2].data();
+        for(int i=0; i<layer_params[n_layers-2].length(); i++,m_i+=layer_params[n_layers-2].mod())  {
+            for(int j=0; j<layer_params[n_layers-2].width(); j++)   {
+                if( m_i[j] > L1_delta )
+                    m_i[j] -= L1_delta;
+                else if( m_i[j] < -L1_delta )
+                    m_i[j] += L1_delta;
+                else
+                    m_i[j] = 0.;
+            }
+        }
+    }
+
+}
+
+void mNNet::computeOutput(const Vec& input, Vec& output) const
+{
+    neuron_outputs_per_layer[0](0) << input;
+    fpropNet(1);
+    output << neuron_outputs_per_layer[n_layers-1](0);
+}
+
+//! compute (pre-final-non-linearity) network top-layer output given input
+void mNNet::fpropNet(int n_examples) const
+{
+    PLASSERT_MSG(n_examples<=minibatch_size,"mNNet::fpropNet: nb input vectors treated should be <= minibatch_size\n");
+    for (int i=0;i<n_layers-1;i++)
+    {
+        Mat prev_layer = neuron_extended_outputs_per_layer[i];
+        Mat next_layer = neuron_outputs_per_layer[i+1];
+        if (n_examples!=minibatch_size) {
+            prev_layer = prev_layer.subMatRows(0,n_examples);
+            next_layer = next_layer.subMatRows(0,n_examples);
+        }
+
+        // try to use BLAS for the expensive operation
+        productScaleAcc(next_layer, prev_layer, false, layer_params[i], true, 1, 0);
+
+        // compute layer's output non-linearity
+        if (i+1<n_layers-1) {
+            for (int k=0;k<n_examples;k++)  {
+                Vec L=next_layer(k);
+                compute_tanh(L,L);
+            }
+        }   else if (output_type=="NLL")    {
+            for (int k=0;k<n_examples;k++)  {
+                Vec L=next_layer(k);
+                log_softmax(L,L);
+            }
+        }   else if (output_type=="cross_entropy")  {
+            for (int k=0;k<n_examples;k++)  {
+                Vec L=next_layer(k);
+                log_sigmoid(L,L);
+            }
+         }
+    }
+}
+
+//! compute train costs given the (pre-final-non-linearity) network top-layer output
+void mNNet::fbpropLoss(const Mat& output, const Mat& target, const Vec& example_weight, Mat& costs) const
+{
+    int n_examples = output.length();
+    Mat out_grad = neuron_gradients_per_layer[n_layers-1];
+    if (n_examples!=minibatch_size)
+        out_grad = out_grad.subMatRows(0,n_examples);
+    int target_class;
+    Vec outp, grad;
+    if (output_type=="NLL") {
+        for (int i=0;i<n_examples;i++)  {
+            target_class = int(round(target(i,0)));
+            #ifdef BOUNDCHECK
+            if(target_class>=noutputs)
+                PLERROR("In mNNet::fbpropLoss one target value %d is higher then allowed by nout %d",
+                        target_class, noutputs);
+            #endif          
+            outp = output(i);
+            grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            costs(i,0) = -outp[target_class];
+            costs(i,1) = (target_class == argmax(outp))?0:1;
+            grad[target_class]-=1;
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+    }
+    else if(output_type=="cross_entropy")   {
+        for (int i=0;i<n_examples;i++)  {
+            target_class = int(round(target(i,0)));
+            outp = output(i);
+            grad = out_grad(i);
+            exp(outp,grad); // map log-prob to prob
+            if( target_class == 1 ) {
+                costs(i,0) = - outp[0];
+                costs(i,1) = (grad[0]>0.5)?0:1;
+            }   else    {
+                costs(i,0) = - pl_log( 1.0 - grad[0] );
+                costs(i,1) = (grad[0]>0.5)?1:0;
+            }
+            grad[0] -= (real)target_class; // ?
+            if (example_weight[i]!=1.0)
+                costs(i,0) *= example_weight[i];
+        }
+    }
+    else // if (output_type=="MSE")
+    {
+        substract(output,target,out_grad);
+        for (int i=0;i<n_examples;i++)  {
+            costs(i,0) = pownorm(out_grad(i));
+            if (example_weight[i]!=1.0) {
+                out_grad(i) *= example_weight[i];
+                costs(i,0) *= example_weight[i];
+            }
+        }
+    }
+}
+
+void mNNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    Vec w(1);
+    w[0]=1;
+    Mat outputM = output.toMat(1,output.length());
+    Mat targetM = target.toMat(1,output.length());
+    Mat costsM = costs.toMat(1,costs.length());
+    fbpropLoss(outputM,targetM,w,costsM);
+}
+
+void mNNet::computeOutputs(const Mat& input, Mat& output) const
+{
+    PLASSERT(test_minibatch_size<=minibatch_size);
+    neuron_outputs_per_layer[0].subMat(0,0,input.length(),input.width()) << input;
+    fpropNet(input.length());
+    output << neuron_outputs_per_layer[n_layers-1].subMat(0,0,output.length(),output.width());
+}
+void mNNet::computeOutputsAndCosts(const Mat& input, const Mat& target, 
+                                      Mat& output, Mat& costs) const
+{//TODO
+    int n=input.length();
+    PLASSERT(target.length()==n);
+    output.resize(n,outputsize());
+    costs.resize(n,nTestCosts());
+    computeOutputs(input,output);
+
+    Vec w(n);
+    w.fill(1);
+    fbpropLoss(output,target,w,costs);
+}
+
+TVec<string> mNNet::getTestCostNames() const
+{
+    TVec<string> costs;
+    if (output_type=="NLL")
+    {
+        costs.resize(3);
+        costs[0]="NLL";
+        costs[1]="class_error";
+    }
+    else if (output_type=="cross_entropy")  {
+        costs.resize(3);
+        costs[0]="cross_entropy";
+        costs[1]="class_error";
+    }
+    else if (output_type=="MSE")
+    {
+        costs.resize(1);
+        costs[0]="MSE";
+    }
+    return costs;
+}
+
+TVec<string> mNNet::getTrainCostNames() const
+{
+    TVec<string> costs = getTestCostNames();
+    costs.append("train_seconds");
+    costs.append("cum_train_seconds");
+    return costs;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h	2007-10-01 18:34:35 UTC (rev 8126)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h	2007-10-01 19:01:51 UTC (rev 8127)
@@ -0,0 +1,232 @@
+// -*- C++ -*-
+// mNNet.h
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio, PAM
+
+/*! \file mNNet.h */
+
+
+#ifndef mNNet_INC
+#define mNNet_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * Multi-layer neural network based on matrix-matrix multiplications.
+ */
+class mNNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! number of outputs to the network
+    int noutputs;
+
+    //! sizes of hidden layers
+    TVec<int> hidden_layer_sizes;
+
+    //! initial learning rate
+    real init_lrate;
+
+    //! learning rate decay factor
+    real lrate_decay;
+
+    //! update the parameters only so often
+    int minibatch_size;
+
+    //! type of output cost: "NLL" for classification problems, "MSE" for regression
+    string output_type;
+
+    //! L1 penalty applied to the output layer's parameters
+    real output_layer_L1_penalty_factor;
+
+public:
+    //#####  Public Not Build Options  ########################################
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    mNNet();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+    virtual void computeOutputs(const Mat& input, Mat& output) const;
+    virtual void computeOutputsAndCosts(const Mat& input, const Mat& target, 
+                                        Mat& output, Mat& costs) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods: computeOutputAndCosts(),
+    // computeCostsOnly(), test(), nTestCosts(), nTrainCosts(),
+    // resetInternalState(), isStatefulLearner()
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(mNNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+
+    //! number of layers of weights (2 for a neural net with one hidden layer)
+    int n_layers;
+
+    //! layer sizes (derived from hidden_layer_sizes, inputsize_ and outputsize_)
+    TVec<int> layer_sizes;
+
+    //! All the parameters in one vector
+    Vec all_params;
+    //! Alternate access to the params - one matrix per layer
+    TVec<Mat> biases;
+    TVec<Mat> weights;
+    //! Alternate access to the params - one matrix of dimension
+    //! layer_sizes[i+1] x (layer_sizes[i]+1) per layer
+    //! (neuron biases in the first column)
+    TVec<Mat> layer_params;
+
+    //! Gradient structures - reflect the parameter structures 
+    Vec all_params_gradient; 
+    TVec<Mat> layer_params_gradient;
+
+    //! Outputs of the neurons
+    Mat neuron_gradients;   // one row per example of a minibatch, has
+                            // concatenation of layer 0, layer 1, ... gradients.
+    TVec<Mat> neuron_gradients_per_layer;   // pointing into neuron_gradients
+                                            // (one row per example of a minibatch)
+
+    //! Gradients on the neurons - same structure as for outputs
+    mutable Mat neuron_extended_outputs;
+    mutable TVec<Mat> neuron_extended_outputs_per_layer;    // with 1's in the
+                                                            // first pseudo-neuron, for doing biases
+    mutable TVec<Mat> neuron_outputs_per_layer;  
+
+    Mat targets; // one target row per example in a minibatch
+    Vec example_weights; // one element per example in a minibatch
+    Mat train_costs; // one row per example in a minibatch
+
+    //! Holds training time, an additional cost
+    real cumulative_training_time;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    //! One minibatch training step
+    virtual void onlineStep(int t, const Mat& targets, Mat& train_costs, Vec example_weights);
+
+    //! compute a minibatch of size n_examples network top-layer output given layer 0 output (= network input)
+    //! (note that log-probabilities are computed for classification tasks, output_type=NLL)
+    virtual void fpropNet(int n_examples) const;
+
+    //! compute train costs given the network top-layer output
+    //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
+    virtual void fbpropLoss(const Mat& output, const Mat& target, const Vec& example_weights, Mat& train_costs) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(mNNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Tue Oct  2 17:54:19 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 2 Oct 2007 17:54:19 +0200
Subject: [Plearn-commits] r8128 - trunk/plearn/vmat
Message-ID: <200710021554.l92FsJAT017276@sheep.berlios.de>

Author: tihocan
Date: 2007-10-02 17:54:19 +0200 (Tue, 02 Oct 2007)
New Revision: 8128

Modified:
   trunk/plearn/vmat/NoSplitSplitter.cc
Log:
Fixed typo and indent

Modified: trunk/plearn/vmat/NoSplitSplitter.cc
===================================================================
--- trunk/plearn/vmat/NoSplitSplitter.cc	2007-10-01 19:01:51 UTC (rev 8127)
+++ trunk/plearn/vmat/NoSplitSplitter.cc	2007-10-02 15:54:19 UTC (rev 8128)
@@ -127,7 +127,8 @@
 //////////////
 TVec<VMat> NoSplitSplitter::getSplit(int k)
 {
-    PLASSERT_MSG(k==0,"In NoSplitSplitter::getSplit - asked for a split that don't exist!");
+    PLASSERT_MSG(k==0,"In NoSplitSplitter::getSplit - Asked for a split that "
+                      "does not exist");
     TVec<VMat> result(1);
     result[0] = dataset;
     return result;



From saintmlx at mail.berlios.de  Tue Oct  2 21:44:29 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 2 Oct 2007 21:44:29 +0200
Subject: [Plearn-commits] r8129 - trunk/plearn/math
Message-ID: <200710021944.l92JiTC5016357@sheep.berlios.de>

Author: saintmlx
Date: 2007-10-02 21:44:28 +0200 (Tue, 02 Oct 2007)
New Revision: 8129

Modified:
   trunk/plearn/math/VecStatsCollector.cc
   trunk/plearn/math/VecStatsCollector.h
Log:
- added remote version of 'append' method



Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-10-02 15:54:19 UTC (rev 8128)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-10-02 19:44:28 UTC (rev 8129)
@@ -228,7 +228,16 @@
          ArgDoc ("x"," the new data\n"),
          ArgDoc ("weight"," the weight of the data")));
 
- }
+   declareMethod(
+       rmm, "append", &VecStatsCollector::remote_append,
+       (BodyDoc("Appends all the StatsCollectors of an "
+                "existing VecStatsCollector into this one.\n"),
+        ArgDoc ("vsc","the other VecStatsCollector\n"),
+        ArgDoc ("fieldname_prefix","prefix concatenated "
+                "to the existing field names\n"),
+        ArgDoc ("new_fieldnames","new name for appended fields (overrides prefix)\n")));
+   
+}
 
 int VecStatsCollector::length() const
 {
@@ -797,6 +806,13 @@
     }
 }
 
+void VecStatsCollector::remote_append(const VecStatsCollector* vsc, 
+                                      const string fieldname_prefix,
+                                      const TVec<string>& new_fieldnames)
+{
+    append(*vsc, fieldname_prefix, new_fieldnames);
+}
+
 void VecStatsCollector::merge(VecStatsCollector& other)
 {
     PLASSERT_MSG(fieldnames == other.fieldnames,

Modified: trunk/plearn/math/VecStatsCollector.h
===================================================================
--- trunk/plearn/math/VecStatsCollector.h	2007-10-02 15:54:19 UTC (rev 8128)
+++ trunk/plearn/math/VecStatsCollector.h	2007-10-02 19:44:28 UTC (rev 8129)
@@ -278,6 +278,10 @@
     void append(const VecStatsCollector& vsc, const string fieldname_prefix="",
                 const TVec<string>& new_fieldnames = TVec<string>() );
 
+    //! remote version of append: takes pointer to other VecStatsCollector
+    void remote_append(const VecStatsCollector* vsc, const string fieldname_prefix,
+                       const TVec<string>& new_fieldnames);
+
     //! sets the size of the observation window
     virtual void setWindowSize(int sz);
 



From louradou at mail.berlios.de  Tue Oct  2 22:59:37 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 2 Oct 2007 22:59:37 +0200
Subject: [Plearn-commits] r8130 - in trunk: plearn/ker
	plearn_learners/unsupervised
Message-ID: <200710022059.l92KxbBe019858@sheep.berlios.de>

Author: louradou
Date: 2007-10-02 22:59:36 +0200 (Tue, 02 Oct 2007)
New Revision: 8130

Modified:
   trunk/plearn/ker/GeodesicDistanceKernel.cc
   trunk/plearn/ker/GeodesicDistanceKernel.h
   trunk/plearn/ker/Kernel.cc
   trunk/plearn/ker/Kernel.h
   trunk/plearn/ker/VMatKernel.cc
   trunk/plearn/ker/VMatKernel.h
   trunk/plearn_learners/unsupervised/Isomap.cc
   trunk/plearn_learners/unsupervised/Isomap.h
   trunk/plearn_learners/unsupervised/KernelProjection.cc
Log:
some debugging and improvement with kernel functions



Modified: trunk/plearn/ker/GeodesicDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/GeodesicDistanceKernel.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -71,6 +71,19 @@
     build();
 }
 
+GeodesicDistanceKernel::GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn,
+                                               string the_geodesic_file, bool the_pow_distance,
+					       string the_method)
+    : geodesic_file(the_geodesic_file),
+      knn(the_knn),
+      pow_distance(the_pow_distance),
+      shortest_algo(the_method)
+{
+    distance_kernel = the_distance_kernel;
+    build();
+}
+
+
 PLEARN_IMPLEMENT_OBJECT(GeodesicDistanceKernel,
                         "Computes the geodesic distance based on k nearest neighbors.",
                         ""

Modified: trunk/plearn/ker/GeodesicDistanceKernel.h
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/GeodesicDistanceKernel.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -88,6 +88,9 @@
     //! Convenient constructor.
     GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
                            string the_geodesic_file = "", bool the_pow_distance = false);
+    GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
+                           string the_geodesic_file = "", bool the_pow_distance = false,
+			   string the_method = "floyd");
 
     // ******************
     // * Kernel methods *

Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/Kernel.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -48,11 +48,11 @@
 #include <plearn/base/tostring.h>
 #include <plearn/base/ProgressBar.h>
 #include <plearn/math/TMat_maths.h>
+#include <plearn/base/RemoteDeclareMethod.h>
 
 namespace PLearn {
 using namespace std;
 
-using namespace std;
   
 PLEARN_IMPLEMENT_ABSTRACT_OBJECT(Kernel,
                                  "A Kernel is a real-valued function K(x,y).",
@@ -112,6 +112,21 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void Kernel::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "returnComputedGramMatrix", &Kernel::returnComputedGramMatrix,
+        (BodyDoc("\n"),
+         RetDoc ("Returns the Gram Matrix")));
+}
+
 ///////////
 // build //
 ///////////
@@ -370,6 +385,15 @@
     }
 }
 
+Mat Kernel::returnComputedGramMatrix() const
+{
+    if (!data)
+        PLERROR("Kernel::returnComputedGramMatrix should be called only after setDataForKernelMatrix");
+    int l=data.length();
+    Mat K(l,l);
+    computeGramMatrix(K);
+    return K;
+}
 /////////////////////////////
 // computeSparseGramMatrix //
 /////////////////////////////

Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/Kernel.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -45,7 +45,6 @@
 
 #include <plearn/base/Object.h>
 #include <plearn/vmat/VMat.h>
-//#include "PLMPI.h"
 
 namespace PLearn {
 using namespace std;
@@ -74,6 +73,9 @@
     int n_examples;
 
     static void declareOptions(OptionList& ol);
+    
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
 
 public:
 
@@ -148,6 +150,7 @@
 
     //! Call evaluate_i_j to fill each of the entries (i,j) of symmetric matrix K.
     virtual void computeGramMatrix(Mat K) const;
+    virtual Mat returnComputedGramMatrix() const;
 
     /**
      *  Fill K[i] with the non-zero elements of row i of the Gram matrix.

Modified: trunk/plearn/ker/VMatKernel.cc
===================================================================
--- trunk/plearn/ker/VMatKernel.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/VMatKernel.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -2,7 +2,7 @@
 
 // VMatKernel.cc
 //
-// Copyright (C) 2005 Benoit Cromp 
+// Copyright (C) 2005 Benoit Cromp
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -36,7 +36,7 @@
  * $Id$ 
  ******************************************************* */
 
-// Authors: Benoit Cromp
+// Authors: Benoit Cromp, Jerome Louradour
 
 /*! \file VMatKernel.cc */
 
@@ -49,8 +49,8 @@
 //////////////////
 // VMatKernel //
 //////////////////
+// ### Initialize all fields to their default value here
 VMatKernel::VMatKernel() 
-/* ### Initialize all fields to their default value here */
 {
     // ...
 
@@ -69,20 +69,14 @@
 ////////////////////
 void VMatKernel::declareOptions(OptionList& ol)
 {
-    // ### Declare all of this object's options here
-    // ### For the "flags" of each option, you should typically specify  
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or 
-    // ### OptionBase::tuningoption. Another possible flag to be combined with
-    // ### is OptionBase::nosave
+    declareOption(ol,"source",&VMatKernel::source,
+                  OptionBase::buildoption,
+        "Gram matrix");
 
-    // ### ex:
-    // declareOption(ol, "myoption", &VMatKernel::myoption, OptionBase::buildoption,
-    //               "Help text describing this option");
-    // ...
+    declareOption(ol,"train_indices",&VMatKernel::train_indices,
+                  OptionBase::learntoption,
+        "List of (real)indices corresponding to training samples");
 
-    declareOption(ol,"source",&VMatKernel::source,OptionBase::buildoption,
-                  "Gram matrix");
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -102,39 +96,109 @@
 ////////////
 void VMatKernel::build_()
 {
-    // ### This method should do the real building of the object,
-    // ### according to set 'options', in *any* situation. 
-    // ### Typical situations include:
-    // ###  - Initial building of an object from a few user-specified options
-    // ###  - Building of a "reloaded" object: i.e. from the complete set of all serialised options.
-    // ###  - Updating or "re-building" of an object after a few "tuning" options have been modified.
-    // ### You should assume that the parent class' build_() has already been called.
+    PLASSERT( !(source) || ( source->length() == source->width() ) );
+    if ( !specify_dataset )
+        train_indices.resize(0);
 }
 
 //////////////
 // evaluate //
 //////////////
-real VMatKernel::evaluate(const Vec& x1, const Vec& x2) const {
-    PLASSERT( source );
+real VMatKernel::evaluate(const Vec& x1, const Vec& x2) const
+{
     PLASSERT( x1.size()==1 && x2.size()==1 );
-    return source->get(int(x1[0]),int(x2[0]));
+    return evaluate( x1[0], x2[0] );
 }
 
-void VMatKernel::computeGramMatrix(Mat K) const
+real VMatKernel::evaluate(real x1, real x2) const
 {
+    PLASSERT( fabs(x1-(real)((int)x1)) < 0.1 );
+    PLASSERT( fabs(x2-(real)((int)x2)) < 0.1 );
+    return evaluate( int(x1), int(x2) );
+}
+
+real VMatKernel::evaluate(int x1, int x2) const
+{
     PLASSERT( source );
-    K << source->toMat();
+    PLASSERT( x1 >= 0 );
+    PLASSERT( x1 < source->length() );
+    PLASSERT( x2 >= 0 );
+    PLASSERT( x2 < source->width() );
+    return source->get( x1, x2);
 }
 
-
 //////////////////
 // evaluate_i_j //
 //////////////////
-real VMatKernel::evaluate_i_j(int i, int j) const {
+real VMatKernel::evaluate_i_j(int i, int j) const
+{
     PLASSERT( source );
-    return source->get(i,j);
+    if( train_indices.length() == 0 )
+        return evaluate( i, j );
+    PLASSERT( i >= 0 );
+    PLASSERT( i < n_examples );
+    PLASSERT( j >= 0 );
+    PLASSERT( j < n_examples );
+    return evaluate( train_indices[i], train_indices[j] );
 }
 
+//////////////////
+// evaluate_i_x //
+//////////////////
+real VMatKernel::evaluate_i_x(int i, const Vec& x, real squared_norm_of_x) const
+{
+    if( train_indices.length() == 0 )
+        return evaluate( i, (int)x[0] );
+    PLASSERT( i >= 0 );
+    PLASSERT( i < n_examples );
+    PLASSERT( x.size() == 1 );
+    return evaluate( train_indices[i], x[0] );
+}
+
+//////////////////
+// evaluate_x_i //
+//////////////////
+real VMatKernel::evaluate_x_i(const Vec& x, int i, real squared_norm_of_x) const
+{
+//    if( is_symmetric )
+//        return evaluate_i_x( i, x, squared_norm_of_x);
+    if( train_indices.length() == 0 )
+        return evaluate( (int)x[0], i);
+    PLASSERT( i >= 0 );
+    PLASSERT( i < n_examples );
+    PLASSERT( x.size() == 1 );
+    return evaluate( x[0], train_indices[i]);
+}
+
+///////////////////////
+// computeGramMatrix //
+///////////////////////
+void VMatKernel::computeGramMatrix(Mat K) const
+{
+    PLASSERT( source );
+    if( train_indices.length() > 0 )
+    {
+        K.resize(n_examples, n_examples);
+        if( is_symmetric )
+            for(int i = 0; i < n_examples; i++ )
+	    {
+	        K(i,i) = evaluate( train_indices[i], train_indices[i] );
+                for(int j = 0; j < i; j++ )
+                {
+	            K(i,j) = evaluate( train_indices[i], train_indices[j] );
+		    K(j,i) = K(i,j);
+	        }
+            }
+        else
+            for(int i = 0; i < n_examples; i++ )
+                for(int j = 0; j < n_examples; j++ )
+	            K(i,j) = evaluate( train_indices[i], train_indices[j] );
+    }
+    else
+        K << source->toMat();
+}
+
+
 /////////////////////////////////
 // makeDeepCopyFromShallowCopy //
 /////////////////////////////////
@@ -142,27 +206,60 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    // ### Call deepCopyField on all "pointer-like" fields 
-    // ### that you wish to be deepCopied rather than 
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("VMatKernel::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    deepCopyField(source, copies);
+    deepCopyField(train_indices, copies);
 }
 
-/* ### This method will be overridden if computations need to be done,
-   ### or to forward the call to another object.
-   ### In this case, be careful that it may be called BEFORE the build_()
-   ### method has been called, if the 'specify_dataset' option is used.
 ////////////////////////////
 // setDataForKernelMatrix //
 ////////////////////////////
-void VMatKernel::setDataForKernelMatrix(VMat the_data) {
+void VMatKernel::setDataForKernelMatrix(VMat the_data)
+{
+    inherited::setDataForKernelMatrix(the_data);
+
+    if( n_examples > 1 )
+    {
+        PLASSERT( data_inputsize == 1 );
+        train_indices.resize(n_examples);
+        for(int i = 0; i < n_examples; i++)
+        {
+            PLASSERT( the_data->get(i,0) >= 0 );
+            PLASSERT( !(source) || ( the_data->get(i,0) < (real)source->width() ) );
+            train_indices[i] = the_data->get(i,0);
+        }
+    }
+    else
+    {
+	PLASSERT( source );
+        PLWARNING("in VMatKernel::setDataForKernelMatrix: all values in the VMatKernel source are taken into acount for training");
+	n_examples = source->width();
+	train_indices.resize(0);
+    }
 }
-*/
 
+////////////////////////////
+// addDataForKernelMatrix //
+////////////////////////////
+void VMatKernel::addDataForKernelMatrix(const Vec& newRow)
+{
+    PLASSERT( newRow.size() == 1 );
+    inherited::addDataForKernelMatrix( newRow );
+    if( train_indices.length() == 0 )
+    {
+        PLASSERT( source );
+	n_examples = source->width();
+        train_indices.resize( n_examples );
+        for(int i = 0; i < n_examples; i++)
+	    train_indices[i] = (real)i;
+    }
+    PLASSERT( newRow[0] > 0 );
+    PLASSERT( !(source) || ( newRow[0] < source->width() ) );
+    train_indices.resize( n_examples + 1 );
+    train_indices[ n_examples ] = newRow[0];
+    n_examples += 1;
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/ker/VMatKernel.h
===================================================================
--- trunk/plearn/ker/VMatKernel.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn/ker/VMatKernel.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -44,10 +44,11 @@
 #ifndef VMatKernel_INC
 #define VMatKernel_INC
 
-#include <plearn/ker/Kernel.h>
+#include "Kernel.h"
 #include <plearn/vmat/VMat.h>
 
 namespace PLearn {
+using namespace std;
 
 class VMatKernel: public Kernel
 {
@@ -62,8 +63,11 @@
     // * Protected options *
     // *********************
 
-    // ### declare protected option fields (such as learnt parameters) here
-    // ...
+    // *************************
+    // * Public learnt options *
+    // *************************
+
+    Vec train_indices;
     
 public:
 
@@ -73,6 +77,7 @@
 
     VMat source;
 
+
     // ### declare public option fields (such as build options) here
     // ...
 
@@ -124,25 +129,20 @@
 
     //! Compute K(x1,x2).
     virtual real evaluate(const Vec& x1, const Vec& x2) const;
+    virtual real evaluate(real x1, real x2) const;
+    virtual real evaluate(int x1, int x2) const;
 
-    //! Overridden for efficiency.
-    virtual real evaluate_i_j(int i, int j) const;
-    virtual void computeGramMatrix(Mat K) const;
-
-    // *** SUBCLASS WRITING: ***
-    // While in general not necessary, in case of particular needs 
-    // (efficiency concerns for ex) you may also want to overload
-    // some of the following methods:
-    // virtual real evaluate_i_x(int i, const Vec& x, real squared_norm_of_x=-1) const;
-    // virtual real evaluate_x_i(const Vec& x, int i, real squared_norm_of_x=-1) const;
-    // virtual real evaluate_i_x_again(int i, const Vec& x, real squared_norm_of_x=-1, bool first_time = false) const;
-    // virtual real evaluate_x_i_again(const Vec& x, int i, real squared_norm_of_x=-1, bool first_time = false) const;
-    // virtual void setDataForKernelMatrix(VMat the_data);
-    // virtual void addDataForKernelMatrix(const Vec& newRow);
+    //! Overridden methods
+    virtual void setDataForKernelMatrix(VMat the_data);
+    virtual void addDataForKernelMatrix(const Vec& newRow);
+    virtual real evaluate_i_j(int i, int j) const; //! Compute K(xi,xj) on training samples
+    virtual real evaluate_i_x(int i, const Vec& x, real squared_norm_of_x=-1) const;
+    virtual real evaluate_x_i(const Vec& x, int i, real squared_norm_of_x=-1) const;
+    virtual void computeGramMatrix(Mat K) const;   //! Overridden for more efficiency
+    
+    //! Maybe later someone wants to implement a special behaviours with these functions
     // virtual void setParameters(Vec paramvec);
     // virtual Vec getParameters() const;
-  
-
 };
 
 // Declares a few other classes and functions related to this class.

Modified: trunk/plearn_learners/unsupervised/Isomap.cc
===================================================================
--- trunk/plearn_learners/unsupervised/Isomap.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn_learners/unsupervised/Isomap.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -52,11 +52,13 @@
 ////////////
 Isomap::Isomap() 
     : geodesic_file(""),
+      geodesic_method("djikstra"),
       knn(10)
 {
     kernel_is_distance = true;
     // Default distance kernel is the classical Euclidean distance.
     distance_kernel = new DistanceKernel(2);
+    min_eigenvalue = 0;
 }
 
 PLEARN_IMPLEMENT_OBJECT(Isomap,
@@ -93,6 +95,10 @@
     declareOption(ol, "geodesic_file", &Isomap::geodesic_file, OptionBase::buildoption,
                   "If provided, the geodesic distances will be saved in this file in binary format.");
 
+    declareOption(ol, "geodesic_method", &Isomap::geodesic_method, OptionBase::buildoption,
+                  "'floyd' or 'djikstra': the method to compute the geodesic distances."
+		  "(cf. GeodesicDistanceKernel)");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 
@@ -130,7 +136,7 @@
     if (distance_kernel &&
         (!kpca_kernel ||
          (dynamic_cast<GeodesicDistanceKernel*>((Kernel*) kpca_kernel))->distance_kernel != distance_kernel)) {
-        this->kpca_kernel = new GeodesicDistanceKernel(distance_kernel, knn, geodesic_file, true);
+        this->kpca_kernel = new GeodesicDistanceKernel(distance_kernel, knn, geodesic_file, true, geodesic_method);
         // We have modified the KPCA kernel, we must rebuild the KPCA.
         inherited::build();
     }

Modified: trunk/plearn_learners/unsupervised/Isomap.h
===================================================================
--- trunk/plearn_learners/unsupervised/Isomap.h	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn_learners/unsupervised/Isomap.h	2007-10-02 20:59:36 UTC (rev 8130)
@@ -74,6 +74,7 @@
 
     Ker distance_kernel;
     string geodesic_file;
+    string geodesic_method;
     int knn;
 
     // ****************

Modified: trunk/plearn_learners/unsupervised/KernelProjection.cc
===================================================================
--- trunk/plearn_learners/unsupervised/KernelProjection.cc	2007-10-02 19:44:28 UTC (rev 8129)
+++ trunk/plearn_learners/unsupervised/KernelProjection.cc	2007-10-02 20:59:36 UTC (rev 8130)
@@ -201,6 +201,7 @@
 ///////////////////
 void KernelProjection::computeOutput(const Vec& input, Vec& output) const
 {
+    PLASSERT( outputsize() > 0 );
     static real* result_ptr;
     if (first_output) {
         // Initialize k_x_xi, used_eigenvectors and result correctly.
@@ -243,7 +244,9 @@
 void KernelProjection::forget()
 {
     stage = 0;
+    cout << "forget: n_comp_kept = " << n_comp_kept << endl;
     n_comp_kept = n_comp;
+    cout << "forget: n_comp_kept = " << n_comp_kept << endl;
     n_examples = 0;
     first_output = true;
     last_input.resize(0);
@@ -338,7 +341,6 @@
     time_for_gram = clock() - time_for_gram;
     if (verbosity >= 3) {
         cout << flush;
-        cout << "Time to compute the Gram matrix: " << real(time_for_gram) / real(CLOCKS_PER_SEC) << endl;
     }
     // (2) Compute its eigenvectors and eigenvalues.
     eigenVecOfSymmMat(gram, n_comp + ignore_n_first, eigenvalues, eigenvectors);
@@ -346,12 +348,14 @@
         eigenvalues = eigenvalues.subVec(ignore_n_first, eigenvalues.length() - ignore_n_first);
         eigenvectors = eigenvectors.subMatRows(ignore_n_first, eigenvectors.length() - ignore_n_first);
     }
+    
     n_comp_kept = eigenvalues.length(); // Could be different of n_comp.
     // (3) Discard low eigenvalues.
     int p = 0;
     while (p < n_comp_kept && eigenvalues[p] > min_eigenvalue)
         p++;
     n_comp_kept = p;
+
     // (4) Optionally remove the discarded components.
     if (free_extra_components) {
         eigenvalues.resize(n_comp_kept);



From manzagop at mail.berlios.de  Wed Oct  3 18:03:42 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 3 Oct 2007 18:03:42 +0200
Subject: [Plearn-commits] r8131 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710031603.l93G3gHc006390@sheep.berlios.de>

Author: manzagop
Date: 2007-10-03 18:03:41 +0200 (Wed, 03 Oct 2007)
New Revision: 8131

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h
Log:
Reorganized code in onlineStep() by creating bpropNet(), bpropUpdateNet()
and l1regularizeOutput().


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc	2007-10-02 20:59:36 UTC (rev 8130)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.cc	2007-10-03 16:03:41 UTC (rev 8131)
@@ -210,6 +210,8 @@
         }
     }
 
+    Profiler::activate();
+
 }
 
 // ### Nothing to add here, simply calls build_
@@ -335,58 +337,9 @@
 
     fpropNet(minibatch_size);
     fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);
+    bpropUpdateNet(t);
 
-    // mean gradient over minibatch_size examples has less variance
-    // can afford larger learning rate (divide by sqrt(minibatch) 
-    // instead of minibatch)
-    real lrate = init_lrate/(1 + t*lrate_decay);
-    lrate /= sqrt(real(minibatch_size));
-
-    for (int i=n_layers-1;i>0;i--)
-    {
-        // here neuron_gradients_per_layer[i] contains the gradient on activations (weighted sums)
-        //      (minibatch_size x layer_size[i])
-        Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
-        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
-        Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
-
-        if (i>1) // if not first hidden layer then compute gradient on previous layer
-        {
-            // propagate gradients
-            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
-                            weights[i-1],false,1,0);
-            // propagate through tanh non-linearity
-            // TODO IN NEED OF OPTIMIZATION
-            for (int j=0;j<previous_neurons_gradient.length();j++)
-            {
-                real* grad = previous_neurons_gradient[j];
-                real* out = previous_neurons_output[j];
-                for (int k=0;k<previous_neurons_gradient.width();k++,out++)
-                    grad[k] *= (1 - *out * *out); // gradient through tanh derivative
-            }
-        }
-        // compute gradient on parameters and update them in one go (more efficient)
-        productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
-                            neuron_extended_outputs_per_layer[i-1],false,
-                            -lrate,1);
-    }
-
-    // Output layer L1 regularization
-    if( output_layer_L1_penalty_factor != 0. )    {
-        real L1_delta = lrate * output_layer_L1_penalty_factor;
-        real* m_i = layer_params[n_layers-2].data();
-        for(int i=0; i<layer_params[n_layers-2].length(); i++,m_i+=layer_params[n_layers-2].mod())  {
-            for(int j=0; j<layer_params[n_layers-2].width(); j++)   {
-                if( m_i[j] > L1_delta )
-                    m_i[j] -= L1_delta;
-                else if( m_i[j] < -L1_delta )
-                    m_i[j] += L1_delta;
-                else
-                    m_i[j] = 0.;
-            }
-        }
-    }
-
+    l1regularizeOutputs();
 }
 
 void mNNet::computeOutput(const Vec& input, Vec& output) const
@@ -490,6 +443,103 @@
     }
 }
 
+//! Performs the backprop update. Must be called after the fbpropNet.
+void mNNet::bpropUpdateNet(int t)
+{
+    // mean gradient over minibatch_size examples has less variance
+    // can afford larger learning rate (divide by sqrt(minibatch)
+    // instead of minibatch)
+    real lrate = init_lrate/(1 + t*lrate_decay);
+    lrate /= sqrt(real(minibatch_size));
+
+    for (int i=n_layers-1;i>0;i--)  {
+        // here neuron_gradients_per_layer[i] contains the gradient on
+        // activations (weighted sums)
+        //      (minibatch_size x layer_size[i])
+        Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
+        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
+        Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
+
+        if (i>1) // if not first hidden layer then compute gradient on previous layer
+        {
+            // propagate gradients
+            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
+                            weights[i-1],false,1,0);
+            // propagate through tanh non-linearity
+            // TODO IN NEED OF OPTIMIZATION
+            for (int j=0;j<previous_neurons_gradient.length();j++)  {
+                real* grad = previous_neurons_gradient[j];
+                real* out = previous_neurons_output[j];
+                for (int k=0;k<previous_neurons_gradient.width();k++,out++)
+                    grad[k] *= (1 - *out * *out); // gradient through tanh derivative
+            }
+        }
+        // compute gradient on parameters and update them in one go (more
+        // efficient)
+        productScaleAcc(layer_params[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            -lrate,1);
+    }
+}
+
+//! Computes the gradients without doing the update.
+//! Must be called after fbpropLoss
+void mNNet::bpropNet(int t)
+{
+    for (int i=n_layers-1;i>0;i--)  {
+        // here neuron_gradients_per_layer[i] contains the gradient on
+        // activations (weighted sums)
+        //      (minibatch_size x layer_size[i])
+        Mat previous_neurons_gradient = neuron_gradients_per_layer[i-1];
+        Mat next_neurons_gradient = neuron_gradients_per_layer[i];
+        Mat previous_neurons_output = neuron_outputs_per_layer[i-1];
+
+        if (i>1) // if not first hidden layer then compute gradient on previous layer
+        {
+            // propagate gradients
+            productScaleAcc(previous_neurons_gradient,next_neurons_gradient,false,
+                            weights[i-1],false,1,0);
+            // propagate through tanh non-linearity
+            // TODO IN NEED OF OPTIMIZATION
+            for (int j=0;j<previous_neurons_gradient.length();j++)  {
+                real* grad = previous_neurons_gradient[j];
+                real* out = previous_neurons_output[j];
+                for (int k=0;k<previous_neurons_gradient.width();k++,out++)
+                    grad[k] *= (1 - *out * *out); // gradient through tanh derivative
+            }
+        }
+        // compute gradient on parameters 
+        productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            1,0);
+    }
+}
+
+void mNNet::l1regularizeOutputs()
+{
+    // mean gradient over minibatch_size examples has less variance
+    // can afford larger learning rate (divide by sqrt(minibatch)
+    // instead of minibatch)
+    real lrate = init_lrate/(1 + stage*lrate_decay);
+    lrate /= sqrt(real(minibatch_size));
+
+    // Output layer L1 regularization
+    if( output_layer_L1_penalty_factor != 0. )    {
+        real L1_delta = lrate * output_layer_L1_penalty_factor;
+        real* m_i = layer_params[n_layers-2].data();
+        for(int i=0; i<layer_params[n_layers-2].length();i++,m_i+=layer_params[n_layers-2].mod())  {
+            for(int j=0; j<layer_params[n_layers-2].width(); j++)   {
+                if( m_i[j] > L1_delta )
+                    m_i[j] -= L1_delta;
+                else if( m_i[j] < -L1_delta )
+                    m_i[j] += L1_delta;
+                else
+                    m_i[j] = 0.;
+            }
+        }
+    }
+}
+
 void mNNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h	2007-10-02 20:59:36 UTC (rev 8130)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/mNNet.h	2007-10-03 16:03:41 UTC (rev 8131)
@@ -199,6 +199,11 @@
     //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
     virtual void fbpropLoss(const Mat& output, const Mat& target, const Vec& example_weights, Mat& train_costs) const;
 
+    virtual void bpropUpdateNet(const int t);
+    virtual void bpropNet(const int t);
+    
+    void l1regularizeOutputs();
+
 private:
     //#####  Private Member Functions  ########################################
 



From manzagop at mail.berlios.de  Wed Oct  3 18:05:34 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 3 Oct 2007 18:05:34 +0200
Subject: [Plearn-commits] r8132 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710031605.l93G5Y4r006645@sheep.berlios.de>

Author: manzagop
Date: 2007-10-03 18:05:33 +0200 (Wed, 03 Oct 2007)
New Revision: 8132

Added:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
This class inherits from mNNet and implements Pascal V's idea for a stochastic gradient
based on a step size adapted in a way similar to rprop.


Added: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-03 16:03:41 UTC (rev 8131)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-03 16:05:33 UTC (rev 8132)
@@ -0,0 +1,269 @@
+// -*- C++ -*-
+
+// PvGradNNet.cc
+//
+// Copyright (C) 2007 PA M, Pascal V
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: P AM, Pascal V
+
+/*! \file PvGradNNet.cc */
+
+#include "PvGradNNet.h"
+#include <plearn/math/pl_erf.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PvGradNNet,
+    "Multi-layer neural network for experimenting with Pascal V's gradient idea.",
+    "See the twiki's SRPropGradient entry.\n"
+    );
+
+PvGradNNet::PvGradNNet()
+    : mNNet(),
+      pv_initial_stepsize(1e-1),
+      pv_min_stepsize(1e-6),
+      pv_max_stepsize(50.0),
+      pv_acceleration(1.2),
+      pv_deceleration(0.5),
+      pv_min_samples(2),
+      pv_required_confidence(0.80),
+      pv_random_sample_step(false)
+
+{
+    random_gen = new PRandom();
+}
+
+void PvGradNNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "pv_initial_stepsize",
+                  &PvGradNNet::pv_initial_stepsize,
+                  OptionBase::buildoption,
+                  "Initial size of steps in parameter space");
+
+    declareOption(ol, "pv_min_stepsize",
+                  &PvGradNNet::pv_min_stepsize,
+                  OptionBase::buildoption,
+                  "Minimal size of steps in parameter space");
+
+    declareOption(ol, "pv_max_stepsize",
+                  &PvGradNNet::pv_max_stepsize,
+                  OptionBase::buildoption,
+                  "Maximal size of steps in parameter space");
+
+    declareOption(ol, "pv_acceleration",
+                  &PvGradNNet::pv_acceleration,
+                  OptionBase::buildoption,
+                  "Coefficient by which to multiply the step sizes.");
+
+    declareOption(ol, "pv_deceleration",
+                  &PvGradNNet::pv_deceleration,
+                  OptionBase::buildoption,
+                  "Coefficient by which to multiply the step sizes.");
+
+    declareOption(ol, "pv_min_samples",
+                  &PvGradNNet::pv_min_samples,
+                  OptionBase::buildoption,
+                  "PV's minimum number of samples to estimate gradient sign.\n"
+                  "This should at least be 2.");
+
+    declareOption(ol, "pv_required_confidence",
+                  &PvGradNNet::pv_required_confidence,
+                  OptionBase::buildoption,
+                  "Minimum required confidence (probability of being positive or negative) for taking a step.");
+
+    declareOption(ol, "pv_random_sample_step",
+                  &PvGradNNet::pv_random_sample_step,
+                  OptionBase::buildoption,
+                  "If this is set to true, then we will randomly choose the step sign\n"
+                  "for each parameter based on the estimated probability of it being\n"
+                  "positive or negative.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+// TODO - reloading an object will not work! layer_params will juste get lost.
+void PvGradNNet::build_()
+{
+    pv_gradstats = new VecStatsCollector();
+
+    int n = all_params.length();
+    pv_all_stepsigns.resize(n);
+    pv_all_stepsizes.resize(n);
+    //pv_all_nsamples.resize(n);    // *stat*
+
+    // Get some structure on the previous Vecs
+    pv_layer_stepsigns.resize(n_layers-1);
+    pv_layer_stepsizes.resize(n_layers-1);
+    //pv_layer_nsamples.resize(n_layers-1); // *stat*
+    for (int i=0,p=0;i<n_layers-1;i++)  {
+        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        //pv_layer_nsamples[i]=pv_all_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1); // *stat*
+        p+=np;
+    }
+
+}
+
+// ### Nothing to add here, simply calls build_
+void PvGradNNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void PvGradNNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(pv_gradstats, copies);
+    deepCopyField(pv_all_stepsigns, copies);
+    deepCopyField(pv_layer_stepsigns, copies);
+    deepCopyField(pv_all_stepsizes, copies);
+    deepCopyField(pv_layer_stepsizes, copies);
+    //deepCopyField(pv_all_nsamples, copies); // *stat*
+    //deepCopyField(pv_layer_nsamples, copies); // *stat*
+
+}
+
+void PvGradNNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    inherited::forget();
+
+    pv_gradstats->forget();
+    pv_all_stepsigns.fill(0);
+    pv_all_stepsizes.fill(pv_initial_stepsize);
+    //pv_all_nsamples.fill(0);
+}
+
+//! Performs the backprop update. Must be called after the fbpropNet.
+void PvGradNNet::bpropUpdateNet(int t)
+{
+    bpropNet(t);
+    pv_gradstats->update(all_params_gradient);
+
+    int np = all_params.length();
+    int ns;
+    real m, e, prob_pos, prob_neg;
+
+    for(int k=0; k<np; k++) {
+        StatsCollector& st = pv_gradstats->getStats(k);
+        ns = (int)st.nnonmissing();
+        if(ns>pv_min_samples)   {
+            m = st.mean();
+            e = st.stderror();
+            // test to see if numerical problems
+            if( fabs(m) < 1e-15 || e < 1e-15 )  {
+                cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                continue;
+            }
+
+            // TODO - for current treatment, not necessary to compute actual prob.
+            // Comparing the ratio would be sufficient.
+            prob_pos = gauss_01_cum(m/e);
+            prob_neg = 1.-prob_pos;
+
+            if(!pv_random_sample_step)  {
+    
+                // We adapt the stepsize before taking the step
+                // gradient is positive
+                if(prob_pos>=pv_required_confidence)    {
+                    //pv_all_stepsizes[k] *= (pv_all_stepsigns[k]?pv_acceleration:pv_deceleration);
+                    if(pv_all_stepsigns[k]>0)   {
+                        pv_all_stepsizes[k]*=pv_acceleration;
+                        if( pv_all_stepsizes[k] > pv_max_stepsize )
+                            pv_all_stepsizes[k] = pv_max_stepsize;
+                    }
+                    else if(pv_all_stepsigns[k]<0)  {
+                        pv_all_stepsizes[k]*=pv_deceleration;
+                        if( pv_all_stepsizes[k] < pv_min_stepsize )
+                            pv_all_stepsizes[k] = pv_min_stepsize;
+                    }
+                    all_params[k] -= pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = 1;
+                    st.forget();
+                }
+                // gradient is negative
+                else if(prob_neg>=pv_required_confidence)   {
+                    //pv_all_stepsizes[k] *= ((!pv_all_stepsigns[k])?pv_acceleration:pv_deceleration);
+                    if(pv_all_stepsigns[k]<0)   {
+                        pv_all_stepsizes[k]*=pv_acceleration;
+                        if( pv_all_stepsizes[k] > pv_max_stepsize )
+                            pv_all_stepsizes[k] = pv_max_stepsize;
+                    }
+                    else if(pv_all_stepsigns[k]>0)  {
+                        pv_all_stepsizes[k]*=pv_deceleration;
+                        if( pv_all_stepsizes[k] < pv_min_stepsize )
+                            pv_all_stepsizes[k] = pv_min_stepsize;
+                    }
+                    all_params[k] += pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = -1;
+                    st.forget();
+                }
+            }
+            /*else  // random sample update direction (sign)
+            {
+                bool ispos = (random_gen->binomial_sample(prob_pos)>0);
+                if(ispos) // picked positive
+                    all_params[k] += pv_all_stepsizes[k];
+                else  // picked negative
+                    all_params[k] -= pv_all_stepsizes[k];
+                pv_all_stepsizes[k] *= (pv_all_stepsigns[k]==ispos)?pv_acceleration :pv_deceleration;
+                pv_all_stepsigns[k] = ispos;
+                st.forget();
+            }*/
+        }
+        //pv_all_nsamples[k] = ns; // *stat*
+    }
+
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-03 16:03:41 UTC (rev 8131)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-03 16:05:33 UTC (rev 8132)
@@ -0,0 +1,163 @@
+// -*- C++ -*-
+// PvGradNNet.h
+//
+// Copyright (C) 2007 PA M
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: PA M
+
+/*! \file PvGradNNet.h */
+
+
+#ifndef PvGradNNet_INC
+#define PvGradNNet_INC
+
+#include <plearn_learners/generic/EXPERIMENTAL/mNNet.h>
+
+namespace PLearn {
+
+/**
+ * Multi-layer neural network based on matrix-matrix multiplications.
+ */
+class PvGradNNet : public mNNet
+{
+    typedef mNNet inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Initial size of steps in parameter space
+    real pv_initial_stepsize;
+
+    //! Bounds for the step sizes
+    real pv_min_stepsize, pv_max_stepsize;
+
+    //! Coefficients by which to multiply the step sizes
+    real pv_acceleration, pv_deceleration;
+
+    //! PV's gradient minimum number of samples to estimate confidence
+    int pv_min_samples;
+
+    //! Minimum required confidence (probability of being positive or negative)
+    //! for taking a step.
+    real pv_required_confidence;
+
+    //! If this is set to true, then we will randomly choose the step sign for
+    // each parameter based on the estimated probability of it being positive
+    // or negative.
+    bool pv_random_sample_step;
+
+public:
+    //#####  Public Not Build Options  ########################################
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    PvGradNNet();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(PvGradNNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+    // ### Declare protected option fields (such as learned parameters) here
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    virtual void bpropUpdateNet(const int t);
+//    virtual void bpropNet(const int t);
+    
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    //! accumulated statistics of gradients on each parameter.
+    PP<VecStatsCollector> pv_gradstats;
+
+    //! Temporary add-on. Allows an undetermined signed value (zero).
+    TVec<int> pv_all_stepsigns;
+    TVec< TMat<int> > pv_layer_stepsigns;
+
+    //! The step size (absolute value) to be taken for each parameter.
+    Vec pv_all_stepsizes;
+    TVec<Mat> pv_layer_stepsizes;
+
+    //! Holds the number of samples gathered for each weight
+    //! This is purely for outputing stats.
+    //TVec<int> pv_all_nsamples;
+    //TVec< TMat<int> > pv_layer_nsamples;
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PvGradNNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Wed Oct  3 18:45:29 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 18:45:29 +0200
Subject: [Plearn-commits] r8133 - trunk/scripts
Message-ID: <200710031645.l93GjTQT010273@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 18:45:27 +0200 (Wed, 03 Oct 2007)
New Revision: 8133

Modified:
   trunk/scripts/dbidispatch
Log:
help type fix


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-10-03 16:05:33 UTC (rev 8132)
+++ trunk/scripts/dbidispatch	2007-10-03 16:45:27 UTC (rev 8133)
@@ -6,13 +6,13 @@
 ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
-%s
+%(ShortHelp)s
 
 common options:
   The -h, --help print the long help(this)
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
   The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
-  The '--test' option makes dbidispatch generate the file $ScriptName, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in $ScriptName (so you can check the script).
+  The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
   The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
 
 dbidispatch --test --file=tests
@@ -87,7 +87,7 @@
   aplearn myscript.plearn numhidden=25 wd=0.001
 
 In the file of the parameter --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
-"""%ShortHelp
+"""%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
 
 if len(sys.argv) == 1:
     print ShortHelp



From nouiz at mail.berlios.de  Wed Oct  3 18:59:20 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 18:59:20 +0200
Subject: [Plearn-commits] r8134 - trunk/plearn/vmat
Message-ID: <200710031659.l93GxKXG021501@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 18:59:19 +0200 (Wed, 03 Oct 2007)
New Revision: 8134

Modified:
   trunk/plearn/vmat/MemoryVMatrix.cc
   trunk/plearn/vmat/MemoryVMatrix.h
Log:
Added option deep_copy_memory_data. This way we can save memory if the data is constant.


Modified: trunk/plearn/vmat/MemoryVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MemoryVMatrix.cc	2007-10-03 16:45:27 UTC (rev 8133)
+++ trunk/plearn/vmat/MemoryVMatrix.cc	2007-10-03 16:59:19 UTC (rev 8134)
@@ -55,14 +55,16 @@
 
 MemoryVMatrix::MemoryVMatrix()
     : synch_data(true),
-      data(Mat())
+      data(Mat()),
+      deep_copy_memory_data(true)
 {
     memory_data = data;
 }
 
 MemoryVMatrix::MemoryVMatrix(int l, int w)
     : inherited(l, w),
-      synch_data(false)
+      synch_data(false),
+      deep_copy_memory_data(true)
 {
     data.resize(l,w);
     memory_data = data;
@@ -72,7 +74,9 @@
 MemoryVMatrix::MemoryVMatrix(const Mat& the_data)
     : inherited(the_data.length(), the_data.width()),
       synch_data(true),
-      data(the_data)
+      data(the_data),
+      deep_copy_memory_data(true)
+
 {
     memory_data = the_data;
     defineSizes(the_data.width(), 0, 0);
@@ -81,7 +85,9 @@
 MemoryVMatrix::MemoryVMatrix(VMat the_source)
     : inherited(the_source->length(), the_source->width()),
       memory_data(the_source->toMat()),
-      synch_data(false)
+      synch_data(false),
+      deep_copy_memory_data(true)
+
 {
     copySizesFrom(the_source);
     setMetaInfoFrom(the_source);
@@ -109,6 +115,10 @@
                    "If provided, will be used to set this VMatrix's"
                    " fieldnames." );
 
+    declareOption( ol, "deep_copy_memory_data", &MemoryVMatrix::deep_copy_memory_data,
+                   OptionBase::buildoption,
+                   "If true, when this object is deep copied, we will deep copy the memory_data");
+
     /* This field was declared as an option, but the author does not remember
      * why. The problem is that we do not want it to be a learnt option, since
      * it may save the whole dataset pointed by 'source', which could waste a
@@ -127,9 +137,11 @@
 void MemoryVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(memory_data,  copies);
+    if(deep_copy_memory_data){
+        deepCopyField(memory_data,  copies);
+        deepCopyField(data,         copies);
+    }
     deepCopyField(fieldnames,   copies);
-    deepCopyField(data,         copies);
     deepCopyField(source,       copies);
 }
 
@@ -281,9 +293,10 @@
 ////////////
 VMat MemoryVMatrix::subMat(int i, int j, int l, int w)
 {
-    VMat result = new MemoryVMatrix(memory_data.subMat(i,j,l,w));
+    MemoryVMatrix* result = new MemoryVMatrix(memory_data.subMat(i,j,l,w));
+    result->deep_copy_memory_data=deep_copy_memory_data;
     result->setMetaInfoFrom(this);
-    return result;
+    return (VMat)result;
 }
 
 /////////

Modified: trunk/plearn/vmat/MemoryVMatrix.h
===================================================================
--- trunk/plearn/vmat/MemoryVMatrix.h	2007-10-03 16:45:27 UTC (rev 8133)
+++ trunk/plearn/vmat/MemoryVMatrix.h	2007-10-03 16:59:19 UTC (rev 8134)
@@ -74,6 +74,7 @@
 
     Mat data;
     VMat source;
+    bool deep_copy_memory_data;
 
 private:
 



From nouiz at mail.berlios.de  Wed Oct  3 18:59:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 18:59:53 +0200
Subject: [Plearn-commits] r8135 - trunk/plearn/vmat
Message-ID: <200710031659.l93Gxr0N022041@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 18:59:53 +0200 (Wed, 03 Oct 2007)
New Revision: 8135

Modified:
   trunk/plearn/vmat/ConcatSetsSplitter.cc
Log:
implemented function ConcatSetsSplitter::makeDeepCopyFromShallowCopy


Modified: trunk/plearn/vmat/ConcatSetsSplitter.cc
===================================================================
--- trunk/plearn/vmat/ConcatSetsSplitter.cc	2007-10-03 16:59:19 UTC (rev 8134)
+++ trunk/plearn/vmat/ConcatSetsSplitter.cc	2007-10-03 16:59:53 UTC (rev 8135)
@@ -95,15 +95,8 @@
 void ConcatSetsSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(splitters, copies);
 
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("ConcatSetsSplitter::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 /////////////



From nouiz at mail.berlios.de  Wed Oct  3 19:03:35 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 19:03:35 +0200
Subject: [Plearn-commits] r8136 - trunk/plearn_learners/hyper
Message-ID: <200710031703.l93H3ZVO025457@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 19:03:34 +0200 (Wed, 03 Oct 2007)
New Revision: 8136

Modified:
   trunk/plearn_learners/hyper/HyperCommand.cc
   trunk/plearn_learners/hyper/HyperCommand.h
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
added verbosity option and use it to print some data


Modified: trunk/plearn_learners/hyper/HyperCommand.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.cc	2007-10-03 16:59:53 UTC (rev 8135)
+++ trunk/plearn_learners/hyper/HyperCommand.cc	2007-10-03 17:03:34 UTC (rev 8136)
@@ -48,6 +48,7 @@
 using namespace std;
 
 HyperCommand::HyperCommand()
+    : verbosity(0)
 {
 }
 

Modified: trunk/plearn_learners/hyper/HyperCommand.h
===================================================================
--- trunk/plearn_learners/hyper/HyperCommand.h	2007-10-03 16:59:53 UTC (rev 8135)
+++ trunk/plearn_learners/hyper/HyperCommand.h	2007-10-03 17:03:34 UTC (rev 8136)
@@ -68,6 +68,13 @@
 
     PLEARN_DECLARE_ABSTRACT_OBJECT(HyperCommand);
 
+    /**
+     *  Level of verbosity. If 0, should not write anything on cerr. If >0 may
+     *  write some info on the steps performed (the amount of detail written
+     *  depends on the value of this option).
+     */
+    int verbosity; 
+
     // ****************
     // * Constructors *
     // ****************

Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2007-10-03 16:59:53 UTC (rev 8135)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2007-10-03 17:03:34 UTC (rev 8136)
@@ -217,6 +217,9 @@
                     strategy[commandnum]->setExperimentDirectory("");
             }
 
+            if(verbosity>0)
+                cerr<<"HyperLearner: starting the optimization"<<endl;
+
             results = strategy[commandnum]->optimize();
         }
 

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-03 16:59:53 UTC (rev 8135)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-03 17:03:34 UTC (rev 8136)
@@ -302,6 +302,9 @@
     Vec results;
     while(option_vals)
     {
+        if(verbosity>0)
+            cerr<<"In HyperOptimize::optimize() We optimize with parameters "<<option_names<<" with value "<<option_vals<<endl;
+
         // This will also call build and forget on the learner unless unnecessary
         // because the modified options don't require it.
         hlearner->setLearnerOptions(option_names, option_vals);



From nouiz at mail.berlios.de  Wed Oct  3 19:04:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 19:04:34 +0200
Subject: [Plearn-commits] r8137 - trunk/plearn_learners/meta
Message-ID: <200710031704.l93H4YBC026826@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 19:04:33 +0200 (Wed, 03 Oct 2007)
New Revision: 8137

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
bug fix of the train_stats size


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-10-03 17:03:34 UTC (rev 8136)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-10-03 17:04:33 UTC (rev 8137)
@@ -669,12 +669,15 @@
                 PP<ProgressBar> pb;
                 if(report_progress) pb = new ProgressBar("computing weighted training error of whole model",n);
                 train_stats->forget();
-                static Vec err(nTrainCosts());
+                Vec err(nTrainCosts());
                 int nb_class_0=0;
                 int nb_class_1=0;
                 real cum_weights_0=0;
                 real cum_weights_1=0;
 
+                bool save_forward_sub_learner_test_costs = 
+                    forward_sub_learner_test_costs;
+                forward_sub_learner_test_costs=false;
                 for (int i=0;i<n;i++)
                 {
                     if(report_progress) pb->update(i);
@@ -692,6 +695,9 @@
                     train_stats->update(err);
                 }
                 train_stats->finalize();
+                forward_sub_learner_test_costs = 
+                    save_forward_sub_learner_test_costs;
+
             }
             if (verbosity>2)
                 cout << "At stage " << stage << 



From nouiz at mail.berlios.de  Wed Oct  3 19:11:11 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 19:11:11 +0200
Subject: [Plearn-commits] r8138 - trunk/python_modules/plearn/pymake
Message-ID: <200710031711.l93HBBtT001155@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 19:11:10 +0200 (Wed, 03 Oct 2007)
New Revision: 8138

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Made some function use all information from parameter. That way the can be reused from other place as from dbi.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-10-03 17:04:33 UTC (rev 8137)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-10-03 17:11:10 UTC (rev 8138)
@@ -697,7 +697,7 @@
     else:
         return _process_distcc_hosts(contents)
 
-def process_hostspath_list(hostspath_list):
+def process_hostspath_list(hostspath_list, default_nice_value, local_hostname):
     list_of_hosts = []
     nice_values = {} # dictionary containing hostname:nice_value
     for hostspath in hostspath_list:
@@ -733,7 +733,7 @@
 
         f.close()
 
-    if 'localhost' not in list_of_hosts and myhostname not in list_of_hosts:
+    if 'localhost' not in list_of_hosts and local_hostname not in list_of_hosts:
         list_of_hosts.extend(['localhost'] * nprocesses_on_localhost)
 
     return (list_of_hosts, nice_values)
@@ -757,7 +757,7 @@
         if distcc_list_of_hosts is not None:
             print '*** Overriding distcc settings. (Remove pymake *.hosts file to use distcc settings.)'
         print '*** Parallel compilation using list of hosts from file(s): ' + string.join( hostspath_list, ', ' )
-        (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list)
+        (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list, default_nice_value,myhostname)
     elif distcc_list_of_hosts is not None:
         print '*** Parallel compilation using distcc list of hosts (%d)' % len(distcc_list_of_hosts)
         list_of_hosts = distcc_list_of_hosts



From nouiz at mail.berlios.de  Wed Oct  3 20:04:13 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 20:04:13 +0200
Subject: [Plearn-commits] r8139 - in trunk: commands plearn_learners/generic
Message-ID: <200710031804.l93I4DYS001120@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 20:04:13 +0200 (Wed, 03 Oct 2007)
New Revision: 8139

Added:
   trunk/plearn_learners/generic/TransformOutputLearner.cc
   trunk/plearn_learners/generic/TransformOutputLearner.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added an learner that do a transformation on the output. Ex transform from a regression to a classification


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-10-03 17:11:10 UTC (rev 8138)
+++ trunk/commands/plearn_noblas_inc.h	2007-10-03 18:04:13 UTC (rev 8139)
@@ -159,6 +159,7 @@
 #include <plearn_learners/generic/ChainedLearners.h>
 #include <plearn_learners/generic/StackedLearner.h>
 #include <plearn_learners/generic/TestingLearner.h>
+#include <plearn_learners/generic/TransformOutputLearner.h>
 #include <plearn_learners/generic/VPLPreprocessedLearner.h>
 #include <plearn_learners/generic/VPLPreprocessedLearner2.h>
 #include <plearn_learners/generic/VPLCombinedLearner.h>

Added: trunk/plearn_learners/generic/TransformOutputLearner.cc
===================================================================
--- trunk/plearn_learners/generic/TransformOutputLearner.cc	2007-10-03 17:11:10 UTC (rev 8138)
+++ trunk/plearn_learners/generic/TransformOutputLearner.cc	2007-10-03 18:04:13 UTC (rev 8139)
@@ -0,0 +1,208 @@
+// -*- C++ -*-
+
+// plearn_learners/generic/TransformOutputLearner.cc
+//
+// Copyright (C) 2007 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file plearn_learners/generic/TransformOutputLearner.cc */
+
+
+#include "plearn_learners/generic/TransformOutputLearner.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    TransformOutputLearner,
+    "Transform a Learner who give the log probality as output to give the probability as output",
+    "MULTI-LINE \nHELP");
+
+TransformOutputLearner::TransformOutputLearner()
+    :inherited(),
+     output_function(-1),
+     warning0(true),
+     warning1(true)
+{
+    forward_test=true;
+}
+
+void TransformOutputLearner::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, "myoption", &TransformOutputLearner::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+    declareOption(ol, "output_function", &TransformOutputLearner::output_function,
+                  OptionBase::buildoption,
+                  "The operation to do on the output\n"
+                  "0: We transform the sublearner log probability output to probability output\n"
+                  "1: We transform the sublearner probability output of class to class output\n"
+                  "2: We transform the sublearner regression output to class output.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void TransformOutputLearner::build_()
+{
+    tmp_output2.resize(learner_->outputsize());
+}
+
+// ### Nothing to add here, simply calls build_
+void TransformOutputLearner::build()
+{
+    inherited::build();
+    build_();
+}
+
+void TransformOutputLearner::computeOutput(const Vec& input, Vec& output) const
+{
+    // Compute the output from the input.
+    // int nout = outputsize();
+    // output.resize(nout);
+    // ...
+    if(output_function==0){//logprob to prob
+        learner_->computeOutput(input,output);
+        exp(output,output);
+    }else if(output_function==1){//logprob or prob to class
+        learner_->computeOutput(input,tmp_output2);
+        output[0]=argmax(tmp_output2);
+    }else if(output_function==2){//Regression to class v1
+        learner_->computeOutput(input,tmp_output2);
+        output[0]=int(round(tmp_output2[0]));
+//    }else if(output_function==2){//Regression to class v2
+//         learner_->computeOutput(input, output);
+//         if (multiclass_outputs.length() <= 0) return;
+//         real closest_value=multiclass_outputs[0];
+//         real margin_to_closest_value=abs(output[0] - multiclass_outputs[0]);
+//         for (int value_ind = 1; value_ind < multiclass_outputs.length(); value_ind++)
+//         {
+//             real v=abs(output[0] - multiclass_outputs[value_ind]);
+//             if (v < margin_to_closest_value)
+//             {
+//                 closest_value = multiclass_outputs[value_ind];
+//                 margin_to_closest_value = v;
+//             }
+//         }
+//         output[0] = closest_value;
+    }else
+        PLERROR("In TransformOutputLearner::computeOutput - unknow output_function %d",output_function);
+}
+
+void TransformOutputLearner::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    PLASSERT( learner_ );
+    if(output_function==0){
+        if(warning0){
+            PLWARNING("In TransformOutputLearner::computeCostsFromOutputs - you are loosing precision");
+            warning0=false;
+        }
+        compute_log(output,tmp_output2);
+        learner_->computeCostsFromOutputs(input,tmp_output2,target,costs);
+    }else if(output_function==1 || output_function==2){
+        if(warning1){
+            PLWARNING("In TransformOutputLearner::computeCostsFromOutputs - we can't compute the costs from\n"
+                      "outputs with output_function %d. We use TransformOutputLearner::computeOutputAndCosts.",output_function);
+            warning1=false;
+        }
+        computeOutputAndCosts(input,target,tmp_output2,costs);
+    }else
+        PLERROR("In TransformOutputLearner::computeCostsFromOutputs - unknow output_function %d",output_function);
+}
+void TransformOutputLearner::computeOutputAndCosts(const Vec& input,  const Vec& target,
+                                          Vec& output, Vec& costs) const
+{
+    PLASSERT( learner_ );
+    if(output_function==0){//logprob to prob
+        learner_->computeOutputAndCosts(input,target,output,costs);
+        exp(output,output);
+    }else if(output_function==1){//logprob or prob to class
+        learner_->computeOutputAndCosts(input,target,tmp_output2,costs);
+        output[0]=argmax(output);
+    }else if(output_function==2){//Regression to class v1
+        learner_->computeOutputAndCosts(input,target,tmp_output2,costs);
+        output[0]=int(round(tmp_output2[0]));
+    }else
+        PLERROR("In TransformOutputLearner::computeOutputAndCosts - unknow output_function %d",output_function);
+}
+
+void TransformOutputLearner::test(VMat testset, PP<VecStatsCollector> test_stats,
+                    VMat testoutputs, VMat testcosts) const
+{
+    PLASSERT( learner_ );
+    if(output_function==0){
+        learner_->test(testset, test_stats, testoutputs, testcosts);
+        for(int i = 0;i<testoutputs.length();i++){
+            Vec v = testoutputs(i);
+            exp(v,v);
+        }
+    }
+    else
+        PLERROR("In TransformOutputLearner::test");
+}
+int TransformOutputLearner::outputsize() const
+{
+    PLASSERT( learner_ );
+    if(output_function==0)
+        return learner_->outputsize();
+    else if (output_function==1 || output_function==2)
+        return 1;
+    else
+        PLERROR("In TransformOutputLearner::outputsize");
+    return -1;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/generic/TransformOutputLearner.h
===================================================================
--- trunk/plearn_learners/generic/TransformOutputLearner.h	2007-10-03 17:11:10 UTC (rev 8138)
+++ trunk/plearn_learners/generic/TransformOutputLearner.h	2007-10-03 18:04:13 UTC (rev 8139)
@@ -0,0 +1,151 @@
+// -*- C++ -*-
+
+// plearn_learners/generic/TransformOutputLearner.h
+//
+// Copyright (C) 2007 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file plearn_learners/generic/TransformOutputLearner.h */
+
+
+#ifndef TransformOutputLearner_INC
+#define TransformOutputLearner_INC
+
+#include <plearn_learners/generic/EmbeddedLearner.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class TransformOutputLearner : public EmbeddedLearner
+{
+    typedef EmbeddedLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    
+    //! The operation to do on the output
+    int output_function;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    TransformOutputLearner();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+                                       Vec& output, Vec& costs) const;
+
+    virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+                      VMat testoutputs, VMat testcosts) const;
+    virtual int outputsize() const;
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(TransformOutputLearner);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+//    static Vec tmp_output;
+    mutable Vec tmp_output2;
+    mutable bool warning0;
+    mutable bool warning1;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(TransformOutputLearner);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Wed Oct  3 21:58:25 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 21:58:25 +0200
Subject: [Plearn-commits] r8140 - in trunk/plearn_learners:
	online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results
	online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results
	regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results
Message-ID: <200710031958.l93JwPnt014032@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 21:58:24 +0200 (Wed, 03 Oct 2007)
New Revision: 8140

Modified:
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log
   trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/RUN.log
Log:
fixed the test from previous commit


Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log	2007-10-03 18:04:13 UTC (rev 8139)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/RUN.log	2007-10-03 19:58:24 UTC (rev 8140)
@@ -7,6 +7,7 @@
 [DeepBeliefNet] build_() called
 [DeepBeliefNet] build_layers_and_connections() called
 [DeepBeliefNet] build_classification_cost() called
+HyperLearner: starting the optimization
 [DeepBeliefNet] train() called 
 [DeepBeliefNet]   training_schedule = 20 10 
 [DeepBeliefNet]   cumulative_schedule = 0 20 30 

Modified: trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/RUN.log	2007-10-03 18:04:13 UTC (rev 8139)
+++ trunk/plearn_learners/online/test/ModuleLearner/.pytest/PL_ModuleLearner_Greedy/expected_results/RUN.log	2007-10-03 19:58:24 UTC (rev 8140)
@@ -1,4 +1,13 @@
  WARNING: RBMMatrixConnection: cannot forget() without random_gen
  WARNING: RBMMatrixConnection: cannot forget() without random_gen
  WARNING: GradNNetLayerModule: cannot forget() without random_gen
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization
  WARNING: In PLearner::initTrain (called by 'ModuleLearner') - The learner is already trained
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/RUN.log	2007-10-03 18:04:13 UTC (rev 8139)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/RUN.log	2007-10-03 19:58:24 UTC (rev 8140)
@@ -0,0 +1,2 @@
+HyperLearner: starting the optimization
+HyperLearner: starting the optimization



From manzagop at mail.berlios.de  Wed Oct  3 22:16:16 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 3 Oct 2007 22:16:16 +0200
Subject: [Plearn-commits] r8141 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710032016.l93KGG4m016755@sheep.berlios.de>

Author: manzagop
Date: 2007-10-03 22:16:16 +0200 (Wed, 03 Oct 2007)
New Revision: 8141

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
- Replaced the pv_gradstats (a VecStatsCollector) by some in-class statistics
to allow for more flexibility (such as discounting).
- Added 4 functions (XxxGrad()) to experiment with different weight update
strategies.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-03 19:58:24 UTC (rev 8140)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-03 20:16:16 UTC (rev 8141)
@@ -57,6 +57,7 @@
       pv_deceleration(0.5),
       pv_min_samples(2),
       pv_required_confidence(0.80),
+      pv_strategy(1),
       pv_random_sample_step(false)
 
 {
@@ -101,6 +102,11 @@
                   OptionBase::buildoption,
                   "Minimum required confidence (probability of being positive or negative) for taking a step.");
 
+    declareOption(ol, "pv_strategy",
+                  &PvGradNNet::pv_strategy,
+                  OptionBase::buildoption,
+                  "Strategy to use for the weight updates (number from 1 to 4).");
+
     declareOption(ol, "pv_random_sample_step",
                   &PvGradNNet::pv_random_sample_step,
                   OptionBase::buildoption,
@@ -115,25 +121,32 @@
 // TODO - reloading an object will not work! layer_params will juste get lost.
 void PvGradNNet::build_()
 {
-    pv_gradstats = new VecStatsCollector();
-
     int n = all_params.length();
+    pv_all_nsamples.resize(n);
+    pv_all_sum.resize(n);
+    pv_all_sumsquare.resize(n);
     pv_all_stepsigns.resize(n);
     pv_all_stepsizes.resize(n);
-    //pv_all_nsamples.resize(n);    // *stat*
 
     // Get some structure on the previous Vecs
+    pv_layer_nsamples.resize(n_layers-1);
+    pv_layer_sum.resize(n_layers-1);
+    pv_layer_sumsquare.resize(n_layers-1);
     pv_layer_stepsigns.resize(n_layers-1);
     pv_layer_stepsizes.resize(n_layers-1);
-    //pv_layer_nsamples.resize(n_layers-1); // *stat*
+    int np;
     for (int i=0,p=0;i<n_layers-1;i++)  {
-        int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        np=layer_sizes[i+1]*(1+layer_sizes[i]);
+        pv_layer_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        pv_layer_sum.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+        pv_layer_sumsquare.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-        //pv_layer_nsamples[i]=pv_all_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1); // *stat*
         p+=np;
     }
 
+//    pv_gradstats = new VecStatsCollector();
+
 }
 
 // ### Nothing to add here, simply calls build_
@@ -148,14 +161,18 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(pv_gradstats, copies);
+    deepCopyField(pv_all_nsamples, copies); 
+    deepCopyField(pv_layer_nsamples, copies);
+    deepCopyField(pv_all_sum, copies); 
+    deepCopyField(pv_layer_sum, copies);
+    deepCopyField(pv_all_sumsquare, copies);
+    deepCopyField(pv_layer_sumsquare, copies);
     deepCopyField(pv_all_stepsigns, copies);
     deepCopyField(pv_layer_stepsigns, copies);
     deepCopyField(pv_all_stepsizes, copies);
     deepCopyField(pv_layer_stepsizes, copies);
-    //deepCopyField(pv_all_nsamples, copies); // *stat*
-    //deepCopyField(pv_layer_nsamples, copies); // *stat*
 
+//    deepCopyField(pv_gradstats, copies);
 }
 
 void PvGradNNet::forget()
@@ -165,28 +182,58 @@
     //! a fresh learner!)
     inherited::forget();
 
-    pv_gradstats->forget();
+    pv_all_nsamples.fill(0);
+    pv_all_sum.fill(0.0);
+    pv_all_sumsquare.fill(0.0);
     pv_all_stepsigns.fill(0);
     pv_all_stepsizes.fill(pv_initial_stepsize);
-    //pv_all_nsamples.fill(0);
+
+//    pv_gradstats->forget();
 }
 
 //! Performs the backprop update. Must be called after the fbpropNet.
 void PvGradNNet::bpropUpdateNet(int t)
 {
     bpropNet(t);
-    pv_gradstats->update(all_params_gradient);
 
+    switch( pv_strategy )   {
+        case 1 :
+            pvGrad();
+            break;
+        case 2 :
+            discountGrad();
+            break;
+        case 3 :
+            globalSyncGrad();
+            break;
+        case 4 :
+            neuronSyncGrad();
+            break;
+        default :
+            PLERROR("PvGradNNet::bpropUpdateNet() - No such pv_strategy.");
+    }
+}
+
+void PvGradNNet::pvGrad()   
+{
     int np = all_params.length();
-    int ns;
     real m, e, prob_pos, prob_neg;
 
     for(int k=0; k<np; k++) {
-        StatsCollector& st = pv_gradstats->getStats(k);
-        ns = (int)st.nnonmissing();
-        if(ns>pv_min_samples)   {
-            m = st.mean();
-            e = st.stderror();
+        // update stats
+        pv_all_nsamples[k]++;
+        pv_all_sum[k] += all_params_gradient[k];
+        pv_all_sumsquare[k] += all_params_gradient[k] * all_params_gradient[k];
+
+        if(pv_all_nsamples[k]>pv_min_samples)   {
+            m = pv_all_sum[k] / pv_all_nsamples[k];
+            // e is the standard error
+            //e = sqrt( (pv_all_sumsquare[k] - (pv_all_sum[k]*pv_all_sum[k])/pv_all_nsamples[k]) / (real)(pv_all_nsamples[k]*(pv_all_nsamples[k]-1)) );
+            // variance
+            e = real((pv_all_sumsquare[k] - square(pv_all_sum[k])/pv_all_nsamples[k])/(pv_all_nsamples[k]-1));
+            // standard error 
+            e = sqrt(e/pv_all_nsamples[k]);
+
             // test to see if numerical problems
             if( fabs(m) < 1e-15 || e < 1e-15 )  {
                 cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
@@ -216,7 +263,9 @@
                     }
                     all_params[k] -= pv_all_stepsizes[k];
                     pv_all_stepsigns[k] = 1;
-                    st.forget();
+                    pv_all_nsamples[k]=0;
+                    pv_all_sum[k]=0.0;
+                    pv_all_sumsquare[k]=0.0;
                 }
                 // gradient is negative
                 else if(prob_neg>=pv_required_confidence)   {
@@ -233,7 +282,9 @@
                     }
                     all_params[k] += pv_all_stepsizes[k];
                     pv_all_stepsigns[k] = -1;
-                    st.forget();
+                    pv_all_nsamples[k]=0;
+                    pv_all_sum[k]=0.0;
+                    pv_all_sumsquare[k]=0.0;
                 }
             }
             /*else  // random sample update direction (sign)
@@ -253,6 +304,18 @@
 
 }
 
+void PvGradNNet::discountGrad()
+{
+}
+
+void PvGradNNet::globalSyncGrad()
+{
+}
+
+void PvGradNNet::neuronSyncGrad()
+{
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-03 19:58:24 UTC (rev 8140)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-03 20:16:16 UTC (rev 8141)
@@ -69,6 +69,8 @@
     //! for taking a step.
     real pv_required_confidence;
 
+    int pv_strategy;
+
     //! If this is set to true, then we will randomly choose the step sign for
     // each parameter based on the estimated probability of it being positive
     // or negative.
@@ -113,7 +115,11 @@
     static void declareOptions(OptionList& ol);
 
     virtual void bpropUpdateNet(const int t);
-//    virtual void bpropNet(const int t);
+
+    void pvGrad(); 
+    void discountGrad();
+    void globalSyncGrad();
+    void neuronSyncGrad();
     
 private:
     //#####  Private Member Functions  ########################################
@@ -124,8 +130,15 @@
 private:
     //#####  Private Data Members  ############################################
 
-    //! accumulated statistics of gradients on each parameter.
-    PP<VecStatsCollector> pv_gradstats;
+    //! Holds the number of samples gathered for each weight
+    TVec<int> pv_all_nsamples;
+    TVec< TMat<int> > pv_layer_nsamples;
+    //! Sum of collected gradients 
+    Vec pv_all_sum;
+    TVec<Mat> pv_layer_sum;
+    //! Sum of squares of collected gradients 
+    Vec pv_all_sumsquare;
+    TVec<Mat> pv_layer_sumsquare;
 
     //! Temporary add-on. Allows an undetermined signed value (zero).
     TVec<int> pv_all_stepsigns;
@@ -135,11 +148,10 @@
     Vec pv_all_stepsizes;
     TVec<Mat> pv_layer_stepsizes;
 
-    //! Holds the number of samples gathered for each weight
-    //! This is purely for outputing stats.
-    //TVec<int> pv_all_nsamples;
-    //TVec< TMat<int> > pv_layer_nsamples;
+    //! accumulated statistics of gradients on each parameter.
+    //PP<VecStatsCollector> pv_gradstats;
 
+
 };
 
 // Declares a few other classes and functions related to this class



From nouiz at mail.berlios.de  Wed Oct  3 22:30:44 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 22:30:44 +0200
Subject: [Plearn-commits] r8142 - trunk/plearn_learners/hyper
Message-ID: <200710032030.l93KUiJ7018616@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 22:30:44 +0200 (Wed, 03 Oct 2007)
New Revision: 8142

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Allow the HyperOptimize to get the last trained learner. This way we can use the class to get statistics at different stages of the trainings.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2007-10-03 20:16:16 UTC (rev 8141)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2007-10-03 20:30:44 UTC (rev 8142)
@@ -230,7 +230,7 @@
             if(expdir=="")
                 PLERROR("Cannot save final model: no experiment directory has been set");
             if( getLearner().isNull() )
-                PLERROR("Cannot save final model: no best learner has been obtained");
+                PLERROR("Cannot save final model: no final learner available");
             PLearn::save(expdir+"final_learner.psave",*getLearner());
         }
 

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-03 20:16:16 UTC (rev 8141)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-03 20:30:44 UTC (rev 8142)
@@ -119,7 +119,9 @@
 {
     declareOption(
         ol, "which_cost", &HyperOptimize::which_cost, OptionBase::buildoption,
-        "An index or a name in the tester's statnames to be used as the objective cost to minimize");
+        "An index or a name in the tester's statnames to be used as the"
+        " objective cost to minimize. If the index <0, we will take the last"
+        " learner.");
 
     declareOption(
         ol, "min_n_trials", &HyperOptimize::min_n_trials, OptionBase::buildoption,
@@ -330,7 +332,9 @@
             results = runTest(trialnum);
 
         reportResult(trialnum,results);
-        real objective = results[which_cost_pos];
+        real objective = MISSING_VALUE;
+        if (which_cost_pos>=0)
+            objective = results[which_cost_pos];
 
         option_vals = oracle->generateNextTrial(option_vals,objective);
 
@@ -351,13 +355,14 @@
         PLWARNING("In HyperOptimize::optimize - No trials at all were completed;\n"
                   "perhaps the oracle settings are wrong?");
 
-    // revert to best_learner
-    hlearner->setLearner(best_learner);
+    // revert to best_learner if one found. Otherwise take the last.
+    if(best_learner!=NULL)
+        hlearner->setLearner(best_learner);
 
-    if (best_results.isEmpty())
+    if (best_results.isEmpty() && which_cost_pos>=0)
         // This could happen for instance if all results are NaN.
-        PLWARNING("In HyperOptimize::optimize - Could not find a best result, something "
-                  "must be wrong");
+        PLWARNING("In HyperOptimize::optimize - Could not find a best result,"
+                  " something must be wrong");
     else
         // report best result again, if not empty
         reportResult(-1,best_results);



From nouiz at mail.berlios.de  Wed Oct  3 22:57:20 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Oct 2007 22:57:20 +0200
Subject: [Plearn-commits] r8143 - trunk/plearn_learners/hyper
Message-ID: <200710032057.l93KvKYj021212@sheep.berlios.de>

Author: nouiz
Date: 2007-10-03 22:57:20 +0200 (Wed, 03 Oct 2007)
New Revision: 8143

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
bugfix of last commit and better implementation.


Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-03 20:30:44 UTC (rev 8142)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-03 20:57:20 UTC (rev 8143)
@@ -121,7 +121,7 @@
         ol, "which_cost", &HyperOptimize::which_cost, OptionBase::buildoption,
         "An index or a name in the tester's statnames to be used as the"
         " objective cost to minimize. If the index <0, we will take the last"
-        " learner.");
+        " learner as the best.");
 
     declareOption(
         ol, "min_n_trials", &HyperOptimize::min_n_trials, OptionBase::buildoption,
@@ -335,7 +335,12 @@
         real objective = MISSING_VALUE;
         if (which_cost_pos>=0)
             objective = results[which_cost_pos];
-
+        else
+        {//The best is always the last
+            best_objective = objective;
+            best_results = results;
+            best_learner = hlearner->getLearner();
+        }
         option_vals = oracle->generateNextTrial(option_vals,objective);
 
         if(!is_missing(objective) &&
@@ -355,11 +360,10 @@
         PLWARNING("In HyperOptimize::optimize - No trials at all were completed;\n"
                   "perhaps the oracle settings are wrong?");
 
-    // revert to best_learner if one found. Otherwise take the last.
-    if(best_learner!=NULL)
-        hlearner->setLearner(best_learner);
+    // revert to best_learner if one found.
+    hlearner->setLearner(best_learner);
 
-    if (best_results.isEmpty() && which_cost_pos>=0)
+    if (best_results.isEmpty())
         // This could happen for instance if all results are NaN.
         PLWARNING("In HyperOptimize::optimize - Could not find a best result,"
                   " something must be wrong");



From saintmlx at mail.berlios.de  Wed Oct  3 23:59:51 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 3 Oct 2007 23:59:51 +0200
Subject: [Plearn-commits] r8144 - trunk/python_modules/plearn/pyext
Message-ID: <200710032159.l93LxpmU027965@sheep.berlios.de>

Author: saintmlx
Date: 2007-10-03 23:59:50 +0200 (Wed, 03 Oct 2007)
New Revision: 8144

Modified:
   trunk/python_modules/plearn/pyext/__init__.py
Log:
- added explicit garbage collection at process exit



Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2007-10-03 20:57:20 UTC (rev 8143)
+++ trunk/python_modules/plearn/pyext/__init__.py	2007-10-03 21:59:50 UTC (rev 8144)
@@ -37,8 +37,16 @@
 import cgitb
 cgitb.enable(format='PLearn')
 
+import atexit
+def cleanupWrappedObjects():
+    import gc
+    gc.collect()
+    ramassePoubelles()
+atexit.register(cleanupWrappedObjects)
+
 print versionString()
 
+
 # Redefines function TMat to emulate pyplearn behaviour
 def TMat( *args ):
     """Returns a list of lists, each inner list being a row of the TMat"""



From nouiz at mail.berlios.de  Thu Oct  4 15:23:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Oct 2007 15:23:14 +0200
Subject: [Plearn-commits] r8145 - trunk/plearn/base
Message-ID: <200710041323.l94DNEoc023444@sheep.berlios.de>

Author: nouiz
Date: 2007-10-04 15:23:13 +0200 (Thu, 04 Oct 2007)
New Revision: 8145

Modified:
   trunk/plearn/base/lexical_cast.cc
   trunk/plearn/base/lexical_cast.h
Log:
We do not consider an empty string as a number.


Modified: trunk/plearn/base/lexical_cast.cc
===================================================================
--- trunk/plearn/base/lexical_cast.cc	2007-10-03 21:59:50 UTC (rev 8144)
+++ trunk/plearn/base/lexical_cast.cc	2007-10-04 13:23:13 UTC (rev 8145)
@@ -140,8 +140,9 @@
 #endif
 }
 
-// this function handle numbers with exponents (such as 10.2E09)
-// as well as Nans. String can have trailing whitespaces on both sides
+/////////////////
+// pl_isnumber //
+/////////////////
 bool pl_isnumber(const string& str, double* dbl)
 {
     double d;
@@ -150,6 +151,8 @@
     d = pl_strtod(s.c_str(),&l);
     if(s=="")d=MISSING_VALUE;
     if(dbl!=NULL)*dbl=d;
+    if(s=="")
+        return false;
     return ((unsigned char)(l-s.c_str())==s.length());
 }
 
@@ -160,6 +163,8 @@
     d = pl_strtof(s.c_str(),&l);
     if(s=="")d=MISSING_VALUE;
     if(dbl!=NULL)*dbl=d;
+    if(s=="")
+        return false;
     return ((unsigned char)(l-s.c_str())==s.length());
 }
 

Modified: trunk/plearn/base/lexical_cast.h
===================================================================
--- trunk/plearn/base/lexical_cast.h	2007-10-03 21:59:50 UTC (rev 8144)
+++ trunk/plearn/base/lexical_cast.h	2007-10-04 13:23:13 UTC (rev 8145)
@@ -60,8 +60,9 @@
 double pl_strtod(const char* nptr, char** endptr);
 float  pl_strtof(const char* nptr, char** endptr);
 
-// this function handle numbers with exponents (such as 10.2E09)
-// as well as Nans. String can have trailing whitespaces on both sides
+//! This function handle numbers with exponents (such as 10.2E09)
+//! as well as Nans. String can have trailing whitespaces on both sides
+//! return true for empty string!
 bool pl_isnumber(const string& s,double* dbl=NULL);
 bool pl_isnumber(const string& s,float* dbl);
 



From nouiz at mail.berlios.de  Thu Oct  4 16:19:58 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Oct 2007 16:19:58 +0200
Subject: [Plearn-commits] r8146 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200710041419.l94EJwLr027538@sheep.berlios.de>

Author: nouiz
Date: 2007-10-04 16:19:54 +0200 (Thu, 04 Oct 2007)
New Revision: 8146

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
Log:
Modified to calculate the neighrest neighbors in two way: ignore missing value, use impoted covpres value for missing. Also tell how many point was used


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-10-04 13:23:13 UTC (rev 8145)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-10-04 14:19:54 UTC (rev 8146)
@@ -133,16 +133,18 @@
   row 0: nb_present
   row 1: mean/mode imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/mean_median_mode_file.pmat
   row 2: median/mode imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/mean_median_mode_file.pmat
-  row 3: treeconditionalmean imputation from prep/data/targeted_ind_no_imp.vmat.metadata/TreeCondMean/dir/'field_names'/Split0/test1_outputs.pmat
-  row 4: covariance preservation imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/covariance_file.pmat
-  row 5 to 24: (row - 4) * i neighbors imputation from neighborhood/test_train_imputed_with_covariance_preservation.pmat.metadata/neighborhood_file.pmat
+  row 3: mode
+  row 4: treeconditionalmean imputation from prep/data/targeted_ind_no_imp.vmat.metadata/TreeCondMean/dir/'field_names'/Split0/test1_outputs.pmat
+  row 5: covariance preservation imputation from preprocessing/final_train_input_preprocessed.pmat.metadata/covariance_file.pmat
+  row 6 to 24: (row - 4) * i neighbors imputation from neighborhood/test_train_imputed_with_covariance_preservation.pmat.metadata/neighborhood_file.pmat
   lire le train_set
 */
+    int nb_neighbors=100;
     MODULE_LOG << "build_() called" << endl;
     if (train_set)
     {
-        build_ball_tree();
-        output_file_name = train_metadata + "/TestImputation/output.pmat";
+        build_ball_tree(nb_neighbors*3);
+        output_file_name = train_metadata + "/TestImputation2/output.pmat";
         for (int iteration = 1; iteration <= train_set->width(); iteration++)
         {
             cout << "In TestImputations, Iteration # " << iteration << endl;
@@ -150,14 +152,14 @@
             computeMeanMedianModeStats();
             computeTreeCondMeanStats();
             computeCovPresStats();
-            computeNeighborhoodStats();
+            computeNeighborhoodStats(nb_neighbors,nb_neighbors*3);
             train();
         }
         endtestimputation("In TestImputations::build_(): we are done here");
     }
 }
 
-void TestImputations::build_ball_tree()
+void TestImputations::build_ball_tree(int nb_neighbors)
 { 
    // initialize primary dataset
     cout << "initialize the train set" << endl;
@@ -203,7 +205,7 @@
     if (!reference_set_with_covpres) PLERROR("In TestImputations::build_ball_tree() no reference_set_with_covpres provided.");
     if (!reference_set_with_missing) PLERROR("In TestImputations::build_ball_tree() no reference_set_with_missing provided.");
     ball_tree = new ExhaustiveNearestNeighbors();
-    ball_tree->setOption("num_neighbors", "100");
+    ball_tree->setOption("num_neighbors", tostring(nb_neighbors));
     ball_tree->setOption("copy_input", "0");
     ball_tree->setOption("copy_target", "0");
     ball_tree->setOption("copy_weight", "0");
@@ -302,7 +304,7 @@
     cout << "initialize the header file" << endl;
     train_set->lockMetaDataDir();
     header_record.resize(train_width);
-    header_file_name = train_metadata + "/TestImputation/header.pmat";
+    header_file_name = train_metadata + "/TestImputation2/header.pmat";
     cout << "header_file_name: " << header_file_name << endl;
     if (!isfile(header_file_name)) createHeaderFile();
     else getHeaderRecord();
@@ -332,8 +334,8 @@
     
     // find the available samples with non-missing values for this variable
     train_stats = train_set->getStats(to_deal_with_next);
-    train_total = train_stats.n();
-    train_missing = train_stats.nmissing();
+    train_total = (int)train_stats.n();
+    train_missing = (int)train_stats.nmissing();
     train_present = train_total - train_missing;
     indices.resize((int) train_present);
     int ind_next = 0;
@@ -357,8 +359,8 @@
     // load the test samples for this variable
     if (indices.length() > max_number_of_samples) test_length = max_number_of_samples;
     else if (indices.length() < min_number_of_samples)
-        PLERROR("TestImputations::initialize() Their is less examples for the variable %s then the min_number_of semples(%d)",
-                to_deal_with_name.c_str(),min_number_of_samples);
+        PLERROR("TestImputations::initialize() Their is less examples(%d) for the variable %s then the min_number_of semples(%d)",
+                indices.length(),to_deal_with_name.c_str(),min_number_of_samples);
     else test_length = indices.length();
     test_width = train_width;
     test_samples_set = new MemoryVMatrix(test_length, test_width);
@@ -413,7 +415,6 @@
         PLERROR("In TestImputations::computeTreeCondMeanStats(): The '%s' file was not found in the tcf directory.",tcmf_file_name.c_str());
     tcmf_file = new FileVMatrix(tcmf_file_name);
     int tcmf_length = tcmf_file->length();
-    int tcmf_width = tcmf_file->width();
     if (tcmf_length < train_length) 
         PLERROR("In TestImputations::computeTreeCondMeanStats(): there are only %d records in the tree conditional output file. We need %d.",tcmf_length,train_length);
     tcmf_mean_err = 0.0;
@@ -486,12 +487,18 @@
     return cvpf_value;
 }
 
-void TestImputations::computeNeighborhoodStats()
+//nb_neighbors, the number of neighbors to calculate
+//max_miss_neigbors, the additional neighbors we found so that we have replacement neighbors for neighbors with missing value
+void TestImputations::computeNeighborhoodStats(int nb_neighbors,int max_miss_neigbors)
 {
     knnf_input.resize(train_width);
-    knnf_neighbors.resize(100);
-    knnf_mean_err.resize(100);
-    knnf_mean_err.clear();
+    knnf_neighbors.resize(nb_neighbors+max_miss_neigbors);
+    knnf_mean_cov_err.resize(nb_neighbors);
+    knnf_mean_miss_err.resize(nb_neighbors);
+    knnf_nmiss_value_count.resize(nb_neighbors);
+    knnf_mean_cov_err.clear();
+    knnf_mean_miss_err.clear();
+    knnf_nmiss_value_count.clear();
     ProgressBar* pb = new ProgressBar( "computing the neighborhood imputation errors for " + to_deal_with_name, test_length);
     for (int test_row = 0; test_row < test_length; test_row++)
     {
@@ -503,33 +510,37 @@
             else knnf_input[test_col] = train_input[test_col];
         }
         ball_tree->computeOutput(knnf_input, knnf_neighbors);
-        real knnf_sum_value = 0.0;
-        real knnf_sum_cov = 0.0;
-        real knnv_value_count = 0.0;
-        for (int knnf_row = 0; knnf_row < knnf_neighbors.size(); knnf_row++)
+        real knnf_sum_cov_value = 0.0;
+        real knnf_sum_miss_value = 0.0;
+        int  knnv_value_count = 0;
+        for (int knnf_row = 0; knnf_row < knnf_neighbors.size() && knnv_value_count<nb_neighbors; knnf_row++)
         {
             real knnf_value = ref_mis((int) knnf_neighbors[knnf_row], to_deal_with_next);
-            if (!is_missing(knnf_value))
+            if(!is_missing(knnf_value))
             {
-                knnf_sum_value += knnf_value;
-                knnv_value_count += 1.0;
+                knnf_sum_miss_value += knnf_value;
+                knnf_nmiss_value_count[knnv_value_count]+=1;
+                knnf_mean_miss_err[knnv_value_count] += pow(to_deal_with_value - (knnf_sum_miss_value / (knnv_value_count+1)), 2);
+                knnv_value_count += 1;
             }
-            if (knnv_value_count > 0.0)
+            if (!is_missing(knnf_value) && knnf_row<nb_neighbors)
             {
-                knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_value / knnv_value_count), 2);
-                continue;
+                knnf_sum_cov_value += knnf_value;
+                knnf_mean_cov_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov_value / (knnf_row+1)), 2);
+            }else if(knnf_row<nb_neighbors){
+                knnf_value = ref_cov((int) knnf_neighbors[knnf_row], to_deal_with_next);
+                if (is_missing(knnf_value))
+                    PLERROR("In TestImputations::computeNeighborhoodStats(): missing value found in the reference with covariance preserved at: %i , %i",
+                            (int) knnf_neighbors[knnf_row], to_deal_with_next);
+                knnf_sum_cov_value += knnf_value;
+                knnf_mean_cov_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov_value / (knnf_row+1)), 2);
             }
-            knnf_value = ref_cov((int) knnf_neighbors[knnf_row], to_deal_with_next);
-            if (is_missing(knnf_value))
-                PLERROR("In TestImputations::computeNeighborhoodStats(): missing value found in the reference with covariance preserved at: %i , %i",
-                         (int) knnf_neighbors[knnf_row], to_deal_with_next);
-            knnf_sum_cov += knnf_value;
-            knnf_mean_err[knnf_row] += pow(to_deal_with_value - (knnf_sum_cov / (knnf_row + 1)), 2);
         }
-        pb->update( test_row );
+        pb->update( test_row );  
     }
     delete pb;
-    knnf_mean_err/=(real) test_length;
+    knnf_mean_cov_err/=test_length;
+    knnf_mean_miss_err/=knnf_nmiss_value_count;
 }
 
 void TestImputations::createHeaderFile()
@@ -538,14 +549,14 @@
     for (int train_col = 0; train_col < train_width; train_col++)
     {
         train_stats = train_set->getStats(train_col);
-        train_total = train_stats.n();
-        train_missing = train_stats.nmissing();
+        train_total = (int)train_stats.n();
+        train_missing = (int)train_stats.nmissing();
         train_present = train_total - train_missing;
         if (train_missing <= 0.0) header_record[train_col] = 0.0;                       // no missing, noting to do.
         else if (train_present < min_number_of_samples){
             header_record[train_col] = -1.0; // should not happen
-            PLERROR("In TestImputations::createHeaderFile: train_present(%d) < min_number_of_samples (%d)",
-                    train_present,min_number_of_samples);
+            PLERROR("In TestImputations::createHeaderFile: train_present(%d) < min_number_of_samples (%d) for variable %d()",
+                    train_present,min_number_of_samples,train_col,train_set.fieldName(train_col).c_str());
         }
         else header_record[train_col] = 1.0;                                            // test imputations
     }
@@ -570,34 +581,46 @@
     // initialize the output file
     cout << "initialize the output file: " << output_file_name << endl;
     train_set->lockMetaDataDir();
-    output_record.resize(knnf_mean_err.size()+5);
+    output_record.resize(6+knnf_mean_cov_err.size()+knnf_mean_miss_err.size()+knnf_nmiss_value_count.size());
     if (!isfile(output_file_name)) createOutputFile();
     else getOutputRecord(to_deal_with_next);
-    output_record[0] = mmmf_mean_err;
-    output_record[1] = mmmf_median_err;
-    output_record[2] = mmmf_mode_err;
-    output_record[3] = tcmf_mean_err;
-    output_record[4] = cvpf_mean_err;
-    for (int knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
-    {
-       output_record[knnf_row + 5] = knnf_mean_err[knnf_row];
-    }
+    output_record.resize(6);
+    output_record[0] = test_length;
+    output_record[1] = mmmf_mean_err;
+    output_record[2] = mmmf_median_err;
+    output_record[3] = mmmf_mode_err;
+    output_record[4] = tcmf_mean_err;
+    output_record[5] = cvpf_mean_err;
+    output_record.append(knnf_mean_cov_err);
+    output_record.append(knnf_mean_miss_err);
+    output_record.append(knnf_nmiss_value_count);
     updateOutputRecord(to_deal_with_next);
     train_set->unlockMetaDataDir();
 }
 
 void TestImputations::createOutputFile()
 {
-    output_names.resize(knnf_mean_err.size()+5);
-    output_names[0] = "mean";
-    output_names[1] = "median";
-    output_names[2] = "mode";
-    output_names[3] = "tree_cond";
-    output_names[4] = "cov_pres";
-    for (int knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++)
+    output_names.resize(6,knnf_mean_cov_err.size()+knnf_mean_miss_err.size()
+                        + knnf_nmiss_value_count.size());
+    output_names[0] = "test_length";
+    output_names[1] = "mean";
+    output_names[2] = "median";
+    output_names[3] = "mode";
+    output_names[4] = "tree_cond";
+    output_names[5] = "cov_pres";
+    for (int knnf_row = 0; knnf_row < knnf_mean_cov_err.size(); knnf_row++)
     {
-       output_names[knnf_row + 5] = "KNN_" + tostring(knnf_row);
+        output_names.append("KNN_COV_" + tostring(knnf_row+1));
     }
+    for (int knnf_row = 0; knnf_row < knnf_mean_cov_err.size(); knnf_row++)
+    {
+        output_names.append("KNN_MISS_" + tostring(knnf_row+1));
+    }
+    for (int knnf_row = 0; knnf_row < knnf_nmiss_value_count.size(); knnf_row++)
+    {
+        output_names.append("KNN_NB_MISS_" + tostring(knnf_row+1));
+    }
+
     output_record.clear();
     output_file = new FileVMatrix(output_file_name, train_width, output_names);
     for (int train_col = 0; train_col < train_width; train_col++)

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-10-04 13:23:13 UTC (rev 8145)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.h	2007-10-04 14:19:54 UTC (rev 8146)
@@ -130,13 +130,13 @@
 
     //! This does the actual building.
     void build_();
-    void build_ball_tree();
+    void build_ball_tree(int nb_neighbors);
     void initialize();
     void computeMeanMedianModeStats();
     void computeTreeCondMeanStats();
     void computeCovPresStats();
     real covariancePreservationValue(int col);
-    void computeNeighborhoodStats();
+    void computeNeighborhoodStats(int nb_neighbors,int max_miss_neigbors);
     void createHeaderFile();
     void getHeaderRecord();
     void updateHeaderRecord(int var_col);
@@ -157,9 +157,9 @@
     TVec<string> train_names;
     PPath train_metadata;
     StatsCollector train_stats;
-    real train_total;
-    real train_missing;
-    real train_present;
+    int train_total;
+    int train_missing;
+    int train_present;
     PPath header_file_name;
     VMat header_file;
     Vec header_record;
@@ -182,7 +182,9 @@
     real cvpf_mean_err;
     Vec knnf_input;
     Vec knnf_neighbors;
-    Vec knnf_mean_err;
+    Vec knnf_mean_cov_err;
+    Vec knnf_mean_miss_err;
+    Vec knnf_nmiss_value_count;
     Vec weights;
     WeightedDistance* weighted_distance_kernel;
     PPath output_file_name;



From nouiz at mail.berlios.de  Thu Oct  4 18:21:07 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Oct 2007 18:21:07 +0200
Subject: [Plearn-commits] r8147 - trunk/plearn_learners/testers
Message-ID: <200710041621.l94GL7ej005339@sheep.berlios.de>

Author: nouiz
Date: 2007-10-04 18:21:06 +0200 (Thu, 04 Oct 2007)
New Revision: 8147

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Added some test that verify if all the requested stats have their associated split available


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-10-04 14:19:54 UTC (rev 8146)
+++ trunk/plearn_learners/testers/PTester.cc	2007-10-04 16:21:06 UTC (rev 8147)
@@ -392,6 +392,19 @@
         }
         statnames_processed = temp[d];
     }
+
+    //Check if all the statnames_processed have their splits present
+    int nb_testset=splitter->nSetsPerSplit()-1;
+    for(int i=0;i<statnames_processed.length();i++){
+        int id = statnames_processed[i].find('[');
+        char c=statnames_processed[i][id+5];
+        if(c=='n'){}
+        else if(c>(nb_testset+'0'))
+            PLWARNING("In PTester::build_() - the statnames %s ask for"
+                      " test set %c while their is only %d test set.",
+                      statnames_processed[i].c_str(),
+                      c,nb_testset);
+    }
 }
 
 // ### Nothing to add here, simply calls build_



From nouiz at mail.berlios.de  Thu Oct  4 18:32:58 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Oct 2007 18:32:58 +0200
Subject: [Plearn-commits] r8148 - trunk/plearn_learners/regressors
Message-ID: <200710041632.l94GWw43014369@sheep.berlios.de>

Author: nouiz
Date: 2007-10-04 18:32:57 +0200 (Thu, 04 Oct 2007)
New Revision: 8148

Modified:
   trunk/plearn_learners/regressors/BaseRegressorConfidence.cc
Log:
small optimisation and removed warning of uninitialized variable


Modified: trunk/plearn_learners/regressors/BaseRegressorConfidence.cc
===================================================================
--- trunk/plearn_learners/regressors/BaseRegressorConfidence.cc	2007-10-04 16:21:06 UTC (rev 8147)
+++ trunk/plearn_learners/regressors/BaseRegressorConfidence.cc	2007-10-04 16:32:57 UTC (rev 8148)
@@ -169,12 +169,12 @@
     real train_set_weight;
     train_set_inputv.resize(train_set->inputsize());
     train_set_targetv.resize(1);
-    real distance = -1.0;
-    int nearest_neighbor;
+    real distance = REAL_MAX;
+    int nearest_neighbor = -1;
     for (int row = 0; row < train_set->length(); row++)
     {
         train_set->getExample(row, train_set_inputv, train_set_targetv, train_set_weight);
-        if (distance < 0 || powdistance(inputv, train_set_inputv) < distance)
+        if (powdistance(inputv, train_set_inputv) < distance)
         {
             distance = powdistance(inputv, train_set_inputv);
             nearest_neighbor = row;



From tihocan at mail.berlios.de  Thu Oct  4 19:19:42 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 4 Oct 2007 19:19:42 +0200
Subject: [Plearn-commits] r8149 - trunk/plearn/base
Message-ID: <200710041719.l94HJgV3025635@sheep.berlios.de>

Author: tihocan
Date: 2007-10-04 19:19:42 +0200 (Thu, 04 Oct 2007)
New Revision: 8149

Modified:
   trunk/plearn/base/lexical_cast.cc
   trunk/plearn/base/lexical_cast.h
Log:
- Fixed typo in comment for 'pl_isnumber'
- Made code of implementation for 'pl_isnumber' slightly easier to read


Modified: trunk/plearn/base/lexical_cast.cc
===================================================================
--- trunk/plearn/base/lexical_cast.cc	2007-10-04 16:32:57 UTC (rev 8148)
+++ trunk/plearn/base/lexical_cast.cc	2007-10-04 17:19:42 UTC (rev 8149)
@@ -149,11 +149,14 @@
     string s=removeblanks(str);
     char* l;
     d = pl_strtod(s.c_str(),&l);
-    if(s=="")d=MISSING_VALUE;
-    if(dbl!=NULL)*dbl=d;
-    if(s=="")
+    if(s.empty())
+        d = MISSING_VALUE;
+    if(dbl)
+        *dbl=d;
+    if(s.empty())
         return false;
-    return ((unsigned char)(l-s.c_str())==s.length());
+    else
+        return ((unsigned char)(l-s.c_str())==s.length());
 }
 
 bool pl_isnumber(const string& str, float* dbl) {
@@ -161,11 +164,14 @@
     string s=removeblanks(str);
     char* l;
     d = pl_strtof(s.c_str(),&l);
-    if(s=="")d=MISSING_VALUE;
-    if(dbl!=NULL)*dbl=d;
-    if(s=="")
+    if(s.empty())
+        d = MISSING_VALUE;
+    if(dbl)
+        *dbl=d;
+    if(s.empty())
         return false;
-    return ((unsigned char)(l-s.c_str())==s.length());
+    else
+        return ((unsigned char)(l-s.c_str())==s.length());
 }
 
 // Return true if conversion to a long is possible

Modified: trunk/plearn/base/lexical_cast.h
===================================================================
--- trunk/plearn/base/lexical_cast.h	2007-10-04 16:32:57 UTC (rev 8148)
+++ trunk/plearn/base/lexical_cast.h	2007-10-04 17:19:42 UTC (rev 8149)
@@ -60,11 +60,11 @@
 double pl_strtod(const char* nptr, char** endptr);
 float  pl_strtof(const char* nptr, char** endptr);
 
-//! This function handle numbers with exponents (such as 10.2E09)
-//! as well as Nans. String can have trailing whitespaces on both sides
-//! return true for empty string!
-bool pl_isnumber(const string& s,double* dbl=NULL);
-bool pl_isnumber(const string& s,float* dbl);
+//! This function handles numbers with exponents (such as 10.2E09)
+//! as well as Nans. String can have trailing whitespaces on both sides.
+//! Returns false for an empty string.
+bool pl_isnumber(const string& s, double* dbl=NULL);
+bool pl_isnumber(const string& s, float* dbl);
 
 // Return true if the number can be converted to a long
 bool pl_islong(const string& s);



From tihocan at mail.berlios.de  Thu Oct  4 20:03:23 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 4 Oct 2007 20:03:23 +0200
Subject: [Plearn-commits] r8150 - trunk/plearn/ker
Message-ID: <200710041803.l94I3No0028885@sheep.berlios.de>

Author: tihocan
Date: 2007-10-04 20:03:22 +0200 (Thu, 04 Oct 2007)
New Revision: 8150

Modified:
   trunk/plearn/ker/GeodesicDistanceKernel.cc
   trunk/plearn/ker/GeodesicDistanceKernel.h
Log:
- Fixed typo: it is Dijkstra, not Djikstra
- Made it so Floyd is always the default algorithm (hopefully we are computing geodesic distances on sparse graphs)
- 'geodesic_file' is now a PPath instead of a string
- Removed extra constructor that could be just merged within an existing one


Modified: trunk/plearn/ker/GeodesicDistanceKernel.cc
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.cc	2007-10-04 17:19:42 UTC (rev 8149)
+++ trunk/plearn/ker/GeodesicDistanceKernel.cc	2007-10-04 18:03:22 UTC (rev 8150)
@@ -55,35 +55,24 @@
     : geodesic_file(""),
       knn(10),
       pow_distance(false),
-      shortest_algo("djikstra")
+      shortest_algo("floyd")
 {
     distance_kernel = new DistanceKernel(2);
 }
 
-GeodesicDistanceKernel::GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn,
-                                               string the_geodesic_file, bool the_pow_distance)
-    : geodesic_file(the_geodesic_file),
-      knn(the_knn),
-      pow_distance(the_pow_distance),
-      shortest_algo("djikstra")
+GeodesicDistanceKernel::GeodesicDistanceKernel(
+        Ker the_distance_kernel, int the_knn,
+        const PPath& the_geodesic_file, bool the_pow_distance,
+        const string& the_method):
+    geodesic_file(the_geodesic_file),
+    knn(the_knn),
+    pow_distance(the_pow_distance),
+    shortest_algo(the_method)
 {
     distance_kernel = the_distance_kernel;
     build();
 }
 
-GeodesicDistanceKernel::GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn,
-                                               string the_geodesic_file, bool the_pow_distance,
-					       string the_method)
-    : geodesic_file(the_geodesic_file),
-      knn(the_knn),
-      pow_distance(the_pow_distance),
-      shortest_algo(the_method)
-{
-    distance_kernel = the_distance_kernel;
-    build();
-}
-
-
 PLEARN_IMPLEMENT_OBJECT(GeodesicDistanceKernel,
                         "Computes the geodesic distance based on k nearest neighbors.",
                         ""
@@ -111,7 +100,7 @@
     declareOption(ol, "shortest_algo", &GeodesicDistanceKernel::shortest_algo, OptionBase::buildoption,
                   "The algorithm used to compute the geodesic distances:\n"
                   " - floyd     : Floyd's algorithm\n"
-                  " - djikstra  : Djikstra's algorithm");
+                  " - dijkstra  : Dijkstra's algorithm");
 
     // Learnt options.
 
@@ -264,7 +253,7 @@
     TMat<int> neighborhoods =
         Kernel::computeKNNeighbourMatrixFromDistanceMatrix(
             distances, knn, true, report_progress != 0);
-    // Compute geodesic distance by Floyd or Djikstra's algorithm.
+    // Compute geodesic distance by Floyd or Dijkstra's algorithm.
     Mat geodesic(n,n);
     real big_value = REAL_MAX / 3.0; // To make sure no overflow.
     PP<ProgressBar> pb;
@@ -298,7 +287,7 @@
             if (report_progress)
                 pb->update(k + 1);
         }
-    } else if (shortest_algo == "djikstra") {
+    } else if (shortest_algo == "dijkstra") {
         // First build a symmetric neighborhoods matrix
         // (j is a neighbor of i if it was already a neighbor, or if i was a
         // neighbor of j).
@@ -347,7 +336,7 @@
         PLERROR("In GeodesicDistanceKernel::setDataForKernelMatrix - Unknown value for 'shortest_algo'");
     }
     // Save the result in geo_distances.
-    if (geodesic_file == "") {
+    if (geodesic_file.isEmpty()) {
         geo_distances = VMat(geodesic);
     } else {
         // Use a FileVMatrix to save on disk.

Modified: trunk/plearn/ker/GeodesicDistanceKernel.h
===================================================================
--- trunk/plearn/ker/GeodesicDistanceKernel.h	2007-10-04 17:19:42 UTC (rev 8149)
+++ trunk/plearn/ker/GeodesicDistanceKernel.h	2007-10-04 18:03:22 UTC (rev 8150)
@@ -72,7 +72,7 @@
     // ************************
 
     Ker distance_kernel;
-    string geodesic_file;
+    PPath geodesic_file;
     int knn;
     bool pow_distance;
     string shortest_algo;
@@ -87,10 +87,9 @@
 
     //! Convenient constructor.
     GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
-                           string the_geodesic_file = "", bool the_pow_distance = false);
-    GeodesicDistanceKernel(Ker the_distance_kernel, int the_knn = 10,
-                           string the_geodesic_file = "", bool the_pow_distance = false,
-			   string the_method = "floyd");
+                           const PPath& the_geodesic_file = "",
+                           bool the_pow_distance = false,
+                           const string& the_method = "floyd");
 
     // ******************
     // * Kernel methods *



From manzagop at mail.berlios.de  Thu Oct  4 21:03:19 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 4 Oct 2007 21:03:19 +0200
Subject: [Plearn-commits] r8151 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710041903.l94J3J5a003150@sheep.berlios.de>

Author: manzagop
Date: 2007-10-04 21:03:19 +0200 (Thu, 04 Oct 2007)
New Revision: 8151

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
- Coded the DiscountGrad() strategy which implements a soft invalidation
of weight statistics. Added 2 discount options. Sample stats are now real 
instead of int.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-04 18:03:22 UTC (rev 8150)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-04 19:03:19 UTC (rev 8151)
@@ -58,8 +58,10 @@
       pv_min_samples(2),
       pv_required_confidence(0.80),
       pv_strategy(1),
-      pv_random_sample_step(false)
-
+      pv_random_sample_step(false),
+      pv_self_discount(0.5),
+      pv_other_discount(0.95),
+      n_updates(0)
 {
     random_gen = new PRandom();
 }
@@ -114,6 +116,18 @@
                   "for each parameter based on the estimated probability of it being\n"
                   "positive or negative.");
 
+    declareOption(ol, "pv_self_discount",
+                  &PvGradNNet::pv_self_discount,
+                  OptionBase::buildoption,
+                  "Discount used to perform soft invalidation of a weight's statistics\n"
+                  "after its update.");
+
+    declareOption(ol, "pv_other_discount",
+                  &PvGradNNet::pv_other_discount,
+                  OptionBase::buildoption,
+                  "Discount used to perform soft invalidation of other weights'\n" 
+                  "statistics after a weight update.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -188,6 +202,9 @@
     pv_all_stepsigns.fill(0);
     pv_all_stepsizes.fill(pv_initial_stepsize);
 
+    // used in the discountGrad() strategy
+    n_updates = 0; 
+    
 //    pv_gradstats->forget();
 }
 
@@ -304,8 +321,85 @@
 
 }
 
+//! This gradient strategy is much like the one from PvGrad, however:
+//!     - there is no hard statistic invalidation, it's soft with discounts.
+//!     - a weight update will also affect other weights' statistics.
+//! Note that discounting is applied after the update, never during.
+//! With pv_self_discount=0.0 and pv_other_discount=1.0, should be same as
+//! PvGrad().
 void PvGradNNet::discountGrad()
 {
+    int np = all_params.length();
+    real m, e, prob_pos, prob_neg;
+    int stepsign;
+
+    // 
+    real discount = pow(pv_other_discount,n_updates);
+    n_updates = 0;
+    if( discount < 0.001 )
+        PLWARNING("PvGradNNet::discountGrad() - discount < 0.001 - that seems small...");
+    real sd = pv_self_discount / pv_other_discount; // trick: apply this self discount
+                                                    // and then discount
+                                                    // everyone the same
+    for(int k=0; k<np; k++) {
+        // Perform soft invalidation
+        pv_all_nsamples[k] *= discount;
+        pv_all_sum[k] *= discount;
+        pv_all_sumsquare[k] *= discount;
+
+        // update stats
+        pv_all_nsamples[k]++;
+        pv_all_sum[k] += all_params_gradient[k];
+        pv_all_sumsquare[k] += all_params_gradient[k] * all_params_gradient[k];
+
+        if(pv_all_nsamples[k]>pv_min_samples)   {
+            m = pv_all_sum[k] / pv_all_nsamples[k];
+            e = real((pv_all_sumsquare[k] - square(pv_all_sum[k])/pv_all_nsamples[k])/(pv_all_nsamples[k]-1));
+            e = sqrt(e/pv_all_nsamples[k]);
+
+            // test to see if numerical problems
+            if( fabs(m) < 1e-15 || e < 1e-15 )  {
+                cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                continue;
+            }
+
+            // TODO - for current treatment, not necessary to compute actual
+            // prob. Comparing the ratio would be sufficient.
+            prob_pos = gauss_01_cum(m/e);
+            prob_neg = 1.-prob_pos;
+
+            if(prob_pos>=pv_required_confidence)
+                stepsign = 1;
+            else if(prob_neg>=pv_required_confidence)
+                stepsign = -1;
+            else
+                continue;
+
+            // consecutive steps of same sign, accelerate
+            if( stepsign*pv_all_stepsigns[k]>0 )  {
+                pv_all_stepsizes[k]*=pv_acceleration;
+                if( pv_all_stepsizes[k] > pv_max_stepsize )
+                    pv_all_stepsizes[k] = pv_max_stepsize;            
+            // else if different signs decelerate
+            }   else if( stepsign*pv_all_stepsigns[k]<0 )   {
+                pv_all_stepsizes[k]*=pv_deceleration;
+                if( pv_all_stepsizes[k] < pv_min_stepsize )
+                    pv_all_stepsizes[k] = pv_min_stepsize;
+            // else (previous sign was undetermined
+            }//   else    {
+            //}
+            // step
+            if( stepsign > 0 )
+                all_params[k] -= pv_all_stepsizes[k];
+            else
+                all_params[k] += pv_all_stepsizes[k];
+            pv_all_stepsigns[k] = stepsign;
+            // soft invalidation of self
+            pv_all_nsamples[k]*=sd;
+            pv_all_sum[k]*=sd;
+            pv_all_sumsquare[k]*=sd;
+        }
+    }
 }
 
 void PvGradNNet::globalSyncGrad()

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-04 18:03:22 UTC (rev 8150)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-04 19:03:19 UTC (rev 8151)
@@ -76,6 +76,10 @@
     // or negative.
     bool pv_random_sample_step;
 
+    //! For the discounting strategy. Used to discount stats when there are
+    //! updates
+    real pv_self_discount, pv_other_discount;
+
 public:
     //#####  Public Not Build Options  ########################################
 
@@ -131,8 +135,8 @@
     //#####  Private Data Members  ############################################
 
     //! Holds the number of samples gathered for each weight
-    TVec<int> pv_all_nsamples;
-    TVec< TMat<int> > pv_layer_nsamples;
+    Vec pv_all_nsamples;
+    TVec< Mat > pv_layer_nsamples;
     //! Sum of collected gradients 
     Vec pv_all_sum;
     TVec<Mat> pv_layer_sum;
@@ -148,6 +152,10 @@
     Vec pv_all_stepsizes;
     TVec<Mat> pv_layer_stepsizes;
 
+    //! Number of weight updates performed during a call to bpropUpdateNet
+    // int is enough?
+    int n_updates;
+
     //! accumulated statistics of gradients on each parameter.
     //PP<VecStatsCollector> pv_gradstats;
 



From nouiz at mail.berlios.de  Thu Oct  4 21:05:07 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Oct 2007 21:05:07 +0200
Subject: [Plearn-commits] r8152 - trunk/plearn_learners/testers
Message-ID: <200710041905.l94J57Xx003330@sheep.berlios.de>

Author: nouiz
Date: 2007-10-04 21:05:05 +0200 (Thu, 04 Oct 2007)
New Revision: 8152

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
fix for pytest


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-10-04 19:03:19 UTC (rev 8151)
+++ trunk/plearn_learners/testers/PTester.cc	2007-10-04 19:05:05 UTC (rev 8152)
@@ -394,16 +394,18 @@
     }
 
     //Check if all the statnames_processed have their splits present
-    int nb_testset=splitter->nSetsPerSplit()-1;
-    for(int i=0;i<statnames_processed.length();i++){
-        int id = statnames_processed[i].find('[');
-        char c=statnames_processed[i][id+5];
-        if(c=='n'){}
-        else if(c>(nb_testset+'0'))
-            PLWARNING("In PTester::build_() - the statnames %s ask for"
-                      " test set %c while their is only %d test set.",
-                      statnames_processed[i].c_str(),
-                      c,nb_testset);
+    if(splitter!=NULL){
+        int nb_testset=splitter->nSetsPerSplit()-1;
+        for(int i=0;i<statnames_processed.length();i++){
+            int id = statnames_processed[i].find('[');
+            char c=statnames_processed[i][id+5];
+            if(c=='n'){}
+            else if(c>(nb_testset+'0'))
+                PLWARNING("In PTester::build_() - the statnames %s ask for"
+                          " test set %c while their is only %d test set.",
+                          statnames_processed[i].c_str(),
+                          c,nb_testset);
+        }
     }
 }
 



From manzagop at mail.berlios.de  Thu Oct  4 21:39:11 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 4 Oct 2007 21:39:11 +0200
Subject: [Plearn-commits] r8153 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710041939.l94JdBIS005377@sheep.berlios.de>

Author: manzagop
Date: 2007-10-04 21:39:10 +0200 (Thu, 04 Oct 2007)
New Revision: 8153

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
Log:
-In PvGrad() and DiscountGrad(), computing the probability of the gradient
being positive or negative was not required (can simply check the ratio
of mean and stderr) but VERY expensive. Fixed this, yielding a x12 speedup
for a network of topology 16-100-26.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-04 19:05:05 UTC (rev 8152)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-04 19:39:10 UTC (rev 8153)
@@ -234,7 +234,9 @@
 void PvGradNNet::pvGrad()   
 {
     int np = all_params.length();
-    real m, e, prob_pos, prob_neg;
+    real m, e;//, prob_pos, prob_neg;
+    real ratio;
+    real limit_ratio = gauss_01_quantile(pv_required_confidence);
 
     for(int k=0; k<np; k++) {
         // update stats
@@ -259,14 +261,16 @@
 
             // TODO - for current treatment, not necessary to compute actual prob.
             // Comparing the ratio would be sufficient.
-            prob_pos = gauss_01_cum(m/e);
-            prob_neg = 1.-prob_pos;
+            //prob_pos = gauss_01_cum(m/e);
+            //prob_neg = 1.-prob_pos;
+            ratio = m/e;
 
             if(!pv_random_sample_step)  {
     
                 // We adapt the stepsize before taking the step
                 // gradient is positive
-                if(prob_pos>=pv_required_confidence)    {
+                //if(prob_pos>=pv_required_confidence)    {
+                if(ratio>=limit_ratio)  {
                     //pv_all_stepsizes[k] *= (pv_all_stepsigns[k]?pv_acceleration:pv_deceleration);
                     if(pv_all_stepsigns[k]>0)   {
                         pv_all_stepsizes[k]*=pv_acceleration;
@@ -285,7 +289,8 @@
                     pv_all_sumsquare[k]=0.0;
                 }
                 // gradient is negative
-                else if(prob_neg>=pv_required_confidence)   {
+                //else if(prob_neg>=pv_required_confidence)   {
+                if(ratio<=-limit_ratio) {
                     //pv_all_stepsizes[k] *= ((!pv_all_stepsigns[k])?pv_acceleration:pv_deceleration);
                     if(pv_all_stepsigns[k]<0)   {
                         pv_all_stepsizes[k]*=pv_acceleration;
@@ -330,9 +335,12 @@
 void PvGradNNet::discountGrad()
 {
     int np = all_params.length();
-    real m, e, prob_pos, prob_neg;
+    real m, e;//, prob_pos, prob_neg;
     int stepsign;
 
+    real ratio;
+    real limit_ratio = gauss_01_quantile(pv_required_confidence);
+
     // 
     real discount = pow(pv_other_discount,n_updates);
     n_updates = 0;
@@ -365,7 +373,7 @@
 
             // TODO - for current treatment, not necessary to compute actual
             // prob. Comparing the ratio would be sufficient.
-            prob_pos = gauss_01_cum(m/e);
+/*            prob_pos = gauss_01_cum(m/e);
             prob_neg = 1.-prob_pos;
 
             if(prob_pos>=pv_required_confidence)
@@ -373,6 +381,13 @@
             else if(prob_neg>=pv_required_confidence)
                 stepsign = -1;
             else
+                continue;*/
+            ratio=m/e;
+            if(ratio>=limit_ratio)
+                stepsign = 1;
+            else if(ratio<=-limit_ratio)
+                stepsign = -1;
+            else
                 continue;
 
             // consecutive steps of same sign, accelerate



From nouiz at mail.berlios.de  Sat Oct  6 22:01:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sat, 6 Oct 2007 22:01:14 +0200
Subject: [Plearn-commits] r8154 - trunk/plearn/math
Message-ID: <200710062001.l96K1EVP009537@sheep.berlios.de>

Author: nouiz
Date: 2007-10-06 22:01:14 +0200 (Sat, 06 Oct 2007)
New Revision: 8154

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Added verification


Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-10-04 19:39:10 UTC (rev 8153)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-10-06 20:01:14 UTC (rev 8154)
@@ -251,7 +251,11 @@
     in.smartReadUntilNext("[", statname);
     string fieldname;
     in.smartReadUntilNext("]", fieldname);
-    int fieldnum = getFieldNum(fieldname);  
+    if(fieldname.size()==0)
+        PLERROR("In VecStatsCollector::getStat - the stat asked is invalid."
+                "Parsed stat name '%s' and field name '%d'",
+                statname.c_str(),fieldname.c_str());
+    int fieldnum = getFieldNum(fieldname);
     if(fieldnum<0)
         PLERROR("In VecStatsCollector::getStat invalid fieldname: %s;\n"
                 "valid fieldnames are: %s",fieldname.c_str(),



From nouiz at mail.berlios.de  Sat Oct  6 22:07:56 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sat, 6 Oct 2007 22:07:56 +0200
Subject: [Plearn-commits] r8155 - trunk/python_modules/plearn/learners
Message-ID: <200710062007.l96K7utU009889@sheep.berlios.de>

Author: nouiz
Date: 2007-10-06 22:07:56 +0200 (Sat, 06 Oct 2007)
New Revision: 8155

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
-weaklearner function now take the sub class as parameter
-added train stats
-added computeOutput_at_stage()
-forward weaklearner cost


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-10-06 20:01:14 UTC (rev 8154)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-10-06 20:07:56 UTC (rev 8155)
@@ -8,18 +8,21 @@
 #        Initialize a AdaBoost for 3 classes learner
 #        trainSet1 is used for the first sub AdaBoost learner,
 #        trainSet2 is used for the second sub learner
-#        weakLearner should be a function that return a new weak learner
+#        weakLearner should be a function that take the class number for the one vs other
+#                and should return a new weak learner
 #        """
         self.trainSet1=trainSet1
         self.trainSet2=trainSet2
+
+        if weakLearner:
+            self.learner1 = self.myAdaBoostLearner(weakLearner(0),trainSet1)
+            self.learner1.setExperimentDirectory(plargs.expdirr+"/learner1")
+            self.learner1.setTrainingSet(trainSet1,True)
             
-        self.learner1 = self.myAdaBoostLearner(weakLearner(),trainSet1)
-        self.learner1.setExperimentDirectory(plargs.expdirr+"/learner1")
-        self.learner1.setTrainingSet(trainSet1,True)
-        
-        self.learner2 = self.myAdaBoostLearner(weakLearner(),trainSet2)
-        self.learner2.setExperimentDirectory(plargs.expdirr+"/learner2")
-        self.learner2.setTrainingSet(trainSet2,True)
+            self.learner2 = self.myAdaBoostLearner(weakLearner(2),trainSet2)
+            self.learner2.setExperimentDirectory(plargs.expdirr+"/learner2")
+            self.learner2.setTrainingSet(trainSet2,True)
+
         self.nstages = 0
         self.stage = 0
         self.train_time = 0
@@ -31,13 +34,14 @@
         l.pseudo_loss_adaboost=plargs.pseudo_loss_adaboost
         l.weight_by_resampling=plargs.weight_by_resampling
         l.setTrainingSet(trainSet,True)
-        tmp=VecStatsCollector()
-        tmp.setFieldNames(l.getTrainCostNames())
-        l.setTrainStatsCollector(tmp)
         l.early_stopping=False
         l.compute_training_error=True
         l.forward_sub_learner_test_costs=True
         l.provide_learner_expdir=True
+#        l.save_often=True
+        tmp=VecStatsCollector()
+        tmp.setFieldNames(l.getTrainCostNames())
+        l.setTrainStatsCollector(tmp)
         return l
 
     def train(self):
@@ -49,6 +53,12 @@
         self.stage=self.learner1.stage
         t2=time.time()
         self.train_time+=t2-t1
+        self.train_stats=VecStatsCollector()
+        self.train_stats.append(self.learner1.getTrainStatsCollector(),
+                                "sublearner1.",[])
+        self.train_stats.append(self.learner2.getTrainStatsCollector(),
+                                "sublearner2.",[])
+
         
     def getTestCostNames(self):
         costnames = ["class_error","linear_class_error","square_class_error"]
@@ -65,13 +75,18 @@
             costnames.append("subweaklearner2."+c)
         return costnames
     
-    def computeOutput(self,example):
-        """ compute the output for the example in the parameter
+    def getTrainCostNames(self):
+        costnames = ["sublearner1."+x for x in self.learner1.getTrainCostNames()]
+        costnames += ["sublearner2."+x for x in self.learner2.getTrainCostNames()]                                
+        return costnames
+
+    def computeOutput_at_stage(self,input,stage):
+        """ compute the output for the input with the first stage weak learner
         
         return a tuple: (predicted result, output of sub learner1,output of sub learner2)
         """
-        out1=self.learner1.computeOutput(example)[0]
-        out2=self.learner2.computeOutput(example)[0]
+        out1=self.learner1.computeOutput_at_stage(input,stage)[0]
+        out2=self.learner2.computeOutput_at_stage(input,stage)[0]
         ind1=int(round(out1))
         ind2=int(round(out2))
         if ind1==ind2==0:
@@ -83,8 +98,28 @@
         else:
             ind=self.confusion_target
         return (ind,out1,out2)
+
+    def computeOutput(self,input):
+        """ compute the output for the input
+        
+        return a tuple: (predicted result, output of sub learner1,output of sub learner2)
+        """
+        out1=self.learner1.computeOutput(input)[0]
+        out2=self.learner2.computeOutput(input)[0]
+        ind1=int(round(out1))
+        ind2=int(round(out2))
+        if ind1==ind2==0:
+            ind=0
+        elif ind1==1 and ind2==0:
+            ind=1
+        elif ind1==ind2==1:
+            ind=2
+        else:
+            ind=self.confusion_target
+        return (ind,out1,out2)
     
-    def computeCostsFromOutput(self,input,output,target,costs=[]):
+    def computeCostsFromOutput(self,input,output,target_,costs=[],forward_sub_learner_costs=True):
+        target=int(target_)
         del costs[:]
         class_error=int(output[0] != target)
         linear_class_error=abs(output[0]-target)
@@ -118,10 +153,11 @@
             t2=array([0.])
         o1=array([output[1]])
         o2=[output[2]]
-        c1=self.learner1.computeCostsFromOutputs(input,o1,t1)
-        c2=self.learner2.computeCostsFromOutputs(input,o2,t2)
-        costs.extend(c1)
-        costs.extend(c2)
+        if forward_sub_learner_costs:
+            c1=self.learner1.computeCostsFromOutputs(input,o1,t1)
+            c2=self.learner2.computeCostsFromOutputs(input,o2,t2)
+            costs.extend(c1)
+            costs.extend(c2)
         return costs
 
     def computeOutputAndCosts(self,input,target):
@@ -129,19 +165,25 @@
         costs=self.computeCostsFromOutput(input,output,target)
         return (output,costs)
 
-    def test(self,testset,test_stats,return_outputs,return_costs):
-        print "In AdaBoostMultiClasses.py::test Not implemented"
-        sys.exit(1)
-        
+    def test(self,testSet,test_stats,return_outputs,return_costs):
+        testSet1=pl.ProcessingVMatrix(source=testSet,
+                               prg = "[%0:%"+str(testSet.inputsize-1)+"] @CLASSE_REEL 1 0 ifelse :CLASSE_REEL")
+        testSet2=pl.ProcessingVMatrix(source=testSet,
+                               prg = "[%0:%"+str(testSet.inputsize-1)+"] @CLASSE_REEL 2 - 0 1 ifelse :CLASSE_REEL")
+
         stats1=pl.VecStatsCollector()
         stats2=pl.VecStatsCollector()
-        (test_stats1, testoutputs1, testcosts1)=self.learner1.test(test_stats,stats1,True,return_costs)
-        (test_stats2, testoutputs2, testcosts2)=self.learner2.test(test_stats,stats2,True,return_costs)
+        (test_stats1, testoutputs1, testcosts1)=self.learner1.test(testSet1,
+                                                                   stats1,
+                                                                   True,True)
+        (test_stats2, testoutputs2, testcosts2)=self.learner2.test(testSet2,
+                                                                   stats2,
+                                                                   True,True)
         outputs=[]
         costs=[]
         #calculate stats, outputs, costs
-        test_mat=testset.getMat()
-        for i in range(testset.length()):
+        test_mat=testSet.getMat()
+        for i in range(len(test_mat)):
             out1=testoutputs1[i][0]
             out2=testoutputs2[i][0]
             ind1=int(round(out1))
@@ -154,9 +196,20 @@
                 ind=2
             else:
                 ind=self.confusion_target
-            outputs.append([ind,out1,out2])
-            self.computeCostsFromOutput(test_mat[i][:-2],ind,test_mat[i][-1])
-        
+            output=[ind,out1,out2]
+            if return_outputs:
+                outputs.append(output)
+            input=test_mat[i][:-1]
+            target=test_mat[i][-1]
+            cost=self.computeCostsFromOutput(input,output,target,
+                                             forward_sub_learner_costs=False)
+            cost.extend(testcosts1[i])
+            cost.extend(testcosts2[i])
+            test_stats.update(cost,1)
+            if return_costs:
+                costs.append(cost)
+        return(test_stats,outputs,costs)
+    
     def outputsize(self):
         return len(self.getTestCostNames())
 



From larocheh at mail.berlios.de  Tue Oct  9 15:13:09 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 9 Oct 2007 15:13:09 +0200
Subject: [Plearn-commits] r8156 - trunk/plearn_learners_experimental
Message-ID: <200710091313.l99DD9gq031347@sheep.berlios.de>

Author: larocheh
Date: 2007-10-09 15:13:07 +0200 (Tue, 09 Oct 2007)
New Revision: 8156

Added:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
Log:
Badly named deep network, which focuses on features that are related to the target...


Added: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-06 20:07:56 UTC (rev 8155)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-09 13:13:07 UTC (rev 8156)
@@ -0,0 +1,1155 @@
+// -*- C++ -*-
+
+// StackedFocusedAutoassociatorsNet.cc
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedFocusedAutoassociatorsNet.cc */
+
+
+#define PL_LOG_MODULE_NAME "StackedFocusedAutoassociatorsNet"
+#include <plearn/io/pl_log.h>
+
+#include "StackedFocusedAutoassociatorsNet.h"
+#include <plearn/vmat/VMat_computeNearestNeighbors.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMMixedConnection.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    StackedFocusedAutoassociatorsNet,
+    "Neural net, trained layer-wise in a greedy but focused fashion using autoassociators/RBMs and a supervised non-parametric gradient.",
+    "It is highly inspired by the StackedFocusedAutoassociators class,\n"
+    "and can use use the same RBMLayer and RBMConnection components.\n"
+    );
+
+StackedFocusedAutoassociatorsNet::StackedFocusedAutoassociatorsNet() :
+    cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
+    greedy_learning_rate( 0. ),
+    greedy_decrease_ct( 0. ),
+    supervised_greedy_learning_rate( 0. ),
+    supervised_greedy_decrease_ct( 0. ),
+    fine_tuning_learning_rate( 0. ),
+    fine_tuning_decrease_ct( 0. ),
+    k_neighbors( 1 ),
+    n_classes( -1 ),
+    n_layers( 0 ),
+    train_set_representations_up_to_date(false),
+    currently_trained_layer( 0 )
+{
+    // random_gen will be initialized in PLearner::build_()
+    random_gen = new PRandom();
+    nstages = 0;
+}
+
+void StackedFocusedAutoassociatorsNet::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "cd_learning_rate", 
+                  &StackedFocusedAutoassociatorsNet::cd_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the RBM "
+                  "contrastive divergence training");
+
+    declareOption(ol, "cd_decrease_ct", 
+                  &StackedFocusedAutoassociatorsNet::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "the RBMs contrastive\n"
+                  "divergence training. When a hidden layer has finished "
+                  "its training,\n"
+                  "the learning rate is reset to it's initial value.\n");
+
+    declareOption(ol, "greedy_learning_rate", 
+                  &StackedFocusedAutoassociatorsNet::greedy_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the autoassociator "
+                  "gradient descent training");
+
+    declareOption(ol, "greedy_decrease_ct", 
+                  &StackedFocusedAutoassociatorsNet::greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "the autoassociator\n"
+                  "gradient descent training. When a hidden layer has finished "
+                  "its training,\n"
+                  "the learning rate is reset to it's initial value.\n");
+
+    declareOption(ol, "supervised_greedy_learning_rate", 
+                  &StackedFocusedAutoassociatorsNet::supervised_greedy_learning_rate,
+                  OptionBase::buildoption,
+                  "Supervised, non-parametric, greedy learning rate");
+
+    declareOption(ol, "supervised_greedy_decrease_ct", 
+                  &StackedFocusedAutoassociatorsNet::supervised_greedy_decrease_ct,
+                  OptionBase::buildoption,
+                  "Supervised, non-parametric, greedy decrease constant");
+
+    declareOption(ol, "fine_tuning_learning_rate", 
+                  &StackedFocusedAutoassociatorsNet::fine_tuning_learning_rate,
+                  OptionBase::buildoption,
+                  "The learning rate used during the fine tuning gradient descent");
+
+    declareOption(ol, "fine_tuning_decrease_ct", 
+                  &StackedFocusedAutoassociatorsNet::fine_tuning_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during "
+                  "fine tuning\n"
+                  "gradient descent.\n");
+
+    declareOption(ol, "training_schedule", 
+                  &StackedFocusedAutoassociatorsNet::training_schedule,
+                  OptionBase::buildoption,
+                  "Number of examples to use during each phase of greedy pre-training.\n"
+                  "The number of fine-tunig steps is defined by nstages.\n"
+        );
+
+    declareOption(ol, "layers", &StackedFocusedAutoassociatorsNet::layers,
+                  OptionBase::buildoption,
+                  "The layers of units in the network. The first element\n"
+                  "of this vector should be the input layer and the\n"
+                  "subsequent elements should be the hidden layers. The\n"
+                  "output layer should not be included in layers.\n");
+
+    declareOption(ol, "connections", &StackedFocusedAutoassociatorsNet::connections,
+                  OptionBase::buildoption,
+                  "The weights of the connections between the layers");
+
+    declareOption(ol, "reconstruction_connections", 
+                  &StackedFocusedAutoassociatorsNet::reconstruction_connections,
+                  OptionBase::buildoption,
+                  "The reconstruction weights of the autoassociators");
+
+    declareOption(ol, "unsupervised_layers", 
+                  &StackedFocusedAutoassociatorsNet::unsupervised_layers,
+                  OptionBase::buildoption,
+                  "Additional units for greedy unsupervised learning");
+
+    declareOption(ol, "unsupervised_connections", 
+                  &StackedFocusedAutoassociatorsNet::unsupervised_connections,
+                  OptionBase::buildoption,
+                  "Additional connections for greedy unsupervised learning");
+
+    declareOption(ol, "k_neighbors", 
+                  &StackedFocusedAutoassociatorsNet::k_neighbors,
+                  OptionBase::buildoption,
+                  "Number of good nearest neighbors to attract and bad nearest "
+                  "neighbors to repel.");
+
+    declareOption(ol, "n_classes", 
+                  &StackedFocusedAutoassociatorsNet::n_classes,
+                  OptionBase::buildoption,
+                  "Number of classes.");
+
+    declareOption(ol, "greedy_stages", 
+                  &StackedFocusedAutoassociatorsNet::greedy_stages,
+                  OptionBase::learntoption,
+                  "Number of training samples seen in the different greedy "
+                  "phases.\n"
+        );
+
+    declareOption(ol, "n_layers", &StackedFocusedAutoassociatorsNet::n_layers,
+                  OptionBase::learntoption,
+                  "Number of layers"
+        );
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void StackedFocusedAutoassociatorsNet::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    MODULE_LOG << "build_() called" << endl;
+
+    if(inputsize_ > 0 && targetsize_ > 0)
+    {
+        // Initialize some learnt variables
+        n_layers = layers.length();
+        
+        train_set_representations_up_to_date = false;
+
+        if( n_classes <= 0 )
+            PLERROR("StackedFocusedAutoassociatorsNet::build_() - \n"
+                    "n_classes should be > 0.\n");
+        test_votes.resize(n_classes);
+
+        if( k_neighbors <= 0 )
+            PLERROR("StackedFocusedAutoassociatorsNet::build_() - \n"
+                    "k_neighbors should be > 0.\n");
+        test_nearest_neighbors_indices.resize(k_neighbors);
+
+        if( weightsize_ > 0 )
+            PLERROR("StackedFocusedAutoassociatorsNet::build_() - \n"
+                    "usage of weighted samples (weight size > 0) is not\n"
+                    "implemented yet.\n");
+
+        if( training_schedule.length() != n_layers-1 )        
+            PLERROR("StackedFocusedAutoassociatorsNet::build_() - \n"
+                    "training_schedule should have %d elements.\n",
+                    n_layers-1);
+        
+        if(greedy_stages.length() == 0)
+        {
+            greedy_stages.resize(n_layers-1);
+            greedy_stages.clear();
+        }
+
+        if(stage > 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer>1
+                  && greedy_stages[currently_trained_layer-1] <= 0)
+                currently_trained_layer--;
+        }
+
+        build_layers_and_connections();
+    }
+}
+
+void StackedFocusedAutoassociatorsNet::build_layers_and_connections()
+{
+    MODULE_LOG << "build_layers_and_connections() called" << endl;
+
+    if( connections.length() != n_layers-1 )
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be %d connections.\n",
+                n_layers-1);
+
+    if( reconstruction_connections.length() != n_layers-1 )
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be %d reconstruction connections.\n",
+                n_layers-1);
+
+    if(unsupervised_layers.length() != n_layers-2 
+       && unsupervised_layers.length() != 0)
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be either 0 of %d unsupervised_layers.\n",
+                n_layers-2);
+        
+    if(unsupervised_connections.length() != n_layers-2 
+       && unsupervised_connections.length() != 0)
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be either 0 of %d unsupervised_connections.\n",
+                n_layers-2);
+        
+    if(unsupervised_connections.length() != unsupervised_layers.length())
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be as many unsupervised_connections and "
+                "unsupervised_layers.\n");
+        
+
+    if(layers[0]->size != inputsize_)
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "layers[0] should have a size of %d.\n",
+                inputsize_);
+    
+
+    activations.resize( n_layers );
+    expectations.resize( n_layers );
+    activation_gradients.resize( n_layers );
+    expectation_gradients.resize( n_layers );
+
+    greedy_layers.resize(n_layers-1);
+    greedy_connections.resize(n_layers-1);
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        if( layers[i]->size != connections[i]->down_size )
+            PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a down_size of %d.\n",
+                    i, layers[i]->size);
+
+        if( connections[i]->up_size != layers[i+1]->size )
+            PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "connections[%i] should have a up_size of %d.\n",
+                    i, layers[i+1]->size);
+
+        if(unsupervised_layers.length() != 0 &&
+           unsupervised_connections.length() != 0 && 
+           unsupervised_layers[i] && unsupervised_connections[i])
+        {
+            if( layers[i]->size != 
+                unsupervised_connections[i]->down_size )
+                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                        "- \n"
+                        "connections[%i] should have a down_size of %d.\n",
+                        i, unsupervised_layers[i]->size);
+            
+            if( unsupervised_connections[i]->up_size != 
+                unsupervised_layers[i]->size )
+                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                        "- \n"
+                        "connections[%i] should have a up_size of %d.\n",
+                        i, unsupervised_layers[i+1]->size);
+            
+            if( layers[i+1]->size + unsupervised_layers[i]->size != 
+                reconstruction_connections[i]->down_size )
+                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                        "- \n"
+                        "recontruction_connections[%i] should have a down_size of "
+                        "%d.\n",
+                        i, layers[i+1]->size + unsupervised_layers[i]->size);
+            
+            if( reconstruction_connections[i]->up_size != 
+                layers[i]->size )
+                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                        "- \n"
+                        "recontruction_connections[%i] should have a up_size of "
+                        "%d.\n",
+                        i, layers[i]->size);
+
+            if( !(unsupervised_layers[i]->random_gen) )
+            {
+                unsupervised_layers[i]->random_gen = random_gen;
+                unsupervised_layers[i]->forget();
+            }
+            
+            if( !(unsupervised_connections[i]->random_gen) )
+            {
+                unsupervised_connections[i]->random_gen = random_gen;
+                unsupervised_connections[i]->forget();
+            }
+
+            PP<RBMMixedLayer> greedy_layer = new RBMMixedLayer();
+            greedy_layer->sub_layers.resize(2);
+            greedy_layer->sub_layers[0] = layers[i+1];
+            greedy_layer->sub_layers[1] = unsupervised_layers[i];
+            greedy_layer->build();
+
+            PP<RBMMixedConnection> greedy_connection = new RBMMixedConnection();
+            greedy_connection->sub_connections.resize(2,1);
+            greedy_connection->sub_connections(1,0) = connections[i];
+            greedy_connection->sub_connections(2,0) = unsupervised_connections[i];
+            greedy_connection->build();
+            
+            greedy_layers[i] = greedy_layer;
+            greedy_connections[i] = greedy_connection;
+        }
+        else
+        {
+            if( layers[i+1]->size != reconstruction_connections[i]->down_size )
+                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                        "- \n"
+                        "recontruction_connections[%i] should have a down_size of "
+                        "%d.\n",
+                        i, layers[i+1]->size);
+            
+            if( reconstruction_connections[i]->up_size != layers[i]->size )
+                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                        "- \n"
+                        "recontruction_connections[%i] should have a up_size of "
+                        "%d.\n",
+                        i, layers[i]->size);
+ 
+            greedy_layers[i] = layers[i+1];
+            greedy_connections[i] = connections[i];
+        }
+
+        if( !(layers[i]->random_gen) )
+        {
+            layers[i]->random_gen = random_gen;
+            layers[i]->forget();
+        }
+
+        if( !(connections[i]->random_gen) )
+        {
+            connections[i]->random_gen = random_gen;
+            connections[i]->forget();
+        }
+
+        if( !(reconstruction_connections[i]->random_gen) )
+        {
+            reconstruction_connections[i]->random_gen = random_gen;
+            reconstruction_connections[i]->forget();
+        }        
+
+        activations[i].resize( layers[i]->size );
+        expectations[i].resize( layers[i]->size );
+        activation_gradients[i].resize( layers[i]->size );
+        expectation_gradients[i].resize( layers[i]->size );
+    }
+
+    if( !(layers[n_layers-1]->random_gen) )
+    {
+        layers[n_layers-1]->random_gen = random_gen;
+        layers[n_layers-1]->forget();
+    }
+    activations[n_layers-1].resize( layers[n_layers-1]->size );
+    expectations[n_layers-1].resize( layers[n_layers-1]->size );
+    activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+    expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
+}
+
+// ### Nothing to add here, simply calls build_
+void StackedFocusedAutoassociatorsNet::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void StackedFocusedAutoassociatorsNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // deepCopyField(, copies);
+
+    // Public options
+    deepCopyField(training_schedule, copies);
+    deepCopyField(layers, copies);
+    deepCopyField(connections, copies);
+    deepCopyField(reconstruction_connections, copies);
+    deepCopyField(unsupervised_layers, copies);
+    deepCopyField(unsupervised_connections, copies);
+
+    // Protected options
+    deepCopyField(activations, copies);
+    deepCopyField(expectations, copies);
+    deepCopyField(activation_gradients, copies);
+    deepCopyField(expectation_gradients, copies);
+    deepCopyField(greedy_activation, copies);
+    deepCopyField(greedy_expectation, copies);
+    deepCopyField(greedy_activation_gradient, copies);
+    deepCopyField(greedy_expectation_gradient, copies);
+    deepCopyField(reconstruction_activations, copies);
+    deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
+    deepCopyField(greedy_layers, copies);
+    deepCopyField(greedy_connections, copies);
+    deepCopyField(similar_example_representation, copies);
+    deepCopyField(dissimilar_example_representation, copies);
+    deepCopyField(input_representation, copies);
+    deepCopyField(previous_input_representation, copies);
+    deepCopyField(dissimilar_gradient_contribution, copies);
+    deepCopyField(pos_down_val, copies);
+    deepCopyField(pos_up_val, copies);
+    deepCopyField(neg_down_val, copies);
+    deepCopyField(neg_up_val, copies);
+    deepCopyField(class_datasets, copies);
+    deepCopyField(other_classes_proportions, copies);
+    deepCopyField(nearest_neighbors_indices, copies);
+    deepCopyField(test_nearest_neighbors_indices, copies);
+    deepCopyField(test_votes, copies);
+    deepCopyField(train_set_representations, copies);
+    deepCopyField(train_set_representations_vmat, copies);
+    deepCopyField(train_set_targets, copies);
+    deepCopyField(greedy_stages, copies);
+}
+
+
+int StackedFocusedAutoassociatorsNet::outputsize() const
+{
+    //if(currently_trained_layer < n_layers)
+    //    return layers[currently_trained_layer]->size;
+    //return layers[n_layers-1]->size;
+    return n_classes;
+}
+
+void StackedFocusedAutoassociatorsNet::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+
+    train_set_representations_up_to_date = false;
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        connections[i]->forget();
+        reconstruction_connections[i]->forget();
+    }
+    
+    stage = 0;
+    greedy_stages.clear();
+}
+
+void StackedFocusedAutoassociatorsNet::train()
+{
+    MODULE_LOG << "train() called " << endl;
+    MODULE_LOG << "  training_schedule = " << training_schedule << endl;
+
+    Vec input( inputsize() );
+    Vec similar_example( inputsize() );
+    Vec dissimilar_example( inputsize() );
+    Vec target( targetsize() );
+    Vec target2( targetsize() );
+    real weight; // unused
+    real weight2; // unused
+
+    Vec similar_example_index(1);
+
+    TVec<string> train_cost_names = getTrainCostNames() ;
+    Vec train_costs( train_cost_names.length() );
+    train_costs.fill(MISSING_VALUE) ;
+
+    int nsamples = train_set->length();
+    int sample;
+
+    PP<ProgressBar> pb;
+
+    // clear stats of previous epoch
+    train_stats->forget();
+
+    int init_stage;
+
+    /***** initial greedy training *****/
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        MODULE_LOG << "Training connection weights between layers " << i
+            << " and " << i+1 << endl;
+
+        int end_stage = training_schedule[i];
+        int* this_stage = greedy_stages.subVec(i,1).data();
+        init_stage = *this_stage;
+
+        MODULE_LOG << "  stage = " << *this_stage << endl;
+        MODULE_LOG << "  end_stage = " << end_stage << endl;
+        MODULE_LOG << "  greedy_learning_rate = " << greedy_learning_rate << endl;
+
+        if( report_progress && *this_stage < end_stage )
+            pb = new ProgressBar( "Training layer "+tostring(i)
+                                  +" of "+classname(),
+                                  end_stage - init_stage );
+
+        train_costs.fill(MISSING_VALUE);
+        reconstruction_activations.resize(layers[i]->size);
+        reconstruction_activation_gradients.resize(layers[i]->size);
+        reconstruction_expectation_gradients.resize(layers[i]->size);
+
+        similar_example_representation.resize(layers[i+1]->size);
+        dissimilar_example_representation.resize(layers[i+1]->size);
+        dissimilar_gradient_contribution.resize(layers[i+1]->size);
+        input_representation.resize(layers[i+1]->size);
+
+        greedy_activation.resize(greedy_layers[i]->size);
+        greedy_expectation.resize(greedy_layers[i]->size);
+        greedy_activation_gradient.resize(greedy_layers[i]->size);
+        greedy_expectation_gradient.resize(greedy_layers[i]->size);
+
+        pos_down_val.resize(layers[i]->size);
+        pos_up_val.resize(greedy_layers[i]->size);
+        neg_down_val.resize(layers[i]->size);
+        neg_up_val.resize(greedy_layers[i]->size);
+
+        for( ; *this_stage<end_stage ; (*this_stage)++ )
+        {
+            
+            sample = *this_stage % nsamples;
+            train_set->getExample(sample, input, target, weight);
+            // Find similar example
+
+            int sim_index = random_gen->uniform_multinomial_sample(k_neighbors);
+            train_set->getExample(nearest_neighbors_indices(sample,sim_index),
+                                  similar_example, target2, weight2);
+
+            if(round(target[0]) != round(target2[0]))
+                PLERROR("StackedFocusedAutoassociatorsNet::train(): similar"
+                    " example is not from same class!");
+
+            // Find dissimilar example
+
+            int dissim_class_index = random_gen->multinomial_sample(
+                other_classes_proportions(round(target[0])));
+
+            int dissim_index = random_gen->uniform_multinomial_sample(
+                class_datasets[dissim_class_index]->length());
+
+            class_datasets[dissim_class_index]->getExample(dissim_index,
+                                  dissimilar_example, target2, weight2);
+
+            if(round(target[0]) == round(target2[0]))
+                PLERROR("StackedFocusedAutoassociatorsNet::train(): dissimilar"
+                    " example is from same class!");
+
+            greedyStep( input, target, i, train_costs, *this_stage,
+                        similar_example, dissimilar_example);
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( *this_stage - init_stage + 1 );
+        }
+    }
+
+    /***** fine-tuning by gradient descent *****/
+    if( stage < nstages )
+    {
+
+        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+        MODULE_LOG << "  stage = " << stage << endl;
+        MODULE_LOG << "  nstages = " << nstages << endl;
+        MODULE_LOG << "  fine_tuning_learning_rate = " << 
+            fine_tuning_learning_rate << endl;
+
+        init_stage = stage;
+        if( report_progress && stage < nstages )
+            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                  + classname(),
+                                  nstages - init_stage );
+
+        setLearningRate( fine_tuning_learning_rate );
+        train_costs.fill(MISSING_VALUE);
+
+        similar_example_representation.resize(
+            layers[n_layers-1]->size);
+        dissimilar_example_representation.resize(
+            layers[n_layers-1]->size);
+        dissimilar_gradient_contribution.resize(
+            layers[n_layers-1]->size);
+        similar_example.resize(inputsize());
+        dissimilar_example.resize(inputsize());
+
+        for( ; stage<nstages ; stage++ )
+        {
+            sample = stage % nsamples;
+            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                setLearningRate( fine_tuning_learning_rate
+                                 / (1. + fine_tuning_decrease_ct * stage ) );
+
+            train_set->getExample( sample, input, target, weight );
+
+            // Find similar example
+
+            int sim_index = random_gen->uniform_multinomial_sample(k_neighbors);
+            train_set->getExample(nearest_neighbors_indices(sample,sim_index),
+                                  similar_example, target2, weight2);
+
+            if(round(target[0]) != round(target2[0]))
+                PLERROR("StackedFocusedAutoassociatorsNet::train(): similar"
+                    " example is not from same class!");
+
+            // Find dissimilar example
+
+            int dissim_class_index = random_gen->multinomial_sample(
+                other_classes_proportions(round(target[0])));
+
+            int dissim_index = random_gen->uniform_multinomial_sample(
+                class_datasets[dissim_class_index]->length());
+
+            class_datasets[dissim_class_index]->getExample(dissim_index,
+                                  dissimilar_example, target2, weight2);
+
+            if(round(target[0]) == round(target2[0]))
+                PLERROR("StackedFocusedAutoassociatorsNet::train(): dissimilar"
+                    " example is from same class!");
+
+            fineTuningStep( input, target, train_costs, 
+                            similar_example, dissimilar_example);
+            train_stats->update( train_costs );
+
+            if( pb )
+                pb->update( stage - init_stage + 1 );
+        }
+    }
+    
+    train_stats->finalize();
+    MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
+
+    // Update currently_trained_layer
+    if(stage > 0)
+        currently_trained_layer = n_layers;
+    else
+    {            
+        currently_trained_layer = n_layers-1;
+        while(currently_trained_layer>1 
+              && greedy_stages[currently_trained_layer-1] <= 0)
+            currently_trained_layer--;
+    }
+}
+
+void StackedFocusedAutoassociatorsNet::greedyStep( 
+    const Vec& input, const Vec& target, int index, 
+    Vec train_costs, int this_stage, Vec similar_example, Vec dissimilar_example )
+{
+    PLASSERT( index < n_layers );
+    real lr;
+    train_set_representations_up_to_date = false;
+
+    // Get similar example representation
+    
+    computeRepresentation(similar_example, similar_example_representation, 
+                          index+1);
+
+    // Get dissimilar example representation
+
+    computeRepresentation(dissimilar_example, dissimilar_example_representation, 
+                          index+1);
+
+    // Get example representation
+
+    computeRepresentation(input, previous_input_representation, 
+                          index);
+    greedy_connections[index]->fprop(previous_input_representation,
+                                     greedy_activation);
+    greedy_layers[index]->fprop(greedy_activation,
+                                greedy_expectation);
+    input_representation << greedy_expectation.subVec(0,layers[index+1]->size);
+
+    // Autoassociator learning
+
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) )
+    {
+        if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                       * this_stage); 
+        else
+            lr = greedy_learning_rate;
+
+        layers[index]->setLearningRate( lr );
+        greedy_connections[index]->setLearningRate( lr );
+        reconstruction_connections[index]->setLearningRate( lr );
+        greedy_layers[index]->setLearningRate( lr );
+
+        reconstruction_connections[ index ]->fprop( greedy_expectation,
+                                                    reconstruction_activations);
+        layers[ index ]->fprop( reconstruction_activations,
+                                layers[ index ]->expectation);
+        
+        layers[ index ]->activation << reconstruction_activations;
+        layers[ index ]->expectation_is_up_to_date = true;
+        real rec_err = layers[ index ]->fpropNLL(previous_input_representation);
+        train_costs[index] = rec_err;
+        
+        layers[ index ]->bpropNLL(previous_input_representation, rec_err,
+                                  reconstruction_activation_gradients);
+    }
+
+    // Compute supervised gradient
+    
+    // Similar example contribution
+    substract(input_representation,similar_example_representation,
+              expectation_gradients[index+1]);
+    expectation_gradients[index+1] *= 4/layers[index+1]->size;
+    
+    // Dissimilar example contribution
+    real dist = sqrt(powdistance(input_representation,
+                                 dissimilar_example_representation,
+                                 2));
+    
+    substract(input_representation,dissimilar_example_representation,
+              dissimilar_gradient_contribution);
+
+    dissimilar_gradient_contribution *= -5.54*
+        safeexp(-2.77*dist/layers[index+1]->size)/dist;
+    
+    expectation_gradients[index+1] += dissimilar_gradient_contribution;
+
+    // RBM learning
+    if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
+    {
+        greedy_layers[index]->expectation << greedy_expectation;
+        greedy_layers[index]->expectation_is_up_to_date = true;
+        greedy_layers[index]->generateSample();
+        
+        // accumulate positive stats using the expectation
+        // we deep-copy because the value will change during negative phase
+        pos_down_val = expectations[index];
+        pos_up_val = greedy_layers[index]->expectation;
+        
+        // down propagation, starting from a sample of layers[index+1]
+        greedy_connections[index]->setAsUpInput( greedy_layers[index]->sample );
+        
+        layers[index]->getAllActivations( greedy_connections[index] );
+        layers[index]->computeExpectation();
+        layers[index]->generateSample();
+        
+        // negative phase
+        greedy_connections[index]->setAsDownInput( layers[index]->sample );
+        greedy_layers[index]->getAllActivations( greedy_connections[index] );
+        greedy_layers[index]->computeExpectation();
+        // accumulate negative stats
+        // no need to deep-copy because the values won't change before update
+        neg_down_val = layers[index]->sample;
+        neg_up_val = greedy_layers[index]->expectation;
+    }
+    
+    // Update hidden layer bias and weights
+
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) )
+    {
+        layers[ index ]->update(reconstruction_activation_gradients);
+    
+        reconstruction_connections[ index ]->bpropUpdate( 
+            greedy_expectation,
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+        greedy_layers[ index ]->bpropUpdate( 
+            greedy_activation,
+            greedy_expectation,
+            // reused
+            reconstruction_activation_gradients,
+            reconstruction_expectation_gradients);
+        
+        greedy_connections[ index ]->bpropUpdate( 
+            previous_input_representation,
+            greedy_activation,
+            reconstruction_expectation_gradients, //reused
+            reconstruction_activation_gradients);
+    }
+     
+
+    if( !fast_exact_is_equal( supervised_greedy_decrease_ct , 0 ) )
+        lr = supervised_greedy_learning_rate/(1 + supervised_greedy_decrease_ct 
+                               * this_stage); 
+    else
+        lr = supervised_greedy_learning_rate;
+    
+    layers[index]->setLearningRate( lr );
+    connections[index]->setLearningRate( lr );
+    layers[index+1]->setLearningRate( lr );
+    
+    layers[ index+1 ]->bpropUpdate( 
+        greedy_activation.subVec(0,layers[index+1]->size),
+        greedy_expectation.subVec(0,layers[index+1]->size),
+        activation_gradients[index+1], 
+        expectation_gradients[index+1]);
+    
+    connections[ index ]->bpropUpdate( 
+        previous_input_representation,
+        greedy_activation.subVec(0,layers[index+1]->size),
+        expectation_gradients[index],
+        activation_gradients[index+1]);
+
+    // RBM updates
+
+    if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
+    {
+        if( !fast_exact_is_equal( cd_decrease_ct , 0 ) )
+            lr = cd_learning_rate/(1 + cd_decrease_ct 
+                                       * this_stage); 
+        else
+            lr = cd_learning_rate;
+
+        layers[index]->setLearningRate( lr );
+        greedy_connections[index]->setLearningRate( lr );
+        greedy_layers[index]->setLearningRate( lr );
+
+        layers[index]->update( pos_down_val, neg_down_val );
+        greedy_connections[index]->update( pos_down_val, pos_up_val,
+                                    neg_down_val, neg_up_val );
+        greedy_layers[index]->update( pos_up_val, neg_up_val );
+    }
+}
+
+void StackedFocusedAutoassociatorsNet::fineTuningStep( 
+    const Vec& input, const Vec& target,
+    Vec& train_costs, Vec similar_example, Vec dissimilar_example )
+{
+    train_set_representations_up_to_date = false;
+
+    // Get similar example representation
+    
+    computeRepresentation(similar_example, similar_example_representation, 
+                          n_layers-1);
+
+    // Get dissimilar example representation
+
+    computeRepresentation(dissimilar_example, dissimilar_example_representation, 
+                          n_layers-1);
+
+    // Get example representation
+
+    computeRepresentation(input, previous_input_representation, 
+                          n_layers-1);
+
+    // Compute supervised gradient
+
+    // Similar example contribution
+    substract(input_representation,similar_example_representation,
+              expectation_gradients[n_layers-1]);
+    expectation_gradients[n_layers-1] *= 4/layers[n_layers-1]->size;
+    
+    // Dissimilar example contribution
+    real dist = sqrt(powdistance(input_representation,
+                                 dissimilar_example_representation,
+                                 2));
+    
+    substract(input_representation,dissimilar_example_representation,
+              dissimilar_gradient_contribution);
+
+    dissimilar_gradient_contribution *= -5.54*
+        safeexp(-2.77*dist/layers[n_layers-1]->size)/dist;
+    
+    expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+
+
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        layers[i]->bpropUpdate( activations[i],
+                                expectations[i],
+                                activation_gradients[i],
+                                expectation_gradients[i] );
+        
+        
+        connections[i-1]->bpropUpdate( expectations[i-1],
+                                       activations[i],
+                                       expectation_gradients[i-1],
+                                       activation_gradients[i] );
+    }        
+}
+
+void StackedFocusedAutoassociatorsNet::computeRepresentation(const Vec& input,
+                                                             Vec& representation,
+                                                             int layer) const
+{
+    if(layer == 0)
+    {
+        representation.resize(input.length());
+        representation << input;
+        return;
+    }
+
+    expectations[0] << input;
+    for( int i=0 ; i<layer; i++ )
+    {
+        connections[i]->fprop( expectations[i], activations[i+1] );
+        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+    }
+    representation.resize(expectations[layer].length());
+    representation << expectations[layer];
+}
+
+void StackedFocusedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
+{
+    updateTrainSetRepresentations();
+
+    computeRepresentation(input,input_representation, 
+                          max(currently_trained_layer,n_layers-1));
+
+    computeNearestNeighbors(train_set_representations_vmat,input_representation,
+                            test_nearest_neighbors_indices);
+
+    test_votes.clear();
+    for(int i=0; i<test_nearest_neighbors_indices.length(); i++)
+        test_votes[train_set_targets[test_nearest_neighbors_indices[i]]]++;
+
+    output[0] = argmax(test_votes);
+
+}
+
+void StackedFocusedAutoassociatorsNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+
+    //Assumes that computeOutput has been called
+
+    costs.resize( getTestCostNames().length() );
+    costs.fill( MISSING_VALUE );
+
+    if( currently_trained_layer<n_layers )
+    {
+        greedy_connections[currently_trained_layer-1]->fprop(
+            expectations[currently_trained_layer-1],
+            greedy_activation);
+        
+        greedy_layers[currently_trained_layer-1]->fprop(greedy_activation,
+                                    greedy_expectation);
+        
+        reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
+            greedy_expectation,
+            reconstruction_activations);
+        layers[ currently_trained_layer-1 ]->fprop( 
+            reconstruction_activations,
+            layers[ currently_trained_layer-1 ]->expectation);
+        
+        layers[ currently_trained_layer-1 ]->activation << 
+            reconstruction_activations;
+        layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        costs[ currently_trained_layer-1 ]  = 
+            layers[ currently_trained_layer-1 ]->fpropNLL(
+                expectations[currently_trained_layer-1]);
+    }
+
+    if( round(output[0]) == round(target[0]) )
+        costs[n_layers-1] = 0;
+    else
+        costs[n_layers-1] = 1;
+}
+
+//////////
+// test //
+//////////
+void StackedFocusedAutoassociatorsNet::updateTrainSetRepresentations() const
+{
+    if(!train_set_representations_up_to_date)
+    {
+        // Precompute training set examples' representation
+        int l = max(currently_trained_layer,n_layers-1);
+        Vec input( inputsize() );
+        Vec target( targetsize() );
+        Vec train_set_representation;
+        real weight;
+
+        train_set_representations.resize(train_set->length(), layers[l]->size);
+        train_set_targets.resize(train_set->length());
+        
+        for(int i=0; i<train_set->length(); i++)
+        {
+            train_set->getExample(i,input,target,weight);
+            computeRepresentation(input,train_set_representation,l);
+            train_set_representations(i) << train_set_representation;
+            train_set_targets[i] = round(target[0]);
+        }
+        train_set_representations_vmat = VMat(train_set_representations);
+
+        train_set_representations_up_to_date = true;
+    }
+}
+
+TVec<string> StackedFocusedAutoassociatorsNet::getTestCostNames() const
+{
+    // Return the names of the costs computed by computeCostsFromOutputs
+    // (these may or may not be exactly the same as what's returned by
+    // getTrainCostNames).
+
+    TVec<string> cost_names(0);
+
+    for( int i=0; i<layers.size()-1; i++)
+        cost_names.push_back("reconstruction_error_" + tostring(i+1));
+        
+    cost_names.append( "class_error" );
+
+    return cost_names;
+}
+
+TVec<string> StackedFocusedAutoassociatorsNet::getTrainCostNames() const
+{
+    return getTestCostNames() ;    
+}
+
+void StackedFocusedAutoassociatorsNet::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set,call_forget);
+    
+    train_set_representations_up_to_date = false;
+
+    Vec input( inputsize() );
+    Vec target( targetsize() );
+    real weight; // unused
+
+    // Separate classes
+    for(int k=0; k<n_classes; k++)
+    {
+        class_datasets[k] = new ClassSubsetVMatrix();
+        class_datasets[k]->classes.resize(1);
+        class_datasets[k]->classes[0] = k;
+        class_datasets[k]->source = training_set;
+        class_datasets[k]->build();
+    }
+
+    // Find other classes proportions
+    other_classes_proportions.fill(0);
+    for(int k=0; k<n_classes; k++)
+    {
+        real sum = 0;
+        for(int j=0; j<n_classes; j++)
+        {
+            if(j==k) continue;
+            other_classes_proportions(k,j) = class_datasets[j]->length();
+            sum += class_datasets[j]->length();
+        }
+        other_classes_proportions(k) /= sum;
+    }
+
+    // Find training nearest neighbors
+    input.resize(training_set->inputsize());
+    target.resize(training_set->targetsize());
+    nearest_neighbors_indices.resize(training_set->length(), k_neighbors);
+    TVec<int> nearest_neighbors_indices_row;
+    for(int k=0; k<n_classes; k++)
+    {
+        for(int i=0; i<class_datasets[k]->length(); i++)
+        {
+            class_datasets[k]->getExample(i,input,target,weight);
+            nearest_neighbors_indices_row = nearest_neighbors_indices(
+                class_datasets[k]->indices[i]);
+            computeNearestNeighbors((VMatrix *)class_datasets[k],input,
+                                    nearest_neighbors_indices_row,
+                                    i);
+        }
+    }
+}
+
+
+//#####  Helper functions  ##################################################
+
+void StackedFocusedAutoassociatorsNet::setLearningRate( real the_learning_rate )
+{
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( the_learning_rate );
+        connections[i]->setLearningRate( the_learning_rate );
+    }
+    layers[n_layers-1]->setLearningRate( the_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-06 20:07:56 UTC (rev 8155)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-09 13:13:07 UTC (rev 8156)
@@ -0,0 +1,351 @@
+// -*- C++ -*-
+
+// StackedFocusedAutoassociatorsNet.h
+//
+// Copyright (C) 2007 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedFocusedAutoassociatorsNet.h */
+
+
+#ifndef StackedFocusedAutoassociatorsNet_INC
+#define StackedFocusedAutoassociatorsNet_INC
+
+#include <plearn/vmat/ClassSubsetVMatrix.h>
+#include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/RBMClassificationModule.h>
+#include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMMixedLayer.h>
+#include <plearn_learners/online/RBMConnection.h>
+#include <plearn/misc/PTimer.h>
+
+namespace PLearn {
+
+/**
+ * Neural net, trained layer-wise in a greedy but focused fashion 
+ * using autoassociators/RBMs and a supervised non-parametric gradient.
+ * It is highly inspired by the StackedAutoassociators class, 
+ * and can use use the same RBMLayer and RBMConnection components.
+ */
+class StackedFocusedAutoassociatorsNet : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Contrastive divergence learning rate
+    real cd_learning_rate;
+    
+    //! Contrastive divergence decrease constant
+    real cd_decrease_ct;
+
+    //! The learning rate used during the autoassociator gradient descent training
+    real greedy_learning_rate;
+
+    //! The decrease constant of the learning rate used during the autoassociator
+    //! gradient descent training. When a hidden layer has finished its training,
+    //! the learning rate is reset to it's initial value.
+    real greedy_decrease_ct;
+
+    //! Supervised, non-parametric, greedy learning rate
+    real supervised_greedy_learning_rate;
+
+    //! Supervised, non-parametric, greedy decrease constant
+    real supervised_greedy_decrease_ct;
+
+    //! The learning rate used during the fine tuning gradient descent
+    real fine_tuning_learning_rate;
+
+    //! The decrease constant of the learning rate used during fine tuning
+    //! gradient descent
+    real fine_tuning_decrease_ct;
+
+    //! Number of examples to use during each phase of greedy pre-training.
+    //! The number of fine-tunig steps is defined by nstages.
+    TVec<int> training_schedule;
+
+    //! The layers of units in the network
+    TVec< PP<RBMLayer> > layers;
+
+    //! The weights of the connections between the layers
+    TVec< PP<RBMConnection> > connections;
+
+    //! The reconstruction weights of the autoassociators
+    TVec< PP<RBMConnection> > reconstruction_connections;
+
+    //! Additional units for greedy unsupervised learning
+    TVec< PP<RBMLayer> > unsupervised_layers;
+
+    //! Additional connections for greedy unsupervised learning
+    TVec< PP<RBMConnection> > unsupervised_connections;
+
+    //! Number of good nearest neighbors to attract and
+    //! bad nearest neighbors to repel.
+    int k_neighbors;
+
+    //! Number of classes
+    int n_classes;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Number of layers
+    int n_layers;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    StackedFocusedAutoassociatorsNet();
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    virtual void train();
+
+    //! Computes the output from the input.
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    /**
+     *  Precomputes the representations of the training set examples, 
+     *  to speed up nearest neighbors searches in that space.
+     */
+    virtual void updateTrainSetRepresentations() const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and  for which it updates the VecStatsCollector train_stats.
+    virtual TVec<std::string> getTrainCostNames() const;
+
+    /**
+     *  Declares the training set.  Then calls build() and forget() if
+     *  necessary.  Also sets this learner's inputsize_ targetsize_ weightsize_
+     *  from those of the training_set.  Note: You shouldn't have to override
+     *  this in subclasses, except in maybe to forward the call to an
+     *  underlying learner.
+     */
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+    void greedyStep( const Vec& input, const Vec& target, int index, 
+                     Vec train_costs, int stage, Vec similar_example,
+                     Vec dissimilar_example);
+
+    void fineTuningStep( const Vec& input, const Vec& target,
+                         Vec& train_costs, Vec similar_example, 
+                         Vec dissimilar_example );
+
+    void computeRepresentation( const Vec& input, 
+                                Vec& representation, int layer) const;
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(StackedFocusedAutoassociatorsNet);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+    //! Stores the activations of the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activations;
+
+    //! Stores the expectations of the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectations;
+
+    //! Stores the gradient of the cost wrt the activations of 
+    //! the input and hidden layers
+    //! (at the input of the layers)
+    mutable TVec<Vec> activation_gradients;
+
+    //! Stores the gradient of the cost wrt the expectations of 
+    //! the input and hidden layers
+    //! (at the output of the layers)
+    mutable TVec<Vec> expectation_gradients;
+
+    //! Stores the activation of the trained hidden layer during a greedy step
+    mutable Vec greedy_activation;
+
+    //! Stores the expectation of the trained hidden layer during a greedy step
+    mutable Vec greedy_expectation;
+
+    //! Stores the activation gradient of the trained 
+    //! hidden layer during a greedy step
+    mutable Vec greedy_activation_gradient;
+
+    //! Stores the expectation gradient of the trained 
+    //! hidden layer during a greedy step
+    mutable Vec greedy_expectation_gradient;
+
+    //! Reconstruction activations
+    mutable Vec reconstruction_activations;
+    
+    //! Reconstruction activation gradients
+    mutable Vec reconstruction_activation_gradients;
+
+    //! Reconstruction expectation gradients
+    mutable Vec reconstruction_expectation_gradients;
+
+    //! Layers used for greedy learning
+    TVec< PP<RBMLayer> > greedy_layers;
+
+    //! Connections used for greedy learning
+    TVec< PP<RBMConnection> > greedy_connections;
+
+    //! Similar example representation
+    Vec similar_example_representation;
+
+    //! Dissimilar example representation
+    Vec dissimilar_example_representation;
+
+    //! Example representation
+    mutable Vec input_representation;
+
+    //! Example representation at the previous layer, in a greedy step
+    Vec previous_input_representation;
+
+    //! Dissimilar gradient contribution
+    Vec dissimilar_gradient_contribution;
+
+    //! Positive down statistic
+    Vec pos_down_val;
+    //! Positive up statistic
+    Vec pos_up_val;
+    //! Negative down statistic
+    Vec neg_down_val;
+    //! Negative up statistic
+    Vec neg_up_val;
+
+    //! Datasets for each class
+    TVec< PP<ClassSubsetVMatrix> > class_datasets;
+
+    //! Proportions of examples from the other classes (columns), for each
+    //! class (rows)
+    Mat other_classes_proportions;
+
+    //! Nearest neighbors for each training example
+    TMat<int> nearest_neighbors_indices;
+
+    //! Nearest neighbors for each test example
+    mutable TVec<int> test_nearest_neighbors_indices;
+
+    //! Nearest neighbor votes for test example
+    TVec<int> test_votes;
+
+    //! Data set mapped to last hidden layer space
+    mutable Mat train_set_representations;
+    mutable VMat train_set_representations_vmat;
+    mutable TVec<int> train_set_targets;
+
+    //! Indication that train_set_representations is up to date
+    mutable bool train_set_representations_up_to_date;
+
+    //! Stages of the different greedy phases
+    TVec<int> greedy_stages;
+
+    //! Currently trained layer (1 means the first hidden layer,
+    //! n_layers means the output layer)
+    int currently_trained_layer;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void build_layers_and_connections();
+
+    void build_classification_cost();
+
+    void setLearningRate( real the_learning_rate );
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here    
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StackedFocusedAutoassociatorsNet);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Tue Oct  9 19:18:50 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Oct 2007 19:18:50 +0200
Subject: [Plearn-commits] r8157 - in trunk/plearn: base vmat
Message-ID: <200710091718.l99HIo0J003518@sheep.berlios.de>

Author: nouiz
Date: 2007-10-09 19:18:49 +0200 (Tue, 09 Oct 2007)
New Revision: 8157

Modified:
   trunk/plearn/base/stringutils.cc
   trunk/plearn/base/stringutils.h
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
-Added an split_quoted_delimiter() function where we allow the delimiter to be quoted. When they are quoted they do not trigger a split.
-Use this function in TextFilesVMatrix sot that we can escape caractere in csv file


Modified: trunk/plearn/base/stringutils.cc
===================================================================
--- trunk/plearn/base/stringutils.cc	2007-10-09 13:13:07 UTC (rev 8156)
+++ trunk/plearn/base/stringutils.cc	2007-10-09 17:18:49 UTC (rev 8157)
@@ -366,6 +366,41 @@
     return res;
 }
 
+// One version where we allow to quote a delimiter so that it is not considered as a delimiter.
+// TODO: optimize...
+// the double_quote are only considered if at the boundary of deliminer.
+// should execute in O(n+k) where n is the number of character in s and k is the number of field in k.
+vector<string> split_quoted_delimiter(const string& s, char delimiter, string double_quote){
+    if(double_quote.length()==1)
+        PLASSERT(delimiter!=double_quote[0]);
+    vector<string> ret = split(s, delimiter);
+    vector<string> ret2;
+    int delim_size=double_quote.size();
+    for(uint i=0; i<ret.size();i++){
+        bool bw=string_begins_with(ret[i],double_quote);
+        bool ew=string_ends_with(ret[i],double_quote);
+        if(bw && ew){
+            ret2.push_back(ret[i].substr(delim_size,
+                                         ret[i].size()-delim_size)); 
+        }else if(bw){
+            string tmp=ret[i].substr(delim_size);
+            tmp+=delimiter;
+            for(uint j=i+1;j<ret.size();j++){
+                if(string_ends_with(ret[j],double_quote)){
+                    tmp+=ret[j].substr(0,ret[j].size()-delim_size);
+                    ret2.push_back(tmp);
+                    i=j;
+                    break;
+                }
+                tmp+=ret[j];
+                tmp+=delimiter;
+            }
+        }else
+            ret2.push_back(ret[i]);
+    }
+    return ret2;
+    
+}
 vector<string> split(const string& s, const string& delimiters, bool keep_delimiters)
 {
     vector<string> result;

Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2007-10-09 13:13:07 UTC (rev 8156)
+++ trunk/plearn/base/stringutils.h	2007-10-09 17:18:49 UTC (rev 8157)
@@ -169,6 +169,8 @@
 */
 vector<string> split(const string& s, const string& delimiters=" \t\n\r", bool keepdelimiters=false);
 
+vector<string> split_quoted_delimiter(const string& s, char delimiter, string double_quote);
+
 /*!     Split the string on the first occurence of a delimiter and returns 
   what was left of the delimitor and what was right of it.
   If no delimitor character is found, the original string is returned 

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2007-10-09 13:13:07 UTC (rev 8156)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2007-10-09 17:18:49 UTC (rev 8157)
@@ -52,6 +52,7 @@
 TextFilesVMatrix::TextFilesVMatrix()
     : idxfile(0),
       delimiter("\t"),
+      quote_delimiter(""),
       auto_build_map(false),
       auto_extend_map(true),
       build_vmatrix_stringmap(false)
@@ -160,6 +161,8 @@
                     length_++;
                 }
             }
+            else
+                PLWARNING("In TextFilesVMatrix::buildIdx() - The line %d is blank",lineno);
         } // end of loop over lines of file
     } // end of loop over files
 
@@ -242,6 +245,10 @@
     {
         string fnam = txtfilenames[k];
         txtfiles[k] = fopen(fnam.c_str(),"r");
+        if(txtfiles[k]==NULL){
+            perror("Can't open file");
+            PLERROR("In TextFilesVMatrix::build_ - Can't open file %s",fnam.c_str());
+        }
     }
 
     // open the index file
@@ -434,7 +441,7 @@
 
 TVec<string> TextFilesVMatrix::splitIntoFields(const string& raw_row) const
 {
-    return split(raw_row, delimiter[0]);
+    return split_quoted_delimiter(raw_row, delimiter[0],quote_delimiter);
 }
 
 TVec<string> TextFilesVMatrix::getTextFields(int i) const
@@ -693,6 +700,10 @@
                   "- \",\"  : used for CSV files\n"
                   "- \";\"  : used for a variant of CSV files");
 
+    declareOption(ol, "quote_delimiter", &TextFilesVMatrix::quote_delimiter, OptionBase::buildoption,
+                  "The escate caractere where the delimiter is not considered."
+                  " //! - '\"' : used frequently.");
+
     declareOption(ol, "skipheader", &TextFilesVMatrix::skipheader, OptionBase::buildoption,
                   "An (optional) list of integers, one for each of the txtfilenames,\n"
                   "indicating the number of header lines at the top of the file to be skipped.");

Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2007-10-09 13:13:07 UTC (rev 8156)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2007-10-09 17:18:49 UTC (rev 8157)
@@ -103,6 +103,10 @@
     //! - ';'  : used for a variant of CSV files
     string delimiter;
 
+    //! The escate caractere where the delimiter is not considered.
+    //! - '"' : used frequently
+    string quote_delimiter;
+
     //! An (optional) list of integers, one for each of the txtfilenames
     //! indicating the number of header lines at the top of the file to be skipped.
     TVec<int> skipheader;



From tihocan at mail.berlios.de  Tue Oct  9 22:08:35 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 9 Oct 2007 22:08:35 +0200
Subject: [Plearn-commits] r8158 - trunk/plearn_learners/hyper
Message-ID: <200710092008.l99K8Zsd018464@sheep.berlios.de>

Author: tihocan
Date: 2007-10-09 22:08:35 +0200 (Tue, 09 Oct 2007)
New Revision: 8158

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Replaced cerr by perr, and fixed line over 80 characters

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-09 17:18:49 UTC (rev 8157)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2007-10-09 20:08:35 UTC (rev 8158)
@@ -305,7 +305,9 @@
     while(option_vals)
     {
         if(verbosity>0)
-            cerr<<"In HyperOptimize::optimize() We optimize with parameters "<<option_names<<" with value "<<option_vals<<endl;
+            perr << "In HyperOptimize::optimize() - We optimize with "
+                "parameters " << option_names << " with value " << option_vals
+                << endl;
 
         // This will also call build and forget on the learner unless unnecessary
         // because the modified options don't require it.



From nouiz at mail.berlios.de  Tue Oct  9 22:27:15 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Oct 2007 22:27:15 +0200
Subject: [Plearn-commits] r8159 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200710092027.l99KRFJA021527@sheep.berlios.de>

Author: nouiz
Date: 2007-10-09 22:27:14 +0200 (Tue, 09 Oct 2007)
New Revision: 8159

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h
Log:
code clarification: put global variable local when used only locally, better if hierarchie.


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc	2007-10-09 20:08:35 UTC (rev 8158)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.cc	2007-10-09 20:27:14 UTC (rev 8159)
@@ -163,21 +163,20 @@
 {
     // initialization with merge instructions
     sec_row = 0;
-    sec_col = 0;
     sec_length = external_dataset->length();
     sec_width = external_dataset->width();
     sec_names.resize(sec_width);
     sec_ins.resize(sec_width);
     sec_input.resize(sec_width);
-    ins_col = 0;
     extension_width = 0;
     sec_names << external_dataset->fieldNames();
-    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    for (int sec_col = 0; sec_col < sec_width; sec_col++)
     {
         sec_ins[sec_col] = "mean";
     }
-    for (ins_col = 0; ins_col < merge_instructions.size(); ins_col++)
+    for (int ins_col = 0; ins_col < merge_instructions.size(); ins_col++)
     {
+        int sec_col;
         for (sec_col = 0; sec_col < sec_width; sec_col++)
         {
             if (merge_instructions[ins_col].first == sec_names[sec_col]) break;
@@ -190,10 +189,9 @@
         else PLERROR("In MergeDond2Files: unsupported merge instruction: %", (merge_instructions[ins_col].second).c_str());
         if (sec_ins[sec_col] != "skip") extension_width += 1;
     }
-    ext_col = 0;
     extension_pos.resize(sec_width);
     extension_names.resize(extension_width);
-    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    for (int ext_col = 0, sec_col = 0; sec_col < sec_width; sec_col++)
     {
         if (sec_ins[sec_col] == "skip")
         {
@@ -216,7 +214,6 @@
     
     // initialize primary dataset
     main_row = 0;
-    main_col = 0;
     main_length = train_set->length();
     main_width = train_set->width();
     main_input.resize(main_width);
@@ -224,12 +221,13 @@
     main_ins.resize(main_width);
     main_names << train_set->fieldNames();
     primary_width = 0;
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         main_ins[main_col] = "as_is";
     }
-    for (ins_col = 0; ins_col < missing_instructions.size(); ins_col++)
+    for (int ins_col = 0; ins_col < missing_instructions.size(); ins_col++)
     {
+        int main_col = 0;
         for (main_col = 0; main_col < main_width; main_col++)
         {
             if (missing_instructions[ins_col].first == main_names[main_col]) break;
@@ -243,9 +241,8 @@
         else PLERROR("In MergeDond2Files: unsupported merge instruction: %", (missing_instructions[ins_col].second).c_str());
         if (main_ins[main_col] != "skip") primary_width += 1;
     }
-    prim_col = 0;
     primary_names.resize(primary_width);
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0, prim_col = 0; main_col < main_width; main_col++)
     {
         if (main_ins[main_col] != "skip")
         {
@@ -259,12 +256,12 @@
     merge_width = primary_width + extension_width;
     merge_output.resize(merge_width);
     merge_names.resize(merge_width);
-    for (prim_col = 0; prim_col < primary_width; prim_col++)
+    for (int prim_col = 0; prim_col < primary_width; prim_col++)
     {
        merge_names[merge_col] = primary_names[prim_col];
        merge_col +=1;
     }
-    for (ext_col = 0; ext_col < extension_width; ext_col++)
+    for (int ext_col = 0; ext_col < extension_width; ext_col++)
     {
        merge_names[merge_col] = extension_names[ext_col];
        merge_col +=1;
@@ -324,20 +321,20 @@
 
 void MergeDond2Files::accumulateVec()
 {
-    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    for (int sec_col = 0; sec_col < sec_width; sec_col++)
     {
         if (is_missing(sec_input[sec_col])) continue;
-        if (sec_ins[sec_col] == "skip") continue;
-        ext_col = extension_pos[sec_col];
+        else if (sec_ins[sec_col] == "skip") continue;
+        int ext_col = extension_pos[sec_col];
         if (sec_ins[sec_col] == "mean")
         {
             sec_values(ext_col, 0) += sec_input[sec_col];
             sec_value_cnt(ext_col, 0) += 1.0;
         }
-        if (sec_ins[sec_col] == "mode")
+        else if (sec_ins[sec_col] == "mode")
         {
             sec_value_found = false;
-            for (sec_value_col = 0; sec_value_col < sec_value_ind[sec_col]; sec_value_col++)
+            for (int sec_value_col = 0; sec_value_col < sec_value_ind[sec_col]; sec_value_col++)
             {
                 if (sec_values(ext_col, sec_value_col) == sec_input[sec_col])
                 {
@@ -357,7 +354,7 @@
                 sec_value_ind[sec_col] += 1;
             }
         }
-        if (sec_ins[sec_col] == "present")
+        else if (sec_ins[sec_col] == "present")
         {
             sec_value_cnt(ext_col, 0) = 1.0;
         }
@@ -367,59 +364,49 @@
 void MergeDond2Files::combineAndPut()
 {
     merge_col = 0;
-    for (main_col = 0; main_col < main_width; main_col++)
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         if (main_ins[main_col] == "skip") continue;
-        if (main_ins[main_col] == "as_is")
+        else if (main_ins[main_col] == "as_is")
         {
             merge_output[merge_col] = main_input[main_col];
-            merge_col +=1;
-            continue;
         }
-        if (main_ins[main_col] == "zero_is_missing")
+        else if (main_ins[main_col] == "zero_is_missing")
         {
             if (main_input[main_col] == 0.0) merge_output[merge_col] = MISSING_VALUE;
             else merge_output[merge_col] = main_input[main_col];
-            merge_col +=1;
-            continue;
         }
-        if (main_ins[main_col] == "2436935_is_missing")
+        else if (main_ins[main_col] == "2436935_is_missing")
         {
             if (main_input[main_col] == 2436935.0) merge_output[merge_col] = MISSING_VALUE;
             else merge_output[merge_col] = main_input[main_col];
-            merge_col +=1;
-            continue;
         }
-        if (main_ins[main_col] == "present")
+        else if (main_ins[main_col] == "present")
         {
             if (is_missing(main_input[main_col])) merge_output[merge_col] = 0.0;
             else merge_output[merge_col] = 1.0;
-            merge_col +=1;
-            continue;
         }
+        merge_col +=1;
     }
-    for (sec_col = 0; sec_col < sec_width; sec_col++)
+    for (int sec_col = 0; sec_col < sec_width; sec_col++)
     {
         if (sec_ins[sec_col] == "skip") continue;
-        ext_col = extension_pos[sec_col];
+        int ext_col = extension_pos[sec_col];
         if (sec_ins[sec_col] == "mean")
         {
             if (sec_value_cnt(ext_col, 0) <= 0.0)  merge_output[merge_col] = MISSING_VALUE;
             else merge_output[merge_col] = sec_values(ext_col, 0) / sec_value_cnt(ext_col, 0);
-            merge_col +=1;
-            continue;
         }
-        if (sec_ins[sec_col] == "mode")
+        else if (sec_ins[sec_col] == "mode")
         {
             if (sec_value_ind[sec_col] <= 0.0)
             {
                 merge_output[merge_col] = MISSING_VALUE;
-                merge_col +=1;
                 continue;
             }
             merge_output[merge_col] = sec_values(ext_col, 0);
             sec_value_count_max = sec_value_cnt(ext_col, 0);
-            for (sec_value_col = 1; sec_value_col < sec_value_ind[sec_col]; sec_value_col++)
+            for (int sec_value_col = 1; sec_value_col < sec_value_ind[sec_col]; sec_value_col++)
             {
                 if (sec_value_cnt(ext_col, sec_value_col) >= sec_value_count_max)
                 {
@@ -427,15 +414,12 @@
                     sec_value_count_max = sec_value_cnt(ext_col, sec_value_col);
                 }
             }
-            merge_col +=1;
-            continue;
         }
-        if (sec_ins[sec_col] == "present")
+        else if (sec_ins[sec_col] == "present")
         {
             merge_output[merge_col] = sec_value_cnt(ext_col, 0);
-            merge_col +=1;
-            continue;
         }
+        merge_col +=1;
     }
     if (main_input[train_ind] > 0.0)
     {

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h	2007-10-09 20:08:35 UTC (rev 8158)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MergeDond2Files.h	2007-10-09 20:27:14 UTC (rev 8159)
@@ -148,11 +148,9 @@
     int sec_length;
     int sec_width;
     int sec_row;
-    int sec_col;
     Vec sec_input;
     TVec<string> sec_names;
     TVec<string> sec_ins;
-    int ins_col;
     int extension_width;
     int ext_col;
     TVec<int> extension_pos;
@@ -162,18 +160,15 @@
     TVec<int> sec_value_ind;
     bool sec_value_found;
     real sec_value_count_max;
-    int sec_value_col;
     
     // primary dataset variables
     int main_length;
     int main_width;
     int main_row;
-    int main_col;
     Vec main_input;
     TVec<string> main_names;
     TVec<string> main_ins;
     int primary_width;
-    int prim_col;
     TVec<string> primary_names;
     
     // merge dataset variables



From lamblin at mail.berlios.de  Wed Oct 10 01:31:03 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 10 Oct 2007 01:31:03 +0200
Subject: [Plearn-commits] r8160 - trunk/scripts
Message-ID: <200710092331.l99NV3NX022003@sheep.berlios.de>

Author: lamblin
Date: 2007-10-10 01:31:02 +0200 (Wed, 10 Oct 2007)
New Revision: 8160

Modified:
   trunk/scripts/collectres
Log:
We can now collect data in .amat files.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-10-09 20:27:14 UTC (rev 8159)
+++ trunk/scripts/collectres	2007-10-09 23:31:02 UTC (rev 8160)
@@ -53,36 +53,40 @@
 #    
 
 import sys,string
-from plearn.vmat.PMat import *
+from plearn.vmat.smartReadMat import *
 from numpy import *
 
 # should probably be an option
 separator = "_"
-def get_col_index(a,colspec):
-  if colspec in a.fieldnames:
-    index=a.fieldnames.index(colspec)
+def get_col_index(fieldnames,colspec):
+  if colspec in fieldnames:
+    index=fieldnames.index(colspec)
   else:
     index=int(colspec)
   return index
 
-def selectres(loc_specs,a):
+def selectres(loc_specs,a,fieldnames):
+  print 'b'
   res = []
   loc_mode = loc_specs[0]
+  print 'c'
   if loc_mode=="pos":
-    row=get_col_index(a,loc_specs[1])
+    print 'd'
+    row=get_col_index(fieldnames,loc_specs[1])
     if row<0:
-      row = max(0,a.length+row)
+      row = max(0,a.shape[0]+row)
     i=2
-    if row<a.length:
+    if row<a.shape[0]:
       while len(loc_specs[i:])>0:
         if loc_specs[i]=="all":
           res.extend(a[row])
         else:
-          index=get_col_index(a,loc_specs[i])
+          index=get_col_index(fieldnames,loc_specs[i])
           res.append(a[row,index])
         i+=1
   elif loc_mode=="mincol":
-    mcol = get_col_index(a,loc_specs[1])
+    print 'e'
+    mcol = get_col_index(fieldnames,loc_specs[1])
     mrow = argmin(a[:,mcol])
     print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
     i=1
@@ -90,30 +94,41 @@
       if loc_specs[i]=="all":
         res.extend(a[mrow])
       else:
-        index=get_col_index(a,loc_specs[i])
+        index=get_col_index(fieldnames,loc_specs[i])
         res.append(a[mrow,index])
       i+=1
   elif loc_mode=="cols":
+    print 'f'
     minrow = int(loc_specs[3])
     maxrow = int(loc_specs[4])
+    print 'g'
     if maxrow<0:
-      maxrow = a.length+maxrow
+      maxrow = a.shape[0]+maxrow
     maxrow += 1
     i=5
     b=[]
     res = zeros([maxrow-minrow,0])
+    print 'h'
     while len(loc_specs[i:])>0:
-      col = get_col_index(a,loc_specs[i])
+      col = get_col_index(fieldnames,loc_specs[i])
+      print 'i'
       if col<0:
+        print 'i1'
         b = array(range(minrow,maxrow))
-        b.resize(res.shape[0],1)
       else:
+        print 'i2'
         b=a[minrow:maxrow,col].copy()
-        la = a.length
+        la = a.shape[0]
         if la<maxrow:
             b[la-minrow:] = float('NaN')
+      print 'j'
       if b.any():
+        b.resize(res.shape[0],1)
+        print 'k'
+        print 'res.shape =', res.shape
+        print 'b.shape =', b.shape
         res=concatenate([res,b],1)
+        print 'l'
       if i==5:
         i=6
       else:
@@ -128,9 +143,15 @@
   all_results = []
   for filename in filenames:
     try:
+      print 'a'
+      spouik = smartReadMat(filename)
+      print 'a1'
       file_res = selectres(loc_specs,
-                           PMat(filename))
+                           *spouik)
+      print 'a2'
+      print 'x'
       all_results.append([file_res,filename])
+      print 'y'
     except ValueError,v:
       print >>sys.stderr, "caught ValueError exception in", filename
       print >>sys.stderr, v
@@ -143,6 +164,7 @@
       print >>sys.stderr, "caught IOError exception in", filename
       print >>sys.stderr, v
       print >>sys.stderr, ""
+  print 'z'
   return all_results
 
 def compare_res(x,y):
@@ -263,6 +285,7 @@
   if len(args)<=3:
     print "Usage: collectres <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
     print 
+    print "File formats pmat, amat and csv are supported."
     print "The <spec> can be the following (note how the <spec> has to be surrounded by quotes):"
     print '  "min <location-spec>" : identify the mininum of <location-spec> over the <file*.pmat>'
     print '                          prints the values of the selected <location-spec> and the name'



From lamblin at mail.berlios.de  Wed Oct 10 01:51:29 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 10 Oct 2007 01:51:29 +0200
Subject: [Plearn-commits] r8161 - trunk/scripts
Message-ID: <200710092351.l99NpT7X022596@sheep.berlios.de>

Author: lamblin
Date: 2007-10-10 01:51:29 +0200 (Wed, 10 Oct 2007)
New Revision: 8161

Modified:
   trunk/scripts/collectres
Log:
Remove debug print statements


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-10-09 23:31:02 UTC (rev 8160)
+++ trunk/scripts/collectres	2007-10-09 23:51:29 UTC (rev 8161)
@@ -50,7 +50,7 @@
 # Finally I could display the curves (after possibly editing the gnuplot command in the output file) using
 #
 #   smartplot letter-sgrad-vs-BlockNatGrad-best-valid-NLL-plot.eps letter-sgrad-vs-BlockNatGrad-best-valid-NLL-plot
-#    
+#
 
 import sys,string
 from plearn.vmat.smartReadMat import *
@@ -66,12 +66,9 @@
   return index
 
 def selectres(loc_specs,a,fieldnames):
-  print 'b'
   res = []
   loc_mode = loc_specs[0]
-  print 'c'
   if loc_mode=="pos":
-    print 'd'
     row=get_col_index(fieldnames,loc_specs[1])
     if row<0:
       row = max(0,a.shape[0]+row)
@@ -85,7 +82,6 @@
           res.append(a[row,index])
         i+=1
   elif loc_mode=="mincol":
-    print 'e'
     mcol = get_col_index(fieldnames,loc_specs[1])
     mrow = argmin(a[:,mcol])
     print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
@@ -98,37 +94,26 @@
         res.append(a[mrow,index])
       i+=1
   elif loc_mode=="cols":
-    print 'f'
     minrow = int(loc_specs[3])
     maxrow = int(loc_specs[4])
-    print 'g'
     if maxrow<0:
       maxrow = a.shape[0]+maxrow
     maxrow += 1
     i=5
     b=[]
     res = zeros([maxrow-minrow,0])
-    print 'h'
     while len(loc_specs[i:])>0:
       col = get_col_index(fieldnames,loc_specs[i])
-      print 'i'
       if col<0:
-        print 'i1'
         b = array(range(minrow,maxrow))
       else:
-        print 'i2'
         b=a[minrow:maxrow,col].copy()
         la = a.shape[0]
         if la<maxrow:
             b[la-minrow:] = float('NaN')
-      print 'j'
       if b.any():
         b.resize(res.shape[0],1)
-        print 'k'
-        print 'res.shape =', res.shape
-        print 'b.shape =', b.shape
         res=concatenate([res,b],1)
-        print 'l'
       if i==5:
         i=6
       else:
@@ -143,15 +128,9 @@
   all_results = []
   for filename in filenames:
     try:
-      print 'a'
-      spouik = smartReadMat(filename)
-      print 'a1'
       file_res = selectres(loc_specs,
-                           *spouik)
-      print 'a2'
-      print 'x'
+                           *smartReadMat(filename))
       all_results.append([file_res,filename])
-      print 'y'
     except ValueError,v:
       print >>sys.stderr, "caught ValueError exception in", filename
       print >>sys.stderr, v
@@ -164,7 +143,6 @@
       print >>sys.stderr, "caught IOError exception in", filename
       print >>sys.stderr, v
       print >>sys.stderr, ""
-  print 'z'
   return all_results
 
 def compare_res(x,y):



From lamblin at mail.berlios.de  Wed Oct 10 02:59:07 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 10 Oct 2007 02:59:07 +0200
Subject: [Plearn-commits] r8163 - trunk/plearn_learners/online
Message-ID: <200710100059.l9A0x7cv027683@sheep.berlios.de>

Author: lamblin
Date: 2007-10-10 02:59:06 +0200 (Wed, 10 Oct 2007)
New Revision: 8163

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
- safest computation of partition functin
- new remote method to perform a CD update, given the statistics


Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-10-10 00:50:05 UTC (rev 8162)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-10-10 00:59:06 UTC (rev 8163)
@@ -142,6 +142,11 @@
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
+    virtual int getConfigurationCount()
+    {
+        return INFINITE_CONFIGURATIONS;
+    }
+
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-10-10 00:50:05 UTC (rev 8162)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-10-10 00:59:06 UTC (rev 8163)
@@ -265,6 +265,29 @@
     inherited::declareOptions(ol);
 }
 
+void RBMModule::declareMethods(RemoteMethodMap& rmm)
+{
+    // Make sure that inherited methods are declared
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(rmm, "CDUpdate", &RBMModule::CDUpdate,
+                  (BodyDoc("Perform one CD_k update"),
+                   ArgDoc ("v_0", "Positive phase statistics on visible layer"),
+                   ArgDoc ("h_0", "Positive phase statistics on hidden layer"),
+                   ArgDoc ("v_k", "Negative phase statistics on visible layer"),
+                   ArgDoc ("h_k", "Negative phase statistics on hidden layer")
+                  ));
+}
+
+void RBMModule::CDUpdate(const Mat& v_0, const Mat& h_0,
+                         const Mat& v_k, const Mat& h_k)
+{
+    visible_layer->update(v_0, v_k);
+    hidden_layer->update(h_0, h_k);
+    connection->update(v_0, h_0, v_k, h_k);
+    partition_function_is_stale = true;
+}
+
 ////////////
 // build_ //
 ////////////
@@ -804,7 +827,38 @@
     {
         if (partition_function_is_stale && !during_training)
         {
+            // Save layers' state
+            Mat visible_activations = visible_layer->activations.copy();
+            Mat visible_expectations = visible_layer->getExpectations().copy();
+            Mat visible_samples = visible_layer->samples.copy();
+
+            Mat hidden_activations = hidden_layer->activations.copy();
+            Mat hidden_expectations = hidden_layer->getExpectations().copy();
+            Mat hidden_samples = hidden_layer->samples.copy();
+
             computePartitionFunction();
+
+            // Restore layers' state
+            visible_layer->activations.resize(visible_activations.length(),
+                                              visible_activations.width());
+            visible_layer->activations << visible_activations;
+
+            visible_layer->setExpectations(visible_expectations);
+
+            visible_layer->samples.resize(visible_samples.length(),
+                                          visible_samples.width());
+            visible_layer->samples << visible_samples;
+
+            hidden_layer->activations.resize(hidden_activations.length(),
+                                              hidden_activations.width());
+            hidden_layer->activations << hidden_activations;
+
+            hidden_layer->setExpectations(hidden_expectations);
+
+            hidden_layer->samples.resize(hidden_samples.length(),
+                                          hidden_samples.width());
+            hidden_layer->samples << hidden_samples;
+
             partition_function_is_stale=false;
         }
         if (visible && !visible_is_output
@@ -1057,14 +1111,14 @@
         }
         if (hidden && hidden_is_output)
         {
-            hidden->resize(hidden_layer->samples.length(),
-                           hidden_layer->samples.width());
+            hidden->resize(hidden_layer->getExpectations().length(),
+                           hidden_layer->getExpectations().width());
             *hidden << hidden_layer->getExpectations();
         }
         if (hidden_act && hidden_act_is_output)
         {
-            hidden_act->resize(hidden_layer->samples.length(),
-                               hidden_layer->samples.width());
+            hidden_act->resize(hidden_layer->activations.length(),
+                               hidden_layer->activations.width());
             *hidden_act << hidden_layer->activations;
         }
         found_a_valid_configuration = true;

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-10-10 00:50:05 UTC (rev 8162)
+++ trunk/plearn_learners/online/RBMModule.h	2007-10-10 00:59:06 UTC (rev 8163)
@@ -107,6 +107,10 @@
 
     // Your other public member functions go here
 
+    //! Perform one CD_k update, given the Markov chain statistics
+    virtual void CDUpdate(const Mat& v_0, const Mat& h_0,
+                          const Mat& v_k, const Mat& h_k);
+
     //! given the input, compute the output (possibly resize it appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
 
@@ -329,6 +333,10 @@
 
     void computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat& neg_log_pvisible_given_phidden);
 
+protected:
+    static void declareMethods(RemoteMethodMap& rmm);
+
+
 private:
     //#####  Private Member Functions  ########################################
 



From lamblin at mail.berlios.de  Wed Oct 10 02:50:06 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 10 Oct 2007 02:50:06 +0200
Subject: [Plearn-commits] r8162 - trunk/python_modules/plearn/vmat
Message-ID: <200710100050.l9A0o6RV027100@sheep.berlios.de>

Author: lamblin
Date: 2007-10-10 02:50:05 +0200 (Wed, 10 Oct 2007)
New Revision: 8162

Modified:
   trunk/python_modules/plearn/vmat/readAMat.py
Log:
Allow empty line at the end of amat


Modified: trunk/python_modules/plearn/vmat/readAMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/readAMat.py	2007-10-09 23:51:29 UTC (rev 8161)
+++ trunk/python_modules/plearn/vmat/readAMat.py	2007-10-10 00:50:05 UTC (rev 8162)
@@ -70,7 +70,8 @@
         elif not line.startswith('#'):
             # Add all non-comment lines.
             row = [ safefloat(x) for x in line.strip().split() ]
-            a.append(row)
+            if row:
+                a.append(row)
 
     f.close()
     return array(a), fieldnames



From nouiz at mail.berlios.de  Wed Oct 10 15:27:19 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Oct 2007 15:27:19 +0200
Subject: [Plearn-commits] r8164 - trunk/python_modules/plearn/parallel
Message-ID: <200710101327.l9ADRJ4D010101@sheep.berlios.de>

Author: nouiz
Date: 2007-10-10 15:27:19 +0200 (Wed, 10 Oct 2007)
New Revision: 8164

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-Print more jobs status information
-bugfix


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 00:59:06 UTC (rev 8163)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 13:27:19 UTC (rev 8164)
@@ -242,11 +242,36 @@
 
     def wait(self):
         print "[DBI] WARNING the wait function was not overrided by the sub class!"
-        
+
+    def print_jobs_status(self):
+        finished=0
+        running=0
+        waiting=0
+        error=0
+        init=0
+        unfinished=[]        
+        for t in self.tasks:
+            if t.status==STATUS_INIT:
+                init+=1
+                unfinished+=t.id
+            elif t.status==STATUS_RUNNING:
+                running+=1
+                unfinished+=t.id
+            elif t.status==STATUS_FINISHED:
+                finished+=1
+            elif t.status==STATUS_WAITING:
+                waiting+=i
+                unfinished+=t.id
+            else:
+                print "[DBI] jobs %i have a bad status: %d",t.id
+            print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, error: %d, init: %d"%(len(self.tasks),finished, running, waiting, error, init)
+            print "[DBI] jobs unfinished: ",[i.id for i in unfinished]
+                                
 class Task:
 
-    def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, gen_unique_id = True, args = {}):
+    def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, id=-1, gen_unique_id = True, args = {}):
         self.add_unique_id = 0
+        self.id=id
         # The "python utils.py..." command is not exactly the same for every
         # task in a batch, so it cannot be considered a "pre-command", and
         # has to be actually part of the command.  Since the user-provided
@@ -367,6 +392,7 @@
         self.started=0
         self.nb_proc=50
         self.mt=None
+        self.args=args
         DBIBase.__init__(self, commands, **args)
         self.pre_tasks=["echo '[DBI] executing on host' $HOSTNAME"]+self.pre_tasks
         self.post_tasks=["echo '[DBI] exit status' $?"]+self.post_tasks
@@ -380,10 +406,13 @@
             commands=[commands]
 
         # create the information about the tasks
+        id=len(self.tasks)+1
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format,self.pre_tasks,
-                                   self.post_tasks,self.dolog,False))
+                                   self.post_tasks,self.dolog,id,False,
+                                   self.args))
+            id+=1
 
 
     def run_one_job(self, task):
@@ -450,7 +479,13 @@
 
     def wait(self):
         if self.mt:
-            self.mt.join()
+            try:
+                self.mt.join()
+            except KeyboardInterrupt, e:
+                print "[DBI] Catched KeyboardInterrupt"
+                self.print_jobs_status()
+                raise
+            
         else:
             print "[DBI] WARNING jobs not started!"
         print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
@@ -504,9 +539,12 @@
 
         # create the information about the tasks
         for command in commands:
+            id=len(self.tasks)+1
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format,self.pre_tasks,
-                                   self.post_tasks,self.dolog,False,self.args))
+                                   self.post_tasks,self.dolog,id,False,
+                                   self.args))
+            id+=1
     def run(self):
         pre_batch_command = ';'.join( self.pre_batch );
         post_batch_command = ';'.join( self.post_batch );
@@ -592,7 +630,7 @@
         
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
-
+        self.args = args
         self.add_commands(commands)
 
     def add_commands(self,commands):
@@ -600,6 +638,7 @@
             commands=[commands]
 
         # create the information about the tasks
+        id=len(self.tasks)+1
         for command in commands:
             pos = string.find(command,' ')
             if pos>=0:
@@ -665,8 +704,9 @@
 
             self.tasks.append(Task(newcommand, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
-                                   self.post_tasks,self.dolog,False,args))
-
+                                   self.post_tasks,self.dolog,id,False,
+                                   self.args))
+            id+=1
             #keeps a list of the temporary files created, so that they can be deleted at will            
 
     def run_all_job(self):
@@ -836,6 +876,7 @@
         post_tasks=self.post_tasks
         dolog=self.dolog
         args=self.args
+        id=len(self.tasks)+1
         for command in commands:
             pos = string.find(command,' ')
             if pos>=0:
@@ -858,7 +899,8 @@
                 raise Exception("The command '"+c+"' do not exist or have execution permission!")
             self.tasks.append(Task(command, tmp_dir, log_dir,
                                    time_format, pre_tasks,
-                                   post_tasks,dolog,False,args))
+                                   post_tasks,dolog,id,False,self.args))
+            id+=1
         #keeps a list of the temporary files created, so that they can be deleted at will            
 
     def run_one_job(self,task):
@@ -905,16 +947,22 @@
         # Execute post-batchs
         self.exec_post_batch()
             
-        print "[DBI] The Log file are under %s"%self.log_dir
         
     def clean(self):
         pass
 
     def wait(self):
         if self.mt:
-            self.mt.join()
+            try:
+                self.mt.join()
+            except KeyboardInterrupt, e:
+                print "[DBI] Catched KeyboardInterrupt"
+                self.print_jobs_status()
+                print "[DBI] The Log file are under %s"%self.log_dir
+                raise
         else:
             print "[DBI] WARNING jobs not started!"
+        print "[DBI] The Log file are under %s"%self.log_dir
                 
 class SshHost:
     def __init__(self, hostname,nice=19,get_avail=True):
@@ -1013,7 +1061,7 @@
         print "[DBI] Use at your own risk!"
         self.nb_proc=1
         DBIBase.__init__(self, commands, **args)
-
+        self.args=args
         self.add_commands(commands)
         self.hosts= find_all_ssh_hosts()
         print "[DBI] hosts: ",self.hosts
@@ -1023,10 +1071,13 @@
             commands=[commands]
             
         # create the information about the tasks
+        id=len(self.tasks)+1
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
-                                   self.post_tasks,self.dolog,False))
+                                   self.post_tasks,self.dolog,id,False,
+                                   self.args))
+            id+=1
             
     def getHost(self):
         self.hosts.sort(cmp= cmp_ssh_hosts)



From manzagop at mail.berlios.de  Wed Oct 10 17:34:30 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 10 Oct 2007 17:34:30 +0200
Subject: [Plearn-commits] r8165 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710101534.l9AFYUkG018962@sheep.berlios.de>

Author: manzagop
Date: 2007-10-10 17:34:30 +0200 (Wed, 10 Oct 2007)
New Revision: 8165

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
- Bug fix: n_updates was not properly computed.
- Added the neuronDiscountGrad (now strategy 3) to perform specific
discounting to neighbour weights (within a neuron).



Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-10 13:27:19 UTC (rev 8164)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-10 15:34:30 UTC (rev 8165)
@@ -61,6 +61,7 @@
       pv_random_sample_step(false),
       pv_self_discount(0.5),
       pv_other_discount(0.95),
+      pv_within_neuron_discount(0.95),
       n_updates(0)
 {
     random_gen = new PRandom();
@@ -128,6 +129,12 @@
                   "Discount used to perform soft invalidation of other weights'\n" 
                   "statistics after a weight update.");
 
+    declareOption(ol, "pv_within_neuron_discount",
+                  &PvGradNNet::pv_within_neuron_discount,
+                  OptionBase::buildoption,
+                  "Discount used to perform soft invalidation of other weights'\n"
+                  "(same neuron) statistics after a weight update.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -149,6 +156,7 @@
     pv_layer_stepsigns.resize(n_layers-1);
     pv_layer_stepsizes.resize(n_layers-1);
     int np;
+    int n_neurons=0;
     for (int i=0,p=0;i<n_layers-1;i++)  {
         np=layer_sizes[i+1]*(1+layer_sizes[i]);
         pv_layer_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
@@ -157,10 +165,10 @@
         pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
         p+=np;
+        n_neurons+=layer_sizes[i+1];
     }
+    n_neuron_updates.resize(n_neurons);
 
-//    pv_gradstats = new VecStatsCollector();
-
 }
 
 // ### Nothing to add here, simply calls build_
@@ -185,7 +193,7 @@
     deepCopyField(pv_layer_stepsigns, copies);
     deepCopyField(pv_all_stepsizes, copies);
     deepCopyField(pv_layer_stepsizes, copies);
-
+    deepCopyField(n_neuron_updates, copies);
 //    deepCopyField(pv_gradstats, copies);
 }
 
@@ -204,7 +212,8 @@
 
     // used in the discountGrad() strategy
     n_updates = 0; 
-    
+    n_neuron_updates.fill(0);    
+
 //    pv_gradstats->forget();
 }
 
@@ -221,9 +230,12 @@
             discountGrad();
             break;
         case 3 :
+            neuronDiscountGrad();
+            break;
+        case 4 :
             globalSyncGrad();
             break;
-        case 4 :
+        case 5 :
             neuronSyncGrad();
             break;
         default :
@@ -373,9 +385,8 @@
 
             // TODO - for current treatment, not necessary to compute actual
             // prob. Comparing the ratio would be sufficient.
-/*            prob_pos = gauss_01_cum(m/e);
+            /*prob_pos = gauss_01_cum(m/e);
             prob_neg = 1.-prob_pos;
-
             if(prob_pos>=pv_required_confidence)
                 stepsign = 1;
             else if(prob_neg>=pv_required_confidence)
@@ -413,10 +424,103 @@
             pv_all_nsamples[k]*=sd;
             pv_all_sum[k]*=sd;
             pv_all_sumsquare[k]*=sd;
+            n_updates++;
+
         }
     }
 }
 
+//! Same as discountGrad but also performs discount based on within neuron 
+//! relationships
+void PvGradNNet::neuronDiscountGrad()
+{
+    real m, e;//, prob_pos, prob_neg;
+    int stepsign;
+
+    real ratio;
+    real limit_ratio = gauss_01_quantile(pv_required_confidence);
+
+    //
+    real discount = pow(pv_other_discount,n_updates);
+    real d;
+    n_updates = 0;
+    if( discount < 0.001 )
+        PLWARNING("PvGradNNet::discountGrad() - discount < 0.001 - that seems small...");
+    real sd = pv_self_discount / pv_other_discount; // trick: apply this self discount
+                                                    // and then discount
+                                                    // everyone the same
+    sd /= pv_within_neuron_discount;
+
+    // k is an index on all the parameters.
+    // kk is an index on all neurons.
+    for(int l=0,k=0,kk=0; l<n_layers-1; l++)    {
+        for(int n=0; n<layer_sizes[l+1]; n++,kk++)   {
+            d = discount * pow(pv_within_neuron_discount,n_neuron_updates[kk]);
+            n_neuron_updates[kk]=0;
+            for(int w=0; w<1+layer_sizes[l]; w++,k++)   {
+
+                // Perform soft invalidation
+                pv_all_nsamples[k] *= d;
+                pv_all_sum[k] *= d;
+                pv_all_sumsquare[k] *= d;
+
+                // update stats
+                pv_all_nsamples[k]++;
+                pv_all_sum[k] += all_params_gradient[k];
+                pv_all_sumsquare[k] += all_params_gradient[k] * all_params_gradient[k];
+
+                if(pv_all_nsamples[k]>pv_min_samples)   {
+                    m = pv_all_sum[k] / pv_all_nsamples[k];
+                    e = real((pv_all_sumsquare[k] - square(pv_all_sum[k])/pv_all_nsamples[k])/(pv_all_nsamples[k]-1));
+                    e = sqrt(e/pv_all_nsamples[k]);
+
+                    // test to see if numerical problems
+                    if( fabs(m) < 1e-15 || e < 1e-15 )  {
+                        cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                        continue;
+                    }
+
+                    ratio=m/e;
+                    if(ratio>=limit_ratio)
+                        stepsign = 1;
+                    else if(ratio<=-limit_ratio)
+                        stepsign = -1;
+                    else
+                        continue;
+
+                    // consecutive steps of same sign, accelerate
+                    if( stepsign*pv_all_stepsigns[k]>0 )  {
+                        pv_all_stepsizes[k]*=pv_acceleration;
+                        if( pv_all_stepsizes[k] > pv_max_stepsize )
+                            pv_all_stepsizes[k] = pv_max_stepsize;
+                    // else if different signs decelerate
+                    }   else if( stepsign*pv_all_stepsigns[k]<0 )   {
+                        pv_all_stepsizes[k]*=pv_deceleration;
+                        if( pv_all_stepsizes[k] < pv_min_stepsize )
+                            pv_all_stepsizes[k] = pv_min_stepsize;
+                    // else (previous sign was undetermined
+                    }//   else    {
+                    //}
+                    // step
+                    if( stepsign > 0 )
+                        all_params[k] -= pv_all_stepsizes[k];
+                    else
+                        all_params[k] += pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = stepsign;
+                    // soft invalidation of self
+                    pv_all_nsamples[k]*=sd;
+                    pv_all_sum[k]*=sd;
+                    pv_all_sumsquare[k]*=sd;
+                    n_updates++;
+                    n_neuron_updates[kk]++;
+                }
+            //////////////////////////////////
+            }
+        }
+    }
+
+}
+
 void PvGradNNet::globalSyncGrad()
 {
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-10 13:27:19 UTC (rev 8164)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-10 15:34:30 UTC (rev 8165)
@@ -78,7 +78,7 @@
 
     //! For the discounting strategy. Used to discount stats when there are
     //! updates
-    real pv_self_discount, pv_other_discount;
+    real pv_self_discount, pv_other_discount, pv_within_neuron_discount;
 
 public:
     //#####  Public Not Build Options  ########################################
@@ -122,6 +122,7 @@
 
     void pvGrad(); 
     void discountGrad();
+    void neuronDiscountGrad();
     void globalSyncGrad();
     void neuronSyncGrad();
     
@@ -156,6 +157,8 @@
     // int is enough?
     int n_updates;
 
+    TVec<int> n_neuron_updates;
+
     //! accumulated statistics of gradients on each parameter.
     //PP<VecStatsCollector> pv_gradstats;
 



From nouiz at mail.berlios.de  Wed Oct 10 20:38:22 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Oct 2007 20:38:22 +0200
Subject: [Plearn-commits] r8166 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200710101838.l9AIcMLZ013688@sheep.berlios.de>

Author: nouiz
Date: 2007-10-10 20:38:21 +0200 (Wed, 10 Oct 2007)
New Revision: 8166

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
put the executed command for the cluster backend in a file to solve the trouble with bash quote


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 15:34:30 UTC (rev 8165)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 18:38:21 UTC (rev 8166)
@@ -235,7 +235,7 @@
                 print "[DBI] post_batch_command:",post_batch_command
             
     def clean(self):
-        pass
+        print "[DBI] WARNING the clean function was not overrided by the sub class!"
 
     def run(self):
         pass
@@ -400,6 +400,9 @@
         self.nb_proc=int(self.nb_proc)
         self.backend_failed=0
         self.jobs_failed=0
+        
+        if not os.path.exists(self.tmp_dir):
+            os.mkdir(self.tmp_dir)
 
     def add_commands(self,commands):
         if not isinstance(commands, list):
@@ -418,6 +421,15 @@
     def run_one_job(self, task):
         DBIBase.run(self)
         
+        remote_command=string.join(task.commands,';')
+        filename=os.path.join(self.tmp_dir,task.unique_id)
+        filename=os.path.abspath(filename)
+        f=open(filename,'w')
+        f.write(remote_command+'\n')
+        f.close()
+        os.chmod(filename, 0750)
+        self.temp_files.append(filename)
+        
         command = "cluster" 
         if self.arch == "32":
             command += " --typecpu 32bits"
@@ -429,7 +441,8 @@
             command += " --duree "+self.duree
         if self.cluster_wait:
             command += " --wait"
-        command += " --execute '"+string.join(task.commands,';') + "'"
+        command += " --execute '"+ filename + "'"
+
         self.started+=1
         started=self.started# not thread safe!!!
         print "[DBI,%d/%d,%s] %s"%(started,len(self.tasks),time.ctime(),command)
@@ -475,7 +488,8 @@
 
     def clean(self):
         #TODO: delete all log files for the current batch
-        pass
+        for f in self.temp_files:
+            os.remove(f)
 
     def wait(self):
         if self.mt:
@@ -488,6 +502,7 @@
             
         else:
             print "[DBI] WARNING jobs not started!"
+
         print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
         print "[DBI] Their was %d jobs that returned a failure status."%(self.jobs_failed)
         

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-10-10 15:34:30 UTC (rev 8165)
+++ trunk/scripts/dbidispatch	2007-10-10 18:38:21 UTC (rev 8166)
@@ -272,7 +272,10 @@
 jobs.run()
 jobs.wait()
 # There is %d command in the script"""%(len(commands)))
-        
+    if "test" in dbi_param:
+        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
+    else:
+        SCRIPT.write("jobs.clean()") 
     SCRIPT.close()
     os.system("chmod +x %s"%(ScriptName));
 
@@ -285,7 +288,11 @@
     t2=time.time()
     print "it took %f s to create the DBI objects"%(t2-t1)
     jobs.run()
+    t3=time.time()
     jobs.wait()
-    t3=time.time()
+    if "test" in dbi_param:
+        print "[DBI dispatch] In test mode, we do not make dbi errase temp file"
+    else:
+        jobs.clean()
     print "it took %f s to launch all the commands"%(t3-t2)
 



From nouiz at mail.berlios.de  Wed Oct 10 21:09:28 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Oct 2007 21:09:28 +0200
Subject: [Plearn-commits] r8167 - trunk/python_modules/plearn/parallel
Message-ID: <200710101909.l9AJ9SmX015705@sheep.berlios.de>

Author: nouiz
Date: 2007-10-10 21:09:28 +0200 (Wed, 10 Oct 2007)
New Revision: 8167

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
print jobs status after the wait() function


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 18:38:21 UTC (rev 8166)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 19:09:28 UTC (rev 8167)
@@ -33,7 +33,7 @@
 STATUS_FINISHED = 0
 STATUS_RUNNING = 1
 STATUS_WAITING = 2
-STATUS_ERROR = 3
+STATUS_INIT = 3
 
 #original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
 class LockedIterator:
@@ -247,25 +247,24 @@
         finished=0
         running=0
         waiting=0
-        error=0
         init=0
         unfinished=[]        
         for t in self.tasks:
             if t.status==STATUS_INIT:
                 init+=1
-                unfinished+=t.id
+                unfinished.append(t.id)
             elif t.status==STATUS_RUNNING:
                 running+=1
-                unfinished+=t.id
+                unfinished.append(t.id)
             elif t.status==STATUS_FINISHED:
                 finished+=1
             elif t.status==STATUS_WAITING:
                 waiting+=i
-                unfinished+=t.id
+                unfinished.append(t.id)
             else:
                 print "[DBI] jobs %i have a bad status: %d",t.id
-            print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, error: %d, init: %d"%(len(self.tasks),finished, running, waiting, error, init)
-            print "[DBI] jobs unfinished: ",[i.id for i in unfinished]
+            print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, init: %d"%(len(self.tasks),finished, running, waiting, init)
+            print "[DBI] jobs unfinished (starting at 1): ",unfinished
                                 
 class Task:
 
@@ -323,7 +322,7 @@
                 string.join([self.log_file,'FINISHED_TIME',time_format],' '))
 
         #print "self.commands =", self.commands
-
+        self.status=STATUS_INIT
     def get_status(self):
         #TODO: catch exception if value not available
         status = get_config_value(self.log_file,'STATUS')
@@ -420,6 +419,7 @@
 
     def run_one_job(self, task):
         DBIBase.run(self)
+        task.status=STATUS_RUNNING
         
         remote_command=string.join(task.commands,';')
         filename=os.path.join(self.tmp_dir,task.unique_id)
@@ -447,6 +447,7 @@
         started=self.started# not thread safe!!!
         print "[DBI,%d/%d,%s] %s"%(started,len(self.tasks),time.ctime(),command)
         if self.test:
+            task.status=STATUS_FINISHED
             return
 
         task.launch_time = time.time()
@@ -469,7 +470,8 @@
             self.backend_failed+=1
         elif task.dbi_return_status!=0:
             self.jobs_failed+=1
-            
+        task.status=STATUS_FINISHED
+  
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
         if self.test:
@@ -502,7 +504,7 @@
             
         else:
             print "[DBI] WARNING jobs not started!"
-
+        self.print_jobs_status()
         print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
         print "[DBI] Their was %d jobs that returned a failure status."%(self.jobs_failed)
         
@@ -622,7 +624,8 @@
         print "[DBI] All the log will be in the directory: ",self.log_dir
         # Launch bqsubmit
         if not self.test:
-            task.set_scheduled_time()
+            for t in self.tasks:
+                t.set_scheduled_time()
             self.p = Popen( 'bqsubmit', shell=True)
             self.p.wait()
         else:
@@ -833,7 +836,8 @@
         if self.test == False:
             (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
             print "[DBI] Executing: condor_submit " + condor_file
-            task.set_scheduled_time()
+            for task in self.tasks:
+                task.set_scheduled_time()
             self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
         else:
             print "[DBI] Created condor file: " + condor_file
@@ -932,6 +936,7 @@
         print "[DBI,%d/%d,%s] %s"%(self.started,len(self.tasks),time.ctime(),c)
         p = Popen(c, shell=True,stdout=output,stderr=error)
         p.wait()
+        task.status=STATUS_FINISHED
             
     def clean(self):
         if len(self.temp_files)>0:
@@ -977,6 +982,7 @@
                 raise
         else:
             print "[DBI] WARNING jobs not started!"
+        self.print_jobs_status()
         print "[DBI] The Log file are under %s"%self.log_dir
                 
 class SshHost:
@@ -1120,6 +1126,7 @@
         
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
         task.p.wait()
+        task.status=STATUS_FINISHED
 
 
     def run_one_job2(self, host):
@@ -1165,6 +1172,7 @@
                 del err[-1]
                 print "return status", task.return_status
             sleep(1)
+            task.status=STATUS_FINISHED
       
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
@@ -1193,6 +1201,7 @@
     def wait(self):
         #TODO
         self.mt.join()
+        self.print_jobs_status()
 
 
 # creates an object of type ('DBI' + launch_system) if it exists



From larocheh at mail.berlios.de  Thu Oct 11 00:15:21 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:15:21 +0200
Subject: [Plearn-commits] r8168 - trunk/plearn_learners_experimental
Message-ID: <200710102215.l9AMFLGS025582@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:15:21 +0200 (Thu, 11 Oct 2007)
New Revision: 8168

Modified:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
Log:
Correted some bugs...


Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-10 19:09:28 UTC (rev 8167)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-10 22:15:21 UTC (rev 8168)
@@ -42,6 +42,7 @@
 
 #include "StackedFocusedAutoassociatorsNet.h"
 #include <plearn/vmat/VMat_computeNearestNeighbors.h>
+#include <plearn/vmat/GetInputVMatrix.h>
 #include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMMixedConnection.h>
 
@@ -260,22 +261,30 @@
                 "there should be %d connections.\n",
                 n_layers-1);
 
-    if( reconstruction_connections.length() != n_layers-1 )
+    if( !fast_exact_is_equal( greedy_learning_rate, 0 ) 
+        && reconstruction_connections.length() != n_layers-1 )
         PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "there should be %d reconstruction connections.\n",
                 n_layers-1);
-
-    if(unsupervised_layers.length() != n_layers-2 
+    
+    if(  !( reconstruction_connections.length() == 0
+            || reconstruction_connections.length() == n_layers-1 ) )
+        PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be either 0 or %d reconstruction connections.\n",
+                n_layers-1);
+    
+    
+    if(unsupervised_layers.length() != n_layers-1 
        && unsupervised_layers.length() != 0)
         PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "there should be either 0 of %d unsupervised_layers.\n",
-                n_layers-2);
+                n_layers-1);
         
-    if(unsupervised_connections.length() != n_layers-2 
+    if(unsupervised_connections.length() != n_layers-1 
        && unsupervised_connections.length() != 0)
         PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
                 "there should be either 0 of %d unsupervised_connections.\n",
-                n_layers-2);
+                n_layers-1);
         
     if(unsupervised_connections.length() != unsupervised_layers.length())
         PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() - \n"
@@ -328,21 +337,24 @@
                         "connections[%i] should have a up_size of %d.\n",
                         i, unsupervised_layers[i+1]->size);
             
-            if( layers[i+1]->size + unsupervised_layers[i]->size != 
-                reconstruction_connections[i]->down_size )
-                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
-                        "- \n"
-                        "recontruction_connections[%i] should have a down_size of "
-                        "%d.\n",
-                        i, layers[i+1]->size + unsupervised_layers[i]->size);
-            
-            if( reconstruction_connections[i]->up_size != 
-                layers[i]->size )
-                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
-                        "- \n"
-                        "recontruction_connections[%i] should have a up_size of "
-                        "%d.\n",
-                        i, layers[i]->size);
+            if( reconstruction_connections.length() != 0 )
+            {
+                if( layers[i+1]->size + unsupervised_layers[i]->size != 
+                    reconstruction_connections[i]->down_size )
+                    PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                            "- \n"
+                            "recontruction_connections[%i] should have a down_size of "
+                            "%d.\n",
+                            i, layers[i+1]->size + unsupervised_layers[i]->size);
+                
+                if( reconstruction_connections[i]->up_size != 
+                    layers[i]->size )
+                    PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                            "- \n"
+                            "recontruction_connections[%i] should have a up_size of "
+                            "%d.\n",
+                            i, layers[i]->size);
+            }
 
             if( !(unsupervised_layers[i]->random_gen) )
             {
@@ -360,12 +372,13 @@
             greedy_layer->sub_layers.resize(2);
             greedy_layer->sub_layers[0] = layers[i+1];
             greedy_layer->sub_layers[1] = unsupervised_layers[i];
+            greedy_layer->size = layers[i+1]->size + unsupervised_layers[i]->size;
             greedy_layer->build();
 
             PP<RBMMixedConnection> greedy_connection = new RBMMixedConnection();
             greedy_connection->sub_connections.resize(2,1);
-            greedy_connection->sub_connections(1,0) = connections[i];
-            greedy_connection->sub_connections(2,0) = unsupervised_connections[i];
+            greedy_connection->sub_connections(0,0) = connections[i];
+            greedy_connection->sub_connections(1,0) = unsupervised_connections[i];
             greedy_connection->build();
             
             greedy_layers[i] = greedy_layer;
@@ -373,20 +386,22 @@
         }
         else
         {
-            if( layers[i+1]->size != reconstruction_connections[i]->down_size )
-                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
-                        "- \n"
-                        "recontruction_connections[%i] should have a down_size of "
-                        "%d.\n",
-                        i, layers[i+1]->size);
+            if( reconstruction_connections.length() != 0 )
+            {
+                if( layers[i+1]->size != reconstruction_connections[i]->down_size )
+                    PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                            "- \n"
+                            "recontruction_connections[%i] should have a down_size of "
+                            "%d.\n",
+                            i, layers[i+1]->size);
             
-            if( reconstruction_connections[i]->up_size != layers[i]->size )
-                PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
-                        "- \n"
-                        "recontruction_connections[%i] should have a up_size of "
-                        "%d.\n",
-                        i, layers[i]->size);
- 
+                if( reconstruction_connections[i]->up_size != layers[i]->size )
+                    PLERROR("StackedFocusedAutoassociatorsNet::build_layers_and_connections() "
+                            "- \n"
+                            "recontruction_connections[%i] should have a up_size of "
+                            "%d.\n",
+                            i, layers[i]->size);
+            }
             greedy_layers[i] = layers[i+1];
             greedy_connections[i] = connections[i];
         }
@@ -403,7 +418,8 @@
             connections[i]->forget();
         }
 
-        if( !(reconstruction_connections[i]->random_gen) )
+        if( reconstruction_connections.length() != 0
+            && !(reconstruction_connections[i]->random_gen) )
         {
             reconstruction_connections[i]->random_gen = random_gen;
             reconstruction_connections[i]->forget();
@@ -508,11 +524,11 @@
     train_set_representations_up_to_date = false;
 
     for( int i=0 ; i<n_layers-1 ; i++ )
-    {
         connections[i]->forget();
+    
+    for( int i=0; i<reconstruction_connections.length(); i++)
         reconstruction_connections[i]->forget();
-    }
-    
+
     stage = 0;
     greedy_stages.clear();
 }
@@ -593,8 +609,9 @@
             // Find similar example
 
             int sim_index = random_gen->uniform_multinomial_sample(k_neighbors);
-            train_set->getExample(nearest_neighbors_indices(sample,sim_index),
-                                  similar_example, target2, weight2);
+            class_datasets[(int)round(target[0])]->getExample(
+                nearest_neighbors_indices(sample,sim_index),
+                similar_example, target2, weight2);
 
             if(round(target[0]) != round(target2[0]))
                 PLERROR("StackedFocusedAutoassociatorsNet::train(): similar"
@@ -603,7 +620,7 @@
             // Find dissimilar example
 
             int dissim_class_index = random_gen->multinomial_sample(
-                other_classes_proportions(round(target[0])));
+                other_classes_proportions((int)round(target[0])));
 
             int dissim_index = random_gen->uniform_multinomial_sample(
                 class_datasets[dissim_class_index]->length());
@@ -611,7 +628,7 @@
             class_datasets[dissim_class_index]->getExample(dissim_index,
                                   dissimilar_example, target2, weight2);
 
-            if(round(target[0]) == round(target2[0]))
+            if(((int)round(target[0])) == ((int)round(target2[0])))
                 PLERROR("StackedFocusedAutoassociatorsNet::train(): dissimilar"
                     " example is from same class!");
 
@@ -664,17 +681,18 @@
             // Find similar example
 
             int sim_index = random_gen->uniform_multinomial_sample(k_neighbors);
-            train_set->getExample(nearest_neighbors_indices(sample,sim_index),
-                                  similar_example, target2, weight2);
+            class_datasets[(int)round(target[0])]->getExample(
+                nearest_neighbors_indices(sample,sim_index),
+                similar_example, target2, weight2);
 
-            if(round(target[0]) != round(target2[0]))
+            if(((int)round(target[0])) != ((int)round(target2[0])))
                 PLERROR("StackedFocusedAutoassociatorsNet::train(): similar"
                     " example is not from same class!");
 
             // Find dissimilar example
 
             int dissim_class_index = random_gen->multinomial_sample(
-                other_classes_proportions(round(target[0])));
+                other_classes_proportions((int)round(target[0])));
 
             int dissim_index = random_gen->uniform_multinomial_sample(
                 class_datasets[dissim_class_index]->length());
@@ -682,7 +700,7 @@
             class_datasets[dissim_class_index]->getExample(dissim_index,
                                   dissimilar_example, target2, weight2);
 
-            if(round(target[0]) == round(target2[0]))
+            if(((int)round(target[0])) == ((int)round(target2[0])))
                 PLERROR("StackedFocusedAutoassociatorsNet::train(): dissimilar"
                     " example is from same class!");
 
@@ -759,7 +777,7 @@
                                 layers[ index ]->expectation);
         
         layers[ index ]->activation << reconstruction_activations;
-        layers[ index ]->expectation_is_up_to_date = true;
+        layers[ index ]->setExpectationByRef(layers[ index ]->expectation);
         real rec_err = layers[ index ]->fpropNLL(previous_input_representation);
         train_costs[index] = rec_err;
         
@@ -790,8 +808,7 @@
     // RBM learning
     if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
     {
-        greedy_layers[index]->expectation << greedy_expectation;
-        greedy_layers[index]->expectation_is_up_to_date = true;
+        greedy_layers[index]->setExpectation( greedy_expectation );
         greedy_layers[index]->generateSample();
         
         // accumulate positive stats using the expectation
@@ -969,7 +986,7 @@
     updateTrainSetRepresentations();
 
     computeRepresentation(input,input_representation, 
-                          max(currently_trained_layer,n_layers-1));
+                          min(currently_trained_layer,n_layers-1));
 
     computeNearestNeighbors(train_set_representations_vmat,input_representation,
                             test_nearest_neighbors_indices);
@@ -991,7 +1008,8 @@
     costs.resize( getTestCostNames().length() );
     costs.fill( MISSING_VALUE );
 
-    if( currently_trained_layer<n_layers )
+    if( currently_trained_layer<n_layers 
+        && reconstruction_connections.length() != 0 )
     {
         greedy_connections[currently_trained_layer-1]->fprop(
             expectations[currently_trained_layer-1],
@@ -1009,13 +1027,14 @@
         
         layers[ currently_trained_layer-1 ]->activation << 
             reconstruction_activations;
-        layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        layers[ currently_trained_layer-1 ]->setExpectationByRef( 
+            layers[ currently_trained_layer-1 ]->expectation);
         costs[ currently_trained_layer-1 ]  = 
             layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[currently_trained_layer-1]);
     }
 
-    if( round(output[0]) == round(target[0]) )
+    if( ((int)round(output[0])) == ((int)round(target[0])) )
         costs[n_layers-1] = 0;
     else
         costs[n_layers-1] = 1;
@@ -1029,7 +1048,7 @@
     if(!train_set_representations_up_to_date)
     {
         // Precompute training set examples' representation
-        int l = max(currently_trained_layer,n_layers-1);
+        int l = min(currently_trained_layer,n_layers-1);
         Vec input( inputsize() );
         Vec target( targetsize() );
         Vec train_set_representation;
@@ -1043,7 +1062,7 @@
             train_set->getExample(i,input,target,weight);
             computeRepresentation(input,train_set_representation,l);
             train_set_representations(i) << train_set_representation;
-            train_set_targets[i] = round(target[0]);
+            train_set_targets[i] = (int)round(target[0]);
         }
         train_set_representations_vmat = VMat(train_set_representations);
 
@@ -1083,6 +1102,7 @@
     real weight; // unused
 
     // Separate classes
+    class_datasets.resize(n_classes);
     for(int k=0; k<n_classes; k++)
     {
         class_datasets[k] = new ClassSubsetVMatrix();
@@ -1093,6 +1113,7 @@
     }
 
     // Find other classes proportions
+    other_classes_proportions.resize(n_classes,n_classes);
     other_classes_proportions.fill(0);
     for(int k=0; k<n_classes; k++)
     {
@@ -1118,9 +1139,10 @@
             class_datasets[k]->getExample(i,input,target,weight);
             nearest_neighbors_indices_row = nearest_neighbors_indices(
                 class_datasets[k]->indices[i]);
-            computeNearestNeighbors((VMatrix *)class_datasets[k],input,
-                                    nearest_neighbors_indices_row,
-                                    i);
+            computeNearestNeighbors(
+                new GetInputVMatrix((VMatrix *)class_datasets[k]),input,
+                nearest_neighbors_indices_row,
+                i);
         }
     }
 }



From larocheh at mail.berlios.de  Thu Oct 11 00:27:35 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:27:35 +0200
Subject: [Plearn-commits] r8169 - trunk/plearn/var
Message-ID: <200710102227.l9AMRZ35027596@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:27:34 +0200 (Thu, 11 Oct 2007)
New Revision: 8169

Modified:
   trunk/plearn/var/NllGeneralGaussianVariable.cc
   trunk/plearn/var/NllGeneralGaussianVariable.h
Log:


Modified: trunk/plearn/var/NllGeneralGaussianVariable.cc
===================================================================
--- trunk/plearn/var/NllGeneralGaussianVariable.cc	2007-10-10 22:15:21 UTC (rev 8168)
+++ trunk/plearn/var/NllGeneralGaussianVariable.cc	2007-10-10 22:27:34 UTC (rev 8169)
@@ -43,41 +43,41 @@
 #include <plearn/var/NllGeneralGaussianVariable.h>
 #include <plearn/var/Var_operators.h>
 #include <plearn/math/plapack.h>
-//#include "Var_utils.h"
 
 namespace PLearn {
 using namespace std;
 
-// R += alpa ( M - v1 v2')
-template<class T>
-void my_weird_product(const TMat<T>& R, const TMat<T>& M, const TVec<T>& v1, const TVec<T>& v2,T alpha)
-{
-#ifdef BOUNDCHECK
-    if (M.length() != R.length() || M.width() != R.width() || v1.length()!=M.length() || M.width()!=v2.length() )
-        PLERROR("my_weird_product: incompatible arguments' sizes");
-#endif
-    const T* v_1=v1.data();
-    const T* v_2=v2.data();
-    for (int i=0;i<M.length();i++)
-    {
-        T* mi = M[i];
-        T* ri = R[i];
-        T v1i = v_1[i];
-        for (int j=0;j<M.width();j++)
-            ri[j] += alpha*(mi[j] - v1i * v_2[j]);
-    }
-}
-
 /** NllGeneralGaussianVariable **/
 
 PLEARN_IMPLEMENT_OBJECT(NllGeneralGaussianVariable,
-                        "to do.",
-                        " I said TO DO.\n");
+                        "Computes the NLL under a Gaussian distribution centered "
+                        "around a data point.",
+                        "This variable computes the negative log-likelihood "
+                        "under a Gaussian distribution\n"
+                        "centered near a data point. The likelihood is computed "
+                        "for some given neighbors\n"
+                        "of the data point. A set of bases defining the "
+                        "principal components of the\n"
+                        "covariance matrix, the difference mu between "
+                        "the data point and the center of \n"
+                        "the Gaussian and the noise variance in all directions "
+                        "of the space must be\n"
+                        "specified. Gradient is propagated in all these "
+                        "parameters. Optionally, the \n"
+                        "gradient for mu can be computed based on the likelihood "
+                        "of less nearest neighbors.\n"
+                        "It is assumed that this Gaussian is part of a mixture "
+                        "model with L components.\n"
+    );
   
-NllGeneralGaussianVariable::NllGeneralGaussianVariable(const VarArray& the_varray, real thelogL, int the_mu_nneighbors) 
+NllGeneralGaussianVariable::NllGeneralGaussianVariable(const VarArray& the_varray, real thelogL, bool the_use_mu, int the_mu_nneighbors) 
     : inherited(the_varray,the_varray[3]->length(),1), 
-      n(varray[0]->width()), log_L(thelogL),ncomponents(varray[0]->length()),
-      nneighbors(varray[3]->length()), mu_nneighbors(the_mu_nneighbors)
+      n(varray[3]->size()), 
+      ncomponents(varray[0]->length()%varray[3]->size()),
+      nneighbors(varray[4]->length()),
+      log_L(thelogL),
+      use_mu(the_use_mu),
+      mu_nneighbors(the_mu_nneighbors)
 {
     build_();
 }
@@ -95,54 +95,65 @@
 {
     
     // The VarArray constaints the following variables:
-    //    - varray[0] = the tangent plane (ncomponents x n)
+    //    - varray[0] = the tangent plane (ncomponents x n sized vector)
     //    - varray[1] = mu(data_point) (n x 1)
     //    - varray[2] = sigma_noise (1 x 1)
-    //    - varray[3] = neighbor_distances (nneighbors x n)
+    //    - varray[3] = input data point around which the Gaussian is centered
+    //    - varray[4] = nearest neighbors (nneighbors x n)
      
-    if(varray.length() != 4 && varray.length() != 6)
-        PLERROR("In NllGeneralGaussianVariable constructor: varray is of length %d but should be of length %d", varray.length(), 4);
+    if(varray.length() != 5)
+        PLERROR("In NllGeneralGaussianVariable::build_(): varray is of "
+                "length %d but should be of length %d", varray.length(), 5);
     
-    if(varray[1]->length() != n || varray[1]->width() != 1) PLERROR("In NllGeneralGaussianVariable constructor: varray[1] is of size (%d,%d), but should be of size (%d,%d)",
-                                                                    varray[1]->length(), varray[1]->width(),
-                                                                    ncomponents, 1);
-    if(varray[2]->length() != 1 || varray[2]->width() != 1) PLERROR("In NllGeneralGaussianVariable constructor: varray[2] is of size (%d,%d), but should be of size (%d,%d)",
-                                                                    varray[2]->length(), varray[2]->width(),
-                                                                    1, 1);
-    if(varray[3]->width() != n) PLERROR("In NllGeneralGaussianVariable constructor: varray[3] is of size (%d,%d), but should be of size (%d,%d)",
-                                        varray[3]->length(), varray[3]->width(),
-                                        nneighbors, n);
+    if(varray[1]->length() != n || varray[1]->width() != 1) 
+        PLERROR("In NllGeneralGaussianVariable::build_(): varray[1] "
+                "is of size (%d,%d), but should be of size (%d,%d)",
+                varray[1]->length(), varray[1]->width(),
+                ncomponents, 1);
 
+    if(varray[2]->length() != 1 || varray[2]->width() != 1) 
+        PLERROR("In NllGeneralGaussianVariable::build_(): varray[2] "
+                "is of size (%d,%d), but should be of size (%d,%d)",
+                varray[2]->length(), varray[2]->width(),
+                1, 1);
+    
+    if(varray[3]->length() != n || varray[3]->width() != 1) 
+        PLERROR("In NllGeneralGaussianVariable::build_(): varray[3] "
+                "is of size (%d,%d), but should be of size (%d,%d)",
+                varray[3]->length(), varray[3]->width(),
+                n,1);
+
+    if(varray[4]->width() != n) 
+        PLERROR("In NllGeneralGaussianVariable::build_(): varray[4] "
+                "is of size (%d,%d), but should be of size (%d,%d)",
+                varray[3]->length(), varray[3]->width(),
+                nneighbors, n);
+
     if(mu_nneighbors < 0) mu_nneighbors = nneighbors;
-    use_noise = (varray.length() == 6);
+    if(mu_nneighbors > nneighbors)
+        PLERROR("In NllGeneralGaussianVariable::build_(): mu_nneighbors "
+            "cannot be > than number of provided neighbors");
 
-    F = varray[0]->matValue;
-    mu = varray[1]->value;
+    F = varray[0]->value.toMat(ncomponents,n);
+    if(use_mu) mu = varray[1]->value;
     sn = varray[2]->value;
-    diff_y_x = varray[3]->matValue;
-    if(use_noise)
-    {
-        noise_var = varray[4]->value;
-        mu_noisy = varray[5]->value;
-    }
+    input = varray[3]->value;
+    neighbors = varray[4]->matValue;
+
+    diff_neighbor_input.resize(n);
     z.resize(nneighbors,n);
     U.resize(ncomponents,n);
     Ut.resize(n,n);
     V.resize(ncomponents,ncomponents);
     inv_Sigma_F.resize(ncomponents,n);
     inv_Sigma_z.resize(nneighbors,n);
-    if(use_noise) 
-    {
-        inv_Sigma_z_noisy.resize(nneighbors,n);
-        zj_noisy.resize(n);
-    } 
     temp_ncomp.resize(ncomponents);
 }
 
 
 void NllGeneralGaussianVariable::recomputeSize(int& len, int& wid) const
 {
-    len = varray[3]->length();
+    len = varray[4]->length();
     wid = 1;
 }
 
@@ -157,22 +168,29 @@
     {
         sm_svd[k] = mypow(S[k],2);
         U(k) << Ut(k);
-    }  
+    }
 
     real mahal = 0;
     real norm_term = 0;
     real dotp = 0;
     real coef = 0;
     inv_Sigma_z.clear();
-    if(use_noise) inv_Sigma_z_noisy.clear();
     tr_inv_Sigma = 0;
     for(int j=0; j<nneighbors;j++)
     {
         zj = z(j);
-        substract(diff_y_x(j),mu,zj); // z = y - x - mu(x)
+        if(use_mu)
+        {
+            substract(neighbors(j),input,diff_neighbor_input); 
+            substract(diff_neighbor_input,mu,zj); 
+        }
+        else
+        {
+            substract(neighbors(j),input,zj); 
+        }
       
         mahal = -0.5*pownorm(zj)/sn[0];      
-        norm_term = - n/2.0 * Log2Pi - 0.5*(n-ncomponents)*log(sn[0]);
+        norm_term = - n/2.0 * Log2Pi - 0.5*(n-ncomponents)*pl_log(sn[0]);
 
         inv_sigma_zj = inv_Sigma_z(j);
         inv_sigma_zj << zj; 
@@ -186,32 +204,14 @@
             uk = U(k);
             dotp = dot(zj,uk);
             coef = (1.0/(sm_svd[k]+sn[0]) - 1.0/sn[0]);
-            //inv_sigma_zj += dotp*coef*uk;
             multiplyAcc(inv_sigma_zj,uk,dotp*coef);
             mahal -= square(dotp)*0.5*coef;
-            norm_term -= 0.5*log(sm_svd[k]);
+            norm_term -= 0.5*pl_log(sm_svd[k]);
             if(j==0)
-                tr_inv_Sigma += coef;//*pownorm(uk,2);
-        }      
+                tr_inv_Sigma += coef;
+        }
 
         value[j] = -1*(norm_term + mahal);
-
-        if(use_noise)
-        {
-            substract(zj,noise_var,zj_noisy);
-        
-            inv_sigma_zj_noisy = inv_Sigma_z_noisy(j);
-            inv_sigma_zj_noisy << zj_noisy; 
-            inv_sigma_zj_noisy /= sn[0];
-
-            for(int k=0; k<ncomponents; k++)
-            { 
-                uk = U(k);
-                dotp = dot(zj_noisy,uk);
-                coef = (1.0/(sm_svd[k]+sn[0]) - 1.0/sn[0]);
-                multiplyAcc(inv_sigma_zj_noisy,uk,dotp*coef);
-            }      
-        }
     }
 
     inv_Sigma_F.clear();
@@ -224,13 +224,36 @@
         for(int k2=0; k2<ncomponents;k2++)
         {
             uk2 = U(k2);
-            //inv_sigma_fk += (1.0/(sm_svd[k2]+sn[0]) - 1.0/sn[0])*dot(fk,uk2)*uk2; 
-            multiplyAcc(inv_sigma_fk,uk2,(1.0/(sm_svd[k2]+sn[0]) - 1.0/sn[0])*dot(fk,uk2));
+            multiplyAcc(inv_sigma_fk,uk2,
+                        (1.0/(sm_svd[k2]+sn[0]) - 1.0/sn[0])*dot(fk,uk2));
         }
     }
 }
 
+// grad_F += alpa ( M - v1 v2')
+void NllGeneralGaussianVariable::bprop_to_bases(const Mat& R, const Mat& M, 
+                                                const Vec& v1, 
+                                                const Vec& v2, real alpha)
+{
+#ifdef BOUNDCHECK
+    if (M.length() != R.length() || M.width() != R.width() 
+        || v1.length()!=M.length() || M.width()!=v2.length() )
+        PLERROR("NllGeneralGaussianVariable::bprop_to_bases(): incompatible "
+                "arguments' sizes");
+#endif
 
+    const real* v_1=v1.data();
+    const real* v_2=v2.data();
+    for (int i=0;i<M.length();i++)
+    {
+        real* mi = M[i];
+        real* ri = R[i];
+        real v1i = v_1[i];
+        for (int j=0;j<M.width();j++)
+            ri[j] += alpha*(mi[j] - v1i * v_2[j]);
+    }
+}
+
 void NllGeneralGaussianVariable::bprop()
 {
     real coef = exp(-log_L);
@@ -239,21 +262,22 @@
         // dNLL/dF
 
         product(temp_ncomp,F,inv_Sigma_z(neighbor));
-        my_weird_product(varray[0]->matGradient,inv_Sigma_F,temp_ncomp,inv_Sigma_z(neighbor),gradient[neighbor]*coef);
+        bprop_to_bases(varray[0]->matGradient,inv_Sigma_F,
+                         temp_ncomp,inv_Sigma_z(neighbor),
+                         gradient[neighbor]*coef);
 
-        if(neighbor < mu_nneighbors)
+        if(use_mu && neighbor < mu_nneighbors)
         {
             // dNLL/dmu
 
-            multiplyAcc(varray[1]->gradient, inv_Sigma_z(neighbor),-1.0*gradient[neighbor] *coef) ;
-        
-            if(use_noise)
-                multiplyAcc(varray[5]->gradient, inv_Sigma_z_noisy(neighbor),-1.0*gradient[neighbor] *coef) ;
+            multiplyAcc(varray[1]->gradient, inv_Sigma_z(neighbor),
+                        -1.0*gradient[neighbor] *coef) ;
         }
 
         // dNLL/dsn
 
-        varray[2]->gradient[0] += gradient[neighbor]*coef* 0.5*(tr_inv_Sigma - pownorm(inv_Sigma_z(neighbor)));
+        varray[2]->gradient[0] += gradient[neighbor]*coef* 
+            0.5*(tr_inv_Sigma - pownorm(inv_Sigma_z(neighbor)));
       
     }
 }
@@ -261,9 +285,37 @@
 
 void NllGeneralGaussianVariable::symbolicBprop()
 {
-    PLERROR("Not implemented");
+    PLERROR("In NllGeneralGaussianVariable::symbolicBprop(): Not implemented");
 }
 
+void NllGeneralGaussianVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    NaryVariable::makeDeepCopyFromShallowCopy(copies);
+    
+    deepCopyField(input, copies);
+    deepCopyField(neighbors, copies);
+    deepCopyField(diff_neighbor_input, copies);
+    deepCopyField(mu, copies);
+    deepCopyField(sm_svd, copies);
+    deepCopyField(sn, copies);
+    deepCopyField(S, copies);
+    deepCopyField(uk, copies);
+    deepCopyField(fk, copies);
+    deepCopyField(uk2, copies);
+    deepCopyField(inv_sigma_zj, copies);
+    deepCopyField(zj, copies);
+    deepCopyField(inv_sigma_fk, copies);
+    deepCopyField(temp_ncomp, copies);
+    deepCopyField(F, copies);
+    deepCopyField(F_copy, copies);
+    deepCopyField(z, copies);
+    deepCopyField(U, copies);
+    deepCopyField(Ut, copies);
+    deepCopyField(V, copies);
+    deepCopyField(inv_Sigma_F, copies);
+    deepCopyField(inv_Sigma_z, copies);
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/var/NllGeneralGaussianVariable.h
===================================================================
--- trunk/plearn/var/NllGeneralGaussianVariable.h	2007-10-10 22:15:21 UTC (rev 8168)
+++ trunk/plearn/var/NllGeneralGaussianVariable.h	2007-10-10 22:27:34 UTC (rev 8169)
@@ -47,23 +47,38 @@
 {
     typedef NaryVariable inherited;
   
-public:
-    bool use_noise;
-    int n; // dimension of the vectors
-    real min_diff; // minimum difference between sigma_noise and the sigma_manifolds
-    real log_L; 
-    int ncomponents; // nb of vectors in f
-    int nneighbors; // nb of neighbors
-    int mu_nneighbors; // nb of neighbors to learn mu
+protected: 
+    //! Dimensionality of the input vectors
+    int n; 
+    //! Number of components (i.e. number of vectors in f)
+    int ncomponents; 
+    //! Number of nearest neighbors
+    int nneighbors;
+    //! Trace of the inverse of the covariance matrix
     real tr_inv_Sigma;
-    Vec mu, sm_svd, sn, S,mu_noisy,noise_var;
-    Vec uk, fk, uk2, inv_sigma_zj,inv_sigma_zj_noisy, zj,zj_noisy, inv_sigma_fk;
+
+    //! Temporary storage variables
+    Vec input, diff_neighbor_input, mu, sm_svd, sn, S;
+    Vec uk, fk, uk2, inv_sigma_zj, zj, inv_sigma_fk;
     Vec temp_ncomp;
-    Mat F,F_copy, diff_y_x, z, U, Ut, V, inv_Sigma_F,inv_Sigma_z, inv_Sigma_z_noisy;
+    Mat neighbors, F,F_copy, z, U, Ut, V, inv_Sigma_F, inv_Sigma_z;
 
+public:
+    //! Log of number of components L
+    real log_L; 
+    //! Indication that a parameter corresponding to the difference
+    //! between the Gaussian center and the input data point position
+    //! should be used.
+    bool use_mu;
+    //! Number of nearest neighbors to learn mu,
+    //! which must be < then nneighbors
+    int mu_nneighbors; 
+
+   
     //!  Default constructor for persistence
     NllGeneralGaussianVariable() {}
-    NllGeneralGaussianVariable(const VarArray& the_varray, real thelogL, int mu_nneighbors);
+    NllGeneralGaussianVariable(const VarArray& the_varray, real thelogL, 
+                               bool use_mu, int mu_nneighbors);
 
     PLEARN_DECLARE_OBJECT(NllGeneralGaussianVariable);
 
@@ -73,20 +88,25 @@
     virtual void fprop();
     virtual void bprop();
     virtual void symbolicBprop();
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
 protected:
     void build_();
+
+private:
+
+    void bprop_to_bases(const Mat& R, const Mat& M, const Vec& v1, 
+                        const Vec& v2,real alpha);
+
 };
 
 DECLARE_OBJECT_PTR(NllGeneralGaussianVariable);
 
-inline Var nll_general_gaussian(Var tangent_plane_var, Var mu_var, Var sn_var, Var neighbors_dist_var, 
-                                real log_L, int mu_nneighbors, Var noise_var=0, Var mu_noisy=0)
+inline Var nll_general_gaussian(Var tangent_plane_var, Var mu_var, Var sn_var, 
+                                Var neighbors_var, 
+                                real log_L, bool use_mu, int mu_nneighbors)
 {
-    if(!noise_var)
-        return new NllGeneralGaussianVariable(tangent_plane_var & mu_var & sn_var & neighbors_dist_var,log_L, mu_nneighbors);
-    else
-        return new NllGeneralGaussianVariable(tangent_plane_var & mu_var & sn_var & neighbors_dist_var & noise_var & mu_noisy,log_L, mu_nneighbors);
+    return new NllGeneralGaussianVariable(tangent_plane_var & mu_var & sn_var & neighbors_var,log_L, use_mu, mu_nneighbors);
 }
 
                             



From larocheh at mail.berlios.de  Thu Oct 11 00:30:12 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:30:12 +0200
Subject: [Plearn-commits] r8170 - trunk/plearn/math
Message-ID: <200710102230.l9AMUCBR028241@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:30:11 +0200 (Thu, 11 Oct 2007)
New Revision: 8170

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
Corrected an error message.


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-10-10 22:27:34 UTC (rev 8169)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-10-10 22:30:11 UTC (rev 8170)
@@ -6661,7 +6661,7 @@
     int w=src.width();
 #ifdef BOUNDCHECK
     if (w!=dest.length() || l!=dest.width())
-        PLERROR("tranpose(TMat<T>(%d,%d),T,TMat<T>(%d,%d)) args of unequal dimensions",
+        PLERROR("transpose(TMat<T>(%d,%d),T,TMat<T>(%d,%d)) args of unequal dimensions",
                 src.length(),src.width(),dest.length(),dest.width());
 #endif
     int dmod=dest.mod();



From larocheh at mail.berlios.de  Thu Oct 11 00:33:16 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:33:16 +0200
Subject: [Plearn-commits] r8171 - trunk/plearn/vmat
Message-ID: <200710102233.l9AMXGKa003551@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:33:15 +0200 (Thu, 11 Oct 2007)
New Revision: 8171

Modified:
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
   trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
Log:
Added an option to specify a shared shift and scale for all inputs.


Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2007-10-10 22:30:11 UTC (rev 8170)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.cc	2007-10-10 22:33:15 UTC (rev 8171)
@@ -60,6 +60,8 @@
 
 ShiftAndRescaleVMatrix::ShiftAndRescaleVMatrix(bool call_build_)
     : inherited(call_build_),
+      shared_shift(0),
+      shared_scale(1),
       automatic(1),
       n_train(0),
       n_inputs(-1),
@@ -171,6 +173,18 @@
                   "Quantity multiplied to each shifted element of a row of the"
                   " source vmatrix.");
 
+    declareOption(ol, "shared_shift", &ShiftAndRescaleVMatrix::shared_shift,
+                  OptionBase::buildoption,
+                  "Quantity added to ALL INPUT elements of a row of the source\n"
+                  "vmatrix. This option is ignored if 'shift' is provided\n"
+                  "or 'automatic' is true.\n");
+
+    declareOption(ol, "shared_scale", &ShiftAndRescaleVMatrix::shared_scale,
+                  OptionBase::buildoption,
+                  "Quantity multiplied to ALL shifted INPUT elements of a row of the"
+                  "source vmatrix. This option is ignored if 'scale' is provided,\n"
+                  "'automatic' is true or 'no_scale' is true.\n");
+
     declareOption(ol, "automatic", &ShiftAndRescaleVMatrix::automatic,
                   OptionBase::buildoption,
                   "Whether shift and scale are determined from the mean and"
@@ -301,6 +315,33 @@
                 }
 
             }
+            else
+            {
+                n_inputs = source->inputsize();
+                if (n_inputs<0)
+                {
+                    n_inputs = source->inputsize();
+                    if (n_inputs<0)
+                        PLERROR("ShiftAndRescaleVMatrix: either n_inputs should be"
+                                " provided explicitly\n"
+                                "or the source VMatrix should have a set value of"
+                                " inputsize.\n");
+                }
+                
+                if( shift.length() == 0)
+                {
+                    shift.resize(source->width()) ; 
+                    shift.subVec(n_inputs, shift.length()-n_inputs).fill(0);
+                    shift.subVec(0,n_inputs).fill(shared_shift);
+                }
+                
+                if( scale.length() == 0)
+                {
+                    scale.resize(source->width()) ; 
+                    scale.subVec(n_inputs, scale.length()-n_inputs).fill(1);
+                    scale.subVec(0,n_inputs).fill(shared_scale);
+                }
+            }
         }
         reset_dimensions();
         setMetaInfoFromSource();

Modified: trunk/plearn/vmat/ShiftAndRescaleVMatrix.h
===================================================================
--- trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2007-10-10 22:30:11 UTC (rev 8170)
+++ trunk/plearn/vmat/ShiftAndRescaleVMatrix.h	2007-10-10 22:33:15 UTC (rev 8171)
@@ -68,6 +68,9 @@
     Vec scale;
     Vec min_max;
 
+    real shared_shift;
+    real shared_scale;
+
     //! find shift and scale automatically?
     bool automatic;
 



From larocheh at mail.berlios.de  Thu Oct 11 00:41:34 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:41:34 +0200
Subject: [Plearn-commits] r8172 - trunk/plearn_learners/distributions
Message-ID: <200710102241.l9AMfYYM025208@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:41:33 +0200 (Thu, 11 Oct 2007)
New Revision: 8172

Modified:
   trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc
   trunk/plearn_learners/distributions/NonLocalManifoldParzen.h
Log:


Modified: trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc
===================================================================
--- trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc	2007-10-10 22:33:15 UTC (rev 8171)
+++ trunk/plearn_learners/distributions/NonLocalManifoldParzen.cc	2007-10-10 22:41:33 UTC (rev 8172)
@@ -43,263 +43,192 @@
 
 #include "NonLocalManifoldParzen.h"
 #include <plearn/display/DisplayUtils.h>
-#include <plearn/vmat/LocalNeighborsDifferencesVMatrix.h>
-//#include <plearn/vmat/RandomNeighborsDifferencesVMatrix.h>
-#include <plearn/var/ProductVariable.h>
-#include <plearn/var/PlusVariable.h>
-#include <plearn/var/SoftplusVariable.h>
-#include <plearn/var/SumAbsVariable.h>
-#include <plearn/var/SumSquareVariable.h>
-#include <plearn/var/VarRowVariable.h>
-#include <plearn/var/SourceVariable.h>
-#include <plearn/var/Var_operators.h>
-//#include <plearn/var/DiagonalGaussianVariable.h>
-#include <plearn/vmat/ConcatColumnsVMatrix.h>
-#include <plearn/var/SumOfVariable.h>
-#include <plearn/var/RowOfVariable.h>
-//#include <plearn/var/RowPowNormVariable.h>
-#include <plearn/var/SumVariable.h>
-#include <plearn/var/TanhVariable.h>
-#include <plearn/var/NllGeneralGaussianVariable.h>
-#include <plearn/var/DiagonalizedFactorsProductVariable.h>
 #include <plearn/math/plapack.h>
+#include <plearn/var/AffineTransformVariable.h>
+#include <plearn/var/AffineTransformWeightPenalty.h>
 #include <plearn/var/ColumnSumVariable.h>
-#include <plearn/vmat/VMat_basic_stats.h>
-#include <plearn/vmat/ConcatRowsVMatrix.h>
-#include <plearn/vmat/SubVMatrix.h>
-#include <plearn/var/PDistributionVariable.h>
-#include <plearn_learners/distributions/UniformDistribution.h>
-#include <plearn_learners/distributions/GaussianDistribution.h>
-#include <plearn/display/DisplayUtils.h>
-#include <plearn/opt/GradientOptimizer.h>
-#include <plearn/var/TransposeVariable.h>
-#include <plearn/var/Var_utils.h>
-#include <plearn/var/ConcatRowsVariable.h>
-#include <plearn/var/RowSumVariable.h>
-#include <plearn/var/ThresholdBpropVariable.h>
+#include <plearn/var/NllGeneralGaussianVariable.h>
 #include <plearn/var/NoBpropVariable.h>
 #include <plearn/var/ReshapeVariable.h>
+#include <plearn/var/SourceVariable.h>
 #include <plearn/var/SquareVariable.h>
-#include <plearn/var/ExpVariable.h>
-#include <plearn/io/load_and_save.h>
-#include <plearn/vmat/VMat_computeNearestNeighbors.h>
-#include <plearn/vmat/FractionSplitter.h>
-#include <plearn/vmat/RepeatSplitter.h>
-#include <plearn/var/FNetLayerVariable.h>
-#include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn/var/SumOfVariable.h>
+#include <plearn/var/TanhVariable.h>
+#include <plearn/var/ThresholdBpropVariable.h>
+#include <plearn/var/Var_operators.h>
+#include <plearn/vmat/AppendNeighborsVMatrix.h>
+#include <plearn/vmat/ConcatColumnsVMatrix.h>
 
+
 namespace PLearn {
 using namespace std;
 
 
 NonLocalManifoldParzen::NonLocalManifoldParzen()
-    :  //weight_embedding(1),
-    curpos(0),
-    weight_decay(0), penalty_type("L2_square"),
-//noise_grad_factor(0.01),noise(0), noise_type("gaussian"), omit_last(0),
-learn_mu(true),
-//magnified_version(false),
-reference_set(0), sigma_init(0.1), sigma_min(0.00001), nneighbors(5), nneighbors_density(-1), mu_nneighbors(2), ncomponents(1), sigma_threshold_factor(1), variances_transfer_function("softplus"), architecture_type("single_neural_network"),
-    n_hidden_units(-1), batch_size(1), svd_threshold(1e-8), rw_n_step(1000), rw_size_step(0.01), rw_ith_component(0), rw_file_name("random_walk_"), rw_save_every(100), store_prediction(false), optstage_per_lstage(-1), save_every(-1)
+    :  
+    reference_set(0), 
+    ncomponents(1), 
+    nneighbors(5), 
+    nneighbors_density(-1), 
+    store_prediction(false),
+    learn_mu(false),
+    sigma_init(0.1), 
+    sigma_min(0.00001), 
+    mu_nneighbors(2), 
+    sigma_threshold_factor(-1), 
+    svd_threshold(1e-8), 
+    nhidden(10), 
+    weight_decay(0),
+    penalty_type("L2_square"),
+    batch_size(1)
 {
 }
 
-PLEARN_IMPLEMENT_OBJECT(NonLocalManifoldParzen, "Non-Local version of Manifold Parzen Windows",
-                        "Manifold Parzen Windows density model, where the parameters of\n"
-                        "the gaussians in the mixture are predicted by a neural network."
+PLEARN_IMPLEMENT_OBJECT(NonLocalManifoldParzen, 
+                        "Non-Local version of Manifold Parzen Windows",
+                        "Manifold Parzen Windows density model, where the\n"
+                        "parameters of the kernel for each training point\n"
+                        "are predicted by a neural network.\n"
     );
 
 
 void NonLocalManifoldParzen::declareOptions(OptionList& ol)
 {
 
-//  declareOption(ol, "weight_embedding", &NonLocalManifoldParzen::weight_embedding, OptionBase::buildoption,
-//                "Embedding penalty weight\n");
-
-    declareOption(ol, "weight_decay", &NonLocalManifoldParzen::weight_decay, OptionBase::buildoption,
-                  "Global weight decay for all layers\n");
-
-    declareOption(ol, "penalty_type", &NonLocalManifoldParzen::penalty_type,
-                  OptionBase::buildoption,
-                  "Penalty to use on the weights (for weight and bias decay).\n"
-                  "Can be any of:\n"
-                  "  - \"L1\": L1 norm,\n"
-                  "  - \"L1_square\": square of the L1 norm,\n"
-                  "  - \"L2_square\" (default): square of the L2 norm.\n");
-
-//  declareOption(ol, "omit_last", &NonLocalManifoldParzen::omit_last, OptionBase::buildoption,
-//		"Number of training examples at the end of trainin set to ignore in the training.\n"
-//		);
-
-    declareOption(ol, "learn_mu", &NonLocalManifoldParzen::learn_mu, OptionBase::buildoption,
-                  "Indication that mu should be learned.\n"
+    declareOption(ol, "parameters", &NonLocalManifoldParzen::parameters, 
+                  OptionBase::learntoption,
+                  "Parameters of the tangent_predictor function.\n"
         );
 
-    declareOption(ol, "nneighbors", &NonLocalManifoldParzen::nneighbors, OptionBase::buildoption,
-                  "Number of nearest neighbors to consider for gradient descent.\n"
+    declareOption(ol, "reference_set", &NonLocalManifoldParzen::reference_set, 
+                  OptionBase::learntoption,
+                  "Reference points for density computation.\n"
         );
 
-    declareOption(ol, "nneighbors_density", &NonLocalManifoldParzen::nneighbors_density, OptionBase::buildoption,
-                  "Number of nearest neighbors to consider for p(x) density estimation.\n"
+    declareOption(ol, "ncomponents", &NonLocalManifoldParzen::ncomponents, 
+                  OptionBase::buildoption,
+                  "Number of \"principal components\" to predict\n"
+                  "for kernel parameters prediction.\n"
         );
 
-    declareOption(ol, "mu_nneighbors", &NonLocalManifoldParzen::mu_nneighbors, OptionBase::buildoption,
-                  "Number of nearest neighbors to learn the mus (if < 0, mu_nneighbors = nneighbors).\n"
+    declareOption(ol, "nneighbors", &NonLocalManifoldParzen::nneighbors, 
+                  OptionBase::buildoption,
+                  "Number of nearest neighbors to consider in training procedure.\n"
         );
 
-    declareOption(ol, "ncomponents", &NonLocalManifoldParzen::ncomponents, OptionBase::buildoption,
-                  "Number of tangent vectors to predict.\n"
+    declareOption(ol, "nneighbors_density", 
+                  &NonLocalManifoldParzen::nneighbors_density, 
+                  OptionBase::buildoption,
+                  "Number of nearest neighbors to consider for\n"
+                  "p(x) density estimation.\n"
         );
 
-    declareOption(ol, "sigma_threshold_factor", &NonLocalManifoldParzen::sigma_threshold_factor, OptionBase::buildoption,
-                  "Threshold factor of the gradient on the sigma noise. \n"
-        );
-
-    declareOption(ol, "optimizer", &NonLocalManifoldParzen::optimizer, OptionBase::buildoption,
-                  "Optimizer that optimizes the cost function.\n"
-        );
-		
-    declareOption(ol, "variances_transfer_function", &NonLocalManifoldParzen::variances_transfer_function,
+    declareOption(ol, "store_prediction", 
+                  &NonLocalManifoldParzen::store_prediction, 
                   OptionBase::buildoption,
-                  "Type of output transfer function for predicted variances, to force them to be >0:\n"
-                  "  square : take the square\n"
-                  "  exp : apply the exponential\n"
-                  "  softplus : apply the function log(1+exp(.))\n"
+                  "Indication that the predicted parameters should be stored.\n"
+                  "This may make testing faster. Note that the predictions are\n"
+                  "stored after the last training stage\n"
         );
-		
-    declareOption(ol, "architecture_type", &NonLocalManifoldParzen::architecture_type, OptionBase::buildoption,
-                  "For pre-defined tangent_predictor types: \n"
-                  "   single_neural_network : prediction = b + W*tanh(c + V*x), where W has n_hidden_units columns\n"
-                  "                          where the resulting vector is viewed as a ncomponents by n matrix\n"
-                  "   embedding_neural_network: prediction[k,i] = d(e[k])/d(x[i), where e(x) is an ordinary neural\n"
-                  "                             network representing the embedding function (see output_type option)\n"
-                  "where (b,W,c,V) are parameters to be optimized.\n"
-        );
 
-    declareOption(ol, "n_hidden_units", &NonLocalManifoldParzen::n_hidden_units, OptionBase::buildoption,
-                  "Number of hidden units (if architecture_type is some kind of neural network)\n"
+
+    declareOption(ol, "paramsvalues", 
+                  &NonLocalManifoldParzen::paramsvalues, 
+                  OptionBase::learntoption,
+                  "The learned parameter vector.\n"
         );
 
-    declareOption(ol, "hidden_layer", &NonLocalManifoldParzen::hidden_layer, OptionBase::buildoption,
-                  "A user-specified NAry Var that computes the output of the first hidden layer\n"
-                  "from the network input vector and a set of parameters. Its first argument should\n"
-                  "be the network input and the remaining arguments the tunable parameters.\n");
+    // ** Gaussian kernel options
 
-    declareOption(ol, "batch_size", &NonLocalManifoldParzen::batch_size, OptionBase::buildoption,
-                  "    how many samples to use to estimate the average gradient before updating the weights\n"
-                  "    0 is equivalent to specifying training_set->length() \n");
-
-    declareOption(ol, "svd_threshold", &NonLocalManifoldParzen::svd_threshold, OptionBase::buildoption,
-                  "Threshold to accept singular values of F in solving for linear combination weights on tangent subspace.\n"
+    declareOption(ol, "learn_mu", &NonLocalManifoldParzen::learn_mu, 
+                  OptionBase::buildoption,
+                  "Indication that the deviation from the training point\n"
+                  "in a Gaussian kernel (called mu) should be learned.\n"
         );
 
-    declareOption(ol, "parameters", &NonLocalManifoldParzen::parameters, OptionBase::learntoption,
-                  "Parameters of the tangent_predictor function.\n"
+    declareOption(ol, "sigma_init", &NonLocalManifoldParzen::sigma_init, 
+                  OptionBase::buildoption,
+                  "Initial minimum value for sigma noise.\n"
         );
 
-    declareOption(ol, "shared_parameters", &NonLocalManifoldParzen::shared_parameters, OptionBase::buildoption,
-                  "Parameters of another NonLocalManifoldParzen estimator to share with this current object.\n"
+    declareOption(ol, "sigma_min", &NonLocalManifoldParzen::sigma_min, 
+                  OptionBase::buildoption,
+                  "The minimum value for sigma noise.\n"
         );
 
-    declareOption(ol, "L", &NonLocalManifoldParzen::L, OptionBase::learntoption,
-                  "Number of gaussians.\n"
+    declareOption(ol, "mu_nneighbors", &NonLocalManifoldParzen::mu_nneighbors, 
+                  OptionBase::buildoption,
+                  "Number of nearest neighbors to learn the mus \n"
+                  "(if < 0, mu_nneighbors = nneighbors).\n"
         );
 
-//  declareOption(ol, "Us", &NonLocalManifoldParzen::Us, OptionBase::learntoption,
-//		"The U matrices for the reference set.\n"
-//		);
-
-    declareOption(ol, "mus", &NonLocalManifoldParzen::mus, OptionBase::learntoption,
-                  "The mu vectors for the reference set.\n"
+    declareOption(ol, "sigma_threshold_factor", 
+                  &NonLocalManifoldParzen::sigma_threshold_factor, 
+                  OptionBase::buildoption,
+                  "Threshold factor of the gradient on the sigma noise\n"
+                  "parameter of the Gaussian kernel. If < 0, then\n"
+                  "no threshold is used."
         );
 
-//  declareOption(ol, "sms", &NonLocalManifoldParzen::sms, OptionBase::learntoption,
-//		"The sm values for the reference set.\n"
-//                );
-
-    declareOption(ol, "sns", &NonLocalManifoldParzen::sns, OptionBase::learntoption,
-                  "The sn values for the reference set.\n"
+    declareOption(ol, "svd_threshold", 
+                  &NonLocalManifoldParzen::svd_threshold, OptionBase::buildoption,
+                  "Threshold to accept singular values of F in solving for\n"
+                  "linear combination weights on tangent subspace.\n"
         );
 
-    declareOption(ol, "sms", &NonLocalManifoldParzen::sms, OptionBase::learntoption,
-                  "The sm values for the reference set.\n"
-        );
+    // ** Neural network predictor **
 
-    declareOption(ol, "Fs", &NonLocalManifoldParzen::Fs, OptionBase::learntoption,
-                  "The F values for the reference set.\n"
+    declareOption(ol, "nhidden", 
+                  &NonLocalManifoldParzen::nhidden, OptionBase::buildoption,
+                  "Number of hidden units of the neural network.\n"
         );
 
-    declareOption(ol, "sigma_min", &NonLocalManifoldParzen::sigma_min, OptionBase::buildoption,
-                  "The minimum value for sigma noise.\n"
-        );
+    declareOption(ol, "weight_decay", &NonLocalManifoldParzen::weight_decay, 
+                  OptionBase::buildoption,
+                  "Global weight decay for all layers.\n");
 
-    declareOption(ol, "sigma_init", &NonLocalManifoldParzen::sigma_init, OptionBase::buildoption,
-                  "Initial minimum value for sigma noise.\n"
-        );
+    declareOption(ol, "penalty_type", &NonLocalManifoldParzen::penalty_type,
+                  OptionBase::buildoption,
+                  "Penalty to use on the weights (for weight and bias decay).\n"
+                  "Can be any of:\n"
+                  "  - \"L1\": L1 norm,\n"
+                  "  - \"L2_square\" (default): square of the L2 norm.\n");
 
-    declareOption(ol, "rw_n_step", &NonLocalManifoldParzen::rw_n_step, OptionBase::buildoption,
-                  "Number of steps in the random walk (for compute output).\n"
+    declareOption(ol, "optimizer", &NonLocalManifoldParzen::optimizer, 
+                  OptionBase::buildoption,
+                  "Optimizer that optimizes the cost function.\n"
         );
 
-    declareOption(ol, "rw_size_step", &NonLocalManifoldParzen::rw_size_step, OptionBase::buildoption,
-                  "Size of the steps in the random walk (for compute output).\n"
-        );
+    declareOption(ol, "batch_size", 
+                  &NonLocalManifoldParzen::batch_size, OptionBase::buildoption,
+                  "How many samples to use to estimate the average gradient\n"
+                  "before updating the weights. If <= 0, is equivalent to\n"
+                  "specifying training_set->length() \n");
 
-    declareOption(ol, "rw_ith_component", &NonLocalManifoldParzen::rw_ith_component, OptionBase::buildoption,
-                  "Which principal component to follow.\n"
-        );
 
-    declareOption(ol, "rw_save_every", &NonLocalManifoldParzen::rw_save_every, OptionBase::buildoption,
-                  "Number of iterations between savings of random walk results.\n"
-        );
-    declareOption(ol, "rw_file_name", &NonLocalManifoldParzen::rw_file_name, OptionBase::buildoption,
-                  "File name for the random walk saves.\n"
-        );
+    // ** Stored outputs of neural network
 
-    declareOption(ol, "store_prediction", &NonLocalManifoldParzen::store_prediction, OptionBase::buildoption,
-                  "Indication that the predicted parameters should be stored.\n"
-                  "This may make testing faster. Note that the predictions are\n"
-                  "stored after the last training stage, so if the predictor is\n"
-                  "modified later on (e.g. if the parameters of the predictor are shared), then\n"
-                  "this option might give different testing results.\n"
+    declareOption(ol, "mus", 
+                  &NonLocalManifoldParzen::mus, OptionBase::learntoption,
+                  "The stored mu vectors for the reference set.\n"
         );
 
-    declareOption(ol, "optstage_per_lstage", &NonLocalManifoldParzen::optstage_per_lstage, OptionBase::buildoption,
-                  "Number of optimizer stages. If < 0, then it is determined as a function of\n"
-                  "the training set length and of the batch size.\n"
+    declareOption(ol, "sns", &NonLocalManifoldParzen::sns, 
+                  OptionBase::learntoption,
+                  "The stored sigma noise values for the reference set.\n"
         );
 
-    declareOption(ol, "save_every", &NonLocalManifoldParzen::save_every, OptionBase::buildoption,
-                  "Number of iterations since the last save after which the parameters must be saved.\n"
-                  "If < 0, then the parameters are saved at the end of train().\n"
+    declareOption(ol, "sms", &NonLocalManifoldParzen::sms, 
+                  OptionBase::learntoption,
+                  "The stored sigma manifold values for the reference set.\n"
         );
 
-
-//  declareOption(ol, "noise", &NonLocalManifoldParzen::noise, OptionBase::buildoption,
-//		"Noise parameter for the training data. For uniform noise, this gives the half the length \n" "of the uniform window (centered around the origin), and for gaussian noise, this gives the variance of the noise in all directions.\n"
-//                );
-
-//  declareOption(ol, "noise_type", &NonLocalManifoldParzen::noise_type, OptionBase::buildoption,
-//		"Type of the noise (\"uniform\" or \"gaussian\").\n"
-//                );
-
-//  declareOption(ol, "noise_grad_factor", &NonLocalManifoldParzen::noise_grad_factor, OptionBase::buildoption,
-//		"Gradient factor to apply to the noise signal error.\n"
-//                );
-
-//  declareOption(ol, "magnified_version", &NonLocalManifoldParzen::magnified_version, OptionBase::buildoption,
-//		"Indication that, when computing the log density, the magnified estimation should be used.\n"
-//                );
-
-    declareOption(ol, "reference_set", &NonLocalManifoldParzen::reference_set, OptionBase::learntoption,
-                  "Reference points for density computation.\n"
+    declareOption(ol, "Fs", &NonLocalManifoldParzen::Fs, OptionBase::learntoption,
+                  "The storaged \"principal components\" (F) values for\n"
+                  "the reference set.\n"
         );
 
-    declareOption(ol, "curpos", &NonLocalManifoldParzen::curpos, OptionBase::learntoption,
-                  "Position of the current example in the training set.\n"
-        );
 
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -307,356 +236,149 @@
 void NonLocalManifoldParzen::build_()
 {
 
-    n = PLearner::inputsize_;
-
-    if (n>0)
+    if (inputsize_>0)
     {
+        if (nhidden <= 0) 
+            PLERROR("NonLocalManifoldParzen::Number of hidden units "
+                    "should be positive, now %d\n",nhidden);
 
-        VarArray params;
-
-        int sp_index = 0;
-
         Var log_n_examples(1,1,"log(n_examples)");
         if(train_set)
         {
             L = train_set->length();
-            reference_set = train_set; // Maybe things could be changed here to make access faster!
+            reference_set = train_set; 
         }
 
         log_L= pl_log((real) L);
+        parameters.resize(0);
+        
+        // Neural network prediction of principal components
 
-        {
+        x = Var(inputsize_);
+        x->setName("x");
 
-            x = Var(n);
-            Var a; // outputs of hidden layer
+        W = Var(nhidden+1,inputsize_,"W");
+        parameters.append(W);
 
-            if (hidden_layer) // user-specified hidden layer Var
-            {
-                if(shared_parameters.size() != 0)
-                    PLERROR("In NonLocalManifoldParzen:build_(): shared parameters is not implemented for user-specified hidden layer Var");
-                NaryVariable* layer_var = dynamic_cast<NaryVariable*>((Variable*)hidden_layer);
-                if (!layer_var)
-                    PLERROR("In NonLocalManifoldParzen::build - 'hidden_layer' should be "
-                            "from a subclass of NaryVariable");
-                if (layer_var->varray.size() < 1)
-                    layer_var->varray.resize(1);
-                layer_var->varray[0] = transpose(x);
-                layer_var->build(); // make sure everything is consistent and finish the build
-                if (layer_var->varray.size()<2)
-                    PLERROR("In NonLocalManifoldParzen::build - 'hidden_layer' should have parameters");
-                for (int i=1;i<layer_var->varray.size();i++)
-                    params.append(layer_var->varray[i]);
-                a = transpose(layer_var);
-                n_hidden_units = layer_var->width();
-            }
-            else // standard hidden layer
-            {
-                if (n_hidden_units <= 0)
-                    PLERROR("NonLocalManifoldParzen::Number of hidden units should be positive, now %d\n",n_hidden_units);
-                if(shared_parameters.size() != 0)
-                {
-                    c = shared_parameters[sp_index++];
-                    V = shared_parameters[sp_index++];
-                }
-                else
-                {
-                    c = Var(n_hidden_units,1,"c ");
-                    V = Var(n_hidden_units,n,"V ");
-                }
-                params.append(c);
-                params.append(V);
-                a = tanh(c + product(V,x));
-            }
+        Var a; // outputs of hidden layer
+        a = affine_transform(x,W);
+        a->setName("a");
 
-            if(shared_parameters != 0)
-            {
-                muV = shared_parameters[sp_index++];
-                snV = shared_parameters[sp_index++];
-                snb = shared_parameters[sp_index++];
-            }
-            else
-            {
-                muV = Var(n,n_hidden_units,"muV ");
-                snV = Var(1,n_hidden_units,"snV ");
-                snb = Var(1,1,"snB ");
-            }
-            params.append(muV);
-            params.append(snV);
-            params.append(snb);
+        V = Var(ncomponents*(inputsize_+1),nhidden,"V");
+        parameters.append(V);
 
-            if(architecture_type == "embedding_neural_network")
-            {
-                if(shared_parameters.size() != 0)
-                    W = shared_parameters[sp_index++];
-                else
-                    W = Var(ncomponents,n_hidden_units,"W ");
-                tangent_plane = diagonalized_factors_product(W,1-a*a,V);
-                embedding = product(W,a);
-                output_embedding = Func(x,embedding);
-                params.append(W);
-            }
-            else if(architecture_type == "single_neural_network")
-            {
-                if(shared_parameters.size() != 0)
-                {
-                    b = shared_parameters[sp_index++];
-                    W = shared_parameters[sp_index++];
-                }
-                else
-                {
-                    b = Var(ncomponents*n,1,"b");
-                    W = Var(ncomponents*n,n_hidden_units,"W ");
-                }
-                tangent_plane = reshape(b + product(W,a),ncomponents,n);
-                params.append(b);
-                params.append(W);
-            }
-            else
-                PLERROR("NonLocalManifoldParzen::build_, unknown architecture_type option %s",
-                        architecture_type.c_str());
-            if(learn_mu)
-                mu = product(muV,a);
-            else
-            {
-                mu = new SourceVariable(n,1);
-                mu->value.fill(0);
-                mu_nneighbors = 0;
-            }
-            min_sig = new SourceVariable(1,1);
-            min_sig->value[0] = sigma_min;
-            min_sig->setName("min_sig");
-            if(shared_parameters.size() != 0)
-                init_sig = shared_parameters[sp_index++];
-            else
-            {
-                init_sig = Var(1,1);
-                init_sig->setName("init_sig");
-            }
-            params.append(init_sig);
+        // TODO: instead, make NllGeneralGaussianVariable use vector... (DONE)
+        //components = reshape(affine_transform(V,a),ncomponents,n);
+        components = affine_transform(V,a);
+        components->setName("components");
 
-            if(variances_transfer_function == "softplus") sn = softplus(snb + product(snV,a))  + min_sig + softplus(init_sig);
-            else if(variances_transfer_function == "square") sn = square(snb + product(snV,a)) + min_sig + square(init_sig);
-            else if(variances_transfer_function == "exp") sn = exp(snb + product(snV,a)) + min_sig + exp(init_sig);
-            else PLERROR("In NonLocalManifoldParzen::build_ : unknown variances_transfer_function option %s ", variances_transfer_function.c_str());
+        // Gaussian kernel parameters prediction
 
+        muV = Var(inputsize_+1,nhidden,"muV");
+        snV = Var(2,nhidden,"snV");
+    
+        parameters.append(muV);
+        parameters.append(snV);
 
+        if(learn_mu)
+            mu = affine_transform(muV,a);
+        else
+        {
+            mu = new SourceVariable(inputsize_,1);
+            mu->value.clear();
+        }
+        mu->setName("mu");
 
-            if(sigma_threshold_factor > 0)
-            {
-                sn = threshold_bprop(sn,sigma_threshold_factor);
-            }
+        min_sig = new SourceVariable(1,1);
+        min_sig->value[0] = sigma_min;
+        min_sig->setName("min_sig");
+        init_sig = Var(1,1);
+        init_sig->setName("init_sig");
+        parameters.append(init_sig);
 
-            /*
-              if(noise > 0)
-              {
-              if(noise_type == "uniform")
-              {
-              PP<UniformDistribution> temp = new UniformDistribution();
-              Vec lower_noise(n);
-              Vec upper_noise(n);
-              for(int i=0; i<n; i++)
-              {
-              lower_noise[i] = -1*noise;
-              upper_noise[i] = noise;
-              }
-              temp->min = lower_noise;
-              temp->max = upper_noise;
-              dist = temp;
-              }
-              else if(noise_type == "gaussian")
-              {
-              PP<GaussianDistribution> temp = new GaussianDistribution();
-              Vec mu(n); mu.clear();
-              Vec eig_values(n);
-              Mat eig_vectors(n,n); eig_vectors.clear();
-              for(int i=0; i<n; i++)
-              {
-              eig_values[i] = noise; // maybe should be adjusted to the sigma noiseat the input
-              eig_vectors(i,i) = 1.0;
-              }
-              temp->mu = mu;
-              temp->eigenvalues = eig_values;
-              temp->eigenvectors = eig_vectors;
-              dist = temp;
-              }
-              else PLERROR("In GaussianContinuumDistribution::build_() : noise_type %c not defined",noise_type.c_str());
-              noise_var = new PDistributionVariable(x,dist);
-              for(int k=0; k<ncomponents; k++)
-              {
-              Var index_var = new SourceVariable(1,1);
-              index_var->value[0] = k;
-              Var f_k = new VarRowVariable(tangent_plane,index_var);
-              noise_var = noise_var - product(f_k,noise_var)* transpose(f_k)/pownorm(f_k,2);
-              }
+        sn = square(affine_transform(snV,a)) + min_sig + square(init_sig);
+        sn->setName("sn");
+        
+        if(sigma_threshold_factor > 0)
+            sn = threshold_bprop(sn,sigma_threshold_factor);
 
-              noise_var = no_bprop(noise_var);
-              noise_var->setName(noise_type);
-              }
-              else
-              {
-
-              noise_var = new SourceVariable(n,1);
-              noise_var->setName("no noise");
-              for(int i=0; i<n; i++)
-              noise_var->value[i] = 0;
-              }
-            */
-
-            // Path for noisy mu
-            //Var a_noisy = tanh(c + product(V,x+noise_var));
-            //mu_noisy = no_bprop(product(muV,a_noisy),noise_grad_factor);
-
-
-            tangent_plane->setName("tangent_plane ");
-            mu->setName("mu ");
-            sn->setName("sn ");
-            a->setName("a ");
-            if(architecture_type == "embedding_neural_network")
-                embedding->setName("embedding ");
-            x->setName("x ");
-
-            predictor = Func(x, params , tangent_plane & mu & sn );
-        }
-
-        if(shared_parameters.size() == 0)
-        {
-            if (parameters.size()>0 && parameters.nelems() == predictor->parameters.nelems())
-                predictor->parameters.copyValuesFrom(parameters);
-            parameters.resize(predictor->parameters.size());
-            for (int i=0;i<parameters.size();i++)
-                parameters[i] = predictor->parameters[i];
-        }
+        predictor = Func(x, parameters , components & mu & sn );
+    
         Var target_index = Var(1,1);
         target_index->setName("target_index");
         Var neighbor_indexes = Var(nneighbors,1);
         neighbor_indexes->setName("neighbor_indexes");
-        Var random_index = Var(1,1);
-        random_index->setName("neighbor_index");
-        /*
-        // The following variables are discarded to
-        // make the gradient computation faster
-        // and more stable in nlmp_general_gaussian
-        log_p_x = Var(L,1);
-        log_p_x->setName("log_p_x");
 
-        // Initialisation hack for nlmp_general_gaussian hack
-        for(int i=0; i<log_p_x.length(); i++)
-        log_p_x->value[i] = MISSING_VALUE;
-
-        log_p_target = new VarRowsVariable(log_p_x,target_index);
-        log_p_target->value[0] = log(1.0/L);
-        log_p_target->setName("log_p_target");
-        log_p_neighbors =new VarRowsVariable(log_p_x,neighbor_indexes);
-        log_p_neighbors->setName("log_p_neighbors");
-        */
-
-        tangent_targets = Var(nneighbors,n);
+        tangent_targets = Var(nneighbors,inputsize_);
         if(mu_nneighbors < 0 ) mu_nneighbors = nneighbors;
 
-        // compute - sum_{neighbors of x} log ( P(neighbor|x) ) according to semi-spherical model
         Var nll;
-        //if(noise <= 0)
-        nll = nll_general_gaussian(tangent_plane, mu, sn, tangent_targets, log_L, mu_nneighbors,0,0); // + log_n_examples;
-        //else
-        //nll = nll_general_gaussian(tangent_plane, mu, sn, tangent_targets, log_L, mu_nneighbors,noise_var,mu_noisy); // + log_n_examples;
+        nll = nll_general_gaussian(components, mu, sn, tangent_targets, 
+                                   log_L, learn_mu, mu_nneighbors); 
 
         Var knn = new SourceVariable(1,1);
         knn->setName("knn");
         knn->value[0] = nneighbors;
         sum_nll = new ColumnSumVariable(nll) / knn;
-        /*
-          if(architecture_type == "embedding_neural_network")
-          {
-          // Notes: - seulement prendre le plus proche voisin d'un voisin random
-          //        - il va peut-?tre falloir utiliser des fonctions de distances diff?rentes
-          //        - peut-?tre utiliser les directions principales apprises!
-          //        - peut-?tre utiliser les distances dans l'espace initial pour pond?rer!
-          //        - il va peut-?tre falloir mettre un poids diff?rent sur ce nouveau co?t
-          //        - utiliser ici une VarRowsVariable(...)
-          //        - question: est-ce que je devrais faire une bprop partout, juste sur embedding
-          //          juste sur neighbor et random, ... ?
 
-          //Var nearest_emb = product(W, tanh(c + product(V,rowOf(reference_set,neighbor_indexes))));
-          Var random_emb = product(W, tanh(c + product(V,rowOf(reference_set,random_index))));
-
-          //Var nearest_emb_diff = nearest_emb - embedding;
-          //Var random_emb_diff = random_emb - embedding;
-          //sum_nll += weight_embedding * (sum(square(nearest_emb_diff)) - sum(square(random_emb_diff)));
-          sum_nll += weight_embedding * diagonal_gaussian(random_emb,embedding,no_bprop(rowPowNorm(tangent_plane,2)));
-          }
-        */
+        // Weight decay penalty
         if(weight_decay > 0 )
         {
-            if(penalty_type == "L1_square") sum_nll += (square(sumabs(W))+ square(sumabs(V)) + square(sumabs(muV)) + square(sumabs(snV)))*weight_decay;
-            else if(penalty_type == "L1") sum_nll += (sumabs(W)+ sumabs(V) + sumabs(muV) + sumabs(snV))*weight_decay;
-            else if(penalty_type == "L2_square") sum_nll += (sumsquare(W)+ sumsquare(V) + sumsquare(muV) + sumsquare(snV))*weight_decay;
-            else PLERROR("In NonLocalManifoldParzen::build_(): penalty_type %s not recognized", penalty_type.c_str());
+            sum_nll += affine_transform_weight_penalty(
+                W,weight_decay,0,penalty_type) + 
+                affine_transform_weight_penalty(
+                V,weight_decay,0,penalty_type) + 
+                affine_transform_weight_penalty(
+                muV,weight_decay,0,penalty_type) + 
+                affine_transform_weight_penalty(
+                snV,weight_decay,0,penalty_type);
         }
-        /*
-          if(architecture_type == "embedding_neural_network")
-          {
-          Var random_diff = Var(n,1);
-          cost_of_one_example = Func(x & tangent_targets & target_index & neighbor_indexes & random_diff & random_index, predictor->parameters, sum_nll);
-          }
-          else
-        */
-        cost_of_one_example = Func(x & tangent_targets & target_index & neighbor_indexes, predictor->parameters, sum_nll);
 
-        if(nneighbors_density >= L || nneighbors_density < 0) nneighbors_density = L;
+        cost_of_one_example = Func(x & tangent_targets & target_index & 
+                                   neighbor_indexes, parameters, sum_nll);
 
-        t_row.resize(n);
-        Ut_svd.resize(n,n);
+        if(nneighbors_density >= L || nneighbors_density < 0) 
+            nneighbors_density = L;
+
+        // Output storage variables
+        t_row.resize(inputsize_);
+        Ut_svd.resize(inputsize_,inputsize_);
         V_svd.resize(ncomponents,ncomponents);
-        F.resize(tangent_plane->length(),tangent_plane->width());
-        z.resize(n);
-        x_minus_neighbor.resize(n);
-        neighbor_row.resize(n);
-        // log_density and Kernel methods
-        U_temp.resize(ncomponents,n);
-        mu_temp.resize(n);
+        F.resize(components->length(),components->width());
+        z.resize(inputsize_);
+        x_minus_neighbor.resize(inputsize_);
+        neighbor_row.resize(inputsize_);
+
+        // log_density and Kernel methods variables
+        U_temp.resize(ncomponents,inputsize_);
+        mu_temp.resize(inputsize_);
         sm_temp.resize(ncomponents);
         sn_temp.resize(1);
-        diff.resize(n);
+        diff.resize(inputsize_);
 
-        mus.resize(L, n);
+        mus.resize(L, inputsize_);
         sns.resize(L);
         sms.resize(L,ncomponents);
         Fs.resize(L);
         for(int i=0; i<L; i++)
         {
-            Fs[i].resize(ncomponents,n);
+            Fs[i].resize(ncomponents,inputsize_);
         }
+
+        if(paramsvalues.length() == parameters.nelems())
+            parameters << paramsvalues;
+        else
+        {
+            paramsvalues.resize(parameters.nelems());
+            initializeParams();
+            if(optimizer)
+                optimizer->reset();
+        }
+        parameters.makeSharedValue(paramsvalues);
     }
 
 }
 
-/*
-  void NonLocalManifoldParzen::update_reference_set_parameters()
-  {
-  // Compute Us, mus, sms, sns
-  Us.resize(L);
-  mus.resize(L, n);
-  sms.resize(L,ncomponents);
-  sns.resize(L);
-
-  for(int t=0; t<L; t++)
-  {
-  Us[t].resize(ncomponents,n);
-  reference_set->getRow(t,t_row);
-  predictor->fprop(t_row, F.toVec() & mus(t) & sns.subVec(t,1));
-
-  // N.B. this is the SVD of F'
-  lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-  for (int k=0;k<ncomponents;k++)
-  {
-  sms(t,k) = mypow(S_svd[k],2);
-  Us[t](k) << Ut_svd(k);
-  }
-  }
-
-  }
-*/
-
 void NonLocalManifoldParzen::knn(const VMat& vm, const Vec& x, const int& k, TVec<int>& neighbors, bool sortk) const
 {
     int n = vm->length();
@@ -674,18 +396,6 @@
         neighbors[i] = int(distances(i,1));
     }
 
-
-    /*
-      for (int i = 0, j=0; i < k  && j<n; j++)
-      {
-      real d = distances(j,0);
-      if (include_current_point || d>0)  //Ouach, caca!!!
-      {
-      neighbors[i] = int(distances(j,1));
-      i++;
-      }
-      }
-    */
 }
 
 // ### Nothing to add here, simply calls build_
@@ -704,64 +414,65 @@
 #endif
 
 void NonLocalManifoldParzen::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{  inherited::makeDeepCopyFromShallowCopy(copies);
+{  
+    inherited::makeDeepCopyFromShallowCopy(copies);
 
- deepCopyField(cost_of_one_example, copies);
- varDeepCopyField(x, copies);
- varDeepCopyField(b, copies);
- varDeepCopyField(W, copies);
- varDeepCopyField(c, copies);
- varDeepCopyField(V, copies);
- varDeepCopyField(muV, copies);
- varDeepCopyField(snV, copies);
- varDeepCopyField(snb, copies);
- varDeepCopyField(tangent_targets, copies);
- varDeepCopyField(tangent_plane, copies);
- varDeepCopyField(mu, copies);
- varDeepCopyField(sn, copies);
- varDeepCopyField(sum_nll, copies);
- varDeepCopyField(min_sig, copies);
- varDeepCopyField(init_sig, copies);
- varDeepCopyField(embedding, copies);
- deepCopyField(output_embedding, copies);
- deepCopyField(predictor, copies);
+    // Protected
 
- deepCopyField(U_temp,copies);
- deepCopyField(F, copies);
- deepCopyField(distances,copies);
- deepCopyField(mu_temp,copies);
- deepCopyField(sm_temp,copies);
- deepCopyField(sn_temp,copies);
- deepCopyField(diff,copies);
- deepCopyField(z,copies);
- deepCopyField(x_minus_neighbor,copies);
- deepCopyField(t_row,copies);
- deepCopyField(neighbor_row,copies);
- deepCopyField(log_gauss,copies);
- deepCopyField(t_dist,copies);
- deepCopyField(t_nn,copies);
- deepCopyField(Ut_svd, copies);
- deepCopyField(V_svd, copies);
- deepCopyField(S_svd, copies);
+    deepCopyField(cost_of_one_example, copies);
+    varDeepCopyField(x, copies);
+    varDeepCopyField(W, copies);
+    varDeepCopyField(V, copies);
+    varDeepCopyField(muV, copies);
+    varDeepCopyField(snV, copies);
+    varDeepCopyField(tangent_targets, copies);
+    varDeepCopyField(components, copies);
+    varDeepCopyField(mu, copies);
+    varDeepCopyField(sn, copies);
+    varDeepCopyField(sum_nll, copies);
+    varDeepCopyField(min_sig, copies);
+    varDeepCopyField(init_sig, copies);
+    deepCopyField(predictor, copies);
+    deepCopyField(U_temp,copies);
+    deepCopyField(F, copies);
+    deepCopyField(distances,copies);
+    deepCopyField(mu_temp,copies);
+    deepCopyField(sm_temp,copies);
+    deepCopyField(sn_temp,copies);
+    deepCopyField(diff,copies);
+    deepCopyField(z,copies);
+    deepCopyField(x_minus_neighbor,copies);
+    deepCopyField(t_row,copies);
+    deepCopyField(neighbor_row,copies);
+    deepCopyField(log_gauss,copies);
+    deepCopyField(t_dist,copies);
+    deepCopyField(t_nn,copies);
+    deepCopyField(Ut_svd, copies);
+    deepCopyField(V_svd, copies);
+    deepCopyField(S_svd, copies);
+    deepCopyField(mus, copies);
+    deepCopyField(sns, copies);
+    deepCopyField(sms, copies);
+    deepCopyField(Fs, copies);
+    deepCopyField(train_set_with_targets, copies);
+    deepCopyField(targets_vmat, copies);
+    varDeepCopyField(totalcost, copies);
+    deepCopyField(paramsvalues, copies);
+    
+    // Public
 
- deepCopyField(mus, copies);
- deepCopyField(sns, copies);
- deepCopyField(sms, copies);
- deepCopyField(Fs, copies);
+    deepCopyField(parameters, copies);    
+    deepCopyField(reference_set,copies);
+    deepCopyField(optimizer, copies);
 
- deepCopyField(parameters, copies);
- deepCopyField(shared_parameters, copies);
-
- deepCopyField(reference_set,copies);
- varDeepCopyField(hidden_layer, copies);
- deepCopyField(optimizer, copies);
-
 }
 
 
 void NonLocalManifoldParzen::forget()
 {
+    inherited::forget();
     if (train_set) initializeParams();
+    if(optimizer) optimizer->reset();
     stage = 0;
 }
 
@@ -772,14 +483,6 @@
     // except for sn...
     bool flag = (nstages == stage);
 
-    if(store_prediction && flag)
-    {
-        for(int i=0; i<L; i++)
-        {
-            sns[i] += sigma_min - min_sig->value[0];
-        }
-    }
-
     // Update sigma_min, in case it was changed,
     // e.g. using an HyperLearner
     min_sig->value[0] = sigma_min;
@@ -787,51 +490,28 @@
     // Set train_stats if not already done.
     if (!train_stats)
         train_stats = new VecStatsCollector();
-    /*
-    VMat train_set_with_targets;
-    VMat targets_vmat;
-    Var totalcost;
-    int nsamples;
-    */
+
     if (!cost_of_one_example)
         PLERROR("NonLocalManifoldParzen::train: build has not been run after setTrainingSet!");
-    /*
-      if(stage==0)
-      {
-      train_set = new SubVMatrix(train_set,0,0,train_set.length()-omit_last,train_set.width());
-      }
-    */
-    /*
-      if(architecture_type == "embedding_neural_network")
-      targets_vmat = hconcat(local_neighbors_differences(train_set, nneighbors, false, true),random_neighbors_differences(train_set,1,false,true));
-      else*/
 
     if(stage == 0)
     {
-        targets_vmat = local_neighbors_differences(train_set, nneighbors, false, true);
-
-        train_set_with_targets = hconcat(train_set, targets_vmat);
-        train_set_with_targets->defineSizes(inputsize()+ inputsize()*nneighbors+1+nneighbors /*+ (architecture_type == "embedding_neural_network" ? inputsize()+1:0)*/,0);
+        targets_vmat = append_neighbors(
+            train_set, nneighbors, true);
         nsamples = batch_size>0 ? batch_size : train_set->length();
 
         totalcost = meanOf(train_set_with_targets, cost_of_one_example, nsamples);
 
         if(optimizer)
         {
-            if(shared_parameters.size()!=0)
-                optimizer->setToOptimize(shared_parameters, totalcost);
-            else
-                optimizer->setToOptimize(parameters, totalcost);
+            optimizer->setToOptimize(parameters, totalcost);
             optimizer->build();
         }
         else PLERROR("NonLocalManifoldParzen::train can't train without setting an optimizer first!");
     }
 
-    dynamic_cast<SumOfVariable*>( (Variable*) totalcost)->curpos = curpos;
+    int optstage_per_lstage = train_set->length()/nsamples;
 
-    // number of optimizer stages corresponding to one learner stage (one epoch)
-    if(optstage_per_lstage < 0) optstage_per_lstage = train_set->length()/nsamples;
-
     PP<ProgressBar> pb;
     if(report_progress>0)
         pb = new ProgressBar("Training NonLocalManifoldParzen from stage " + tostring(stage) + " to " + tostring(nstages), nstages-stage);
@@ -850,31 +530,13 @@
         if(verbosity>2)
             cout << "Epoch " << stage << " train objective: " << train_stats->getMean() << endl;
         ++stage;
-        if(stage%save_every == 0 && store_prediction && !flag)
-        {
-            for(int t=0; t<L;t++)
-            {
-                reference_set->getRow(t,neighbor_row);
-                predictor->fprop(neighbor_row, F.toVec() & mus(t) & sns.subVec(t,1));
-                // N.B. this is the SVD of F'
-                lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-                for (int k=0;k<ncomponents;k++)
-                {
-                    sms(t,k) = mypow(S_svd[k],2);
-                    Fs[t](k) << Ut_svd(k);
-                }
-
-            }
-        }
-
         if(pb)
             pb->update(stage-initial_stage);
-
     }
     if(verbosity>1)
         cout << "EPOCH " << stage << " train objective: " << train_stats->getMean() << endl;
 
-    if(save_every < 0 && store_prediction && !flag)
+    if(store_prediction && !flag)
     {
         for(int t=0; t<L;t++)
         {
@@ -887,10 +549,9 @@
                 sms(t,k) = mypow(S_svd[k],2);
                 Fs[t](k) << Ut_svd(k);
             }
-
+            sns[t] += sigma_min - min_sig->value[0];
         }
     }
-    curpos = dynamic_cast<SumOfVariable*>( (Variable*) totalcost)->curpos;
 }
 
 //////////////////////
@@ -898,55 +559,17 @@
 //////////////////////
 void NonLocalManifoldParzen::initializeParams()
 {
-    resetGenerator(seed_);
-
-    if (architecture_type=="embedding_neural_network")
-    {
-        real delta = 1.0 / sqrt(real(inputsize()));
-        random_gen->fill_random_uniform(V->value, -delta, delta);
-        delta = 1.0 / real(n_hidden_units);
-        random_gen->fill_random_uniform(W->matValue, -delta, delta);
-        c->value.clear();
-        snb->value.clear();
-        random_gen->fill_random_uniform(snV->matValue, -delta, delta);
-        random_gen->fill_random_uniform(muV->matValue, -delta, delta);
-        //min_sig->value[0] = sigma_init;
-        //min_d->value.fill(diff_init);
-        if(variances_transfer_function == "softplus") {
-            init_sig->value[0] = pl_log(exp(sigma_init)-1); }
-        else if(variances_transfer_function == "square") { init_sig->value[0] = sqrt(sigma_init);}
-        else if(variances_transfer_function == "exp") {
-            init_sig->value[0] = pl_log(sigma_init); }
-    }
-    else if (architecture_type=="single_neural_network")
-    {
-        real delta = 1.0 / sqrt(real(inputsize()));
-        if (!hidden_layer)
-           random_gen->fill_random_uniform(V->value, -delta, delta);
-        delta = 1.0 / real(n_hidden_units);
-        random_gen->fill_random_uniform(W->matValue, -delta, delta);
-        if (!hidden_layer) c->value.clear();
-        snb->value.clear();
-        random_gen->fill_random_uniform(snV->matValue, -delta, delta);
-        random_gen->fill_random_uniform(muV->matValue, -delta, delta);
-        b->value.clear();
-        //min_sig->value[0] = sigma_init;
-        //min_d->value.fill(diff_init);
-        if(variances_transfer_function == "softplus") {
-            init_sig->value[0] = pl_log(exp(sigma_init)-1); }
-        else if(variances_transfer_function == "square") { init_sig->value[0] = sqrt(sigma_init);}
-        else if(variances_transfer_function == "exp") {
-            init_sig->value[0] = pl_log(sigma_init);}
-    }
-    else PLERROR("other types not handled yet!");
-
-    /*
-      for(int i=0; i<log_p_x.length(); i++)
-      //p_x->value[i] = log(1.0/p_x.length());
-      log_p_x->value[i] = MISSING_VALUE;
-    */
-    if(optimizer)
-        optimizer->reset();
+    real delta = 1.0 / sqrt(real(inputsize_));
+    random_gen->fill_random_uniform(W->value, -delta, delta);
+    delta = 1.0 / real(nhidden);
+    random_gen->fill_random_uniform(V->matValue, -delta, delta);
+    random_gen->fill_random_uniform(snV->matValue, -delta, delta);
+    random_gen->fill_random_uniform(muV->matValue, -delta, delta);
+    W->matValue(0).clear();
+    V->matValue(0).clear();
+    muV->matValue(0).clear();
+    snV->matValue(0).clear();
+    init_sig->value[0] = sqrt(sigma_init);
 }
 
 /////////////////
@@ -970,38 +593,12 @@
         }
     }
 
-
     min_sig->value[0] = sigma_min;
 
-/*
-  if(magnified_version)
-  {
-  predictor->fprop(x, F.toVec() & mu_temp & sn_temp);
-
-  // N.B. this is the SVD of F'
-  lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-  for (int k=0;k<ncomponents;k++)
-  {
-  sm_temp[k] = mypow(S_svd[k],2);
-  F(k) << Ut_svd(k);
-  }
-
-  mahal = -0.5*pownorm(mu_temp)/sn_temp[0];
-  norm_term = - n/2.0 * Log2Pi - 0.5*(n-ncomponents)*log(sn_temp[0]);
-  for(int k=0; k<ncomponents; k++)
-  {
-  mahal -= square(dot(F(k), mu_temp))*(0.5/(sm_temp[k]+sn_temp[0]) - 0.5/sn_temp[0]);
-  norm_term -= 0.5*log(sm_temp[k]+sn_temp[0]);
-  }
-
-  ret = mahal + norm_term + log((real)nneighbors) - log((real)L);
-  }
-  else
-  {*/
     if(nneighbors_density != L)
     {
         // Fetching nearest neighbors for density estimation.
-        knn(reference_set,x,nneighbors_density,t_nn,bool(0));
+        knn(reference_set,x,nneighbors_density,t_nn,0);
         log_gauss.resize(t_nn.length());
         for(int neighbor=0; neighbor<t_nn.length(); neighbor++)
         {
@@ -1019,27 +616,29 @@
             }
             else
             {
-                mu_temp << mus(t_nn[neighbor]);
+                if(learn_mu)
+                    mu_temp << mus(t_nn[neighbor]);
                 sn_temp[0] = sns[t_nn[neighbor]];
                 sm_temp << sms(t_nn[neighbor]);
                 U_temp << Fs[t_nn[neighbor]];
             }
-            substract(t_row,neighbor_row,x_minus_neighbor);
-            //substract(x_minus_neighbor,mus(t_nn[neighbor]),z);
-            substract(x_minus_neighbor,mu_temp,z);
-
-            //mahal = -0.5*pownorm(z)/sns[t_nn[neighbor]];
-            //norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*log(sns[t_nn[neighbor]]);
-
+            if(learn_mu)
+            {
+                substract(t_row,neighbor_row,x_minus_neighbor);
+                substract(x_minus_neighbor,mu_temp,z);
+            }
+            else
+                substract(t_row,neighbor_row,z);
+                
             mahal = -0.5*pownorm(z)/sn_temp[0];
-            norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*pl_log(sn_temp[0]);
+            norm_term = - inputsize_/2.0 * Log2Pi 
+                - log_L - 0.5*(inputsize_-ncomponents)*pl_log(sn_temp[0]);
 
 
             for(int k=0; k<ncomponents; k++)
             {
-                //mahal -= square(dot(z,Us[t_nn[neighbor]](k)))*(0.5/(sms(t_nn[neighbor],k)+sns[t_nn[neighbor]]) - 0.5/sns[t_nn[neighbor]]); // Pourrait ?tre acc?l?r?!
-                //norm_term -= 0.5*log(sms(t_nn[neighbor],k)+sns[t_nn[neighbor]]);
-                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) - 0.5/sn_temp[0]);
+                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) 
+                                                   - 0.5/sn_temp[0]);
                 norm_term -= 0.5*pl_log(sm_temp[k]+sn_temp[0]);
             }
 
@@ -1067,28 +666,29 @@
             }
             else
             {
-                mu_temp << mus(t);
+                if(learn_mu)
+                    mu_temp << mus(t);
                 sn_temp[0] = sns[t];
                 sm_temp << sms(t);
                 U_temp << Fs[t];
             }
 
-            substract(t_row,neighbor_row,x_minus_neighbor);
-            //substract(x_minus_neighbor,mus(t),z);
-            substract(x_minus_neighbor,mu_temp,z);
+            if(learn_mu)
+            {
+                substract(t_row,neighbor_row,x_minus_neighbor);
+                substract(x_minus_neighbor,mu_temp,z);
+            }
+            else
+                substract(t_row,neighbor_row,z);
 
-            //mahal = -0.5*pownorm(z)/sns[t];
-            //norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*log(sns[t]);
-
             mahal = -0.5*pownorm(z)/sn_temp[0];
-            norm_term = - n/2.0 * Log2Pi - log_L - 0.5*(n-ncomponents)*pl_log(sn_temp[0]);
+            norm_term = - inputsize_/2.0 * Log2Pi - log_L 
+                - 0.5*(inputsize_-ncomponents)*pl_log(sn_temp[0]);
 
             for(int k=0; k<ncomponents; k++)
             {
-                //mahal -= square(dot(z,Us[t](k)))*(0.5/(sms(t,k)+sns[t]) - 0.5/sns[t]); // Pourrait ?tre acc?l?r?!
-                //norm_term -= 0.5*log(sms(t,k)+sns[t]);
-
-                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) - 0.5/sn_temp[0]);
+                mahal -= square(dot(z,U_temp(k)))*(0.5/(sm_temp[k]+sn_temp[0]) 
+                                                   - 0.5/sn_temp[0]);
                 norm_term -= 0.5*pl_log(sm_temp[k]+sn_temp[0]);
             }
 
@@ -1096,24 +696,11 @@
         }
     }
     ret = logadd(log_gauss);
-    //}
 
     return ret;
 }
 
-/*
-  Mat NonLocalManifoldParzen::getEigenvectors(int j) const {
-  {
-  return Us[j];
-  }
 
-  Vec NonLocalManifoldParzen::getTrainPoint(int j) const {
-  Vec ret(reference_set->width());
-  reference_set->getRow(j,ret);
-  return ret;
-  }
-*/
-
 ///////////////////
 // computeOutput //
 ///////////////////
@@ -1121,10 +708,7 @@
 {
     switch(outputs_def[0])
     {
-    case 'm':
-        output_embedding(input);
-        output << embedding->value;
-        break;
+        /*
     case 'r':
     {
         string fsave = "";
@@ -1198,7 +782,9 @@
         output << F.toVec();
         break;
     }
+        */
     default:
+        
         inherited::computeOutput(input,output);
     }
 }
@@ -1210,6 +796,7 @@
 {
     switch(outputs_def[0])
     {
+        /*
     case 'm':
         return ncomponents;
         break;
@@ -1217,39 +804,12 @@
         return n;
     case 't':
         return ncomponents*n;
+        */
     default:
         return inherited::outputsize();
     }
 }
 
-real NonLocalManifoldParzen::evaluate(Vec x1,Vec x2,real scale)
-{
-    real ret;
-
-    // Update sigma_min, in case it was changed,
-    // e.g. using an HyperLearner
-    min_sig->value[0] = sigma_min;
-
-    predictor->fprop(x2, F.toVec() & mu_temp & sn_temp);
-
-    // N.B. this is the SVD of F'
-    lapackSVD(F, Ut_svd, S_svd, V_svd,'A',1.5);
-    for (int k=0;k<ncomponents;k++)
-    {
-        sm_temp[k] = mypow(S_svd[k],2);
-        F(k) << Ut_svd(k);
-    }
-
-    diff = x1 - x2;
-    diff -= mu_temp;
-    ret = scale * pownorm(diff)/sn_temp[0];
-    for (int k = 0; k < ncomponents ; k++) {
-        ret += scale * (1.0 /( sm_temp[k] + sn_temp[0]) - 1.0/sn_temp[0]) * square(dot(F(k), diff));
-    }
-    return ret;
-
-}
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/distributions/NonLocalManifoldParzen.h
===================================================================
--- trunk/plearn_learners/distributions/NonLocalManifoldParzen.h	2007-10-10 22:33:15 UTC (rev 8171)
+++ trunk/plearn_learners/distributions/NonLocalManifoldParzen.h	2007-10-10 22:41:33 UTC (rev 8172)
@@ -68,46 +68,34 @@
     // * protected options *
     // *********************
 
-    // ### declare protected option fields (such as learnt parameters) here
-
-
-    // NON-OPTION FIELDS
-    //! Input size
-    int n;
+    //! Number of gaussians
+    int L;
+    //! Logarithm of number of gaussians
+    real log_L;
     //! Cost of one example
     Func cost_of_one_example;
     //! Input vector
     Var x;
     //! Parameters of the neural network
-    Var b, W, c, V, muV, snV, snb;
+    Var W, V, muV, snV;
     //! Tangent vector targets
     Var tangent_targets;
     //! Tangent vectors spanning the tangent plane, given by
     //! the neural network
-    Var tangent_plane;
+    Var components;
     //! Mean of the gaussian
     Var mu;
     //! Sigma^2_noise of the gaussian
     Var sn;
-    //Var mu_noisy, noise_var;
-    //PP<PDistribution> dist;
     //! Sum of NLL cost
     Var sum_nll;
     //! Mininum value of sigma^2_noise
     Var min_sig;
     //! Initial (approximate) value of sigma^2_noise
     Var init_sig;
-    //! Embedding computed by the (embedding) neural network
-    Var embedding;
-    //! Function to output the embedding
-    Func output_embedding;
     //! Predictor of the parameters of the gaussian at x
     Func predictor; // predicts everything about the gaussian
 
-    //TVec< Mat > Us;
-    //Mat mus,sms;
-    //Vec sns;
-
     //! log_density and Kernel methods' temporary variables
     mutable Mat U_temp, F, distances;
     //! log_density and Kernel methods' temporary variables
@@ -132,101 +120,66 @@
     //! Predictions for F
     TVec<Mat> Fs;
 
-    //! Position of the current example in
-    //! the training set
-    int curpos;
-
+    //! Training set concatenated with nearest neighbor targets
     VMat train_set_with_targets;
+    //! Nearest neighbor differences targets
     VMat targets_vmat;
+    //! Total cost Var
     Var totalcost;
+    //! Batch size
     int nsamples;
 
+    //! Parameter values
+    Vec paramsvalues;
+
 public:
 
     // ************************
     // * public build options *
     // ************************
 
-    // ### declare public option fields (such as build options) here
+    // ** General parameters **
 
     //! Parameters of the model
-    //! It is put here so that these parameters can
-    //! be shared by different NonLocalManifoldParzen
-    //! objects (eventually, should use the "friend class"
-    //! principal instead)
     VarArray parameters;
+    //! Reference set of points in the gaussian mixture
+    VMat reference_set;
+    //! Number of reduced dimensions (number of tangent vectors to compute)
+    int ncomponents;
+    //! Number of neighbors used for gradient descent
+    int nneighbors;
+    //! Number of neighbors for the p(x) density estimation
+    int nneighbors_density;
+    //! Indication that the predicted parameters should be stored
+    bool store_prediction;
+    
+    // ** Gaussian kernel options **
 
-
-    // Embedding penalty weight
-    //real weight_embedding;
-
-    //! Weight decay for all weights
-    real weight_decay;
-    //! Penalty type to use on the weights
-    string penalty_type;
-
-    //real noise_grad_factor;
-    //real noise;
-    //string noise_type;
-    //int omit_last;
-    //bool magnified_version;
-
     //! Indication that the mean of the gaussians should be learned
     bool learn_mu;
-    //! Reference set of points in the gaussian mixture
-    VMat reference_set;
     //! Initial (approximate) value of sigma^2_noise
     real sigma_init;
     //! Minimum value of sigma^2_noise
     real sigma_min;
-    //! Number of gaussians
-    int L;
-    //! User specified hidden layer NaryVariable subclass
-    Var hidden_layer;
-    //! Logarithm of number of gaussians
-    real log_L;
-    //! Number of neighbors used for gradient descent
-    int nneighbors;
-    //! Number of neighbors for the p(x) density estimation
-    int nneighbors_density;
     //! Number of neighbors to learn the mus
     int mu_nneighbors;
-    //! Number of reduced dimensions (number of tangent vectors to compute)
-    int ncomponents;
     //! Threshold applied on the update rule for sigma^2_noise
     real sigma_threshold_factor;
-    //! Variance transfer function ("square", "exp" or "softplus")
-    string variances_transfer_function;
+    //! SVD threshold on the eigen values
+    real svd_threshold;
+
+    // ** Neural network predictor option **
+
+    //! Number of hidden units
+    int nhidden;
+    //! Weight decay for all weights
+    real weight_decay;
+    //! Penalty type to use on the weights
+    string penalty_type;
     //! Optimizer of the neural network
     PP<Optimizer> optimizer;
-    //! Architecture type of the neural network ("single_neural_network" or "embedding_neural_nework")
-    string architecture_type;
-    //! Number of hidden units
-    int n_hidden_units;
     //! Batch size of the gradient-based optimization
     int batch_size;
-    //! SVD threshold on the eigen values
-    real svd_threshold;
-    //! Number of steps in the random walk
-    int rw_n_step;
-    //! Size of the step
-    real rw_size_step;
-    //! Which principal component to follow
-    int rw_ith_component;
-    //! File name for the rw saves
-    string rw_file_name;
-    //! Number of iterations between rw saves
-    int rw_save_every;
-    //! Indication that the predicted parameters should be stored
-    bool store_prediction;
-    //! Parameters to share
-    VarArray shared_parameters;
-    //! Number of optimizer stages
-    int optstage_per_lstage;
-    //! Number of iterations since the last save
-    //! after which the parameters
-    //! must be saved
-    int save_every;
 
     // ****************
     // * Constructors *
@@ -235,7 +188,6 @@
     //! Default constructor.
     NonLocalManifoldParzen();
 
-
     // ********************
     // * PLearner methods *
     // ********************
@@ -247,6 +199,9 @@
 
     //void update_reference_set_parameters();
 
+    //! Finds nearest neighbors of "x" in set "vm" and 
+    //! puts their indices in "neighbors". The neighbors
+    //! can be sorted if "sortk" is true
     void knn(const VMat& vm, const Vec& x, const int& k, TVec<int>& neighbors, bool sortk) const;
 
 protected:
@@ -284,9 +239,6 @@
     //! Return log of probability density log(p(y)).
     virtual real log_density(const Vec& x) const;
 
-    //! Return log density of ith point in reference_set
-    real log_density(int i);
-
     //! The role of the train method is to bring the learner up to stage==nstages,
     //! updating the train_stats collector with training costs measured on-line in the process.
     virtual void train();
@@ -330,7 +282,6 @@
 
     //Mat getEigenvectors(int j) const;
     //Vec getTrainPoint(int j) const;
-    real evaluate(const Vec x1,const Vec x2,real scale=1);
 };
 
 // Declares a few other classes and functions related to this class.



From larocheh at mail.berlios.de  Thu Oct 11 00:43:13 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:43:13 +0200
Subject: [Plearn-commits] r8173 - trunk/plearn_learners/distributions
Message-ID: <200710102243.l9AMhD0I027303@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:43:12 +0200 (Thu, 11 Oct 2007)
New Revision: 8173

Modified:
   trunk/plearn_learners/distributions/NGramDistribution.cc
   trunk/plearn_learners/distributions/NGramDistribution.h
Log:


Modified: trunk/plearn_learners/distributions/NGramDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/NGramDistribution.cc	2007-10-10 22:41:33 UTC (rev 8172)
+++ trunk/plearn_learners/distributions/NGramDistribution.cc	2007-10-10 22:43:12 UTC (rev 8173)
@@ -52,7 +52,12 @@
 // NGramDistribution //
 ///////////////////////
 NGramDistribution::NGramDistribution() :
-    nan_replace(false),n(2),additive_constant(0),discount_constant(0.01), validation_proportion(0.10), smoothing("no_smoothing"),lambda_estimation("manual")
+    nan_replace(false),
+    n(2),
+    additive_constant(0),
+    discount_constant(0.01), 
+    smoothing("no_smoothing"),
+    lambda_estimation("manual")
 {
     forget();
     // In a N-Gram, the predicted size is always one.
@@ -78,7 +83,8 @@
     // ### OptionBase::tuningoption. Another possible flag to be combined with
     // ### is OptionBase::nosave
 
-    declareOption(ol, "nan_replace", &NGramDistribution::nan_replace, OptionBase::buildoption,
+    declareOption(ol, "nan_replace", &NGramDistribution::nan_replace, 
+                  OptionBase::buildoption,
                   "Indication that the missing values in context (nan) should be\n"
                   "replaced by a default value (-1). nan fields should correspond\n"
                   "to context not accessible (like in the beginning of a sentence).\n"
@@ -91,13 +97,16 @@
         "'predictor_size' and 'predicted_size', i.e. predictor_size = n-1\n"
         "and predicted_size = 1.");
 
-    declareOption(ol, "additive_constant", &NGramDistribution::additive_constant, OptionBase::buildoption,
+    declareOption(ol, "additive_constant", &NGramDistribution::additive_constant, 
+                  OptionBase::buildoption,
                   "Additive constant for add-delta smoothing");
-    declareOption(ol, "discount_constant", &NGramDistribution::discount_constant, OptionBase::buildoption,
+
+    declareOption(ol, "discount_constant", &NGramDistribution::discount_constant, 
+                  OptionBase::buildoption,
                   "Discount constant for absolut discounting smoothing");
-    declareOption(ol, "validation_proportion", &NGramDistribution::validation_proportion, OptionBase::buildoption,
-                  "Proportion of the training set used for validation (EM)");
-    declareOption(ol, "smoothing", &NGramDistribution::smoothing, OptionBase::buildoption,
+
+    declareOption(ol, "smoothing", &NGramDistribution::smoothing, 
+                  OptionBase::buildoption,
                   "Smoothing method. Choose among:\n"
                   "- \"no_smoothing\"\n"
                   "- \"add-delta\"\n"
@@ -105,17 +114,26 @@
                   "- \"witten-bell\"\n"
                   "- \"absolute-discounting\"\n"
         );
-    declareOption(ol, "lambda_estimation", &NGramDistribution::lambda_estimation, OptionBase::buildoption,
+    declareOption(ol, "lambda_estimation", &NGramDistribution::lambda_estimation, 
+                  OptionBase::buildoption,
                   "Lambdas estimation method. Choose among:\n"
                   "- \"manual\" (lambdas field should be specified)\n"
                   "- \"EM\"\n"
         );
-    declareOption(ol, "lambdas", &NGramDistribution::lambdas, OptionBase::buildoption,
+    declareOption(ol, "lambdas", &NGramDistribution::lambdas, 
+                  OptionBase::buildoption,
                   "Lambdas of the interpolated ngram");
-    declareOption(ol, "tree", &NGramDistribution::tree, OptionBase::buildoption,
+
+    declareOption(ol, "validation_set", &NGramDistribution::validation_set, 
+                  OptionBase::buildoption,
+                  "Validation set used to estimate the lambdas with the\n"
+                  "EM algorithm.");
+
+    declareOption(ol, "tree", &NGramDistribution::tree, OptionBase::learntoption,
                   "NGramTree of the frequencies");
 
-    declareOption(ol, "voc_size", &NGramDistribution::voc_size, OptionBase::learntoption,
+    declareOption(ol, "voc_size", &NGramDistribution::voc_size, 
+                  OptionBase::learntoption,
                   "Vocabulary size");
 
     // Now call the parent class' declareOptions().
@@ -136,7 +154,7 @@
 void NGramDistribution::build()
 {
     // now set in the constructor to -1
-    //predictor_size = n - 1;
+    predictor_size = n - 1;
     inherited::build();
     build_();
 }
@@ -350,51 +368,53 @@
 
 void NGramDistribution::train()
 {
-    VMat contexts_train;
-    VMat contexts_validation;
 
+//    if(smoothing == "jelinek-mercer" && lambda_estimation == "EM")
+//    {
+//        if(validation_proportion <= 0 || validation_proportion >= 1)
+//            PLERROR("In NGramDistribution:build_() : validation_proportion should be in (0,1)");
+//        // Making FractionSplitter
+//        PP<FractionSplitter> fsplit = new FractionSplitter();
+//        TMat<pair<real,real> > splits(1,2);
+//        splits(0,0).first = 0; splits(0,0).second = 1-validation_proportion;
+//        splits(0,1).first = 1-validation_proportion; splits(0,1).second = 1;
+//        fsplit->splits = splits;
+//        fsplit->build();
+//
+//        // Making RepeatSplitter
+//        PP<RepeatSplitter> rsplit = new RepeatSplitter();
+//        rsplit->n = 1;
+//        rsplit->shuffle = true;
+//        rsplit->seed = 123456;
+//        rsplit->to_repeat = fsplit;
+//        rsplit->setDataSet(train_set);
+//        rsplit->build();
+//
+//        TVec<VMat> vmat_splits = rsplit->getSplit();
+//        contexts_train = vmat_splits[0];
+//        contexts_validation = vmat_splits[1];
+//    }
+//    else
 
-    if(smoothing == "jelinek-mercer" && lambda_estimation == "EM")
-    {
-        if(validation_proportion <= 0 || validation_proportion >= 1)
-            PLERROR("In NGramDistribution:build_() : validation_proportion should be in (0,1)");
-        // Making FractionSplitter
-        PP<FractionSplitter> fsplit = new FractionSplitter();
-        TMat<pair<real,real> > splits(1,2);
-        splits(0,0).first = 0; splits(0,0).second = 1-validation_proportion;
-        splits(0,1).first = 1-validation_proportion; splits(0,1).second = 1;
-        fsplit->splits = splits;
-        fsplit->build();
 
-        // Making RepeatSplitter
-        PP<RepeatSplitter> rsplit = new RepeatSplitter();
-        rsplit->n = 1;
-        rsplit->shuffle = true;
-        rsplit->seed = 123456;
-        rsplit->to_repeat = fsplit;
-        rsplit->setDataSet(train_set);
-        rsplit->build();
-
-        TVec<VMat> vmat_splits = rsplit->getSplit();
-        contexts_train = vmat_splits[0];
-        contexts_validation = vmat_splits[1];
-    }
-    else
-        contexts_train = train_set;
-
     //Putting ngrams in the tree
     Vec row(n);
     TVec<int> int_row(n);
 
-
-    PP<ProgressBar> pb =  new ProgressBar("Inserting ngrams in NGramTree", contexts_train->length());
-    for(int i=0; i<contexts_train->length(); i++)
+    if(stage == 0 && nstages>0)
     {
-        contexts_train->getRow(i,row);
-        getNGrams(row,int_row);
-        tree->add(int_row);
-
-        pb->update(i+1);
+        PP<ProgressBar> pb =  new ProgressBar("Inserting ngrams in NGramTree", train_set->length());
+        for(int i=0; i<train_set->length(); i++)
+        {
+            train_set->getRow(i,row);
+            getNGrams(row,int_row);
+            tree->add(int_row);
+            
+            pb->update(i+1);
+        }
+        stage++;
+        if(smoothing == "jelinek-mercer" && lambda_estimation == "EM")
+            stage--; //Will be incremented in EM estimation
     }
 
     // Smoothing techniques parameter estimation
@@ -403,7 +423,12 @@
         //Jelinek-Mercer: EM estimation of lambdas
         if(lambda_estimation == "EM")
         {
-            lambdas.resize(n+1); lambdas.fill(1.0/(n+1));
+            if(stage == 0) 
+            {
+                lambdas.resize(n+1); lambdas.fill(1.0/(n+1));
+            }
+            if(!validation_set) PLERROR("In NGramDistribution:build_() : "
+                                        "validation_set needs to be provided");
             real diff = EM_PRECISION+1;
             real l_old = 0, l_new = -REAL_MAX;
             Vec e(n+1);
@@ -411,7 +436,8 @@
             TVec<int> ngram(n);
             real p_sum = 0;
             int n_ngram = 0;
-            while(diff > EM_PRECISION)
+            //while(diff > EM_PRECISION)
+            while(stage < nstages)
             {
                 if(verbosity > 2)
                     cout << "EM diff: " << diff << endl;
@@ -421,13 +447,15 @@
                 // E step
 
                 e.fill(0);
-                for(int t=0; t<contexts_validation->length(); t++)
+                //for(int t=0; t<contexts_validation->length(); t++)
+                for(int t=0; t<validation_set->length(); t++)
                 {
                     p_sum = 0;
 
                     // get w_{t-n+1}^t
 
-                    contexts_validation->getRow(t,row);
+                    //contexts_validation->getRow(t,row);
+                    validation_set->getRow(t,row);
                     getNGrams(row,ngram);
 
                     TVec<int> freq = tree->freq(ngram);
@@ -455,6 +483,7 @@
                     lambdas[j] = e[j]/n_ngram;
 
                 diff = l_new-l_old;
+                stage++;
             }
 
             //Test

Modified: trunk/plearn_learners/distributions/NGramDistribution.h
===================================================================
--- trunk/plearn_learners/distributions/NGramDistribution.h	2007-10-10 22:41:33 UTC (rev 8172)
+++ trunk/plearn_learners/distributions/NGramDistribution.h	2007-10-10 22:43:12 UTC (rev 8173)
@@ -88,9 +88,6 @@
     //! Discount constant for absolute discounting smoothing
     real discount_constant;
 
-    //! Proportion of the training set used for validation
-    real validation_proportion;
-
     //! Smoothing parameter
     string smoothing;
 



From larocheh at mail.berlios.de  Thu Oct 11 00:47:41 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:47:41 +0200
Subject: [Plearn-commits] r8174 - trunk/plearn_learners/generic
Message-ID: <200710102247.l9AMlfVW032758@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:47:40 +0200 (Thu, 11 Oct 2007)
New Revision: 8174

Modified:
   trunk/plearn_learners/generic/FeatureSetNNet.cc
   trunk/plearn_learners/generic/FeatureSetNNet.h
Log:


Modified: trunk/plearn_learners/generic/FeatureSetNNet.cc
===================================================================
--- trunk/plearn_learners/generic/FeatureSetNNet.cc	2007-10-10 22:43:12 UTC (rev 8173)
+++ trunk/plearn_learners/generic/FeatureSetNNet.cc	2007-10-10 22:47:40 UTC (rev 8174)
@@ -377,6 +377,8 @@
     }
     
     feat_input.resize(nfeats);
+    if(dist_rep_dim<=0) nnet_input = feat_input; // Keep sizes synchronized
+
     offset = 0;
     id = 0;
     for(int i=0; i<ni; i++)

Modified: trunk/plearn_learners/generic/FeatureSetNNet.h
===================================================================
--- trunk/plearn_learners/generic/FeatureSetNNet.h	2007-10-10 22:43:12 UTC (rev 8173)
+++ trunk/plearn_learners/generic/FeatureSetNNet.h	2007-10-10 22:47:40 UTC (rev 8174)
@@ -104,7 +104,7 @@
     //! Gradient on feature input (useless for now)
     Vec gradient_feat_input;
     //! Input vector to NNet (after mapping into distributed representations)
-    Vec nnet_input;
+    mutable Vec nnet_input;
     //! Gradient for vector to NNet
     Vec gradient_nnet_input;
     //! First hidden layer value
@@ -179,6 +179,9 @@
     //! Gradient on bias of output layer for distributed
     //! representation predictor
     Vec gradient_bout_dist_rep;
+    //! Proposal distribution for importance sampling
+    //! estimation of the gradient.
+    Vec output_empirical_distribution;
 
 public:
 
@@ -258,12 +261,16 @@
     bool possible_targets_vary;
     //! FeatureSets to apply on input
     TVec<PP<FeatureSet> > feat_sets;
-    //! Indication that the input IDs should be used as the feature ID.
-    //! The ID/string mapping provided by the input VMatrix Dictionary
-    //! objects is hence used.
-    //! This implies that all VMatrices (even those at test time) that
-    //! provide the input vectors should use the same Dictionary objects.
-    bool use_input_as_feature;
+    //  //! Indication that the input IDs should be used as the feature ID.
+    //  //! The ID/string mapping provided by the input VMatrix Dictionary
+    //  //! objects is hence used.
+    //  //! This implies that all VMatrices (even those at test time) that
+    //  //! provide the input vectors should use the same Dictionary objects.
+    //  bool use_input_as_feature;
+    //  //! Indication that an estimate of the gradient using
+    //  //! importance sampling should be used.
+    //  //! See (Bengio and S?n?cal, 2003) for more details.
+    //  bool use_importance_sampling_gradient;
 
 private:
     void build_();



From larocheh at mail.berlios.de  Thu Oct 11 00:48:41 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:48:41 +0200
Subject: [Plearn-commits] r8175 - trunk/plearn_learners/online
Message-ID: <200710102248.l9AMmfsR000594@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:48:40 +0200 (Thu, 11 Oct 2007)
New Revision: 8175

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-10 22:47:40 UTC (rev 8174)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-10 22:48:40 UTC (rev 8175)
@@ -594,6 +594,7 @@
     deepCopyField(expectation_gradients, copies);
     deepCopyField(reconstruction_activations, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
+    deepCopyField(reconstruction_expectation_gradients, copies);
     deepCopyField(reconstruction_activation_gradients_from_hid_rec, copies);
     deepCopyField(reconstruction_expectation_gradients_from_hid_rec, copies);
     deepCopyField(hidden_reconstruction_activations, copies);



From larocheh at mail.berlios.de  Thu Oct 11 00:51:08 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 11 Oct 2007 00:51:08 +0200
Subject: [Plearn-commits] r8176 - trunk/plearn_learners/online
Message-ID: <200710102251.l9AMp8Qt002758@sheep.berlios.de>

Author: larocheh
Date: 2007-10-11 00:51:06 +0200 (Thu, 11 Oct 2007)
New Revision: 8176

Modified:
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
Log:
Added functions to deal with setting the expectation fields and with the expectation(s)_is(are)_up_to_date. Also, added the specification of the output_size and input_size in the build() of RBMMixedLayer.


Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-10-10 22:48:40 UTC (rev 8175)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-10-10 22:51:06 UTC (rev 8176)
@@ -575,6 +575,24 @@
     bias << rbm_bias;
 }
 
+////////////////////
+// setExpectation //
+////////////////////
+void RBMLayer::setExpectation(const Vec& the_expectation)
+{
+    expectation << the_expectation;
+    expectation_is_up_to_date=true;
+}
+
+/////////////////////////
+// setExpectationByRef //
+/////////////////////////
+void RBMLayer::setExpectationByRef(const Vec& the_expectation)
+{
+    expectation = the_expectation;
+    expectation_is_up_to_date=true;
+}
+
 /////////////////////
 // setExpectations //
 /////////////////////

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-10-10 22:48:40 UTC (rev 8175)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-10-10 22:51:06 UTC (rev 8176)
@@ -132,12 +132,19 @@
     //! Sets batch_size and resize activations, expectations, and samples
     virtual void setBatchSize( int the_batch_size );
 
+    //! Copy the given expectation in the 'expectation' vector.
+    virtual void setExpectation(const Vec& the_expectation);
+
+    //! Make the 'expectation' vector point to the given data vector (so no
+    //! copy is performed).
+    virtual void setExpectationByRef(const Vec& the_expectation);
+
     //! Copy the given expectations in the 'expectations' matrix.
-    void setExpectations(const Mat& the_expectations);
+    virtual void setExpectations(const Mat& the_expectations);
 
     //! Make the 'expectations' matrix point to the given data matrix (so no
     //! copy is performed).
-    void setExpectationsByRef(const Mat& the_expectations);
+    virtual void setExpectationsByRef(const Mat& the_expectations);
 
     //! Accessor to the 'expectations' matrix.
     const Mat& getExpectations();

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-10-10 22:48:40 UTC (rev 8175)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-10-10 22:51:06 UTC (rev 8176)
@@ -93,6 +93,74 @@
         sub_layers[i]->setBatchSize( the_batch_size );
 }
 
+////////////////////
+// setExpectation //
+////////////////////
+void RBMMixedLayer::setExpectation(const Vec& the_expectation)
+{
+    expectation << the_expectation;
+    expectation_is_up_to_date=true;
+    for( int i = 0; i < n_layers; i++ )
+        sub_layers[i]->expectation_is_up_to_date=true;
+}
+
+/////////////////////////
+// setExpectationByRef //
+/////////////////////////
+void RBMMixedLayer::setExpectationByRef(const Vec& the_expectation)
+{
+    expectation = the_expectation;
+    expectation_is_up_to_date=true;
+
+     // Rearrange pointers
+    for( int i = 0; i < n_layers; i++ )
+    {
+        int init_pos = init_positions[i];
+        PP<RBMLayer> layer = sub_layers[i];
+        int layer_size = layer->size;
+        
+        layer->setExpectationByRef( expectation.subVec(init_pos, layer_size) );
+    }
+
+}
+
+/////////////////////
+// setExpectations //
+/////////////////////
+void RBMMixedLayer::setExpectations(const Mat& the_expectations)
+{
+    batch_size = the_expectations.length();
+    setBatchSize( batch_size );
+    expectations << the_expectations;
+    expectations_are_up_to_date=true;
+    for( int i = 0; i < n_layers; i++ )
+        sub_layers[i]->expectations_are_up_to_date=true;
+}
+
+//////////////////////////
+// setExpectationsByRef //
+//////////////////////////
+void RBMMixedLayer::setExpectationsByRef(const Mat& the_expectations)
+{
+    batch_size = the_expectations.length();
+    setBatchSize( batch_size );
+    expectations = the_expectations;
+    expectations_are_up_to_date=true;
+
+    // Rearrange pointers
+    for( int i = 0; i < n_layers; i++ )
+    {
+        int init_pos = init_positions[i];
+        PP<RBMLayer> layer = sub_layers[i];
+        int layer_size = layer->size;
+
+        layer->setExpectationsByRef(expectations.subMatColumns(init_pos,
+                                                              layer_size));
+    }
+}
+
+
+
 ///////////////////////
 // getUnitActivation //
 ///////////////////////
@@ -588,7 +656,7 @@
         layer->sample = sample.subVec(init_pos, layer_size);
         layer->samples = samples.subMatColumns(init_pos, layer_size);
 
-        layer->expectation = expectation.subVec(init_pos, layer_size);
+        layer->setExpectationByRef( expectation.subVec(init_pos, layer_size) );
         layer->setExpectationsByRef(expectations.subMatColumns(init_pos,
                                                               layer_size));
 
@@ -612,6 +680,9 @@
             layer->forget();
         }
     }
+
+    input_size = size;
+    output_size = size;
 }
 
 void RBMMixedLayer::build()

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-10-10 22:48:40 UTC (rev 8175)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-10-10 22:51:06 UTC (rev 8176)
@@ -79,6 +79,20 @@
     //! Sets batch_size and resize activations, expectations, and samples
     virtual void setBatchSize( int the_batch_size );
 
+    //! Copy the given expectation in the 'expectation' vector.
+    virtual void setExpectation(const Vec& the_expectation);
+
+    //! Make the 'expectation' vector point to the given data vector (so no
+    //! copy is performed).
+    virtual void setExpectationByRef(const Vec& the_expectation);
+
+    //! Copy the given expectations in the 'expectations' matrix.
+    virtual void setExpectations(const Mat& the_expectations);
+
+    //! Make the 'expectations' matrix point to the given data matrix (so no
+    //! copy is performed).
+    virtual void setExpectationsByRef(const Mat& the_expectations);
+
     // Your other public member functions go here
     //! Uses "rbmc" to compute the activation of unit "i" of this layer.
     //! This activation is computed by the "i+offset"-th unit of "rbmc"



From nouiz at mail.berlios.de  Thu Oct 11 19:03:08 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 11 Oct 2007 19:03:08 +0200
Subject: [Plearn-commits] r8177 - trunk/python_modules/plearn/parallel
Message-ID: <200710111703.l9BH38ke027864@sheep.berlios.de>

Author: nouiz
Date: 2007-10-11 19:03:07 +0200 (Thu, 11 Oct 2007)
New Revision: 8177

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Added 2 second of sleep between launch of command on cluster, to be sure it work correctly. We do not trust it complety and the old apdispatch did the same


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-10 22:51:06 UTC (rev 8176)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-11 17:03:07 UTC (rev 8177)
@@ -90,13 +90,14 @@
 
 #original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
 class MultiThread:
-    def __init__( self, function, argsVector, maxThreads=5, print_when_finished=None):
+    def __init__( self, function, argsVector, maxThreads=5, print_when_finished=None, sleep_time = 0):
         self._function     = function
         self._argsIterator = LockedIterator( iter( argsVector ) )
         self._threadPool   = []
         self.print_when_finish = print_when_finished
         self.running = 0
         self.init_len_list = len(argsVector)
+        self.sleep_time = sleep_time
         
         if maxThreads==-1:
             nb_thread=len(argsVector)
@@ -122,7 +123,7 @@
                     
     def start( self  ):
         for thread in self._threadPool:
-            time.sleep( 0 ) # necessary to give other threads a chance to run
+            time.sleep( self.sleep_time ) # necessary to give other threads a chance to run
             self.running+=1
             thread.start()
             
@@ -480,7 +481,9 @@
         self.exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :"[DBI,%s]"%time.ctime())
+        self.mt=MultiThread(self.run_one_job,self.tasks,
+                            self.nb_proc,lambda :"[DBI,%s]"%time.ctime(),
+                            sleep_time=2)
         self.mt.start()
 
         # Execute post-batchs



From nouiz at mail.berlios.de  Fri Oct 12 15:59:25 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 12 Oct 2007 15:59:25 +0200
Subject: [Plearn-commits] r8178 - trunk/doc
Message-ID: <200710121359.l9CDxP5a023196@sheep.berlios.de>

Author: nouiz
Date: 2007-10-12 15:59:25 +0200 (Fri, 12 Oct 2007)
New Revision: 8178

Modified:
   trunk/doc/
Log:
Modified svn:ignore properties



Property changes on: trunk/doc
___________________________________________________________________
Name: svn:ignore
   - installation_guide
faq
machine_learning
programmers_guide
python_modules_html
tools_guide
users_guide
tutonly
LibraryReference-No-Dot
LibraryReference
LibraryReference-No-Source
*.aux
*.bbl
*.blg
*.dvi
*.log
*.out
*.ps
*.pdf
*.toc


   + installation_guide
faq
tmp
machine_learning
programmers_guide
python_modules_html
tools_guide
users_guide
tutonly
LibraryReference-No-Dot
LibraryReference
LibraryReference-No-Source
*.dvi
*.ps
*.pdf





From simonl at mail.berlios.de  Fri Oct 12 20:25:34 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Fri, 12 Oct 2007 20:25:34 +0200
Subject: [Plearn-commits] r8179 - trunk/plearn/var/EXPERIMENTAL
Message-ID: <200710121825.l9CIPYV8025180@sheep.berlios.de>

Author: simonl
Date: 2007-10-12 20:25:33 +0200 (Fri, 12 Oct 2007)
New Revision: 8179

Added:
   trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.cc
   trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.h
Log:
Stochastic sampling variable...


Added: trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.cc	2007-10-12 13:59:25 UTC (rev 8178)
+++ trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.cc	2007-10-12 18:25:33 UTC (rev 8179)
@@ -0,0 +1,219 @@
+// -*- C++ -*-
+
+// MultiSampleVariable.cc
+//
+// Copyright (C) 2007 Simon Lemieux, Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Simon Lemieux, Pascal Vincent
+
+/*! \file MultiSampleVariable.cc */
+
+
+#include "MultiSampleVariable.h"
+
+namespace PLearn {
+using namespace std;
+
+/** MultiSampleVariable **/
+
+PLEARN_IMPLEMENT_OBJECT(
+    MultiSampleVariable,
+    "Different max variables done on separate groups of the input",
+    "This variables samples" 
+    "\non subvectors of the input, which lengths are defined by the field groupsize"
+    "\n"
+    );
+
+
+//! Constructor
+
+MultiSampleVariable::MultiSampleVariable(Variable* input, int groupsize)
+    : inherited(input, input->length(), input->width()),
+      groupsize(groupsize),
+      random_gen(NULL)
+{
+    build_();
+}
+
+void MultiSampleVariable::recomputeSize(int& l, int& w) const
+{
+    if (input) {
+        l = input->length();
+        w = input->width() ;
+    } else
+        l = w = 0;
+}
+
+// ### computes value from input's value
+void MultiSampleVariable::fprop()
+{
+    int k;
+    Mat inputValue = input->matValue;
+
+    Vec inputValue_n;
+    Vec value_n;
+
+    for(int n=0; n<inputValue.length(); n++)
+    {
+        k=0;
+        inputValue_n = inputValue(n);
+        value_n = matValue(n);
+
+        //we set all values to 0. before sampling "ones"
+        for (int i=0; i<value_n.length(); i++)
+            value_n[i]=0.;
+        
+        while ( k < this->width() )
+        {            
+            sample_range(inputValue_n, value_n, k, groupsize);         
+            k+=groupsize;
+        }
+    }
+}
+
+// ### computes input's gradient from gradient
+void MultiSampleVariable::bprop()
+{}    
+// ### You can implement these methods:
+// void MultiSampleVariable::bbprop() {}
+// void MultiSampleVariable::symbolicBprop() {}
+// void MultiSampleVariable::rfprop() {}
+
+
+// ### Nothing to add here, simply calls build_
+void MultiSampleVariable::build()
+{
+    inherited::build();
+    build_();
+}
+
+void MultiSampleVariable::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    //deepCopyField(groupsizes, copies);
+    // ### If you want to deepCopy a Var field:
+    // varDeepCopyField(somevariable, copies);   
+}
+
+void MultiSampleVariable::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    //declareOption(ol, "groupsizes", &MultiSampleVariable::groupsizes,
+    //              OptionBase::buildoption,
+    //              "this tells how to \"divide\" our diffrents inputs\nex: groupsizes = [1,2,3] says we divide our output like this :\n[x1],[x2,x3],[x4,x5,x6] and apply a maximum algorithm on each group separately");
+
+    declareOption(ol, "groupsize", &MultiSampleVariable::groupsize,
+                  OptionBase::buildoption,
+                  "shortcut if you want all groupsizes to be equals, for example if you set the value of this option to be 3, it will make groupsizes = [3,3,...,3]");   
+
+    declareOption(ol, "random_gen", &MultiSampleVariable::random_gen,
+                  OptionBase::buildoption,
+                  "random generator");
+            
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void MultiSampleVariable::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+    
+    if (groupsize <= 0)
+        PLERROR("Groupsize(s) not specified or invalid in MultiSampleVariable");    
+    if (input->width() % groupsize != 0)
+        PLERROR("Invalid groupsize in MultiSampleVariable (%i does not divide %i)", groupsize, input->width());
+
+    
+    if(random_gen == NULL)
+        random_gen = new PRandom();
+    
+}
+
+
+////////////////
+// some utils //
+////////////////
+
+void MultiSampleVariable::sample_range(Vec &x, Vec &y, int start, int length)
+{
+    if(length != 1)
+    {
+        y[start+random_gen->multinomial_sample(x.subVec(start,length))] = 1;
+    }
+    else // if groupsize == 1
+    {
+        Vec temp(2);
+        temp[0] = 1.-x[start];
+        temp[1] = temp[0];
+        y[start] = random_gen->multinomial_sample(temp);
+    }
+}
+
+
+
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.h
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.h	2007-10-12 13:59:25 UTC (rev 8178)
+++ trunk/plearn/var/EXPERIMENTAL/MultiSampleVariable.h	2007-10-12 18:25:33 UTC (rev 8179)
@@ -0,0 +1,165 @@
+// -*- C++ -*-
+
+// MultiSampleVariable.h
+//
+// Copyright (C) 2007 Simon Lemieux, Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Simon Lemieux, Pascal Vincent
+
+/*! \file MultiSampleVariable.h */
+
+
+#ifndef MultiSampleVariable_INC
+#define MultiSampleVariable_INC
+
+#include <plearn/var/UnaryVariable.h>
+#include <plearn/math/PRandom.h>
+
+namespace PLearn {
+using namespace std;
+
+/*! * MultiSampleVariable * */
+
+/**
+ * 
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class MultiSampleVariable : public UnaryVariable
+{
+    typedef UnaryVariable inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+    int groupsize;
+
+    PP<PRandom> random_gen;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor, usually does nothing
+    MultiSampleVariable():
+         groupsize(-1)
+    {}
+
+
+    // ### If your class has parameters, you probably want a constructor that
+    // ### initializes them
+    MultiSampleVariable(Variable* input, int groupsize);
+
+    // Your other public member functions go here
+
+    //#####  PLearn::Variable methods #########################################
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void recomputeSize(int& l, int& w) const;
+    virtual void fprop();
+    virtual void bprop();
+
+    // ### These ones are not always implemented
+    // virtual void bbprop();
+    // virtual void symbolicBprop();
+    // virtual void rfprop();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(MultiSampleVariable);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+    //somes utils    
+    void sample_range(Vec &x, Vec &y, int start, int length);
+    
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(MultiSampleVariable);
+
+// ### Put here a convenient method for building your variable.
+// ### e.g., if your class is TotoVariable, with two parameters foo_type foo
+// ### and bar_type bar, you could write:
+// inline Var MultiSample(Var v, TVec<int> groupsizes, char computation_type)
+// { return new MultiSampleVariable(v, groupsizes, computation_type); }
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Fri Oct 12 22:22:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 12 Oct 2007 22:22:53 +0200
Subject: [Plearn-commits] r8180 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200710122022.l9CKMruC000313@sheep.berlios.de>

Author: nouiz
Date: 2007-10-12 22:22:53 +0200 (Fri, 12 Oct 2007)
New Revision: 8180

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h
Log:
better readability: put global variable local when used only locally


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc	2007-10-12 18:25:33 UTC (rev 8179)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.cc	2007-10-12 20:22:53 UTC (rev 8180)
@@ -36,8 +36,6 @@
 
 /*! \file AnalyzeDond2DiscreteVariables.cc */
 
-#define PL_LOG_MODULE_NAME "AnalyzeDond2DiscreteVariables"
-#include <plearn/io/pl_log.h>
 
 #include "AnalyzeDond2DiscreteVariables.h"
 
@@ -106,7 +104,6 @@
 ////////////
 void AnalyzeDond2DiscreteVariables::build_()
 {
-    MODULE_LOG << "build_() called" << endl;
     if (train_set)
     {
         analyzeDiscreteVariable();
@@ -117,19 +114,17 @@
 void AnalyzeDond2DiscreteVariables::analyzeDiscreteVariable()
 {    
     // initialize primary dataset
-    main_row = 0;
-    main_col = 0;
-    main_length = train_set->length();
-    main_width = train_set->width();
-    main_input.resize(main_width);
-    main_names.resize(main_width);
+    int main_length = train_set->length();
+    int main_width = train_set->width();
+    Vec main_input(main_width);
+    TVec<string> main_names(main_width);
     main_names << train_set->fieldNames();
     
     // check for valid options
-    number_of_values = values_to_analyze.size();
-    variable_col = -1;
-    target_col = -1;
-    for (main_col = 0; main_col < main_width; main_col++)
+    int number_of_values = values_to_analyze.size();
+    int variable_col = -1;
+    int target_col = -1;
+    for (int main_col = 0; main_col < main_width; main_col++)
     {
         if (variable_name == main_names[main_col]) variable_col = main_col;
         if (target_name == main_names[main_col]) target_col = main_col;
@@ -139,27 +134,27 @@
     if (number_of_values <= 0) PLERROR("In AnalyzeDond2DiscreteVariables: invalid values_to_analyze");
     
     // initialize working variables
-    value_target_sum.resize(number_of_values);
-    value_present_count.resize(number_of_values);
+    Vec value_target_sum(number_of_values);
+    Vec value_present_count(number_of_values);
     value_target_sum.clear();
     value_present_count.clear();
-    target_sum = 0.0;
-    target_squared_sum = 0.0;
-    variable_present_count = 0.0;
+    real target_sum = 0.0;
+    real target_squared_sum = 0.0;
+    real variable_present_count = 0.0;
     
     //Now, we can process the discrete variable.
     ProgressBar* pb = 0;
     pb = new ProgressBar( "Analyzing discrete variable " + variable_name, main_length);
-    for (main_row = 0; main_row < main_length; main_row++)
+    for (int main_row = 0; main_row < main_length; main_row++)
     {
         train_set->getRow(main_row, main_input);
-        variable_value = main_input[variable_col];
+        real variable_value = main_input[variable_col];
         if (is_missing(variable_value)) continue;
-        target_value = main_input[target_col];
+        real target_value = main_input[target_col];
         target_sum += target_value;
         target_squared_sum += target_value * target_value;
         variable_present_count += 1.0;
-        for (value_col = 0; value_col < number_of_values; value_col++)
+        for (int value_col = 0; value_col < number_of_values; value_col++)
         {
             if (variable_value < values_to_analyze[value_col].first || variable_value > values_to_analyze[value_col].second) continue;
             value_target_sum[value_col] += target_value;
@@ -173,16 +168,16 @@
         cout << "In AnalyzeDond2DiscreteVariables: no value present for this variable" << endl;
         return;
     }
-    target_mean = target_sum / variable_present_count;
+    real target_mean = target_sum / variable_present_count;
     cout << "In AnalyzeDond2DiscreteVariables, for variable:  " << variable_name << endl;
     cout << variable_present_count << " values are present out of " << main_length << " samples." << endl;
-    for (value_col = 0; value_col < number_of_values; value_col++)
+    for (int value_col = 0; value_col < number_of_values; value_col++)
     {
-        ssxy = value_target_sum[value_col] - value_present_count[value_col] * target_mean;
-        ss2xy = ssxy * ssxy;
-        ssxx = value_present_count[value_col] * (1.0 -  value_present_count[value_col] / variable_present_count);
-        ssyy = target_squared_sum - target_sum * target_mean;
-        correlation_coefficient = ss2xy / (ssxx * ssyy);
+        real ssxy = value_target_sum[value_col] - value_present_count[value_col] * target_mean;
+        real ss2xy = ssxy * ssxy;
+        real ssxx = value_present_count[value_col] * (1.0 -  value_present_count[value_col] / variable_present_count);
+        real ssyy = target_squared_sum - target_sum * target_mean;
+        real correlation_coefficient = ss2xy / (ssxx * ssyy);
         cout << "For value from: " << values_to_analyze[value_col].first << " to: " << values_to_analyze[value_col].second 
              << " occurence: " << value_present_count[value_col] << " correlation coefficient: " << correlation_coefficient << endl;
     }

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h	2007-10-12 18:25:33 UTC (rev 8179)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/AnalyzeDond2DiscreteVariables.h	2007-10-12 20:22:53 UTC (rev 8180)
@@ -117,33 +117,6 @@
 
     // The rest of the private stuff goes here
     
-    // input instructions variables
-    int value_col;
-    int number_of_values;
-    int variable_col;
-    int target_col;
-    Vec value_target_sum;
-    Vec value_present_count;
-    real target_sum;
-    real target_squared_sum;
-    real variable_present_count;
-    real ssxy;
-    real ss2xy;
-    real ssxx;
-    real ssyy;
-    real correlation_coefficient;
-    
-    // primary dataset variables
-    int main_length;
-    int main_width;
-    int main_row;
-    int main_col;
-    Vec main_input;
-    TVec<string> main_names;
-    real variable_value;
-    real target_value;
-    real target_mean;
-    
 };
 
 // Declares a few other classes and functions related to this class



From nouiz at mail.berlios.de  Fri Oct 12 22:25:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 12 Oct 2007 22:25:06 +0200
Subject: [Plearn-commits] r8181 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200710122025.l9CKP62c000503@sheep.berlios.de>

Author: nouiz
Date: 2007-10-12 22:25:06 +0200 (Fri, 12 Oct 2007)
New Revision: 8181

Removed:
   branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h
Log:
remove duplicate class


Deleted: branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc	2007-10-12 20:22:53 UTC (rev 8180)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.cc	2007-10-12 20:25:06 UTC (rev 8181)
@@ -1,327 +0,0 @@
-// -*- C++ -*-
-
-// GaussianizeVMatrix.cc
-//
-// Copyright (C) 2006 Olivier Delalleau
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Olivier Delalleau
-
-/*! \file GaussianizeVMatrix.cc */
-
-
-#include "GaussianizeVMatrix.h"
-#include <plearn/math/pl_erf.h>
-#include <plearn/vmat/VMat_computeStats.h>
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    GaussianizeVMatrix,
-    "Transforms its source VMatrix so that its features look Gaussian.",
-
-    "This VMat transforms the features of its source that are obviously non-\n"
-    "Gaussian, i.e. when the difference between the maximum and minimum\n"
-    "value is too large compared to the standard deviation (the meaning of\n"
-    "'too large' being controlled by the 'threshold_ratio' option).\n"
-    "\n"
-    "When this happens, the values of a features are sorted and their rank\n"
-    "is used to transform them through the inverse cumulative of a normal\n"
-    "Gaussian, resulting on a distribution that actually looks Gaussian.\n"
-    "Note that, unless specified otherwise through the options, only the\n"
-    "input features are transformed.\n"
-    "\n"
-    "An additional 'train_source' VMat can also be specified in order to\n"
-    "transform new data (in the 'source' option) while the transformation\n"
-    "parameters are learned on a fixed 'train_source' VMat (e.g. when new\n"
-    "test data are obtained and need to be properly Gaussianized).\n"
-);
-
-////////////////////////
-// GaussianizeVMatrix //
-////////////////////////
-GaussianizeVMatrix::GaussianizeVMatrix():
-    gaussianize_input(true),
-    gaussianize_target(false),
-    gaussianize_weight(false),
-    gaussianize_extra(false),
-    threshold_ratio(10)
-{}
-
-////////////////////
-// declareOptions //
-////////////////////
-void GaussianizeVMatrix::declareOptions(OptionList& ol)
-{
-    declareOption(ol, "threshold_ratio", &GaussianizeVMatrix::threshold_ratio,
-                                         OptionBase::buildoption,
-        "A source's feature will be Gaussianized when the following holds:\n"
-        "(max - min) / stddev > threshold_ratio.");
-
-    declareOption(ol, "gaussianize_input",
-                  &GaussianizeVMatrix::gaussianize_input,
-                  OptionBase::buildoption,
-        "Whether or not to Gaussianize the input part.");
-
-    declareOption(ol, "gaussianize_target",
-                  &GaussianizeVMatrix::gaussianize_target,
-                  OptionBase::buildoption,
-        "Whether or not to Gaussianize the target part.");
-
-    declareOption(ol, "gaussianize_weight",
-                  &GaussianizeVMatrix::gaussianize_weight,
-                  OptionBase::buildoption,
-        "Whether or not to Gaussianize the weight part.");
-
-    declareOption(ol, "gaussianize_extra",
-                  &GaussianizeVMatrix::gaussianize_extra,
-                  OptionBase::buildoption,
-        "Whether or not to Gaussianize the extra part.");
-
-    declareOption(ol, "excluded_fields",
-                  &GaussianizeVMatrix::excluded_fields,
-                  OptionBase::buildoption,
-        "A list of fields to exclude from the process by field name.");
-
-    declareOption(ol, "train_source", &GaussianizeVMatrix::train_source,
-                                      OptionBase::buildoption,
-        "An optional VMat that will be used instead of 'source' to compute\n"
-        "the transformation parameters from the distribution statistics.");
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-///////////
-// build //
-///////////
-void GaussianizeVMatrix::build()
-{
-    inherited::build();
-    build_();
-}
-
-////////////
-// build_ //
-////////////
-void GaussianizeVMatrix::build_()
-{
-    if (!source)
-        return;
-
-    if (train_source) {
-        assert( train_source->width() == source->width() );
-        assert( train_source->inputsize()  == source->inputsize() &&
-                train_source->targetsize() == source->targetsize() &&
-                train_source->weightsize() == source->weightsize() &&
-                train_source->extrasize()  == source->extrasize() );
-    }
-
-    VMat the_source = train_source ? train_source : source;
-
-    assert( the_source->inputsize() >= 0 && the_source->targetsize() >= 0 &&
-            the_source->weightsize() >= 0 && the_source->extrasize() >= 0 );
-
-    // Find which excluded_fields to exclude
-    int the_source_col;
-    int the_source_width = the_source->width();
-    TVec<string> the_source_names(the_source_width);
-    the_source_names << the_source->fieldNames();
-    int excluded_fields_col;
-    int excluded_fields_width = excluded_fields.size();
-    TVec<int> excluded_fields_selected(the_source_width);
-    excluded_fields_selected.clear();
-    for (excluded_fields_col = 0; excluded_fields_col < excluded_fields_width; excluded_fields_col++)
-    {
-        for (the_source_col = 0; the_source_col < the_source_width; the_source_col++)
-        {
-            if (excluded_fields[excluded_fields_col] == the_source_names[the_source_col]) break;
-        }
-        if (the_source_col >= the_source_width)
-            PLERROR("In GaussianizeVMatrix: no field with this name in input dataset: %s", (excluded_fields[excluded_fields_col]).c_str());
-        excluded_fields_selected[the_source_col] = 1;
-    }
-
-
-    // Find which dimensions to Gaussianize.
-    features_to_gaussianize.resize(0);
-    int col = 0;
-    if (gaussianize_input)
-        features_to_gaussianize.append(
-                TVec<int>(col, col + the_source->inputsize() - 1, 1));
-    col += the_source->inputsize();
-    if (gaussianize_target)
-        features_to_gaussianize.append(
-                TVec<int>(col, col + the_source->targetsize() - 1, 1));
-    col += the_source->targetsize();
-    if (gaussianize_weight)
-        features_to_gaussianize.append(
-                TVec<int>(col, col + the_source->weightsize() - 1, 1));
-    col += the_source->weightsize();
-    if (gaussianize_extra)
-        features_to_gaussianize.append(
-                TVec<int>(col, col + the_source->extrasize() - 1, 1));
-    col += the_source->extrasize();
-
-    // Compute source statistics.
-    TVec<StatsCollector> stats = PLearn::computeStats(the_source, -1, false);
-
-    // See which dimensions violate the Gaussian assumption and will be
-    // actually Gaussianized, and store the corresponding list of values.
-    TVec<int> candidates = features_to_gaussianize.copy();
-    features_to_gaussianize.resize(0);
-    counts.resize(0);
-    Vec row(2);
-    for (int i = 0; i < candidates.length(); i++) {
-        int j = candidates[i];
-        StatsCollector& stat = stats[j];
-        if (excluded_fields_selected[j] > 0)
-            continue;
-        if (fast_exact_is_equal(stat.stddev(), 0))
-            continue;
-        if ((stat.max() - stat.min()) > threshold_ratio * stat.stddev()) {
-            features_to_gaussianize.append(j);
-            counts.append(Mat());
-            Mat& counts_j = counts.lastElement();
-            // We use a dummy iterator to get rid of the last element in the
-            // counts, which is the max real value.
-            map<real, StatsCollectorCounts>::const_iterator it, it_dummy;
-            map<real,StatsCollectorCounts>* count_map = stat.getCounts();
-            it_dummy = count_map->begin();
-            it_dummy++;
-            int count_values = 0;
-            for (it = count_map->begin(); it_dummy != count_map->end();
-                                          it++, it_dummy++)
-            {
-                row[0] = it->first;
-                row[1] = count_values;
-                count_values += (int) it->second.n;
-                counts_j.appendRow(row);
-            }
-            // This scales the ranks so that they are between 0 and 1.
-            counts_j.column(1) /= row[1];
-        }
-    }
-
-    // Obtain meta information from source.
-    setMetaInfoFromSource();
-}
-
-///////////////
-// getNewRow //
-///////////////
-void GaussianizeVMatrix::getNewRow(int i, const Vec& v) const
-{
-    assert( source );
-    source->getRow(i, v);
-    for (int k = 0; k < features_to_gaussianize.length(); k++) {
-        int j = features_to_gaussianize[k];
-        real current_val = v[j];
-        if (is_missing(current_val))
-            continue;
-        // Find closest value in the training data.
-        Mat& counts_j = counts[k];
-        real closest;
-        if (current_val < counts_j(0, 0)) {
-            // Smaller than the minimum.
-            closest = 0;
-        } else if (current_val > counts_j(counts_j.length() - 1, 0)) {
-            // Higher than the maximum.
-            closest = 1;
-        } else {
-            int min = 0;
-            int max = counts_j.length() - 1;
-            while (max - min > 1) {
-                int mid = (max + min) / 2;
-                real mid_val = counts_j(mid, 0);
-                if (current_val < mid_val)
-                    max = mid;
-                else if (current_val > mid_val)
-                    min = mid;
-                else {
-                    // Found the exact value.
-                    min = max = mid;
-                }
-            }
-            if (min == max)
-                closest = counts_j(min, 1);
-            else {
-                assert( max - min == 1 );
-                if (fabs(current_val - counts_j(min, 0)) <
-                    fabs(current_val - counts_j(max, 0)))
-                {
-                    closest = counts_j(min, 1);
-                } else
-                    closest = counts_j(max, 1);
-            }
-        }
-        assert( closest >= 0 && closest <= 1 );
-        // The expectation of the minimum and maximum of n numbers taken from a
-        // uniform(0,1) distribution are respectively 1/n+1 and n/n+1: we shift
-        // and rescale 'closest' to be in [1/n+1, n/n+1] before using the
-        // inverse of the Gaussian cumulative function.
-        real n = counts_j.length();
-        closest = (n - 1) / (n + 1) * closest + 1 / (n + 1);
-        v[j] = gauss_01_quantile(closest);
-    }
-}
-
-/////////////////////////////////
-// makeDeepCopyFromShallowCopy //
-/////////////////////////////////
-void GaussianizeVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("GaussianizeVMatrix::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h	2007-10-12 20:22:53 UTC (rev 8180)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/GaussianizeVMatrix.h	2007-10-12 20:25:06 UTC (rev 8181)
@@ -1,160 +0,0 @@
-// -*- C++ -*-
-
-// GaussianizeVMatrix.h
-//
-// Copyright (C) 2006 Olivier Delalleau
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Olivier Delalleau
-
-/*! \file GaussianizeVMatrix.h */
-
-
-#ifndef GaussianizeVMatrix_INC
-#define GaussianizeVMatrix_INC
-
-#include "ImputationVMatrix.h"
-
-namespace PLearn {
-
-/**
- * The first sentence should be a BRIEF DESCRIPTION of what the class does.
- * Place the rest of the class programmer documentation here.  Doxygen supports
- * Javadoc-style comments.  See http://www.doxygen.org/manual.html
- *
- * @todo Write class to-do's here if there are any.
- *
- * @deprecated Write deprecated stuff here if there is any.  Indicate what else
- * should be used instead.
- */
-class GaussianizeVMatrix : public SourceVMatrix
-{
-    typedef SourceVMatrix inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-    //! ### declare public option fields (such as build options) here
-    //! Start your comments with Doxygen-compatible comments such as //!
-
-    bool gaussianize_input;
-    bool gaussianize_target;
-    bool gaussianize_weight;
-    bool gaussianize_extra;
-    real threshold_ratio;
-    TVec<string> excluded_fields;
-    VMat train_source;
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    // ### Make sure the implementation in the .cc
-    // ### initializes all fields to reasonable default values.
-    GaussianizeVMatrix();
-
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
-    PLEARN_DECLARE_OBJECT(GaussianizeVMatrix);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-protected:
-
-    //! List of features that need to be Gaussianized.
-    TVec<int> features_to_gaussianize;
-
-    //! Scaling factor to map the rank to [0,1].
-    Vec scaling_factor;
-
-    //! The j-th element is a two-column matrix whose first row is the (sorted)
-    //! list of values appearing in the variable features_to_gaussianize[j],
-    //! and the second row indicates how many occurences are strictly below
-    //! each of these values (i.e. gives their rank).
-    TVec<Mat> counts;
-
-    //#####  Protected Options  ###############################################
-
-    // ### Declare protected option fields (such as learned parameters) here
-    // ...
-
-protected:
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
-    static void declareOptions(OptionList& ol);
-
-    //! Fill the vector 'v' with the content of the i-th row.
-    //! v is assumed to be the right size.
-    //! ### This function must be overridden in your class
-    virtual void getNewRow(int i, const Vec& v) const;
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    // (PLEASE IMPLEMENT IN .cc)
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(GaussianizeVMatrix);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From manzagop at mail.berlios.de  Mon Oct 15 17:56:46 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 15 Oct 2007 17:56:46 +0200
Subject: [Plearn-commits] r8182 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710151556.l9FFuk36015857@sheep.berlios.de>

Author: manzagop
Date: 2007-10-15 17:56:46 +0200 (Mon, 15 Oct 2007)
New Revision: 8182

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Removed code related to the "pv" gradient technique. This code is now in PvGradNNet.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-10-12 20:25:06 UTC (rev 8181)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-10-15 15:56:46 UTC (rev 8182)
@@ -74,18 +74,6 @@
       target_stdev_activation(3), // 2.5% of the time we are above 1
       //corr_profiling_start(0), 
       //corr_profiling_end(0),
-      use_pvgrad(false),
-      // Next 5 values inspired those used in the '94 rprop tech report
-      // but we are in stochastic case
-      pv_initial_stepsize(1e-1),
-      pv_min_stepsize(1e-6),
-      pv_max_stepsize(50.0),
-      pv_acceleration(1.2),
-      pv_deceleration(0.5),
-      pv_min_samples(2),
-      pv_required_confidence(0.80),
-      pv_random_sample_step(false),
-      pv_gradstats(new VecStatsCollector()),
       n_layers(-1),
       cumulative_training_time(0)
 {
@@ -284,57 +272,6 @@
     //              "Stage to end the profiling of the gradients' and the\n"
     //              "natural gradients' correlations.\n");
 
-    declareOption(ol, "use_pvgrad",
-                  &NatGradNNet::use_pvgrad,
-                  OptionBase::buildoption,
-                  "Use Pascal Vincent's gradient technique.\n"
-                  "All options specific to this technique start with pv_...\n"
-                  "This is currently very experimental. Current code is \n"
-                  "NOT YET optimised for speed (nor supports minibatch).");
-
-    declareOption(ol, "pv_initial_stepsize",
-                  &NatGradNNet::pv_initial_stepsize,
-                  OptionBase::buildoption,
-                  "Initial size of steps in parameter space");
-
-    declareOption(ol, "pv_min_stepsize",
-                  &NatGradNNet::pv_min_stepsize,
-                  OptionBase::buildoption,
-                  "Minimal size of steps in parameter space");
-
-    declareOption(ol, "pv_max_stepsize",
-                  &NatGradNNet::pv_max_stepsize,
-                  OptionBase::buildoption,
-                  "Maximal size of steps in parameter space");
-
-    declareOption(ol, "pv_acceleration",
-                  &NatGradNNet::pv_acceleration,
-                  OptionBase::buildoption,
-                  "Coefficient by which to multiply the step sizes.");
-
-    declareOption(ol, "pv_deceleration",
-                  &NatGradNNet::pv_deceleration,
-                  OptionBase::buildoption,
-                  "Coefficient by which to multiply the step sizes.");
-
-    declareOption(ol, "pv_min_samples",
-                  &NatGradNNet::pv_min_samples,
-                  OptionBase::buildoption,
-                  "PV's minimum number of samples to estimate gradient sign.\n"
-                  "This should at least be 2.");
-
-    declareOption(ol, "pv_required_confidence",
-                  &NatGradNNet::pv_required_confidence,
-                  OptionBase::buildoption,
-                  "Minimum required confidence (probability of being positive or negative) for taking a step.");
-
-    declareOption(ol, "pv_random_sample_step",
-                  &NatGradNNet::pv_random_sample_step,
-                  OptionBase::buildoption,
-                  "If this is set to true, then we will randomly choose the step sign\n"
-                  "for each parameter based on the estimated probability of it being\n"
-                  "positive or negative.");
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -367,9 +304,6 @@
     if( output_layer_L1_penalty_factor < 0. )
         PLWARNING("NatGradNNet::build_ - output_layer_L1_penalty_factor is negative!\n");
 
-    if(use_pvgrad && minibatch_size!=1)
-        PLERROR("PV's gradient technique (triggered by use_pvgrad): support for minibatch not yet implemented (must have minibatch_size=1)");
-    
     while (hidden_layer_sizes.length()>0 && hidden_layer_sizes[hidden_layer_sizes.length()-1]==0)
         hidden_layer_sizes.resize(hidden_layer_sizes.length()-1);
     n_layers = hidden_layer_sizes.length()+2;
@@ -560,12 +494,6 @@
     deepCopyField(group_params_delta, copies);
     deepCopyField(layer_params_delta, copies);
 
-    deepCopyField(pv_gradstats, copies);
-    deepCopyField(pv_all_stepsizes, copies);
-    deepCopyField(pv_all_stepsigns, copies);
-    deepCopyField(pv_all_intstepsigns, copies);
-    //deepCopyField(pv_all_nsamples, copies); // *stat*
-
 /*
     deepCopyField(, copies);
 */
@@ -597,34 +525,6 @@
     if (params_averaging_coeff!=1.0)
         all_mparams << all_params;
     
-    if(use_pvgrad)
-    {
-        pv_gradstats->forget();
-        int n = all_params.length();
-        pv_all_stepsizes.resize(n);
-        pv_all_stepsizes.fill(pv_initial_stepsize);
-        pv_all_stepsigns.resize(n);
-        pv_all_stepsigns.fill(true);    // TODO should be init to undetermined
-        pv_all_intstepsigns.resize(n);
-        pv_all_intstepsigns.fill(0);
-        //pv_all_nsamples.resize(n);    // *stat*
-
-        // Get some structure on the previous Vecs
-        pv_layer_stepsizes.resize(n_layers-1);
-        pv_layer_stepsigns.resize(n_layers-1);
-        pv_layer_intstepsigns.resize(n_layers-1);
-        //pv_layer_nsamples.resize(n_layers-1); // *stat*
-        for (int i=0,p=0;i<n_layers-1;i++)
-        {
-            int np=layer_sizes[i+1]*(1+layer_sizes[i]);
-            pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-            pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-            pv_layer_intstepsigns[i]=pv_all_intstepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
-            //pv_layer_nsamples[i]=pv_all_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);   // *stat*
-            p+=np;
-        }
-    }
-
     // *stat*
     /*if( pa_gradstats.length() == 0 )    {
         pa_gradstats.resize(noutputs);
@@ -673,7 +573,7 @@
     Vec costs = costs_plus_time.subVec(0,train_costs.width());
     int nsamples = train_set->length();
 
-    // *stat* - Need some stats for pvgrad analysis
+    // *stat* - Need some stats for grad analysis
     //sum_gradient_norms = 0.0;
     //all_params_cum_gradient.fill(0.0);
     
@@ -730,7 +630,7 @@
     //    ng_corrprof->printAndReset();
     //}
 
-    // *stat* - Need some stats for pvgrad analysis
+    // *stat* - Need some stats for grad analysis
     // The SGrad stats include the learning rate.
     //cout << "sum_gradient_norms " << sum_gradient_norms 
     //     << " norm(all_params_cum_gradient,2.0) " << norm(all_params_cum_gradient,2.0) << endl;
@@ -813,13 +713,8 @@
             }
         }
         // compute gradient on parameters, possibly update them
-        if (use_pvgrad)
+        if (full_natgrad || params_natgrad_template || params_natgrad_per_input_template) 
         {
-            productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
-                            neuron_extended_outputs_per_layer[i-1],false,1,0);
-        }
-        else if (full_natgrad || params_natgrad_template || params_natgrad_per_input_template) 
-        {
 //alternate
             if( params_natgrad_per_input_template && i==1 ){ // parameters are transposed
                 Profiler::pl_profile_start("ProducScaleAccOnlineStep");
@@ -857,12 +752,8 @@
                             1,0);*/
         }
     }
-    if (use_pvgrad)
+    if (full_natgrad)
     {
-        pvGradUpdate();
-    }
-    else if (full_natgrad)
-    {
         (*full_natgrad)(t/minibatch_size,all_params_gradient,all_params_delta); // compute update direction by natural gradient
         if (output_layer_lrate_scale!=1.0)
             layer_params_delta[n_layers-2] *= output_layer_lrate_scale; // scale output layer's learning rate
@@ -934,126 +825,6 @@
 */
 }
 
-void NatGradNNet::paGradUpdate()
-{
-
-}
-
-void NatGradNNet::pvGradUpdate()
-{
-    int np = all_params_gradient.length();
-    if(pv_all_stepsizes.length()==0)
-    {
-        pv_all_stepsizes.resize(np);
-        pv_all_stepsizes.fill(pv_initial_stepsize);
-        pv_all_stepsigns.resize(np);
-        pv_all_stepsigns.fill(true);
-        pv_all_intstepsigns.resize(np);
-        pv_all_intstepsigns.fill(0);
-        //pv_all_nsamples.resize(np);   // *stat*
-    }
-    pv_gradstats->update(all_params_gradient);
-
-    // *stat* - Need some stats for pvgrad analysis
-    //real gradient_squared_sum = 0.0;
-
-    for(int k=0; k<np; k++)
-    {
-        StatsCollector& st = pv_gradstats->getStats(k);
-        int ns = (int)st.nnonmissing();
-        if(ns>pv_min_samples)
-        {
-            real m = st.mean();
-            real e = st.stderror();
-
-            // test to see if solve numerical problems
-            if( fabs(m) < 1e-15 || e < 1e-15 )  {
-                cout << "small mean and error ratio." << endl;
-                continue;
-            }
-
-            real prob_pos = gauss_01_cum(m/e);
-            real prob_neg = 1.-prob_pos;
-            if(!pv_random_sample_step)
-            {
-                // We adapt the stepsize before taking the step
-                // gradient is positive
-                if(prob_pos>=pv_required_confidence)
-                {
-                    pv_all_stepsizes[k] *= (pv_all_stepsigns[k]?pv_acceleration:pv_deceleration);
-                    if( pv_all_stepsizes[k] > pv_max_stepsize )
-                        pv_all_stepsizes[k] = pv_max_stepsize;
-                    else if( pv_all_stepsizes[k] < pv_min_stepsize )
-                        pv_all_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] -= pv_all_stepsizes[k];
-                    pv_all_stepsigns[k] = true;
-                    pv_all_intstepsigns[k] = -1;
-                    st.forget();
-
-                    // *stat* - Need some stats for pvgrad analysis
-                    //gradient_squared_sum += pv_all_stepsizes[k]*pv_all_stepsizes[k];
-                    //all_params_cum_gradient[k] -= pv_all_stepsizes[k];
-
-                }
-                // gradient is negative
-                else if(prob_neg>=pv_required_confidence)
-                {
-                    pv_all_stepsizes[k] *= ((!pv_all_stepsigns[k])?pv_acceleration:pv_deceleration);
-                    if( pv_all_stepsizes[k] > pv_max_stepsize )
-                        pv_all_stepsizes[k] = pv_max_stepsize;
-                    else if( pv_all_stepsizes[k] < pv_min_stepsize )
-                        pv_all_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] += pv_all_stepsizes[k];
-                    pv_all_stepsigns[k] = false;
-                    pv_all_intstepsigns[k] = 1;
-                    st.forget();
-
-                    // *stat* - Need some stats for pvgrad analysis
-                    //gradient_squared_sum += pv_all_stepsizes[k]*pv_all_stepsizes[k];
-                    //all_params_cum_gradient[k] += pv_all_stepsizes[k];
-                
-                }   
-                // no step
-                else    {
-                    pv_all_intstepsigns[k] = 0;
-                }
-            }
-            else  // random sample update direction (sign)
-            {
-                bool ispos = (random_gen->binomial_sample(prob_pos)>0);
-                if(ispos) // picked positive
-                    all_params[k] += pv_all_stepsizes[k];
-                else  // picked negative
-                    all_params[k] -= pv_all_stepsizes[k];
-                pv_all_stepsizes[k] *= (pv_all_stepsigns[k]==ispos) ?pv_acceleration :pv_deceleration;
-                pv_all_stepsigns[k] = ispos;
-                st.forget();
-            }
-        }
-        //pv_all_nsamples[k] = ns; // *stat*
-    }
-
-    // *stat* - Need some stats for pvgrad analysis
-    //sum_gradient_norms += sqrt( gradient_squared_sum );
-
-    // Ouput for profiling: step sizes and number of samples
-    // horribly inefficient!
-/*    ofstream fd_ss;
-    fd_ss.open("step_sizes.txt", ios::app);
-    fd_ss << pv_layer_stepsizes[0](0) << " " << pv_layer_stepsizes[1](0) << endl;
-    fd_ss.close();
-    ofstream fd_ns;
-    fd_ns.open("nsamples.txt", ios::app);
-    fd_ns << pv_layer_nsamples[0](0) << " " << pv_layer_nsamples[1](0) << endl;
-    fd_ns.close();
-    ofstream fd_ssgn;
-    fd_ssgn.open("step_signs.txt", ios::app);
-    fd_ssgn << pv_layer_intstepsigns[0](0) << " " << pv_layer_intstepsigns[1](0) << endl;
-    fd_ssgn.close();
-*/
-
-}
-
 void NatGradNNet::computeOutput(const Vec& input, Vec& output) const
 {
     Profiler::pl_profile_start("NatGradNNet::computeOutput");

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-10-12 20:25:06 UTC (rev 8181)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-10-15 15:56:46 UTC (rev 8182)
@@ -163,55 +163,10 @@
     // *stat* end
 
 public:
-    //*************************************************************
-    //*** Members used for Pascal Vincent's gradient technique  ***
-
-    //! Use Pascal's gradient 
-    bool use_pvgrad;
-
-    //! Initial size of steps in parameter space
-    real pv_initial_stepsize;
-
-    //! Bounds for the step sizes
-    real pv_min_stepsize, pv_max_stepsize;
-
-    //! Coefficients by which to multiply the step sizes  
-    real pv_acceleration, pv_deceleration;
-
-    //! PV's gradient minimum number of samples to estimate confidence
-    int pv_min_samples;
-
-    //! Minimum required confidence (probability of being positive or negative) for taking a step. 
-    real pv_required_confidence;
-
-    //! If this is set to true, then we will randomly choose the step sign for
-    // each parameter based on the estimated probability of it being positive or
-    // negative.
-    bool pv_random_sample_step;
-
-protected:
-    //! accumulated statistics of gradients on each parameter.
-    PP<VecStatsCollector> pv_gradstats;
-
-    //! The step size (absolute value) to be taken for each parameter.
-    Vec pv_all_stepsizes;
-    TVec<Mat> pv_layer_stepsizes;
-
-    //! Indicates whether the previous step was positive (true) or negative (false)
-    TVec<bool> pv_all_stepsigns;
-    TVec< TMat<bool> > pv_layer_stepsigns;
-
-    //! Temporary add-on. Allows an undetermined signed value (zero).
-    TVec<int> pv_all_intstepsigns;
-    TVec< TMat<int> > pv_layer_intstepsigns;
-
-
-public:
     //#####  Public Member Functions  #########################################
 
     NatGradNNet();
 
-
     //#####  PLearner Member Functions  #######################################
 
     //! Returns the size of this learner's output, (which typically
@@ -323,13 +278,7 @@
     //! and write into neuron_gradients_per_layer[n_layers-2], gradient on pre-non-linearity activation
     void fbpropLoss(const Mat& output, const Mat& target, const Vec& example_weights, Mat& train_costs) const;
 
-    //! gradient computation and weight update in Pascal Vincent's gradient technique
-    void pvGradUpdate();
 
-    //! a related idea 
-    void paGradUpdate();
-
-
 private:
     //#####  Private Member Functions  ########################################
 
@@ -358,8 +307,6 @@
     Mat targets; // one target row per example in a minibatch
     Vec example_weights; // one element per example in a minibatch
     Mat train_costs; // one row per example in a minibatch
-
-
     
 };
 



From manzagop at mail.berlios.de  Mon Oct 15 20:31:15 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 15 Oct 2007 20:31:15 +0200
Subject: [Plearn-commits] r8183 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710151831.l9FIVFbt008507@sheep.berlios.de>

Author: manzagop
Date: 2007-10-15 20:31:15 +0200 (Mon, 15 Oct 2007)
New Revision: 8183

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
Log:
-Changed handling of small mean-error ratios. Accumulate counts and do a global
output instead of doing an output for each time.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-15 15:56:46 UTC (rev 8182)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.cc	2007-10-15 18:31:15 UTC (rev 8183)
@@ -212,8 +212,8 @@
 
     // used in the discountGrad() strategy
     n_updates = 0; 
+    n_small_ratios=0.0;
     n_neuron_updates.fill(0);    
-
 //    pv_gradstats->forget();
 }
 
@@ -241,6 +241,12 @@
         default :
             PLERROR("PvGradNNet::bpropUpdateNet() - No such pv_strategy.");
     }
+    // hack
+    if( (t%160000)==0 )  {
+        cout << n_small_ratios << " small ratios." << endl;
+        n_small_ratios = 0.0;
+    }
+        
 }
 
 void PvGradNNet::pvGrad()   
@@ -267,7 +273,8 @@
 
             // test to see if numerical problems
             if( fabs(m) < 1e-15 || e < 1e-15 )  {
-                cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                //cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                n_small_ratios++;
                 continue;
             }
 
@@ -335,7 +342,6 @@
         }
         //pv_all_nsamples[k] = ns; // *stat*
     }
-
 }
 
 //! This gradient strategy is much like the one from PvGrad, however:
@@ -379,7 +385,8 @@
 
             // test to see if numerical problems
             if( fabs(m) < 1e-15 || e < 1e-15 )  {
-                cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                //cout << "PvGradNNet::bpropUpdateNet() - small mean-error ratio." << endl;
+                n_small_ratios++;
                 continue;
             }
 
@@ -514,7 +521,6 @@
                     n_updates++;
                     n_neuron_updates[kk]++;
                 }
-            //////////////////////////////////
             }
         }
     }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-15 15:56:46 UTC (rev 8182)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/PvGradNNet.h	2007-10-15 18:31:15 UTC (rev 8183)
@@ -162,7 +162,7 @@
     //! accumulated statistics of gradients on each parameter.
     //PP<VecStatsCollector> pv_gradstats;
 
-
+    real n_small_ratios;
 };
 
 // Declares a few other classes and functions related to this class



From nouiz at mail.berlios.de  Mon Oct 15 22:09:49 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Oct 2007 22:09:49 +0200
Subject: [Plearn-commits] r8184 - in trunk: commands/PLearnCommands
	plearn/base plearn/dict plearn/ker plearn/math plearn/misc
	plearn/sys plearn/vmat plearn_learners/classifiers
	plearn_learners/nearest_neighbors plearn_learners/online
	plearn_learners/regressors plearn_learners/testers
	plearn_learners/unsupervised
Message-ID: <200710152009.l9FK9nG1016129@sheep.berlios.de>

Author: nouiz
Date: 2007-10-15 22:09:46 +0200 (Mon, 15 Oct 2007)
New Revision: 8184

Modified:
   trunk/commands/PLearnCommands/PLearnCommandRegistry.h
   trunk/commands/PLearnCommands/PairwiseDiffsCommand.cc
   trunk/commands/PLearnCommands/RunCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
   trunk/plearn/base/RemoteDeclareMethod.h
   trunk/plearn/base/RemoteMethodMap.h
   trunk/plearn/base/diff.h
   trunk/plearn/base/pl_repository_revision.cc
   trunk/plearn/dict/Dictionary.h
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn/math/ObservationWindow.h
   trunk/plearn/misc/Calendar.h
   trunk/plearn/misc/PLearnServer.h
   trunk/plearn/misc/vmatmain.cc
   trunk/plearn/sys/procinfo.cc
   trunk/plearn/vmat/DisregardRowsVMatrix.cc
   trunk/plearn/vmat/FilterSplitter.cc
   trunk/plearn/vmat/FilteredVMatrix.cc
   trunk/plearn/vmat/GramVMatrix.cc
   trunk/plearn/vmat/JulianizeVMatrix.h
   trunk/plearn/vmat/MultiInstanceVMatrix.h
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn_learners/classifiers/KNNClassifier.cc
   trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h
   trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.cc
   trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h
   trunk/plearn_learners/nearest_neighbors/GenericNearestNeighbors.cc
   trunk/plearn_learners/online/BackConvolution2DModule.cc
   trunk/plearn_learners/online/Convolution2DModule.cc
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/LayerCostModule.h
   trunk/plearn_learners/online/MaxSubsampling2DModule.cc
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/RBMClassificationModule.cc
   trunk/plearn_learners/online/RBMConv2DConnection.cc
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/Subsampling2DModule.cc
   trunk/plearn_learners/online/Supersampling2DModule.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PerformanceEvaluator.h
   trunk/plearn_learners/unsupervised/KernelProjection.cc
   trunk/plearn_learners/unsupervised/PCA.cc
Log:
Change the order of import to get rid of useless warning


Modified: trunk/commands/PLearnCommands/PLearnCommandRegistry.h
===================================================================
--- trunk/commands/PLearnCommands/PLearnCommandRegistry.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/commands/PLearnCommands/PLearnCommandRegistry.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,9 +40,9 @@
 #ifndef PLearnCommandRegistry_INC
 #define PLearnCommandRegistry_INC
 
+#include "PLearnCommand.h"
 #include <map>
 #include <string>
-#include "PLearnCommand.h"
 
 
 namespace PLearn {

Modified: trunk/commands/PLearnCommands/PairwiseDiffsCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/PairwiseDiffsCommand.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/commands/PLearnCommands/PairwiseDiffsCommand.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,6 +40,13 @@
 
 /*! \file PairwiseDiffsCommand.cc */
 
+// From PLearn
+#include "PairwiseDiffsCommand.h"
+#include <plearn/base/stringutils.h>
+#include <plearn/math/StatsCollector.h>
+#include <plearn/vmat/VMat.h>
+#include <plearn/db/getDataSet.h>
+
 // From C++ stdlib
 #include <deque>
 #include <iomanip>
@@ -47,13 +54,6 @@
 // From Boost
 #include <boost/format.hpp>
 
-// From PLearn
-#include "PairwiseDiffsCommand.h"
-#include <plearn/base/stringutils.h>
-#include <plearn/math/StatsCollector.h>
-#include <plearn/vmat/VMat.h>
-#include <plearn/db/getDataSet.h>
-
 namespace PLearn {
 using namespace std;
 using boost::format;

Modified: trunk/commands/PLearnCommands/RunCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/RunCommand.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/commands/PLearnCommands/RunCommand.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,8 +38,6 @@
  ******************************************************* */
 
 /*! \file RunCommand.cc */
-#include <algorithm>
-
 #include "RunCommand.h"
 #include <plearn/base/general.h>
 #include <plearn/io/fileutils.h>
@@ -53,6 +51,8 @@
 #include <plearn/io/openString.h>
 #include <plearn/io/openFile.h>
 
+#include <algorithm>
+
 namespace PLearn {
 using namespace std;
 

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -36,10 +36,8 @@
  * $Id$
  ******************************************************* */
 
-// From C++ stdlib
-#include <exception>
-
 // From PLearn
+#include <plearn/base/RemoteDeclareMethod.h>
 #include "plearn_main.h"
 #include "PLearnCommandRegistry.h"
 #include <plearn/base/ProgressBar.h>
@@ -52,8 +50,10 @@
 #include <plearn/misc/Calendar.h>
 #include <plearn/misc/PLearnService.h>
 #include <plearn/vmat/VMat.h>
-#include <plearn/base/RemoteDeclareMethod.h>
 
+// From C++ stdlib
+#include <exception>
+
 #if USING_MPI
 #include <plearn/sys/PLMPI.h>
 #endif

Modified: trunk/plearn/base/RemoteDeclareMethod.h
===================================================================
--- trunk/plearn/base/RemoteDeclareMethod.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/base/RemoteDeclareMethod.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,15 +40,15 @@
 #ifndef RemoteDeclareMethod_INC
 #define RemoteDeclareMethod_INC
 
-// From C++ stdlib
-#include <string>
-
 // From PLearn
 #include "RemoteMethodMap.h"
 #include "RemoteMethodDoc.h"
 #include "RemoteTrampoline.h"
 #include <plearn/base/StaticInitializer.h>
 
+// From C++ stdlib
+#include <string>
+
 namespace PLearn {
 
 // What follows is a utility cast to bring const methods into non-const

Modified: trunk/plearn/base/RemoteMethodMap.h
===================================================================
--- trunk/plearn/base/RemoteMethodMap.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/base/RemoteMethodMap.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,17 +40,16 @@
 #ifndef RemoteMethodMap_INC
 #define RemoteMethodMap_INC
 
+// From PLearn
+#include "RemoteTrampoline.h"
+#include "PP.h"
+
 // From C++ stdlib
 #include <map>
 #include <string>
 #include <utility>
 #include <vector>
 
-// From PLearn
-#include "PP.h"
-
-#include "RemoteTrampoline.h"
-
 namespace PLearn {
 
 /**

Modified: trunk/plearn/base/diff.h
===================================================================
--- trunk/plearn/base/diff.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/base/diff.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -45,14 +45,14 @@
 #define diff_INC
 
 //#include <plearn/base/Object.h>
-#include <map>
-#include <string>
 #include <plearn/base/OptionBase.h>
 #include <plearn/base/tostring.h>
 #include <plearn/io/openString.h>
 #include <plearn/io/PStream.h>
 #include <plearn/math/pl_math.h>    //!< For 'real'.
 #include <plearn/math/TVec_decl.h>
+#include <map>
+#include <string>
 
 using namespace std;
 

Modified: trunk/plearn/base/pl_repository_revision.cc
===================================================================
--- trunk/plearn/base/pl_repository_revision.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/base/pl_repository_revision.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -45,8 +45,8 @@
 #define TO_STRING(s) #s
 #define MACRO_TO_STRING(s) TO_STRING(s)
 
+#include <plearn/base/RemoteDeclareMethod.h>
 #include "pl_repository_revision.h"
-#include <plearn/base/RemoteDeclareMethod.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/dict/Dictionary.h
===================================================================
--- trunk/plearn/dict/Dictionary.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/dict/Dictionary.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -43,8 +43,8 @@
 
 #ifndef Dictionary_INC
 #define Dictionary_INC
+#include <plearn/base/Object.h>
 #include <plearn/base/stringutils.h>
-#include <plearn/base/Object.h>
 #include <map>
 #include <string>
 

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/ker/SummationKernel.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -36,10 +36,10 @@
 
 /*! \file SummationKernel.cc */
 
+#include "SummationKernel.h"
 #include <plearn/base/stringutils.h>
 #include <plearn/base/lexical_cast.h>
 #include <plearn/vmat/SelectColumnsVMatrix.h>
-#include "SummationKernel.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/math/ObservationWindow.h
===================================================================
--- trunk/plearn/math/ObservationWindow.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/math/ObservationWindow.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -37,15 +37,15 @@
 #ifndef ObservationWindow_INC
 #define ObservationWindow_INC
 
-// From C++ stdlib
-#include <map>
-
 // From PLearn
 #include <plearn/base/Object.h>
 #include <plearn/base/PP.h>
 #include <plearn/base/tuple.h>
 #include <plearn/math/TVec.h>
 
+// From C++ stdlib
+#include <map>
+
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn/misc/Calendar.h
===================================================================
--- trunk/plearn/misc/Calendar.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/misc/Calendar.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -44,14 +44,14 @@
 #ifndef Calendar_H
 #define Calendar_H
 
+#include <plearn/base/Object.h>
+#include <plearn/base/PDate.h>
+#include "PRange.h"
+
 // C++ stdlib
 #include <map>
 #include <limits.h>
 
-#include <plearn/base/Object.h>
-#include <plearn/base/PDate.h>
-#include "PRange.h"
-
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn/misc/PLearnServer.h
===================================================================
--- trunk/plearn/misc/PLearnServer.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/misc/PLearnServer.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,8 +40,8 @@
 
 /*! \file PLearnServer.h */
 
+#include <plearn/base/Object.h>
 #include <plearn/io/PStream.h>
-#include <plearn/base/Object.h>
 #include <map>
 
 #ifndef PLearnServer_INC

Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/misc/vmatmain.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -35,10 +35,6 @@
  * $Id$
  ******************************************************* */
 
-#include <algorithm>                         // for max
-#include <iostream>
-#include <iomanip>                           // for setw and such
-
 #include "vmatmain.h"
 #include <commands/PLearnCommands/PLearnCommandRegistry.h>
 #include <plearn/base/general.h>
@@ -58,6 +54,10 @@
 #include <plearn/io/openFile.h>
 #include <plearn/base/HelpSystem.h>
 
+#include <algorithm>                         // for max
+#include <iostream>
+#include <iomanip>                           // for setw and such
+
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn/sys/procinfo.cc
===================================================================
--- trunk/plearn/sys/procinfo.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/sys/procinfo.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -1,13 +1,14 @@
-#include <sys/types.h>
-#include <cstdio>
-#include <cstdlib>
-#include <iostream>
+#include <plearn/base/RemoteDeclareMethod.h>  
 #include "procinfo.h"
 #include <plearn/base/plerror.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/base/tostring.h>  
-#include <plearn/base/RemoteDeclareMethod.h>  
 
+#include <sys/types.h>
+#include <cstdio>
+#include <cstdlib>
+#include <iostream>
+
 #if defined(WIN32) && defined(_MSC_VER)
 // unistd.h is not available under Microsoft Visual Studio, and some function
 // names are not the same.

Modified: trunk/plearn/vmat/DisregardRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/DisregardRowsVMatrix.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/DisregardRowsVMatrix.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -42,8 +42,8 @@
 
 #define PL_LOG_MODULE_NAME "DisregardRowsVMatrix"
 
+#include "DisregardRowsVMatrix.h"
 #include <plearn/io/pl_log.h>
-#include "DisregardRowsVMatrix.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/FilterSplitter.cc
===================================================================
--- trunk/plearn/vmat/FilterSplitter.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/FilterSplitter.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,9 +40,9 @@
 
 /*! \file FilterSplitter.cc */
 
-#include <plearn/io/fileutils.h>   //!< For newFilename.
 #include "FilterSplitter.h"
 #include "FilteredVMatrix.h"
+#include <plearn/io/fileutils.h>   //!< For newFilename.
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/FilteredVMatrix.cc
===================================================================
--- trunk/plearn/vmat/FilteredVMatrix.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/FilteredVMatrix.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,8 +40,8 @@
 
 /*! \file FilteredVMatrix.cc */
 
+#include "FilteredVMatrix.h"
 #include <plearn/base/ProgressBar.h>
-#include "FilteredVMatrix.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/GramVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GramVMatrix.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/GramVMatrix.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -41,8 +41,8 @@
 /*! \file GramVMatrix.cc */
 
 
+#include "GramVMatrix.h"
 #include <time.h>         //!< For clock().
-#include "GramVMatrix.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn/vmat/JulianizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/JulianizeVMatrix.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/JulianizeVMatrix.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -41,13 +41,13 @@
 #ifndef JulianizeVMatrix_INC
 #define JulianizeVMatrix_INC
 
+#include "SourceVMatrix.h"
+#include "VMat.h"
+
 #include <utility>
 #include <vector>
 #include <algorithm>
 
-#include "SourceVMatrix.h"
-#include "VMat.h"
-
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn/vmat/MultiInstanceVMatrix.h
===================================================================
--- trunk/plearn/vmat/MultiInstanceVMatrix.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/MultiInstanceVMatrix.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -44,9 +44,9 @@
 #ifndef MultiInstanceVMatrix_INC
 #define MultiInstanceVMatrix_INC
 
+#include "RowBufferedVMatrix.h"
 #include <vector>
 #include <utility>
-#include "RowBufferedVMatrix.h"
 #include <plearn/io/PPath.h>
 
 namespace PLearn {

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -37,11 +37,11 @@
 // Author: Pascal Vincent, Christian Hudon
 
 /*! \file TextFilesVMatrix.cc */
+#include "TextFilesVMatrix.h"
 #include <plearn/base/PDate.h>
 #include <plearn/base/ProgressBar.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/io/load_and_save.h>
-#include "TextFilesVMatrix.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/classifiers/KNNClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/classifiers/KNNClassifier.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -41,11 +41,11 @@
 /*! \file KNNClassifier.cc */
 
 
+#include "KNNClassifier.h"
 #include <assert.h>
 #include <math.h>
 #include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
 #include <plearn/ker/GaussianKernel.h>
-#include "KNNClassifier.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h
===================================================================
--- trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -44,14 +44,14 @@
 #ifndef BallTreeNearestNeighbors_INC
 #define BallTreeNearestNeighbors_INC
 
-#include <queue>
 
+#include "BinaryBallTree.h"
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
 #include <plearn/vmat/SelectRowsVMatrix.h>
 #include <plearn/ker/DistanceKernel.h>
 
-#include "BinaryBallTree.h"
+#include <queue>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.cc
===================================================================
--- trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -41,10 +41,10 @@
 /*! \file StatefulLearner.cc */
 
 
+#include "ExhaustiveNearestNeighbors.h"
 #include <assert.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/ker/DistanceKernel.h>
-#include "ExhaustiveNearestNeighbors.h"
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h
===================================================================
--- trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -42,14 +42,14 @@
 #ifndef ExhaustiveNearestNeighbors_INC
 #define ExhaustiveNearestNeighbors_INC
 
+// From PLearn
+#include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
+#include <plearn/ker/Kernel.h>
+
 // From C++ stdlib
 #include <utility>                           //!< for pair
 #include <queue>                             //!< for priority_queue<>
 
-// From PLearn
-#include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
-#include <plearn/ker/Kernel.h>
-
 namespace PLearn {
 
 /**

Modified: trunk/plearn_learners/nearest_neighbors/GenericNearestNeighbors.cc
===================================================================
--- trunk/plearn_learners/nearest_neighbors/GenericNearestNeighbors.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/nearest_neighbors/GenericNearestNeighbors.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,15 +40,15 @@
 
 /*! \file StatefulLearner.cc */
 
+// From PLearn
+#include "GenericNearestNeighbors.h"
+#include <plearn/ker/DistanceKernel.h>
+
 #include <assert.h>
 
 // From C++ stdlib
 #include <algorithm>
 
-// From PLearn
-#include "GenericNearestNeighbors.h"
-#include <plearn/ker/DistanceKernel.h>
-
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn_learners/online/BackConvolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/BackConvolution2DModule.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,11 +38,11 @@
 
 
 #define PL_LOG_MODULE_NAME "BackConvolution2DModule"
-#include <plearn/io/pl_log.h>
 
 #include "BackConvolution2DModule.h"
 #include <plearn/math/convolutions.h>
 #include <plearn/math/TMat_maths.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/Convolution2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Convolution2DModule.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/Convolution2DModule.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,12 +38,12 @@
 
 
 #define PL_LOG_MODULE_NAME "Convolution2DModule"
-#include <plearn/io/pl_log.h>
 
 #include "Convolution2DModule.h"
 #include <plearn/math/convolutions.h>
 #include <plearn/math/TMat_maths.h>
 #include <plearn/sys/Profiler.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,9 +38,9 @@
 
 
 #define PL_LOG_MODULE_NAME "DeepBeliefNet"
-#include <plearn/io/pl_log.h>
 
 #include "DeepBeliefNet.h"
+#include <plearn/io/pl_log.h>
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
 

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -39,10 +39,11 @@
 #ifndef LayerCostModule_INC
 #define LayerCostModule_INC
 
-#include <map>
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn/vmat/VMat.h>
 
+#include <map>
+
 namespace PLearn {
 
 /**

Modified: trunk/plearn_learners/online/MaxSubsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/MaxSubsampling2DModule.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,9 +38,9 @@
 
 
 #define PL_LOG_MODULE_NAME "MaxSubsampling2DModule"
-#include <plearn/io/pl_log.h>
 
 #include "MaxSubsampling2DModule.h"
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,10 +38,10 @@
 
 
 #define PL_LOG_MODULE_NAME "ModuleLearner"
-#include <plearn/io/pl_log.h>
 
 #include "ModuleLearner.h"
 #include <plearn_learners/online/NullModule.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/RBMClassificationModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMClassificationModule.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/RBMClassificationModule.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,10 +38,9 @@
 
 
 #define PL_LOG_MODULE_NAME "RBMClassificationModule"
-#include <plearn/io/pl_log.h>
 
 #include "RBMClassificationModule.h"
-
+#include <plearn/io/pl_log.h>
 #include <plearn/math/TMat_maths.h>
 
 namespace PLearn {

Modified: trunk/plearn_learners/online/RBMConv2DConnection.cc
===================================================================
--- trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/RBMConv2DConnection.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -37,11 +37,11 @@
 /*! \file RBMConv2DConnection.cc */
 
 #define PL_LOG_MODULE_NAME "RBMConv2DConnection"
-#include <plearn/io/pl_log.h>
 
 #include "RBMConv2DConnection.h"
 #include <plearn/math/TMat_maths.h>
 #include <plearn/math/convolutions.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/RBMModule.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,10 +40,10 @@
 #ifndef RBMModule_INC
 #define RBMModule_INC
 
-#include <map>
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn_learners/online/RBMConnection.h>
 #include <plearn_learners/online/RBMLayer.h>
+#include <map>
 
 namespace PLearn {
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,9 +38,9 @@
 
 
 #define PL_LOG_MODULE_NAME "StackedAutoassociatorsNet"
-#include <plearn/io/pl_log.h>
 
 #include "StackedAutoassociatorsNet.h"
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/Subsampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Subsampling2DModule.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/Subsampling2DModule.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,11 +38,11 @@
 
 
 #define PL_LOG_MODULE_NAME "Subsampling2DModule"
-#include <plearn/io/pl_log.h>
 
 #include "Subsampling2DModule.h"
 #include <plearn/math/convolutions.h>
 #include <plearn/math/TMat_maths.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/online/Supersampling2DModule.cc
===================================================================
--- trunk/plearn_learners/online/Supersampling2DModule.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/online/Supersampling2DModule.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,11 +38,11 @@
 
 
 #define PL_LOG_MODULE_NAME "Supersampling2DModule"
-#include <plearn/io/pl_log.h>
 
 #include "Supersampling2DModule.h"
 #include <plearn/math/convolutions.h>
 #include <plearn/math/TMat_maths.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -43,16 +43,15 @@
 #define PL_LOG_MODULE_NAME "GaussianProcessRegressor"
 
 // From PLearn
+#include "GaussianProcessRegressor.h"
 #include <plearn/base/stringutils.h>
-#include <plearn/io/pl_log.h>
 #include <plearn/vmat/ExtendedVMatrix.h>
 #include <plearn/math/pl_erf.h>
 #include <plearn/var/GaussianProcessNLLVariable.h>
 #include <plearn/var/ObjectOptionVariable.h>
 #include <plearn/opt/Optimizer.h>
+#include <plearn/io/pl_log.h>
 
-#include "GaussianProcessRegressor.h"
-
 namespace PLearn {
 using namespace std;
 

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/testers/PTester.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -38,13 +38,13 @@
 
 /*! \file PTester.cc */
 
+#include "PTester.h"
 #include <plearn/io/load_and_save.h>
 #include <plearn/io/openString.h>
 #include <plearn/math/VecStatsCollector.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/vmat/MemoryVMatrix.h>
 #include <assert.h>
-#include "PTester.h"
 #include <plearn/base/RemoteDeclareMethod.h>
 
 #include <plearn/misc/PLearnService.h>

Modified: trunk/plearn_learners/testers/PerformanceEvaluator.h
===================================================================
--- trunk/plearn_learners/testers/PerformanceEvaluator.h	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/testers/PerformanceEvaluator.h	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,8 +40,8 @@
 #ifndef PerformanceEvaluator_INC
 #define PerformanceEvaluator_INC
 
+#include <plearn/base/Object.h>
 #include <plearn/base/PP.h>
-#include <plearn/base/Object.h>
 #include <plearn/vmat/VMat.h>
 #include <plearn_learners/generic/PLearner.h>
 

Modified: trunk/plearn_learners/unsupervised/KernelProjection.cc
===================================================================
--- trunk/plearn_learners/unsupervised/KernelProjection.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/unsupervised/KernelProjection.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -40,8 +40,8 @@
 
 /*! \file KernelProjection.cc */
 
+#include "KernelProjection.h"
 #include <time.h>               //!< For clock().
-#include "KernelProjection.h"
 #include <plearn/math/plapack.h>            //!< For eigenVecOfSymmMat.
 
 namespace PLearn {

Modified: trunk/plearn_learners/unsupervised/PCA.cc
===================================================================
--- trunk/plearn_learners/unsupervised/PCA.cc	2007-10-15 18:31:15 UTC (rev 8183)
+++ trunk/plearn_learners/unsupervised/PCA.cc	2007-10-15 20:09:46 UTC (rev 8184)
@@ -39,10 +39,10 @@
 #define PL_LOG_MODULE_NAME "PCA"
 
 /*! \file PCA.cc */
+#include "PCA.h"
 #include <plearn/io/pl_log.h>
 #include <plearn/vmat/CenteredVMatrix.h>
 #include <plearn/vmat/GetInputVMatrix.h>
-#include "PCA.h"
 #include <plearn/math/plapack.h>
 #include <plearn/math/random.h>     //!< For fill_random_normal.
 #include <plearn/vmat/VMat_basic_stats.h>



From larocheh at mail.berlios.de  Mon Oct 15 22:46:14 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Mon, 15 Oct 2007 22:46:14 +0200
Subject: [Plearn-commits] r8185 - trunk/plearn_learners_experimental
Message-ID: <200710152046.l9FKkEGY018949@sheep.berlios.de>

Author: larocheh
Date: 2007-10-15 22:46:13 +0200 (Mon, 15 Oct 2007)
New Revision: 8185

Modified:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
Log:
Corrected some bugs...


Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-15 20:09:46 UTC (rev 8184)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-15 20:46:13 UTC (rev 8185)
@@ -796,15 +796,23 @@
     real dist = sqrt(powdistance(input_representation,
                                  dissimilar_example_representation,
                                  2));
-    
-    substract(input_representation,dissimilar_example_representation,
-              dissimilar_gradient_contribution);
 
-    dissimilar_gradient_contribution *= -5.54*
-        safeexp(-2.77*dist/layers[index+1]->size)/dist;
-    
-    expectation_gradients[index+1] += dissimilar_gradient_contribution;
+    if( dist == 0 )
+        PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
+                  " example representation is exactly the sample as the"
+                  " input example. Gradient would be infinite! Skipping this"
+                  " example...");
+    else
+    {
+        substract(input_representation,dissimilar_example_representation,
+                  dissimilar_gradient_contribution);
+        
+        dissimilar_gradient_contribution *= -5.54*
+            safeexp(-2.77*dist/layers[index+1]->size)/dist;
 
+        expectation_gradients[index+1] += dissimilar_gradient_contribution;
+    }
+
     // RBM learning
     if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
     {
@@ -936,14 +944,22 @@
                                  dissimilar_example_representation,
                                  2));
     
-    substract(input_representation,dissimilar_example_representation,
-              dissimilar_gradient_contribution);
+    if( dist == 0 )
+        PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
+                  " example representation is exactly the sample as the"
+                  " input example. Gradient would be infinite! Skipping this"
+                  " example...");
+    else
+    {
 
-    dissimilar_gradient_contribution *= -5.54*
-        safeexp(-2.77*dist/layers[n_layers-1]->size)/dist;
-    
-    expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+        substract(input_representation,dissimilar_example_representation,
+                  dissimilar_gradient_contribution);
 
+        dissimilar_gradient_contribution *= -5.54*
+            safeexp(-2.77*dist/layers[n_layers-1]->size)/dist;
+        
+        expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+    }
 
     for( int i=n_layers-1 ; i>0 ; i-- )
     {



From saintmlx at mail.berlios.de  Mon Oct 15 23:13:02 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 15 Oct 2007 23:13:02 +0200
Subject: [Plearn-commits] r8186 - trunk/plearn/vmat
Message-ID: <200710152113.l9FLD2mX020772@sheep.berlios.de>

Author: saintmlx
Date: 2007-10-15 23:13:01 +0200 (Mon, 15 Oct 2007)
New Revision: 8186

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
- added remote_getStats



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-10-15 20:46:13 UTC (rev 8185)
+++ trunk/plearn/vmat/VMatrix.cc	2007-10-15 21:13:01 UTC (rev 8186)
@@ -233,6 +233,13 @@
          ArgDoc ("l", "length"),
          ArgDoc ("w", "width"),
          RetDoc ("The sub-VMatrix")));
+
+
+    declareMethod(
+        rmm, "getStats", &VMatrix::remote_getStats,
+        (BodyDoc("Returns the unconditonal statistics for all fields\n"),
+         RetDoc ("Stats vector")));
+
 }
 
 
@@ -1366,6 +1373,19 @@
     return field_stats;
 }
 
+TVec<StatsCollector*> VMatrix::remote_getStats() const
+{
+    static TVec<StatsCollector*> field_p_stats;
+    if(field_p_stats.isEmpty())
+    {
+        TVec<StatsCollector> st= getStats();
+        field_p_stats.resize(st.length());
+        for(int i= 0; i < st.length(); ++i)
+            field_p_stats[i]= &st[i];
+    }
+    return field_p_stats;
+}
+
 ////////////////////
 // getBoundingBox //
 ////////////////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2007-10-15 20:46:13 UTC (rev 8185)
+++ trunk/plearn/vmat/VMatrix.h	2007-10-15 21:13:01 UTC (rev 8186)
@@ -588,6 +588,7 @@
      *  created).
      */
     TVec<StatsCollector> getStats() const;
+    TVec<StatsCollector*> remote_getStats() const;
 
     StatsCollector& getStats(int fieldnum) const
     { return getStats()[fieldnum]; }



From saintmlx at mail.berlios.de  Tue Oct 16 00:01:58 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 16 Oct 2007 00:01:58 +0200
Subject: [Plearn-commits] r8187 - trunk/plearn/vmat
Message-ID: <200710152201.l9FM1wev022668@sheep.berlios.de>

Author: saintmlx
Date: 2007-10-16 00:01:57 +0200 (Tue, 16 Oct 2007)
New Revision: 8187

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
- fixed remote_getStats



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-10-15 21:13:01 UTC (rev 8186)
+++ trunk/plearn/vmat/VMatrix.cc	2007-10-15 22:01:57 UTC (rev 8187)
@@ -1373,15 +1373,15 @@
     return field_stats;
 }
 
-TVec<StatsCollector*> VMatrix::remote_getStats() const
+TVec<PP<StatsCollector> > VMatrix::remote_getStats() const
 {
-    static TVec<StatsCollector*> field_p_stats;
     if(field_p_stats.isEmpty())
     {
         TVec<StatsCollector> st= getStats();
         field_p_stats.resize(st.length());
+        CopiesMap cm;
         for(int i= 0; i < st.length(); ++i)
-            field_p_stats[i]= &st[i];
+            field_p_stats[i]= st[i].deepCopy(cm);
     }
     return field_p_stats;
 }

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2007-10-15 21:13:01 UTC (rev 8186)
+++ trunk/plearn/vmat/VMatrix.h	2007-10-15 22:01:57 UTC (rev 8187)
@@ -113,6 +113,7 @@
 
     /// Statistics for each field.
     mutable TVec<StatsCollector> field_stats;  ///< stats[i] contains stats for field #i
+    mutable TVec<PP<StatsCollector> > field_p_stats; //same, for remote calls
 
     /// The string mapping for each field, in both directions.
     mutable TVec<map<string,real> > map_sr;
@@ -588,7 +589,7 @@
      *  created).
      */
     TVec<StatsCollector> getStats() const;
-    TVec<StatsCollector*> remote_getStats() const;
+    TVec<PP<StatsCollector> > remote_getStats() const;
 
     StatsCollector& getStats(int fieldnum) const
     { return getStats()[fieldnum]; }



From nouiz at mail.berlios.de  Tue Oct 16 15:12:26 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 16 Oct 2007 15:12:26 +0200
Subject: [Plearn-commits] r8188 - trunk/plearn_learners/generic
Message-ID: <200710161312.l9GDCQ4N031121@sheep.berlios.de>

Author: nouiz
Date: 2007-10-16 15:12:26 +0200 (Tue, 16 Oct 2007)
New Revision: 8188

Modified:
   trunk/plearn_learners/generic/BestAveragingPLearner.cc
Log:
changed the order of include so that we remove not usefull warning


Modified: trunk/plearn_learners/generic/BestAveragingPLearner.cc
===================================================================
--- trunk/plearn_learners/generic/BestAveragingPLearner.cc	2007-10-15 22:01:57 UTC (rev 8187)
+++ trunk/plearn_learners/generic/BestAveragingPLearner.cc	2007-10-16 13:12:26 UTC (rev 8188)
@@ -36,13 +36,13 @@
 
 /*! \file BestAveragingPLearner.cc */
 
+// From PLearn
+#include "BestAveragingPLearner.h"
+#include <plearn/base/ProgressBar.h>
+
 // From C++ stdlib
 #include <algorithm>
 
-// From PLearn
-#include <plearn/base/ProgressBar.h>
-#include "BestAveragingPLearner.h"
-
 namespace PLearn {
 using namespace std;
 



From nouiz at mail.berlios.de  Tue Oct 16 18:16:24 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 16 Oct 2007 18:16:24 +0200
Subject: [Plearn-commits] r8189 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200710161616.l9GGGObd014178@sheep.berlios.de>

Author: nouiz
Date: 2007-10-16 18:16:23 +0200 (Tue, 16 Oct 2007)
New Revision: 8189

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
Added mem and os option for cluster


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-16 13:12:26 UTC (rev 8188)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-16 16:16:23 UTC (rev 8189)
@@ -393,6 +393,8 @@
         self.nb_proc=50
         self.mt=None
         self.args=args
+        self.mem=None
+        self.os=None
         DBIBase.__init__(self, commands, **args)
         self.pre_tasks=["echo '[DBI] executing on host' $HOSTNAME"]+self.pre_tasks
         self.post_tasks=["echo '[DBI] exit status' $?"]+self.post_tasks
@@ -442,6 +444,10 @@
             command += " --duree "+self.duree
         if self.cluster_wait:
             command += " --wait"
+        if self.mem:
+            command += " --memoire "+self.mem
+        if self.os:
+            command += " --os "+self.os
         command += " --execute '"+ filename + "'"
 
         self.started+=1
@@ -520,7 +526,7 @@
         self.queue = "qwork at ms"
         self.long = False
         self.duree = "12:00:00"
-
+        self.mem = None
         DBIBase.__init__(self, commands, **args)
 
         self.nb_proc = int(self.nb_proc)

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-10-16 13:12:26 UTC (rev 8188)
+++ trunk/scripts/dbidispatch	2007-10-16 16:16:23 UTC (rev 8189)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -48,6 +48,8 @@
 cluster only options:
   The '--wait' is transfered to cluster. This must be enabled if there is not nb_process available nodes. Otherwise when there are no nodes available, the launch of that command fails.
   The '--nowait' means the --wait option is not given to the cluster command, as in the default.
+  The '--mem=X' speficify the number of meg the program need to execute.
+  The '--os=X' speficify the os of the server: fc4 or fc7. Default: fc4
   
 condor only options:
   The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
@@ -130,6 +132,10 @@
         launch_cmd = "Condor"
     elif argv.startswith("--duree="):
         dbi_param["duree"]=argv[8:]
+    elif argv.startswith("--mem="):
+        dbi_param["mem"]=argv[6:]
+    elif argv.startswith("--os="):
+        dbi_param["os"]=argv[5:]
     elif argv.startswith("--local"):
         launch_cmd = "Local"
         if len(argv)>7:



From lamblin at mail.berlios.de  Wed Oct 17 20:01:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 17 Oct 2007 20:01:31 +0200
Subject: [Plearn-commits] r8190 - trunk/plearn_learners/online
Message-ID: <200710171801.l9HI1VtI030320@sheep.berlios.de>

Author: lamblin
Date: 2007-10-17 20:01:31 +0200 (Wed, 17 Oct 2007)
New Revision: 8190

Modified:
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Log:
Fix some warnings and whitespaces.


Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-10-16 16:16:23 UTC (rev 8189)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-10-17 18:01:31 UTC (rev 8190)
@@ -55,10 +55,10 @@
     "Some only apply for binomial layers. \n");
 
 LayerCostModule::LayerCostModule():
+    cost_function(""),
     histo_size(10),
     alpha(0.0),
     momentum(0.0),
-    cost_function(""),
     cost_function_completename("")
 {
     output_size = 1;
@@ -77,18 +77,18 @@
                   OptionBase::buildoption,
         "The cost function applied to the layer:\n"
         "- \"pascal\":"
-	                    " Pascal Vincent's God given cost function.\n"
+        " Pascal Vincent's God given cost function.\n"
         "- \"correlation\":"
-	                    " average of a function applied to the correlations between outputs.\n"
+        " average of a function applied to the correlations between outputs.\n"
         "- \"kl_div\":"
-	                    " KL divergence between distrubution of outputs (sampled with x)\n"
+        " KL divergence between distrubution of outputs (sampled with x)\n"
         "- \"kl_div_simple\":"
-	                    " simple version of kl_div where we count at least one sample per histogram's bin\n"
+        " simple version of kl_div where we count at least one sample per histogram's bin\n"
         "- \"stochastic_cross_entropy\" [default]:"
-	                    " average cross-entropy between pairs of binomial units\n"
+        " average cross-entropy between pairs of binomial units\n"
         "- \"stochastic_kl_div\":"
-	                    " average KL divergence between pairs of binomial units\n"
-                 );
+        " average KL divergence between pairs of binomial units\n"
+        );
 
     declareOption(ol, "histo_size", &LayerCostModule::histo_size,
                   OptionBase::buildoption,
@@ -144,9 +144,9 @@
     PLASSERT( histo_size > 1 );
     PLASSERT( momentum >= 0.0);
     PLASSERT( momentum < 1);
-    
+
     norm_factor = 1./(real)(input_size*(input_size-1));
-    
+
     string im = lowerstring( cost_function );
     // choose HERE the *default* cost function
     if( im == "" )
@@ -160,7 +160,7 @@
     if( ( cost_function == "stochastic_cross_entropy")
      || ( cost_function == "stochastic_kl_div") )
         is_cost_function_stochastic = true;
-        
+
     // list HERE all *non stochastic* cost functions
     // and the specific initialization
     else if( ( cost_function == "kl_div")
@@ -225,16 +225,16 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField(inputs_histo, copies);
-    
+
     deepCopyField(inputs_expectation, copies);
     deepCopyField(inputs_stds, copies);
-    
+
     deepCopyField(inputs_correlations, copies);
     deepCopyField(inputs_cross_quadratic_mean, copies);
-    
+
     deepCopyField(inputs_expectation_trainMemory, copies);
     deepCopyField(inputs_cross_quadratic_mean_trainMemory, copies);
-    
+
     deepCopyField(ports, copies);
 }
 
@@ -267,7 +267,7 @@
 {
     int n_samples = inputs.length();
     costs.resize( n_samples, output_size );
-    
+
     if( !is_cost_function_stochastic )
     {
         costs.clear(); // costs(i,0) = 0
@@ -278,7 +278,7 @@
         //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
         //! between probabilities of outputs vectors for all units
         //! ************************************************************
-        //! 
+        //!
         //!      cost = - MEAN_{i,j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
         //!
         //!           = - MEAN_{i,j#i} SUM_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
@@ -300,7 +300,7 @@
         //! ************************************************************
 
             computeHisto(inputs);
-            
+
             costs(0,0) = computeKLdiv();
         }
         else if( cost_function == "kl_div_simple" )
@@ -315,7 +315,7 @@
         //! ************************************************************
 
             computeSafeHisto(inputs);
-            
+
             // Computing the KL divergence
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
@@ -331,7 +331,7 @@
         //! a Pascal Vincent's god-given similarity measure
         //! between outputs vectors for all units
         //! ************************************************************
-        //! 
+        //!
         //!      cost = MEAN_{i,j#i} f( Ex[q{i}.q{j}] ) - alpha. MEAN_{i} f( Ex[q{i}] )
         //!
         //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer
@@ -340,7 +340,7 @@
         //! ************************************************************
 
             computePascalStatistics(inputs);
-                                    
+
             // Computing the cost
             for (int i = 0; i < input_size; i++)
             {
@@ -370,7 +370,7 @@
         //! ************************************************************
 
             computeCorrelationStatistics(inputs);
-                        
+
             // Computing the cost
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
@@ -379,10 +379,10 @@
             costs(0,0) *= norm_factor;
         }
 
-        
+
         return; // Do not fprop with the conventional stochastic fprop...
     }
-    
+
     for (int isample = 0; isample < n_samples; isample++)
         fprop(inputs(isample), costs(isample,0));
 }
@@ -394,8 +394,8 @@
 
     cost = 0.0;
     real  qi, qj, comp_qi, comp_qj; // The outputs (units i,j)
-                                    // and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
-                                      
+                                    // and some basic operations on it (e.g.: 1-qi, qi/(1-qi))
+
     if( cost_function == "stochastic_cross_entropy" )
     {
     //! ************************************************************
@@ -421,10 +421,10 @@
            {
                qj = input[j];
                comp_qj = 1.0 - qj;
-               
+
                // H(pi||pj) = H(pi) + D_{KL}(pi||pj)
                cost += qi*safeflog(qj) + comp_qi*safeflog(comp_qj);
-               
+
                // The symetric part (loop  j=i+1...size)
                cost += qj*safeflog(qi) + comp_qj*safeflog(comp_qi);
            }
@@ -432,7 +432,7 @@
         // Normalization w.r.t. number of units
         cost *= norm_factor;
     }
-    
+
     else if( cost_function == "stochastic_kl_div" )
     {
     //! ************************************************************
@@ -457,7 +457,7 @@
                comp_qi = REAL_MAX;
            else
                comp_qi = qi/(1.0 - qi);
-       
+
            for( int j = 0; j < i; j++ )
            {
                qj = input[j];
@@ -465,13 +465,13 @@
                    comp_qj = REAL_MAX;
                else
                    comp_qj = qj/(1.0 - qj);
-               
+
                //     - D_{KL}(pi||pj) - D_{KL}(pj||pi)
                cost += (qj-qi)*safeflog(comp_qi/comp_qj);
            }
         }
         // Normalization w.r.t. number of units
-        cost *= norm_factor;   
+        cost *= norm_factor;
     }
 
     else
@@ -598,7 +598,7 @@
         else if( cost_function == "kl_div" )
         {
             computeHisto(*p_inputs);
-            
+
             for (int isample = 0; isample < n_samples; isample++)
             {
 
@@ -607,7 +607,7 @@
                 for (int i = 0; i < input_size; i++)
                 {
                     (*p_inputs_grad)(isample, i) = 0.0;
-                    
+
                     qi = (*p_inputs)(isample,i);
                     int index_i = histo_index(qi);
                     if( ( index_i == histo_size-1 ) ) // we do not care about this...
@@ -620,11 +620,11 @@
                         shift_i = -1;
                     // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
                     //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-                    
+
                     for (int j = 0; j < i; j++)
                     {
                         (*p_inputs_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
-                        
+
                         qj = (*p_inputs)(isample,j);
                         int index_j = histo_index(qj);
                         if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
@@ -637,7 +637,7 @@
                             shift_j = -1;
                             // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
                           //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
+
                         (*p_inputs_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                     }
                 }
@@ -648,8 +648,8 @@
                     (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
                 }
             }
-            
-            
+
+
             // debug Check
             int i=0;
             real cost_before = computeKLdiv();
@@ -661,19 +661,19 @@
                   (*p_inputs)(isample,i) += dq(qi);
                   computeHisto(*p_inputs);
                   real cost_after = computeKLdiv();
-                  (*p_inputs)(isample,i) -= dq(qi);                  
+                  (*p_inputs)(isample,i) -= dq(qi);
                   cout << "\tglobal cost comparison:" << cost_after - cost_before;
                   cout << "  <?>  " << (*p_inputs_grad)(isample, i)*dq(qi) << endl;
                 }
             }
-            
-            
+
+
         }
 
         else if( cost_function == "kl_div_simple" )
         {
             computeSafeHisto(*p_inputs);
-            
+
             for (int isample = 0; isample < n_samples; isample++)
             {
 
@@ -682,7 +682,7 @@
                 for (int i = 0; i < input_size; i++)
                 {
                     (*p_inputs_grad)(isample, i) = 0.0;
-                    
+
                     qi = (*p_inputs)(isample,i);
                     int index_i = histo_index(qi);
                     if( ( index_i == histo_size-1 ) ) // we do not care about this...
@@ -695,11 +695,11 @@
                         shift_i = -1;
                     // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
                     //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-                    
+
                     for (int j = 0; j < i; j++)
                     {
                         (*p_inputs_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
-                        
+
                         qj = (*p_inputs)(isample,j);
                         int index_j = histo_index(qj);
                         if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
@@ -712,7 +712,7 @@
                             shift_j = -1;
                             // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
                           //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-                        
+
                         (*p_inputs_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
                     }
                 }
@@ -728,7 +728,7 @@
         else if( cost_function == "pascal" )
         {
             computePascalStatistics(*p_inputs);
-            
+
             if( momentum > 0.0 )
                 for (int isample = 0; isample < n_samples; isample++)
                 {
@@ -780,7 +780,7 @@
         else if( cost_function == "correlation")
         {
             computeCorrelationStatistics(*p_inputs);
-            
+
             if( momentum > 0.0 )
                 PLERROR( "not implemented yet");
             else
@@ -789,13 +789,13 @@
                     Vec dSTDi_dqi, dCROSSij_dqj;
                     dSTDi_dqi.resize( input_size );
                     dCROSSij_dqj.resize( input_size );
-                    
+
                     for (int i = 0; i < input_size; i++)
                     {
                         qi = (*p_inputs)(isample, i);
 
                         //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
-                        //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                        //!                  = ( qi(t) - E(Qi) ) / n_samples
                         //!
                         //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
                         //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
@@ -806,7 +806,7 @@
                         //!
                         dCROSSij_dqj[i] = ( qi - inputs_expectation[i] )*one_count;
                         dSTDi_dqi[i] = dCROSSij_dqj[i] / inputs_stds[i];
-                        
+
                         for (int j = 0; j < i; j++)
                         {
                             qj = (*p_inputs)(isample,j);
@@ -815,13 +815,13 @@
                             real dfunc_dCorr = deriv_func_correlation( inputs_correlations(i,j) );
                             real correlation_num = ( inputs_cross_quadratic_mean(i,j)
                                                      - inputs_expectation[i]*inputs_expectation[j] );
-                                  
-                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * ( 
+
+                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * (
                                                                     correlation_denum * dCROSSij_dqj[j]
                                                                   - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
                                                                     ) / (correlation_denum * correlation_denum);
 
-                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * ( 
+                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * (
                                                                     correlation_denum * dCROSSij_dqj[i]
                                                                   - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
                                                                     ) / (correlation_denum * correlation_denum);
@@ -853,10 +853,10 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
-    inputs_expectation.clear(); 
-    inputs_cross_quadratic_mean.clear(); 
 
+    inputs_expectation.clear();
+    inputs_cross_quadratic_mean.clear();
+
     for (int isample = 0; isample < n_samples; isample++)
     {
         input = inputs(isample);
@@ -867,7 +867,7 @@
                  inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
-    
+
     for (int i = 0; i < input_size; i++)
     {
         inputs_expectation[i] *= one_count;
@@ -912,9 +912,9 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
+
     inputs_expectation.clear();
-    inputs_cross_quadratic_mean.clear(); 
+    inputs_cross_quadratic_mean.clear();
     inputs_correlations.clear();
 
     for (int isample = 0; isample < n_samples; isample++)
@@ -928,7 +928,7 @@
                  inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
-    
+
     for (int i = 0; i < input_size; i++)
     {
         //! Normalization
@@ -951,7 +951,7 @@
         }
     }
     //! Be careful: 'inputs_correlations' matrix is only computed
-    //!  on the triangle subpart 'i' > 'j' 
+    //!  on the triangle subpart 'i' > 'j'
     //!  ('i'/'j': first/second argument)
 
     if(  during_training )
@@ -985,7 +985,7 @@
     real grad_update = 0.0;
 
     real Ni_ki = inputs_histo(i,index_i);
-    real Ni_ki_shift1 = inputs_histo(i,index_i+1);            
+    real Ni_ki_shift1 = inputs_histo(i,index_i+1);
     real Nj_ki        = inputs_histo(j,index_i);
     real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
@@ -1009,7 +1009,7 @@
     {
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
-                                                               
+
         if( fast_exact_is_equal(Ni_ki, one_count) )
         {
             differ_count_j_after = Nj_ki;
@@ -1018,19 +1018,19 @@
         else
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki )
-	                   *over_dq;
+                           *over_dq;
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
             grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after )
-	                  *(real)(1+n_differ_j_after)*over_dq ;
+                          *(real)(1+n_differ_j_after)*over_dq ;
 
             if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas ou on regroupe avec le dernier";
             {
                 // removing the term of the sum that will be modified
                 grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )
-		               *over_dq;                
+                               *over_dq;
             }
             else
             {
@@ -1048,15 +1048,17 @@
                 if( ki < histo_size )
                 {
                     grad_update -= KLdivTerm( inputs_histo( i, ki ), differ_count_j_before )
-		                   *(real)(1+n_differ_j_before)*over_dq;
+                                   *(real)(1+n_differ_j_before)*over_dq;
 
-                    if( differ_count_j_before > Nj_ki_shift1 )                
+                    if( differ_count_j_before > Nj_ki_shift1 )
                         grad_update += KLdivTerm( inputs_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )
-			               *(real)(n_differ_j_before)*over_dq;
+                                       *(real)(n_differ_j_before)*over_dq;
                         // pb avec differ_count_j_after plus haut??? semble pas
                 }
                 else
-                    "cas ou on regroupe avec le dernier (easy)";
+                {
+                    // cas ou on regroupe avec le dernier (easy)
+                }
             }
         }
         else
@@ -1081,14 +1083,16 @@
             if( kj < histo_size )
             {
                 grad_update += KLdivTerm( differ_count_i_after, inputs_histo( j, kj ) )
-		               *(real)(1+n_differ_i_after)*over_dq;
+                               *(real)(1+n_differ_i_after)*over_dq;
 
                 if( differ_count_i_before > 0 )
                     grad_update -= KLdivTerm( differ_count_i_before, inputs_histo( j, kj ) )
-		                   *(real)(1+n_differ_i_before)*over_dq;
+                                   *(real)(1+n_differ_i_before)*over_dq;
             }
             else
-                "cas ou on regroupe avec le dernier";   
+            {
+                // cas ou on regroupe avec le dernier
+            }
         }
     }
     return grad_update*over_dq;
@@ -1099,25 +1103,25 @@
     //PLASSERT( over_dq > 0.0 )
 
     real grad_update = 0.0;
-                    
+
     real Ni_ki = inputs_histo(i,index_i);
     PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
                                                   // if inputs_histo is up to date,
                                                   // the input(isample,i) has been counted
     real Ni_ki_shift1 = inputs_histo(i,index_i+1);
-                    
+
     real Nj_ki        = inputs_histo(j,index_i);
     real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
 
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki );
-                                                               
+
         // adding the term of the sum with its modified value
         grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
 
         grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1 );
-        
+
         grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );
 
     return grad_update *over_dq;
@@ -1135,7 +1139,7 @@
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
                 {
-                    // These variables are used in case one bin of 
+                    // These variables are used in case one bin of
                     // the histogram is empty for one unit
                     // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
                     // In such case, we ''differ'' the count for the next bin and so on.
@@ -1171,22 +1175,22 @@
                         }
                     }
                     if( differ_count_i > 0.0 )
-                    {   
-                        "cas ou on regroupe avec le dernier";   
+                    {
+                        // cas ou on regroupe avec le dernier
 //                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
 //                                  *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
-//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
                     }
-                     
+
                     else if ( differ_count_j > 0.0 )
                     {
-                        "cas ou on regroupe avec le dernier";
+                        // cas ou on regroupe avec le dernier
 //                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
 //                                 *(real)(1+last_n_differ) *HISTO_STEP;
 //                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
 //                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
-                    }    
+                    }
                 }
             // Normalization w.r.t. number of units
             return cost *norm_factor;
@@ -1197,8 +1201,8 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
-    
-    inputs_histo.clear(); 
+
+    inputs_histo.clear();
     for (int isample = 0; isample < n_samples; isample++)
     {
         input = inputs(isample);
@@ -1214,7 +1218,7 @@
     int n_samples = inputs.length();
     one_count = 1. / (real)(n_samples+histo_size);
     Vec input;
-    
+
     inputs_histo.fill(one_count);
 
     for (int isample = 0; isample < n_samples; isample++)
@@ -1272,7 +1276,7 @@
 
     inputs_expectation.clear();
     inputs_stds.clear();
-    
+
     inputs_correlations.clear();
     inputs_cross_quadratic_mean.clear();
     if( momentum > 0.0)

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-10-16 16:16:23 UTC (rev 8189)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-10-17 18:01:31 UTC (rev 8190)
@@ -57,11 +57,11 @@
     //#####  Public Build Options  ############################################
 
     string cost_function;
-    
+
     int histo_size;
-    
+
     real alpha;
-    
+
     real momentum;
 
     //#####  Public Learnt Options  ###########################################
@@ -75,13 +75,13 @@
     //! or (for some) 'pascal'
     Vec inputs_expectation;
     Vec inputs_stds;         //! only for 'correlation' cost function
-    
+
     Mat inputs_correlations; //! only for 'correlation' cost function
     Mat inputs_cross_quadratic_mean;
 
     //! The generic name of the cost function
     string cost_function_completename;
-    
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -93,7 +93,7 @@
     virtual void fprop(const Mat& inputs, Mat& costs);
     //! Overridden.
     virtual void fprop(const TVec<Mat*>& ports_value);
-    
+
     //! backpropagate the derivative w.r.t. activation
     virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
                                 const TVec<Mat*>& ports_gradient);
@@ -163,14 +163,14 @@
     //! Range of a histogram's bin ( HISTO_STEP = 1/histo_size )
     real HISTO_STEP;
     //! the weight of a sample within a batch (usually, 1/n_samples)
-    real one_count; 
+    real one_count;
 
     //! Variables for (non stochastic) Pascal's/correlation function
     //! -------------------------------------------------------------
-    //! Statistics on outputs (estimated empiricially on the data)    
+    //! Statistics on outputs (estimated empiricially on the data)
     Vec inputs_expectation_trainMemory;
     Mat inputs_cross_quadratic_mean_trainMemory;
-        
+
     //! Map from a port name to its index in the 'ports' vector.
     map<string, int> portname_to_index;
 



From lamblin at mail.berlios.de  Wed Oct 17 20:21:09 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 17 Oct 2007 20:21:09 +0200
Subject: [Plearn-commits] r8191 - trunk/plearn/vmat
Message-ID: <200710171821.l9HIL9u8031303@sheep.berlios.de>

Author: lamblin
Date: 2007-10-17 20:21:09 +0200 (Wed, 17 Oct 2007)
New Revision: 8191

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
Fix same annoying warning


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-10-17 18:01:31 UTC (rev 8190)
+++ trunk/plearn/vmat/VMatrix.cc	2007-10-17 18:21:09 UTC (rev 8191)
@@ -38,8 +38,6 @@
  * $Id$
  ******************************************************* */
 
-#include <sstream>
-#include <iomanip>
 #include "VMatrix.h"
 #include "DiskVMatrix.h"
 #include "FileVMatrix.h"

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2007-10-17 18:01:31 UTC (rev 8190)
+++ trunk/plearn/vmat/VMatrix.h	2007-10-17 18:21:09 UTC (rev 8191)
@@ -45,16 +45,16 @@
 
 //#include <cstdlib>
 #include <plearn/base/Object.h>
-#include <map>
 #include <plearn/base/PP.h>
 #include <plearn/math/TMat.h>
 #include <plearn/math/StatsCollector.h>
 #include "VMField.h"
 #include <plearn/dict/Dictionary.h>
 #include <plearn/math/TMat_maths.h>
-
 #include <plearn/io/PPath.h>
 
+#include <map>
+
 namespace PLearn {
 using namespace std;
 



From lamblin at mail.berlios.de  Wed Oct 17 21:02:57 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 17 Oct 2007 21:02:57 +0200
Subject: [Plearn-commits] r8192 - trunk/python_modules/plearn/parallel
Message-ID: <200710171902.l9HJ2vBO001146@sheep.berlios.de>

Author: lamblin
Date: 2007-10-17 21:02:56 +0200 (Wed, 17 Oct 2007)
New Revision: 8192

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Removed trailing spaces


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-17 18:21:09 UTC (rev 8191)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-17 19:02:56 UTC (rev 8192)
@@ -40,7 +40,7 @@
     def __init__( self, iterator ):
         self._lock     = Lock()
         self._iterator = iterator
-        
+
     def __iter__( self ):
         return self
 
@@ -50,8 +50,8 @@
             return self._iterator.next()
         finally:
             self._lock.release()
- 
-    
+
+
     def next( self ):
         try:
             self._lock.acquire()
@@ -73,21 +73,21 @@
             self._lock.acquire()
             self._last+=1
             if len(self._list)>self._last:
-                return 
+                return
             else:
                 return self._list[self._last]
         finally:
             self._lock.release()
- 
-    
+
+
     def append( self, a ):
         try:
             self._lock.acquire()
             list.append(a)
         finally:
             self._lock.release()
-            
 
+
 #original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
 class MultiThread:
     def __init__( self, function, argsVector, maxThreads=5, print_when_finished=None, sleep_time = 0):
@@ -98,7 +98,7 @@
         self.running = 0
         self.init_len_list = len(argsVector)
         self.sleep_time = sleep_time
-        
+
         if maxThreads==-1:
             nb_thread=len(argsVector)
         elif maxThreads<=0:
@@ -110,7 +110,7 @@
             nb_thread=len(argsVector)
         for i in range( nb_thread ):
             self._threadPool.append( Thread( target=self._tailRecurse ) )
-            
+
     def _tailRecurse( self ):
         for args in self._argsIterator:
             self._function( args )
@@ -120,17 +120,18 @@
                 print self.print_when_finish(),"left running: %d/%d"%(self.running,self.init_len_list)
             else:
                 print self.print_when_finish,"left running: %d/%d"%(self.running,self.init_len_list)
-                    
+
     def start( self  ):
         for thread in self._threadPool:
-            time.sleep( self.sleep_time ) # necessary to give other threads a chance to run
+            # necessary to give other threads a chance to run
+            time.sleep( self.sleep_time )
             self.running+=1
             thread.start()
-            
+
     def join( self, timeout=None ):
         for thread in self._threadPool:
             thread.join( timeout )
-                    
+
 class DBIBase:
 
     def __init__(self, commands, **args ):
@@ -198,9 +199,9 @@
             self.post_batch = [self.post_batch]
 
     def n_avail_machines(self): raise NotImplementedError, "DBIBase.n_avail_machines()"
-    
+
     def add_commands(self,commands): raise NotImplementedError, "DBIBase.add_commands()"
-    
+
     def get_redirection(self,stdout_file,stderr_file):
         """Calcule the needed redirection based of the objects attribute
         return a tuple (stdout,stderr) that can be used with popen
@@ -214,7 +215,7 @@
         elif int(self.file_redirect_stderr):
             error = open(stderr_file, 'w')
         return (output,error)
-            
+
     def exec_pre_batch(self):
         # Execute pre-batch
         if len(self.pre_batch)>0:
@@ -224,17 +225,17 @@
                 self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
             else:
                 print "[DBI] pre_batch_command:",pre_batch_command
-            
+
     def exec_post_batch(self):
         # Execute post-batch
         if len(self.post_batch)>0:
-            post_batch_command = ';'.join( self.post_batch )        
+            post_batch_command = ';'.join( self.post_batch )
             if not self.test:
                 (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
                 self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
             else:
                 print "[DBI] post_batch_command:",post_batch_command
-            
+
     def clean(self):
         print "[DBI] WARNING the clean function was not overrided by the sub class!"
 
@@ -249,7 +250,7 @@
         running=0
         waiting=0
         init=0
-        unfinished=[]        
+        unfinished=[]
         for t in self.tasks:
             if t.status==STATUS_INIT:
                 init+=1
@@ -266,7 +267,7 @@
                 print "[DBI] jobs %i have a bad status: %d",t.id
             print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, init: %d"%(len(self.tasks),finished, running, waiting, init)
             print "[DBI] jobs unfinished (starting at 1): ",unfinished
-                                
+
 class Task:
 
     def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, id=-1, gen_unique_id = True, args = {}):
@@ -285,7 +286,7 @@
         for key in args.keys():
             self.__dict__[key] = args[key]
         self.dolog = dolog
-        
+
         formatted_command = re.sub( '[^a-zA-Z0-9]', '_', command );
         if gen_unique_id:
             self.unique_id = get_new_sid('')#compation intense
@@ -338,7 +339,7 @@
         except:
             pass
         return None
-        
+
     def get_stderr(self):
         try:
             if isinstance(self.p.stderr, file):
@@ -354,7 +355,7 @@
             set_config_value(self.log_file, 'STATUS',str(STATUS_WAITING))
             set_config_value(self.log_file, 'SCHEDULED_TIME',
                              time.strftime(self.time_format, time.localtime(time.time())))
-            
+
     def get_waiting_time(self):
         # get the string representation
         str_sched = get_config_value(self.log_file,'SCHEDULED_TIME')
@@ -402,7 +403,7 @@
         self.nb_proc=int(self.nb_proc)
         self.backend_failed=0
         self.jobs_failed=0
-        
+
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
 
@@ -423,7 +424,7 @@
     def run_one_job(self, task):
         DBIBase.run(self)
         task.status=STATUS_RUNNING
-        
+
         remote_command=string.join(task.commands,';')
         filename=os.path.join(self.tmp_dir,task.unique_id)
         filename=os.path.abspath(filename)
@@ -432,8 +433,8 @@
         f.close()
         os.chmod(filename, 0750)
         self.temp_files.append(filename)
-        
-        command = "cluster" 
+
+        command = "cluster"
         if self.arch == "32":
             command += " --typecpu 32bits"
         elif self.arch == "64":
@@ -478,7 +479,7 @@
         elif task.dbi_return_status!=0:
             self.jobs_failed+=1
         task.status=STATUS_FINISHED
-  
+
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
         if self.test:
@@ -510,13 +511,13 @@
                 print "[DBI] Catched KeyboardInterrupt"
                 self.print_jobs_status()
                 raise
-            
+
         else:
             print "[DBI] WARNING jobs not started!"
         self.print_jobs_status()
         print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
         print "[DBI] Their was %d jobs that returned a failure status."%(self.jobs_failed)
-        
+
 class DBIbqtools(DBIBase):
 
     def __init__( self, commands, **args ):
@@ -531,7 +532,7 @@
 
         self.nb_proc = int(self.nb_proc)
         self.micro = int(self.micro)
-        
+
 ### We can't accept the symbols "," as this cause trouble with bqtools
         if self.log_dir.find(',')!=-1 or self.log_file.find(',')!=-1:
             print "[DBI] ERROR: The log file and the log dir should not have the symbol ','"
@@ -544,7 +545,7 @@
         print "[DBI] All bqtools file will be in ",self.temp_dir
         os.mkdir(self.temp_dir)
         os.chdir(self.temp_dir)
-        
+
         if self.long:
             self.queue = "qlong at ms"
             # Get max job duration from environment variable if it is set.
@@ -640,21 +641,21 @@
         else:
             print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
             if self.dolog:
-                print "[DBI] The scheduling time will not be logged when you will submit the generated file" 
+                print "[DBI] The scheduling time will not be logged when you will submit the generated file"
 
         # Execute post-batchs
         self.exec_post_batch()
 
     def wait(self):
         print "[DBI] WARNING cannot wait until all jobs are done for bqtools, use bqwatch or bqstatus"
-                
+
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
         if not os.path.exists(self.log_dir):
             os.mkdir(self.log_dir) # condor log are always generated
-        
+
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
         self.args = args
@@ -675,10 +676,10 @@
                 c=command
                 c2=""
 
-            # We use the absolute path so that we don't have corner case as with ./ 
+            # We use the absolute path so that we don't have corner case as with ./
             c = os.path.normpath(os.path.join(os.getcwd(), c))
             command = "".join([c,c2])
-            
+
                 # We will execute the command on the specified architecture
                 # if it is specified. If the executable exist for both
                 # architecture we execute on both. Otherwise we execute on the
@@ -723,7 +724,7 @@
             else:
                 self.targetcondorplatform=self.cplat
                 newcommand=command
-            
+
             if not os.path.exists(c):
                 raise Exception("The command '"+c+"' do not exist!")
             elif not os.access(c, os.X_OK):
@@ -734,7 +735,7 @@
                                    self.post_tasks,self.dolog,id,False,
                                    self.args))
             id+=1
-            #keeps a list of the temporary files created, so that they can be deleted at will            
+            #keeps a list of the temporary files created, so that they can be deleted at will
 
     def run_all_job(self):
         if len(self.tasks)==0:
@@ -750,17 +751,17 @@
                 condor_datas.append(condor_data)
                 self.temp_files.append(condor_data)
                 param_dat = open(condor_data, 'w')
-                
+
                 param_dat.write( dedent('''\
                 #!/bin/bash
                 %s''' %('\n'.join(task.commands))))
                 param_dat.close()
-        
 
+
         condor_file = os.path.join(self.tmp_dir, self.unique_id + ".condor")
         self.temp_files.append(condor_file)
         condor_dat = open( condor_file, 'w' )
-        
+
         req=""
         if self.targetcondorplatform == 'BOTH':
             req="((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
@@ -802,7 +803,7 @@
                 if mtimed>mtimel:
                     print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your need!'
                     overwrite_launch_file=True
-        
+
         if not os.path.exists(launch_file) or overwrite_launch_file:
             self.temp_files.append(launch_file)
             launch_dat = open(launch_file,'w')
@@ -838,7 +839,7 @@
         if not os.path.exists('configobj.py'):
             shutil.copy( get_plearndir()+
                          '/python_modules/plearn/parallel/configobj.py',  configobj_file)
-            self.temp_files.append(configobj_file)            
+            self.temp_files.append(configobj_file)
             os.chmod(configobj_file, 0755)
 
         # Launch condor
@@ -851,8 +852,8 @@
         else:
             print "[DBI] Created condor file: " + condor_file
             if self.dolog:
-                print "[DBI] The scheduling time will not be logged when you will submit the condor file" 
-            
+                print "[DBI] The scheduling time will not be logged when you will submit the condor file"
+
     def clean(self):
         if len(self.temp_files)>0:
             sleep(20)
@@ -861,21 +862,21 @@
                     os.remove(file_name)
                 except os.error:
                     pass
-                pass    
+                pass
 
 
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
 
         self.exec_pre_batch()
-        
+
         self.run_all_job()
 
         self.exec_post_batch()
 
     def wait(self):
         print "[DBI] WARNING no waiting for all job to finish implemented for condor, use 'condor_q' or 'condor_wait %s/condor.log'"%(self.log_dir)
-                
+
     def clean(self):
         pass
 
@@ -890,7 +891,7 @@
         self.started=0
         self.nb_proc=int(self.nb_proc)
         self.add_commands(commands)
-            
+
     def add_commands(self,commands):
         if not isinstance(commands, list):
             commands=[commands]
@@ -913,23 +914,23 @@
             else:
                 c=command
                 c2=""
-                
-            # We use the absolute path so that we don't have corner case as with ./ 
+
+            # We use the absolute path so that we don't have corner case as with ./
             c = os.path.normpath(os.path.join(os.getcwd(), c))
             command = "".join([c,c2])
-            
+
             # We will execute the command on the specified architecture
             # if it is specified. If the executable exist for both
             # architecture we execute on both. Otherwise we execute on the
             # same architecture as the architecture of the launch computer
-            
+
             if not os.access(c, os.X_OK):
                 raise Exception("The command '"+c+"' do not exist or have execution permission!")
             self.tasks.append(Task(command, tmp_dir, log_dir,
                                    time_format, pre_tasks,
                                    post_tasks,dolog,id,False,self.args))
             id+=1
-        #keeps a list of the temporary files created, so that they can be deleted at will            
+        #keeps a list of the temporary files created, so that they can be deleted at will
 
     def run_one_job(self,task):
         c = (';'.join(task.commands))
@@ -946,7 +947,7 @@
         p = Popen(c, shell=True,stdout=output,stderr=error)
         p.wait()
         task.status=STATUS_FINISHED
-            
+
     def clean(self):
         if len(self.temp_files)>0:
             sleep(20)
@@ -955,7 +956,7 @@
                     os.remove(file_name)
                 except os.error:
                     pass
-                pass    
+                pass
 
     def run(self):
         if self.test:
@@ -972,11 +973,11 @@
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :("[DBI,%s]"%time.ctime()))
         self.mt.start()
-        
+
         # Execute post-batchs
         self.exec_post_batch()
-            
-        
+
+
     def clean(self):
         pass
 
@@ -993,7 +994,7 @@
             print "[DBI] WARNING jobs not started!"
         self.print_jobs_status()
         print "[DBI] The Log file are under %s"%self.log_dir
-                
+
 class SshHost:
     def __init__(self, hostname,nice=19,get_avail=True):
         self.hostname= hostname
@@ -1004,7 +1005,7 @@
         self.nice=nice
         if get_avail:
             self.getAvailability()
-        
+
     def getAvailability(self):
         # simple heuristic: mips / load
         t= time.time()
@@ -1013,7 +1014,7 @@
             self.lastupd= t
             #print  self.hostname, self.bogomips, self.loadavg, (self.bogomips / (self.loadavg + 0.5))
         return self.bogomips / (self.loadavg + 0.5)
-        
+
     def getAllHostInfo(self):
         cmd= ["ssh", self.hostname ,"cat /proc/cpuinfo;cat /proc/loadavg"]
         p= Popen(cmd, stdout=PIPE)
@@ -1056,7 +1057,7 @@
 
     def __repr__(self):
         return str(self)
-        
+
 def find_all_ssh_hosts():
     hostspath_list = [os.path.join(os.getenv("HOME"),".pymake",get_platform()+'.hosts')]
     if os.path.exists(hostspath_list[0])==0:
@@ -1076,7 +1077,7 @@
         if s.working:
             h.append(s)
         else:
-            print "[DBI] host not working:",s.hostname            
+            print "[DBI] host not working:",s.hostname
         print s
     print h
     return h
@@ -1099,7 +1100,7 @@
     def add_commands(self,commands):
         if not isinstance(commands, list):
             commands=[commands]
-            
+
         # create the information about the tasks
         id=len(self.tasks)+1
         for command in commands:
@@ -1108,31 +1109,31 @@
                                    self.post_tasks,self.dolog,id,False,
                                    self.args))
             id+=1
-            
+
     def getHost(self):
         self.hosts.sort(cmp= cmp_ssh_hosts)
         print "hosts= "
         for h in self.hosts: print h
         self.hosts[0].addToLoadavg(1.0)
         return self.hosts[0]
-    
+
     def run_one_job(self, task):
         DBIBase.run(self)
 
         host= self.getHost()
-        
+
         cwd= os.getcwd()
         command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
         print "[DBI] "+command
 
         if self.test:
             return
-        
+
         task.launch_time = time.time()
         task.set_scheduled_time()
-        
+
         (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
-        
+
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
         task.p.wait()
         task.status=STATUS_FINISHED
@@ -1147,21 +1148,21 @@
             print "task",task
             command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + " ; echo $?'"
             print "[DBI, %s] %s"%(time.ctime(),command)
-            
+
             if self.test:
                 return
-        
+
             task.launch_time = time.time()
             task.set_scheduled_time()
-        
+
 ###            (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
-        
+
             task.p = Popen(command, shell=True,stdout=PIPE,stderr=PIPE)
             wait = task.p.wait()
             returncode = p.returncode
             if returncode:
                 self.working=False
-                
+
             elif wait!=0:
                 self.working=False
                 #redo it
@@ -1182,7 +1183,7 @@
                 print "return status", task.return_status
             sleep(1)
             task.status=STATUS_FINISHED
-      
+
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
         if not self.file_redirect_stdout and self.nb_proc>1:



From lamblin at mail.berlios.de  Wed Oct 17 21:05:37 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 17 Oct 2007 21:05:37 +0200
Subject: [Plearn-commits] r8193 - trunk/plearn/math
Message-ID: <200710171905.l9HJ5bRZ001397@sheep.berlios.de>

Author: lamblin
Date: 2007-10-17 21:05:36 +0200 (Wed, 17 Oct 2007)
New Revision: 8193

Modified:
   trunk/plearn/math/PRandom.cc
   trunk/plearn/math/PRandom.h
Log:
Code to help debugging RNGs.


Modified: trunk/plearn/math/PRandom.cc
===================================================================
--- trunk/plearn/math/PRandom.cc	2007-10-17 19:02:56 UTC (rev 8192)
+++ trunk/plearn/math/PRandom.cc	2007-10-17 19:05:36 UTC (rev 8193)
@@ -54,6 +54,9 @@
 // PRandom //
 /////////////
 PRandom::PRandom(int32_t seed):
+#ifdef BOUNDCHECK
+    samples_count(0),
+#endif
     exponential_distribution(0),
     normal_distribution(0),
     uniform_01(0),
@@ -66,6 +69,9 @@
 }
 
 PRandom::PRandom(const PRandom& rhs):
+#ifdef BOUNDCHECK
+    samples_count           (rhs.get_samples_count()),
+#endif
     rgen                    (*(rhs.get_rgen())),
     the_seed                (rhs.get_the_seed()),
     fixed_seed              (rhs.get_fixed_seed()),
@@ -86,6 +92,9 @@
 
 PRandom PRandom::operator=(const PRandom& rhs)
 {
+#ifdef BOUNDCHECK
+    samples_count = rhs.get_samples_count();
+#endif
     rgen =          *(rhs.get_rgen());
     the_seed =      rhs.get_the_seed();
     fixed_seed =    rhs.get_fixed_seed();
@@ -206,6 +215,9 @@
 ////////////////
 real PRandom::exp_sample() {
     ensure_exponential_distribution();
+#ifdef BOUNDCHECK
+    samples_count++;
+#endif
     return real((*exponential_distribution)(*uniform_01));
 }
 
@@ -255,6 +267,11 @@
 /////////////////
 real PRandom::gaussian_01() {
     ensure_normal_distribution();
+#ifdef BOUNDCHECK
+    // Drawing one Gaussian sample has the same effect on the underlying
+    // generator as drawing two uniform or exponential samples
+    samples_count += 2;
+#endif
     return real((*normal_distribution)(*uniform_01));
 }
 
@@ -304,6 +321,9 @@
     // Systematically construct the uniform_01 member, which is the basis for most
     // of the random operations.
     ensure_uniform_01();
+#ifdef BOUNDCHECK
+    samples_count = 0;
+#endif
 }
 
 ////////////////////////
@@ -359,6 +379,9 @@
 // uniform_sample //
 ////////////////////
 real PRandom::uniform_sample() {
+#ifdef BOUNDCHECK
+    samples_count++;
+#endif
     return real((*uniform_01)());
 }
 
@@ -416,4 +439,3 @@
   End:
 */
 // vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :
-        

Modified: trunk/plearn/math/PRandom.h
===================================================================
--- trunk/plearn/math/PRandom.h	2007-10-17 19:02:56 UTC (rev 8192)
+++ trunk/plearn/math/PRandom.h	2007-10-17 19:05:36 UTC (rev 8193)
@@ -65,11 +65,16 @@
 {
 
 private:
-  
+
     typedef Object inherited;
 
 protected:
 
+#ifdef BOUNDCHECK
+    //! Number of samples drawn from the underlying generator.
+    int samples_count;
+#endif
+
     //! The underlying Boost random number generator used.
     boost::mt19937 rgen;
 
@@ -135,6 +140,9 @@
     uint32_t get_the_seed()   const { return the_seed; }
     int32_t  get_fixed_seed() const { return fixed_seed; }
     int32_t  get_seed()       const { return seed_; }
+#ifdef BOUNDCHECK
+    int get_samples_count()   const { return samples_count; }
+#endif
 
 private:
 
@@ -358,7 +366,7 @@
             w /= (1. - old_w); // New sum of weights is 1 minus what we took out
         }
     }
-    
+
     return r;
 }
 



From nouiz at mail.berlios.de  Wed Oct 17 22:41:57 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 17 Oct 2007 22:41:57 +0200
Subject: [Plearn-commits] r8194 - trunk/python_modules/plearn/pymake
Message-ID: <200710172041.l9HKfvUc006908@sheep.berlios.de>

Author: nouiz
Date: 2007-10-17 22:41:57 +0200 (Wed, 17 Oct 2007)
New Revision: 8194

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Changed default value of verbose. In default we do not need the command value executed.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-10-17 19:05:36 UTC (rev 8193)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-10-17 20:41:57 UTC (rev 8194)
@@ -115,8 +115,8 @@
                   N.B. this option is not related to -local_ofiles.
   -v[...]: specify the verbosity level, '-vvv' is equivalent to '-v3'.
            1 -> get *** Running ... / ++++ Computing
-           2 -> get Launched / Finished / Still waiting
-           3 -> get lauched command (default)
+           2 -> get Launched / Finished / Still waiting (default)
+           3 -> get lauched command
            4 -> get extra information
 
 There exist special options that will not compile or link the target, but
@@ -2476,7 +2476,7 @@
     # nice default command and value
     nice_command = 'env nice -n'
     default_nice_value = 10
-    verbose=3
+    verbose=2
 
     ######## Regular pymake processing
 



From nouiz at mail.berlios.de  Thu Oct 18 16:47:39 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Oct 2007 16:47:39 +0200
Subject: [Plearn-commits] r8195 - trunk/scripts
Message-ID: <200710181447.l9IEld3p027078@sheep.berlios.de>

Author: nouiz
Date: 2007-10-18 16:47:39 +0200 (Thu, 18 Oct 2007)
New Revision: 8195

Modified:
   trunk/scripts/dbidispatch
Log:
truncate log dir in case their is too many parameter


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-10-17 20:41:57 UTC (rev 8194)
+++ trunk/scripts/dbidispatch	2007-10-18 14:47:39 UTC (rev 8195)
@@ -242,6 +242,7 @@
     tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
     ### We need to remove the symbols "," as this cause trouble with bqtools
     tmp=re.sub( ',', '-', tmp )
+    tmp=truncate( tmp, 200)
     tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
     dbi_param["log_dir"]=os.path.join("LOGS",tmp)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')



From nouiz at mail.berlios.de  Thu Oct 18 16:58:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Oct 2007 16:58:53 +0200
Subject: [Plearn-commits] r8196 - trunk/scripts
Message-ID: <200710181458.l9IEwraE027577@sheep.berlios.de>

Author: nouiz
Date: 2007-10-18 16:58:50 +0200 (Thu, 18 Oct 2007)
New Revision: 8196

Modified:
   trunk/scripts/dbidispatch
Log:
bugfix


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-10-18 14:47:39 UTC (rev 8195)
+++ trunk/scripts/dbidispatch	2007-10-18 14:58:50 UTC (rev 8196)
@@ -242,7 +242,7 @@
     tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
     ### We need to remove the symbols "," as this cause trouble with bqtools
     tmp=re.sub( ',', '-', tmp )
-    tmp=truncate( tmp, 200)
+    tmp=tmp[:200]
     tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
     dbi_param["log_dir"]=os.path.join("LOGS",tmp)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')



From nouiz at mail.berlios.de  Thu Oct 18 18:37:51 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Oct 2007 18:37:51 +0200
Subject: [Plearn-commits] r8197 - trunk/python_modules/plearn/parallel
Message-ID: <200710181637.l9IGbpEB021490@sheep.berlios.de>

Author: nouiz
Date: 2007-10-18 18:37:50 +0200 (Thu, 18 Oct 2007)
New Revision: 8197

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
truncate again...


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-18 14:58:50 UTC (rev 8196)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-18 16:37:50 UTC (rev 8197)
@@ -292,7 +292,7 @@
             self.unique_id = get_new_sid('')#compation intense
             self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
         else:
-            self.unique_id = formatted_command+'_'+str(datetime.datetime.now()).replace(' ','_')
+            self.unique_id = formatted_command[:200]+'_'+str(datetime.datetime.now()).replace(' ','_')
             self.log_file = os.path.join(log_dir, self.unique_id) + ".log"
 
         if self.add_unique_id:



From larocheh at mail.berlios.de  Thu Oct 18 23:10:45 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 18 Oct 2007 23:10:45 +0200
Subject: [Plearn-commits] r8198 - trunk/plearn_learners_experimental
Message-ID: <200710182110.l9ILAjW4001299@sheep.berlios.de>

Author: larocheh
Date: 2007-10-18 23:10:42 +0200 (Thu, 18 Oct 2007)
New Revision: 8198

Modified:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
Log:
Corrected some bugs...


Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-18 16:37:50 UTC (rev 8197)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-18 21:10:42 UTC (rev 8198)
@@ -822,7 +822,7 @@
         // accumulate positive stats using the expectation
         // we deep-copy because the value will change during negative phase
         pos_down_val = expectations[index];
-        pos_up_val = greedy_layers[index]->expectation;
+        pos_up_val << greedy_layers[index]->expectation;
         
         // down propagation, starting from a sample of layers[index+1]
         greedy_connections[index]->setAsUpInput( greedy_layers[index]->sample );
@@ -983,6 +983,7 @@
     if(layer == 0)
     {
         representation.resize(input.length());
+        expectations[0] << input;
         representation << input;
         return;
     }



From nouiz at mail.berlios.de  Fri Oct 19 21:27:52 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Oct 2007 21:27:52 +0200
Subject: [Plearn-commits] r8199 - in trunk: . python_modules/plearn/pymake
Message-ID: <200710191927.l9JJRqtv022290@sheep.berlios.de>

Author: nouiz
Date: 2007-10-19 21:27:52 +0200 (Fri, 19 Oct 2007)
New Revision: 8199

Modified:
   trunk/pymake.config.model
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Now use PYMAKE_OSARCH as platform if it is present


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-10-18 21:10:42 UTC (rev 8198)
+++ trunk/pymake.config.model	2007-10-19 19:27:52 UTC (rev 8199)
@@ -93,7 +93,7 @@
 # Feel free to use it to adapt configuration to that platform.
 
 # (sys.byteorder does not exist in older versions of python)
-if platform=='linux-i386':
+if platform.startswith('linux-i386'):
     cpp_definitions += [ 'LINUX', 'LITTLEENDIAN' ]
 elif platform=='linux-ppc':
     cpp_definitions += [ 'LINUXPPC', 'BIGENDIAN' ]
@@ -101,7 +101,7 @@
     cpp_definitions += [ 'SPARC', 'BIGENDIAN' ]
 elif platform=='linux-ia64':
     cpp_definitions += [ 'LINUX', 'LITTLEENDIAN' ]
-elif platform=='linux-x86_64':
+elif platform.startswith('linux-x86_64'):
     cpp_definitions += [ 'LINUX', 'LITTLEENDIAN' ]
 elif platform=='sunos5' :
     cpp_definitions += [ 'SPARC', 'BIGENDIAN' ]

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-10-18 21:10:42 UTC (rev 8198)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-10-19 19:27:52 UTC (rev 8199)
@@ -209,6 +209,9 @@
     return result
 
 def get_platform():
+    pymake_osarch = os.getenv('PYMAKE_OSARCH')
+    if pymake_osarch:
+        return pymake_osarch
     platform = sys.platform
     if platform=='linux2':
         linux_type = os.uname()[4]



From louradou at mail.berlios.de  Mon Oct 22 20:10:51 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 22 Oct 2007 20:10:51 +0200
Subject: [Plearn-commits] r8200 - trunk/plearn_learners/online
Message-ID: <200710221810.l9MIApHE024694@sheep.berlios.de>

Author: louradou
Date: 2007-10-22 20:10:48 +0200 (Mon, 22 Oct 2007)
New Revision: 8200

Modified:
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Log:
The module to add a cost function on layers
that depend on the layer outputs
(e.g.: correlation between neuron activities...).
Some clean up of the code.
Now you can use this CostModule not only within NetworkModule
but also with PLearners such as DeepBeliefNet.



Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-10-19 19:27:52 UTC (rev 8199)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-10-22 18:10:48 UTC (rev 8200)
@@ -45,20 +45,22 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     LayerCostModule,
-    "Computes a cost function on Layer, given:            \n",
-    "* Expectations for a binomial RBM hidden layer, or   \n"
-    "* sigmoid(activation) for a layer of a Neural Net, or\n"
-    "* real outputs of any layer                          \n"
-    "and Back-propagates the gradient.                    \n"
-    "\n"
-    "Several cost functions can be chosen.\n"
-    "Some only apply for binomial layers. \n");
+    "Computes a cost function on Layer given its outputs only, and Back-propagates the gradient.\n",
+    "The input port of this Module must be connected to:\n"
+    "- Expectations of a RBM hidden layer (e.g. in a DBN), or\n"
+    "- Activations of a layer (in a Neural Net), or\n"
+    "- Real outputs of any layer.\n"
+    "Based on these values, several cost functions can be chosen.\n"
+    "Be careful: some are valid only for binomial layers. \n");
 
 LayerCostModule::LayerCostModule():
+    nstages_max(-1),
+    stage(0),
+    momentum(0.),
+    histo_size(10),
+    alpha(0.),
+    average_deriv(0.),
     cost_function(""),
-    histo_size(10),
-    alpha(0.0),
-    momentum(0.0),
     cost_function_completename("")
 {
     output_size = 1;
@@ -69,14 +71,10 @@
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 
-    redeclareOption(ol, "input_size", &LayerCostModule::input_size,
-                     OptionBase::nosave,
-        "Size of the layer.");
-
     declareOption(ol, "cost_function", &LayerCostModule::cost_function,
                   OptionBase::buildoption,
         "The cost function applied to the layer:\n"
-        "- \"pascal\":"
+        "- \"pascal\" [default]:"
         " Pascal Vincent's God given cost function.\n"
         "- \"correlation\":"
         " average of a function applied to the correlations between outputs.\n"
@@ -90,6 +88,15 @@
         " average KL divergence between pairs of binomial units\n"
         );
 
+    declareOption(ol, "nstages_max", &LayerCostModule::nstages_max,
+                  OptionBase::buildoption,
+        "Maximal number of updates for which the gradient of the cost function will be propagated.\n"
+	"-1 means: always train without limit.\n");
+
+    declareOption(ol, "momentum", &LayerCostModule::momentum,
+                  OptionBase::buildoption,
+        "(in [0,1[) For non stochastic cost functions, momentum to compute the moving means.\n");
+
     declareOption(ol, "histo_size", &LayerCostModule::histo_size,
                   OptionBase::buildoption,
         "For \"kl_div\" cost functions,\n"
@@ -102,33 +109,12 @@
         "number of bins for the histograms (to estimate distributions of outputs).\n"
         "The higher is histo_size, the more precise is the estimation.\n");
 
-    declareOption(ol, "momentum", &LayerCostModule::momentum,
-                  OptionBase::buildoption,
-        "(in [0,1[) For \"pascal\" cost function, momentum for the moving means.\n");
-
-
-
-    declareOption(ol, "inputs_histo", &LayerCostModule::inputs_histo,
+    declareOption(ol, "inputs_expectation_trainMemory", &LayerCostModule::inputs_expectation_trainMemory,
                   OptionBase::learntoption,
-                  "Histograms (empirical ditribution) of the output, for all units.\n"
-        );
-
-    declareOption(ol, "inputs_expectation", &LayerCostModule::inputs_expectation,
-                  OptionBase::learntoption,
-                  "Expectation of the output (in [0,1[), for all units.\n"
-        );
-
-    declareOption(ol, "inputs_stds", &LayerCostModule::inputs_stds,
-                  OptionBase::learntoption,
-                  "Standard Deviation of the output, for all units.\n"
-        );
-
-    declareOption(ol, "inputs_correlations", &LayerCostModule::inputs_correlations,
-                  OptionBase::learntoption,
                   "Correlation of the outputs, for all pairs of units.\n"
         );
 
-    declareOption(ol, "inputs_cross_quadratic_mean", &LayerCostModule::inputs_cross_quadratic_mean,
+    declareOption(ol, "inputs_cross_quadratic_mean_trainMemory", &LayerCostModule::inputs_cross_quadratic_mean_trainMemory,
                   OptionBase::learntoption,
                   "Expectation of the cross products between outputs, for all pairs of units.\n"
         );
@@ -137,6 +123,11 @@
                   OptionBase::learntoption,
                   "complete name of cost_function (take into account some internal settings).\n"
         );
+
+    declareOption(ol, "stage", &LayerCostModule::stage,
+                  OptionBase::learntoption,
+                  "number of stages that has been done during the training.\n"
+        );
 }
 
 void LayerCostModule::build_()
@@ -145,7 +136,8 @@
     PLASSERT( momentum >= 0.0);
     PLASSERT( momentum < 1);
 
-    norm_factor = 1./(real)(input_size*(input_size-1));
+    if( input_size > 1 )
+        norm_factor = 1./(real)(input_size*(input_size-1));
 
     string im = lowerstring( cost_function );
     // choose HERE the *default* cost function
@@ -157,43 +149,53 @@
         cost_function_completename = string(cost_function);
 
      // list HERE all *stochastic* cost functions
-    if( ( cost_function == "stochastic_cross_entropy")
-     || ( cost_function == "stochastic_kl_div") )
+    if( ( cost_function == "stochastic_cross_entropy" )
+     || ( cost_function == "stochastic_kl_div" ) )
         is_cost_function_stochastic = true;
 
     // list HERE all *non stochastic* cost functions
     // and the specific initialization
-    else if( ( cost_function == "kl_div")
-          || ( cost_function == "kl_div_simple") )
+    else if( ( cost_function == "kl_div" )
+          || ( cost_function == "kl_div_simple" ) )
     {
         is_cost_function_stochastic = false;
-        if( input_size > 1 )
+        if( input_size > 0 )
             inputs_histo.resize(input_size,histo_size);
         HISTO_STEP = 1.0/(real)histo_size;
+
+	if( cost_function == "kl_div" )
+	{
+	    cache_differ_count_i.resize(input_size);
+	    cache_differ_count_j.resize(input_size);
+	    cache_n_differ.resize(input_size);
+	    for( int i = 0; i < input_size; i ++)
+	    {
+	        cache_differ_count_i[i].resize(i);
+	        cache_differ_count_j[i].resize(i);
+	        cache_n_differ[i].resize(i);
+  	        for( int j = 0; j < i; j ++)
+	        {
+	            cache_differ_count_i[i][j].resize(histo_size);
+		    cache_differ_count_j[i][j].resize(histo_size);
+		    cache_n_differ[i][j].resize(histo_size);
+	        }
+            }
+        }
     }
-    else if( (cost_function == "pascal" )
-          || (cost_function == "correlation" ) )
+    else if( ( cost_function == "pascal" )
+          || ( cost_function == "correlation" ) )
     {
         is_cost_function_stochastic = false;
-        if( input_size > 1 )
+        if( ( input_size > 0 ) && (momentum > 0.0) )
         {
-            inputs_expectation.resize(input_size);
-            inputs_cross_quadratic_mean.resize(input_size,input_size);
-            if( cost_function == "correlation" )
-            {
-                inputs_stds.resize(input_size);
-                inputs_correlations.resize(input_size,input_size);
-            }
-            if( momentum > 0.0)
-            {
-                inputs_expectation_trainMemory.resize(input_size);
-                inputs_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
-            }
-            if( cost_function == "pascal" )
-                cost_function_completename = addprepostfix( func_pascal_prefix(), "_", cost_function );
-            else if( cost_function == "correlation" )
-                cost_function_completename = addprepostfix( func_correlation_prefix(), "_", cost_function );
+            inputs_expectation_trainMemory.resize(input_size);
+            inputs_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
         }
+        string slink = "_";
+        if( cost_function == "pascal" )
+            cost_function_completename = "exp_pascal"; //addprepostfix( func_pascal_prefix(), slink, cost_function );
+        else if( cost_function == "correlation" )
+            cost_function_completename = "exp_correlation" ; //addprepostfix( func_correlation_prefix(), slink, cost_function );
     }
     else
         PLERROR("LayerCostModule::build_() does not recognize cost function %s",
@@ -211,15 +213,30 @@
     port_sizes(getPortIndex("cost"), 1) = 1;
 }
 
-
-// ### Nothing to add here, simply calls build_
 void LayerCostModule::build()
 {
     inherited::build();
     build_();
 }
 
+void LayerCostModule::forget()
+{
+    inputs_histo.clear();
 
+    inputs_expectation.clear();
+    inputs_stds.clear();
+    
+    inputs_correlations.clear();
+    inputs_cross_quadratic_mean.clear();
+    if( momentum > 0.0)
+    {
+        inputs_expectation_trainMemory.clear();
+        inputs_cross_quadratic_mean_trainMemory.clear();
+    }
+    one_count = 0.;
+    stage = 0;
+}
+
 void LayerCostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -235,12 +252,14 @@
     deepCopyField(inputs_expectation_trainMemory, copies);
     deepCopyField(inputs_cross_quadratic_mean_trainMemory, copies);
 
+    deepCopyField(cache_differ_count_i, copies);
+    deepCopyField(cache_differ_count_j, copies);
+    deepCopyField(cache_n_differ, copies);
+    
     deepCopyField(ports, copies);
 }
 
 
-
-
 ///////////
 // fprop //
 ///////////
@@ -248,8 +267,6 @@
 
 void LayerCostModule::fprop(const TVec<Mat*>& ports_value)
 {
-    PLASSERT( input_size > 1 );
-
     Mat* p_inputs = ports_value[getPortIndex("input")];
     Mat* p_costs = ports_value[getPortIndex("cost")];
 
@@ -263,14 +280,29 @@
     }
 }
 
-void LayerCostModule::fprop(const Mat& inputs, Mat& costs)
+void LayerCostModule::fprop(const Mat& inputs, const Mat& targets, Mat& costs) const
 {
+    fprop( inputs, costs );
+}
+
+void LayerCostModule::fprop(const Mat& inputs, Mat& costs) const
+{
+    PLASSERT( input_size > 1 );
     int n_samples = inputs.length();
     costs.resize( n_samples, output_size );
 
+    // The fprop will be done during training (only needed computations)
+    if( during_training )
+    {
+        costs.fill( MISSING_VALUE );
+        return;
+    }
+    else
+        costs.clear();
+    
     if( !is_cost_function_stochastic )
     {
-        costs.clear(); // costs(i,0) = 0
+        PLASSERT( inputs.width() == input_size );
 
         if( cost_function == "kl_div" )
         {
@@ -299,9 +331,10 @@
         //!        SEE function computeKLdiv().
         //! ************************************************************
 
-            computeHisto(inputs);
 
-            costs(0,0) = computeKLdiv();
+	    Mat histo;
+	    computeHisto( inputs, histo );
+            costs(0,0) = computeKLdiv( histo );
         }
         else if( cost_function == "kl_div_simple" )
         {
@@ -314,13 +347,14 @@
         //! SEE function computeSafeHisto(real ).
         //! ************************************************************
 
-            computeSafeHisto(inputs);
+            Mat histo;
+	    computeSafeHisto( inputs, histo );
 
             // Computing the KL divergence
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
                     for (int k = 0; k < histo_size; k++)
-                        costs(0,0) += KLdivTerm( inputs_histo(i,k), inputs_histo(j,k));
+                        costs(0,0) += KLdivTerm( histo(i,k), histo(j,k));
 
             // Normalization w.r.t. number of units
             costs(0,0) *= norm_factor;
@@ -339,17 +373,18 @@
         //!
         //! ************************************************************
 
-            computePascalStatistics(inputs);
+            Vec expectation;
+	    Mat cross_quadratic_mean;
+	    computePascalStatistics( inputs, expectation, cross_quadratic_mean );
 
             // Computing the cost
             for (int i = 0; i < input_size; i++)
             {
                 if (alpha > 0.0 )
-                    costs(0,0) -= alpha * func_pascal(inputs_expectation[i]) *(real)(input_size-1);
+                    costs(0,0) -= alpha * func_pascal( expectation[i] ) *(real)(input_size-1);
                 for (int j = 0; j < i; j++)
-                    costs(0,0) += func_pascal(inputs_cross_quadratic_mean(i,j));
+                    costs(0,0) += func_pascal( cross_quadratic_mean(i,j) );
             }
-
             costs(0,0) *= norm_factor;
         }
         else if( cost_function == "correlation" )
@@ -369,22 +404,23 @@
         //!
         //! ************************************************************
 
-            computeCorrelationStatistics(inputs);
+            Vec expectation;
+	    Mat cross_quadratic_mean;
+            Vec stds;
+	    Mat correlations;
+            computeCorrelationStatistics( inputs, expectation, cross_quadratic_mean, stds, correlations );
 
             // Computing the cost
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
-                    costs(0,0) += func_correlation( inputs_correlations(i,j) );
+                    costs(0,0) += func_correlation( correlations(i,j) );
 
             costs(0,0) *= norm_factor;
         }
-
-
-        return; // Do not fprop with the conventional stochastic fprop...
     }
-
-    for (int isample = 0; isample < n_samples; isample++)
-        fprop(inputs(isample), costs(isample,0));
+    else // stochastic cost function
+        for (int isample = 0; isample < n_samples; isample++)
+            fprop(inputs(isample), costs(isample,0));
 }
 
 void LayerCostModule::fprop(const Vec& input, real& cost) const
@@ -487,19 +523,25 @@
 
 
 ////////////////////
-// bpropAccUpdate //
+// bpropUpdate //
 ////////////////////
 
 
+void LayerCostModule::bpropUpdate(const Mat& inputs,
+                                  const Mat& targets,
+                                  const Vec& costs,
+                                  Mat& inputs_grad, bool accumulate)
+{
+    bpropUpdate( inputs, inputs_grad);
+}
+
 void LayerCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
-                                   const TVec<Mat*>& ports_gradient)
+                                     const TVec<Mat*>& ports_gradient)
 {
     PLASSERT( input_size > 1 );
     PLASSERT( ports_value.length() == nPorts() );
     PLASSERT( ports_gradient.length() == nPorts() );
 
-    cout << "bpropAccUpdate" << endl;
-
     const Mat* p_inputs = ports_value[getPortIndex("input")];
     Mat* p_inputs_grad = ports_gradient[getPortIndex("input")];
     Mat* p_cost_grad = ports_gradient[getPortIndex("cost")];
@@ -507,341 +549,346 @@
     if( p_inputs_grad && p_inputs_grad->isEmpty()
         && p_cost_grad && !p_cost_grad->isEmpty() )
     {
+	PLASSERT( p_inputs && !p_inputs->isEmpty());
         int n_samples = p_inputs->length();
+	PLASSERT( p_cost_grad->length() == n_samples );
 
-        PLASSERT( p_inputs && !p_inputs->isEmpty());
-        PLASSERT( p_inputs->length() == n_samples );
-        PLASSERT( p_cost_grad->length() == n_samples );
+        bpropUpdate( *p_inputs, *p_inputs_grad);
 
-        p_inputs_grad->resize(n_samples, input_size);
-        p_inputs_grad->clear();
+        for( int isample = 0; isample < n_samples; isample++ )
+	    for( int i = 0; i < input_size; i++ )
+	        (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0);
 
-        real qi, qj, comp_qi, comp_qj;
-        Vec comp_q(input_size), log_term(input_size);
+	checkProp(ports_gradient);
+    }
+    else if( !p_inputs_grad && !p_cost_grad )
+        return;
+    else
+        PLERROR("In LayerCostModule::bpropAccUpdate - Port configuration not implemented ");
 
-        if( cost_function == "stochastic_cross_entropy" )
+}
+
+//!  important NOTE: the normalization by one_count = 1 / n_samples
+//!                  is supposed to be done in the OnlineLearningModules updates
+//! ( cf. RBMMatrixConnection::bpropUpdate(), RBMBinomialLayer::bpropUpdate() in the batch version, etc. )
+void LayerCostModule::bpropUpdate(const Mat& inputs,
+                                  Mat& inputs_grad)
+{
+    PLASSERT( inputs.width() == input_size );
+    inputs_grad.resize(inputs.length(), input_size );
+    inputs_grad.clear();
+
+    int n_samples = inputs.length();
+    inputs_grad.resize(n_samples, input_size);
+    inputs_grad.clear();
+
+    stage += n_samples;
+    if( (nstages_max>0) && (stage > nstages_max) )
+        return;
+
+    cout << "bpropAccUpdate" << endl;
+
+    real qi, qj, comp_qi, comp_qj;
+    Vec comp_q(input_size), log_term(input_size);
+
+    if( cost_function == "stochastic_cross_entropy" )
+    {
+        for (int isample = 0; isample < n_samples; isample++)
         {
-            for (int isample = 0; isample < n_samples; isample++)
+            for (int i = 0 ; i < input_size ; i++ )
             {
-                for (int i = 0 ; i < input_size ; i++ )
+                qi = inputs(isample,i);
+                comp_qi = 1.0 - qi;
+                comp_q[i] = comp_qi;
+                log_term[i] = safeflog(qi) - safeflog(comp_qi);
+            }
+            for (int i = 0; i < input_size; i++ )
+            {
+                qi = inputs(isample,i);
+                comp_qi = comp_q[i];
+                for (int j = 0; j < i; j++ )
                 {
-                    qi = (*p_inputs)(isample,i);
-                        comp_qi = 1.0 - qi;
-                    comp_q[i] = comp_qi;
-                    log_term[i] = safeflog(qi) - safeflog(comp_qi);
+                    qj = inputs(isample,j);
+                    comp_qj=comp_q[j];
+                    // log(pj) - log(1-pj) + pj/pi - (1-pj)/(1-pi)
+                    inputs(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
+                    // The symetric part (loop  j=i+1...input_size)
+                    inputs(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
                 }
+            }
                 for (int i = 0; i < input_size; i++ )
-                {
-                    qi = (*p_inputs)(isample,i);
-                    comp_qi = comp_q[i];
-                    (*p_inputs_grad)(isample,i) = 0.0;
-                    for (int j = 0; j < i; j++ )
-                    {
-                        qj = (*p_inputs)(isample,j);
-                        comp_qj=comp_q[j];
+                    inputs_grad(isample, i) *= norm_factor;
+        }
+    } // END cost_function == "stochastic_cross_entropy"
 
-                        // log(pj) - log(1-pj) + pj/pi - (1-pj)/(1-pi)
-                        (*p_inputs_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
+    else if( cost_function == "stochastic_kl_div" )
+    {
+        for (int isample = 0; isample < n_samples; isample++)
+        {
+            for (int i = 0; i < input_size; i++ )
+            {
+                qi = inputs(isample,i);
+                comp_qi = 1.0 - qi;
+                if(fast_exact_is_equal(qi, 1.0) || fast_exact_is_equal(qi, 0.0))
+                    comp_q[i] = REAL_MAX;
+                else
+                    comp_q[i] = 1.0/(qi*comp_qi);
+                log_term[i] = safeflog(qi) - safeflog(comp_qi);
+            }
+            for (int i = 0; i < input_size; i++ )
+            {
+                qi = inputs(isample,i);
+                comp_qi = comp_q[i];
 
-                        // The symetric part (loop  j=i+1...input_size)
-                        (*p_inputs_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
-                    }
-                }
-                for (int i = 0; i < input_size; i++ )
+                for (int j = 0; j < i ; j++ )
                 {
-                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0)
-		                                    * norm_factor /(real)n_samples;
+                    qj = inputs(isample,j);
+                    comp_qj=comp_q[j];
+                    //   [qj - qi]/[qi (1-qi)] - log[ qi/(1-qi) * (1-qj)/qj]
+                    inputs_grad(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
+                    // The symetric part (loop  j=i+1...input_size)
+                    inputs_grad(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
                 }
             }
+            for (int i = 0; i < input_size; i++ )
+                inputs_grad(isample, i) *= norm_factor;
         }
+    } // END cost_function == "stochastic_kl_div"
 
-        else if( cost_function == "stochastic_kl_div" )
+    else if( cost_function == "kl_div" )
+    {
+        computeHisto(inputs);
+        real cost_before = computeKLdiv( true );
+    
+        for (int isample = 0; isample < n_samples; isample++)
         {
-            for (int isample = 0; isample < n_samples; isample++)
+            // Computing the difference of KL divergence
+            // for d_q
+            for (int i = 0; i < input_size; i++)
             {
-                for (int i = 0; i < input_size; i++ )
-                {
-                    qi = (*p_inputs)(isample,i);
-                        comp_qi = 1.0 - qi;
-                    if(fast_exact_is_equal(qi, 1.0) || fast_exact_is_equal(qi, 0.0))
-                        comp_q[i] = REAL_MAX;
-                    else
-                        comp_q[i] = 1.0/(qi*comp_qi);
-                    log_term[i] = safeflog(qi) - safeflog(comp_qi);
+                qi=inputs(isample,i);
+                if( histo_index(qi) < histo_size-1 )
+                { 
+                    inputs(isample,i) += dq(qi);
+                    computeHisto(inputs);
+                    real cost_after = computeKLdiv( false );
+                    inputs(isample,i) -= dq(qi); 
+                    inputs_grad(isample, i) = (cost_after - cost_before)*1./dq(qi);
                 }
-                for (int i = 0; i < input_size; i++ )
-                {
-                    qi = (*p_inputs)(isample,i);
-                    comp_qi = comp_q[i];
+                //else inputs_grad(isample, i) = 0.;
 
-                    (*p_inputs_grad)(isample,i) = 0.0;
-                    for (int j = 0; j < i ; j++ )
-                    {
-                        qj = (*p_inputs)(isample,j);
-                        comp_qj=comp_q[j];
+                continue;
 
-                        //   [qj - qi]/[qi (1-qi)] - log[ qi/(1-qi) * (1-qj)/qj]
-                        (*p_inputs_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
+                inputs_grad(isample, i) = 0.;
+                    
+                qi = inputs(isample,i);
+                int index_i = histo_index(qi);
+                if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                    continue;
+                real over_dqi=1.0/dq(qi);
+                // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
+                //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
+                    		    
+                for (int j = 0; j < i; j++)
+                {
+                    inputs_grad(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
 
-                        // The symetric part (loop  j=i+1...input_size)
-                        (*p_inputs_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
-                    }
+                    qj = inputs(isample,j);
+                    int index_j = histo_index(qj);
+                    if( ( index_j == histo_size-1 ) )
+                        continue;
+                    real over_dqj=1.0/dq(qj);
+                    // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
+                    //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
+                        
+                    inputs_grad(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                 }
-                for (int i = 0; i < input_size; i++ )
-                {
-                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0)
-		                                     * norm_factor /(real)n_samples;
-                }
             }
-        }
+        }            
+    } // END cost_function == "kl_div"
 
-        else if( cost_function == "kl_div" )
+    else if( cost_function == "kl_div_simple" )
+    {
+        computeSafeHisto(inputs);
+            
+        for (int isample = 0; isample < n_samples; isample++)
         {
-            computeHisto(*p_inputs);
-
-            for (int isample = 0; isample < n_samples; isample++)
+            // Computing the difference of KL divergence
+            // for d_q
+            for (int i = 0; i < input_size; i++)
             {
+                inputs_grad(isample, i) = 0.0;
 
-                // Computing the difference of KL divergence
-                // for d_q
-                for (int i = 0; i < input_size; i++)
+                qi = inputs(isample,i);
+                int index_i = histo_index(qi);
+                if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                    continue;
+                real over_dqi=1.0/dq(qi);
+                // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
+                //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
+
+                for (int j = 0; j < i; j++)
                 {
-                    (*p_inputs_grad)(isample, i) = 0.0;
+                    inputs_grad(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
 
-                    qi = (*p_inputs)(isample,i);
-                    int index_i = histo_index(qi);
-                    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                    qj = inputs(isample,j);
+                    int index_j = histo_index(qj);
+                    if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
                         continue;
-                    real over_dqi=1.0/dq(qi);
-                    int shift_i;
-                    if( over_dqi > 0.0)
-                        shift_i = 1;
-                    else
-                        shift_i = -1;
-                    // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
-                    //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-
-                    for (int j = 0; j < i; j++)
-                    {
-                        (*p_inputs_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
-
-                        qj = (*p_inputs)(isample,j);
-                        int index_j = histo_index(qj);
-                        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-                            continue;
-                        real over_dqj=1.0/dq(qj);
-                         int shift_j;
-                        if( over_dqj > 0.0)
-                            shift_j = 1;
-                        else
-                            shift_j = -1;
-                            // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
-                          //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-
-                        (*p_inputs_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
-                    }
+                    real over_dqj=1.0/dq(qj);
+                    // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
+                    //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
+                        
+                    inputs_grad(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
                 }
-
-                // Normalization
-                for (int i = 0; i < input_size; i++ )
-                {
-                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
-                }
             }
 
+            // Normalization
+            for (int i = 0; i < input_size; i++ )
+                inputs_grad(isample, i) *= norm_factor;
+        }
+    } // END cost_function == "kl_div simple"
 
-            // debug Check
-            int i=0;
-            real cost_before = computeKLdiv();
+    else if( cost_function == "pascal" )
+    {
+        computePascalStatistics( inputs );
+
+        if( momentum > 0.0 )
             for (int isample = 0; isample < n_samples; isample++)
             {
-                real qi=(*p_inputs)(isample,i);
-                if( histo_index(qi) < histo_size-1 )
+                for (int i = 0; i < input_size; i++)
                 {
-                  (*p_inputs)(isample,i) += dq(qi);
-                  computeHisto(*p_inputs);
-                  real cost_after = computeKLdiv();
-                  (*p_inputs)(isample,i) -= dq(qi);
-                  cout << "\tglobal cost comparison:" << cost_after - cost_before;
-                  cout << "  <?>  " << (*p_inputs_grad)(isample, i)*dq(qi) << endl;
+                    qi = inputs(isample, i);
+                    if (alpha > 0.0 )
+                        inputs_grad(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
+                                                        *(1.0-momentum)
+                                                        *(real)(input_size-1);
+                    for (int j = 0; j < i; j++)
+                    {
+                        real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
+                        qj = inputs(isample,j);
+                        inputs_grad(isample, i) += d_temp *qj*(1.0-momentum);
+                        inputs_grad(isample, j) += d_temp *qi*(1.0-momentum);
+                    }
                 }
+                for (int i = 0; i < input_size; i++)
+                    inputs_grad(isample, i) *= norm_factor;
             }
-
-
-        }
-
-        else if( cost_function == "kl_div_simple" )
-        {
-            computeSafeHisto(*p_inputs);
-
+        else
             for (int isample = 0; isample < n_samples; isample++)
             {
-
-                // Computing the difference of KL divergence
-                // for d_q
                 for (int i = 0; i < input_size; i++)
                 {
-                    (*p_inputs_grad)(isample, i) = 0.0;
-
-                    qi = (*p_inputs)(isample,i);
-                    int index_i = histo_index(qi);
-                    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-                        continue;
-                    real over_dqi=1.0/dq(qi);
-                    int shift_i;
-                    if( over_dqi > 0.0)
-                        shift_i = 1;
-                    else
-                        shift_i = -1;
-                    // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
-                    //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
-
+                    qi = inputs(isample, i);
+                    if (alpha > 0.0 )
+                        inputs_grad(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
+                                                        *(real)(input_size-1);
                     for (int j = 0; j < i; j++)
                     {
-                        (*p_inputs_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
-
-                        qj = (*p_inputs)(isample,j);
-                        int index_j = histo_index(qj);
-                        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-                            continue;
-                        real over_dqj=1.0/dq(qj);
-                         int shift_j;
-                        if( over_dqj > 0.0)
-                            shift_j = 1;
-                        else
-                            shift_j = -1;
-                            // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
-                          //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
-
-                        (*p_inputs_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
+                        real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
+                        qj = inputs(isample,j);
+                        inputs_grad(isample, i) += d_temp *qj;
+                        inputs_grad(isample, j) += d_temp *qi;
                     }
                 }
-
-                // Normalization
-                for (int i = 0; i < input_size; i++ )
-                {
-                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
-                }
+                for (int i = 0; i < input_size; i++)
+                    inputs_grad(isample, i) *= norm_factor;
             }
-        }
+    } // END cost_function == "pascal"
 
-        else if( cost_function == "pascal" )
+    else if( cost_function == "correlation")
+    {
+        computeCorrelationStatistics( inputs );
+
+        if( momentum > 0.0 )
+            PLERROR( "not implemented yet");
+        else
         {
-            computePascalStatistics(*p_inputs);
+            real average_deriv_tmp = 0.;
+            for (int isample = 0; isample < n_samples; isample++)
+            {
+                Vec dSTDi_dqi, dCROSSij_dqj;
+                dSTDi_dqi.resize( input_size );
+                dCROSSij_dqj.resize( input_size );
 
-            if( momentum > 0.0 )
-                for (int isample = 0; isample < n_samples; isample++)
+                for (int i = 0; i < input_size; i++)
                 {
-                    for (int i = 0; i < input_size; i++)
+                    if( fast_exact_is_equal( inputs_stds[i], 0. ) )
                     {
-                        qi = (*p_inputs)(isample, i);
-                        if (alpha > 0.0 )
-                            (*p_inputs_grad)(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
-			                                         *(1.0-momentum) *one_count
-                                                                 *(real)(input_size-1);
-                        for (int j = 0; j < i; j++)
+                        if( isample == 0 )
+                            PLWARNING("wired phenomenon: the %dth output have always expectation %f ( at stage=%d )",
+                                       i, inputs_expectation[i], stage);
+                        if( inputs_expectation[i] < 0.1 )
                         {
-                            real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
-                            qj = (*p_inputs)(isample,j);
-                            (*p_inputs_grad)(isample, i) += d_temp *qj*(1.0-momentum)*one_count;
-                            (*p_inputs_grad)(isample, j) += d_temp *qi*(1.0-momentum)*one_count;
+              	            // We force to switch on the neuron
+                            // (the cost increase much when the expectation is decreased \ 0)
+                            if( ( isample > 0 ) || ( n_samples == 1 ) )
+                                 inputs_grad(isample, i) -= average_deriv;
                         }
-                    }
-                    for (int i = 0; i < input_size; i++)
-                    {
-                        (*p_inputs_grad)(isample, i) *= norm_factor;
-                    }
-                }
-            else
-                for (int isample = 0; isample < n_samples; isample++)
-                {
-                    for (int i = 0; i < input_size; i++)
-                    {
-                        qi = (*p_inputs)(isample, i);
-                        if (alpha > 0.0 )
-                            (*p_inputs_grad)(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
-			                                         *one_count
-                                                                 *(real)(input_size-1);
-                        for (int j = 0; j < i; j++)
+                        else if( inputs_expectation[i] > 0.9 )
                         {
-                            real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
-                            qj = (*p_inputs)(isample,j);
-                            (*p_inputs_grad)(isample, i) += d_temp *qj *one_count;
-                            (*p_inputs_grad)(isample, j) += d_temp *qi *one_count;
+                            // We force to switch off the neuron
+                            // (the cost increase much when we the expectation is increased / 1)
+                            // except for the first sample
+                            if( ( isample > 0 ) || ( n_samples == 1 ) )
+                                inputs_grad(isample, i) += average_deriv;
                         }
+                        else
+                            if ( !(inputs_expectation[i]>-REAL_MAX) || !(inputs_expectation[i]<REAL_MAX)  )
+                               PLERROR("The %dth output have always value %f ( at stage=%d )",
+                                        i, inputs_expectation[i], stage);
+                        continue;
                     }
-                    for (int i = 0; i < input_size; i++)
-                    {
-                        (*p_inputs_grad)(isample, i) *= norm_factor;
-                    }
-                }
-        }
+                    //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
+                    //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                    //!
+                    //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
+                    //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
+                    //!               = 1 / [ 2.STD(Qi) ] * d[ E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
+                    //!               = 1 / [ 2.STD(Qi) ] * [ 2*qi(t) / n_samples - 2*E(Qi) / n_samples ]
+                    //!               = ( qi(t) - E(Qi) ) / ( n_samples * STD(Qi) )
+                    //!               = dCROSSij_dqj[i] / STD(Qi)
 
-        else if( cost_function == "correlation")
-        {
-            computeCorrelationStatistics(*p_inputs);
+                    qi = inputs(isample, i);
+                    dCROSSij_dqj[i] = ( qi - inputs_expectation[i] ); //*one_count;
+                    dSTDi_dqi[i] = dCROSSij_dqj[i] / inputs_stds[i];
 
-            if( momentum > 0.0 )
-                PLERROR( "not implemented yet");
-            else
-                for (int isample = 0; isample < n_samples; isample++)
-                {
-                    Vec dSTDi_dqi, dCROSSij_dqj;
-                    dSTDi_dqi.resize( input_size );
-                    dCROSSij_dqj.resize( input_size );
-
-                    for (int i = 0; i < input_size; i++)
+                    for (int j = 0; j < i; j++)
                     {
-                        qi = (*p_inputs)(isample, i);
+                        qj = inputs(isample,j);
 
-                        //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
-                        //!                  = ( qi(t) - E(Qi) ) / n_samples
-                        //!
-                        //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
-                        //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
-                        //!               = 1 / [ 2.STD(Qi) ] * d[ E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
-                        //!               = 1 / [ 2.STD(Qi) ] * [ 2*qi(t) / n_samples - 2*E(Qi) / n_samples ]
-                        //!               = ( qi(t) - E(Qi) ) / ( n_samples * STD(Qi) )
-                        //!               = dCROSSij_dqj[i] / STD(Qi)
-                        //!
-                        dCROSSij_dqj[i] = ( qi - inputs_expectation[i] )*one_count;
-                        dSTDi_dqi[i] = dCROSSij_dqj[i] / inputs_stds[i];
+                        real correlation_denum = inputs_stds[i]*inputs_stds[j];
+                        //if( fast_exact_is_equal( inputs_stds[j], 0 ) (but because of numerical imprecision...)
+                        if( fast_exact_is_equal( correlation_denum * correlation_denum, 0. ) )
+                            continue;
+                        real dfunc_dCorr = deriv_func_correlation( inputs_correlations(i,j) );
+                        real correlation_num = ( inputs_cross_quadratic_mean(i,j)
+                                                 - inputs_expectation[i]*inputs_expectation[j] );
+                        inputs_grad(isample, i) += dfunc_dCorr * ( 
+                                                     correlation_denum * dCROSSij_dqj[j]
+                                                   - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
+                                                     ) / (correlation_denum * correlation_denum);
 
-                        for (int j = 0; j < i; j++)
-                        {
-                            qj = (*p_inputs)(isample,j);
-
-                            real correlation_denum = inputs_stds[i]*inputs_stds[j];
-                            real dfunc_dCorr = deriv_func_correlation( inputs_correlations(i,j) );
-                            real correlation_num = ( inputs_cross_quadratic_mean(i,j)
-                                                     - inputs_expectation[i]*inputs_expectation[j] );
-
-                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * (
-                                                                    correlation_denum * dCROSSij_dqj[j]
-                                                                  - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
-                                                                    ) / (correlation_denum * correlation_denum);
-
-                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * (
-                                                                    correlation_denum * dCROSSij_dqj[i]
-                                                                  - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
-                                                                    ) / (correlation_denum * correlation_denum);
-                        }
+                        inputs_grad(isample, j) += dfunc_dCorr * ( 
+                                                     correlation_denum * dCROSSij_dqj[i]
+                                                   - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
+                                                     ) / (correlation_denum * correlation_denum);
                     }
-                    for (int i = 0; i < input_size; i++)
-                        (*p_inputs_grad)(isample, i) *= norm_factor;
                 }
+                for (int i = 0; i < input_size; i++)
+                {
+                    average_deriv_tmp += fabs( inputs_grad(isample, i) );
+                    inputs_grad(isample, i) *= norm_factor;
+                }
+            }
+            average_deriv = average_deriv_tmp / (real)( input_size * n_samples );
+            PLASSERT( average_deriv >= 0.);
         }
-        else
-            PLERROR("LayerCostModule::bpropAccUpdate() not implemented for cost function %s",
-                     cost_function.c_str());
+    } // END cost_function == "correlation"
 
-        checkProp(ports_gradient);
-    }
-    else if( !p_inputs_grad && !p_cost_grad )
-        return;
     else
-        PLERROR("In LayerCostModule::bpropAccUpdate - Port configuration not implemented ");
-
+        PLERROR("LayerCostModule::bpropAccUpdate() not implemented for cost function %s",
+                 cost_function.c_str());
 }
 
 
@@ -850,9 +897,21 @@
 ////////////////////////////////////////////////////
 void LayerCostModule::computePascalStatistics(const Mat& inputs)
 {
+     computePascalStatistics( inputs,
+                              inputs_expectation, inputs_cross_quadratic_mean);
+}
+
+void LayerCostModule::computePascalStatistics(const Mat& inputs,
+                                              Vec& expectation, Mat& cross_quadratic_mean) const
+{
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
+    
+    expectation.resize( input_size );
+    expectation.clear(); 
+    cross_quadratic_mean.resize(input_size,input_size);
+    cross_quadratic_mean.clear(); 
 
     inputs_expectation.clear();
     inputs_cross_quadratic_mean.clear();
@@ -862,46 +921,44 @@
         input = inputs(isample);
         for (int i = 0; i < input_size; i++)
         {
-            inputs_expectation[i] += input[i];
+            expectation[i] += input[i];
             for (int j = 0; j < i; j++)
-                 inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
+                 cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
 
     for (int i = 0; i < input_size; i++)
     {
-        inputs_expectation[i] *= one_count;
+        expectation[i] *= one_count;
         for (int j = 0; j < i; j++)
-        {
-             inputs_cross_quadratic_mean(i,j) *= one_count;
-        }
+             cross_quadratic_mean(i,j) *= one_count;
     }
     if( ( momentum > 0.0 ) && during_training )
     {
         for (int i = 0; i < input_size; i++)
         {
-            inputs_expectation[i] = momentum*inputs_expectation_trainMemory[i]
-                                         +(1.0-momentum)*inputs_expectation[i];
-            inputs_expectation_trainMemory[i] = inputs_expectation[i];
+            expectation[i] = momentum*inputs_expectation_trainMemory[i]
+                                         +(1.0-momentum)*expectation[i];
+            inputs_expectation_trainMemory[i] = expectation[i];
             for (int j = 0; j < i; j++)
             {
-                 inputs_cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
-                                                       +(1.0-momentum)*inputs_cross_quadratic_mean(i,j);
-                 inputs_cross_quadratic_mean_trainMemory(i,j) = inputs_cross_quadratic_mean(i,j);
+                 cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
+                                                       +(1.0-momentum)*cross_quadratic_mean(i,j);
+                 inputs_cross_quadratic_mean_trainMemory(i,j) = cross_quadratic_mean(i,j);
             }
         }
     }
 }
-string LayerCostModule::func_pascal_prefix()
+string LayerCostModule::func_pascal_prefix() const
 {
     string prefix = "exp";
     return prefix;
 }
-real LayerCostModule::func_pascal(real value)
+real LayerCostModule::func_pascal(real value) const
 {
     return exp(value);
 }
-real LayerCostModule::deriv_func_pascal(real value)
+real LayerCostModule::deriv_func_pascal(real value) const
 {
     return exp(value);
 }
@@ -909,48 +966,66 @@
 
 void LayerCostModule::computeCorrelationStatistics(const Mat& inputs)
 {
+    computeCorrelationStatistics(inputs,
+                                 inputs_expectation, inputs_cross_quadratic_mean,
+                                 inputs_stds, inputs_correlations);
+}
+
+void LayerCostModule::computeCorrelationStatistics(const Mat& inputs,
+                                                   Vec& expectation, Mat& cross_quadratic_mean,
+                                                   Vec& stds, Mat& correlations) const
+{
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
     Vec input;
 
-    inputs_expectation.clear();
-    inputs_cross_quadratic_mean.clear();
-    inputs_correlations.clear();
+    expectation.resize( input_size );
+    expectation.clear(); 
+    cross_quadratic_mean.resize(input_size,input_size);
+    cross_quadratic_mean.clear(); 
+    stds.resize( input_size );
+    stds.clear();
+    correlations.resize(input_size,input_size);
+    correlations.fill(1.); // The default correlation is 1
 
     for (int isample = 0; isample < n_samples; isample++)
     {
         input = inputs(isample);
         for (int i = 0; i < input_size; i++)
         {
-            inputs_expectation[i] += input[i];
-            inputs_cross_quadratic_mean(i,i) += input[i] * input[i];
+            expectation[i] += input[i];
+            cross_quadratic_mean(i,i) += input[i] * input[i];
             for (int j = 0; j < i; j++)
-                 inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
+                 cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
 
     for (int i = 0; i < input_size; i++)
     {
         //! Normalization
-        inputs_expectation[i] *= one_count;
-        inputs_cross_quadratic_mean(i,i) *= one_count;
+        expectation[i] *= one_count;
+        cross_quadratic_mean(i,i) *= one_count;
 
-            inputs_stds[i] = sqrt( inputs_cross_quadratic_mean(i,i)
-                              - inputs_expectation[i] * inputs_expectation[i] );
+	//! Required temporary variable because of numerical imprecision !//
+	real tmp = cross_quadratic_mean(i,i) - expectation[i] * expectation[i];
+	if( tmp > 0. )
+	    stds[i] = sqrt( tmp );
 
         for (int j = 0; j < i; j++)
         {
             //! Normalization
-            inputs_cross_quadratic_mean(i,j) *= one_count;
+            cross_quadratic_mean(i,j) *= one_count;
 
             //! Correlations
-            inputs_correlations(i,j) = (
-                                  inputs_cross_quadratic_mean(i,j)
-                                  - inputs_expectation[i]*inputs_expectation[j]
-                                  ) / ( inputs_stds[i] * inputs_stds[j] );
+	    tmp = stds[i] * stds[j];
+            if( tmp > 0. )
+	        correlations(i,j) = (
+                                  cross_quadratic_mean(i,j)
+                                  - expectation[i]*expectation[j]
+                                  ) / tmp;
         }
     }
-    //! Be careful: 'inputs_correlations' matrix is only computed
+    //! Be careful: 'correlations' matrix is only computed
     //!  on the triangle subpart 'i' > 'j'
     //!  ('i'/'j': first/second argument)
 
@@ -960,47 +1035,182 @@
             PLERROR("not implemented yet");
     }
 }
-string LayerCostModule::func_correlation_prefix()
+string LayerCostModule::func_correlation_prefix() const
 {
-    string prefix = "squared";
-    return "square";
+    string prefix = "exp";
+    return prefix;
 }
-real LayerCostModule::func_correlation(real correlation)
+real LayerCostModule::func_correlation(real correlation) const
 {
-    return correlation * correlation;
+    return exp(correlation);
 }
-real LayerCostModule::deriv_func_correlation(real correlation)
+real LayerCostModule::deriv_func_correlation(real correlation) const
 {
-    return 2 * correlation;
+    return exp(correlation);
 }
 /////////////////////////
 // Auxiliary Functions //
 /////////////////////////
+real LayerCostModule::computeKLdiv(const Mat& histo) const
+{
+    PLASSERT( histo.length() == input_size );
+    PLASSERT( histo.width() == histo_size );
+    real cost = 0.;
+    for (int i = 0; i < input_size; i++)
+        for (int j = 0; j < i; j++)
+        {
+            // These variables are used in case one bin of 
+            // the histogram is empty for one unit
+            // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+            // In such case, we ''differ'' the count for the next bin and so on.
+            real differ_count_i = 0.;
+            real differ_count_j = 0.;
+            int n_differ = 0;
+//                    real last_positive_Ni_k, last_positive_Nj_k;
+//                    int last_n_differ;
+            for (int k = 0; k < histo_size; k++)
+            {
+                real Ni_k = histo( i, k ) + differ_count_i;
+                real Nj_k = histo( j, k ) + differ_count_j;
+                if( fast_exact_is_equal(Ni_k, 0.0) )
+                {
+                    differ_count_j = Nj_k;
+                    n_differ += 1;
+                }
+                else if( fast_exact_is_equal(Nj_k, 0.0) )
+                {
+                    differ_count_i = Ni_k;
+                    n_differ += 1;
+                }
+                else
+                {
+                    cost += KLdivTerm( Ni_k, Nj_k ) *(real)(1+n_differ) *HISTO_STEP;
+                    differ_count_i = 0.0;
+                    differ_count_j = 0.0;
+                    n_differ = 0;
+//                            last_positive_Ni_k = Ni_k;
+//                            last_positive_Nj_k = Nj_k;
+//                            last_n_differ = n_differ;
+                }
+            }
+//                    if( differ_count_i > 0.0 )
+//                    {   
+//                        "cas ou on regroupe avec le dernier";   
+//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
+//                                  *(real)(1+last_n_differ) *HISTO_STEP;
+//                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+//                    }
+//                     
+//                    else if ( differ_count_j > 0.0 )
+//                    {
+//                        "cas ou on regroupe avec le dernier";
+//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
+//                                 *(real)(1+last_n_differ) *HISTO_STEP;
+//                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
+//                    }    
+        }
+    // Normalization w.r.t. number of units
+    return cost *norm_factor;
+}
 
+real LayerCostModule::computeKLdiv(bool store_in_cache)
+{
+    if( store_in_cache )
+    {
+            real cost = 0.;
+            for (int i = 0; i < input_size; i++)
+                for (int j = 0; j < i; j++)
+                {
+                    // These variables are used in case one bin of 
+                    // the histogram is empty for one unit
+                    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+                    // In such case, we ''differ'' the count for the next bin and so on.
+		    cache_differ_count_i[ i ][ j ].clear();
+		    cache_differ_count_j[ i ][ j ].clear();
+                    cache_n_differ[i][j].fill( 0. );
+//                    real last_positive_Ni_k, last_positive_Nj_k;
+//                    real last_n_differ;
+                    for (int k = 0; k < histo_size; k++)
+                    {
+                        real Ni_k = inputs_histo(i,k) + cache_differ_count_i[i][j][ k ];
+                        real Nj_k = inputs_histo(j,k) + cache_differ_count_j[i][j][ k ];
 
+                        if( fast_exact_is_equal(Ni_k, 0.0) )
+                        {
+			    if( k < histo_size - 1 ) // "cas ou on regroupe avec le dernier";
+			    {
+			        cache_differ_count_j[i][j][ k+1 ] = Nj_k;
+                                cache_n_differ[i][j][ k+1 ] = cache_n_differ[i][j][ k ] + 1;
+                            }
+			}
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+                        {
+			    if( k < histo_size - 1 ) // "cas ou on regroupe avec le dernier";
+			    {
+			        cache_differ_count_i[i][j][ k+1 ] = Ni_k;
+                                cache_n_differ[i][j][ k+1 ] = cache_n_differ[i][j][ k ] + 1;
+                            }
+                        }
+                        else
+                        {
+                            cost += KLdivTerm( Ni_k, Nj_k ) *(real)(1 + cache_n_differ[i][j][ k ]) *HISTO_STEP;
+//                            last_positive_Ni_k = Ni_k;
+//                            last_positive_Nj_k = Nj_k;
+//                            last_n_differ = cache_n_differ[i][j][ k ];
+                        }
+//                    if( cache_differ_count_i[i][j][ histo_size - 1 ] > 0.0 )
+//                        "cas ou on regroupe avec le dernier";
+//                    else if ( cache_differ_count_j[i][j][ histo_size - 1 ] > 0.0 )
+//                        "cas ou on regroupe avec le dernier";
+                    }
+		}
+            // Normalization w.r.t. number of units
+            return cost *norm_factor;
+    }
+    else
+        return computeKLdiv(inputs_histo);
+}
+
+
 real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq)
 {
-    PLASSERT( over_dq > 0.0 );
+    PLASSERT( index_i < histo_size - 1 );
+    // already tested in the code of BackPropAccUpdate()
+    PLASSERT( over_dq > 0. );
+    PLASSERT( inputs_histo( i, index_i ) > 0. );
+    // Verifies that:
+    // ( inputs_histo is up to date
+    //   => ) the input(isample,i) has been counted
 
     real grad_update = 0.0;
+    
+    real Ni_ki, Nj_ki, Ni_ki_shift1, Nj_ki_shift1;
+    real n_differ_before_ki, n_differ_before_ki_shift1;
 
-    real Ni_ki = inputs_histo(i,index_i);
-    real Ni_ki_shift1 = inputs_histo(i,index_i+1);
-    real Nj_ki        = inputs_histo(j,index_i);
-    real Nj_ki_shift1 = inputs_histo(j,index_i+1);
+    if( i > j ) // Because cache memory matrix are symmetric but not completely filled
+    {
+        Ni_ki        = inputs_histo( i, index_i     ) + cache_differ_count_i[ i ][ j ][ index_i ];
+        Nj_ki        = inputs_histo( j, index_i     ) + cache_differ_count_j[ i ][ j ][ index_i ];
+        Ni_ki_shift1 = inputs_histo( i, index_i + 1 ) + cache_differ_count_i[ i ][ j ][ index_i + 1 ];
+        Nj_ki_shift1 = inputs_histo( j, index_i + 1 ) + cache_differ_count_j[ i ][ j ][ index_i + 1 ];
+        n_differ_before_ki = cache_n_differ[ i ][ j ][ index_i ];
+        n_differ_before_ki_shift1 = cache_n_differ[ i ][ j ][ index_i + 1 ];
+    }
+    else // ( i < j ) // Be very careful with indices here!
+    {
+        Ni_ki        = inputs_histo( i, index_i     ) + cache_differ_count_j[ j ][ i ][ index_i ];
+        Nj_ki        = inputs_histo( j, index_i     ) + cache_differ_count_i[ j ][ i ][ index_i ];
+        Ni_ki_shift1 = inputs_histo( i, index_i + 1 ) + cache_differ_count_j[ j ][ i ][ index_i + 1 ];
+        Nj_ki_shift1 = inputs_histo( j, index_i + 1 ) + cache_differ_count_i[ j ][ i ][ index_i + 1 ];
+        n_differ_before_ki = cache_n_differ[ j ][ i ][ index_i ];
+        n_differ_before_ki_shift1 = cache_n_differ[ j ][ i ][ index_i + 1 ];
+    }
+    real additional_differ_count_j_after = 0.;
+    real n_differ_after_ki = n_differ_before_ki;
+    real n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
 
-    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if inputs_histo is up to date,
-                                                  // the input(isample,i) has been counted
-    real differ_count_j_before = 0.0;
-    real differ_count_j_after = 0.0;
-    real differ_count_i_before = 0.0;
-    real differ_count_i_after = 0.0;
-    int n_differ_j_before = 0;
-    int n_differ_j_after = 0;
-    int n_differ_i_before = 0;
-    int n_differ_i_after = 0;
-
     // What follows is only valuable when the qi's are increased (dq>0).
 
     if( !fast_exact_is_equal(Nj_ki, 0.0) )
@@ -1008,99 +1218,147 @@
     // (it was already counted in the next histograms's bin
     {
         // removing the term of the sum that will be modified
-        grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
+        grad_update -= KLdivTerm( Ni_ki,
+	                          Nj_ki )
+	               * ( 1 + n_differ_before_ki);
 
         if( fast_exact_is_equal(Ni_ki, one_count) )
         {
-            differ_count_j_after = Nj_ki;
-            n_differ_j_after += 1;
+            additional_differ_count_j_after = Nj_ki;
+	    n_differ_after_ki_shift1 = n_differ_after_ki + 1;
+	                          // = n_differ_before_ki + 1;
         }
         else
+	{
             // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki )
-                           *over_dq;
+            grad_update += KLdivTerm( Ni_ki - one_count,
+	                              Nj_ki )
+	                   * ( 1 + n_differ_after_ki );
+	}
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after )
-                          *(real)(1+n_differ_j_after)*over_dq ;
+            grad_update += KLdivTerm( Ni_ki_shift1 + one_count,
+	                                  Nj_ki_shift1 + additional_differ_count_j_after )
+	                       * ( 1 + n_differ_after_ki_shift1 );
 
             if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas ou on regroupe avec le dernier";
             {
                 // removing the term of the sum that will be modified
-                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )
-                               *over_dq;
+                grad_update -= KLdivTerm( Ni_ki_shift1,
+		                          Nj_ki_shift1 )
+		               * ( 1 + n_differ_before_ki_shift1 );                
             }
-            else
+            else // ( Ni_ki_shift1 == 0.0 )
             {
                 // We search   ki' > k(i)+1   such that   n(i,ki') > 0
-                differ_count_j_before = Nj_ki_shift1;
-                n_differ_j_before += 1;
+                real additional_differ_count_j_before = 0.;
+		real additional_n_differ_before_ki_shift1 = 0.;
                 int ki;
                 for (ki = index_i+2; ki < histo_size; ki++)
                 {
-                    differ_count_j_before += inputs_histo( j, ki );
+                    additional_differ_count_j_before += inputs_histo( j, ki );
+                    additional_n_differ_before_ki_shift1 += 1;
                     if( inputs_histo( i, ki )>0 )
                         break;
-                    n_differ_j_before += 1;
                 }
                 if( ki < histo_size )
                 {
-                    grad_update -= KLdivTerm( inputs_histo( i, ki ), differ_count_j_before )
-                                   *(real)(1+n_differ_j_before)*over_dq;
+                    grad_update -= KLdivTerm( inputs_histo( i, ki ),
+		                              Nj_ki_shift1 + additional_differ_count_j_before )
+		                   * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
 
-                    if( differ_count_j_before > Nj_ki_shift1 )
-                        grad_update += KLdivTerm( inputs_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )
-                                       *(real)(n_differ_j_before)*over_dq;
-                        // pb avec differ_count_j_after plus haut??? semble pas
+                    if( additional_differ_count_j_before > 0. )
+		    // We have to report the additional count for unit j
+                    {
+                        grad_update += KLdivTerm( inputs_histo( i, ki ),
+			                          additional_differ_count_j_before )
+			               * ( additional_n_differ_before_ki_shift1 );
+                    }
                 }
-                else
-                {
-                    // cas ou on regroupe avec le dernier (easy)
-                }
             }
         }
-        else
+        else // ( Nj_ki_shift1 == 0.0 )
         {
-            differ_count_i_before = Ni_ki_shift1;
-            if( differ_count_i_before>0.0 )
-               n_differ_i_before += 1;
-            differ_count_i_after  = Ni_ki_shift1+one_count;
-            n_differ_i_after += 1;
+            real additional_differ_count_i_before = 0.;
+	    // We search kj > ki+1 tq inputs_histo( j, kj ) > 0.
             int kj;
             for( kj = index_i+2; kj < histo_size; kj++)
             {
-                differ_count_i_after += inputs_histo( i, kj );
-                if( differ_count_i_before > 0 )
-                    differ_count_i_before += inputs_histo( i, kj );
-                if( inputs_histo( j, kj ) > 0 )
+                additional_differ_count_i_before += inputs_histo( i, kj );
+                n_differ_before_ki_shift1 += 1;
+                if( inputs_histo( j, kj ) > 0. )
                     break;
-                n_differ_i_after += 1;
-                if( differ_count_i_before > 0 )
-                    n_differ_i_before += 1;
             }
+	    if ( !fast_exact_is_equal(additional_differ_count_j_after, 0. ) )
+	        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
             if( kj < histo_size )
             {
-                grad_update += KLdivTerm( differ_count_i_after, inputs_histo( j, kj ) )
-                               *(real)(1+n_differ_i_after)*over_dq;
+                if ( fast_exact_is_equal(n_differ_after_ki_shift1, n_differ_before_ki_shift1) )
+		{
+		    // ( no qj were differed after we changed count at bin ki )
+		    // OR ( some qj were differed to bin ki+1 AND the bin were not empty )
+                    grad_update += KLdivTerm( Ni_ki_shift1 + additional_differ_count_i_before + one_count,
+		                             inputs_histo( j, kj ) + additional_differ_count_j_after )
+		                   * ( 1 + n_differ_after_ki_shift1 );
+                }	   		
+		else
+		{
+		    PLASSERT( n_differ_before_ki_shift1 > n_differ_after_ki_shift1 );
+                    grad_update += KLdivTerm( Ni_ki_shift1 + one_count,
+		                              additional_differ_count_j_after )
+		                   * ( 1 + n_differ_after_ki_shift1 );
+                    grad_update += KLdivTerm( additional_differ_count_i_before,
+		                              inputs_histo( j, kj ) )
+		                   * ( n_differ_before_ki_shift1 - n_differ_after_ki_shift1 );
+                }
 
-                if( differ_count_i_before > 0 )
-                    grad_update -= KLdivTerm( differ_count_i_before, inputs_histo( j, kj ) )
-                                   *(real)(1+n_differ_i_before)*over_dq;
+                if( !fast_exact_is_equal(Ni_ki_shift1 + additional_differ_count_i_before,0.0) )
+		{
+                    grad_update -= KLdivTerm( Ni_ki_shift1 + additional_differ_count_i_before,
+		                              inputs_histo( j, kj ) )
+		                   * ( 1 + n_differ_before_ki_shift1 );
+	        }
+		else // ( Ni_ki_shift1' == 0 == Nj_ki_shift1 ) && ( pas de q[i] avant q[j']... )
+		{
+		    // We search ki' > kj+1 tq inputs_histo( i, ki' ) > 0.
+                    real additional_differ_count_j_before = 0.;
+		    real additional_n_differ_before_ki_shift1 = 0.;
+		    int kj2;
+                    for( kj2 = kj+1; kj2 < histo_size; kj2++)
+                    {
+			additional_differ_count_j_before += inputs_histo( j, kj2 );
+                        additional_n_differ_before_ki_shift1 += 1;
+                        if( inputs_histo( i, kj2 ) > 0. )
+                            break;
+		    }
+		    if ( fast_exact_is_equal(additional_differ_count_j_before, 0. ) )
+		        n_differ_after_ki_shift1 = n_differ_before_ki_shift1;
+                    if( kj2 < histo_size )
+		    {
+		        grad_update -= KLdivTerm( inputs_histo( i, kj2 ),
+			                          Nj_ki_shift1 + additional_differ_count_j_before )
+		                       * ( 1 + n_differ_before_ki_shift1 + additional_n_differ_before_ki_shift1 );
+
+                        if( additional_differ_count_j_before > 0. )
+			{
+                            grad_update += KLdivTerm( inputs_histo( i, kj2 ),
+			                              additional_differ_count_j_before )
+		                           * ( additional_n_differ_before_ki_shift1 );
+                        }
+                    }
+	        }
             }
-            else
-            {
-                // cas ou on regroupe avec le dernier
-            }
         }
     }
-    return grad_update*over_dq;
+    return grad_update *HISTO_STEP *over_dq *norm_factor;
 }
 
 real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq)
 {
     //PLASSERT( over_dq > 0.0 )
+    PLASSERT( index_i < histo_size - 1 );
 
     real grad_update = 0.0;
 
@@ -1128,104 +1386,55 @@
 }
 
 
-real LayerCostModule::KLdivTerm(real pi, real pj)
+real LayerCostModule::KLdivTerm(real pi, real pj) const
 {
     return ( pj - pi ) * safeflog( pi/pj );
 }
 
-real LayerCostModule::computeKLdiv()
-{
-            real cost = 0;
-            for (int i = 0; i < input_size; i++)
-                for (int j = 0; j < i; j++)
-                {
-                    // These variables are used in case one bin of
-                    // the histogram is empty for one unit
-                    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
-                    // In such case, we ''differ'' the count for the next bin and so on.
-                    real differ_count_i = 0.0;
-                    real differ_count_j = 0.0;
-                    int n_differ = 0;
-                    real last_positive_Ni_k, last_positive_Nj_k;
-                    int last_n_differ;
-                    for (int k = 0; k < histo_size; k++)
-                    {
-                        real Ni_k = inputs_histo(i,k) + differ_count_i;
-                        real Nj_k = inputs_histo(j,k) + differ_count_j;
-                        if( fast_exact_is_equal(Ni_k, 0.0) )
-                        {
-                         // differ_count_j += inputs_histo(j,k);
-                            differ_count_j = Nj_k;
-                            n_differ += 1;
-                        }
-                        else if( fast_exact_is_equal(Nj_k, 0.0) )
-                        {
-                            differ_count_i = Ni_k;
-                            n_differ += 1;
-                        }
-                        else
-                        {
-                            cost += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ) *HISTO_STEP;
-                            differ_count_i = 0.0;
-                            differ_count_j = 0.0;
-                            n_differ = 0;
-                            last_positive_Ni_k = Ni_k;
-                            last_positive_Nj_k = Nj_k;
-                            last_n_differ = n_differ;
-                        }
-                    }
-                    if( differ_count_i > 0.0 )
-                    {
-                        // cas ou on regroupe avec le dernier
-//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
-//                                  *(real)(1+last_n_differ) *HISTO_STEP;
-//                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
-//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
-                    }
 
-                    else if ( differ_count_j > 0.0 )
-                    {
-                        // cas ou on regroupe avec le dernier
-//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
-//                                 *(real)(1+last_n_differ) *HISTO_STEP;
-//                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
-//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
-                    }
-                }
-            // Normalization w.r.t. number of units
-            return cost *norm_factor;
-}
-
 void LayerCostModule::computeHisto(const Mat& inputs)
 {
+    computeHisto(inputs,
+                 inputs_histo);
+}
+void LayerCostModule::computeHisto(const Mat& inputs,
+                                   Mat& histo) const
+{
     int n_samples = inputs.length();
     one_count = 1. / (real)n_samples;
-    Vec input;
-
-    inputs_histo.clear();
+    
+    histo.resize(input_size,histo_size);
+    histo.clear(); 
     for (int isample = 0; isample < n_samples; isample++)
     {
-        input = inputs(isample);
+        Vec input = inputs(isample);
         for (int i = 0; i < input_size; i++)
-            inputs_histo(i, histo_index(input[i]) ) += one_count;
+	{
+	    PLASSERT( histo_index(input[i]) < histo_size);
+            histo( i, histo_index(input[i]) ) += one_count;
+        }
     }
 }
 
 
-
 void LayerCostModule::computeSafeHisto(const Mat& inputs)
 {
+    computeSafeHisto(inputs,
+                     inputs_histo);
+}
+void LayerCostModule::computeSafeHisto(const Mat& inputs,
+                                       Mat& histo) const
+{
     int n_samples = inputs.length();
     one_count = 1. / (real)(n_samples+histo_size);
-    Vec input;
 
-    inputs_histo.fill(one_count);
-
+    histo.resize(input_size,histo_size);
+    histo.fill(one_count);
     for (int isample = 0; isample < n_samples; isample++)
     {
-        input = inputs(isample);
+        Vec input = inputs(isample);
         for (int i = 0; i < input_size; i++)
-            inputs_histo(i, histo_index(input[i])) += one_count;
+            histo(i, histo_index(input[i])) += one_count;
     }
 }
 
@@ -1233,13 +1442,15 @@
 // Return the index of the (1D) histogram
 // corresponding to the real input value q in [0,1]
 //
-int LayerCostModule::histo_index(real q)
+int LayerCostModule::histo_index(real q) const
 {
-    PLASSERT( (q >= 0.) && (q < 1.) );
+    PLASSERT( (q >= 0.) && (q <= 1.) );
 
-    if( fast_exact_is_equal( q, 1. ) )
+    if( q >= 1. )
        return histo_size - 1;
 
+    PLASSERT( (int)floor(q*(real)histo_size) < histo_size );
+
 // LINEAR SCALE
     return (int)floor(q*(real)histo_size);
 }
@@ -1251,7 +1462,7 @@
 // Note: we do not care about cases where histo_index(q)=histo_size
 //      (this is done in the bpropAccUpdate code)
 //
-real LayerCostModule::dq(real q)
+real LayerCostModule::dq(real q) const
 {
     // ** Simple version **
     return HISTO_STEP;
@@ -1266,28 +1477,14 @@
     // return (real)histo_index(q+1.0/(real)histo_size)/(real)histo_size - q;
 }
 
-
-////////////
-// forget //
-////////////
-void LayerCostModule::forget()
+//////////
+// name //
+//////////
+TVec<string> LayerCostModule::name()
 {
-    inputs_histo.clear();
-
-    inputs_expectation.clear();
-    inputs_stds.clear();
-
-    inputs_correlations.clear();
-    inputs_cross_quadratic_mean.clear();
-    if( momentum > 0.0)
-    {
-        inputs_expectation_trainMemory.clear();
-        inputs_cross_quadratic_mean_trainMemory.clear();
-    }
-    one_count = 0.;
+    return TVec<string>(1, OnlineLearningModule::name);
 }
 
-
 /////////////////
 // addPortName //
 /////////////////

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-10-19 19:27:52 UTC (rev 8199)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-10-22 18:10:48 UTC (rev 8200)
@@ -39,8 +39,7 @@
 #ifndef LayerCostModule_INC
 #define LayerCostModule_INC
 
-#include <plearn_learners/online/OnlineLearningModule.h>
-#include <plearn/vmat/VMat.h>
+#include <plearn_learners/online/CostModule.h>
 
 #include <map>
 
@@ -49,21 +48,29 @@
 /**
  * Computes a cost function for a (hidden) representation. Backpropagates it.
  */
-class LayerCostModule : public OnlineLearningModule
+class LayerCostModule : public CostModule
 {
-    typedef OnlineLearningModule inherited;
+    typedef CostModule inherited;
 
 public:
     //#####  Public Build Options  ############################################
 
+    //! Generic name of the cost function
     string cost_function;
 
-    int histo_size;
+    //! Maximum number of stages we want to propagate the gradient    
+    int nstages_max;
 
+    //! Parameter in pascal's cost function
     real alpha;
-
+    
+    //! Parameter to compute moving means in non stochastic cost functions
     real momentum;
 
+    //! For non stochastic KL divergence cost function
+    int histo_size;
+
+
     //#####  Public Learnt Options  ###########################################
 
     //! Histograms of inputs (estimated empiricially on some data)
@@ -79,6 +86,12 @@
     Mat inputs_correlations; //! only for 'correlation' cost function
     Mat inputs_cross_quadratic_mean;
 
+    //! Variables for (non stochastic) Pascal's/correlation function with momentum
+    //! -------------------------------------------------------------
+    //! Statistics on outputs (estimated empiricially on the data)    
+    Vec inputs_expectation_trainMemory;
+    Mat inputs_cross_quadratic_mean_trainMemory;
+
     //! The generic name of the cost function
     string cost_function_completename;
 
@@ -90,39 +103,48 @@
 
     //! given the input and target, compute the cost
     virtual void fprop(const Vec& input, real& cost) const;
-    virtual void fprop(const Mat& inputs, Mat& costs);
-    //! Overridden.
+    virtual void fprop(const Mat& inputs, Mat& costs) const;
+    virtual void fprop(const Mat& inputs, const Mat& targets, Mat& costs) const;
     virtual void fprop(const TVec<Mat*>& ports_value);
 
     //! backpropagate the derivative w.r.t. activation
+    virtual void bpropUpdate(const Mat& inputs, const Mat& targets,
+                             const Vec& costs, Mat& input_gradients, bool accumulate=false);
+    virtual void bpropUpdate(const Mat& inputs, Mat& inputs_grad);
     virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
                                 const TVec<Mat*>& ports_gradient);
 
     //! Some auxiliary function to deal with empirical histograms
     virtual void computeHisto(const Mat& inputs);
+    virtual void computeHisto(const Mat& inputs, Mat& histo) const;
+    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq);
+    virtual real KLdivTerm(real pi, real pj) const;
+    virtual real computeKLdiv(bool store_in_cache);
+    virtual real computeKLdiv(const Mat& histo) const;
+    virtual int histo_index(real q) const;
+    virtual real dq(real q) const;
+    //! Auxiliary functions for kl_div_simple cost function
     virtual void computeSafeHisto(const Mat& inputs);
-    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq);
+    virtual void computeSafeHisto(const Mat& inputs, Mat& histo) const;
     virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq);
-    virtual real KLdivTerm(real pi, real pj);
-    virtual real computeKLdiv();
-    virtual int histo_index(real q);
-    virtual real dq(real q);
 
     //! Auxiliary function for the pascal's cost function
     virtual void computePascalStatistics(const Mat& inputs);
-    virtual string func_pascal_prefix();
-    virtual real   func_pascal(real correlation);
-    virtual real   deriv_func_pascal(real correlation);
+    virtual void computePascalStatistics(const Mat& inputs,
+                                         Vec& expectation, Mat& cross_quadratic_mean) const;
+    virtual string func_pascal_prefix() const;
+    virtual real   func_pascal(real correlation) const;
+    virtual real   deriv_func_pascal(real correlation) const;
 
     //! Auxiliary function for the correlation's cost function
     virtual void computeCorrelationStatistics(const Mat& inputs);
-    virtual string func_correlation_prefix();
-    virtual real   func_correlation(real correlation);
-    virtual real   deriv_func_correlation(real correlation);
+    virtual void computeCorrelationStatistics(const Mat& inputs,
+                                              Vec& expectation, Mat& cross_quadratic_mean,
+                                              Vec& stds, Mat& correlations) const;
+    virtual string func_correlation_prefix() const;
+    virtual real   func_correlation(real correlation) const;
+    virtual real   deriv_func_correlation(real correlation) const;
 
-    //! Overridden to do nothing (in particular, no warning).
-    virtual void setLearningRate(real dynamic_learning_rate) {}
-
     //! Returns all ports in a RBMModule.
     virtual const TVec<string>& getPorts();
 
@@ -134,6 +156,12 @@
     //! If 'port' does not exist, -1 is returned.
     virtual int getPortIndex(const string& port);
 
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Indicates the name of the computed costs
+    virtual TVec<string> name();
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -151,25 +179,29 @@
 
 protected:
 
-    //! Does stochastic gradient makes sense with our cost function?
+    //! Number of stage the BPropAccUpdate function was called
+    int stage;
+
+    //! Does stochastic gradient (without memory of the past)
+    //! makes sense with our cost function?
     bool is_cost_function_stochastic;
 
     //! Normalizing factor applied to the cost function
     //! to take into acount the number of weights
     real norm_factor;
 
+    real average_deriv;
+
     //! Variables for (non stochastic) KL Div cost function
     //! ---------------------------------------------------
     //! Range of a histogram's bin ( HISTO_STEP = 1/histo_size )
     real HISTO_STEP;
     //! the weight of a sample within a batch (usually, 1/n_samples)
-    real one_count;
 
-    //! Variables for (non stochastic) Pascal's/correlation function
-    //! -------------------------------------------------------------
-    //! Statistics on outputs (estimated empiricially on the data)
-    Vec inputs_expectation_trainMemory;
-    Mat inputs_cross_quadratic_mean_trainMemory;
+    mutable real one_count; 
+    TVec< TVec< Vec > > cache_differ_count_i;
+    TVec< TVec< Vec > > cache_differ_count_j;
+    TVec< TVec< Vec > > cache_n_differ;
 
     //! Map from a port name to its index in the 'ports' vector.
     map<string, int> portname_to_index;



From louradou at mail.berlios.de  Mon Oct 22 20:17:30 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 22 Oct 2007 20:17:30 +0200
Subject: [Plearn-commits] r8201 - trunk/plearn_learners/online
Message-ID: <200710221817.l9MIHUx6025073@sheep.berlios.de>

Author: louradou
Date: 2007-10-22 20:17:28 +0200 (Mon, 22 Oct 2007)
New Revision: 8201

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Included: a batch version of the cost computation
(needed when the cost function is non stochastic, for instance)



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-22 18:10:48 UTC (rev 8200)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-22 18:17:28 UTC (rev 8201)
@@ -38,7 +38,6 @@
 
 
 #define PL_LOG_MODULE_NAME "DeepBeliefNet"
-
 #include "DeepBeliefNet.h"
 #include <plearn/io/pl_log.h>
 
@@ -66,7 +65,6 @@
     n_classes( -1 ),
     use_classification_cost( true ),
     reconstruct_layerwise( false ),
-    n_layers( 0 ),
     i_output_layer( -1 ),
     online ( false ),
     background_gibbs_update_ratio(0),
@@ -86,6 +84,7 @@
     cumulative_testing_time( 0 )
 {
     random_gen = new PRandom();
+    n_layers = 0;
 }
 
 ////////////////////
@@ -275,19 +274,6 @@
                   OptionBase::learntoption | OptionBase::nosave,
                   "Cumulative testing time since age=0, in seconds.\n");
 
-
-    /*
-    declareOption(ol, "n_final_costs", &DeepBeliefNet::n_final_costs,
-                  OptionBase::learntoption,
-                  "Number of final costs");
-     */
-
-    /*
-    declareOption(ol, "", &DeepBeliefNet::,
-                  OptionBase::learntoption,
-                  "");
-     */
-
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -506,7 +492,7 @@
 {
     MODULE_LOG << "build_classification_cost() called" << endl;
 
-    PLASSERT_MSG(batch_size == 1, "DeepBeliefNet::build_classification_cost - "
+    PLASSERT_MSG(batch_size > 1, "DeepBeliefNet::build_classification_cost - "
             "This method has not been verified yet for minibatch "
             "compatibility");
 
@@ -594,40 +580,41 @@
             final_module->random_gen = random_gen;
             final_module->forget();
         }
-    }
-    else
-    {
-        if( layers[n_layers-1]->size != final_cost->input_size )
-            PLERROR("DeepBeliefNet::build_final_cost() - "
-                    "layers[%i]->size (%d) != final_cost->input_size (%d)."
-                    "\n", n_layers-1, layers[n_layers-1]->size,
-                    final_cost->input_size);
-    }
 
-    // check target size and final_cost->input_size
-    if( n_classes == 0 ) // regression
-    {
-        if( final_cost->input_size != targetsize() )
-            PLERROR("DeepBeliefNet::build_final_cost() - "
+        // check target size and final_cost->input_size
+        if( n_classes == 0 ) // regression
+        {
+            if( final_cost->input_size != targetsize() )
+                PLERROR("DeepBeliefNet::build_final_cost() - "
                     "final_cost->input_size (%d) != targetsize() (%d), "
                     "although we are doing regression (n_classes == 0).\n",
                     final_cost->input_size, targetsize());
-    }
-    else
-    {
-        if( final_cost->input_size != n_classes )
-            PLERROR("DeepBeliefNet::build_final_cost() - "
+        }
+        else
+        {
+            if( final_cost->input_size != n_classes )
+                PLERROR("DeepBeliefNet::build_final_cost() - "
                     "final_cost->input_size (%d) != n_classes (%d), "
                     "although we are doing classification (n_classes != 0).\n",
                     final_cost->input_size, n_classes);
 
-        if( targetsize_ >= 0 && targetsize() != 1 )
-            PLERROR("DeepBeliefNet::build_final_cost() - "
+            if( targetsize_ >= 0 && targetsize() != 1 )
+                PLERROR("DeepBeliefNet::build_final_cost() - "
                     "targetsize() (%d) != 1, "
                     "although we are doing classification (n_classes != 0).\n",
                     targetsize());
+        }
     }
+    else
+    {
+        if( layers[n_layers-1]->size != final_cost->input_size )
+            PLERROR("DeepBeliefNet::build_final_cost() - "
+                    "layers[%i]->size (%d) != final_cost->input_size (%d)."
+                    "\n", n_layers-1, layers[n_layers-1]->size,
+                    final_cost->input_size);
+    }
 
+
     // Share random_gen with final_cost, unless it already has one
     if( !(final_cost->random_gen) )
     {
@@ -910,7 +897,7 @@
         int end_stage = min(cumulative_schedule[n_layers-1], nstages);
         if( use_classification_cost && (stage < end_stage) )
         {
-            PLASSERT_MSG(batch_size == 1, "'use_classification_cost' code not "
+            PLASSERT_MSG(batch_size > 1, "'use_classification_cost' code not "
                     "verified with mini-batch learning yet");
 
             MODULE_LOG << "Training the classification module" << endl;
@@ -950,7 +937,6 @@
             }
         }
 
-
         /***** fine-tuning by gradient descent *****/
         end_stage = min(cumulative_schedule[n_layers], nstages);
         if( stage >= end_stage )
@@ -1474,11 +1460,11 @@
                 save_layer_activations.resize(source_act.length(),
                                               source_act.width());
                 save_layer_activations << source_act;
-                const Mat& source_exp = layers[i]->getExpectations();
-                save_layer_expectations.resize(source_exp.length(),
-                                               source_exp.width());
-                save_layer_expectations << source_exp;
             }
+            const Mat& source_exp = layers[i]->getExpectations();
+            save_layer_expectations.resize(source_exp.length(),
+                                           source_exp.width());
+            save_layer_expectations << source_exp;
 
             if (reconstruct_layerwise)
             {
@@ -1497,8 +1483,9 @@
             if( i > 0 )
             {
                 layers[i]->activations << save_layer_activations;
-                layers[i]->getExpectations() << save_layer_expectations;
             }
+            layers[i]->getExpectations() << save_layer_expectations;
+
         }
     }
 
@@ -1657,7 +1644,7 @@
 {
     real lr;
     PLASSERT( joint_layer );
-    PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
+    PLASSERT_MSG(batch_size > 1, "Not implemented for mini-batches");
 
     layers[0]->expectation << input;
     for( int i=0 ; i<n_layers-2 ; i++ )
@@ -1956,7 +1943,7 @@
     // do it AFTER the bprop to avoid interfering with activations used in bprop
     // (and do not worry that the weights have changed a bit). This is incoherent
     // with the current implementation in the greedy stage.
-    if (reconstruct_layerwise)
+    if ( reconstruct_layerwise )
     {
         Mat rc = train_costs.column(reconstruction_cost_index);
         rc.clear();
@@ -2232,6 +2219,7 @@
     }
 }
 
+
 void DeepBeliefNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
@@ -2248,8 +2236,7 @@
                 target, costs[nll_cost_index] );
 
         costs[class_cost_index] =
-            (argmax(output.subVec(0, n_classes)) == (int) round(target[0]))? 0
-            : 1;
+            (argmax(output.subVec(0, n_classes)) == (int) round(target[0]))? 0 : 1;
     }
 
     if( final_cost )
@@ -2283,6 +2270,82 @@
 
 }
 
+//! This function is usefull when the NLL CostModule AND/OR the final_cost Module
+//! are more efficient with batch computation (or need to be computed on a bunch of examples, as LayerCostModule)
+void DeepBeliefNet::computeOutputsAndCosts(const Mat& inputs, const Mat& targets, 
+                                      Mat& outputs, Mat& costs) const
+{
+    int nsamples = inputs.length();
+    PLASSERT( targets.length() == nsamples );
+    outputs.resize( nsamples, outputsize() );
+    costs.resize( nsamples, cost_names.length() );
+    costs.fill( MISSING_VALUE );
+    for (int isample = 0; isample < nsamples; isample++ )
+    {
+        Vec in_i = inputs(isample);
+        Vec out_i = outputs(isample); 
+	computeOutput(in_i, out_i);
+        if( !partial_costs.isEmpty() )
+        {
+            Vec pcosts;
+            for( int i=0 ; i<n_layers-1 ; i++ )
+                // propagate into local cost associated to output of layer i+1
+                if( partial_costs[ i ] )
+                {
+                    partial_costs[ i ]->fprop( layers[ i+1 ]->expectation,
+                                               targets(isample), pcosts);
+
+                    costs(isample).subVec(partial_costs_indices[i], pcosts.length())
+                        << pcosts;
+                }
+        }
+	if (reconstruct_layerwise)
+           costs(isample).subVec(reconstruction_cost_index, reconstruction_costs.length())
+                << reconstruction_costs;
+    }
+    computeClassifAndFinalCostsFromOutputs(inputs, outputs, targets, costs);
+}
+
+void DeepBeliefNet::computeClassifAndFinalCostsFromOutputs(const Mat& inputs, const Mat& outputs,
+                                           const Mat& targets, Mat& costs) const
+{
+    // Compute the costs from *already* computed output.
+
+    int nsamples = inputs.length();
+    PLASSERT( nsamples > 0 );
+    PLASSERT( targets.length() == nsamples );
+    PLASSERT( targets.width() == 1 );
+    PLASSERT( outputs.length() == nsamples );
+    PLASSERT( costs.length() == nsamples );
+
+
+    if( use_classification_cost )
+    {
+        Vec pcosts;
+        classification_cost->CostModule::fprop( outputs.subMat(0, 0, nsamples, n_classes),
+                                                targets, pcosts );
+        costs.subMat( 0, nll_cost_index, nsamples, 1) << pcosts;
+
+        for (int isample = 0; isample < nsamples; isample++ )
+	    costs(isample,class_cost_index) =
+                (argmax(outputs(isample).subVec(0, n_classes)) == (int) round(targets(isample,0))) ? 0 : 1;
+    }
+
+    if( final_cost )
+    {
+        int init = use_classification_cost ? n_classes : 0;
+        final_cost->fprop( outputs.subMat(0, init, nsamples, outputs(0).size() - init ),
+                           targets, final_cost_values );
+
+        costs.subMat(0, final_cost_index, nsamples, final_cost_values.width())
+            << final_cost_values;
+    }
+
+    if( !partial_costs.isEmpty() )
+        PLERROR("cannot compute partial costs in DeepBeliefNet::computeCostsFromOutputs(Mat&, Mat&, Mat&, Mat&)"
+	        "(expectations are not up to date in the batch version)");
+}
+
 void DeepBeliefNet::test(VMat testset, PP<VecStatsCollector> test_stats, VMat testoutputs, VMat testcosts) const
 {
 
@@ -2375,7 +2438,6 @@
         final_cost->setLearningRate( the_learning_rate );
 }
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-10-22 18:10:48 UTC (rev 8200)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-10-22 18:17:28 UTC (rev 8201)
@@ -36,7 +36,6 @@
 
 /*! \file DeepBeliefNet.h */
 
-
 #ifndef DeepBeliefNet_INC
 #define DeepBeliefNet_INC
 
@@ -52,6 +51,7 @@
 #include <plearn/sys/Profiler.h>
 
 namespace PLearn {
+using namespace std;
 
 /**
  * Neural net, learned layer-wise in a greedy fashion.
@@ -221,10 +221,15 @@
     // (PLEASE IMPLEMENT IN .cc)
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
+    virtual void computeOutputsAndCosts(const Mat& inputs, const Mat& targets, 
+                                        Mat& outputs, Mat& costs) const;
+
     //! Computes the costs from already computed output.
     // (PLEASE IMPLEMENT IN .cc)
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
+    virtual void computeClassifAndFinalCostsFromOutputs(const Mat& inputs, const Mat& outputs,
+                                                        const Mat& targets, Mat& costs) const;
 
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
@@ -404,6 +409,9 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
 private:
     //#####  Private Member Functions  ########################################
 



From nouiz at mail.berlios.de  Mon Oct 22 20:18:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Oct 2007 20:18:53 +0200
Subject: [Plearn-commits] r8202 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200710221818.l9MIIr6V025159@sheep.berlios.de>

Author: nouiz
Date: 2007-10-22 20:18:53 +0200 (Mon, 22 Oct 2007)
New Revision: 8202

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc
   branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h
Log:
localised the globals variable when this make the class easier to understant


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc	2007-10-22 18:17:28 UTC (rev 8201)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.cc	2007-10-22 18:18:53 UTC (rev 8202)
@@ -105,8 +105,8 @@
 void MissingIndicatorVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
 {
     source->getExample(i, source_input, target, weight);
-    new_col = 0;
-    for (int source_col = 0; source_col < source_inputsize; source_col++)
+    for (int source_col = 0, new_col = 0; source_col < source_inputsize;
+	 source_col++)
     {
       input[new_col] = source_input[source_col];
       new_col += 1;
@@ -197,36 +197,35 @@
     if(train_inputsize < 1) PLERROR("In MissingIndicatorVMatrix::inputsize of the train vmat must be supplied, got : %i", train_inputsize);
     source_width = source->width();
     source_targetsize = source->targetsize();
-    source_weightsize = source->weightsize();
     source_inputsize = source->inputsize();
     if (train_width != source_width) PLERROR("In MissingIndicatorVMatrix::train set and source width must agree, got : %i, %i", train_width, source_width);
     if (train_targetsize != source_targetsize) PLERROR("In MissingIndicatorVMatrix::train set and source targetsize must agree, got : %i, %i", train_targetsize, source_targetsize);
-    if (train_weightsize != source_weightsize) PLERROR("In MissingIndicatorVMatrix::train set and source weightsize must agree, got : %i, %i", train_weightsize, source_weightsize);
+    if (train_weightsize != source->weightsize()) PLERROR("In MissingIndicatorVMatrix::train set and source weightsize must agree, got : %i, %i", train_weightsize, source->weightsize());
     if (train_inputsize != source_inputsize) PLERROR("In MissingIndicatorVMatrix::train set and source inputsize must agree, got : %i, %i", train_inputsize, source_inputsize);
     train_input.resize(train_width);
     train_var_missing.resize(train_inputsize);
     train_var_missing.clear();
-    for (train_row = 0; train_row < train_length; train_row++)
+    for (int train_row = 0; train_row < train_length; train_row++)
     {
         train_set->getRow(train_row, train_input);
-        for (train_col = 0; train_col < train_inputsize; train_col++)
+        for (int train_col = 0; train_col < train_inputsize; train_col++)
         {
             if (is_missing(train_input[train_col])) train_var_missing[train_col] = 1;
         }
     }
-    new_width = train_width;
-    new_inputsize = train_inputsize;
-    for (train_col = 0; train_col < train_inputsize; train_col++)
+    int new_width = train_width;
+    int new_inputsize = train_inputsize;
+    for (int train_col = 0; train_col < train_inputsize; train_col++)
     {
         new_width += train_var_missing[train_col];
         new_inputsize += train_var_missing[train_col];
     }
     train_field_names.resize(train_width);
     source_rel_pos.resize(new_width);
-    new_field_names.resize(new_width);
+    TVec<string> new_field_names(new_width);
     train_field_names = train_set->fieldNames();
-    new_col = 0;
-    for (train_col = 0; train_col < train_inputsize; train_col++)
+    int new_col = 0;
+    for (int train_col = 0; train_col < train_inputsize; train_col++)
     {
       new_field_names[new_col] = train_field_names[train_col];
       source_rel_pos[new_col] = train_col;
@@ -238,7 +237,7 @@
           new_col += 1;
       }
     }
-    for (train_col = train_inputsize; train_col < train_width; train_col++)
+    for (int train_col = train_inputsize; train_col < train_width; train_col++)
     {
       new_field_names[new_col] = train_field_names[train_col];
       source_rel_pos[new_col] = train_col;

Modified: branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h	2007-10-22 18:17:28 UTC (rev 8201)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/MissingIndicatorVMatrix.h	2007-10-22 18:18:53 UTC (rev 8202)
@@ -96,8 +96,6 @@
   int          train_inputsize;
   int          train_targetsize;
   int          train_weightsize;
-  int          train_row;
-  int          train_col;
   Vec          train_input;
   TVec<string> train_field_names;
   TVec<int>    train_var_missing;
@@ -105,13 +103,8 @@
   int          source_width;
   int          source_inputsize;
   int          source_targetsize;
-  int          source_weightsize;
   Vec          source_input;
   TVec<int>    source_rel_pos;
-  int          new_width;
-  int          new_inputsize;
-  int          new_col;
-  TVec<string> new_field_names;
 
           void         build_();
           void         buildNewRecordFormat();



From louradou at mail.berlios.de  Mon Oct 22 20:28:47 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 22 Oct 2007 20:28:47 +0200
Subject: [Plearn-commits] r8203 - trunk/plearn_learners/online
Message-ID: <200710221828.l9MISlh8025825@sheep.berlios.de>

Author: louradou
Date: 2007-10-22 20:28:47 +0200 (Mon, 22 Oct 2007)
New Revision: 8203

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-10-22 18:18:53 UTC (rev 8202)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-10-22 18:28:47 UTC (rev 8203)
@@ -409,9 +409,6 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
-    //! Declare the methods that are remote-callable
-    static void declareMethods(RemoteMethodMap& rmm);
-
 private:
     //#####  Private Member Functions  ########################################
 



From nouiz at mail.berlios.de  Mon Oct 22 20:38:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Oct 2007 20:38:23 +0200
Subject: [Plearn-commits] r8204 - trunk/plearn/io
Message-ID: <200710221838.l9MIcNoH026453@sheep.berlios.de>

Author: nouiz
Date: 2007-10-22 20:38:22 +0200 (Mon, 22 Oct 2007)
New Revision: 8204

Modified:
   trunk/plearn/io/PStream.h
Log:
typo fix


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-10-22 18:28:47 UTC (rev 8203)
+++ trunk/plearn/io/PStream.h	2007-10-22 18:38:22 UTC (rev 8204)
@@ -1523,7 +1523,7 @@
 operator<<(PStream &out, const set<T> &v)
 { writeSet(out, v); return out; }
 
-/// @deprected Use openFile instead.
+/// @deprecated Use openFile instead.
 class PIFStream: public PStream
 {
 public:



From nouiz at mail.berlios.de  Mon Oct 22 20:46:41 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Oct 2007 20:46:41 +0200
Subject: [Plearn-commits] r8205 - trunk/plearn_learners_experimental
Message-ID: <200710221846.l9MIkfh4026935@sheep.berlios.de>

Author: nouiz
Date: 2007-10-22 20:46:41 +0200 (Mon, 22 Oct 2007)
New Revision: 8205

Modified:
   trunk/plearn_learners_experimental/linearalign.h
Log:
changed comment to be doxygen friendly


Modified: trunk/plearn_learners_experimental/linearalign.h
===================================================================
--- trunk/plearn_learners_experimental/linearalign.h	2007-10-22 18:38:22 UTC (rev 8204)
+++ trunk/plearn_learners_experimental/linearalign.h	2007-10-22 18:46:41 UTC (rev 8205)
@@ -18,7 +18,7 @@
 using namespace std;
 
 /**
- at descr : calculates the nodekernel...note that more the similarity more the nodekernel.its computing the exp(-distance).distance is computed taking into account the deviation and an additional scaling factor sigma
+calculates the nodekernel...note that more the similarity more the nodekernel.its computing the exp(-distance).distance is computed taking into account the deviation and an additional scaling factor sigma
 @param : x : property matrix of first molecule
 @param : y : property matrix of the model (template)
 @param : sigma : additional scaling factor. 
@@ -69,10 +69,11 @@
 }
 
 /**
+given a node kernel it picks the top n node kernels in each row and column . only these weights need to considered as all other weights will probably come out to be zero anyway. this way we can reduce the number of variables in our optimization problem 
 @param : nkmat : node kernel matrix
 @param : weights : this is a boolean matrix which will contain which weights are to be considered
 @param : n : minimum number of weights to be considered in a row or column
- at descr : given a node kernel it picks the top n node kernels in each row and column . only these weights need to considered as all other weights will probably come out to be zero anyway. this way we can reduce the number of variables in our optimization problem */
+*/
 void static findRelevantWeights(const Mat& nkmat,vector< vector<bool> >& weights , int n){
 	const int nx = nkmat.nrows();
 	const int ny = nkmat.ncols();
@@ -154,6 +155,7 @@
 
 
 /**
+see documenation about the alignment procedure. this is the function being used to align
 @param : xmat : coordinates of molecule x (to be transformed)
 @param : ymat : coordinates of molecule y
 @param : wij : weight matrix
@@ -161,7 +163,7 @@
 @param : xm : weighted centroid of x
 @param : ym : weighted centroid of y
 @return : the error which gives an estimate of how well the molecules were aligned 
- at descr : see documenation about the alignment procedure. this is the function being used to align*/
+*/
 real static calcTransformation4(const Mat &xmat,const Mat& ymat,const Mat& wij,const Mat &nk,Mat& rot,Vec& xm,Vec& ym){
 	int newn = xmat.nrows()+ymat.nrows();
 	Mat xmat2(newn,xmat.ncols());
@@ -211,7 +213,7 @@
 
 
 /**
- at descr : extension of calcLinearWeights ... uses fixed sigma and automatically selects suitable thresh from a fixed list. this is the version used for all calculations.
+extension of calcLinearWeights ... uses fixed sigma and automatically selects suitable thresh from a fixed list. this is the version used for all calculations.
 @see : calcLinearWeights */
 void static autoThreshLP(const Mat& dist1,const Mat& dist2,const Mat& nk,const vector< pair<int,int> >& wlist,const vector< vector<bool> >& wfilter,Mat& wm){
 	int nterms = 0;
@@ -340,11 +342,11 @@
 }
 
 /**
+given the molecule names , reads the vrml files and the properties and passes on the appropriate data to autoThreshLP. this is the front end that is used.
 @param : name1 : name of molecule to be aligned
 @param : name2 : name of molecule with which to align
 @param : wm : weight matrix
 @param : isweighted : take into account deviations ?
- at descr : given the molecule names , reads the vrml files and the properties and passes on the appropriate data to autoThreshLP. this is the front end that is used.
 @see : autoThreshLP */
 void static performLP(PMolecule name1,MoleculeTemplate name2,Mat& wm,bool isweighted){
 try{	



From louradou at mail.berlios.de  Mon Oct 22 23:28:14 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Mon, 22 Oct 2007 23:28:14 +0200
Subject: [Plearn-commits] r8206 - trunk/plearn_learners/online
Message-ID: <200710222128.l9MLSEQj006434@sheep.berlios.de>

Author: louradou
Date: 2007-10-22 23:28:13 +0200 (Mon, 22 Oct 2007)
New Revision: 8206

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
corrections to last commit (which broke a test)

In this last version of class DeepBeliefNet:
included a batch version of the cost computation
(needed when the cost function is non stochastic, for instance)



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-22 18:46:41 UTC (rev 8205)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-22 21:28:13 UTC (rev 8206)
@@ -492,7 +492,7 @@
 {
     MODULE_LOG << "build_classification_cost() called" << endl;
 
-    PLASSERT_MSG(batch_size > 1, "DeepBeliefNet::build_classification_cost - "
+    PLASSERT_MSG(batch_size == 1, "DeepBeliefNet::build_classification_cost - "
             "This method has not been verified yet for minibatch "
             "compatibility");
 
@@ -897,7 +897,7 @@
         int end_stage = min(cumulative_schedule[n_layers-1], nstages);
         if( use_classification_cost && (stage < end_stage) )
         {
-            PLASSERT_MSG(batch_size > 1, "'use_classification_cost' code not "
+            PLASSERT_MSG(batch_size == 1, "'use_classification_cost' code not "
                     "verified with mini-batch learning yet");
 
             MODULE_LOG << "Training the classification module" << endl;
@@ -1644,7 +1644,7 @@
 {
     real lr;
     PLASSERT( joint_layer );
-    PLASSERT_MSG(batch_size > 1, "Not implemented for mini-batches");
+    PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
 
     layers[0]->expectation << input;
     for( int i=0 ; i<n_layers-2 ; i++ )



From nouiz at mail.berlios.de  Mon Oct 22 23:37:41 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 22 Oct 2007 23:37:41 +0200
Subject: [Plearn-commits] r8207 - trunk/plearn/python
Message-ID: <200710222137.l9MLbfUX006782@sheep.berlios.de>

Author: nouiz
Date: 2007-10-22 23:37:41 +0200 (Mon, 22 Oct 2007)
New Revision: 8207

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
typo for doxygen


Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-10-22 21:28:13 UTC (rev 8206)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-10-22 21:37:41 UTC (rev 8207)
@@ -574,7 +574,7 @@
 //! and then bridge to using numarray.  Fieldnames and other metainfos
 //! are lost when converting to Python.
 //!
-//! @TODO  Must provide a complete Python wrapper over VMatrix objects
+//! @todo  Must provide a complete Python wrapper over VMatrix objects
 template<> struct ConvertToPyObject<PP<VMatrix> >
 { static PyObject* newPyObject(const PP<VMatrix>& vm); };
 



From larocheh at mail.berlios.de  Tue Oct 23 21:27:39 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Tue, 23 Oct 2007 21:27:39 +0200
Subject: [Plearn-commits] r8208 - trunk/plearn_learners/online
Message-ID: <200710231927.l9NJRdfq020089@sheep.berlios.de>

Author: larocheh
Date: 2007-10-23 21:27:39 +0200 (Tue, 23 Oct 2007)
New Revision: 8208

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
Corrected some bugs: layers[i]->forget() was not called in class forget() and SAA's setLearning had a bug...


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-22 21:37:41 UTC (rev 8207)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-10-23 19:27:39 UTC (rev 8208)
@@ -706,6 +706,9 @@
 {
     inherited::forget();
 
+    for( int i=0 ; i<n_layers ; i++ )
+        layers[i]->forget();
+
     for( int i=0 ; i<n_layers-1 ; i++ )
         connections[i]->forget();
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-22 21:37:41 UTC (rev 8207)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-23 19:27:39 UTC (rev 8208)
@@ -637,6 +637,9 @@
     */
     inherited::forget();
 
+    for( int i=0 ; i<n_layers ; i++ )
+        layers[i]->forget();
+    
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
         connections[i]->forget();
@@ -659,6 +662,12 @@
         }        
     }
 
+    if(direct_connections.length() != 0)
+    {        
+        for( int i=0 ; i<n_layers-1 ; i++)
+            direct_connections[i]->forget();
+    }
+
     stage = 0;
     greedy_stages.clear();
 }
@@ -776,7 +785,6 @@
                     if(partial_costs.length() != 0 && partial_costs[i])
                         partial_costs[i]->setLearningRate( lr );
                 }
-
                 sample = *this_stage % nsamples;
                 train_set->getExample(sample, input, target, weight);
                 greedyStep( input, target, i, train_costs );
@@ -1662,8 +1670,8 @@
     }
     layers[n_layers-1]->setLearningRate( the_learning_rate );
 
-    final_cost->setLearningRate( fine_tuning_learning_rate );
-    final_module->setLearningRate( fine_tuning_learning_rate );
+    final_cost->setLearningRate( the_learning_rate );
+    final_module->setLearningRate( the_learning_rate );
 }
 
 



From nouiz at mail.berlios.de  Wed Oct 24 17:52:33 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 24 Oct 2007 17:52:33 +0200
Subject: [Plearn-commits] r8209 - trunk/plearn/python
Message-ID: <200710241552.l9OFqXXr022085@sheep.berlios.de>

Author: nouiz
Date: 2007-10-24 17:52:33 +0200 (Wed, 24 Oct 2007)
New Revision: 8209

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
fix case for doxygen


Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-10-23 19:27:39 UTC (rev 8208)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-10-24 15:52:33 UTC (rev 8209)
@@ -88,8 +88,8 @@
 
 //! Used to retrieve integer values from python if possible without precision
 //! loss, and convert them to requested type.
-//! @TODO: put I's name in error message?
-//! @TODO: call PyErr_Print() before PyErr_Clear()?
+//! @todo: put I's name in error message?
+//! @todo: call PyErr_Print() before PyErr_Clear()?
 template <class I>
 I integerFromPyObject(PyObject* pyobj, bool print_traceback)
 {
@@ -334,7 +334,7 @@
 
 ///***///***
 // PARTIAL specialisation from T*.  Assume Object*.
-// TODO: fix this assumption
+// todo: fix this assumption
 template <class T>
 struct ConvertFromPyObject<T*>
 {



From nouiz at mail.berlios.de  Wed Oct 24 21:12:32 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 24 Oct 2007 21:12:32 +0200
Subject: [Plearn-commits] r8210 - trunk/plearn/math
Message-ID: <200710241912.l9OJCWxA018076@sheep.berlios.de>

Author: nouiz
Date: 2007-10-24 21:12:31 +0200 (Wed, 24 Oct 2007)
New Revision: 8210

Modified:
   trunk/plearn/math/convolutions.cc
   trunk/plearn/math/plapack.cc
   trunk/plearn/math/random.cc
   trunk/plearn/math/random.h
Log:
Removed duplicate documentation. Doxygen generate warning when their is multiple file that comment the same function.


Modified: trunk/plearn/math/convolutions.cc
===================================================================
--- trunk/plearn/math/convolutions.cc	2007-10-24 15:52:33 UTC (rev 8209)
+++ trunk/plearn/math/convolutions.cc	2007-10-24 19:12:31 UTC (rev 8210)
@@ -49,11 +49,6 @@
 namespace PLearn {
 using namespace std;
 
-//! Convolve a source signal of length NS with a kernel of length NK
-//! with steps S, and put result in a destination signal which should
-//! be of length NS-NK+1.
-//! The destination signal is
-//!    dest_signal[i] = sum_{j=0}^{NK-1} source_signal[i*step+j]*kernel[j]
 void convolve1D(const Vec& source_signal, const Vec& kernel,
                 const Vec& dest_signal, int step, bool accumulate)
 {
@@ -83,17 +78,6 @@
     }
 }
 
-//! Back-convolve INTO a "source" signal of length NS with a kernel of length
-//! NK and FROM a "destination" signal which should be of length NS-NK+1
-//! This is EXACTLY the TRANSPOSE operation of a convolve1D with the same
-//! arguments, with computations flowing in the other direction.
-//! for i=0 to nd-1:
-//!   for j=0 to nk-1:
-//!    source_signal[i*step+j] += dest_signal[i]*kernel[j]
-//! If the accumulate flag is not set, then source_signal is first cleared.
-//! N.B. THIS IS THE SAME AS COMPUTING dC/dsource_signal (into the
-//! source_signal argument), GIVEN dC/ddest_signal, i.e. this function
-//! does part of the work done by convolve1Dbackprop.
 void backConvolve1D(const Vec& source_signal, const Vec& kernel,
                     const Vec& dest_signal, int step, bool accumulate)
 {
@@ -127,11 +111,6 @@
 }
 
 
-//! Increment dC/dsource_signal and dC/dkernel, given dC/ddest_signal, with
-//! dest_signal computed as per convolve1D(source_signal, kernel, dest_signal):
-//!    dC/dsource_signal[k] += sum_{j=0}^{NK-1} 1_{k>=j && k-j<ND} dC_ddest_signal[k-j]*kernel[j]
-//!    dC/dkernel[j] += sum_{k=0}^{ND-1} 1_{k>=j && k-j<ND} dC_ddest_signal[k-j]*source_signal[k]
-//! (consider the equivalence: k = i+j)
 void convolve1Dbackprop(const Vec& source_signal, const Vec& kernel,
                         const Vec& dC_ddest_signal,
                         const Vec& dC_dsource_signal, const Vec& dC_dkernel,
@@ -186,9 +165,6 @@
     }
 }
 
-//! Same as above, but increments only dC/dkernel, not dC/dsource_signal
-//!    dC/dkernel[j] += sum_{k=0}^{ND-1} 1_{k>=j && k-j<ND} dC_ddest_signal[k-j]*source_signal[k]
-//! (consider the equivalence: k = i+j)
 void convolve1Dbackprop(const Vec& source_signal,
                         const Vec& dC_ddest_signal,
                         const Vec& dC_dkernel,
@@ -227,11 +203,6 @@
 }
 
 
-//! Increment dC/ddest_signal and dC/dkernel, given dC/ddest_signal, with
-//! source_signal computed as per
-//! backConvolve1D(source_signal, kernel, dest_signal):
-//!    dC/ddest_signal[i] += sum_{j=0}^{NK-1} dC_dsource_signal[i+j]*kernel[j]
-//!    dC/dkernel[j] += sum_{i=0}^{ND-1} dC_dsource_signal[i+j]*dest_signal[i]
 void backConvolve1Dbackprop(const Vec& kernel, const Vec& dest_signal,
                             const Vec& dC_ddest_signal,
                             const Vec& dC_dsource_signal,
@@ -291,8 +262,6 @@
 }
 
 
-//! Same as above, but increments only dC/dkernel, not  dC/ddest_signal
-//!    dC/dkernel[j] += sum_{i=0}^{ND-1} dC_dsource_signal[i+j]*dest_signal[i]
 void backConvolve1Dbackprop(const Vec& dest_signal,
                             const Vec& dC_dsource_signal,
                             const Vec& dC_dkernel,
@@ -332,12 +301,6 @@
 }
 
 
-//! Convolve a (N1S x N2S) source image with a (N1K x N2K) kernel matrix,
-//! and put result in a destination matrix of dimensions (N1D x N2D), stepping
-//! by (step1,step2) in each direction, with NiS = NiD*stepi + NiK - 1.
-//! The destination image is
-//!    dest_image[i,j] = 
-//!      sum_{k1=0}^{N1K-1} sum_{k2=0}^{N2K-1} source_image[i*step1+k1,j*step2+k2]*kernel[k1,k2]
 void convolve2D(const Mat& source_image, const Mat& kernel,
                 const Mat& dest_image,
                 int step1, int step2, bool accumulate)
@@ -390,24 +353,6 @@
     }
 }
 
-//! Back-convolve INTO a (N1S x N2S) "source" image with a (N1K x N2K)
-//! kernel matrix, and FROM a "destination" image of dimensions (N1D x
-//! N2D), with NiS = NiD + NiK - 1.
-//! This is EXACTLY the TRANSPOSE of convolve2D(source_image, kernel,
-//! dest_image, 1, 1) with the same arguments, computations flowing in
-//! the other direction.
-//! The kernel window is stepped by one in both directions. The
-//! destination image is
-//! for i1=0 to N1D-1:
-//!  for i2=0 to N2D-1:
-//!   for j1=0 to N1K-1:
-//!    for j2=0 to N2K-1:
-//!     source_image[i1+j1,i2+j2] += dest_image[i1,i2]*kernel[j1,j2]
-//! If the accumulate flag is not set, then source_image is first cleared.
-//! N.B. When dest_image has been computed from kernel and source_image
-//! using convolve2D, THIS IS THE SAME AS COMPUTING dC/dsource_image
-//! (into the source_image argument), GIVEN dC/ddest_image, i.e.
-//! this function does part of the work done by convolve2Dbackprop.
 void backConvolve2D(const Mat& source_image, const Mat& kernel,
                     const Mat& dest_image,
                     int step1, int step2, bool accumulate)
@@ -461,14 +406,6 @@
     }
 }
 
-//! Increment dC/dsource_image and dC/dkernel, given dC/ddest_image, with
-//! dest_image computed as per convolve2D(source_image, kernel, dest_image):
-//! for i1=0 to N1D-1:
-//!  for i2=0 to N2D-1:
-//!   for j1=0 to N1K-1:
-//!    for j2=0 to N2K-1:
-//!     dC/dsource_image[i1+j1,i2+j2] += dC/dest_image[i1,i2]*kernel[j1,j2]
-//!     dC/dkernel[j1,j2] += dC/dest_image[i1,i2]*source_image[i1+j1,i2+j2]
 void convolve2Dbackprop(const Mat& source_image, const Mat& kernel,
                         const Mat& dC_ddest_image,
                         const Mat& dC_dsource_image, const Mat& dC_dkernel,
@@ -558,12 +495,6 @@
 }
 
 
-//! As above, but increments only dC/dkernel, not dC/dsource_image
-//! for i1=0 to N1D-1:
-//!  for i2=0 to N2D-1:
-//!   for j1=0 to N1K-1:
-//!    for j2=0 to N2K-1:
-//!     dC/dkernel[j1,j2] += dC/dest_image[i1,i2]*source_image[i1+j1,i2+j2]
 void convolve2Dbackprop(const Mat& source_image,
                         const Mat& dC_ddest_image,
                         const Mat& dC_dkernel,
@@ -622,15 +553,6 @@
 }
 
 
-//! Increment dC/ddest_image and dC/dkernel, given dC/dsource_image, with
-//! source_image computed as per
-//! backConvolve2D(source_image, kernel, dest_image):
-//! for i1=0 to N1D-1:
-//!  for i2=0 to N2D-1:
-//!   for j1=0 to N1K-1:
-//!    for j2=0 to N2K-1:
-//!     dC/ddest_image[i1,i2] += dC/dsource_image[i1+j1,i2+j2]*kernel[j1,j2]
-//!     dC/dkernel[j1,j2] += dC/dsource_image[i1+j1,i2+j2]*dest_image[i1,i2]
 void backConvolve2Dbackprop(const Mat& kernel, const Mat& dest_image,
                             const Mat& dC_ddest_image,
                             const Mat& dC_dsource_image, const Mat& dC_dkernel,
@@ -721,12 +643,6 @@
     }
 }
 
-//! As above, but increments only dC/dkernel, not dC/ddest_image
-//! for i1=0 to N1D-1:
-//!  for i2=0 to N2D-1:
-//!   for j1=0 to N1K-1:
-//!    for j2=0 to N2K-1:
-//!     dC/dkernel[j1,j2] += dC/dsource_image[i1+j1,i2+j2]*dest_image[i1,i2]
 void backConvolve2Dbackprop(const Mat& dest_image,
                             const Mat& dC_dsource_image,
                             const Mat& dC_dkernel,

Modified: trunk/plearn/math/plapack.cc
===================================================================
--- trunk/plearn/math/plapack.cc	2007-10-24 15:52:33 UTC (rev 8209)
+++ trunk/plearn/math/plapack.cc	2007-10-24 19:12:31 UTC (rev 8210)
@@ -873,14 +873,6 @@
     return sum_GCV;
 }
 
-//! Estimator of generalization error estimator called Generalized Cross-Validation (Craven & Wahba 1979),
-//! computed from the SVD of the input matrix X in the ridge regression. See the comments
-//! for GCV. This function implements the formula:
-//!          n ( ||Y||^2 - ||Z||^2 + sum_{j=1}^p z_j^2 (weight_decay / (d_j^2 + weight_decay))^2 )
-//!    GCV = ------------------------------------------------------------------------------------
-//!                   ( n - p + sum_{j=1}^p (weight_decay  / (d_j^2 + weight_decay)) )^2
-//! where Z = U' Y, z_j is the j-th element of Z and d_j is the j-th singular value of X, with X = U D V' the SVD.
-//! The vector s with s_i = (weight_decay  / (d_j^2 + weight_decay)) must also be pre-computed.
 real GCVfromSVD(real n, real Y2minusZ2, Vec Z, Vec s)
 {
     int p = s.length();
@@ -895,27 +887,6 @@
     return GCV;
 }
 
-//! Perform ridge regression WITH model selection (i.e. choosing the weight decay).
-//! The selection of weight decay is done in order to minimize the Generalized Cross Validation
-//! (GCV) criterion(Craven & Wahba 1979). The ridge regression weights minimize
-//!    min ||Y - X*W'||^2 + weight_decay ||W||^2.
-//! where Y is nxm, X is nxp, W is mxp, and this procedure ALSO selects a weight_decay value.
-//! The GCV is obtained by performing and SVD of X = U D V' and using the formula from
-//! (Bates, Lindstrom, Wahba, Yandell 1986) [tech report at http://www.stat.wisc.edu/~wahba/ftp1/oldie/775r.pdf]
-//! (here for m=1):
-//!          n ( ||Y||^2 - ||Z||^2 + sum_{j=1}^p z_j^2 (weight_decay / (d_j^2 + weight_decay))^2 )
-//!    GCV = ------------------------------------------------------------------------------------
-//!                   ( n - p + sum_{j=1}^p (weight_decay  / (d_j^2 + weight_decay)) )^2
-//! where Z = U' Y, z_j is the j-th element of Z and d_j is the j-th singular value of X.
-//! This formula can be efficiently re-computed for different values of weight decay.
-//! For this purpose, pre-compute the SVD can call GCVfromSVD. Once a weight decay
-//! has been selected, the SVD can also be used (optionally) to obtain the minimizing weights:
-//!    W = V inv(D^2 + weight_decay I) D Z
-//! If a positve initial_weight_decay_guess is provided, then instead of trying all the eigenvalues
-//! the algorithm searches from this initial guess, never going more than explore_threshold steps
-//! from the best weight decay found up to now. The weight decays tried are intermediate values
-//! (geometric average) between consecutive eigenvalues.
-//! Set best_GCV to the GCV of the selected weight decay and return that selected weight decay.
 real ridgeRegressionByGCV(Mat X, Mat Y, Mat W, real& best_gcv, bool X_is_transposed, 
                           real initial_weight_decay_guess, int explore_threshold, real min_weight_decay)
 {
@@ -1248,11 +1219,6 @@
 }
 
 #if 0
-//! Auxiliary function used by generalizedCFRidgeRegression in order to compute the estimated generalization error
-//! associated with a given choice of weight decay. The eigenvalues and eigenvectors are those of the squared design matrix.
-//! The eigenvectors are in the ROWS of the matrix.
-//! The RHS_matrix is eigenvectors*inputs'*targets, pre-computed.
-//! The computational cost of this call is O((rank+n_examples)*n_outputs*n_inputs)
 real LOOMSEofRidgeRegression(Mat inputs, Mat targets, Mat weights, real weight_decay, Vec eigenvalues, Mat eigenvectors, Mat predictions, 
                              Mat RHS_matrix, bool inputs_are_transposed)
 {

Modified: trunk/plearn/math/random.cc
===================================================================
--- trunk/plearn/math/random.cc	2007-10-24 15:52:33 UTC (rev 8209)
+++ trunk/plearn/math/random.cc	2007-10-24 19:12:31 UTC (rev 8210)
@@ -61,10 +61,6 @@
     =================
 */
 
-/*  
-    log_gamma(): 
-    returns the natural logarithm of the gamma function
-*/
 
 real  log_gamma(real xx)
 {
@@ -85,7 +81,6 @@
     return -tmp+pl_log(2.5066282746310005*ser/x);
 }
 
-/*! returns the natural logarithm of the beta function */
 real log_beta(real x, real y)
 {
     return log_gamma(x) + log_gamma(y) - log_gamma(x+y);
@@ -141,8 +136,6 @@
     return frac;
 }
 
-/*! returns the incomplete beta function B_z(x,y)  */
-/*! Note that z must be in [0,1] */
 real incomplete_beta(real z, real x, real y)
 {
     if (z>1 || z<0) PLERROR("incomplete_beta(z,x,y): z =%f must be in [0,1]",z);
@@ -154,7 +147,6 @@
     return 1-coeff*incomplete_beta_continued_fraction(1-z,y,x)/y;
 }
 
-/*! Student-t cumulative distribution function */
 real student_t_cdf(real t, int nb_degrees_of_freedom)
 {
     real p_t = 0.5*incomplete_beta(nb_degrees_of_freedom/(nb_degrees_of_freedom+t*t),0.5*nb_degrees_of_freedom,0.5);
@@ -188,10 +180,6 @@
     iset     = 0;
 }
 
-/*  
-    seed(): generates a seed for random number generators, using the cpu time.
-*/
-
 void  seed()
 {
     time_t  ltime;
@@ -204,10 +192,6 @@
                 60*60*24*today->tm_mday);
 }
 
-/*  
-    get_seed(): returns the current value of the 'seed'.
-*/
-
 int32_t get_seed()
 {
     int32_t seed = the_seed;
@@ -218,33 +202,21 @@
     Constants used by the 'numerical recipes' random number generators.
 */
 
-#define NTAB 32                 /*   needs for ran1 & ran2   */
-#define EPS 1.2e-7              /*   needs for ran1 & ran2   */
-#define RNMX (1.0-EPS)          /*   needs for ran1 & ran2   */
-#define IM1 2147483563          /*   needs for ran2          */
-#define IM2 2147483399          /*   needs for ran2          */
-#define AM1 (1.0/IM1)           /*   needs for ran2          */
-#define IMM1 (IM1-1)            /*   needs for ran2          */
-#define IA1 40014               /*   needs for ran2          */
-#define IA2 40692               /*   needs for ran2          */
-#define IQ1 53668               /*   needs for ran2          */
-#define IQ2 52774               /*   needs for ran2          */
-#define IR1 12211               /*   needs for ran2          */
-#define IR2 3791                /*   needs for ran2          */
-#define NDIV1 (1+IMM1/NTAB)     /*   needs for ran2          */
+#define NTAB 32                 /*   needs for ran1 & uniform_sample()   */
+#define EPS 1.2e-7              /*   needs for ran1 & uniform_sample()   */
+#define RNMX (1.0-EPS)          /*   needs for ran1 & uniform_sample()   */
+#define IM1 2147483563          /*   needs for uniform_sample()          */
+#define IM2 2147483399          /*   needs for uniform_sample()          */
+#define AM1 (1.0/IM1)           /*   needs for uniform_sample()          */
+#define IMM1 (IM1-1)            /*   needs for uniform_sample()          */
+#define IA1 40014               /*   needs for uniform_sample()          */
+#define IA2 40692               /*   needs for uniform_sample()          */
+#define IQ1 53668               /*   needs for uniform_sample()          */
+#define IQ2 52774               /*   needs for uniform_sample()          */
+#define IR1 12211               /*   needs for uniform_sample()          */
+#define IR2 3791                /*   needs for uniform_sample()          */
+#define NDIV1 (1+IMM1/NTAB)     /*   needs for uniform_sample()          */
 
-/*  
-    ran2(): long period ramdom number generator from the 'numerical recipes'.
-
-    Rem: - It is a long period (> 2 x 10^18) random number generator of L'Ecuyer
-    with Bays-Durham shuffle and added safeguards.
-
-    - Returns a uniform random deviate between 0.0 and 1.0
-    (exclusive of the endpoint values).
-
-    - Initialized with a negative seed.
-*/
-
 real uniform_sample()  
 {
     int j;
@@ -386,10 +358,6 @@
     ----------------
 */
 
-/*  
-    gamdev(): returns a deviate distributed as a gamma distribution from the 'numerical recipes'.
-*/
-
 real  gamdev(int ia)
 {
     int j;
@@ -418,11 +386,6 @@
     return x;
 }
 
-/*  
-    poidev(): returns a deviate distributed as a poisson distribution of mean (lambda) 'xm'
-    from the 'numerical recipes'.
-*/
-
 real  poidev(real xm)
 {
     static real sq,alxm,g,oldm=(-1.0);
@@ -556,7 +519,6 @@
     return int(N*uniform_sample());
 }
 
-//!  sample each element from uniform distribution U[minval,maxval]
 void fill_random_uniform(const Vec& dest, real minval, real maxval)
 {
     Vec::iterator it = dest.begin();
@@ -566,7 +528,6 @@
         *it = real(uniform_sample()*scale+minval);
 }
 
-//!  sample each element from the given set
 void fill_random_discrete(const Vec& dest, const Vec& set)
 {
     Vec::iterator it = dest.begin();
@@ -576,7 +537,6 @@
         *it = set[uniform_multinomial_sample(n)];
 }
 
-//!  sample each element from Normal(mean,sdev^2) distribution
 void fill_random_normal(const Vec& dest, real mean, real stdev)
 {
     Vec::iterator it = dest.begin();
@@ -585,7 +545,6 @@
         *it = real(gaussian_mu_sigma(mean,stdev));
 }
 
-//!  sample each element from multivariate Normal(mean,diag(sdev^2)) distribution
 void fill_random_normal(const Vec& dest, const Vec& mean, const Vec& stdev)
 {
 #ifdef BOUNDCHECK

Modified: trunk/plearn/math/random.h
===================================================================
--- trunk/plearn/math/random.h	2007-10-24 15:52:33 UTC (rev 8209)
+++ trunk/plearn/math/random.h	2007-10-24 19:12:31 UTC (rev 8210)
@@ -58,6 +58,7 @@
 real log_beta(real x, real y);
 
 /*! returns the incomplete beta function B_z(x,y)  (BUGGED?)*/
+/*! Note that z must be in [0,1] */
 real incomplete_beta(real z, real x, real y);
 /*! returns the incomplete beta function B_z(x,y)  */
 //double incbet(double x, double y, double z);
@@ -77,7 +78,15 @@
 /*!   returns the current seed used by the random number generator   */
 int32_t  get_seed();
 
-/*!   returns a random number uniformly distributed between 0 and 1   */
+/*! Long period ramdom number generator from the 'numerical recipes'.
+
+    Rem: - It is a long period (> 2 x 10^18) random number generator of L'Ecuyer
+    with Bays-Durham shuffle and added safeguards.
+
+    - Initialized with a negative seed.
+   @returns a random number uniformly distributed between 0 and 1 \
+   (exclusive of the endpoint values).
+*/
 real  uniform_sample();
 /*!   returns a random number uniformly distributed between a and b   */
 real  bounded_uniform(real a,real b);
@@ -96,9 +105,10 @@
   of means and standard deviations for each gaussian   */
 real  gaussian_mixture_mu_sigma(Vec& w, const Vec& mu, const Vec& sigma);
 
-/*!   returns a gamma distributed random number   */
+/*!   @returns a deviate distributed as a gamma distribution from the 'numerical recipes'. */
 real  gamdev(int ia);
-/*!   returns a poisson random number with lambda = "xm"   */
+/*!   @returns returns a deviate distributed as a poisson distribution of mean (lambda) 'xm'\
+    from the 'numerical recipes'.  */
 real  poidev(real xm);
 /*!   returns a binomial random number with probability = 'pp' and trials number = 'n'   */
 real  bnldev(real pp, int n=1);



From nouiz at mail.berlios.de  Wed Oct 24 22:52:57 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 24 Oct 2007 22:52:57 +0200
Subject: [Plearn-commits] r8211 - in trunk/plearn: base python
Message-ID: <200710242052.l9OKqvAR023579@sheep.berlios.de>

Author: nouiz
Date: 2007-10-24 22:52:57 +0200 (Wed, 24 Oct 2007)
New Revision: 8211

Modified:
   trunk/plearn/base/ObjectConversions.h
   trunk/plearn/base/ObjectGraphIterator.h
   trunk/plearn/python/PythonObjectWrapper.h
Log:
@function do not exist in doxygen. @fn is not usefull. So I remove @function


Modified: trunk/plearn/base/ObjectConversions.h
===================================================================
--- trunk/plearn/base/ObjectConversions.h	2007-10-24 19:12:31 UTC (rev 8210)
+++ trunk/plearn/base/ObjectConversions.h	2007-10-24 20:52:57 UTC (rev 8211)
@@ -59,8 +59,6 @@
 //#####  isConvertibleToObjectPtr  ############################################
 
 /**
- * @function isConvertibleToObjectPtr
- *
  * @brief Return true if \c toObjectPtr() or \c toIndexedObjectPtr would
  * succeed.
  */
@@ -107,8 +105,6 @@
 //#####  indexableObjectSize  #################################################
 
 /**
- *  @function indexableObjectSize
- *
  *  @brief Return 0 if the object is not indexable; otherwise, return one more
  *  than the maximum index allowed by \c toIndexedObjectPtr(); in other words,
  *  return the equivalent of the \c size() accessor on a vector.
@@ -163,8 +159,6 @@
 
 
 /**
- * @function toObjectPtr
- *
  * @brief Attempt to return a pointer to \c Object (or an error if the passed
  * argument cannot be considered an \c Object subclass)
  *
@@ -200,8 +194,6 @@
 //#####  toIndexedObjectPtr  ##################################################
 
 /**
- * @function toIndexedObjectPtr
- *
  * @brief Return the \c Object* at index \c i of an \c Array or \c TVec
  *
  * Produces a PLError if the conversion cannot be done.

Modified: trunk/plearn/base/ObjectGraphIterator.h
===================================================================
--- trunk/plearn/base/ObjectGraphIterator.h	2007-10-24 19:12:31 UTC (rev 8210)
+++ trunk/plearn/base/ObjectGraphIterator.h	2007-10-24 20:52:57 UTC (rev 8211)
@@ -298,7 +298,6 @@
 //#####  Broadcast  ###########################################################
 
 /**
- *  @function memfun_broadcast
  *  @brief    Call a specific member function across a graph of \c Objects.
  *
  *  The global function memfun_broadcast is used to call a member function on a
@@ -424,7 +423,6 @@
 
 
 /**
- *  @function memfun_broadcast_optname
  *  @brief    Call a specific member function across a graph of \c Objects with
  *            the option name as argument.
  *
@@ -531,7 +529,6 @@
 
 
 /**
- *  @function setoption_broadcast
  *  @brief    Broadcast a call to setOption only for specific classes
  *
  *  This function recursively calls Object::setOption (with a constant

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-10-24 19:12:31 UTC (rev 8210)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-10-24 20:52:57 UTC (rev 8211)
@@ -851,7 +851,6 @@
     //#####  Low-Level PyObject Creation  #####################################
 
     /**
-     *  @function newPyObject
      *  @brief    Create a raw \c PyObject* from various types
      */
 



From nouiz at mail.berlios.de  Wed Oct 24 22:54:59 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 24 Oct 2007 22:54:59 +0200
Subject: [Plearn-commits] r8212 - trunk/scripts
Message-ID: <200710242054.l9OKsxS9023698@sheep.berlios.de>

Author: nouiz
Date: 2007-10-24 22:54:58 +0200 (Wed, 24 Oct 2007)
New Revision: 8212

Modified:
   trunk/scripts/dbidispatch
Log:
Now will check the environnement variable DBIDISPATCH_DEFAULT_OPTION for default option.
Those option can be overrided by the parameter.
Also some clean up and print more information.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-10-24 20:52:57 UTC (rev 8211)
+++ trunk/scripts/dbidispatch	2007-10-24 20:54:58 UTC (rev 8212)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--mem=X] [--os=X] [--test|--notest] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
 LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %(ShortHelp)s
@@ -13,6 +13,7 @@
   The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
   The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
   The '--test' option makes dbidispatch generate the file %(ScriptName)s, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in %(ScriptName)s (so you can check the script).
+  The '--notest' option cancel the '--test' option.
   The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
 
 dbidispatch --test --file=tests
@@ -88,7 +89,9 @@
   aplearn myscript.plearn numhidden=25 wd=0.01
   aplearn myscript.plearn numhidden=25 wd=0.001
 
-In the file of the parameter --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
+In the file of the option --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
+
+The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
 
 if len(sys.argv) == 1:
@@ -107,10 +110,14 @@
     launch_cmd = 'Cluster'
 else:
     launch_cmd = 'Local'
+to_parse=[]
+env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")
+if env:
+    to_parse=env.split()
+to_parse.extend(sys.argv[1:])
+command_argv = to_parse[:]
+for argv in to_parse:
 
-command_argv = sys.argv[1:]
-for argv in sys.argv[1:]:
-
     if argv == "--help" or argv == "-h":
         print LongHelp
         sys.exit(0)
@@ -152,6 +159,8 @@
         dbi_param["file_redirect_stderr"]=False
     elif argv == "--test":
         dbi_param["test"]=True
+    elif argv == "--notest":
+        dbi_param["test"]=False
     elif argv.startswith("--file="):
         FILE = argv[7:]
     elif argv == "--32"  or argv == "--64" or argv == "--3264":
@@ -172,19 +181,20 @@
             assert(argv[7]=="=")
             dbi_param["micro"]=argv[8:]
     elif argv[0:1] == '-':
-	print "Unknow parameter (%s)",argv
+	print "Unknow option (%s)",argv
 	print ShortHelp
         sys.exit(1)
     else:
         break
     command_argv.remove(argv)
 
-print "\n\nThe jobs will be launched on the system:", launch_cmd,"\n\n"
-
 if len(command_argv) == 0 and FILE == "":
     print ShortHelp
     sys.exit(1)
 
+print "\n\nThe jobs will be launched on the system:", launch_cmd
+print "With options: ",dbi_param
+print "With the command to be expended:"," ".join(command_argv),"\n\n"
 
 def generate_combination(repl):
     if repl == []:
@@ -208,18 +218,14 @@
         p = re.compile('\{\{\S*\}\}')
         reg = p.search(arg)
         if reg:
-#            print "reg:",reg.group()[2:-2]
             curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
-#            print "curargs:",curargs
             newcurargs = []
             for curarg in curargs:
                 new = p.sub(curarg,arg)
-#                print "new:",new
                 newcurargs.append(new)
             repl.append(newcurargs)
         else:
             repl.append([arg])
-#    print "repl: ",repl
     argscombination = generate_combination(repl)
     return argscombination
 



From larocheh at mail.berlios.de  Thu Oct 25 21:50:06 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 25 Oct 2007 21:50:06 +0200
Subject: [Plearn-commits] r8213 - trunk/plearn_learners_experimental
Message-ID: <200710251950.l9PJo67s024914@sheep.berlios.de>

Author: larocheh
Date: 2007-10-25 21:50:05 +0200 (Thu, 25 Oct 2007)
New Revision: 8213

Modified:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
Log:
Debugging...


Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-24 20:54:58 UTC (rev 8212)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-25 19:50:05 UTC (rev 8213)
@@ -67,6 +67,9 @@
     fine_tuning_decrease_ct( 0. ),
     k_neighbors( 1 ),
     n_classes( -1 ),
+    do_not_use_knn_classifier(false),
+    output_weights_l1_penalty_factor(0),
+    output_weights_l2_penalty_factor(0),
     n_layers( 0 ),
     train_set_representations_up_to_date(false),
     currently_trained_layer( 0 )
@@ -174,6 +177,12 @@
                   OptionBase::buildoption,
                   "Number of classes.");
 
+    declareOption(ol, "do_not_use_knn_classifier", 
+                  &StackedFocusedAutoassociatorsNet::do_not_use_knn_classifier,
+                  OptionBase::buildoption,
+                  "Use standard neural net architecture, not the nearest "
+                  "neighbor model.");
+
     declareOption(ol, "greedy_stages", 
                   &StackedFocusedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -186,6 +195,18 @@
                   "Number of layers"
         );
 
+    declareOption(ol, "final_module", 
+                  &StackedFocusedAutoassociatorsNet::final_module,
+                  OptionBase::learntoption,
+                  "Output layer of neural net"
+        );
+
+    declareOption(ol, "final_cost", 
+                  &StackedFocusedAutoassociatorsNet::final_cost,
+                  OptionBase::learntoption,
+                  "Cost on output layer of neural net"
+        );
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -249,9 +270,59 @@
         }
 
         build_layers_and_connections();
+
+        if( do_not_use_knn_classifier & (!final_module || !final_cost) )
+            build_output_layer_and_cost();
     }
 }
 
+void StackedFocusedAutoassociatorsNet::build_output_layer_and_cost()
+{
+    GradNNetLayerModule* gnl = new GradNNetLayerModule();
+    gnl->input_size = layers[n_layers-1]->size;
+    gnl->output_size = n_classes;
+    gnl->L1_penalty_factor = output_weights_l1_penalty_factor;
+    gnl->L2_penalty_factor = output_weights_l2_penalty_factor;
+    gnl->random_gen = random_gen;
+    gnl->build();
+
+    SoftmaxModule* sm = new SoftmaxModule();
+    sm->input_size = n_classes;
+    sm->random_gen = random_gen;
+    sm->build();
+
+    ModuleStackModule* msm = new ModuleStackModule();
+    msm->modules.resize(2);
+    msm->modules[0] = gnl;
+    msm->modules[1] = sm;
+    msm->random_gen = random_gen;
+    msm->build();
+    final_module = msm;
+
+    final_module->forget();
+
+    NLLCostModule* nll = new NLLCostModule();
+    nll->input_size = n_classes;
+    nll->random_gen = random_gen;
+    nll->build();
+    
+    ClassErrorCostModule* class_error = new ClassErrorCostModule();
+    class_error->input_size = n_classes;
+    class_error->random_gen = random_gen;
+    class_error->build();
+
+    CombiningCostsModule* comb_costs = new CombiningCostsModule();
+    comb_costs->cost_weights.resize(2);
+    comb_costs->cost_weights[0] = 1;
+    comb_costs->cost_weights[1] = 0;
+    comb_costs->sub_costs.resize(2);
+    comb_costs->sub_costs[0] = nll;
+    comb_costs->sub_costs[1] = class_error;
+
+    final_cost = comb_costs;
+    final_cost->forget();
+}
+
 void StackedFocusedAutoassociatorsNet::build_layers_and_connections()
 {
     MODULE_LOG << "build_layers_and_connections() called" << endl;
@@ -487,6 +558,9 @@
     deepCopyField(pos_up_val, copies);
     deepCopyField(neg_down_val, copies);
     deepCopyField(neg_up_val, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_gradient, copies);
     deepCopyField(class_datasets, copies);
     deepCopyField(other_classes_proportions, copies);
     deepCopyField(nearest_neighbors_indices, copies);
@@ -496,6 +570,8 @@
     deepCopyField(train_set_representations_vmat, copies);
     deepCopyField(train_set_targets, copies);
     deepCopyField(greedy_stages, copies);
+    deepCopyField(final_module, copies);
+    deepCopyField(final_cost, copies);
 }
 
 
@@ -523,12 +599,26 @@
 
     train_set_representations_up_to_date = false;
 
+    for( int i=0 ; i<n_layers ; i++ )
+        layers[i]->forget();
+    
     for( int i=0 ; i<n_layers-1 ; i++ )
         connections[i]->forget();
     
+    if(unsupervised_layers.length() != 0)
+        for( int i=0 ; i<n_layers-1 ; i++ )
+            unsupervised_layers[i]->forget();
+    
+    if(unsupervised_connections.length() != 0)
+        for( int i=0 ; i<n_layers-1 ; i++ )
+            unsupervised_connections[i]->forget();
+    
     for( int i=0; i<reconstruction_connections.length(); i++)
         reconstruction_connections[i]->forget();
 
+    if( do_not_use_knn_classifier )
+        build_output_layer_and_cost();
+
     stage = 0;
     greedy_stages.clear();
 }
@@ -669,6 +759,10 @@
         similar_example.resize(inputsize());
         dissimilar_example.resize(inputsize());
 
+        final_cost_input.resize(n_classes);
+        final_cost_value.resize(2); // Should be resized anyways
+        final_cost_gradient.resize(n_classes);
+
         for( ; stage<nstages ; stage++ )
         {
             sample = stage % nsamples;
@@ -790,7 +884,7 @@
     // Similar example contribution
     substract(input_representation,similar_example_representation,
               expectation_gradients[index+1]);
-    expectation_gradients[index+1] *= 4/layers[index+1]->size;
+    expectation_gradients[index+1] *= 4/sqrt(layers[index+1]->size);
     
     // Dissimilar example contribution
     real dist = sqrt(powdistance(input_representation,
@@ -808,7 +902,7 @@
                   dissimilar_gradient_contribution);
         
         dissimilar_gradient_contribution *= -5.54*
-            safeexp(-2.77*dist/layers[index+1]->size)/dist;
+            safeexp(-2.77*dist/sqrt(layers[index+1]->size));
 
         expectation_gradients[index+1] += dissimilar_gradient_contribution;
     }
@@ -917,16 +1011,19 @@
 {
     train_set_representations_up_to_date = false;
 
-    // Get similar example representation
-    
-    computeRepresentation(similar_example, similar_example_representation, 
-                          n_layers-1);
+    if( !do_not_use_knn_classifier )
+    {
+        // Get similar example representation
+        
+        computeRepresentation(similar_example, similar_example_representation, 
+                              n_layers-1);
+        
+        // Get dissimilar example representation
+        
+        computeRepresentation(dissimilar_example, dissimilar_example_representation,
+                              n_layers-1);
+    }
 
-    // Get dissimilar example representation
-
-    computeRepresentation(dissimilar_example, dissimilar_example_representation, 
-                          n_layers-1);
-
     // Get example representation
 
     computeRepresentation(input, previous_input_representation, 
@@ -934,32 +1031,49 @@
 
     // Compute supervised gradient
 
-    // Similar example contribution
-    substract(input_representation,similar_example_representation,
-              expectation_gradients[n_layers-1]);
-    expectation_gradients[n_layers-1] *= 4/layers[n_layers-1]->size;
+
+    if( do_not_use_knn_classifier )
+    {
+        // Similar example contribution
+        substract(input_representation,similar_example_representation,
+                  expectation_gradients[n_layers-1]);
+        expectation_gradients[n_layers-1] *= 4/sqrt(layers[n_layers-1]->size);
     
-    // Dissimilar example contribution
-    real dist = sqrt(powdistance(input_representation,
-                                 dissimilar_example_representation,
-                                 2));
+        // Dissimilar example contribution
+        real dist = sqrt(powdistance(input_representation,
+                                     dissimilar_example_representation,
+                                     2));
     
-    if( dist == 0 )
-        PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
-                  " example representation is exactly the sample as the"
-                  " input example. Gradient would be infinite! Skipping this"
-                  " example...");
-    else
-    {
+        if( dist == 0 )
+            PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
+                      " example representation is exactly the sample as the"
+                      " input example. Gradient would be infinite! Skipping this"
+                      " example...");
+        else
+        {
 
-        substract(input_representation,dissimilar_example_representation,
-                  dissimilar_gradient_contribution);
+            substract(input_representation,dissimilar_example_representation,
+                      dissimilar_gradient_contribution);
 
-        dissimilar_gradient_contribution *= -5.54*
-            safeexp(-2.77*dist/layers[n_layers-1]->size)/dist;
+            dissimilar_gradient_contribution *= -5.54*
+                safeexp(-2.77*dist/sqrt(layers[n_layers-1]->size));
         
-        expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+            expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+        }
     }
+    else
+    {
+        final_module->fprop( input_representation, final_cost_input );
+        final_cost->fprop( final_cost_input, target, final_cost_value );
+        
+        final_cost->bpropUpdate( final_cost_input, target,
+                                 final_cost_value[0],
+                                 final_cost_gradient );
+        final_module->bpropUpdate( input_representation,
+                                   final_cost_input,
+                                   expectation_gradients[ n_layers-1 ],
+                                   final_cost_gradient );
+    }
 
     for( int i=n_layers-1 ; i>0 ; i-- )
     {
@@ -1000,20 +1114,29 @@
 
 void StackedFocusedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
 {
-    updateTrainSetRepresentations();
-
-    computeRepresentation(input,input_representation, 
-                          min(currently_trained_layer,n_layers-1));
-
-    computeNearestNeighbors(train_set_representations_vmat,input_representation,
-                            test_nearest_neighbors_indices);
-
-    test_votes.clear();
-    for(int i=0; i<test_nearest_neighbors_indices.length(); i++)
-        test_votes[train_set_targets[test_nearest_neighbors_indices[i]]]++;
-
-    output[0] = argmax(test_votes);
-
+    if( do_not_use_knn_classifier & currently_trained_layer>n_layers-1 )
+    {
+        computeRepresentation(input,input_representation, 
+                              min(currently_trained_layer,n_layers-1));
+        final_module->fprop( input_representation, final_cost_input );
+        output[0] = argmax(final_cost_input);
+    }
+    else
+    {
+        updateTrainSetRepresentations();
+        
+        computeRepresentation(input,input_representation, 
+                              min(currently_trained_layer,n_layers-1));
+        
+        computeNearestNeighbors(train_set_representations_vmat,input_representation,
+                                test_nearest_neighbors_indices);
+        
+        test_votes.clear();
+        for(int i=0; i<test_nearest_neighbors_indices.length(); i++)
+            test_votes[train_set_targets[test_nearest_neighbors_indices[i]]]++;
+        
+        output[0] = argmax(test_votes);
+    }
 }
 
 void StackedFocusedAutoassociatorsNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
@@ -1175,6 +1298,12 @@
         connections[i]->setLearningRate( the_learning_rate );
     }
     layers[n_layers-1]->setLearningRate( the_learning_rate );
+
+    if( do_not_use_knn_classifier )
+    {
+        final_module->setLearningRate( the_learning_rate );
+        final_cost->setLearningRate( the_learning_rate );
+    }
 }
 
 

Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-24 20:54:58 UTC (rev 8212)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-25 19:50:05 UTC (rev 8213)
@@ -42,13 +42,18 @@
 
 #include <plearn/vmat/ClassSubsetVMatrix.h>
 #include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/online/GradNNetLayerModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn_learners/online/CostModule.h>
+#include <plearn_learners/online/ModuleStackModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
+#include <plearn_learners/online/ClassErrorCostModule.h>
+#include <plearn_learners/online/CombiningCostsModule.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMMixedLayer.h>
 #include <plearn_learners/online/RBMConnection.h>
+#include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn/misc/PTimer.h>
 
 namespace PLearn {
@@ -119,6 +124,16 @@
     //! Number of classes
     int n_classes;
 
+    //! Use standard neural net architecture, not 
+    //! the nearest neighbor model.
+    bool do_not_use_knn_classifier;
+
+    //! Output weights l1_penalty_factor
+    real output_weights_l1_penalty_factor;
+
+    //! Output weights l2_penalty_factor
+    real output_weights_l2_penalty_factor;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -275,6 +290,13 @@
     //! Negative up statistic
     Vec neg_up_val;
 
+    //! Input of cost function
+    mutable Vec final_cost_input;
+    //! Cost value
+    mutable Vec final_cost_value;
+    //! Cost gradient on output layer
+    mutable Vec final_cost_gradient;
+
     //! Datasets for each class
     TVec< PP<ClassSubsetVMatrix> > class_datasets;
 
@@ -306,6 +328,12 @@
     //! n_layers means the output layer)
     int currently_trained_layer;
 
+    //! Output layer of neural net
+    PP<OnlineLearningModule> final_module;
+
+    //! Cost on output layer of neural net
+    PP<CostModule> final_cost;
+
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -320,7 +348,7 @@
 
     void build_layers_and_connections();
 
-    void build_classification_cost();
+    void build_output_layer_and_cost();
 
     void setLearningRate( real the_learning_rate );
 



From larocheh at mail.berlios.de  Fri Oct 26 16:12:45 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 26 Oct 2007 16:12:45 +0200
Subject: [Plearn-commits] r8214 - trunk/plearn_learners_experimental
Message-ID: <200710261412.l9QECjOp020003@sheep.berlios.de>

Author: larocheh
Date: 2007-10-26 16:12:45 +0200 (Fri, 26 Oct 2007)
New Revision: 8214

Modified:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
Log:
Degugging again...


Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-25 19:50:05 UTC (rev 8213)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-26 14:12:45 UTC (rev 8214)
@@ -318,6 +318,7 @@
     comb_costs->sub_costs.resize(2);
     comb_costs->sub_costs[0] = nll;
     comb_costs->sub_costs[1] = class_error;
+    comb_costs->build();
 
     final_cost = comb_costs;
     final_cost->forget();
@@ -891,22 +892,22 @@
                                  dissimilar_example_representation,
                                  2));
 
-    if( dist == 0 )
-        PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
-                  " example representation is exactly the sample as the"
-                  " input example. Gradient would be infinite! Skipping this"
-                  " example...");
-    else
-    {
-        substract(input_representation,dissimilar_example_representation,
-                  dissimilar_gradient_contribution);
-        
-        dissimilar_gradient_contribution *= -5.54*
-            safeexp(-2.77*dist/sqrt(layers[index+1]->size));
+    //if( dist == 0 )
+    //    PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
+    //              " example representation is exactly the sample as the"
+    //              " input example. Gradient would be infinite! Skipping this"
+    //              " example...");
+    //else
+    //{
+    substract(input_representation,dissimilar_example_representation,
+              dissimilar_gradient_contribution);
+    
+    dissimilar_gradient_contribution *= -5.54*
+        safeexp(-2.77*dist/sqrt(layers[index+1]->size));
+    
+    expectation_gradients[index+1] += dissimilar_gradient_contribution;
+        //}
 
-        expectation_gradients[index+1] += dissimilar_gradient_contribution;
-    }
-
     // RBM learning
     if( !fast_exact_is_equal( cd_learning_rate, 0 ) )
     {
@@ -1032,44 +1033,45 @@
     // Compute supervised gradient
 
 
-    if( do_not_use_knn_classifier )
+    if( !do_not_use_knn_classifier )
     {
         // Similar example contribution
-        substract(input_representation,similar_example_representation,
+        substract(previous_input_representation,similar_example_representation,
                   expectation_gradients[n_layers-1]);
         expectation_gradients[n_layers-1] *= 4/sqrt(layers[n_layers-1]->size);
     
         // Dissimilar example contribution
-        real dist = sqrt(powdistance(input_representation,
+        real dist = sqrt(powdistance(previous_input_representation,
                                      dissimilar_example_representation,
                                      2));
     
-        if( dist == 0 )
-            PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
-                      " example representation is exactly the sample as the"
-                      " input example. Gradient would be infinite! Skipping this"
-                      " example...");
-        else
-        {
+        //if( dist == 0 )
+        //    PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
+        //              " example representation is exactly the sample as the"
+        //              " input example. Gradient would be infinite! Skipping this"
+        //              " example...");
+        //else
+        //{
 
-            substract(input_representation,dissimilar_example_representation,
-                      dissimilar_gradient_contribution);
-
-            dissimilar_gradient_contribution *= -5.54*
-                safeexp(-2.77*dist/sqrt(layers[n_layers-1]->size));
+        substract(previous_input_representation,
+                  dissimilar_example_representation,
+                  dissimilar_gradient_contribution);
         
-            expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
-        }
+        dissimilar_gradient_contribution *= -5.54*
+            safeexp(-2.77*dist/sqrt(layers[n_layers-1]->size));
+        
+        expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
+        //}
     }
     else
     {
-        final_module->fprop( input_representation, final_cost_input );
+        final_module->fprop( previous_input_representation, final_cost_input );
         final_cost->fprop( final_cost_input, target, final_cost_value );
         
         final_cost->bpropUpdate( final_cost_input, target,
                                  final_cost_value[0],
                                  final_cost_gradient );
-        final_module->bpropUpdate( input_representation,
+        final_module->bpropUpdate( previous_input_representation,
                                    final_cost_input,
                                    expectation_gradients[ n_layers-1 ],
                                    final_cost_gradient );



From tihocan at mail.berlios.de  Fri Oct 26 16:47:09 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 26 Oct 2007 16:47:09 +0200
Subject: [Plearn-commits] r8215 - trunk/plearn_learners/distributions
Message-ID: <200710261447.l9QEl9wv021920@sheep.berlios.de>

Author: tihocan
Date: 2007-10-26 16:47:09 +0200 (Fri, 26 Oct 2007)
New Revision: 8215

Modified:
   trunk/plearn_learners/distributions/GaussMix.cc
   trunk/plearn_learners/distributions/GaussMix.h
Log:
- Removed unused parameters from method setPredictorPredictedSizes_const
- Fixed return value of setPredictorPredictedSizes when 'call_parent' is set to false to match the comments in PDistribution.h
- Added TODO comment about possible optimization


Modified: trunk/plearn_learners/distributions/GaussMix.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussMix.cc	2007-10-26 14:12:45 UTC (rev 8214)
+++ trunk/plearn_learners/distributions/GaussMix.cc	2007-10-26 14:47:09 UTC (rev 8215)
@@ -2831,7 +2831,7 @@
             // appropriate data elsewhere so that there is no need to recompute
             // it again.
             if (previous_predictor_part_had_missing)
-                setPredictorPredictedSizes_const(n_predictor, n_predicted);
+                setPredictorPredictedSizes_const();
 
             previous_predictor_part_had_missing = false;
             x_minus_mu_x.resize(n_predictor);
@@ -3086,18 +3086,18 @@
 bool GaussMix::setPredictorPredictedSizes(int n_i, int n_t,
                                    bool call_parent)
 {
-    bool sizes_changed = true;
+    bool sizes_changed = false;
     if (call_parent)
         sizes_changed =
             inherited::setPredictorPredictedSizes(n_i, n_t, call_parent);
-    setPredictorPredictedSizes_const(n_i, n_t);
+    setPredictorPredictedSizes_const();
     return sizes_changed;
 }
 
 //////////////////////////////////////
 // setPredictorPredictedSizes_const //
 //////////////////////////////////////
-void GaussMix::setPredictorPredictedSizes_const(int n_i, int n_t) const
+void GaussMix::setPredictorPredictedSizes_const() const
 {
     static Mat inv_cov_x;
     static Mat full_cov;
@@ -3211,6 +3211,11 @@
                 y_x_mat[j] << work_mat1;
             }
             // Compute SVD of the covariance of y|x.
+            // TODO Note that if n_predictor == 0 (e.g. when using the Manifold
+            // Parzen algorithm), the covariance of y|x is also the full
+            // covariance, and thus we should instead re-use directly the
+            // (possibly few) eigenvectors of the full covariance matrix
+            // instead of wasting time and memory in the computations below.
             eigenvectors_y_x[j].resize(n_predicted, n_predicted);
             eigenvals = eigenvalues_y_x(j);
             // Ensure covariance matrix is perfectly symmetric.

Modified: trunk/plearn_learners/distributions/GaussMix.h
===================================================================
--- trunk/plearn_learners/distributions/GaussMix.h	2007-10-26 14:12:45 UTC (rev 8214)
+++ trunk/plearn_learners/distributions/GaussMix.h	2007-10-26 14:47:09 UTC (rev 8215)
@@ -331,8 +331,9 @@
 
     //! Main implementation of 'setPredictorPredictedSizes', that needs to be
     //! 'const' as it currently needs to be called in setPredictor(..).
-    void setPredictorPredictedSizes_const(int the_predictor_size,
-                                          int the_predicted_size) const;
+    //! It does not take any parameter since it assumes the predictor and
+    //! predicted sizes have been set already.
+    void setPredictorPredictedSizes_const() const;
 
     //! Fill the 'initial_weights' vector with the weights from the given
     //! VMatrix (which must have a weights column).



From nouiz at mail.berlios.de  Fri Oct 26 18:10:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 26 Oct 2007 18:10:34 +0200
Subject: [Plearn-commits] r8216 - trunk/python_modules/plearn/parallel
Message-ID: <200710261610.l9QGAYib029314@sheep.berlios.de>

Author: nouiz
Date: 2007-10-26 18:10:33 +0200 (Fri, 26 Oct 2007)
New Revision: 8216

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
print only once the status of jobs


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-10-26 14:47:09 UTC (rev 8215)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-10-26 16:10:33 UTC (rev 8216)
@@ -264,9 +264,9 @@
                 waiting+=i
                 unfinished.append(t.id)
             else:
-                print "[DBI] jobs %i have a bad status: %d",t.id
-            print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, init: %d"%(len(self.tasks),finished, running, waiting, init)
-            print "[DBI] jobs unfinished (starting at 1): ",unfinished
+                print "[DBI] jobs %i have an unknow status: %d",t.id
+        print "[DBI] %d jobs. finished: %d, running: %d, waiting: %d, init: %d"%(len(self.tasks),finished, running, waiting, init)
+        print "[DBI] jobs unfinished (starting at 1): ",unfinished
 
 class Task:
 



From larocheh at mail.berlios.de  Fri Oct 26 20:57:25 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 26 Oct 2007 20:57:25 +0200
Subject: [Plearn-commits] r8217 - trunk/plearn_learners_experimental
Message-ID: <200710261857.l9QIvP1k023898@sheep.berlios.de>

Author: larocheh
Date: 2007-10-26 20:57:24 +0200 (Fri, 26 Oct 2007)
New Revision: 8217

Modified:
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
   trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
Log:
Added a precision factor for dissimilar examples...


Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-26 16:10:33 UTC (rev 8216)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.cc	2007-10-26 18:57:24 UTC (rev 8217)
@@ -67,6 +67,7 @@
     fine_tuning_decrease_ct( 0. ),
     k_neighbors( 1 ),
     n_classes( -1 ),
+    dissimilar_example_cost_precision(2.77), // Value taken from original paper
     do_not_use_knn_classifier(false),
     output_weights_l1_penalty_factor(0),
     output_weights_l2_penalty_factor(0),
@@ -177,6 +178,11 @@
                   OptionBase::buildoption,
                   "Number of classes.");
 
+    declareOption(ol, "dissimilar_example_cost_precision", 
+                  &StackedFocusedAutoassociatorsNet::dissimilar_example_cost_precision,
+                  OptionBase::buildoption,
+                  "Parameter that constrols the importance of the dissimilar example cost.");
+
     declareOption(ol, "do_not_use_knn_classifier", 
                   &StackedFocusedAutoassociatorsNet::do_not_use_knn_classifier,
                   OptionBase::buildoption,
@@ -636,7 +642,7 @@
     Vec target2( targetsize() );
     real weight; // unused
     real weight2; // unused
-
+    
     Vec similar_example_index(1);
 
     TVec<string> train_cost_names = getTrainCostNames() ;
@@ -806,11 +812,20 @@
             if( pb )
                 pb->update( stage - init_stage + 1 );
         }
+
+        if(verbosity>2)
+        {
+            Vec train_stats_vec = train_stats->getMean();
+            cout << "similarity_cost = " << train_stats_vec[train_stats_vec.length()-3] << endl;
+            cout << "dissimilarity_cost = " << train_stats_vec[train_stats_vec.length()-2] << endl;
+            cout << "metric_cost = " << train_stats_vec[train_stats_vec.length()-1] << endl;
+        }
     }
     
     train_stats->finalize();
     MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
 
+
     // Update currently_trained_layer
     if(stage > 0)
         currently_trained_layer = n_layers;
@@ -902,8 +917,8 @@
     substract(input_representation,dissimilar_example_representation,
               dissimilar_gradient_contribution);
     
-    dissimilar_gradient_contribution *= -5.54*
-        safeexp(-2.77*dist/sqrt(layers[index+1]->size));
+    dissimilar_gradient_contribution *= -2* dissimilar_example_cost_precision*
+        safeexp(-dissimilar_example_cost_precision*dist/sqrt(layers[index+1]->size));
     
     expectation_gradients[index+1] += dissimilar_gradient_contribution;
         //}
@@ -1040,11 +1055,21 @@
                   expectation_gradients[n_layers-1]);
         expectation_gradients[n_layers-1] *= 4/sqrt(layers[n_layers-1]->size);
     
+        train_costs[train_costs.length()-3] = 
+            2 * sqrt(powdistance(previous_input_representation,
+                                 similar_example_representation,
+                                 2)) / sqrt(layers[n_layers-1]->size);
+        
         // Dissimilar example contribution
         real dist = sqrt(powdistance(previous_input_representation,
                                      dissimilar_example_representation,
                                      2));
-    
+
+        train_costs[train_costs.length()-2] = 
+            2 * sqrt(layers[n_layers-1]->size) * safeexp( -dissimilar_example_cost_precision
+                                                          *dist/sqrt(layers[n_layers-1]->size));
+        train_costs.last() = train_costs[train_costs.length()-3] + 
+            train_costs[train_costs.length()-2];
         //if( dist == 0 )
         //    PLWARNING("StackedFocusedAutoassociatorsNet::fineTuningStep(): dissimilar"
         //              " example representation is exactly the sample as the"
@@ -1057,8 +1082,8 @@
                   dissimilar_example_representation,
                   dissimilar_gradient_contribution);
         
-        dissimilar_gradient_contribution *= -5.54*
-            safeexp(-2.77*dist/sqrt(layers[n_layers-1]->size));
+        dissimilar_gradient_contribution *= -2 * dissimilar_example_cost_precision*
+            safeexp(-dissimilar_example_cost_precision*dist/sqrt(layers[n_layers-1]->size));
         
         expectation_gradients[n_layers-1] += dissimilar_gradient_contribution;
         //}
@@ -1230,7 +1255,11 @@
 
 TVec<string> StackedFocusedAutoassociatorsNet::getTrainCostNames() const
 {
-    return getTestCostNames() ;    
+    TVec<string> cost_names = getTestCostNames();
+    cost_names.push_back("similarity_cost");
+    cost_names.push_back("dissimilarity_cost");
+    cost_names.push_back("metric_cost");
+    return cost_names;    
 }
 
 void StackedFocusedAutoassociatorsNet::setTrainingSet(VMat training_set, bool call_forget)

Modified: trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-26 16:10:33 UTC (rev 8216)
+++ trunk/plearn_learners_experimental/StackedFocusedAutoassociatorsNet.h	2007-10-26 18:57:24 UTC (rev 8217)
@@ -124,6 +124,9 @@
     //! Number of classes
     int n_classes;
 
+    //! Parameter that constrols the importance of the dissimilar example cost
+    real dissimilar_example_cost_precision;
+
     //! Use standard neural net architecture, not 
     //! the nearest neighbor model.
     bool do_not_use_knn_classifier;



From larocheh at mail.berlios.de  Sat Oct 27 23:04:18 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sat, 27 Oct 2007 23:04:18 +0200
Subject: [Plearn-commits] r8218 - trunk/plearn_learners/online
Message-ID: <200710272104.l9RL4IrG027738@sheep.berlios.de>

Author: larocheh
Date: 2007-10-27 23:04:18 +0200 (Sat, 27 Oct 2007)
New Revision: 8218

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Added option to add noise to inputs of autoassociators...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-26 18:57:24 UTC (rev 8217)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-10-27 21:04:18 UTC (rev 8218)
@@ -62,6 +62,7 @@
     online( false ),
     compute_all_test_costs( false ),
     reconstruct_hidden( false ),
+    fraction_of_masked_inputs( 0 ),
     n_layers( 0 ),
     currently_trained_layer( 0 )
 {
@@ -202,6 +203,13 @@
                   "reconstruct their hidden layers (inspired from CD1 in an RBM).\n"
         );
 
+    declareOption(ol, "fraction_of_masked_inputs", 
+                  &StackedAutoassociatorsNet::fraction_of_masked_inputs,
+                  OptionBase::buildoption,
+                  "Random fraction of the autoassociators' input components that\n"
+                  "masked, i.e. unsused to reconstruct the input.\n"
+        );
+
     declareOption(ol, "greedy_stages", 
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -270,6 +278,16 @@
                     " - \n"
                     "cannot use online setting with reconstruct_hidden=true.\n");
 
+        if( fraction_of_masked_inputs < 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "fraction_of_masked_inputs should be > or equal to 0.\n");
+
+        if( online && fraction_of_masked_inputs > 0)
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "masked inputs has not been implemented for online option.\n");
+
         if( !online )
         {
             if( greedy_stages.length() == 0)
@@ -612,6 +630,7 @@
     deepCopyField(final_cost_input, copies);
     deepCopyField(final_cost_value, copies);
     deepCopyField(final_cost_gradient, copies);
+    deepCopyField(masked_autoassociator_input, copies);
     deepCopyField(greedy_stages, copies);
 }
 
@@ -746,6 +765,10 @@
             reconstruction_activations.resize(layers[i]->size);
             reconstruction_activation_gradients.resize(layers[i]->size);
             reconstruction_expectation_gradients.resize(layers[i]->size);
+            masked_autoassociator_input.resize(layers[i]->size);
+            autoassociator_input_indices.resize(layers[i]->size);
+            for( int j=0 ; j < autoassociator_input_indices.length() ; j++ )
+                autoassociator_input_indices[j] = j;
 
             if(reconstruct_hidden)
             {
@@ -891,12 +914,23 @@
 {
     PLASSERT( index < n_layers );
 
+    if( fraction_of_masked_inputs > 0 )
+        random_gen->shuffleElements(autoassociator_input_indices);
+
     expectations[0] << input;
     if(correlation_connections.length() != 0)
     {
         for( int i=0 ; i<index + 1; i++ )
         {
-            connections[i]->fprop( expectations[i], correlation_activations[i] );
+            if( i == index && fraction_of_masked_inputs > 0 )
+            {
+                masked_autoassociator_input << expectations[i];
+                for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
+                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+                connections[i]->fprop( masked_autoassociator_input, correlation_activations[i] );
+            }
+            else
+                connections[i]->fprop( expectations[i], correlation_activations[i] );
             layers[i+1]->fprop( correlation_activations[i],
                                 correlation_expectations[i] );
             correlation_connections[i]->fprop( correlation_expectations[i], 
@@ -909,11 +943,20 @@
     {
         for( int i=0 ; i<index + 1; i++ )
         {
-            connections[i]->fprop( expectations[i], activations[i+1] );
+            if( i == index && fraction_of_masked_inputs > 0 )
+            {
+                masked_autoassociator_input << expectations[i];
+                for( int j=0 ; j < round(fraction_of_masked_inputs*layers[index]->size) ; j++)
+                    masked_autoassociator_input[ autoassociator_input_indices[j] ] = 0; 
+                connections[i]->fprop( masked_autoassociator_input, activations[i+1] );
+            }
+            else
+                connections[i]->fprop( expectations[i], activations[i+1] );
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
     }
 
+
     if( partial_costs && partial_costs[ index ] )
     {
         partial_costs[ index ]->fprop( expectations[ index + 1],
@@ -937,7 +980,13 @@
                                         activation_gradients[ index + 1 ],
                                         expectation_gradients[ index + 1 ] );
 
-        connections[ index ]->bpropUpdate( expectations[ index ],
+        if( fraction_of_masked_inputs > 0 )
+            connections[ index ]->bpropUpdate( masked_autoassociator_input,
+                                               activations[ index + 1 ],
+                                               expectation_gradients[ index ],
+                                               activation_gradients[ index + 1 ] );
+        else
+            connections[ index ]->bpropUpdate( expectations[ index ],
                                            activations[ index + 1 ],
                                            expectation_gradients[ index ],
                                            activation_gradients[ index + 1 ] );
@@ -947,8 +996,12 @@
                                                 reconstruction_activations);
     if(direct_connections.length() != 0)
     {
-        direct_connections[ index ]->fprop( expectations[ index ], 
-                                            direct_activations );
+        if( fraction_of_masked_inputs > 0 )
+            direct_connections[ index ]->fprop( masked_autoassociator_input, 
+                                                direct_activations );
+        else
+            direct_connections[ index ]->fprop( expectations[ index ], 
+                                                direct_activations );            
         direct_and_reconstruction_activations.clear();
         direct_and_reconstruction_activations += direct_activations;
         direct_and_reconstruction_activations += reconstruction_activations;
@@ -957,7 +1010,8 @@
                                 layers[ index ]->expectation);
         
         layers[ index ]->activation << direct_and_reconstruction_activations;
-        layers[ index ]->expectation_is_up_to_date = true;
+        //layers[ index ]->expectation_is_up_to_date = true;  // Won't work for certain RBMLayers
+        layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
         train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
         
         layers[ index ]->bpropNLL(expectations[index], train_costs[index],
@@ -965,12 +1019,19 @@
 
         layers[ index ]->update(direct_and_reconstruction_activation_gradients);
 
-        direct_connections[ index ]->bpropUpdate( 
-            expectations[ index ],
-            direct_activations,
-            reconstruction_expectation_gradients, // Will be overwritten later
-            direct_and_reconstruction_activation_gradients);
-        
+        if( fraction_of_masked_inputs > 0 )
+            direct_connections[ index ]->bpropUpdate( 
+                masked_autoassociator_input,
+                direct_activations,
+                reconstruction_expectation_gradients, // Will be overwritten later
+                direct_and_reconstruction_activation_gradients);
+        else
+            direct_connections[ index ]->bpropUpdate( 
+                expectations[ index ],
+                direct_activations,
+                reconstruction_expectation_gradients, // Will be overwritten later
+                direct_and_reconstruction_activation_gradients);
+            
         reconstruction_connections[ index ]->bpropUpdate( 
             expectations[ index + 1], 
             reconstruction_activations, 
@@ -983,7 +1044,8 @@
                                 layers[ index ]->expectation);
         
         layers[ index ]->activation << reconstruction_activations;
-        layers[ index ]->expectation_is_up_to_date = true;
+        //layers[ index ]->expectation_is_up_to_date = true;
+        layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
         real rec_err = layers[ index ]->fpropNLL(expectations[index]);
         train_costs[index] = rec_err;
 
@@ -997,7 +1059,8 @@
             layers[ index+1 ]->fprop( hidden_reconstruction_activations,
                 layers[ index+1 ]->expectation );
             layers[ index+1 ]->activation << hidden_reconstruction_activations;
-            layers[ index+1 ]->expectation_is_up_to_date = true;
+            //layers[ index+1 ]->expectation_is_up_to_date = true;
+            layers[ index+1 ]->setExpectationByRef( layers[ index+1 ]->expectation );
             real hid_rec_err = layers[ index+1 ]->fpropNLL(expectations[index+1]);
             train_costs[index] += hid_rec_err;
 
@@ -1078,11 +1141,18 @@
             correlation_activation_gradients [ index ],
             correlation_expectation_gradients [ index ]);    
         
-        connections[ index ]->bpropUpdate( 
-            expectations[ index ],
-            correlation_activations[ index ],
-            reconstruction_expectation_gradients, //reused
-            correlation_activation_gradients [ index ]);
+        if( fraction_of_masked_inputs > 0 )
+            connections[ index ]->bpropUpdate( 
+                masked_autoassociator_input,
+                correlation_activations[ index ],
+                reconstruction_expectation_gradients, //reused
+                correlation_activation_gradients [ index ]);
+        else
+            connections[ index ]->bpropUpdate( 
+                expectations[ index ],
+                correlation_activations[ index ],
+                reconstruction_expectation_gradients, //reused
+                correlation_activation_gradients [ index ]);
     }
     else
     {
@@ -1092,11 +1162,18 @@
                                         reconstruction_activation_gradients, 
                                         reconstruction_expectation_gradients);    
         
-        connections[ index ]->bpropUpdate( 
-            expectations[ index ],
-            activations[ index + 1 ],
-            reconstruction_expectation_gradients, //reused
-            reconstruction_activation_gradients);
+        if( fraction_of_masked_inputs > 0 )
+            connections[ index ]->bpropUpdate( 
+                masked_autoassociator_input,
+                activations[ index + 1 ],
+                reconstruction_expectation_gradients, //reused
+                reconstruction_activation_gradients);
+        else
+            connections[ index ]->bpropUpdate( 
+                expectations[ index ],
+                activations[ index + 1 ],
+                reconstruction_expectation_gradients, //reused
+                reconstruction_activation_gradients);
     }
 
 }
@@ -1319,8 +1396,9 @@
                                 layers[ i-1 ]->expectation);
         
         layers[ i-1 ]->activation << reconstruction_activations;
-        layers[ i-1 ]->expectation_is_up_to_date = true;
-        real rec_err = layers[ i-1 ]->fpropNLL(expectations[i-1]);
+        //layers[ i-1 ]->expectation_is_up_to_date = true;
+        layers[ i-1 ]->setExpectationByRef( layers[ i-1 ]->expectation );
+        real rec_err = layers[ i-1 ]->fpropNLL( expectations[i-1] );
         train_costs[i-1] = rec_err;
 
         layers[ i-1 ]->bpropNLL(expectations[i-1], rec_err,
@@ -1548,7 +1626,8 @@
                                 layers[ i ]->expectation);
             
             layers[ i ]->activation << reconstruction_activations;
-            layers[ i ]->expectation_is_up_to_date = true;
+            //layers[ i ]->expectation_is_up_to_date = true;
+            layers[ i ]->setExpectationByRef( layers[ i ]->expectation );
 
             costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
 
@@ -1581,7 +1660,9 @@
         
         layers[ currently_trained_layer-1 ]->activation << 
             reconstruction_activations;
-        layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        //layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
+        layers[ currently_trained_layer-1 ]->setExpectationByRef( 
+            layers[ currently_trained_layer-1 ]->expectation );
         costs[ currently_trained_layer-1 ] = 
             layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
@@ -1596,7 +1677,9 @@
                 layers[ currently_trained_layer ]->expectation );
             layers[ currently_trained_layer ]->activation << 
                 hidden_reconstruction_activations;
-            layers[ currently_trained_layer ]->expectation_is_up_to_date = true;
+            //layers[ currently_trained_layer ]->expectation_is_up_to_date = true;
+            layers[ currently_trained_layer ]->setExpectationByRef( 
+                layers[ currently_trained_layer ]->expectation );
             costs[ currently_trained_layer-1 ] += 
                 layers[ currently_trained_layer ]->fpropNLL(
                     output);

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-10-26 18:57:24 UTC (rev 8217)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-10-27 21:04:18 UTC (rev 8218)
@@ -146,6 +146,10 @@
     //! reconstruct their hidden layers (inspired from CD1 in an RBM)
     bool reconstruct_hidden;
 
+    //! Random fraction of the autoassociators' input components that
+    //! masked, i.e. unsused to reconstruct the input.
+    real fraction_of_masked_inputs;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -297,6 +301,13 @@
     //! Stores the gradient of the cost at the input of final_cost
     mutable Vec final_cost_gradient;
 
+    //! Input of autoassociator where some of the components
+    //! have been masked (set to 0) randomly.
+    Vec masked_autoassociator_input;
+
+    //! Indices of the input components
+    TVec<int> autoassociator_input_indices;
+
     //! Stages of the different greedy phases
     TVec<int> greedy_stages;
 



From tihocan at mail.berlios.de  Mon Oct 29 16:18:44 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 29 Oct 2007 16:18:44 +0100
Subject: [Plearn-commits] r8219 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710291518.l9TFIir4023253@sheep.berlios.de>

Author: tihocan
Date: 2007-10-29 16:18:43 +0100 (Mon, 29 Oct 2007)
New Revision: 8219

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Fixed zombie processes

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-10-27 21:04:18 UTC (rev 8218)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-10-29 15:18:43 UTC (rev 8219)
@@ -38,6 +38,7 @@
 
 
 #include "NatGradSMPNNet.h"
+#include <plearn/io/openFile.h>
 #include <plearn/math/pl_erf.h>
 
 #include <sys/ipc.h>
@@ -711,9 +712,24 @@
 ///////////
 void NatGradSMPNNet::train()
 {
+    static int log_idx = -1;
+    log_idx = (log_idx + 1) % 50;
 
-    if (inputsize_<0)
+    /*
+    PStream tmp_log = openFile("/u/delallea/tmp/tmp_log" + tostring(log_idx),
+                               PStream::raw_ascii, "w");
+
+    tmp_log << "Starting train " << endl;
+    tmp_log.flush();
+    */
+
+    if (inputsize_<0) {
+        /*
+        tmp_log << "Calling build" << endl;
+        tmp_log.flush();
+        */
         build();
+    }
 
     targets.resize(minibatch_size,targetsize());  // the train_set's targetsize()
 
@@ -729,6 +745,9 @@
 
     PP<ProgressBar> pb;
 
+    //tmp_log << "Beginning stuff done" << endl;
+    //tmp_log.flush();
+
     Profiler::reset("training");
     Profiler::start("training");
     Profiler::pl_profile_start("Totaltraining");
@@ -784,6 +803,13 @@
     int stage_idx = 0;
     params_int_ptr[stage_idx] = stage;
 
+    //tmp_log << "Ready to fork" << endl;
+    //tmp_log.flush();
+
+    // No need to call wait() to acknowledge the death of a child process in
+    // order to avoid defunct processes.
+    signal(SIGCLD, SIG_IGN);
+
     // Fork one process/cpu.
     int iam = 0;
     for (int cpu = 1; cpu < ncpus ; cpu++)
@@ -792,6 +818,11 @@
             break;
         }
 
+    if (!iam) {
+        //tmp_log << "Forked" << endl;
+        //tmp_log.flush();
+    }
+
     // Each processor computes gradient over its own subset of samples (between
     // indices 'start' and 'start + my_n_samples' in the training set).
     int n_left = nsamples % ncpus;
@@ -822,6 +853,11 @@
     int my_stage_incr = iam >= stage_incr_left ? stage_incr_per_cpu
                                                : stage_incr_per_cpu + 1;
 
+    if (iam == 0) {
+        //tmp_log << "Starting loop" << endl;
+        //tmp_log.flush();
+    }
+
     for(int i = 0; i < my_stage_incr; i++)
     {
         int sample = start + i % my_n_samples;
@@ -895,6 +931,11 @@
         */
     }
 
+    if (iam == 0) {
+        //tmp_log << "Loop ended" << endl;
+        //tmp_log.flush();
+    }
+
     if (!wait_for_final_update) {
         if (nsteps >  0) {
             //printf("CPU %d final updating (nsteps =%d)\n", iam, nsteps);
@@ -920,6 +961,9 @@
     Profiler::reset("Synchronization");
     Profiler::start("Synchronization");
 
+    //tmp_log << "Synchronization" << endl;
+    //tmp_log.flush();
+
     // Wait until it is our turn.
     while (true) {
         int sem_value = semctl(semaphore_id, 0, GETVAL);
@@ -977,6 +1021,8 @@
         }
     }
 
+    //tmp_log << "Synchronized" << endl;
+    //tmp_log.flush();
     Profiler::end("Synchronization");
     /*
     const Profiler::Stats& synch_stats = Profiler::getStats("Synchronization");
@@ -998,6 +1044,9 @@
         semaphore_id = -1;
     }
 
+    //tmp_log << "Finishing stuff" << endl;
+    //tmp_log.flush();
+
     // Update the learner's stage.
     stage = nstages;
     if (stage != cur_stage)
@@ -1018,6 +1067,9 @@
     train_stats->update( costs_plus_time );
     train_stats->finalize(); // finalize statistics for this epoch
 
+    //tmp_log << "Done!" << endl;
+    //tmp_log.flush();
+
     // profiling gradient correlation
     //if( g_corrprof )    {
     //    PLASSERT( corr_profiling_end <= nstages );
@@ -1252,11 +1304,23 @@
 
 void NatGradSMPNNet::computeOutput(const Vec& input, Vec& output) const
 {
+    /*
+    static int out_idx = -1;
+    out_idx = (out_idx + 1) % 50;
+    PStream out_log_file = openFile("/u/delallea/tmp/out_log_" +
+            tostring(out_idx), PStream::raw_ascii, "w");
+    out_log_file << "Starting to compute output on " << input << endl;
+    out_log_file.flush();
+    */
     Profiler::pl_profile_start("computeOutput");
     neuron_outputs_per_layer[0](0) << input;
     fpropNet(1,false);
     output << neuron_outputs_per_layer[n_layers-1](0);
     Profiler::pl_profile_end("computeOutput");
+    /*
+    out_log_file << "Output computed" << endl;
+    out_log_file.flush();
+    */
 }
 
 //! compute (pre-final-non-linearity) network top-layer output given input



From tihocan at mail.berlios.de  Mon Oct 29 18:39:04 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 29 Oct 2007 18:39:04 +0100
Subject: [Plearn-commits] r8220 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200710291739.l9THd4of027158@sheep.berlios.de>

Author: tihocan
Date: 2007-10-29 18:38:58 +0100 (Mon, 29 Oct 2007)
New Revision: 8220

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
- Added a new option 'synchronize_update' which is a variant where CPUs are synchronized after each mini-batch
- Made sure parameters updates are filled to 0 at build time (though this was probably the case already)


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-10-29 15:18:43 UTC (rev 8219)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-10-29 17:38:58 UTC (rev 8220)
@@ -73,6 +73,7 @@
 NatGradSMPNNet::NatGradSMPNNet():
       delayed_update(true),
       wait_for_final_update(true),
+      synchronize_update(false),
       noutputs(-1),
       params_averaging_coeff(1.0),
       params_averaging_freq(5),
@@ -129,6 +130,14 @@
         "update. It should impact performance only when 'delayed_update' is\n"
         "also true.");
 
+    declareOption(ol, "synchronize_update", &NatGradSMPNNet::synchronize_update,
+                  OptionBase::buildoption,
+        "If true, then processors will in turn update the shared paremeters\n"
+        "after each mini-batch and will wait until all processors did their\n"
+        "update before processing the next mini-batch. Otherwise, no\n"
+        "synchronization is performed and a processor may process multiple\n"
+        "mini-batches before doing a parameter update.");
+
     declareOption(ol, "noutputs", &NatGradSMPNNet::noutputs,
                   OptionBase::buildoption,
                   "Number of outputs of the neural network, which can be derived from  output_type and targetsize_\n");
@@ -458,6 +467,7 @@
     all_params_gradient.resize(n_params);
     all_params_delta.resize(n_params);
     params_update.resize(n_params);
+    params_update.fill(0);
 
     // depending on how parameters are grouped on the first layer
     int n_groups = params_natgrad_per_input_template ? (n_neurons-layer_sizes[1]+layer_sizes[0]+1) : n_neurons;
@@ -579,6 +589,9 @@
     //    ng_corrprof->build();
     //}
 
+    if (synchronize_update && !delayed_update)
+        PLERROR("NatGradSMPNNet::build_ - 'synchronize_update' cannot be used "
+                "when 'delayed_update' is false");
 }
 
 ///////////
@@ -777,22 +790,26 @@
                     "semaphore (errno = %d)", errno);
         semaphore_id = -1;
     }
-    // The semaphore has 'ncpus' + 1 values.
+    // The semaphore has 'ncpus' + 2 values.
     // The first one is the index of the CPU that will be next to update
     // weights.
     // The other ones are 0/1 values that are initialized with 0, and take 1
     // once the corresponding CPU has finished all updates for this training
     // period.
-    // Finally, the last value is the current stage, i.e. the number of samples
-    // with which the network has been updated so far.
-    semaphore_id = semget(IPC_PRIVATE, ncpus + 1, 0666 | IPC_CREAT);
+    // Finally, the last value is 0 when 'synchronize_update' is false, and
+    // otherwise is:
+    // - in a first step, the number of CPUs that have finished performing
+    // their mini-batch computation,
+    // - in a second step, the number of CPUs that have finished updating the
+    // shared parameters.
+    semaphore_id = semget(IPC_PRIVATE, ncpus + 2, 0666 | IPC_CREAT);
     if (semaphore_id == -1)
         PLERROR("In NatGradSMPNNet::train - Could not create semaphore "
                 "(errno = %d)", errno);
     // Initialize all values in the semaphore to zero.
     semun semun_v;
     semun_v.val = 0;
-    for (int i = 0; i < ncpus + 1; i++) {
+    for (int i = 0; i < ncpus + 2; i++) {
         int success = semctl(semaphore_id, i, SETVAL, semun_v);
         if (success != 0)
             PLERROR("In NatGradSMPNNet::train - Could not initialize semaphore"
@@ -876,7 +893,17 @@
             // Note that we should actually call onlineStep only on the subset
             // of samples that are new (compared to the previous mini-batch).
             // This is left as a TODO since it is not a priority.
+            /*
+            string samples_str = tostring(samples);
+            printf("CPU %d computing (cur_stage = %d) on samples: %s\n",
+                    iam, cur_stage, samples_str.c_str());
+                    */
             onlineStep(cur_stage, targets, train_costs, example_weights );
+            /*
+            sleep(iam);
+            string update = tostring(params_update);
+            printf("\nCPU %d's current update: %s\n", iam, update.c_str());
+            */
             nsteps += b + 1;
             /*
             for (int i=0;i<minibatch_size;i++)
@@ -886,18 +913,35 @@
             }
             */
             // Update weights if it is this cpu's turn.
+            bool performed_update = false; // True when this CPU has updated.
+            while (true) {
             int sem_value = semctl(semaphore_id, 0, GETVAL);
             if (sem_value == iam) {
-                //printf("CPU %d updating (nsteps =%d)\n", iam, nsteps);
-                if (delayed_update) {
+                int n_ready = 2 * ncpus;
+                if (synchronize_update && !performed_update) {
+                    // We first indicate that this CPU is ready to perform his
+                    // update.
+                    n_ready = semctl(semaphore_id, ncpus + 1, GETVAL);
+                    n_ready++;
+                    semun_v.val = n_ready;
+                    int success = semctl(semaphore_id, ncpus + 1, SETVAL,
+                                         semun_v);
+                    PLCHECK( success == 0 );
+                }
+                if (delayed_update && n_ready > ncpus && !performed_update) {
+                    //printf("CPU %d updating (nsteps = %d)\n", iam, nsteps);
                     all_params += params_update;
                     params_update.clear();
+                    performed_update = true;
                 }
-                // Update the current stage.
-                cur_stage = params_int_ptr[stage_idx];
-                PLASSERT( cur_stage >= 0 );
-                int new_stage = cur_stage + nsteps;
-                params_int_ptr[stage_idx] = new_stage;
+                if (nsteps > 0) {
+                    // Update the current stage.
+                    cur_stage = params_int_ptr[stage_idx];
+                    PLASSERT( cur_stage >= 0 );
+                    int new_stage = cur_stage + nsteps;
+                    params_int_ptr[stage_idx] = new_stage;
+                    nsteps = 0;
+                }
                 // Give update token to next CPU.
                 sem_value = (sem_value + 1) % ncpus;
                 semun_v.val = sem_value;
@@ -907,14 +951,20 @@
                             "semaphore with next CPU (errno = %d, returned "
                             "value = %d, set value = %d)", errno, success,
                             semun_v.val);
-                nsteps = 0;
-            } else {
-#if 0
-                printf("CPU %d NOT updating (sem_value = %d)\n",
-                        iam, sem_value);
-#endif
+                if (!delayed_update || n_ready >= 2 * ncpus)
+                    // If 'synchronize_update' is false this is always true.
+                    // If 'synchronize_update' is true this means all CPUs have
+                    // updated the parameters.
+                    break;
             }
+            }
         }
+        if (synchronize_update && iam == 0) {
+            // Reset the 'ready' semaphore.
+            semun_v.val = 0;
+            int success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
+            PLCHECK( success == 0 );
+        }
         /*
         if (params_averaging_coeff!=1.0 && 
             b==minibatch_size-1 && 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-10-29 15:18:43 UTC (rev 8219)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-10-29 17:38:58 UTC (rev 8220)
@@ -59,6 +59,7 @@
 
     bool delayed_update;
     bool wait_for_final_update;
+    bool synchronize_update;
 
     int noutputs;
 



From nouiz at mail.berlios.de  Tue Oct 30 21:28:20 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 30 Oct 2007 21:28:20 +0100
Subject: [Plearn-commits] r8221 - trunk/plearn/vmat
Message-ID: <200710302028.l9UKSKeK030976@sheep.berlios.de>

Author: nouiz
Date: 2007-10-30 21:28:20 +0100 (Tue, 30 Oct 2007)
New Revision: 8221

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
in PLearn::VMatrix::printFieldInfo() print more information
in PLearn::VMatrix::getSavedFieldInfos() print more info in PLERROR


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-10-29 17:38:58 UTC (rev 8220)
+++ trunk/plearn/vmat/VMatrix.cc	2007-10-30 20:28:20 UTC (rev 8221)
@@ -450,6 +450,24 @@
         PLERROR("Can't write name of type");
     }
 
+    map<real,StatsCollectorCounts>::const_iterator it = s.counts.begin();
+    map<real,StatsCollectorCounts>::const_iterator countsend = s.counts.end();
+    int n_values = 0;
+    //some value(FLT_MAX, meaby others) are used for others purpose.
+    //We must not cont then.
+    while(it!=countsend)
+    {
+        real val = it->first;
+        const StatsCollectorCounts& co = it->second;
+        string str = getValString(fieldnum, val);
+        if(co.n>0)
+            n_values++;
+        ++it;
+        }
+    char plus = ' ';
+    if (n_values==s.maxnvalues)
+        plus = '+';
+
     out << "nmissing: " << s.nmissing() << '\n';
     out << "nnonmissing: " << s.nnonmissing() << '\n';
     out << "sum: " << s.sum() << '\n';
@@ -457,6 +475,7 @@
     out << "stddev: " << s.stddev() << '\n';
     out << "min: " << s.min() << '\n';
     out << "max: " << s.max() << '\n';
+    out << "ndiffvalue: " << n_values << plus << '\n';
 
     if(!s.counts.empty() && print_binning)
     {
@@ -722,13 +741,15 @@
     Array<VMField> current_fieldinfos(w);
     for(int i=0; i<w; ++i)
     {
-        vector<string> v(split(in.getline()));
+        string line = in.getline();
+        vector<string> v(split(line));
         switch(v.size())
         {
         case 1: current_fieldinfos[i] = VMField(v[0]); break;
         case 2: current_fieldinfos[i] = VMField(v[0], VMField::FieldType(toint(v[1]))); break;
         default: PLERROR("In VMatrix::getSavedFieldInfos Format not recognized in file %s.\n"
-                         "Each line should be '<name> {<type>}'.", filename.absolute().c_str());
+                         "Each line should be '<name> {<type>}'.\n"
+                         "Got: '%s'",filename.absolute().c_str(),line.c_str());
         }
     }
     return current_fieldinfos;



From nouiz at mail.berlios.de  Tue Oct 30 21:30:03 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 30 Oct 2007 21:30:03 +0100
Subject: [Plearn-commits] r8222 - trunk/plearn_learners/regressors
Message-ID: <200710302030.l9UKU34u031366@sheep.berlios.de>

Author: nouiz
Date: 2007-10-30 21:30:02 +0100 (Tue, 30 Oct 2007)
New Revision: 8222

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
removed space from costs name as pmat file can't have field name with space


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2007-10-30 20:28:20 UTC (rev 8221)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2007-10-30 20:30:02 UTC (rev 8222)
@@ -306,9 +306,9 @@
 {
     TVec<string> return_msg(4);
     return_msg[0] = "mse";
-    return_msg[1] = "base confidence";
-    return_msg[2] = "base reward - l2";
-    return_msg[3] = "base reward - l1";
+    return_msg[1] = "base_confidence";
+    return_msg[2] = "base_reward_l2";
+    return_msg[3] = "base_reward_l1";
     return return_msg;
 }
 



