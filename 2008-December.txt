From nouiz at mail.berlios.de  Mon Dec  1 16:09:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 1 Dec 2008 16:09:39 +0100
Subject: [Plearn-commits] r9729 - trunk/plearn/vmat
Message-ID: <200812011509.mB1F9dNe023822@sheep.berlios.de>

Author: nouiz
Date: 2008-12-01 16:09:39 +0100 (Mon, 01 Dec 2008)
New Revision: 9729

Modified:
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
Log:
added instruction zero_or_neg_is_missing


Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-11-27 16:32:22 UTC (rev 9728)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-12-01 15:09:39 UTC (rev 9729)
@@ -77,6 +77,11 @@
             if (tmp2[col] == 0.0) v[merge_col] = MISSING_VALUE;
             else v[merge_col] = tmp2[col];
         }
+        else if (ins[col] == "zero_or_neg_is_missing")
+        {
+            if (tmp2[col] <= 0.0) v[merge_col] = MISSING_VALUE;
+            else v[merge_col] = tmp2[col];
+        }
         else if (ins[col] == "2436935_is_missing")
         {
             if (tmp2[col] == 2436935.0) v[merge_col] = MISSING_VALUE;
@@ -112,7 +117,7 @@
     declareOption(ol, "missing_instructions", &MissingInstructionVMatrix::missing_instructions,
                   OptionBase::buildoption,
                   "The variable missing regeneration instructions in the form of pairs field : instruction.\n"
-                  "Supported instructions are skip, as_is, zero_is_missing, 2436935_is_missing(01JAN1960 in julian day), present.\n"
+                  "Supported instructions are skip, as_is, zero_is_missing, zero_or_neg_is_missing, 2436935_is_missing(01JAN1960 in julian day), present.\n"
                   "If the instruction fieldname end with '*' we will extend it to all the source matrix fieldname that match the regex.\n"
                   "No other regex are supported");
     declareOption(ol, "default_instruction",
@@ -198,6 +203,8 @@
             ins[source_col] = "as_is";
         else if (missing_instructions[ins_col].second == "zero_is_missing")
             ins[source_col] = "zero_is_missing";
+        else if (missing_instructions[ins_col].second == "zero_or_neg_is_missing")
+            ins[source_col] = "zero_or_neg_is_missing";
         else if (missing_instructions[ins_col].second == "2436935_is_missing")
             ins[source_col] = "2436935_is_missing";
         else if (missing_instructions[ins_col].second == "present")



From nouiz at mail.berlios.de  Mon Dec  1 16:13:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 1 Dec 2008 16:13:14 +0100
Subject: [Plearn-commits] r9730 - trunk/plearn/vmat
Message-ID: <200812011513.mB1FDE0s024289@sheep.berlios.de>

Author: nouiz
Date: 2008-12-01 16:13:13 +0100 (Mon, 01 Dec 2008)
New Revision: 9730

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
   trunk/plearn/vmat/VariableDeletionVMatrix.h
Log:
Added option VariableDeletionVMatrix::info_var_with_missing that default to false. If true, will log witch var have missing on the named log.


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-12-01 15:09:39 UTC (rev 9729)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-12-01 15:13:13 UTC (rev 9730)
@@ -44,6 +44,8 @@
 #include <plearn/vmat/VMat_computeStats.h>
 #include <plearn/io/fileutils.h>
 #include <plearn/io/load_and_save.h>
+#define PL_LOG_MODULE_NAME "VariableDeletionVMatrix"
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;
@@ -70,6 +72,7 @@
     max_constant_threshold(0),
     number_of_train_samples(0),
     warn_removed_var(false),
+    info_var_with_missing(false),
     deletion_threshold(-1),
     remove_columns_with_constant_value(-1)
 {}
@@ -131,6 +134,12 @@
                   OptionBase::buildoption,
                   "If true, will print a warning about variable that are removed");
 
+    declareOption(ol, "info_var_with_missing",
+                  &VariableDeletionVMatrix::info_var_with_missing,
+                  OptionBase::buildoption,
+                  "If true, will print the variable that have some missing"
+                  " that we keep.");
+
     declareOption(ol, "save_deleted_columns",
                   &VariableDeletionVMatrix::save_deleted_columns,
                   OptionBase::buildoption,
@@ -284,15 +293,24 @@
     if (min_non_missing_threshold > 0){
         int min_non_missing =
             int(round(min_non_missing_threshold * the_train_source->length()));
-        for (int i = 0; i < is; i++)
+        for (int i = 0; i < is; i++){
             if (stats[i].nnonmissing() >= min_non_missing 
                 && stats[i].nnonmissing() > 0)
                 indices.append(i);
             else if (warn_removed_var)
                 PLWARNING("In VariableDeletionVMatrix::build_() var '%s'"
-                          " have too many missing value: %f/%f",
-                          source->fieldName(i).c_str(), stats[i].nmissing(),
-                          stats[i].n());
+                          " have too many missing (%d/%d). We remove it.",
+                          source->fieldName(i).c_str(),
+                          int(stats[i].nmissing()),
+                          int(stats[i].n()));
+            if (info_var_with_missing && stats[i].nmissing() > 0)
+                MODULE_LOG<<"INFO: In build_() var '"
+                          <<source->fieldName(i).c_str()
+                          <<"' have missing value: "
+                          <<stats[i].nmissing()
+                          <<"/"<< stats[i].n()<<"."<<endl;
+        }
+                
     } else
         for (int i = 0; i < is; i++)
             indices.append(i);
@@ -306,7 +324,7 @@
                 final_indices.append(i);
             else if (warn_removed_var)
                 PLWARNING("In VariableDeletionVMatrix::build_() var '%s'"
-                          " is constant with value: %f",
+                          " is constant with value: %f. We remove it.",
                           source->fieldName(i).c_str(), stat.min());
         }
         indices.resize(final_indices.length());

Modified: trunk/plearn/vmat/VariableDeletionVMatrix.h
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.h	2008-12-01 15:09:39 UTC (rev 9729)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.h	2008-12-01 15:13:13 UTC (rev 9730)
@@ -59,6 +59,8 @@
     real    max_constant_threshold;
     int     number_of_train_samples;
     bool    warn_removed_var;
+    bool    info_var_with_missing;
+
     VMat    train_set;
     string  save_deleted_columns;
 



From nouiz at mail.berlios.de  Mon Dec  1 20:53:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 1 Dec 2008 20:53:30 +0100
Subject: [Plearn-commits] r9731 - in trunk/plearn: sys vmat
Message-ID: <200812011953.mB1JrUZo002765@sheep.berlios.de>

Author: nouiz
Date: 2008-12-01 20:53:30 +0100 (Mon, 01 Dec 2008)
New Revision: 9731

Modified:
   trunk/plearn/sys/procinfo.cc
   trunk/plearn/sys/procinfo.h
   trunk/plearn/vmat/VMatrix.cc
Log:
-moved getUser() and getPid() from VMatrix to procinfo.
-implemented getPid for POSIX computer.


Modified: trunk/plearn/sys/procinfo.cc
===================================================================
--- trunk/plearn/sys/procinfo.cc	2008-12-01 15:13:13 UTC (rev 9730)
+++ trunk/plearn/sys/procinfo.cc	2008-12-01 19:53:30 UTC (rev 9731)
@@ -8,6 +8,7 @@
 #include <cstdio>
 #include <cstdlib>
 #include <iostream>
+#include <nspr/prenv.h>
 
 #if defined(WIN32) && defined(_MSC_VER)
 // unistd.h is not available under Microsoft Visual Studio, and some function
@@ -60,6 +61,32 @@
     return memory_size;
 }
 
+////////////
+// getPid //
+////////////
+int getPid()
+{
+#if _POSIX_VERSION >= 200112L
+#include <unistd.h>
+    return getpid();
+#else
+    return -999;
+#endif
+}
+
+/////////////
+// getUser //
+/////////////
+string getUser()
+{
+    const char* h = PR_GetEnv("USER");
+    if (!h)
+        h = PR_GetEnv("LOGNAME");
+    if (!h)
+        return "USERNAME_NOT_FOUND";
+    return tostring(h);
+}
+
 BEGIN_DECLARE_REMOTE_FUNCTIONS
 
     declareFunction("getSystemTotalMemory", &getSystemTotalMemory,

Modified: trunk/plearn/sys/procinfo.h
===================================================================
--- trunk/plearn/sys/procinfo.h	2008-12-01 15:13:13 UTC (rev 9730)
+++ trunk/plearn/sys/procinfo.h	2008-12-01 19:53:30 UTC (rev 9731)
@@ -46,6 +46,11 @@
 //! Return the total data memory used by the current process in bytes
 size_t getProcessDataMemory();
 
+//! Return the processus id
+int getPid();
+
+string getUser();
+
 } // end of namespace PLearn
 
 #endif

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-12-01 15:13:13 UTC (rev 9730)
+++ trunk/plearn/vmat/VMatrix.cc	2008-12-01 19:53:30 UTC (rev 9731)
@@ -52,6 +52,7 @@
 #include <plearn/base/RemoteDeclareMethod.h>
 #include <nspr/prenv.h>
 #include <plearn/math/TMat_maths.h> //!< for dot, powdistance externalProductAcc
+#include <plearn/sys/procinfo.h> //!< for getPid, getUser
 
 namespace PLearn {
 using namespace std;
@@ -1396,29 +1397,6 @@
 #undef MY_PRINT_ERROR_MST
 }
 
-
-////////////
-// getPid //
-////////////
-int getPid()
-{
-    return -999;
-}
-
-/////////////
-// getUser //
-/////////////
-string getUser()
-{
-    const char* h = PR_GetEnv("USER");
-    if (!h)
-        h = PR_GetEnv("LOGNAME");
-    if (!h)
-        return "USERNAME_NOT_FOUND";
-    return tostring(h);
-        
-}
-
 /////////////////////
 // lockMetaDataDir //
 /////////////////////



From nouiz at mail.berlios.de  Tue Dec  2 19:21:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Dec 2008 19:21:19 +0100
Subject: [Plearn-commits] r9732 - trunk/plearn/io
Message-ID: <200812021821.mB2ILJeU005399@sheep.berlios.de>

Author: nouiz
Date: 2008-12-02 19:21:16 +0100 (Tue, 02 Dec 2008)
New Revision: 9732

Modified:
   trunk/plearn/io/PStream.h
Log:
added error information on perr. We can't put it in LOG or use tostring caused by recursiv import.


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2008-12-01 19:53:30 UTC (rev 9731)
+++ trunk/plearn/io/PStream.h	2008-12-02 18:21:16 UTC (rev 9732)
@@ -929,8 +929,14 @@
     {
         in >> x.first;
         in.skipBlanksAndComments();
-        if(in.get()!=':')
+        if(in.get()!=':'){
+            //we can't use NORMAL_LOG as can't include pl_log.h
+            // can't include pl_log.h as it include PStream.h 
+            // and PStream.h would include pl_log.h(recursive include)
+            //idem for tostring.h
+            perr<<"For the following error, the first half of the pair is '"<<x.first<<"'"<<endl;
             PLERROR("In operator>>(PStream& in, pair<S, T> &x) expected ':' to separate the 2 halves of the pair");
+        }
         in.skipBlanksAndComments();
         in >> x.second;
     }



From nouiz at mail.berlios.de  Tue Dec  2 20:53:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Dec 2008 20:53:55 +0100
Subject: [Plearn-commits] r9733 - trunk/plearn/vmat
Message-ID: <200812021953.mB2JrtA4010167@sheep.berlios.de>

Author: nouiz
Date: 2008-12-02 20:53:54 +0100 (Tue, 02 Dec 2008)
New Revision: 9733

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
Log:
added the option SelectColumnsVMatrix::fields_regex that allow a fields to contain a * only at the end. I would have used boost, but it don't seem to support the glob syntax, which I would like to use.


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-12-02 18:21:16 UTC (rev 9732)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-12-02 19:53:54 UTC (rev 9733)
@@ -41,7 +41,10 @@
 
 #include "SelectColumnsVMatrix.h"
 #include <plearn/io/load_and_save.h>
+#define PL_LOG_MODULE_NAME "SelectColumnsVMatrix"
+#include <plearn/io/pl_log.h>
 
+
 namespace PLearn {
 using namespace std;
 
@@ -60,6 +63,7 @@
 SelectColumnsVMatrix::SelectColumnsVMatrix()
     : extend_with_missing(false),
       fields_partial_match(false),
+      fields_regex(false),
       inverse_fields_selection(false),
       warn_non_selected_field(false)
 {}
@@ -72,6 +76,7 @@
     extend_with_missing(the_extend_with_missing),
     fields(the_fields),
     fields_partial_match(false),
+    fields_regex(false),
     inverse_fields_selection(false),
     warn_non_selected_field(false)
 {
@@ -86,6 +91,7 @@
     extend_with_missing(false),
     indices(the_indices),
     fields_partial_match(false),
+    fields_regex(false),
     inverse_fields_selection(false),
     warn_non_selected_field(false)
 {
@@ -96,6 +102,7 @@
 SelectColumnsVMatrix::SelectColumnsVMatrix(VMat the_source, Vec the_indices)
     : extend_with_missing(false),
       fields_partial_match(false),
+      fields_regex(false),
       inverse_fields_selection(false),
       warn_non_selected_field(false)
 {
@@ -160,6 +167,9 @@
     declareOption(ol, "fields_partial_match", &SelectColumnsVMatrix::fields_partial_match, OptionBase::buildoption,
                   "If set to 1, then a field will be kept iff it contains one of the strings from 'fields'.");
 
+    declareOption(ol, "fields_regex", &SelectColumnsVMatrix::fields_regex, OptionBase::buildoption,
+                  "If set to 1, then a field will be kept iff it match one of the strings from 'fields'. A match is when the two string is equal or when fields end with a *, we allow any end.");
+
     declareOption(ol, "indices", &SelectColumnsVMatrix::indices, OptionBase::buildoption,
                   "The array of column indices to extract.");
 
@@ -319,7 +329,7 @@
 void SelectColumnsVMatrix::getIndicesFromFields(){
             // Find out the indices from the fields.
     indices.resize(0);
-    if (!fields_partial_match) {
+    if (!fields_partial_match && ! fields_regex) {
         TVec<string> missing_field;
         for (int i = 0; i < fields.length(); i++) {
             string the_field = fields[i];
@@ -366,6 +376,31 @@
                       " The columns names are: %s",missing_field.size(),
                       tostring(missing_field).c_str());
         }
+
+    } else if(fields_regex) {
+        // We need to check whether or not we should add each field.
+        TVec<string> source_fields = source->fieldNames();
+        for (int j = 0; j < fields.length(); j++){
+            string f = fields[j];
+            if(f[f.size()-1]=='*'){
+                f.resize(f.size()-1);//remove the last caracter (*)
+                for (int i = 0; i < source_fields.length(); i++){
+                    if (string_begins_with(source_fields[i],f)) {
+                        // We want to add this field.
+                        indices.append(i);
+                        DBG_MODULE_LOG<<fields[j]<<" matched "<<source_fields[i]<<endl;
+                    }
+                }
+            }else{
+                for (int i = 0; i < source_fields.length(); i++){
+                    if(source_fields[i]==f){
+                        indices.append(i);
+                    }
+                }
+
+
+            }
+        }
     } else {
         // We need to check whether or not we should add each field.
         TVec<string> source_fields = source->fieldNames();
@@ -374,6 +409,7 @@
                 if (source_fields[i].find(fields[j]) != string::npos) {
                     // We want to add this field.
                     indices.append(i);
+                    DBG_MODULE_LOG<<fields[j]<<" matched "<<source_fields[i]<<endl;
                     break;
                 }
     }

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-12-02 18:21:16 UTC (rev 9732)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-12-02 19:53:54 UTC (rev 9733)
@@ -72,8 +72,10 @@
     TVec<string> fields;
     PPath save_fields;
     bool fields_partial_match;
+    bool fields_regex;
     bool inverse_fields_selection;
     bool warn_non_selected_field;
+
 public:
 
     //! Default constructor.



From nouiz at mail.berlios.de  Tue Dec  2 21:00:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 2 Dec 2008 21:00:10 +0100
Subject: [Plearn-commits] r9734 - trunk/plearn/vmat
Message-ID: <200812022000.mB2K0A6r010747@sheep.berlios.de>

Author: nouiz
Date: 2008-12-02 21:00:10 +0100 (Tue, 02 Dec 2008)
New Revision: 9734

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
-code cleanup
-small optimization


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-12-02 19:53:54 UTC (rev 9733)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-12-02 20:00:10 UTC (rev 9734)
@@ -384,21 +384,19 @@
             string f = fields[j];
             if(f[f.size()-1]=='*'){
                 f.resize(f.size()-1);//remove the last caracter (*)
-                for (int i = 0; i < source_fields.length(); i++){
+                for (int i = 0; i < source_fields.length(); i++)
                     if (string_begins_with(source_fields[i],f)) {
                         // We want to add this field.
                         indices.append(i);
                         DBG_MODULE_LOG<<fields[j]<<" matched "<<source_fields[i]<<endl;
                     }
-                }
             }else{
-                for (int i = 0; i < source_fields.length(); i++){
+                for (int i = 0; i < source_fields.length(); i++)
                     if(source_fields[i]==f){
                         indices.append(i);
+                        //their should be only one fields with a given name
+                        break;
                     }
-                }
-
-
             }
         }
     } else {



From saintmlx at mail.berlios.de  Wed Dec  3 18:14:02 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 3 Dec 2008 18:14:02 +0100
Subject: [Plearn-commits] r9735 - trunk/plearn/base
Message-ID: <200812031714.mB3HE2rT025271@sheep.berlios.de>

Author: saintmlx
Date: 2008-12-03 18:14:01 +0100 (Wed, 03 Dec 2008)
New Revision: 9735

Modified:
   trunk/plearn/base/plerror.cc
Log:
- allow logging of PLERRORs (useful when throwing exceptions...)



Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-12-02 20:00:10 UTC (rev 9734)
+++ trunk/plearn/base/plerror.cc	2008-12-03 17:14:01 UTC (rev 9735)
@@ -35,6 +35,8 @@
 
 // Authors: Pascal Vincent & Yoshua Bengio
 
+#define PL_LOG_MODULE_NAME "plerror"
+
 #include <cstdarg>
 //#include <cstdlib>
 #include <iostream>
@@ -90,7 +92,8 @@
 #else
     vsprintf(message,msg,args);
 #endif
-
+    //output to module log: can be useful when throwing exceptions
+    DBG_MODULE_LOG << endl << "-------------- PLERROR:" << message << endl << "--------------" << endl;
 #ifndef USE_EXCEPTIONS
 #if USING_MPI
     *error_stream <<" ERROR from rank=" << PLMPI::rank << ": " <<message<<endl;



From saintmlx at mail.berlios.de  Wed Dec  3 18:35:32 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 3 Dec 2008 18:35:32 +0100
Subject: [Plearn-commits] r9736 - in trunk/plearn/base/test/.pytest:
	PL_assert/expected_results PL_check/expected_results
Message-ID: <200812031735.mB3HZWPL031291@sheep.berlios.de>

Author: saintmlx
Date: 2008-12-03 18:35:31 +0100 (Wed, 03 Dec 2008)
New Revision: 9736

Modified:
   trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
   trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
Log:
- fix source line numbers in pytest expected result



Modified: trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-12-03 17:14:01 UTC (rev 9735)
+++ trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-12-03 17:35:31 UTC (rev 9736)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 229
+FATAL ERROR: In file: "plerror.cc" at line 232
 Assertion failed: 3+8 == 123+46
 Function: int main()
     File: assertions.cc

Modified: trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-12-03 17:14:01 UTC (rev 9735)
+++ trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-12-03 17:35:31 UTC (rev 9736)
@@ -1,4 +1,4 @@
-FATAL ERROR: In file: "plerror.cc" at line 237
+FATAL ERROR: In file: "plerror.cc" at line 240
 Check failed: ein == stein
 Function: void PLearn::PLCheckTest::perform()
     File: PLCheckTest.cc



From nouiz at mail.berlios.de  Wed Dec  3 20:03:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 3 Dec 2008 20:03:24 +0100
Subject: [Plearn-commits] r9737 - trunk/plearn_learners/regressors
Message-ID: <200812031903.mB3J3OhK016855@sheep.berlios.de>

Author: nouiz
Date: 2008-12-03 20:03:24 +0100 (Wed, 03 Dec 2008)
New Revision: 9737

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
moved some log to debug level.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-03 17:35:31 UTC (rev 9736)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-03 19:03:24 UTC (rev 9737)
@@ -347,12 +347,12 @@
     }
     PLASSERT(fast_is_less(after_split_error,REAL_MAX)||split_col==-1);
 
-    MODULE_LOG<<"error after split: "<<after_split_error<<endl;
-    MODULE_LOG<<"split value: "<<split_feature_value<<endl;
-    MODULE_LOG<<"split_col: "<<split_col;
+    DBG_MODULE_LOG<<"error after split: "<<after_split_error<<endl;
+    DBG_MODULE_LOG<<"split value: "<<split_feature_value<<endl;
+    DBG_MODULE_LOG<<"split_col: "<<split_col;
     if(split_col>=0)
-        MODULE_LOG<<" "<<train_set->fieldName(split_col);
-    MODULE_LOG<<endl;
+        DBG_MODULE_LOG<<" "<<train_set->fieldName(split_col);
+    DBG_MODULE_LOG<<endl;
 }
 
 tuple<real,real,int>RegressionTreeNode::bestSplitInRow(



From laulysta at mail.berlios.de  Thu Dec  4 01:51:33 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 4 Dec 2008 01:51:33 +0100
Subject: [Plearn-commits] r9738 - trunk/plearn_learners_experimental
Message-ID: <200812040051.mB40pXQV003455@sheep.berlios.de>

Author: laulysta
Date: 2008-12-04 01:51:32 +0100 (Thu, 04 Dec 2008)
New Revision: 9738

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
baseline for expressive data is working


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-03 19:03:24 UTC (rev 9737)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-04 00:51:32 UTC (rev 9738)
@@ -327,7 +327,7 @@
 
         target_layers_n_of_target_elements.resize( targetsize() );
         target_layers_n_of_target_elements.clear();
-
+        tar_layer = 0;
         for( int tar=0; tar<targetsize(); tar++)
         {
             if( tar_layer > target_layers.length() )
@@ -650,7 +650,7 @@
             pb = new ProgressBar( "Recurrent training phase of "+classname(),
                                   end_stage - init_stage );
 
-        int nCost = 2;
+        int nCost = 0;
         train_costs.resize(train_costs.length() + nCost);
         train_n_items.resize(train_n_items.length() + nCost);
         while(stage < end_stage)
@@ -702,24 +702,25 @@
             if( pb )
                 pb->update( stage + 1 - init_stage);
             
-            double totalCosts = 0;
+            //double totalCosts = 0;
             for(int i=0; i<train_costs.length(); i++)
             {
                 if (i < target_layers_weights.length()){
                     if( !fast_exact_is_equal(target_layers_weights[i],0) ){
                         train_costs[i] /= train_n_items[i];
-                        totalCosts += train_costs[i]*target_layers_weights[i];
+                        //totalCosts += train_costs[i]*target_layers_weights[i];
                     }
                     else
                         train_costs[i] = MISSING_VALUE;
                 }
-                
+                /*
                 if (i == train_costs.length()-nCost ){
                     train_costs[i] /= train_n_items[i];
                     totalCosts += train_costs[i]*input_reconstruction_cost_weight;
                 }
                 else if (i == train_costs.length()-1)
                     train_costs[i] = totalCosts;
+                */
             }
 
             if(verbosity>0)
@@ -851,6 +852,8 @@
         masks_list[k] = mask_part.subMatColumns(startcol, targsize);
         startcol += targsize;
     }
+    encoded_seq.resize(seq.length(), seq.width());
+    encoded_seq << seq;
 }
 
 
@@ -1110,20 +1113,26 @@
         int tar = 0;
         if( prediction_cost_weight!=0 )
         {
-            target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
-            target_layers[tar]->activation += target_layers[tar]->bias;
-            target_layers[tar]->setExpectation(target_prediction_list[tar](i));
-            target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
-            bias_gradient *= prediction_cost_weight;
-            //if(use_target_layers_masks)
-            //    bias_gradient *= masks_list[tar](i);
-            target_layers[tar]->update(bias_gradient);
-            if( hidden_layer2 )
-                target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                     hidden_gradient, bias_gradient,true);
-            else
-                target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                     hidden_gradient, bias_gradient,true);
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= prediction_cost_weight;
+                    if(use_target_layers_masks)
+                        bias_gradient *= masks_list[tar](i);
+                    target_layers[tar]->update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                }
+            }
         }
         if (hidden_layer2)
         {



From nouiz at mail.berlios.de  Thu Dec  4 18:51:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 4 Dec 2008 18:51:29 +0100
Subject: [Plearn-commits] r9739 - trunk/python_modules/plearn/pymake
Message-ID: <200812041751.mB4HpTZQ008199@sheep.berlios.de>

Author: nouiz
Date: 2008-12-04 18:51:27 +0100 (Thu, 04 Dec 2008)
New Revision: 9739

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
better help message.


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-12-04 00:51:32 UTC (rev 9738)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-12-04 17:51:27 UTC (rev 9739)
@@ -156,8 +156,8 @@
   -getoptions: print the specific options for the target
   -vcproj: a Visual Studio project file (.vcproj) for the target will be
            created.
-  -o filename: the name of the output file
-  -link-target
+  -o filename: the name of the output file (a symlic to the real file)
+  -link-target filename: The name of the real output file (not a symlic as -o)
   
 The configuration file 'config' for pymake is searched for
 first in the .pymake subdirectory of the current directory, then similarly



From laulysta at mail.berlios.de  Fri Dec  5 00:00:09 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 5 Dec 2008 00:00:09 +0100
Subject: [Plearn-commits] r9740 - trunk/plearn_learners_experimental
Message-ID: <200812042300.mB4N0963001588@sheep.berlios.de>

Author: laulysta
Date: 2008-12-05 00:00:08 +0100 (Fri, 05 Dec 2008)
New Revision: 9740

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
reconstruction working


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-04 17:51:27 UTC (rev 9739)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-04 23:00:08 UTC (rev 9740)
@@ -71,9 +71,6 @@
     );
 
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
-    prediction_cost_weight(1),
-    input_reconstruction_cost_weight(0),
-    hidden_reconstruction_cost_weight(0),
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
     encoding("note_octav_duration"),
@@ -87,6 +84,9 @@
     noisy_recurrent_lr( 0.000001),
     dynamic_gradient_scale_factor( 1.0 ),
     recurrent_lr( 0.00001 ),
+    prediction_cost_weight(1),
+    input_reconstruction_cost_weight(0),
+    hidden_reconstruction_cost_weight(0),
     current_learning_rate(0)
 {
     random_gen = new PRandom();
@@ -672,30 +672,29 @@
                 if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
                 
-                recurrentFprop(train_costs, train_n_items);
-
                 // greedy phase
                 if(input_reconstruction_lr!=0 || hidden_reconstruction_lr!=0){
                     setLearningRate( input_reconstruction_lr );
-                    performGreedyDenoisingPhase(train_costs, train_n_items);
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate(input_reconstruction_cost_weight, 0);
                 }
 
                 // recurrent noisy phase
                 if(noisy_recurrent_lr!=0)
                 {
                     setLearningRate( noisy_recurrent_lr );
-                    //recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(true);
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate(input_reconstruction_cost_weight, 1);
                 }
 
                 // recurrent no noise phase
                 if(recurrent_lr!=0)
                 {
-                    if(corrupt_input) // need to recover the clean sequence
-                        encoded_seq << clean_encoded_seq;                    
-                    setLearningRate( recurrent_lr );
+                    if(corrupt_input) // need to recover the clean sequence                        
+                        encoded_seq << clean_encoded_seq;                  
+                    setLearningRate( recurrent_lr );                    
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(false);
+                    recurrentUpdate(0,1);
                 }
             }
 
@@ -741,49 +740,6 @@
 }
 
 
-void DenoisingRecurrentNet::performGreedyDenoisingPhase(Vec train_costs, Vec train_n_items)
-{
-    hidden_temporal_gradient.resize(hidden_layer->size);
-    hidden_gradient.resize(hidden_layer->size);
-    
-    double cost = 0;
-    Mat reconstruction_weights = getInputConnectionsWeightMatrix();
-    for(int i=hidden_list.length()-1; i>=0; i--){ 
-        // Add contribution of input reconstruction cost in hidden_gradient
-        if(input_reconstruction_cost_weight!=0)
-        {
-            hidden_temporal_gradient.clear();
-            hidden_gradient.clear();
-
-            Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
-            
-            train_costs[train_costs.length()-2] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
-                                                                    clean_input, hidden_gradient, input_reconstruction_cost_weight, input_reconstruction_lr);
-            train_n_items[train_n_items.length()-2]++;
-
-            if(i!=0 && dynamic_connections )
-            {
-                hidden_layer->bpropUpdate(
-                    hidden_act_no_bias_list(i), hidden_list(i),
-                    hidden_temporal_gradient, hidden_gradient);
-                
-                dynamic_connections->bpropUpdate(
-                    hidden_list(i-1),
-                    hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                    hidden_gradient, hidden_temporal_gradient);
-            }
-            else{
-                hidden_layer->bpropUpdate(
-                    hidden_act_no_bias_list(i), hidden_list(i),
-                    hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-            }
-        }
-    }
-
-    
-}
-
-
 //! does encoding if needed and populates the list.
 void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
 {
@@ -852,8 +808,9 @@
         masks_list[k] = mask_part.subMatColumns(startcol, targsize);
         startcol += targsize;
     }
-    encoded_seq.resize(seq.length(), seq.width());
-    encoded_seq << seq;
+
+    encoded_seq.resize(input_part.length(), input_part.width());
+    encoded_seq << input_part;
 }
 
 
@@ -1051,14 +1008,27 @@
     // predict (denoised) input_reconstruction 
     transposeProduct(input_reconstruction_activation, reconstruction_weights, hidden); 
     input_reconstruction_activation += input_reconstruction_bias;
-    applyMultipleSoftmaxToInputWindow(input_reconstruction_activation, input_reconstruction_prob);
-    
-    double neg_log_cost = 0; // neg log softmax
-    for(int k=0; k<input_reconstruction_prob.length(); k++)
-        if(clean_input[k]!=0)
-            neg_log_cost -= clean_input[k]*safelog(input_reconstruction_prob[k]);
 
-    return neg_log_cost;
+    double result_cost = 0;
+    if(encoding=="raw_masked_supervised") // complicated input format... consider it's squared error
+    {
+        real r;
+        input_reconstruction_prob << input_reconstruction_activation;
+        for(int i=0; i<input_reconstruction_activation.length(); i++)
+            r += input_reconstruction_activation[i] - clean_input[i];
+        result_cost = r*r;
+    }
+    else // suppose it's a multiple softmax
+    {
+        applyMultipleSoftmaxToInputWindow(input_reconstruction_activation, input_reconstruction_prob);
+    
+        double neg_log_cost = 0; // neg log softmax
+        for(int k=0; k<input_reconstruction_prob.length(); k++)
+            if(clean_input[k]!=0)
+                neg_log_cost -= clean_input[k]*safelog(input_reconstruction_prob[k]);
+        result_cost = neg_log_cost;
+    }
+    return result_cost;
 }
 
 //! Backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
@@ -1076,8 +1046,10 @@
     multiplyAcc(input_reconstruction_bias, input_reconstruction_activation_grad, -lr);
 
     // update weight
-    // productTransposeAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad);
-    externalProductScaleAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad, -lr);
+    // THIS IS COMMENTED OUT BECAUSE THE reconstruction_weights ARE tied (same) TO THE input_connection weights, 
+    // WHICH GET UPDATED LATER IN recurrentUpdate SO IF WE UPDATE THEM HERE THEY WOULD GET UPDATED TWICE.
+    // WARNING: THIS WOULD NO LONGER BE THE CASE IF THEY WERE NOT TIED!
+    // externalProductScaleAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad, -lr);
 
     // accumulate in hidden_gradient
     productAcc(hidden_gradient, reconstruction_weights, input_reconstruction_activation_grad);
@@ -1099,7 +1071,8 @@
 nll_list
 */
 
-void DenoisingRecurrentNet::recurrentUpdate(bool input_is_corrupted)
+void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
+                                            real temporal_gradient_contribution)
 {
     hidden_temporal_gradient.resize(hidden_layer->size);
     hidden_temporal_gradient.clear();
@@ -1110,7 +1083,6 @@
         else
             hidden_gradient.resize(hidden_layer->size);
         hidden_gradient.clear();
-        int tar = 0;
         if( prediction_cost_weight!=0 )
         {
             for( int tar=0; tar<target_layers.length(); tar++)
@@ -1133,34 +1105,38 @@
                                                              hidden_gradient, bias_gradient,true);
                 }
             }
-        }
-        if (hidden_layer2)
-        {
-            hidden_layer2->bpropUpdate(
-                hidden2_act_no_bias_list(i), hidden2_list(i),
-                bias_gradient, hidden_gradient);
+
+            if (hidden_layer2)
+            {
+                hidden_layer2->bpropUpdate(
+                    hidden2_act_no_bias_list(i), hidden2_list(i),
+                    bias_gradient, hidden_gradient);
                 
-            hidden_connections->bpropUpdate(
-                hidden_list(i),
-                hidden2_act_no_bias_list(i), 
-                hidden_gradient, bias_gradient);
+                hidden_connections->bpropUpdate(
+                    hidden_list(i),
+                    hidden2_act_no_bias_list(i), 
+                    hidden_gradient, bias_gradient);
+            }
         }
             
         // Add contribution of input reconstruction cost in hidden_gradient
-        /*   if(input_is_corrupted && input_reconstruction_cost_weight!=0)
+        if(input_reconstruction_weight!=0)
         {
             Mat reconstruction_weights = getInputConnectionsWeightMatrix();
             Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
 
             fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
-                                                     clean_input, hidden_gradient, input_reconstruction_cost_weight, current_learning_rate);
-                                                     }*/
+                                                     clean_input, hidden_gradient, input_reconstruction_weight, current_learning_rate);
+        }
 
         if(i!=0 && dynamic_connections )
         {   
             // add contribution to gradient of next time step hidden layer
-            hidden_gradient += hidden_temporal_gradient;
-                
+            if(temporal_gradient_contribution>0)
+            { // add weighted contribution of hidden_temporal gradient to hidden_gradient
+                // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
+                multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+            }
             hidden_layer->bpropUpdate(
                 hidden_act_no_bias_list(i), hidden_list(i),
                 hidden_temporal_gradient, hidden_gradient);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-04 17:51:27 UTC (rev 9739)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-04 23:00:08 UTC (rev 9740)
@@ -266,7 +266,8 @@
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
-    void recurrentUpdate(bool input_is_corrupted);
+    void recurrentUpdate(real input_reconstruction_weight,
+                         real temporal_gradient_contribution = 1);
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -377,8 +378,6 @@
     void build_();
 
 
-    void performGreedyDenoisingPhase(Vec train_costs, Vec train_n_items);
-
     void applyMultipleSoftmaxToInputWindow(Vec input_reconstruction_activation, Vec input_reconstruction_prob);
 
     // note: the following functions are declared const because they have



From tihocan at mail.berlios.de  Fri Dec  5 16:30:39 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 5 Dec 2008 16:30:39 +0100
Subject: [Plearn-commits] r9741 - trunk/python_modules/plearn/pymake
Message-ID: <200812051530.mB5FUdIa027958@sheep.berlios.de>

Author: tihocan
Date: 2008-12-05 16:30:37 +0100 (Fri, 05 Dec 2008)
New Revision: 9741

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Fixed typos in help

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-12-04 23:00:08 UTC (rev 9740)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-12-05 15:30:37 UTC (rev 9741)
@@ -156,8 +156,8 @@
   -getoptions: print the specific options for the target
   -vcproj: a Visual Studio project file (.vcproj) for the target will be
            created.
-  -o filename: the name of the output file (a symlic to the real file)
-  -link-target filename: The name of the real output file (not a symlic as -o)
+  -o filename: the name of the output file (a symlink to the real file)
+  -link-target filename: the name of the real output file (not a symlink as -o)
   
 The configuration file 'config' for pymake is searched for
 first in the .pymake subdirectory of the current directory, then similarly



From nouiz at mail.berlios.de  Fri Dec  5 17:01:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 5 Dec 2008 17:01:23 +0100
Subject: [Plearn-commits] r9742 - in trunk/plearn_learners: meta regressors
Message-ID: <200812051601.mB5G1Nik030377@sheep.berlios.de>

Author: nouiz
Date: 2008-12-05 17:01:23 +0100 (Fri, 05 Dec 2008)
New Revision: 9742

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
Remove duplicate date in memory to save memory.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-05 15:30:37 UTC (rev 9741)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-05 16:01:23 UTC (rev 9742)
@@ -41,6 +41,9 @@
 #include <plearn/vmat/ProcessingVMatrix.h>
 #include <plearn/vmat/SubVMatrix.h>
 #include <plearn/vmat/MemoryVMatrix.h>
+#include <plearn_learners/regressors/RegressionTreeRegisters.h>
+#define PL_LOG_MODULE_NAME "MultiClassAdaBoost"
+#include <plearn/io/pl_log.h>
 #ifdef _OPENMP
 #include <omp.h>
 #endif
@@ -511,6 +514,14 @@
         learner2->setTrainingSet(vmat2, call_forget);
     }
 
+    if(learner1->getTrainingSet()->classname()=="RegressionTreeRegisters"
+       && learner2->getTrainingSet()->classname()=="RegressionTreeRegisters")
+    {
+        TMat<RTR_type> t =  ((PP<RegressionTreeRegisters>)(learner2->getTrainingSet()))->getTSortedRow();
+        DBG_MODULE_LOG<<"removing duplicate tsorted_row in RegressionTreeRegisters to save memory"<<endl;
+        ((PP<RegressionTreeRegisters>)(learner1->getTrainingSet()))->setTSortedRow(t);
+    }
+       
     //we do it here as RegressionTree need a trainingSet to know
     // the number of test.
     subcosts2.resize(learner2->nTestCosts());

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-05 15:30:37 UTC (rev 9741)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-05 16:01:23 UTC (rev 9742)
@@ -51,8 +51,9 @@
 //!work with unsigned int, uint16_t, but fail with uint8_t???
 #define RTR_type uint32_t
 //!The type for the leave id
+#ifndef RTR_type_id
 #define RTR_type_id int16_t
-
+#endif
 namespace PLearn {
 using namespace std;
 
@@ -80,7 +81,8 @@
     TMat<RTR_type> tsorted_row;
     TVec<RTR_type_id> leave_register;
     VMat tsource;
- 
+    VMat source;
+
 public:
 
     RegressionTreeRegisters();
@@ -123,7 +125,8 @@
                     "the weightsize only");
         setWeight(i,value);
     }
-    VMat source;
+    void         setTSortedRow(TMat<RTR_type> t){tsorted_row = t;}
+    TMat<RTR_type> getTSortedRow(){return tsorted_row;}
 
 private:
     void         build_();



From nouiz at mail.berlios.de  Fri Dec  5 17:03:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 5 Dec 2008 17:03:14 +0100
Subject: [Plearn-commits] r9743 - trunk/plearn_learners/regressors
Message-ID: <200812051603.mB5G3EwC030496@sheep.berlios.de>

Author: nouiz
Date: 2008-12-05 17:03:13 +0100 (Fri, 05 Dec 2008)
New Revision: 9743

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
moved log from dbg level to extreme level


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-05 16:01:23 UTC (rev 9742)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-05 16:03:13 UTC (rev 9743)
@@ -347,12 +347,12 @@
     }
     PLASSERT(fast_is_less(after_split_error,REAL_MAX)||split_col==-1);
 
-    DBG_MODULE_LOG<<"error after split: "<<after_split_error<<endl;
-    DBG_MODULE_LOG<<"split value: "<<split_feature_value<<endl;
-    DBG_MODULE_LOG<<"split_col: "<<split_col;
+    EXTREME_MODULE_LOG<<"error after split: "<<after_split_error<<endl;
+    EXTREME_MODULE_LOG<<"split value: "<<split_feature_value<<endl;
+    EXTREME_MODULE_LOG<<"split_col: "<<split_col;
     if(split_col>=0)
-        DBG_MODULE_LOG<<" "<<train_set->fieldName(split_col);
-    DBG_MODULE_LOG<<endl;
+        EXTREME_MODULE_LOG<<" "<<train_set->fieldName(split_col);
+    EXTREME_MODULE_LOG<<endl;
 }
 
 tuple<real,real,int>RegressionTreeNode::bestSplitInRow(



From nouiz at mail.berlios.de  Fri Dec  5 18:22:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 5 Dec 2008 18:22:49 +0100
Subject: [Plearn-commits] r9744 - in trunk/plearn_learners: meta regressors
Message-ID: <200812051722.mB5HMnvj010645@sheep.berlios.de>

Author: nouiz
Date: 2008-12-05 18:22:47 +0100 (Fri, 05 Dec 2008)
New Revision: 9744

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/regressors/LocalMedBoost.cc
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
-removed fct RegressionTreeRegisters::initRegisters
-added a new build fct for RegressionTreeRegisters to replace the precedent one.
This was done to use the same structure as the rest of PLearn.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-12-05 16:03:13 UTC (rev 9743)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-12-05 17:22:47 UTC (rev 9744)
@@ -941,13 +941,10 @@
     if(weak_learner_template->classname()=="RegressionTree"){
         //we do this for optimization. Otherwise we will creat a RegressionTreeRegister
         //for each weak_learner. This is time consuming as it sort the dataset
-        if(training_set->classname()!="RegressionTreeRegisters"){
-            PP<RegressionTreeRegisters> sorted_train_set = new RegressionTreeRegisters();
-            sorted_train_set->setOption("report_progress", tostring(report_progress));
-            sorted_train_set->setOption("verbosity", tostring(verbosity));
-            sorted_train_set->initRegisters(training_set);
-            training_set = VMat(sorted_train_set);
-        }
+        if(training_set->classname()!="RegressionTreeRegisters")
+            training_set = new RegressionTreeRegisters(training_set,
+                                                       report_progress,
+                                                       verbosity);
 
         //we need to change the weight of the trainning set to reuse the RegressionTreeRegister
         if(!modif_train_set_weights)

Modified: trunk/plearn_learners/regressors/LocalMedBoost.cc
===================================================================
--- trunk/plearn_learners/regressors/LocalMedBoost.cc	2008-12-05 16:03:13 UTC (rev 9743)
+++ trunk/plearn_learners/regressors/LocalMedBoost.cc	2008-12-05 17:22:47 UTC (rev 9744)
@@ -183,12 +183,9 @@
         initializeLineSearch();
         bound = 1.0;
         if (regression_tree > 0)
-        {
-            sorted_train_set = new RegressionTreeRegisters();
-            sorted_train_set->setOption("report_progress", tostring(report_progress));
-            sorted_train_set->setOption("verbosity", tostring(verbosity));
-            sorted_train_set->initRegisters(train_set);
-        }
+            sorted_train_set = new RegressionTreeRegisters(train_set,
+                                                           report_progress,
+                                                           verbosity);
     }
     PP<ProgressBar> pb;
     if (report_progress) pb = new ProgressBar("LocalMedBoost: train stages: ", nstages);

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-05 16:03:13 UTC (rev 9743)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-05 17:22:47 UTC (rev 9744)
@@ -285,12 +285,9 @@
         sorted_train_set->reinitRegisters();
     }
     else if(!sorted_train_set)
-    {
-        sorted_train_set = new RegressionTreeRegisters();
-        sorted_train_set->setOption("report_progress", tostring(report_progress));
-        sorted_train_set->setOption("verbosity", tostring(verbosity));
-        sorted_train_set->initRegisters(train_set);
-    }
+        sorted_train_set = new RegressionTreeRegisters(train_set,
+                                                       report_progress,
+                                                       verbosity);
     else
     {
         sorted_train_set->reinitRegisters();

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-12-05 16:03:13 UTC (rev 9743)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-12-05 17:22:47 UTC (rev 9744)
@@ -64,6 +64,17 @@
     build();
 }
 
+RegressionTreeRegisters::RegressionTreeRegisters(VMat source_,
+                                                 bool report_progress_,
+                                                 bool verbosity_):
+    report_progress(report_progress_),
+    verbosity(verbosity_),
+    next_id(0)
+{
+    source = source_;
+    build();
+}
+
 RegressionTreeRegisters::~RegressionTreeRegisters()
 {
 }
@@ -116,25 +127,20 @@
 
 void RegressionTreeRegisters::build_()
 {
-    if(source)
-        initRegisters(source);
-}
-
-void RegressionTreeRegisters::initRegisters(VMat the_train_set)
-{   
+    if(!source)
+        return;
     //check that we can put all the examples of the train_set
     //with respect to the size of RTR_type who limit the capacity
-    PLCHECK(the_train_set.length()>0 
-            && (unsigned)the_train_set.length()<=std::numeric_limits<RTR_type>::max());
+    PLCHECK(source.length()>0 
+            && (unsigned)source.length()
+            <= std::numeric_limits<RTR_type>::max());
 
-    if(the_train_set==source && tsource)
-        //we set the existing source file
-        return;
-    source = the_train_set;
-    VMat tmp = VMat(new TransposeVMatrix(the_train_set));
-    PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
-    tsource = VMat(tmp2 );
-    setMetaInfoFrom(the_train_set);
+    if(!tsource){
+        VMat tmp = VMat(new TransposeVMatrix(source));
+        PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
+        tsource = VMat(tmp2 );
+        setMetaInfoFrom(source);
+    }
     leave_register.resize(length());
     sortRows();
 }

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-05 16:03:13 UTC (rev 9743)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-05 17:22:47 UTC (rev 9744)
@@ -72,7 +72,7 @@
 //    VMat train_set;
   
 /*
-  Learnt options: they are sized and initialized if need be, at initRegisters(...) or reinitRegisters()
+  Learnt options: they are sized and initialized if need be, at build() or reinitRegisters()
 */
 
     int       next_id;
@@ -86,6 +86,8 @@
 public:
 
     RegressionTreeRegisters();
+    RegressionTreeRegisters(VMat source_, bool report_progress_ = false,
+                            bool vebosity_ = false);
     virtual              ~RegressionTreeRegisters();
     
     PLEARN_DECLARE_OBJECT(RegressionTreeRegisters);
@@ -93,7 +95,6 @@
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
-    void         initRegisters(VMat train_set);
     void         reinitRegisters();
     inline void         registerLeave(RTR_type_id leave_id, int row)
     { leave_register[row] = leave_id;    }
@@ -125,6 +126,8 @@
                     "the weightsize only");
         setWeight(i,value);
     }
+    
+    //! usefull in MultiClassAdaBoost to save memory
     void         setTSortedRow(TMat<RTR_type> t){tsorted_row = t;}
     TMat<RTR_type> getTSortedRow(){return tsorted_row;}
 



From nouiz at mail.berlios.de  Mon Dec  8 18:55:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 18:55:33 +0100
Subject: [Plearn-commits] r9745 - trunk/plearn_learners/meta
Message-ID: <200812081755.mB8HtX5D032031@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 18:55:32 +0100 (Mon, 08 Dec 2008)
New Revision: 9745

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
Handle correctly the weight.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-05 17:22:47 UTC (rev 9744)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 17:55:32 UTC (rev 9745)
@@ -500,7 +500,13 @@
     input_prg  = "[%0:%"+tostring(training_set->inputsize()-1)+"]";
     target_prg1= "@"+targetname+" 1 0 ifelse :"+targetname;
     target_prg2= "@"+targetname+" 2 - 0 1 ifelse :"+targetname;
-    
+
+    if(training_set->weightsize()>0){
+        int index = training_set->inputsize()+training_set->targetsize();
+        weight_prg = "[%"+tostring(index)+"]";
+    }else
+        weight_prg = "1 :weights";
+        
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
     if(training_set_has_changed || !learner1->getTrainingSet()){



From nouiz at mail.berlios.de  Mon Dec  8 19:13:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 19:13:57 +0100
Subject: [Plearn-commits] r9746 - trunk/plearn_learners/meta
Message-ID: <200812081813.mB8IDvQj022357@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 19:13:56 +0100 (Mon, 08 Dec 2008)
New Revision: 9746

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
remove duplicate memory once.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 17:55:32 UTC (rev 9745)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 18:13:56 UTC (rev 9746)
@@ -507,25 +507,30 @@
     }else
         weight_prg = "1 :weights";
         
+    bool tsorted = false;
+
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
     if(training_set_has_changed || !learner1->getTrainingSet()){
         VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
                                            target_prg1,  weight_prg);
         learner1->setTrainingSet(vmat1, call_forget);
+        tsorted = true;
     }
     if(training_set_has_changed || !learner2->getTrainingSet()){
         VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
                                            target_prg2,  weight_prg);
         learner2->setTrainingSet(vmat2, call_forget);
+        tsorted = true;
     }
 
-    if(learner1->getTrainingSet()->classname()=="RegressionTreeRegisters"
+    if(tsorted &&
+       learner1->getTrainingSet()->classname()=="RegressionTreeRegisters"
        && learner2->getTrainingSet()->classname()=="RegressionTreeRegisters")
     {
-        TMat<RTR_type> t =  ((PP<RegressionTreeRegisters>)(learner2->getTrainingSet()))->getTSortedRow();
+        TMat<RTR_type> t1 =  ((PP<RegressionTreeRegisters>)(learner1->getTrainingSet()))->getTSortedRow();
         DBG_MODULE_LOG<<"removing duplicate tsorted_row in RegressionTreeRegisters to save memory"<<endl;
-        ((PP<RegressionTreeRegisters>)(learner1->getTrainingSet()))->setTSortedRow(t);
+        ((PP<RegressionTreeRegisters>)(learner2->getTrainingSet()))->setTSortedRow(t1);
     }
        
     //we do it here as RegressionTree need a trainingSet to know



From nouiz at mail.berlios.de  Mon Dec  8 19:17:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 19:17:58 +0100
Subject: [Plearn-commits] r9747 - trunk/plearn_learners/hyper
Message-ID: <200812081817.mB8IHwtD027933@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 19:17:56 +0100 (Mon, 08 Dec 2008)
New Revision: 9747

Modified:
   trunk/plearn_learners/hyper/HyperLearner.cc
Log:
moved a PLWARNING to a DBG_MODULE_LOG so to don't show it normaly.


Modified: trunk/plearn_learners/hyper/HyperLearner.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperLearner.cc	2008-12-08 18:13:56 UTC (rev 9746)
+++ trunk/plearn_learners/hyper/HyperLearner.cc	2008-12-08 18:17:56 UTC (rev 9747)
@@ -42,6 +42,8 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/base/PLearnDiff.h>
 #include <plearn/io/load_and_save.h>
+#define PL_LOG_MODULE_NAME "HyperLearner"
+#include <plearn/io/pl_log.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/sys/Profiler.h>
 
@@ -371,7 +373,7 @@
     // this reload mechanism to let us reload multiple (chained) HyperLearners.
     if(expdir.isEmpty()){
         if(verbosity>1)
-            PLWARNING("In HyperLearner::auto_load() - no expdir. Can't reload.");
+            DBG_MODULE_LOG<<"auto_load() - no expdir. Can't reload."<<endl;
         return;
     }
     PPath f = expdir/"hyper_learner_auto_save.psave";



From nouiz at mail.berlios.de  Mon Dec  8 19:31:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 19:31:03 +0100
Subject: [Plearn-commits] r9748 - trunk/plearn_learners/meta
Message-ID: <200812081831.mB8IV3Bw008388@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 19:30:54 +0100 (Mon, 08 Dec 2008)
New Revision: 9748

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
make sure the target is 0.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 18:17:56 UTC (rev 9747)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 18:30:54 UTC (rev 9748)
@@ -485,9 +485,12 @@
                   "it to a target of 2.", target[0]);
         sub_target[0][0]=1;
         sub_target[1][0]=1;
-    }else
+    }else{
         PLERROR("In MultiClassAdaBoost::getSubLearnerTarget - "
                   "We only support target 0/1/2. We got %f.", target[0]); 
+        sub_target[0][0]=0;
+        sub_target[1][0]=0;
+    }
 }
 
 void MultiClassAdaBoost::setTrainingSet(VMat training_set, bool call_forget)



From nouiz at mail.berlios.de  Mon Dec  8 19:35:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 19:35:52 +0100
Subject: [Plearn-commits] r9749 - trunk/plearn_learners/meta
Message-ID: <200812081835.mB8IZqiq025480@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 19:35:51 +0100 (Mon, 08 Dec 2008)
New Revision: 9749

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
made some fct parameter const.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 18:30:54 UTC (rev 9748)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 18:35:51 UTC (rev 9749)
@@ -467,7 +467,7 @@
     return names;
 }
 
-void MultiClassAdaBoost::getSubLearnerTarget(Vec target,
+void MultiClassAdaBoost::getSubLearnerTarget(const Vec target,
                                              TVec<Vec> sub_target) 
 {
     if(fast_is_equal(target[0],0.)){

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-12-08 18:30:54 UTC (rev 9748)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-12-08 18:35:51 UTC (rev 9749)
@@ -190,7 +190,7 @@
     // (PLEASE IMPLEMENT IN .cc)
     void build_();
 
-    static void getSubLearnerTarget(Vec target, TVec<Vec> sub_target);
+    static void getSubLearnerTarget(const Vec target, TVec<Vec> sub_target);
 private:
     //#####  Private Data Members  ############################################
     TVec<Vec> sub_target_tmp;



From nouiz at mail.berlios.de  Mon Dec  8 20:15:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 20:15:16 +0100
Subject: [Plearn-commits] r9751 - in trunk: commands/PLearnCommands
	scripts/Skeletons
Message-ID: <200812081915.mB8JFGNm030319@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 20:15:15 +0100 (Mon, 08 Dec 2008)
New Revision: 9751

Modified:
   trunk/commands/PLearnCommands/LearnerCommand.cc
   trunk/scripts/Skeletons/PLearner.cc
   trunk/scripts/Skeletons/PLearner.h
Log:
added PLearner::finalize in the skeletons and call it at one more place.


Modified: trunk/commands/PLearnCommands/LearnerCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/LearnerCommand.cc	2008-12-08 19:13:29 UTC (rev 9750)
+++ trunk/commands/PLearnCommands/LearnerCommand.cc	2008-12-08 19:15:15 UTC (rev 9751)
@@ -138,8 +138,10 @@
     if(outputs_file!="")
         testoutputs = new FileVMatrix(outputs_file,l,learner->outputsize());
     VMat testcosts;
-    if(set_testset_as_trainingset)
+    if(set_testset_as_trainingset){
         learner->setTrainingSet(testset, false);
+        learner->finalize();
+    }
     if(costs_file!="")
         testcosts = new FileVMatrix(costs_file,l,learner->getTestCostNames());
 

Modified: trunk/scripts/Skeletons/PLearner.cc
===================================================================
--- trunk/scripts/Skeletons/PLearner.cc	2008-12-08 19:13:29 UTC (rev 9750)
+++ trunk/scripts/Skeletons/PLearner.cc	2008-12-08 19:15:15 UTC (rev 9751)
@@ -85,6 +85,13 @@
     // may depend on its inputsize(), targetsize() and set options).
 }
 
+void DERIVEDCLASS::finalize()
+{
+    //! When this method is called the learner know it we will never train it again.
+    //! So it can free resources that are needed only during the training.
+    //! The function test()/computeOutputs()/... should continue to work.
+}
+
 void DERIVEDCLASS::forget()
 {
     //! (Re-)initialize the PLearner in its fresh state (that state may depend

Modified: trunk/scripts/Skeletons/PLearner.h
===================================================================
--- trunk/scripts/Skeletons/PLearner.h	2008-12-08 19:13:29 UTC (rev 9750)
+++ trunk/scripts/Skeletons/PLearner.h	2008-12-08 19:15:15 UTC (rev 9751)
@@ -41,6 +41,14 @@
     // (PLEASE IMPLEMENT IN .cc)
     virtual int outputsize() const;
 
+
+
+    //! When this method is called the learner know it we will never train it again.
+    //! So it can free resources that are needed only during the training.
+    //! The function test()/computeOutputs()/... should continue to work.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void finalize();
+
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).



From nouiz at mail.berlios.de  Mon Dec  8 20:13:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 20:13:29 +0100
Subject: [Plearn-commits] r9750 - in trunk/plearn_learners: generic meta
	regressors
Message-ID: <200812081913.mB8JDTKd030173@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 20:13:29 +0100 (Mon, 08 Dec 2008)
New Revision: 9750

Modified:
   trunk/plearn_learners/generic/PLearner.h
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
added a PLearn::finalize method that can be called when a learner will never be trained again. Example in an AdaBoost learner, the weak_learner.finalize() will be called. This is usefull to same memory.


Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/generic/PLearner.h	2008-12-08 19:13:29 UTC (rev 9750)
@@ -404,6 +404,15 @@
     /**
      *  *** SUBCLASS WRITING: ***
      *
+     * When this method is called the learner know it we will never train it again.
+     * So it can free resources that are needed only during the training.
+     * The functions test()/computeOutputs()/... should continue to work.
+     */
+    virtual void finalize(){};
+
+    /**
+     *  *** SUBCLASS WRITING: ***
+     *
      *  The role of the train method is to bring the learner up to
      *  stage==nstages, updating the stats with training costs measured on-line
      *  in the process.

Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-12-08 19:13:29 UTC (rev 9750)
@@ -281,6 +281,14 @@
     return 1;
 }
 
+void AdaBoost::finalize()
+{
+    for(int i=0;i<weak_learners.size();i++){
+        weak_learners[i]->finalize();
+    }
+
+}
+
 void AdaBoost::forget()
 {
     stage = 0;
@@ -467,6 +475,7 @@
             new_weak_learner->setExperimentDirectory( expdir / ("WeakLearner"+tostring(stage)+"Expdir") );
 
         new_weak_learner->train();
+        new_weak_learner->finalize();
 
         // calculate its weighted training error 
         {

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/meta/AdaBoost.h	2008-12-08 19:13:29 UTC (rev 9750)
@@ -177,6 +177,8 @@
     //! classification, hence returns 1
     virtual int outputsize() const;
 
+    virtual void finalize();
+
     //! (Re-)initializes the PLearner in its fresh state (that state may depend on the 'seed' option)
     //! And sets 'stage' back to 0   (this is the stage of a fresh learner!)
     virtual void forget();

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-08 19:13:29 UTC (rev 9750)
@@ -190,6 +190,12 @@
     return 3;
 }
 
+void MultiClassAdaBoost::finalize()
+{
+    learner1->finalize();
+    learner2->finalize();
+}
+
 void MultiClassAdaBoost::forget()
 {
     //! (Re-)initialize the PLearner in its fresh state (that state may depend

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-12-08 19:13:29 UTC (rev 9750)
@@ -103,6 +103,8 @@
     // (PLEASE IMPLEMENT IN .cc)
     virtual int outputsize() const;
 
+    virtual void finalize();
+
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-08 19:13:29 UTC (rev 9750)
@@ -272,6 +272,18 @@
         pout << the_msg << endl;
 }
 
+void RegressionTree::finalize()
+{
+    root->finalize();
+    priority_queue = 0;
+    split_cols = TVec<int>();
+    split_values = Vec();
+    leave_template = 0;
+    first_leave = 0;
+    if(sorted_train_set)
+        sorted_train_set->finalize();
+}
+
 void RegressionTree::forget()
 {
     stage = 0;

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-12-08 19:13:29 UTC (rev 9750)
@@ -105,6 +105,7 @@
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
     virtual void         train();
+    virtual void         finalize();
     virtual void         forget();
     virtual int          outputsize() const;
     virtual TVec<string> getTrainCostNames() const;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-08 19:13:29 UTC (rev 9750)
@@ -79,6 +79,20 @@
 {
 }
 
+void RegressionTreeNode::finalize(){
+    //those variable are not needed after training.
+    right_leave = 0;
+    left_leave = 0;
+    leave = 0;
+    //missing_leave used in computeOutputsAndNodes
+    if(right_node)
+        right_node->finalize();
+    if(left_node)
+        left_node->finalize();
+    if(missing_node)
+        missing_node->finalize();
+}
+
 void RegressionTreeNode::declareOptions(OptionList& ol)
 { 
     declareOption(ol, "missing_is_valid", &RegressionTreeNode::missing_is_valid, OptionBase::buildoption,

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-12-08 19:13:29 UTC (rev 9750)
@@ -100,6 +100,7 @@
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
+    void         finalize();
     void         initNode(PP<RegressionTree> tree,
                           PP<RegressionTreeLeave> leave);
     void         lookForBestSplit();

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-08 18:35:51 UTC (rev 9749)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-08 19:13:29 UTC (rev 9750)
@@ -130,6 +130,8 @@
     //! usefull in MultiClassAdaBoost to save memory
     void         setTSortedRow(TMat<RTR_type> t){tsorted_row = t;}
     TMat<RTR_type> getTSortedRow(){return tsorted_row;}
+    //to lower total memory as this is not needed anymore
+    virtual void finalize(){tsorted_row = TMat<RTR_type>();}
 
 private:
     void         build_();



From nouiz at mail.berlios.de  Mon Dec  8 20:56:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 20:56:00 +0100
Subject: [Plearn-commits] r9752 - in
	trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir:
	. Split0
Message-ID: <200812081956.mB8Ju0Zc002418@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 20:55:59 +0100 (Mon, 08 Dec 2008)
New Revision: 9752

Modified:
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
Log:
updated test following the finalize commit.


Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-12-08 19:15:15 UTC (rev 9751)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/Split0/final_learner.psave	2008-12-08 19:55:59 UTC (rev 9752)
@@ -255,32 +255,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *8 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *9 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *8 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *10 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 150 ;
-weights_sum = 1.00000000000000244 ;
-targets_sum = 112 ;
-weighted_targets_sum = 0.746666666666667589 ;
-weighted_squared_targets_sum = 0.746666666666667589 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.378311111111112819 0 0.378311111111112819 ] ;
 split_col = 2 ;
@@ -288,7 +266,7 @@
 split_feature_value = 0.00125079586853901747 ;
 after_split_error = 0.074181818181818418 ;
 missing_node = *0 ;
-missing_leave = *11 ->RegressionTreeLeave(
+missing_leave = *9 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -300,20 +278,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *12 ->RegressionTreeNode(
+left_node = *10 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *13 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.266666666666666441 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.048000000000000001 0 0.048000000000000001 ] ;
 split_col = 2 ;
@@ -321,7 +288,7 @@
 split_feature_value = 0.000357032461916012567 ;
 after_split_error = 0.0266666666666666684 ;
 missing_node = *0 ;
-missing_leave = *14 ->RegressionTreeLeave(
+missing_leave = *11 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -333,20 +300,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *15 ->RegressionTreeNode(
+left_node = *12 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *16 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 32 ;
-weights_sum = 0.21333333333333318 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -354,7 +310,7 @@
 split_feature_value = 0.113038628061597313 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *17 ->RegressionTreeLeave(
+missing_leave = *13 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -367,47 +323,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *18 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666665495 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *19 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 31 ;
-weights_sum = 0.206666666666666526 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *16  ;
-right_node = *20 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *14 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *21 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 8 ;
-weights_sum = 0.0533333333333333368 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0266666666666666684 0 0.0266666666666666684 ] ;
 split_col = 2 ;
@@ -415,7 +338,7 @@
 split_feature_value = 0.000981625552665510437 ;
 after_split_error = 0.0106666666666666646 ;
 missing_node = *0 ;
-missing_leave = *22 ->RegressionTreeLeave(
+missing_leave = *15 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -427,20 +350,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *23 ->RegressionTreeNode(
+left_node = *16 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *24 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
 split_col = 2 ;
@@ -448,7 +360,7 @@
 split_feature_value = 0.000528285193333644099 ;
 after_split_error = 0.00666666666666666449 ;
 missing_node = *0 ;
-missing_leave = *25 ->RegressionTreeLeave(
+missing_leave = *17 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -461,47 +373,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *26 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666666189 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666536 ;
-weighted_squared_targets_sum = 0.00666666666666666536 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *27 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0200000000000000004 ;
-weighted_squared_targets_sum = 0.0200000000000000004 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *24  ;
-right_node = *28 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *18 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *29 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0200000000000000004 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -509,7 +388,7 @@
 split_feature_value = 3.42448291945629535e-13 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *30 ->RegressionTreeLeave(
+missing_leave = *19 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -522,51 +401,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *31 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *32 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0133333333333333342 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *29   )
+right_leave = *0  )
 ;
-right_leave = *21   )
+right_leave = *0  )
 ;
-left_leave = *13  ;
-right_node = *33 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *20 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *34 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 110 ;
-weights_sum = 0.73333333333333417 ;
-targets_sum = 108 ;
-weighted_targets_sum = 0.720000000000000751 ;
-weighted_squared_targets_sum = 0.720000000000000751 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.026181818181818306 0 0.026181818181818306 ] ;
 split_col = 4 ;
@@ -574,7 +420,7 @@
 split_feature_value = 1.54709578481515564e-13 ;
 after_split_error = 0.0218181818181818199 ;
 missing_node = *0 ;
-missing_leave = *35 ->RegressionTreeLeave(
+missing_leave = *21 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -587,43 +433,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *36 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666668271 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666668271 ;
-weighted_squared_targets_sum = 0.00666666666666668271 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *37 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 109 ;
-weights_sum = 0.72666666666666746 ;
-targets_sum = 107 ;
-weighted_targets_sum = 0.713333333333334041 ;
-weighted_squared_targets_sum = 0.713333333333334041 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *34   )
+right_leave = *0  )
 ;
-priority_queue = *38 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *33  *15  *23  *28  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *10  ;
-split_cols = 3 [ 2 2 2 ] ;
-split_values = 3 [ 0.00125079586853901747 0.000357032461916012567 0.000981625552665510437 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 4 ;
@@ -642,10 +463,10 @@
 ] ;
 voting_weights = 1 [ 1.94591014905531323 ] ;
 sum_voting_weights = 1.94591014905531323 ;
-initial_sum_weights = 1 ;
+initial_sum_weights = 150 ;
 example_weights = 150 [ 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.0034013605442176956!
 2 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562!
  0.00340136054421769562 0.00340136054421769562 0.0034013605442!
 1769562 
0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.166666666666667018 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00!
 340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 0.00340136054421769562 ] ;
 learners_error = 1 [ 0.0200000000000000004 ] ;
-weak_learner_template = *39 ->RegressionTree(
+weak_learner_template = *22 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -653,7 +474,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *40 ->RegressionTreeLeave(
+leave_template = *23 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -678,7 +499,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 1 ;
 nstages = 4 ;
 report_progress = 1 ;
@@ -698,7 +519,7 @@
 save_often = 0 ;
 compute_training_error = 0 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 0 ;
+modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
@@ -706,7 +527,7 @@
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 1 ;
 report_progress = 1 ;
@@ -716,8 +537,8 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ;
-learner2 = *41 ->AdaBoost(
-weak_learners = 1 [ *42 ->RegressionTree(
+learner2 = *24 ->AdaBoost(
+weak_learners = 1 [ *25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 4 ;
@@ -725,32 +546,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *43 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *44 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *26 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *45 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 150 ;
-weights_sum = 1.00000000000000244 ;
-targets_sum = 74 ;
-weighted_targets_sum = 0.49333333333333268 ;
-weighted_squared_targets_sum = 0.49333333333333268 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.499911111111112305 0 0.499911111111112305 ] ;
 split_col = 2 ;
@@ -758,7 +557,7 @@
 split_feature_value = 0.991025168386145405 ;
 after_split_error = 0.173253056011676648 ;
 missing_node = *0 ;
-missing_leave = *46 ->RegressionTreeLeave(
+missing_leave = *27 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -770,20 +569,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *47 ->RegressionTreeNode(
+left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *48 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 87 ;
-weights_sum = 0.579999999999999849 ;
-targets_sum = 13 ;
-weighted_targets_sum = 0.0866666666666666696 ;
-weighted_squared_targets_sum = 0.0866666666666666696 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.147432950191570877 0 0.147432950191570877 ] ;
 split_col = 1 ;
@@ -791,7 +579,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.104535916061339731 ;
 missing_node = *0 ;
-missing_leave = *49 ->RegressionTreeLeave(
+missing_leave = *29 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -803,20 +591,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *50 ->RegressionTreeNode(
+left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *51 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 59 ;
-weights_sum = 0.393333333333332869 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666709 ;
-weighted_squared_targets_sum = 0.00666666666666666709 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.013107344632768362 0 0.013107344632768362 ] ;
 split_col = 3 ;
@@ -824,7 +601,7 @@
 split_feature_value = 0.924226804347039965 ;
 after_split_error = 0.00888888888888889062 ;
 missing_node = *0 ;
-missing_leave = *52 ->RegressionTreeLeave(
+missing_leave = *31 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -837,47 +614,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *53 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666668271 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *54 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 58 ;
-weights_sum = 0.386666666666666214 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666709 ;
-weighted_squared_targets_sum = 0.00666666666666666709 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *51  ;
-right_node = *55 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *32 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *56 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 28 ;
-weights_sum = 0.186666666666666564 ;
-targets_sum = 12 ;
-weighted_targets_sum = 0.0800000000000000017 ;
-weighted_squared_targets_sum = 0.0800000000000000017 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0914285714285713869 0 0.0914285714285713869 ] ;
 split_col = 2 ;
@@ -885,7 +629,7 @@
 split_feature_value = 0.891579732096156263 ;
 after_split_error = 0.0802318840579709924 ;
 missing_node = *0 ;
-missing_leave = *57 ->RegressionTreeLeave(
+missing_leave = *33 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -897,20 +641,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *58 ->RegressionTreeNode(
+left_node = *34 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *59 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.153333333333333294 ;
-targets_sum = 8 ;
-weighted_targets_sum = 0.0533333333333333368 ;
-weighted_squared_targets_sum = 0.0533333333333333368 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0695652173913043348 0 0.0695652173913043348 ] ;
 split_col = 2 ;
@@ -918,7 +651,7 @@
 split_feature_value = 0.808283414109232878 ;
 after_split_error = 0.0617543859649122839 ;
 missing_node = *0 ;
-missing_leave = *60 ->RegressionTreeLeave(
+missing_leave = *35 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -931,47 +664,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *61 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666665495 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666189 ;
-weighted_squared_targets_sum = 0.00666666666666666189 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *62 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 22 ;
-weights_sum = 0.14666666666666664 ;
-targets_sum = 7 ;
-weighted_targets_sum = 0.0466666666666666688 ;
-weighted_squared_targets_sum = 0.0466666666666666688 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *59  ;
-right_node = *63 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *64 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0333333333333333329 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0266666666666666684 ;
-weighted_squared_targets_sum = 0.0266666666666666684 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0106666666666666646 0 0.0106666666666666646 ] ;
 split_col = 2 ;
@@ -979,7 +679,7 @@
 split_feature_value = 0.982696507149771858 ;
 after_split_error = 0.00666666666666666709 ;
 missing_node = *0 ;
-missing_leave = *65 ->RegressionTreeLeave(
+missing_leave = *37 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -992,51 +692,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *66 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666666536 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666666536 ;
-weighted_squared_targets_sum = 0.00666666666666666536 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *67 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0266666666666666684 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0200000000000000004 ;
-weighted_squared_targets_sum = 0.0200000000000000004 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *64   )
+right_leave = *0  )
 ;
-right_leave = *56   )
+right_leave = *0  )
 ;
-left_leave = *48  ;
-right_node = *68 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *69 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 63 ;
-weights_sum = 0.419999999999999485 ;
-targets_sum = 61 ;
-weighted_targets_sum = 0.406666666666666177 ;
-weighted_squared_targets_sum = 0.406666666666666177 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0258201058201057432 0 0.0258201058201057432 ] ;
 split_col = 2 ;
@@ -1044,7 +711,7 @@
 split_feature_value = 0.997650553369808346 ;
 after_split_error = 0.0200000000000000039 ;
 missing_node = *0 ;
-missing_leave = *70 ->RegressionTreeLeave(
+missing_leave = *39 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1057,43 +724,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *71 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00666666666666665495 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00666666666666665495 ;
-weighted_squared_targets_sum = 0.00666666666666665495 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *72 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 62 ;
-weights_sum = 0.413333333333332831 ;
-targets_sum = 60 ;
-weighted_targets_sum = 0.399999999999999523 ;
-weighted_squared_targets_sum = 0.399999999999999523 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *69   )
+right_leave = *0  )
 ;
-priority_queue = *73 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 4 ;
-next_available_node = 4 ;
-nodes = 4 [ *58  *50  *68  *63  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *45  ;
-split_cols = 3 [ 2 1 2 ] ;
-split_values = 3 [ 0.991025168386145405 0.482293993618237549 0.891579732096156263 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 4 ;
@@ -1112,10 +754,10 @@
 ] ;
 voting_weights = 1 [ 1.22117351768460214 ] ;
 sum_voting_weights = 1.22117351768460214 ;
-initial_sum_weights = 1 ;
+initial_sum_weights = 150 ;
 example_weights = 150 [ 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971!
 132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579!
 71132 0.0036231884057971132 0.0036231884057971132 0.0036231884!
 05797113
2 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.00362318840579711!
 32 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0416666666666668031 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 0.0036231884057971132 ] ;
 learners_error = 1 [ 0.0800000000000000017 ] ;
-weak_learner_template = *74 ->RegressionTree(
+weak_learner_template = *40 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -1123,7 +765,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *75 ->RegressionTreeLeave(
+leave_template = *41 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -1148,7 +790,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 1 ;
 nstages = 4 ;
 report_progress = 1 ;
@@ -1168,7 +810,7 @@
 save_often = 0 ;
 compute_training_error = 0 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 0 ;
+modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
@@ -1176,7 +818,7 @@
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 1 ;
 report_progress = 1 ;
@@ -1187,7 +829,7 @@
 use_a_separate_random_generator_for_testing = 1827  )
 ;
 forward_sub_learner_test_costs = 1 ;
-learner_template = *76 ->AdaBoost(
+learner_template = *42 ->AdaBoost(
 weak_learners = []
 ;
 voting_weights = []
@@ -1198,7 +840,7 @@
 ;
 learners_error = []
 ;
-weak_learner_template = *77 ->RegressionTree(
+weak_learner_template = *43 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 3 ;
@@ -1206,7 +848,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 3 [ 0 1 2 ] ;
-leave_template = *78 ->RegressionTreeLeave(
+leave_template = *44 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -1268,8 +910,12 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
- )
 ;
+train_time = 0 ;
+total_train_time = 0 ;
+test_time = 0 ;
+total_test_time = 0  )
+;
 perf_evaluators = {};
 report_stats = 1 ;
 save_initial_tester = 0 ;
@@ -1296,10 +942,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *79 ->HyperOptimize(
+strategy = 1 [ *45 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *80 ->EarlyStoppingOracle(
+oracle = *46 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-12-08 19:15:15 UTC (rev 9751)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/metainfos.txt	2008-12-08 19:55:59 UTC (rev 9752)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL9656"
+__REVISION__ = "PL9749"
 conf                                          = False
 pseudo                                        = False
 tms                                           = 1

Modified: trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2008-12-08 19:15:15 UTC (rev 9751)
+++ trunk/plearn_learners/meta/test/MultiClassAdaBoost/.pytest/PL_MultiClassAdaBoost/expected_results/expdir/tester.psave	2008-12-08 19:55:59 UTC (rev 9752)
@@ -301,8 +301,12 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
- )
 ;
+train_time = 0 ;
+total_train_time = 0 ;
+test_time = 0 ;
+total_test_time = 0  )
+;
 perf_evaluators = {};
 report_stats = 1 ;
 save_initial_tester = 0 ;



From nouiz at mail.berlios.de  Mon Dec  8 23:33:46 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 8 Dec 2008 23:33:46 +0100
Subject: [Plearn-commits] r9753 - trunk/plearn_learners/meta
Message-ID: <200812082233.mB8MXkhA024498@sheep.berlios.de>

Author: nouiz
Date: 2008-12-08 23:33:45 +0100 (Mon, 08 Dec 2008)
New Revision: 9753

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
-made some static variable non static to be thread safe.
-put an existing PLERROR as a BOUCHCHECK


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-12-08 19:55:59 UTC (rev 9752)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-12-08 22:33:45 UTC (rev 9753)
@@ -355,16 +355,16 @@
                 " of the train_set must be know.");
 
 
-    static Vec input;
-    static Vec output;
-    static Vec target;
+    Vec input;
+    Vec output;
+    Vec target;
     real weight;
 
-    static Vec examples_error;
+    Vec examples_error;
 
     const int n = train_set.length();
-    static TVec<int> train_indices;
-    static Vec pseudo_loss;
+    TVec<int> train_indices;
+    Vec pseudo_loss;
 
     input.resize(inputsize());
     output.resize(weak_learner_template->outputsize());// We use only the first one as the output from the weak learner
@@ -738,9 +738,11 @@
 
     // First cost is negative log-likelihood...  output[0] is the likelihood
     // of the first class
+#ifdef BOUNDCHECK
     if (target.size() > 1)
         PLERROR("AdaBoost::computeCostsFromOutputs: target must contain "
                 "one element only: the 0/1 class");
+#endif
     if (fast_exact_is_equal(target[0], 0)) {
         costs[0] = output[0] >= output_threshold; 
     }



From nouiz at mail.berlios.de  Tue Dec  9 16:14:03 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 16:14:03 +0100
Subject: [Plearn-commits] r9754 - trunk/python_modules/plearn/parallel
Message-ID: <200812091514.mB9FE3Cf028609@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 16:14:02 +0100 (Tue, 09 Dec 2008)
New Revision: 9754

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
small refactor. now the kerberos script file is build in its own fct for simplification.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-12-08 22:33:45 UTC (rev 9753)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-12-09 15:14:02 UTC (rev 9754)
@@ -918,7 +918,12 @@
                     out.write(line_header()+
                               "renew the launch file "+self.launch_file+"\n")
                     out.flush()
-                    self.make_launch_script(bash_exec, True)
+                    launch_tmp_file=self.launch_file+".tmp"
+                    fd=open(launch_tmp_file,'w')
+                    self.make_kerb_script(fd,self.second_lauch_file)
+                    fd.close()
+                    os.chmod(launch_tmp_file, 0755)
+                    os.rename(launch_tmp_file, self.launch_file)
                 out.flush()
                 #we do this as in some case(with dagman) the log file can 
                 #take a few second to be created. So we don't loop too fast
@@ -932,16 +937,29 @@
             
             os.system("pkboost +d "+str(pid))
 
-    def make_launch_script(self, bash_exec, renew=False):
+    def make_kerb_script(self, fd, second_lauch_file):
+        fd.write(dedent('''\
+                    #!/bin/sh
+                    '''))
+        get=self.get_pkdilly_var()
             
+        for g in get:
+            fd.write("export "+g+"\n")
+        fd.write(dedent('''
+                export KRVEXECUTE=%s
+                /usr/sbin/circus "$@"
+                '''%(os.path.abspath(second_lauch_file))))
+
+    def make_launch_script(self, bash_exec):
+            
         #we write in a temp file then move it to be sure no jobs will 
         # read a partially writed file when we renew the file.
 
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
         overwrite_launch_file=False
-        if not os.path.exists(dbi_file) and not renew:
+        if not os.path.exists(dbi_file):
             print '[DBI] WARNING: Can\'t locate file "dbi.py". Maybe the file "'+self.launch_file+'" is not up to date!'
-        elif not renew:
+        else:
             if os.path.exists(self.launch_file):
                 mtimed=os.stat(dbi_file)[8]
                 mtimel=os.stat(self.launch_file)[8]
@@ -951,7 +969,7 @@
         if self.pkdilly:
             overwrite_launch_file = True
                     
-        if self.copy_local_source_file and not renew:
+        if self.copy_local_source_file:
             source_file_dest = os.path.join(self.log_dir,
                                             os.path.basename(self.source_file))
             shutil.copy( self.source_file, source_file_dest)
@@ -962,35 +980,28 @@
         launch_tmp_file=self.launch_file+".tmp"
         if not os.path.exists(self.launch_file) or overwrite_launch_file:
             self.temp_files.append(self.launch_file)
-            launch_fd = open(launch_tmp_file,'w')
-            fd=launch_fd
+            fd = open(launch_tmp_file,'w')
             
             if self.pkdilly:
-                second_lauch_file = self.launch_file+"2.sh"
-                    
+                self.second_lauch_file = self.launch_file+"2.sh"
+                self.make_kerb_script(fd, self.second_lauch_file)
+                fd.close()
+
+                fd = open(self.second_lauch_file,'w')
                 fd.write(dedent('''\
                     #!/bin/sh
                     '''))
-                get=self.get_pkdilly_var()
 
-                for g in get:
-                    launch_fd.write("export "+g+"\n")
-                launch_fd.write(dedent('''
-                export KRVEXECUTE=%s
-                /usr/sbin/circus "$@"
-                '''%(os.path.abspath(second_lauch_file))))
-                if not renew:
-                    fd=open(second_lauch_file,'w')
             bash=not self.source_file or not self.source_file.endswith(".cshrc")
-            if bash and not renew:
+            if bash:
                 fd.write(dedent('''\
                     #!/bin/sh
                     '''))
                 if self.condor_home:
                     fd.write('export HOME=%s\n' % self.condor_home)
-                fd.write('''
+                fd.write(dedent('''
                     cd %s
-                    '''%(os.path.abspath(".")))
+                    '''%(os.path.abspath("."))))
                 if self.source_file:
                     fd.write('source ' + self.source_file + '\n')
 
@@ -1007,7 +1018,7 @@
                     echo "Running: command: \\"$@\\"" 1>&2
                     %s
                     '''%(bash_exec)))
-            elif not renew:
+            else:
                 fd.write(dedent('''\
                     #!/bin/tcsh
                     '''))
@@ -1031,11 +1042,10 @@
                 echo "Running command: $argv"
                 $argv
                 '''))
-            if self.pkdilly and not renew:
-                fd.close()
-                os.chmod(second_lauch_file, 0755)
+            fd.close()
+            if self.pkdilly:
+                os.chmod(self.second_lauch_file, 0755)
 
-            launch_fd.close()
             os.chmod(launch_tmp_file, 0755)
             os.rename(launch_tmp_file, self.launch_file)
 
@@ -1218,7 +1228,6 @@
         condor_submit_fd.close()
 
         self.make_launch_script('sh -c "$@"')
-
         time.sleep(5)#we do this in hope that the error 'launch.sh2.sh is not executable'
 
         return self.condor_submit_exec + " " + self.condor_submit_file



From nouiz at mail.berlios.de  Tue Dec  9 16:17:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 16:17:54 +0100
Subject: [Plearn-commits] r9755 - trunk/python_modules/plearn/parallel
Message-ID: <200812091517.mB9FHsaC029187@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 16:17:54 +0100 (Tue, 09 Dec 2008)
New Revision: 9755

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
made dbidispatch more rebust to pkdilly output. As pkdilly do an ssh, we can have garbage at the beginnig resulting from data printed in script executed at login.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-12-09 15:14:02 UTC (rev 9754)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-12-09 15:17:54 UTC (rev 9755)
@@ -862,13 +862,17 @@
         self.p = Popen( cmd, shell=True, stdout=PIPE, stderr=PIPE)
         self.p.wait()
         assert self.p.stdout.readline()==""
-        err = self.p.stderr.readline()
+
 #example de sortie de pkdilly
 #La tache a soumettre est dans: /tmp/soumet_12368_Qbr7Av
-        if not err.startswith('La tache a soumettre est dans: '):
-            print "[DBI] ERROR: pkdilly returned a bad string:\n", err
+        pkdilly_file=""
+        for err in self.p.stderr.readlines():
+            if err.startswith('La tache a soumettre est dans: '):
+                pkdilly_file = err.split()[-1]
+        if not pkdilly_file:
+            print "[DBI] ERROR: pkdilly didn't returned a good string"
             sys.exit(1)
-        pkdilly_file = err.split()[-1]
+
         pkdilly_fd = open( pkdilly_file, 'r' )
         lines = pkdilly_fd.readlines()
         pkdilly_fd.close()



From nouiz at mail.berlios.de  Tue Dec  9 16:21:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 16:21:07 +0100
Subject: [Plearn-commits] r9756 - trunk/plearn_learners/regressors
Message-ID: <200812091521.mB9FL7cs029472@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 16:21:07 +0100 (Tue, 09 Dec 2008)
New Revision: 9756

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
finalize one more variable.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-09 15:17:54 UTC (rev 9755)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-09 15:21:07 UTC (rev 9756)
@@ -282,6 +282,8 @@
     first_leave = 0;
     if(sorted_train_set)
         sorted_train_set->finalize();
+    if(train_set->classname()=="RegressionTreeRegisters")
+        ((PP<RegressionTreeRegisters>)train_set)->finalize();
 }
 
 void RegressionTree::forget()



From nouiz at mail.berlios.de  Tue Dec  9 16:49:58 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 16:49:58 +0100
Subject: [Plearn-commits] r9757 - trunk/plearn_learners/regressors
Message-ID: <200812091549.mB9FnwN6032082@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 16:49:58 +0100 (Tue, 09 Dec 2008)
New Revision: 9757

Modified:
   trunk/plearn_learners/regressors/RegressionTree.h
Log:
fixed doxygen directive.


Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-12-09 15:21:07 UTC (rev 9756)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-12-09 15:49:58 UTC (rev 9757)
@@ -39,7 +39,7 @@
  * This file is part of the PLearn library.                                     *
  ******************************************************************************** */
 
-/*! \file PLearnLibrary/PLearnAlgo/RegressionTree.h */
+/*! \file RegressionTree.h */
 
 #ifndef RegressionTree_INC
 #define RegressionTree_INC



From nouiz at mail.berlios.de  Tue Dec  9 16:54:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 16:54:55 +0100
Subject: [Plearn-commits] r9758 - trunk/plearn/base
Message-ID: <200812091554.mB9FstYS032464@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 16:54:55 +0100 (Tue, 09 Dec 2008)
New Revision: 9758

Modified:
   trunk/plearn/base/ObjectGraphIterator.h
Log:
better comment for doxygen.


Modified: trunk/plearn/base/ObjectGraphIterator.h
===================================================================
--- trunk/plearn/base/ObjectGraphIterator.h	2008-12-09 15:49:58 UTC (rev 9757)
+++ trunk/plearn/base/ObjectGraphIterator.h	2008-12-09 15:54:55 UTC (rev 9758)
@@ -256,7 +256,7 @@
     }
 
     //! Return the option "pathname" to retrieve the option using
-    //! \getOption() on the root object.  This will be the empty
+    //! getOption() on the root object.  This will be the empty
     //! string for the root object, or if the iterator was
     //! constructed with the option compute_optnames set to 'false'.
     const string& getCurrentOptionName() const



From nouiz at mail.berlios.de  Tue Dec  9 17:14:33 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 17:14:33 +0100
Subject: [Plearn-commits] r9759 - trunk/plearn/misc
Message-ID: <200812091614.mB9GEXG5003865@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 17:14:33 +0100 (Tue, 09 Dec 2008)
New Revision: 9759

Modified:
   trunk/plearn/misc/PLearnService.cc
Log:
added fct comment and removed compiler warning.


Modified: trunk/plearn/misc/PLearnService.cc
===================================================================
--- trunk/plearn/misc/PLearnService.cc	2008-12-09 15:54:55 UTC (rev 9758)
+++ trunk/plearn/misc/PLearnService.cc	2008-12-09 16:14:33 UTC (rev 9759)
@@ -318,7 +318,7 @@
 }
 
 
-
+//! wait for one result
 PP<RemotePLearnServer> PLearnService::waitForResult(TVec< PP<RemotePLearnServer> > servers, 
                                 log_callback_t the_log_callback, 
                                 progress_callback_t the_progress_callback)
@@ -339,7 +339,7 @@
     int server= servers.length();
 
     //send results from reserved servers only even if polling all servers
-    while(server >= 0 && server < min_server || server == servers.length())
+    while((server >= 0 && server < min_server) || server == servers.length())
         server= watchServers(servers, the_log_callback, the_progress_callback);
 
     if(server < 0)



From nouiz at mail.berlios.de  Tue Dec  9 17:15:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 17:15:04 +0100
Subject: [Plearn-commits] r9760 - trunk/plearn/io
Message-ID: <200812091615.mB9GF4S8004044@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 17:15:04 +0100 (Tue, 09 Dec 2008)
New Revision: 9760

Modified:
   trunk/plearn/io/pl_streambuf.cc
Log:
removed compiler warning.


Modified: trunk/plearn/io/pl_streambuf.cc
===================================================================
--- trunk/plearn/io/pl_streambuf.cc	2008-12-09 16:14:33 UTC (rev 9759)
+++ trunk/plearn/io/pl_streambuf.cc	2008-12-09 16:15:04 UTC (rev 9760)
@@ -109,7 +109,7 @@
 
     //fill buffer from underlying streambuf
     for(int i= oldbuflen; i < inbuflen; ++i)
-        if(original_buf.sgetc() != pl_streambuf::eof && original_buf.in_avail() || i == oldbuflen)
+        if((original_buf.sgetc() != pl_streambuf::eof && original_buf.in_avail()) || i == oldbuflen)
             inbuf[i]= original_buf.sbumpc();  //< get a char from underlying streambuf and advance it's pos.
         else
         { //no input available: stop filling buffer (set egptr at current pos)



From nouiz at mail.berlios.de  Tue Dec  9 17:18:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 17:18:35 +0100
Subject: [Plearn-commits] r9761 - trunk/plearn_learners/online
Message-ID: <200812091618.mB9GIZe0004562@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 17:18:35 +0100 (Tue, 09 Dec 2008)
New Revision: 9761

Modified:
   trunk/plearn_learners/online/OnBagsModule.cc
Log:
removed compiler warning in opt mode.


Modified: trunk/plearn_learners/online/OnBagsModule.cc
===================================================================
--- trunk/plearn_learners/online/OnBagsModule.cc	2008-12-09 16:15:04 UTC (rev 9760)
+++ trunk/plearn_learners/online/OnBagsModule.cc	2008-12-09 16:18:35 UTC (rev 9761)
@@ -245,12 +245,12 @@
                 "Cannot resize input_gradients and accumulate into it." );
     else
     {
-        input_gradients.resize(inputs.length(), input_size);
+        input_gradients.resize(n_samples, input_size);
         input_gradients.fill(0);
     }
     bool bprop_wait_new_bag = true;
     int bag_start;
-    for (int j = 0; j < inputs.length(); j++) {
+    for (int j = 0; j < n_samples; j++) {
         int bag_info = int(round(bagtargets(j).lastElement()));
         if (bag_info & SumOverBagsVariable::TARGET_COLUMN_FIRST)
         {



From nouiz at mail.berlios.de  Tue Dec  9 17:52:36 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 17:52:36 +0100
Subject: [Plearn-commits] r9762 - in trunk/plearn_learners: meta regressors
Message-ID: <200812091652.mB9Gqaci008734@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 17:52:35 +0100 (Tue, 09 Dec 2008)
New Revision: 9762

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
-Memory optimization. Share common data between the the AdaBoost instance of an MultiClassAdaBoost.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-09 16:18:35 UTC (rev 9761)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-09 16:52:35 UTC (rev 9762)
@@ -218,6 +218,7 @@
 
 void MultiClassAdaBoost::train()
 {
+    EXTREME_MODULE_LOG<<"train() start"<<endl;
     Profiler::start("MultiClassAdaBoost::train");
 
     learner1->nstages = nstages;
@@ -238,6 +239,8 @@
       PLCHECK(learner1->weak_learner_template->report_progress==false);
       PLCHECK(learner2->weak_learner_template->report_progress==false);
     }
+    
+    EXTREME_MODULE_LOG<<"train() // start"<<endl;
 #pragma omp parallel sections default(none)
 {
 #pragma omp section 
@@ -245,6 +248,7 @@
 #pragma omp section 
     learner2->train();
 }
+    EXTREME_MODULE_LOG<<"train() // end"<<endl;
 #else
     learner1->train();
     learner2->train();
@@ -272,7 +276,8 @@
     const Profiler::Stats& stats_test = Profiler::getStats("MultiClassAdaBoost::test");
     tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
     test_time=tmp-total_test_time;
-    total_test_time=tmp;  
+    total_test_time=tmp; 
+    EXTREME_MODULE_LOG<<"train() end"<<endl;
 }
 
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
@@ -516,32 +521,28 @@
     }else
         weight_prg = "1 :weights";
         
-    bool tsorted = false;
-
     //We don't give it if the script give them one explicitly.
     //This can be usefull for optimization
     if(training_set_has_changed || !learner1->getTrainingSet()){
         VMat vmat1 = new ProcessingVMatrix(training_set, input_prg,
                                            target_prg1,  weight_prg);
         learner1->setTrainingSet(vmat1, call_forget);
-        tsorted = true;
     }
     if(training_set_has_changed || !learner2->getTrainingSet()){
         VMat vmat2 = new ProcessingVMatrix(training_set, input_prg,
                                            target_prg2,  weight_prg);
+        PP<RegressionTreeRegisters> t1 = 
+            (PP<RegressionTreeRegisters>)learner1->getTrainingSet();
+        if(t1->classname()=="RegressionTreeRegisters"){
+            vmat2 = new RegressionTreeRegisters(vmat2,
+                                               t1->getTSortedRow(), 
+                                               t1->getTSource(),
+                                               learner1->report_progress,
+                                               learner1->verbosity);
+        }
         learner2->setTrainingSet(vmat2, call_forget);
-        tsorted = true;
     }
 
-    if(tsorted &&
-       learner1->getTrainingSet()->classname()=="RegressionTreeRegisters"
-       && learner2->getTrainingSet()->classname()=="RegressionTreeRegisters")
-    {
-        TMat<RTR_type> t1 =  ((PP<RegressionTreeRegisters>)(learner1->getTrainingSet()))->getTSortedRow();
-        DBG_MODULE_LOG<<"removing duplicate tsorted_row in RegressionTreeRegisters to save memory"<<endl;
-        ((PP<RegressionTreeRegisters>)(learner2->getTrainingSet()))->setTSortedRow(t1);
-    }
-       
     //we do it here as RegressionTree need a trainingSet to know
     // the number of test.
     subcosts2.resize(learner2->nTestCosts());

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-12-09 16:18:35 UTC (rev 9761)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-12-09 16:52:35 UTC (rev 9762)
@@ -42,6 +42,7 @@
 #include "RegressionTreeRegisters.h"
 #include <plearn/vmat/TransposeVMatrix.h>
 #include <plearn/vmat/MemoryVMatrixNoSave.h>
+#include <plearn/vmat/SubVMatrix.h>
 #include <limits>
 
 namespace PLearn {
@@ -65,6 +66,8 @@
 }
 
 RegressionTreeRegisters::RegressionTreeRegisters(VMat source_,
+                                                 TMat<RTR_type> tsorted_row_,
+                                                 VMat tsource_,
                                                  bool report_progress_,
                                                  bool verbosity_):
     report_progress(report_progress_),
@@ -72,9 +75,22 @@
     next_id(0)
 {
     source = source_;
+    tsource = tsource_;
+    tsorted_row = tsorted_row_;
     build();
 }
 
+RegressionTreeRegisters::RegressionTreeRegisters(VMat source_,
+                                                 bool report_progress_,
+                                                 bool verbosity_):
+    report_progress(report_progress_),
+    verbosity(verbosity_),
+    next_id(0)
+{
+    source = source_;
+    build();
+}
+
 RegressionTreeRegisters::~RegressionTreeRegisters()
 {
 }
@@ -110,7 +126,6 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(tsorted_row, copies);
     deepCopyField(leave_register, copies);
-    deepCopyField(getExample_tmp, copies);
 //tsource should be deep copied, but as currently when it is deep copied
 // the copy is not used anymore to train. To save memory we don't do it.
 // It is deep copied eavily by HyperLearner and HyperOptimizer
@@ -134,13 +149,32 @@
     PLCHECK(source.length()>0 
             && (unsigned)source.length()
             <= std::numeric_limits<RTR_type>::max());
+    PLCHECK(source->targetsize()==1);
+    PLCHECK(source->weightsize()<=1);
+    PLCHECK(source->inputsize()>0);
 
     if(!tsource){
-        VMat tmp = VMat(new TransposeVMatrix(source));
+        VMat tmp = VMat(new TransposeVMatrix(new SubVMatrix(
+                                                 source, 0,0,source->length(),
+                                                 source->inputsize())));
         PP<MemoryVMatrixNoSave> tmp2 = new MemoryVMatrixNoSave(tmp);
         tsource = VMat(tmp2 );
-        setMetaInfoFrom(source);
     }
+    setMetaInfoFrom(source);
+    weightsize_=1;
+    targetsize_=1;
+    target_weight.resize(source->length());
+    if(source->weightsize()<=0){
+        width_++;
+        for(int i=0;i<source->length();i++){
+            target_weight[i].first=source->get(i,inputsize());
+            target_weight[i].second=1.0 / length();
+        }
+    }else
+        for(int i=0;i<source->length();i++){
+            target_weight[i].first=source->get(i,inputsize());
+            target_weight[i].second=source->get(i,inputsize()+targetsize());
+        }
     leave_register.resize(length());
     sortRows();
 }
@@ -161,21 +195,19 @@
     target.resize(reg.length());
     weight.resize(reg.length());
     value.resize(reg.length());
-    int idx_target=inputsize();
-    int idx_weight=inputsize() + targetsize();
     if(weightsize() <= 0){
         weight.fill(1.0 / length());
         for(int i=0;i<reg.length();i++){            
-            target[i] = tsource->get(idx_target, reg[i]);
+            target[i] = target_weight[reg[i]].first;
             value[i]  = tsource->get(col, reg[i]);
         }
     } else {
         //It is better to do multiple pass for memory access.
+        for(int i=0;i<reg.length();i++){
+            target[i] = target_weight[reg[i]].first;
+            weight[i] = target_weight[reg[i]].second;
+        }
         for(int i=0;i<reg.length();i++)
-            target[i] = tsource->get(idx_target, reg[i]);
-        for(int i=0;i<reg.length();i++)
-            weight[i] = tsource->get(idx_weight, reg[i]);
-        for(int i=0;i<reg.length();i++)
             value[i]  = tsource->get(col, reg[i]);
     }
 }
@@ -350,22 +382,10 @@
     if(weightsize()<0)
         PLERROR("In RegressionTreeRegisters::getExample, weightsize_ not defined for this vmat");
 #endif
-    input.resize(inputsize_);
-    getExample_tmp.resize(width());
-    tsource->getColumn(i,getExample_tmp);
-    input << getExample_tmp.subVec(0,input.size());
+    tsource->getColumn(i,input);
 
-    target.resize(targetsize_);
-    if (targetsize_ > 0) {
-        target<<getExample_tmp.subVec(inputsize(),targetsize());
-    }
-
-    if(weightsize()==0)
-        weight = 1;
-    else if(weightsize()>1)
-        PLERROR("In VMatrix::getExample, weightsize_ >1 not supported by this call");
-    else
-        weight = tsource->get(inputsize()+targetsize(),i);
+    target[0]=target_weight[i].first;
+    weight = target_weight[i].second;
 }
 
 

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-09 16:18:35 UTC (rev 9761)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-09 16:52:35 UTC (rev 9762)
@@ -81,6 +81,9 @@
     TMat<RTR_type> tsorted_row;
     TVec<RTR_type_id> leave_register;
     VMat tsource;
+    //we put it in pair instead of two vector to speed up
+    //the getAllRegisteredRow(leave_id, col, reg, target, weight, value) fct
+    TVec<pair<real,real> > target_weight;
     VMat source;
 
 public:
@@ -88,6 +91,9 @@
     RegressionTreeRegisters();
     RegressionTreeRegisters(VMat source_, bool report_progress_ = false,
                             bool vebosity_ = false);
+    RegressionTreeRegisters(VMat source_, TMat<RTR_type> tsorted_row_,
+                            VMat tsource_, bool report_progress_ = false,
+                            bool vebosity_ = false);
     virtual              ~RegressionTreeRegisters();
     
     PLEARN_DECLARE_OBJECT(RegressionTreeRegisters);
@@ -98,17 +104,18 @@
     void         reinitRegisters();
     inline void         registerLeave(RTR_type_id leave_id, int row)
     { leave_register[row] = leave_id;    }
-    inline virtual real get(int i, int j) const{return tsource->get(j,i);}
+    inline virtual real get(int i, int j) const{
+        if(j<inputsize())return tsource->get(j,i);
+        if(j==inputsize())return target_weight[i].first;
+        else  return target_weight[i].second;
+    }
     inline real         getTarget(int row)const
-    {return tsource->get(inputsize(), row);}
+    {return target_weight[row].first;}
     inline real         getWeight(int row)const{
-        if (weightsize() <= 0) return 1.0 / length();
-        else return tsource->get(inputsize() + targetsize(), row );
+        return target_weight[row].second;
     }
     inline void         setWeight(int row,real val){
-        PLASSERT(inputsize()>0&&targetsize()>0);
-        PLASSERT(weightsize() > 0);
-        tsource->put( inputsize() + targetsize(), row, val );
+        target_weight[row].second = val;
     }
     inline RTR_type_id     getNextId(){
         PLCHECK(next_id<std::numeric_limits<RTR_type_id>::max());
@@ -128,9 +135,8 @@
     }
     
     //! usefull in MultiClassAdaBoost to save memory
-    void         setTSortedRow(TMat<RTR_type> t){tsorted_row = t;}
     TMat<RTR_type> getTSortedRow(){return tsorted_row;}
-    //to lower total memory as this is not needed anymore
+    VMat          getTSource(){return tsource;}
     virtual void finalize(){tsorted_row = TMat<RTR_type>();}
 
 private:
@@ -142,8 +148,6 @@
     real         compare(real field1, real field2);
     void         verbose(string msg, int level);
 
-    mutable Vec  getExample_tmp;
-
 };
 
 DECLARE_OBJECT_PTR(RegressionTreeRegisters);



From nouiz at mail.berlios.de  Tue Dec  9 19:29:51 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 19:29:51 +0100
Subject: [Plearn-commits] r9763 - trunk/scripts
Message-ID: <200812091829.mB9ITpus016467@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 19:29:50 +0100 (Tue, 09 Dec 2008)
New Revision: 9763

Modified:
   trunk/scripts/collectres
Log:
-added the option -out_sep that tell the separator to use.
-added the --sep option that is the same as --separator


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2008-12-09 16:52:35 UTC (rev 9762)
+++ trunk/scripts/collectres	2008-12-09 18:29:50 UTC (rev 9763)
@@ -58,6 +58,8 @@
 
 # should probably be an option
 separator = "_"
+output_separator = " "
+
 def get_col_index(fieldnames,colspec):
   if colspec in fieldnames:
     index=fieldnames.index(colspec)
@@ -195,13 +197,13 @@
         minfile=res[1]
         selected=res[0]
     for v in selected:
-      f.write(str(v)+" ")
+      f.write(str(v)+output_separator)
     f.write(minfile+"\n")
   elif mode=="sort":
     results.sort(compare_res)
     for res in results:
       for v in res[0]:
-        f.write(str(v)+" ")
+        f.write(str(v)+output_separator)
       f.write(res[1]+"\n")
   elif mode=="plot":
     # "plot cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ..." : make a smartplot file with <col1> as y-columns (with legend label <namei>), <x> as x-column (-1 means use the row indices). Axes are labeled <xlabel> and <ylabel>. Use rows from <minrow> to <maxrow> (-1 means last) row inclusively.'
@@ -263,7 +265,7 @@
 if __name__=='__main__':
   args = sys.argv[:]
   if len(args)<=3:
-    print "Usage: collectres [--no_printcommand] [--verbose={0,1}] [--separator=X] <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
+    print "Usage: collectres [--no_printcommand] [--verbose={0,1}] [--{sep,separator}=X] [--out_sep={X,tab}] <outputfile> <spec> <file1.pmat> <file2.pmat> ..."
     print 
     print "File formats pmat, amat and csv are supported."
     print "The <spec> can be the following (note how the <spec> has to be surrounded by quotes):"
@@ -297,7 +299,12 @@
   if args[1].startswith("--verbose="):
     verbose=int(args[1].split("=",1)[1])
     del args[1]
-  if args[1].startswith("--separator="):
+  if args[1].startswith("--separator=") or args[1].startswith("--sep=") :
     separator=args[1].split("=",1)[1]
+  if args[1].startswith("--out_sep="):
+    output_separator=args[1].split("=",1)[1]
+    if output_separator == "tab":
+      output_separator = "\t"
+    print "output_separator",output_separator
     del args[1]
   collectres(args[1],args[2],args[3:],printcommand,verbose)



From nouiz at mail.berlios.de  Tue Dec  9 19:45:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 19:45:48 +0100
Subject: [Plearn-commits] r9764 - trunk/scripts
Message-ID: <200812091845.mB9Ijmxi007032@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 19:45:48 +0100 (Tue, 09 Dec 2008)
New Revision: 9764

Modified:
   trunk/scripts/collectres
Log:
put global variable as argument to function.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2008-12-09 18:29:50 UTC (rev 9763)
+++ trunk/scripts/collectres	2008-12-09 18:45:48 UTC (rev 9764)
@@ -56,10 +56,6 @@
 from plearn.vmat.smartReadMat import *
 from numpy import *
 
-# should probably be an option
-separator = "_"
-output_separator = " "
-
 def get_col_index(fieldnames,colspec):
   if colspec in fieldnames:
     index=fieldnames.index(colspec)
@@ -165,7 +161,7 @@
       f.write(str(a[r,c])+" ")
     f.write("\n")
 
-def distinct_experiment_names(filenames):
+def distinct_experiment_names(filenames, separator):
   splitted_filenames = map(lambda fname: string.split(fname.replace("/",separator),separator), filenames)
   distinct_names = []
   for splitted_filename in splitted_filenames:
@@ -181,7 +177,7 @@
     distinct_names.append(string.join(different_parts,separator))
   return distinct_names
 
-def outputres(f,mode,speclist,results):
+def outputres(f, mode, speclist, results, separator, output_separator = " "):
   if not results:
     print "ERROR: no results selected!"
     sys.exit(0)
@@ -210,7 +206,7 @@
     #
     # build the gnuplot command in the smartplot file header
     filenames = map(lambda res: res[1], results) # collect the filenames from the results list
-    distinct_expnames = distinct_experiment_names(filenames) # keep only the option=value parts that are distinct among experiments
+    distinct_expnames = distinct_experiment_names(filenames, separator) # keep only the option=value parts that are distinct among experiments
     f.write('# to set y on log-scale insert this at the beginning below: set logscale y;\n')
     # speclist has the elements: cols <xlabel> <ylabel> <minrow> <maxrow> <x> <col1> <name1> <col2> <name2> ...
     f.write('#: set xlabel "'+speclist[1]+'"; set ylabel "'+speclist[2]+'"; plot ')
@@ -248,7 +244,8 @@
   else:
     raise ValueError("Invalid <spec> mode, expected 'min', 'sort', or 'plot', got "+mode)
 
-def collectres(outputfile,speclist,filenames,printcommand=True,verbose=1):
+def collectres(outputfile,speclist,filenames,printcommand=True,verbose=1,
+               separator="_", output_separator = " "):
   specs= string.split(speclist)
   mode = specs[0]
   f=open(outputfile,"w")
@@ -258,7 +255,7 @@
       f.write(file+" ")
   f.write("\n")
   f.write("# "+speclist + "\n")
-  outputres(f,mode,specs[1:],getres(specs[1:],filenames,verbose))
+  outputres(f,mode,specs[1:],getres(specs[1:],filenames,verbose), separator, output_separator)
   f.flush()
   f.close()
   
@@ -293,6 +290,9 @@
     sys.exit(1)
   printcommand=True
   verbose=1
+  output_separator = " "
+  separator = "_"
+
   if args[1]=="--no_printcommand":
     printcommand=False
     del args[1]
@@ -307,4 +307,5 @@
       output_separator = "\t"
     print "output_separator",output_separator
     del args[1]
-  collectres(args[1],args[2],args[3:],printcommand,verbose)
+
+  collectres(args[1],args[2],args[3:], printcommand, verbose, separator, output_separator)



From nouiz at mail.berlios.de  Tue Dec  9 21:31:39 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 21:31:39 +0100
Subject: [Plearn-commits] r9765 - in trunk: plearn/vmat
	plearn_learners/online
Message-ID: <200812092031.mB9KVdE5019123@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 21:31:38 +0100 (Tue, 09 Dec 2008)
New Revision: 9765

Modified:
   trunk/plearn/vmat/BootstrapVMatrix.cc
   trunk/plearn_learners/online/CrossEntropyCostModule.cc
   trunk/plearn_learners/online/NLLCostModule.cc
Log:
-fix as PLASSERT are controled by NDEBUG and not BOUNDCHECK.
-this remove useless compiler warning.


Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2008-12-09 18:45:48 UTC (rev 9764)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2008-12-09 20:31:38 UTC (rev 9765)
@@ -181,7 +181,7 @@
             real weight;
             for (int i = 0; i < l; i++) {
                 source->getExample(i, input, target, weight);
-#ifdef BOUNDCHECK
+#ifndef NDEBUG
                 // Safety checks on bag information.
                 real bi = target.lastElement();
                 PLASSERT( is_equal(round(bi), bi) );

Modified: trunk/plearn_learners/online/CrossEntropyCostModule.cc
===================================================================
--- trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-12-09 18:45:48 UTC (rev 9764)
+++ trunk/plearn_learners/online/CrossEntropyCostModule.cc	2008-12-09 20:31:38 UTC (rev 9765)
@@ -136,7 +136,7 @@
 
     Mat* prediction = ports_value[0];
     Mat* target = ports_value[1];
-#ifdef BOUNDCHECK
+#ifndef NDEBUG
     Mat* cost = ports_value[2];
 #endif
     Mat* prediction_grad = ports_gradient[0];

Modified: trunk/plearn_learners/online/NLLCostModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLCostModule.cc	2008-12-09 18:45:48 UTC (rev 9764)
+++ trunk/plearn_learners/online/NLLCostModule.cc	2008-12-09 20:31:38 UTC (rev 9765)
@@ -241,7 +241,7 @@
 
     Mat* prediction = ports_value[0];
     Mat* target = ports_value[1];
-#ifdef BOUNDCHECK
+#ifndef NDEBUG
     Mat* cost = ports_value[2];
 #endif
     Mat* prediction_grad = ports_gradient[0];



From nouiz at mail.berlios.de  Tue Dec  9 21:55:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 21:55:14 +0100
Subject: [Plearn-commits] r9766 - trunk/plearn/vmat
Message-ID: <200812092055.mB9KtEcH020507@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 21:55:14 +0100 (Tue, 09 Dec 2008)
New Revision: 9766

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
-changed a PLERROR to a NORMAL_LOG as in some case we don't want an error
-one warning is printed less often
-better warning msg.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-12-09 20:31:38 UTC (rev 9765)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-12-09 20:55:14 UTC (rev 9766)
@@ -43,6 +43,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/io/load_and_save.h>
 #include <plearn/io/fileutils.h>
+#include <plearn/io/pl_log.h>
 
 namespace PLearn {
 using namespace std;
@@ -281,9 +282,8 @@
                 }
             }
             if(!expended)
-                PLERROR("In TextFilesVMatrix::setColumnNamesAndWidth - "
-                        "Don't have find any partial match to %s",
-                        fieldspec[i].first.c_str());
+                NORMAL_LOG<<"In TextFilesVMatrix::setColumnNamesAndWidth - "
+                    "Don't have find any partial match to "<<fieldspec[i].first;
         }
         fieldspec = new_fieldspec;
     }
@@ -319,7 +319,8 @@
                 not_used_fs.append(name);
         }
         //check that we have the good number of fieldspec
-        if(fnames_header.size()!=fieldspec.size())
+        //if partial match is true, we don't want to generate the warning everytime
+        if(fnames_header.size()!=fieldspec.size() && !partial_match)
         {
             PLWARNING("In TextFilesVMatrix::setColumnNamesAndWidth() - "
                     "We read %d field names from the header but have %d"
@@ -328,7 +329,7 @@
 
         if(not_used_fs.size()!=0)
             PLWARNING("TextFilesVMatrix::setColumnNamesAndWidth() - "
-                      "Fieldspecs do not exist in source for field(s): %s\n"
+                      "Fieldspecs exists for field(s) that are not in the source: %s\n"
                       "They will be skipped.",
                       tostring(not_used_fs).c_str());
         if(not_used_fn.size()!=0)



From nouiz at mail.berlios.de  Tue Dec  9 23:01:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 9 Dec 2008 23:01:50 +0100
Subject: [Plearn-commits] r9767 - trunk/plearn/misc
Message-ID: <200812092201.mB9M1oDS027467@sheep.berlios.de>

Author: nouiz
Date: 2008-12-09 23:01:49 +0100 (Tue, 09 Dec 2008)
New Revision: 9767

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
-added a check for the number of argument


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-12-09 20:55:14 UTC (rev 9766)
+++ trunk/plearn/misc/vmatmain.cc	2008-12-09 22:01:49 UTC (rev 9767)
@@ -973,6 +973,9 @@
     }
     else if(command=="diff")
     {
+        if(argc < 4)
+            PLERROR("'vmat diff' must be used that way : vmat diff <dataset1> <dataset2> [<tolerance> [<verbose>]]");
+
         VMat vm1 = getVMat(argv[2], indexf);
         VMat vm2 = getVMat(argv[3], indexf);
         double tol = 1e-6;
@@ -1414,6 +1417,8 @@
     }
     else if(command=="mtime")
     {    
+        if(argc!=3)
+            PLERROR("The command 'vmat mtime' must be used that way: vmat mtime <dataset>");
         VMat m1 = getVMat(argv[2], indexf);
         pout<<m1->getMtime()<<endl;
     }



From nouiz at mail.berlios.de  Wed Dec 10 15:58:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 10 Dec 2008 15:58:57 +0100
Subject: [Plearn-commits] r9768 - in
	trunk/plearn_learners/meta/test/AdaBoost/.pytest:
	PL_AdaBoost_base/expected_results/expdir
	PL_AdaBoost_base/expected_results/expdir/Split0
	PL_AdaBoost_base/expected_results/expdir/Split0/LearnerExpdir
	PL_AdaBoost_conf_rated_adaboost/expected_results/expdir
	PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0
	PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir
	PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0
	PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/LearnerExpdir
Message-ID: <200812101458.mBAEwv0T021450@sheep.berlios.de>

Author: nouiz
Date: 2008-12-10 15:58:55 +0100 (Wed, 10 Dec 2008)
New Revision: 9768

Modified:
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
Log:
updated test following the PLearn::finalise() function addition.


Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/final_learner.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -239,32 +239,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *7 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *8 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *7 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *9 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 159 ;
-weights_sum = 0.999999999999999889 ;
-targets_sum = 99 ;
-weighted_targets_sum = 0.622641509433962237 ;
-weighted_squared_targets_sum = 0.622641509433962237 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.469918120327518563 0 0.469918120327518563 ] ;
 split_col = 2 ;
@@ -272,7 +250,7 @@
 split_feature_value = 0.188677163392186542 ;
 after_split_error = 0.165982151128678812 ;
 missing_node = *0 ;
-missing_leave = *10 ->RegressionTreeLeave(
+missing_leave = *8 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -284,20 +262,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *11 ->RegressionTreeNode(
+left_node = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *12 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 65 ;
-weights_sum = 0.408805031446540845 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.0628930817610062892 ;
-weighted_squared_targets_sum = 0.0628930817610062892 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.106434446057087573 0 0.106434446057087573 ] ;
 split_col = 4 ;
@@ -305,7 +272,7 @@
 split_feature_value = 8.88178419700125232e-16 ;
 after_split_error = 0.0733752620545073397 ;
 missing_node = *0 ;
-missing_leave = *13 ->RegressionTreeLeave(
+missing_leave = *10 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -317,20 +284,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *14 ->RegressionTreeNode(
+left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *15 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.257861635220125784 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -338,7 +294,7 @@
 split_feature_value = 0.185353117285409069 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeLeave(
+missing_leave = *12 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -351,47 +307,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *17 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610067958 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *18 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.251572327044025157 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *15  ;
-right_node = *19 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *13 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *20 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.150943396226415089 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.0628930817610062892 ;
-weighted_squared_targets_sum = 0.0628930817610062892 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0733752620545073397 0 0.0733752620545073397 ] ;
 split_col = 4 ;
@@ -399,7 +322,7 @@
 split_feature_value = 3.44335671087492301e-13 ;
 after_split_error = 0.0528301886792452852 ;
 missing_node = *0 ;
-missing_leave = *21 ->RegressionTreeLeave(
+missing_leave = *14 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -411,20 +334,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *22 ->RegressionTreeNode(
+left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *23 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0251572327044025171 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0251572327044025171 ;
-weighted_squared_targets_sum = 0.0251572327044025171 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -432,7 +344,7 @@
 split_feature_value = 2.63677968348474678e-15 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeLeave(
+missing_leave = *16 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -445,55 +357,22 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *25 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.006289308176100631 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.006289308176100631 ;
-weighted_squared_targets_sum = 0.006289308176100631 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *26 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0188679245283018895 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0188679245283018895 ;
-weighted_squared_targets_sum = 0.0188679245283018895 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *23  ;
-right_node = *27 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *17 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *28 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.125786163522012578 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0377358490566037721 ;
-weighted_squared_targets_sum = 0.0377358490566037721 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0528301886792452852 0 0.0528301886792452852 ] ;
 split_col = 2 ;
 split_balance = 10 ;
 split_feature_value = 0.0166672042033834122 ;
-after_split_error = 0.045283018867924546 ;
+after_split_error = 0.0452830188679245321 ;
 missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
+missing_leave = *18 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -506,51 +385,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *30 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610065182 ;
-targets_sum = 0 ;
-weighted_targets_sum = 1.73472347597680709e-18 ;
-weighted_squared_targets_sum = 1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *31 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.119496855345911937 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0377358490566037721 ;
-weighted_squared_targets_sum = 0.0377358490566037721 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *28   )
+right_leave = *0  )
 ;
-right_leave = *20   )
+right_leave = *0  )
 ;
-left_leave = *12  ;
-right_node = *32 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *19 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *33 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.591194968553459099 ;
-targets_sum = 89 ;
-weighted_targets_sum = 0.559748427672955962 ;
-weighted_squared_targets_sum = 0.559748427672955962 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0595477050715911282 0 0.0595477050715911282 ] ;
 split_col = 3 ;
@@ -558,7 +404,7 @@
 split_feature_value = 0.170219082142077621 ;
 after_split_error = 0.0448775543115166875 ;
 missing_node = *0 ;
-missing_leave = *34 ->RegressionTreeLeave(
+missing_leave = *20 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -570,20 +416,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *35 ->RegressionTreeNode(
+left_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *36 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0188679245283018895 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610062927 ;
-weighted_squared_targets_sum = 0.00628930817610062927 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.00838574423480083903 0 0.00838574423480083903 ] ;
 split_col = 3 ;
@@ -591,7 +426,7 @@
 split_feature_value = 0.141699018667860999 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
+missing_leave = *22 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -604,47 +439,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *38 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.006289308176100631 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *39 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0125786163522012585 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610062927 ;
-weighted_squared_targets_sum = 0.00628930817610062927 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *36  ;
-right_node = *40 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *23 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *41 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 91 ;
-weights_sum = 0.572327044025157217 ;
-targets_sum = 88 ;
-weighted_targets_sum = 0.553459119496855334 ;
-weighted_squared_targets_sum = 0.553459119496855334 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0364918100767157583 0 0.0364918100767157583 ] ;
 split_col = 1 ;
@@ -652,7 +454,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0332075471698113078 ;
 missing_node = *0 ;
-missing_leave = *42 ->RegressionTreeLeave(
+missing_leave = *24 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -665,45 +467,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *43 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610073509 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610073509 ;
-weighted_squared_targets_sum = 0.00628930817610073509 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *44 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 90 ;
-weights_sum = 0.566037735849056589 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.547169811320754707 ;
-weighted_squared_targets_sum = 0.547169811320754707 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *41   )
+right_leave = *0  )
 ;
-right_leave = *33   )
+right_leave = *0  )
 ;
-priority_queue = *45 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *35  *27  *22  *14  *40  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *9  ;
-split_cols = 4 [ 2 4 4 3 ] ;
-split_values = 4 [ 0.188677163392186542 8.88178419700125232e-16 3.44335671087492301e-13 0.170219082142077621 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -719,7 +496,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*46 ->RegressionTree(
+*25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -727,32 +504,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *47 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *48 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *26 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *49 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.999999999999998779 ;
-targets_sum = 93 ;
-weighted_targets_sum = 0.624161073825502677 ;
-weighted_squared_targets_sum = 0.624161073825502677 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.469168055492995228 0 0.469168055492995228 ] ;
 split_col = 1 ;
@@ -760,7 +515,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.260686628807433929 ;
 missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
+missing_leave = *27 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -772,20 +527,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *51 ->RegressionTreeNode(
+left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *52 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 45 ;
-weights_sum = 0.302013422818791732 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0402684563758389236 ;
-weighted_squared_targets_sum = 0.0402684563758389236 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.069798657718120799 0 0.069798657718120799 ] ;
 split_col = 2 ;
@@ -793,7 +537,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.0130872483221476481 ;
 missing_node = *0 ;
-missing_leave = *53 ->RegressionTreeLeave(
+missing_leave = *29 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -805,20 +549,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *54 ->RegressionTreeNode(
+left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *55 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.268456375838926009 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315422 ;
-weighted_squared_targets_sum = 0.00671140939597315422 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0130872483221476498 0 0.0130872483221476498 ] ;
 split_col = 1 ;
@@ -826,7 +559,7 @@
 split_feature_value = 0.414475818795681961 ;
 after_split_error = 0.0120805369127516774 ;
 missing_node = *0 ;
-missing_leave = *56 ->RegressionTreeLeave(
+missing_leave = *31 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -839,47 +572,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *57 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *58 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 39 ;
-weights_sum = 0.261744966442952864 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315422 ;
-weighted_squared_targets_sum = 0.00671140939597315422 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *55  ;
-right_node = *59 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *32 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *60 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.033557046979865772 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.033557046979865772 ;
-weighted_squared_targets_sum = 0.033557046979865772 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -887,7 +587,7 @@
 split_feature_value = 1.66171551518878857e-09 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *61 ->RegressionTreeLeave(
+missing_leave = *33 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -900,49 +600,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *62 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315248 ;
-weighted_squared_targets_sum = 0.00671140939597315248 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *63 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0268456375838926169 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0268456375838926169 ;
-weighted_squared_targets_sum = 0.0268456375838926169 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *60   )
+right_leave = *0  )
 ;
-left_leave = *52  ;
-right_node = *64 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *34 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *65 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 104 ;
-weights_sum = 0.697986577181207268 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.583892617449663809 ;
-weighted_squared_targets_sum = 0.583892617449663809 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.190887971089313102 0 0.190887971089313102 ] ;
 split_col = 4 ;
@@ -950,7 +617,7 @@
 split_feature_value = 0.999999999538717876 ;
 after_split_error = 0.145761307736665707 ;
 missing_node = *0 ;
-missing_leave = *66 ->RegressionTreeLeave(
+missing_leave = *35 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -962,20 +629,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *67 ->RegressionTreeNode(
+left_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *68 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.248322147651006547 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.147651006711409377 ;
-weighted_squared_targets_sum = 0.147651006711409377 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.119717032468710211 0 0.119717032468710211 ] ;
 split_col = 4 ;
@@ -983,7 +639,7 @@
 split_feature_value = 0.725372806051693186 ;
 after_split_error = 0.033557046979865772 ;
 missing_node = *0 ;
-missing_leave = *69 ->RegressionTreeLeave(
+missing_leave = *37 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -995,20 +651,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *70 ->RegressionTreeNode(
+left_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *71 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.127516778523489943 ;
-targets_sum = 19 ;
-weighted_targets_sum = 0.127516778523489943 ;
-weighted_squared_targets_sum = 0.127516778523489943 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -1016,7 +661,7 @@
 split_feature_value = 0.00609705032829710447 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *72 ->RegressionTreeLeave(
+missing_leave = *39 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1029,47 +674,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *73 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0067114093959731386 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0067114093959731386 ;
-weighted_squared_targets_sum = 0.0067114093959731386 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *74 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 18 ;
-weights_sum = 0.120805369127516798 ;
-targets_sum = 18 ;
-weighted_targets_sum = 0.120805369127516798 ;
-weighted_squared_targets_sum = 0.120805369127516798 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *71  ;
-right_node = *75 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *40 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *76 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 18 ;
-weights_sum = 0.120805369127516798 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0201342281879194618 ;
-weighted_squared_targets_sum = 0.0201342281879194618 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.033557046979865772 0 0.033557046979865772 ] ;
 split_col = 4 ;
@@ -1077,7 +689,7 @@
 split_feature_value = 0.999935792696207582 ;
 after_split_error = 0.0100671140939597309 ;
 missing_node = *0 ;
-missing_leave = *77 ->RegressionTreeLeave(
+missing_leave = *41 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1090,49 +702,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *78 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *79 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.11409395973154364 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0201342281879194618 ;
-weighted_squared_targets_sum = 0.0201342281879194618 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *76   )
+right_leave = *0  )
 ;
-left_leave = *68  ;
-right_node = *80 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *42 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *81 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 67 ;
-weights_sum = 0.449664429530200915 ;
-targets_sum = 65 ;
-weighted_targets_sum = 0.436241610738254626 ;
-weighted_squared_targets_sum = 0.436241610738254626 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0260442752679554967 0 0.0260442752679554967 ] ;
 split_col = 3 ;
@@ -1140,7 +719,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0132194427496440392 ;
 missing_node = *0 ;
-missing_leave = *82 ->RegressionTreeLeave(
+missing_leave = *43 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1153,45 +732,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *83 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597316636 ;
-weighted_squared_targets_sum = 0.00671140939597316636 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *84 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.442953020134227771 ;
-targets_sum = 64 ;
-weighted_targets_sum = 0.429530201342281481 ;
-weighted_squared_targets_sum = 0.429530201342281481 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *81   )
+right_leave = *0  )
 ;
-right_leave = *65   )
+right_leave = *0  )
 ;
-priority_queue = *85 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *75  *80  *54  *70  *59  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *49  ;
-split_cols = 4 [ 1 2 4 4 ] ;
-split_values = 4 [ 0.482293993618237549 0.885239426681956321 0.999999999538717876 0.725372806051693186 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -1207,7 +761,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*86 ->RegressionTree(
+*44 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1215,32 +769,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *87 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *88 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *45 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *89 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 173 ;
-weights_sum = 1.00000000000000422 ;
-targets_sum = 91 ;
-weighted_targets_sum = 0.526011560693641633 ;
-weighted_squared_targets_sum = 0.526011560693641633 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.498646797420564281 0 0.498646797420564281 ] ;
 split_col = 2 ;
@@ -1248,7 +780,7 @@
 split_feature_value = 0.995245802370935517 ;
 after_split_error = 0.376402584155050013 ;
 missing_node = *0 ;
-missing_leave = *90 ->RegressionTreeLeave(
+missing_leave = *46 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1260,20 +792,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *91 ->RegressionTreeNode(
+left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *92 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 136 ;
-weights_sum = 0.786127167630060186 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.376402584155050013 0 0.376402584155050013 ] ;
 split_col = 2 ;
@@ -1281,7 +802,7 @@
 split_feature_value = 0.994286124455373455 ;
 after_split_error = 0.31500238638171546 ;
 missing_node = *0 ;
-missing_leave = *93 ->RegressionTreeLeave(
+missing_leave = *48 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1293,20 +814,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *94 ->RegressionTreeNode(
+left_node = *49 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *95 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 109 ;
-weights_sum = 0.630057803468209054 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.31500238638171546 0 0.31500238638171546 ] ;
 split_col = 4 ;
@@ -1314,7 +824,7 @@
 split_feature_value = 8.32667268468867405e-16 ;
 after_split_error = 0.24550237059167368 ;
 missing_node = *0 ;
-missing_leave = *96 ->RegressionTreeLeave(
+missing_leave = *50 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1326,20 +836,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *97 ->RegressionTreeNode(
+left_node = *51 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *98 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.115606936416184927 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -1347,7 +846,7 @@
 split_feature_value = 0.180062798802320762 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *99 ->RegressionTreeLeave(
+missing_leave = *52 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1360,47 +859,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *100 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080924306 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *101 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.109826589595375682 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *98  ;
-right_node = *102 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *53 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *103 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.514450867052023031 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.24550237059167368 0 0.24550237059167368 ] ;
 split_col = 0 ;
@@ -1408,7 +874,7 @@
 split_feature_value = 0.242853945270868343 ;
 after_split_error = 0.19876061032130396 ;
 missing_node = *0 ;
-missing_leave = *104 ->RegressionTreeLeave(
+missing_leave = *54 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1420,20 +886,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *105 ->RegressionTreeNode(
+left_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *106 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 15 ;
-weights_sum = 0.0867052023121386989 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0115606936416184965 ;
-weighted_squared_targets_sum = 0.0115606936416184965 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0200385356454720609 0 0.0200385356454720609 ] ;
 split_col = 1 ;
@@ -1441,7 +896,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0107349298100743173 ;
 missing_node = *0 ;
-missing_leave = *107 ->RegressionTreeLeave(
+missing_leave = *56 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1454,47 +909,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *108 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080924306 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *109 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 14 ;
-weights_sum = 0.0809248554913294532 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0115606936416184965 ;
-weighted_squared_targets_sum = 0.0115606936416184965 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *106  ;
-right_node = *110 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *57 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *111 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 74 ;
-weights_sum = 0.427745664739884235 ;
-targets_sum = 52 ;
-weighted_targets_sum = 0.30057803468208083 ;
-weighted_squared_targets_sum = 0.30057803468208083 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.178722074675831843 0 0.178722074675831843 ] ;
 split_col = 2 ;
@@ -1502,7 +924,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.165519772456188596 ;
 missing_node = *0 ;
-missing_leave = *112 ->RegressionTreeLeave(
+missing_leave = *58 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1515,51 +937,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *113 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0057803468208092847 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0057803468208092847 ;
-weighted_squared_targets_sum = 0.0057803468208092847 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *114 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 73 ;
-weights_sum = 0.421965317919074989 ;
-targets_sum = 51 ;
-weighted_targets_sum = 0.294797687861271585 ;
-weighted_squared_targets_sum = 0.294797687861271585 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *111   )
+right_leave = *0  )
 ;
-right_leave = *103   )
+right_leave = *0  )
 ;
-left_leave = *95  ;
-right_node = *115 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *59 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *116 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 27 ;
-weights_sum = 0.156069364161849661 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
@@ -1567,7 +956,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *117 ->RegressionTreeLeave(
+missing_leave = *60 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1580,49 +969,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *118 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080925694 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *119 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 26 ;
-weights_sum = 0.150289017341040415 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *116   )
+right_leave = *0  )
 ;
-left_leave = *92  ;
-right_node = *120 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *61 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *121 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.213872832369942117 ;
-targets_sum = 37 ;
-weighted_targets_sum = 0.213872832369942117 ;
-weighted_squared_targets_sum = 0.213872832369942117 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -1630,7 +986,7 @@
 split_feature_value = 0.893509913423101043 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *122 ->RegressionTreeLeave(
+missing_leave = *62 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1643,43 +999,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *123 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080925694 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00578034682080925694 ;
-weighted_squared_targets_sum = 0.00578034682080925694 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *124 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 36 ;
-weights_sum = 0.208092485549132872 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.208092485549132872 ;
-weighted_squared_targets_sum = 0.208092485549132872 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *121   )
+right_leave = *0  )
 ;
-priority_queue = *125 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 4 ;
-nodes = 5 [ *110  *105  *97  *120  *0 ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *89  ;
-split_cols = 4 [ 2 2 4 0 ] ;
-split_values = 4 [ 0.995245802370935517 0.994286124455373455 8.32667268468867405e-16 0.242853945270868343 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -1695,7 +1026,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*126 ->RegressionTree(
+*63 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1703,32 +1034,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *127 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *128 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *64 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *129 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.999999999999998779 ;
-targets_sum = 58 ;
-weighted_targets_sum = 0.389261744966442613 ;
-weighted_squared_targets_sum = 0.389261744966442613 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.475474077744245216 0 0.475474077744245216 ] ;
 split_col = 1 ;
@@ -1736,7 +1045,7 @@
 split_feature_value = 0.575210824891620121 ;
 after_split_error = 0.29407734793231588 ;
 missing_node = *0 ;
-missing_leave = *130 ->RegressionTreeLeave(
+missing_leave = *65 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1748,20 +1057,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *131 ->RegressionTreeNode(
+left_node = *66 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *132 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 108 ;
-weights_sum = 0.724832214765099847 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.147651006711409377 ;
-weighted_squared_targets_sum = 0.147651006711409377 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.235147899577429681 0 0.235147899577429681 ] ;
 split_col = 3 ;
@@ -1769,7 +1067,7 @@
 split_feature_value = 0.861099804177139827 ;
 after_split_error = 0.181076457428609006 ;
 missing_node = *0 ;
-missing_leave = *133 ->RegressionTreeLeave(
+missing_leave = *67 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1781,20 +1079,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *134 ->RegressionTreeNode(
+left_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *135 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 102 ;
-weights_sum = 0.684563758389260979 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.107382550335570481 ;
-weighted_squared_targets_sum = 0.107382550335570481 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.181076457428609006 0 0.181076457428609006 ] ;
 split_col = 0 ;
@@ -1802,7 +1089,7 @@
 split_feature_value = 0.591195353843563365 ;
 after_split_error = 0.158960974397215959 ;
 missing_node = *0 ;
-missing_leave = *136 ->RegressionTreeLeave(
+missing_leave = *69 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1814,20 +1101,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *137 ->RegressionTreeNode(
+left_node = *70 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *138 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.322147651006711166 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.0939597315436241642 ;
-weighted_squared_targets_sum = 0.0939597315436241642 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.133109619686800851 0 0.133109619686800851 ] ;
 split_col = 3 ;
@@ -1835,7 +1111,7 @@
 split_feature_value = 0.187798288368589333 ;
 after_split_error = 0.111558538404175941 ;
 missing_node = *0 ;
-missing_leave = *139 ->RegressionTreeLeave(
+missing_leave = *71 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1848,47 +1124,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *140 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *141 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 47 ;
-weights_sum = 0.315436241610738022 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.0939597315436241642 ;
-weighted_squared_targets_sum = 0.0939597315436241642 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *138  ;
-right_node = *142 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *72 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *143 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 54 ;
-weights_sum = 0.362416107382550035 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0134228187919463084 ;
-weighted_squared_targets_sum = 0.0134228187919463084 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0258513547104151122 0 0.0258513547104151122 ] ;
 split_col = 1 ;
@@ -1896,7 +1139,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0131695580600227936 ;
 missing_node = *0 ;
-missing_leave = *144 ->RegressionTreeLeave(
+missing_leave = *73 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1909,49 +1152,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *145 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *146 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 53 ;
-weights_sum = 0.35570469798657689 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0134228187919463084 ;
-weighted_squared_targets_sum = 0.0134228187919463084 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *143   )
+right_leave = *0  )
 ;
-left_leave = *135  ;
-right_node = *147 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *74 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *148 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0402684563758389236 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0402684563758389236 ;
-weighted_squared_targets_sum = 0.0402684563758389236 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -1959,7 +1169,7 @@
 split_feature_value = 0.999999941905052481 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *149 ->RegressionTreeLeave(
+missing_leave = *75 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1972,49 +1182,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *150 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315248 ;
-weighted_squared_targets_sum = 0.00671140939597315248 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *151 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.033557046979865772 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.033557046979865772 ;
-weighted_squared_targets_sum = 0.033557046979865772 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *148   )
+right_leave = *0  )
 ;
-left_leave = *132  ;
-right_node = *152 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *76 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *153 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.275167785234899154 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.241610738255033403 ;
-weighted_squared_targets_sum = 0.241610738255033403 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0589294483548861714 0 0.0589294483548861714 ] ;
 split_col = 3 ;
@@ -2022,7 +1199,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0130600399056774452 ;
 missing_node = *0 ;
-missing_leave = *154 ->RegressionTreeLeave(
+missing_leave = *77 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2034,20 +1211,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *155 ->RegressionTreeNode(
+left_node = *78 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *156 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0268456375838926169 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
@@ -2055,7 +1221,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *157 ->RegressionTreeLeave(
+missing_leave = *79 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2068,47 +1234,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *158 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *159 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0201342281879194618 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *156  ;
-right_node = *160 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *80 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *161 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.248322147651006547 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.241610738255033403 ;
-weighted_squared_targets_sum = 0.241610738255033403 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0130600399056774452 0 0.0130600399056774452 ] ;
 split_col = 1 ;
@@ -2116,7 +1249,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0123042505592841078 ;
 missing_node = *0 ;
-missing_leave = *162 ->RegressionTreeLeave(
+missing_leave = *81 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2129,45 +1262,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *163 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0067114093959731386 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0067114093959731386 ;
-weighted_squared_targets_sum = 0.0067114093959731386 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *164 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 36 ;
-weights_sum = 0.241610738255033403 ;
-targets_sum = 35 ;
-weighted_targets_sum = 0.234899328859060258 ;
-weighted_squared_targets_sum = 0.234899328859060258 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *161   )
+right_leave = *0  )
 ;
-right_leave = *153   )
+right_leave = *0  )
 ;
-priority_queue = *165 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 4 ;
-nodes = 5 [ *137  *142  *160  *147  *0 ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *129  ;
-split_cols = 4 [ 1 3 3 0 ] ;
-split_values = 4 [ 0.575210824891620121 0.861099804177139827 0.0434674056274751697 0.591195353843563365 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -2183,7 +1291,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*166 ->RegressionTree(
+*82 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -2191,32 +1299,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *167 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *168 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *83 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *169 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 148 ;
-weights_sum = 1.00000000000000222 ;
-targets_sum = 95 ;
-weighted_targets_sum = 0.641891891891891997 ;
-weighted_squared_targets_sum = 0.641891891891891997 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.459733382030681148 0 0.459733382030681148 ] ;
 split_col = 2 ;
@@ -2224,7 +1310,7 @@
 split_feature_value = 0.00132212183813046336 ;
 after_split_error = 0.430920430920432751 ;
 missing_node = *0 ;
-missing_leave = *170 ->RegressionTreeLeave(
+missing_leave = *84 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2236,20 +1322,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *171 ->RegressionTreeNode(
+left_node = *85 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *172 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0337837837837837857 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -2257,7 +1332,7 @@
 split_feature_value = 0.0480606101777627803 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *173 ->RegressionTreeLeave(
+missing_leave = *86 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2270,47 +1345,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *174 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *175 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0270270270270270285 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *172  ;
-right_node = *176 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *87 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *177 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 143 ;
-weights_sum = 0.966216216216218338 ;
-targets_sum = 95 ;
-weighted_targets_sum = 0.641891891891891997 ;
-weighted_squared_targets_sum = 0.641891891891891997 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.430920430920432751 0 0.430920430920432751 ] ;
 split_col = 2 ;
@@ -2318,7 +1360,7 @@
 split_feature_value = 0.0173550052261667587 ;
 after_split_error = 0.387008857597093914 ;
 missing_node = *0 ;
-missing_leave = *178 ->RegressionTreeLeave(
+missing_leave = *88 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2330,20 +1372,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *179 ->RegressionTreeNode(
+left_node = *89 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *180 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.162162162162162116 ;
-targets_sum = 24 ;
-weighted_targets_sum = 0.162162162162162116 ;
-weighted_squared_targets_sum = 0.162162162162162116 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -2351,7 +1382,7 @@
 split_feature_value = 0.135512975844814643 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *181 ->RegressionTreeLeave(
+missing_leave = *90 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2364,47 +1395,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *182 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675677101 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675677101 ;
-weighted_squared_targets_sum = 0.00675675675675677101 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *183 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.155405405405405372 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.155405405405405372 ;
-weighted_squared_targets_sum = 0.155405405405405372 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *180  ;
-right_node = *184 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *91 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *185 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 119 ;
-weights_sum = 0.804054054054055167 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.479729729729729049 ;
-weighted_squared_targets_sum = 0.479729729729729049 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.387008857597093914 0 0.387008857597093914 ] ;
 split_col = 2 ;
@@ -2412,7 +1410,7 @@
 split_feature_value = 0.0689879291310910303 ;
 after_split_error = 0.351230694980696034 ;
 missing_node = *0 ;
-missing_leave = *186 ->RegressionTreeLeave(
+missing_leave = *92 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2424,20 +1422,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *187 ->RegressionTreeNode(
+left_node = *93 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *188 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0472972972972972999 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -2445,7 +1432,7 @@
 split_feature_value = 0.510088881577538289 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *189 ->RegressionTreeLeave(
+missing_leave = *94 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2458,47 +1445,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *190 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *191 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0405405405405405428 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *188  ;
-right_node = *192 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *95 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *193 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 112 ;
-weights_sum = 0.756756756756757576 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.479729729729729049 ;
-weighted_squared_targets_sum = 0.479729729729729049 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.351230694980696034 0 0.351230694980696034 ] ;
 split_col = 3 ;
@@ -2506,7 +1460,7 @@
 split_feature_value = 0.141930657011306749 ;
 after_split_error = 0.314935988620199336 ;
 missing_node = *0 ;
-missing_leave = *194 ->RegressionTreeLeave(
+missing_leave = *96 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2518,20 +1472,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *195 ->RegressionTreeNode(
+left_node = *97 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *196 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.114864864864864871 ;
-targets_sum = 17 ;
-weighted_targets_sum = 0.114864864864864871 ;
-weighted_squared_targets_sum = 0.114864864864864871 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 2 ;
@@ -2539,7 +1482,7 @@
 split_feature_value = 0.122353510242232788 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *197 ->RegressionTreeLeave(
+missing_leave = *98 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2552,47 +1495,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *198 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675675713 ;
-weighted_squared_targets_sum = 0.00675675675675675713 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *199 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 16 ;
-weights_sum = 0.108108108108108114 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.108108108108108114 ;
-weighted_squared_targets_sum = 0.108108108108108114 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *196  ;
-right_node = *200 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *99 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *201 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 95 ;
-weights_sum = 0.641891891891891997 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.364864864864864413 ;
-weighted_squared_targets_sum = 0.364864864864864413 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.314935988620199336 0 0.314935988620199336 ] ;
 split_col = 3 ;
@@ -2600,7 +1510,7 @@
 split_feature_value = 0.16348885181551448 ;
 after_split_error = 0.249176005273566148 ;
 missing_node = *0 ;
-missing_leave = *202 ->RegressionTreeLeave(
+missing_leave = *100 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2613,49 +1523,24 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *203 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675677101 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675677101 ;
-weighted_squared_targets_sum = 0.00675675675675677101 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *204 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.635135135135135198 ;
-targets_sum = 53 ;
-weighted_targets_sum = 0.35810810810810767 ;
-weighted_squared_targets_sum = 0.35810810810810767 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *201   )
+right_leave = *0  )
 ;
-right_leave = *193   )
+right_leave = *0  )
 ;
-right_leave = *185   )
+right_leave = *0  )
 ;
-right_leave = *177   )
+right_leave = *0  )
 ;
-priority_queue = *205 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *200  *171  *187  *179  *195  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *169  ;
-split_cols = 4 [ 2 2 2 3 ] ;
-split_values = 4 [ 0.00132212183813046336 0.0173550052261667587 0.0689879291310910303 0.141930657011306749 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -2672,12 +1557,12 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ] ;
-voting_weights = 5 [ 1.13679877806039675 1.1515814803392157 0.711694555952663288 0.924853007982333275 0.41501925503263265 ] ;
-sum_voting_weights = 4.33994707736724195 ;
-initial_sum_weights = 1 ;
-example_weights = 150 [ 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00506327259806789703 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0137564885108083946 0.000521219238036400986 0.00496216976946179701 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.116188573346790697 0.0331580145383834832 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0760448245655967192 0.00119536787918377106 0.000521219238036400986 0.000521219238036400!
 986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.00119536787918377106 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.00506327259806789703 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00119536787918377106 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.00496216976946179701 0.00216366726200823561 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.00216366726200823561 0.000521219238036400986 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.0321920345819545276 0.0482039349033431888 0.000521219238036400986 0.00216366726200823561 0.03219203458195452!
 76 0.021649179367261618 0.000521219238036400986 0.000521219238!
 03640098
6 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00216366726200823561 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.0210184819737942973 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.00331388591284825871 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00216366726200823561 0.00506327259806789703 0.0210184819737942973 0.0331580145383834832 0.0321920345819545276 0.!
 0496503807568692487 0.000521219238036400986 0.00521520520832954425 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00119536787918377106 0.000521219238036400986 ] ;
-learners_error = 5 [ 0.0933333333333333376 0.0908613445378150392 0.194130827514584187 0.135907417796919922 0.303636927486942487 ] ;
-weak_learner_template = *206 ->RegressionTree(
+voting_weights = 5 [ 1.13679877806039786 1.15158148033921659 0.711694555952663621 0.92485300798233383 0.415019255032632539 ] ;
+sum_voting_weights = 4.33994707736724461 ;
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.013756488510808379 0.000521219238036399468 0.00496216976946178487 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.116188573346790752 0.0331580145383834901 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0760448245655967053 0.00119536787918376715 0.000521219238036399468 0.00052121923803639946!
 8 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.00119536787918376715 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.00496216976946178487 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.00216366726200823171 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.0321920345819545345 0.048203934903343168 0.000521219238036399468 0.00216366726200823171 0.0321920345819545345 !
 0.021649179367261618 0.000521219238036399468 0.000521219238036!
 399468 0
.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.0210184819737942973 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.00331388591284825307 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.00506327259806789182 0.0210184819737942973 0.0331580145383834901 0.0321920345819545345 0.049650!
 380756869221 0.000521219238036399468 0.00521520520832954079 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 ] ;
+learners_error = 5 [ 0.0933333333333331433 0.0908613445378149004 0.194130827514584103 0.135907417796919783 0.303636927486942543 ] ;
+weak_learner_template = *101 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -2685,7 +1570,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *207 ->RegressionTreeLeave(
+leave_template = *102 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -2710,7 +1595,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 5 ;
 report_progress = 0 ;
@@ -2730,7 +1615,7 @@
 save_often = 1 ;
 compute_training_error = 1 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 0 ;
+modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
@@ -2738,7 +1623,7 @@
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 5 ;
 report_progress = 0 ;
@@ -2774,10 +1659,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *208 ->HyperOptimize(
+strategy = 1 [ *103 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *209 ->EarlyStoppingOracle(
+oracle = *104 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;
@@ -2806,7 +1691,7 @@
 auto_save_diff_time = 10800 ;
 auto_save_test = 0 ;
 best_objective = 0.140000000000000013 ;
-best_results = 6 [ 0.0533333333333333368 0.166752838955117694 0.0533333333333333368 0.140000000000000013 3.62727311623549786 0.140000000000000013 ] ;
+best_results = 6 [ 0.0533333333333333368 0.16675283895511761 0.0533333333333333368 0.140000000000000013 3.62727311623550586 0.140000000000000013 ] ;
 best_learner = *5  ;
 trialnum = 5 ;
 option_vals = []

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_stats.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test1_stats.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 23.0573431209043029 ;
-sumsquare_ = 27.5380176375199106 ;
-sumcube_ = 46.5250966576119964 ;
-sumfourth_ = 99.2739794243308324 ;
-min_ = 0.0130372181490889943 ;
-max_ = 2.90621616895834611 ;
+sum_ = 23.0573431209042994 ;
+sumsquare_ = 27.5380176375199071 ;
+sumcube_ = 46.5250966576120177 ;
+sumfourth_ = 99.2739794243309746 ;
+min_ = 0.0130372181490889579 ;
+max_ = 2.90621616895834789 ;
 agmemin_ = 149 ;
 agemax_ = 131 ;
-first_ = 0.0130372181490889943 ;
-last_ = 0.0130372181490889943 ;
+first_ = 0.0130372181490889579 ;
+last_ = 0.0130372181490889579 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -81,12 +81,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00969923368429130919 ;
-max_ = 0.00969923368429130919 ;
+min_ = 0.00969923368429130746 ;
+max_ = 0.00969923368429130746 ;
 agmemin_ = 149 ;
 agemax_ = 149 ;
-first_ = 0.00969923368429130919 ;
-last_ = 0.00969923368429130919 ;
+first_ = 0.00969923368429130746 ;
+last_ = 0.00969923368429130746 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00374842516165699345 ;
-max_ = 0.00374842516165699345 ;
+min_ = 0.00374842516165699042 ;
+max_ = 0.00374842516165699042 ;
 agmemin_ = 149 ;
 agemax_ = 149 ;
-first_ = 0.00374842516165699345 ;
-last_ = 0.00374842516165699345 ;
+first_ = 0.00374842516165699042 ;
+last_ = 0.00374842516165699042 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -119,12 +119,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 67.8772502377077274 ;
-sumsquare_ = 118.14393129077213 ;
-sumcube_ = 231.14082196770616 ;
-sumfourth_ = 482.354506226966521 ;
+sum_ = 67.8772502377077842 ;
+sumsquare_ = 118.14393129077223 ;
+sumcube_ = 231.140821967706728 ;
+sumfourth_ = 482.354506226967544 ;
 min_ = 0 ;
-max_ = 2.70339951343224527 ;
+max_ = 2.70339951343224705 ;
 agmemin_ = 149 ;
 agemax_ = 131 ;
 first_ = 0 ;
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+min_ = 4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 149 ;
 agemax_ = 149 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -161,16 +161,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -135.754500475415483 ;
-sumsquare_ = 472.575725163088521 ;
-sumcube_ = -1849.12657574165019 ;
-sumfourth_ = 7717.67209963146615 ;
-min_ = -1.0668519494972486 ;
-max_ = 4.33994707736724195 ;
+sum_ = -135.754500475415597 ;
+sumsquare_ = 472.575725163088919 ;
+sumcube_ = -1849.12657574165473 ;
+sumfourth_ = 7717.67209963148525 ;
+min_ = -1.06685194949724993 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 131 ;
 agemax_ = 149 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -182,16 +182,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -135.754500475415483 ;
-sumsquare_ = 472.575725163088521 ;
-sumcube_ = -1849.12657574165019 ;
-sumfourth_ = 7717.67209963146615 ;
-min_ = -1.0668519494972486 ;
-max_ = 4.33994707736724195 ;
+sum_ = -135.754500475415597 ;
+sumsquare_ = 472.575725163088919 ;
+sumcube_ = -1849.12657574165473 ;
+sumfourth_ = 7717.67209963148525 ;
+min_ = -1.06685194949724993 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 131 ;
 agemax_ = 149 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -203,12 +203,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 70.9371869135480608 ;
-sumsquare_ = 129.839350690236188 ;
-sumcube_ = 269.727712341757183 ;
-sumfourth_ = 608.877705658564992 ;
+sum_ = 70.9371869135481177 ;
+sumsquare_ = 129.839350690236301 ;
+sumcube_ = 269.727712341757695 ;
+sumfourth_ = 608.877705658566242 ;
 min_ = 0 ;
-max_ = 3.41509406938490878 ;
+max_ = 3.41509406938491056 ;
 agmemin_ = 149 ;
 agemax_ = 131 ;
 first_ = 0 ;
@@ -224,16 +224,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -37.5508790328347715 ;
-sumsquare_ = 65.927018120689894 ;
-sumcube_ = -36.38421051855412 ;
-sumfourth_ = 50.6411951502830249 ;
+sum_ = -37.550879032834878 ;
+sumsquare_ = 65.9270181206901213 ;
+sumcube_ = -36.3842105185541556 ;
+sumfourth_ = 50.6411951502831243 ;
 min_ = 0 ;
-max_ = 1.63654756393499667 ;
+max_ = 1.63654756393499756 ;
 agmemin_ = 148 ;
 agemax_ = 142 ;
-first_ = 0.924853007982333275 ;
-last_ = 1.63654756393499667 ;
+first_ = 0.92485300798233383 ;
+last_ = 1.63654756393499756 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -245,16 +245,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 58.1471012748402032 ;
-sumsquare_ = 74.2876873973146559 ;
-sumcube_ = 59.9340515275839181 ;
-sumfourth_ = 94.1517014718979652 ;
-min_ = 2.07643448832154887 ;
-max_ = 5.28966775470349493 ;
+sum_ = 58.1471012748402103 ;
+sumsquare_ = 74.2876873973148406 ;
+sumcube_ = 59.9340515275841312 ;
+sumfourth_ = 94.1517014718984342 ;
+min_ = 2.0764344883215502 ;
+max_ = 5.28966775470349937 ;
 agmemin_ = 84 ;
 agemax_ = 123 ;
-first_ = 3.22801596866076457 ;
-last_ = 3.93971052461342763 ;
+first_ = 3.22801596866076679 ;
+last_ = 3.93971052461343074 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -266,16 +266,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -7.03312503214024254 ;
-sumsquare_ = 133.683529618793727 ;
-sumcube_ = 63.6153416947102102 ;
-sumfourth_ = 303.882312131112542 ;
-min_ = 3.09355109911095827 ;
-max_ = 6.8053204694158973 ;
+sum_ = -7.03312503214035889 ;
+sumsquare_ = 133.683529618793898 ;
+sumcube_ = 63.6153416947098478 ;
+sumfourth_ = 303.882312131113338 ;
+min_ = 3.09355109911095916 ;
+max_ = 6.80532046941589996 ;
 agmemin_ = 148 ;
 agemax_ = 135 ;
-first_ = 4.1267886253375714 ;
-last_ = 5.2635874033979686 ;
+first_ = 4.12678862533757407 ;
+last_ = 5.26358740339797215 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -287,16 +287,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -18.1733302402189523 ;
-sumsquare_ = 117.707682245042491 ;
-sumcube_ = -130.992227419560692 ;
-sumfourth_ = 307.816160654515727 ;
-min_ = 1.75489151804759858 ;
-max_ = 5.89176511046027152 ;
+sum_ = -18.1733302402190127 ;
+sumsquare_ = 117.707682245042605 ;
+sumcube_ = -130.992227419561146 ;
+sumfourth_ = 307.816160654516693 ;
+min_ = 1.7548915180475988 ;
+max_ = 5.89176511046027418 ;
 agmemin_ = 130 ;
 agemax_ = 25 ;
-first_ = 4.11321860501035896 ;
-last_ = 2.26472527099729914 ;
+first_ = 4.11321860501036163 ;
+last_ = 2.26472527099730003 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -308,16 +308,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 47.8651730026865181 ;
-sumsquare_ = 283.462125116786581 ;
-sumcube_ = 888.350651536423925 ;
-sumfourth_ = 3628.99034759896904 ;
-min_ = 0.711694555952663288 ;
-max_ = 6.85505580812373605 ;
+sum_ = 47.8651730026865323 ;
+sumsquare_ = 283.462125116786979 ;
+sumcube_ = 888.350651536426994 ;
+sumfourth_ = 3628.99034759898177 ;
+min_ = 0.711694555952663621 ;
+max_ = 6.85505580812374138 ;
 agmemin_ = 123 ;
 agemax_ = 66 ;
-first_ = 1.84849333401306004 ;
-last_ = 2.98529211207345657 ;
+first_ = 1.8484933340130616 ;
+last_ = 2.98529211207345924 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_stats.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/test2_stats.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -213.43366710400943 ;
-sumsquare_ = 8732.68373561494445 ;
-sumcube_ = 338561.042816411005 ;
-sumfourth_ = 23429594.2572011389 ;
-min_ = 0.0130372181490889943 ;
-max_ = 76.7034798807809608 ;
+sum_ = -213.433667104009203 ;
+sumsquare_ = 8732.68373561498265 ;
+sumcube_ = 338561.042816414207 ;
+sumfourth_ = 23429594.2572014108 ;
+min_ = 0.0130372181490889579 ;
+max_ = 76.7034798807811597 ;
 agmemin_ = 47 ;
 agemax_ = 38 ;
-first_ = 7.8959464583156862 ;
-last_ = 0.0130372181490889943 ;
+first_ = 7.89594645831568975 ;
+last_ = 0.0130372181490889579 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -81,12 +81,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00969923368429130919 ;
-max_ = 0.00969923368429130919 ;
+min_ = 0.00969923368429130746 ;
+max_ = 0.00969923368429130746 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 0.00969923368429130919 ;
-last_ = 0.00969923368429130919 ;
+first_ = 0.00969923368429130746 ;
+last_ = 0.00969923368429130746 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00374842516165699345 ;
-max_ = 0.00374842516165699345 ;
+min_ = 0.00374842516165699042 ;
+max_ = 0.00374842516165699042 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 0.00374842516165699345 ;
-last_ = 0.00374842516165699345 ;
+first_ = 0.00374842516165699042 ;
+last_ = 0.00374842516165699042 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -119,15 +119,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -118.974105250205838 ;
-sumsquare_ = 361.718741373110618 ;
-sumcube_ = -1088.54433267569675 ;
-sumfourth_ = 3368.94905180080559 ;
+sum_ = -118.974105250205852 ;
+sumsquare_ = 361.718741373110731 ;
+sumcube_ = -1088.54433267569812 ;
+sumfourth_ = 3368.94905180081196 ;
 min_ = 0 ;
-max_ = 4.33994707736724195 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 47 ;
 agemax_ = 38 ;
-first_ = 3.20314829930684519 ;
+first_ = 3.20314829930684652 ;
 last_ = 0 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+min_ = 4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -161,16 +161,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 237.948210500411676 ;
-sumsquare_ = 1446.87496549244247 ;
-sumcube_ = 8708.35466140557401 ;
-sumfourth_ = 53903.1848288128895 ;
-min_ = -4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+sum_ = 237.948210500411705 ;
+sumsquare_ = 1446.87496549244293 ;
+sumcube_ = 8708.35466140558492 ;
+sumfourth_ = 53903.1848288129913 ;
+min_ = -4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 38 ;
 agemax_ = 47 ;
 first_ = -2.06634952124644844 ;
-last_ = 4.33994707736724195 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -182,16 +182,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 237.948210500411676 ;
-sumsquare_ = 1446.87496549244247 ;
-sumcube_ = 8708.35466140557401 ;
-sumfourth_ = 53903.1848288128895 ;
-min_ = -4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+sum_ = 237.948210500411705 ;
+sumsquare_ = 1446.87496549244293 ;
+sumcube_ = 8708.35466140558492 ;
+sumfourth_ = 53903.1848288129913 ;
+min_ = -4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 38 ;
 agemax_ = 47 ;
 first_ = -2.06634952124644844 ;
-last_ = 4.33994707736724195 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -203,15 +203,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -118.974105250205838 ;
-sumsquare_ = 361.718741373110618 ;
-sumcube_ = -1088.54433267569675 ;
-sumfourth_ = 3368.94905180080559 ;
+sum_ = -118.974105250205852 ;
+sumsquare_ = 361.718741373110731 ;
+sumcube_ = -1088.54433267569812 ;
+sumfourth_ = 3368.94905180081196 ;
 min_ = 0 ;
-max_ = 4.33994707736724195 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 47 ;
 agemax_ = 38 ;
-first_ = 3.20314829930684519 ;
+first_ = 3.20314829930684652 ;
 last_ = 0 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -224,16 +224,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -40.346432942149562 ;
-sumsquare_ = 52.2063866773546721 ;
-sumcube_ = -75.0396438545297286 ;
-sumfourth_ = 114.886284738728506 ;
+sum_ = -40.3464329421495762 ;
+sumsquare_ = 52.2063866773547218 ;
+sumcube_ = -75.0396438545297855 ;
+sumfourth_ = 114.886284738728719 ;
 min_ = 0 ;
-max_ = 1.63654756393499667 ;
+max_ = 1.63654756393499756 ;
 agmemin_ = 47 ;
 agemax_ = 49 ;
-first_ = 1.63654756393499667 ;
-last_ = 0.924853007982333275 ;
+first_ = 1.63654756393499756 ;
+last_ = 0.92485300798233383 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -245,16 +245,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -16.1112854278747726 ;
-sumsquare_ = 27.3178799413751463 ;
-sumcube_ = -26.7915520541299621 ;
-sumfourth_ = 49.4897368630278365 ;
-min_ = 2.07643448832154887 ;
-max_ = 5.28966775470349493 ;
+sum_ = -16.1112854278748081 ;
+sumsquare_ = 27.3178799413752031 ;
+sumcube_ = -26.7915520541300438 ;
+sumfourth_ = 49.4897368630280354 ;
+min_ = 2.0764344883215502 ;
+max_ = 5.28966775470349937 ;
 agmemin_ = 33 ;
 agemax_ = 25 ;
-first_ = 3.93971052461342763 ;
-last_ = 4.15286897664309773 ;
+first_ = 3.93971052461343074 ;
+last_ = 4.1528689766431004 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -266,16 +266,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 5.85771568121859509 ;
-sumsquare_ = 49.7800097040063818 ;
-sumcube_ = 62.2050339635074678 ;
-sumfourth_ = 197.71117838999271 ;
-min_ = 3.09355109911095827 ;
-max_ = 6.8053204694158973 ;
+sum_ = 5.85771568121856223 ;
+sumsquare_ = 49.7800097040064387 ;
+sumcube_ = 62.2050339635074039 ;
+sumfourth_ = 197.711178389992853 ;
+min_ = 3.09355109911095916 ;
+max_ = 6.80532046941589996 ;
 agmemin_ = 47 ;
 agemax_ = 39 ;
-first_ = 4.1267886253375714 ;
-last_ = 4.1267886253375714 ;
+first_ = 4.12678862533757407 ;
+last_ = 4.12678862533757407 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -287,16 +287,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 76.2390102335463098 ;
-sumsquare_ = 160.742172097361419 ;
-sumcube_ = 356.830904308486083 ;
-sumfourth_ = 834.777632970863806 ;
-min_ = 1.84970601596466655 ;
-max_ = 5.47674585542763825 ;
+sum_ = 76.2390102335463951 ;
+sumsquare_ = 160.742172097361561 ;
+sumcube_ = 356.830904308487163 ;
+sumfourth_ = 834.777632970866875 ;
+min_ = 1.84970601596466766 ;
+max_ = 5.47674585542764181 ;
 agmemin_ = 30 ;
 agemax_ = 8 ;
-first_ = 2.26472527099729914 ;
-last_ = 3.18836559702802624 ;
+first_ = 2.26472527099730003 ;
+last_ = 3.18836559702802802 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -308,16 +308,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -87.9536558510736057 ;
-sumsquare_ = 275.74828119293727 ;
-sumcube_ = -590.104054700513871 ;
-sumfourth_ = 1937.66870355591777 ;
-min_ = 0.711694555952663288 ;
-max_ = 6.85505580812373605 ;
+sum_ = -87.9536558510736768 ;
+sumsquare_ = 275.748281192937782 ;
+sumcube_ = -590.104054700516031 ;
+sumfourth_ = 1937.66870355592437 ;
+min_ = 0.711694555952663621 ;
+max_ = 6.85505580812374138 ;
 agmemin_ = 25 ;
 agemax_ = 20 ;
-first_ = 4.12209089013385377 ;
-last_ = 1.84849333401306004 ;
+first_ = 4.12209089013385732 ;
+last_ = 1.8484933340130616 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/train_stats.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/Split0/train_stats.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -39,12 +39,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.166752838955117694 ;
-max_ = 0.166752838955117694 ;
+min_ = 0.16675283895511761 ;
+max_ = 0.16675283895511761 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.166752838955117694 ;
-last_ = 0.166752838955117694 ;
+first_ = 0.16675283895511761 ;
+last_ = 0.16675283895511761 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 3.62727311623549786 ;
-max_ = 3.62727311623549786 ;
+min_ = 3.62727311623550586 ;
+max_ = 3.62727311623550586 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 3.62727311623549786 ;
-last_ = 3.62727311623549786 ;
+first_ = 3.62727311623550586 ;
+last_ = 3.62727311623550586 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_base/expected_results/expdir/metainfos.txt	2008-12-10 14:58:55 UTC (rev 9768)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9611"
+__REVISION__ = "PL9752"
 conf                                          = False
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/Split0/final_learner.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -239,32 +239,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *7 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *8 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *7 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *9 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 159 ;
-weights_sum = 0.999999999999999889 ;
-targets_sum = 99 ;
-weighted_targets_sum = 0.622641509433962237 ;
-weighted_squared_targets_sum = 0.622641509433962237 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.469918120327518563 0 0.469918120327518563 ] ;
 split_col = 2 ;
@@ -272,7 +250,7 @@
 split_feature_value = 0.188677163392186542 ;
 after_split_error = 0.165982151128678812 ;
 missing_node = *0 ;
-missing_leave = *10 ->RegressionTreeLeave(
+missing_leave = *8 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -284,20 +262,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *11 ->RegressionTreeNode(
+left_node = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *12 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 65 ;
-weights_sum = 0.408805031446540845 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.0628930817610062892 ;
-weighted_squared_targets_sum = 0.0628930817610062892 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.106434446057087573 0 0.106434446057087573 ] ;
 split_col = 4 ;
@@ -305,7 +272,7 @@
 split_feature_value = 8.88178419700125232e-16 ;
 after_split_error = 0.0733752620545073397 ;
 missing_node = *0 ;
-missing_leave = *13 ->RegressionTreeLeave(
+missing_leave = *10 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -317,20 +284,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *14 ->RegressionTreeNode(
+left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *15 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.257861635220125784 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -338,7 +294,7 @@
 split_feature_value = 0.185353117285409069 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeLeave(
+missing_leave = *12 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -351,47 +307,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *17 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610067958 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *18 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.251572327044025157 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *15  ;
-right_node = *19 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *13 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *20 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.150943396226415089 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.0628930817610062892 ;
-weighted_squared_targets_sum = 0.0628930817610062892 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0733752620545073397 0 0.0733752620545073397 ] ;
 split_col = 4 ;
@@ -399,7 +322,7 @@
 split_feature_value = 3.44335671087492301e-13 ;
 after_split_error = 0.0528301886792452852 ;
 missing_node = *0 ;
-missing_leave = *21 ->RegressionTreeLeave(
+missing_leave = *14 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -411,20 +334,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *22 ->RegressionTreeNode(
+left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *23 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0251572327044025171 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0251572327044025171 ;
-weighted_squared_targets_sum = 0.0251572327044025171 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -432,7 +344,7 @@
 split_feature_value = 2.63677968348474678e-15 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeLeave(
+missing_leave = *16 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -445,55 +357,22 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *25 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.006289308176100631 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.006289308176100631 ;
-weighted_squared_targets_sum = 0.006289308176100631 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *26 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0188679245283018895 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0188679245283018895 ;
-weighted_squared_targets_sum = 0.0188679245283018895 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *23  ;
-right_node = *27 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *17 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *28 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.125786163522012578 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0377358490566037721 ;
-weighted_squared_targets_sum = 0.0377358490566037721 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0528301886792452852 0 0.0528301886792452852 ] ;
 split_col = 2 ;
 split_balance = 10 ;
 split_feature_value = 0.0166672042033834122 ;
-after_split_error = 0.045283018867924546 ;
+after_split_error = 0.0452830188679245321 ;
 missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
+missing_leave = *18 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -506,51 +385,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *30 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610065182 ;
-targets_sum = 0 ;
-weighted_targets_sum = 1.73472347597680709e-18 ;
-weighted_squared_targets_sum = 1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *31 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.119496855345911937 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0377358490566037721 ;
-weighted_squared_targets_sum = 0.0377358490566037721 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *28   )
+right_leave = *0  )
 ;
-right_leave = *20   )
+right_leave = *0  )
 ;
-left_leave = *12  ;
-right_node = *32 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *19 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *33 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.591194968553459099 ;
-targets_sum = 89 ;
-weighted_targets_sum = 0.559748427672955962 ;
-weighted_squared_targets_sum = 0.559748427672955962 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0595477050715911282 0 0.0595477050715911282 ] ;
 split_col = 3 ;
@@ -558,7 +404,7 @@
 split_feature_value = 0.170219082142077621 ;
 after_split_error = 0.0448775543115166875 ;
 missing_node = *0 ;
-missing_leave = *34 ->RegressionTreeLeave(
+missing_leave = *20 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -570,20 +416,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *35 ->RegressionTreeNode(
+left_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *36 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0188679245283018895 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610062927 ;
-weighted_squared_targets_sum = 0.00628930817610062927 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.00838574423480083903 0 0.00838574423480083903 ] ;
 split_col = 3 ;
@@ -591,7 +426,7 @@
 split_feature_value = 0.141699018667860999 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
+missing_leave = *22 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -604,47 +439,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *38 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.006289308176100631 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *39 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0125786163522012585 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610062927 ;
-weighted_squared_targets_sum = 0.00628930817610062927 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *36  ;
-right_node = *40 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *23 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *41 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 91 ;
-weights_sum = 0.572327044025157217 ;
-targets_sum = 88 ;
-weighted_targets_sum = 0.553459119496855334 ;
-weighted_squared_targets_sum = 0.553459119496855334 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0364918100767157583 0 0.0364918100767157583 ] ;
 split_col = 1 ;
@@ -652,7 +454,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0332075471698113078 ;
 missing_node = *0 ;
-missing_leave = *42 ->RegressionTreeLeave(
+missing_leave = *24 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -665,45 +467,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *43 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610073509 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610073509 ;
-weighted_squared_targets_sum = 0.00628930817610073509 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *44 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 90 ;
-weights_sum = 0.566037735849056589 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.547169811320754707 ;
-weighted_squared_targets_sum = 0.547169811320754707 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *41   )
+right_leave = *0  )
 ;
-right_leave = *33   )
+right_leave = *0  )
 ;
-priority_queue = *45 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *35  *27  *22  *14  *40  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *9  ;
-split_cols = 4 [ 2 4 4 3 ] ;
-split_values = 4 [ 0.188677163392186542 8.88178419700125232e-16 3.44335671087492301e-13 0.170219082142077621 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -719,7 +496,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*46 ->RegressionTree(
+*25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -727,32 +504,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *47 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *48 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *26 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *49 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.999999999999998779 ;
-targets_sum = 93 ;
-weighted_targets_sum = 0.624161073825502677 ;
-weighted_squared_targets_sum = 0.624161073825502677 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.469168055492995228 0 0.469168055492995228 ] ;
 split_col = 1 ;
@@ -760,7 +515,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.260686628807433929 ;
 missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
+missing_leave = *27 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -772,20 +527,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *51 ->RegressionTreeNode(
+left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *52 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 45 ;
-weights_sum = 0.302013422818791732 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0402684563758389236 ;
-weighted_squared_targets_sum = 0.0402684563758389236 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.069798657718120799 0 0.069798657718120799 ] ;
 split_col = 2 ;
@@ -793,7 +537,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.0130872483221476481 ;
 missing_node = *0 ;
-missing_leave = *53 ->RegressionTreeLeave(
+missing_leave = *29 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -805,20 +549,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *54 ->RegressionTreeNode(
+left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *55 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.268456375838926009 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315422 ;
-weighted_squared_targets_sum = 0.00671140939597315422 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0130872483221476498 0 0.0130872483221476498 ] ;
 split_col = 1 ;
@@ -826,7 +559,7 @@
 split_feature_value = 0.414475818795681961 ;
 after_split_error = 0.0120805369127516774 ;
 missing_node = *0 ;
-missing_leave = *56 ->RegressionTreeLeave(
+missing_leave = *31 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -839,47 +572,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *57 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *58 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 39 ;
-weights_sum = 0.261744966442952864 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315422 ;
-weighted_squared_targets_sum = 0.00671140939597315422 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *55  ;
-right_node = *59 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *32 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *60 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.033557046979865772 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.033557046979865772 ;
-weighted_squared_targets_sum = 0.033557046979865772 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -887,7 +587,7 @@
 split_feature_value = 1.66171551518878857e-09 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *61 ->RegressionTreeLeave(
+missing_leave = *33 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -900,49 +600,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *62 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315248 ;
-weighted_squared_targets_sum = 0.00671140939597315248 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *63 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0268456375838926169 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0268456375838926169 ;
-weighted_squared_targets_sum = 0.0268456375838926169 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *60   )
+right_leave = *0  )
 ;
-left_leave = *52  ;
-right_node = *64 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *34 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *65 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 104 ;
-weights_sum = 0.697986577181207268 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.583892617449663809 ;
-weighted_squared_targets_sum = 0.583892617449663809 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.190887971089313102 0 0.190887971089313102 ] ;
 split_col = 4 ;
@@ -950,7 +617,7 @@
 split_feature_value = 0.999999999538717876 ;
 after_split_error = 0.145761307736665707 ;
 missing_node = *0 ;
-missing_leave = *66 ->RegressionTreeLeave(
+missing_leave = *35 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -962,20 +629,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *67 ->RegressionTreeNode(
+left_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *68 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.248322147651006547 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.147651006711409377 ;
-weighted_squared_targets_sum = 0.147651006711409377 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.119717032468710211 0 0.119717032468710211 ] ;
 split_col = 4 ;
@@ -983,7 +639,7 @@
 split_feature_value = 0.725372806051693186 ;
 after_split_error = 0.033557046979865772 ;
 missing_node = *0 ;
-missing_leave = *69 ->RegressionTreeLeave(
+missing_leave = *37 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -995,20 +651,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *70 ->RegressionTreeNode(
+left_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *71 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.127516778523489943 ;
-targets_sum = 19 ;
-weighted_targets_sum = 0.127516778523489943 ;
-weighted_squared_targets_sum = 0.127516778523489943 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -1016,7 +661,7 @@
 split_feature_value = 0.00609705032829710447 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *72 ->RegressionTreeLeave(
+missing_leave = *39 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1029,47 +674,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *73 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0067114093959731386 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0067114093959731386 ;
-weighted_squared_targets_sum = 0.0067114093959731386 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *74 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 18 ;
-weights_sum = 0.120805369127516798 ;
-targets_sum = 18 ;
-weighted_targets_sum = 0.120805369127516798 ;
-weighted_squared_targets_sum = 0.120805369127516798 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *71  ;
-right_node = *75 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *40 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *76 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 18 ;
-weights_sum = 0.120805369127516798 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0201342281879194618 ;
-weighted_squared_targets_sum = 0.0201342281879194618 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.033557046979865772 0 0.033557046979865772 ] ;
 split_col = 4 ;
@@ -1077,7 +689,7 @@
 split_feature_value = 0.999935792696207582 ;
 after_split_error = 0.0100671140939597309 ;
 missing_node = *0 ;
-missing_leave = *77 ->RegressionTreeLeave(
+missing_leave = *41 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1090,49 +702,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *78 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *79 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.11409395973154364 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0201342281879194618 ;
-weighted_squared_targets_sum = 0.0201342281879194618 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *76   )
+right_leave = *0  )
 ;
-left_leave = *68  ;
-right_node = *80 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *42 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *81 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 67 ;
-weights_sum = 0.449664429530200915 ;
-targets_sum = 65 ;
-weighted_targets_sum = 0.436241610738254626 ;
-weighted_squared_targets_sum = 0.436241610738254626 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0260442752679554967 0 0.0260442752679554967 ] ;
 split_col = 3 ;
@@ -1140,7 +719,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0132194427496440392 ;
 missing_node = *0 ;
-missing_leave = *82 ->RegressionTreeLeave(
+missing_leave = *43 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1153,45 +732,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *83 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597316636 ;
-weighted_squared_targets_sum = 0.00671140939597316636 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *84 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.442953020134227771 ;
-targets_sum = 64 ;
-weighted_targets_sum = 0.429530201342281481 ;
-weighted_squared_targets_sum = 0.429530201342281481 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *81   )
+right_leave = *0  )
 ;
-right_leave = *65   )
+right_leave = *0  )
 ;
-priority_queue = *85 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *75  *80  *54  *70  *59  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *49  ;
-split_cols = 4 [ 1 2 4 4 ] ;
-split_values = 4 [ 0.482293993618237549 0.885239426681956321 0.999999999538717876 0.725372806051693186 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -1207,7 +761,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*86 ->RegressionTree(
+*44 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1215,32 +769,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *87 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *88 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *45 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *89 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 173 ;
-weights_sum = 1.00000000000000422 ;
-targets_sum = 91 ;
-weighted_targets_sum = 0.526011560693641633 ;
-weighted_squared_targets_sum = 0.526011560693641633 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.498646797420564281 0 0.498646797420564281 ] ;
 split_col = 2 ;
@@ -1248,7 +780,7 @@
 split_feature_value = 0.995245802370935517 ;
 after_split_error = 0.376402584155050013 ;
 missing_node = *0 ;
-missing_leave = *90 ->RegressionTreeLeave(
+missing_leave = *46 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1260,20 +792,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *91 ->RegressionTreeNode(
+left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *92 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 136 ;
-weights_sum = 0.786127167630060186 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.376402584155050013 0 0.376402584155050013 ] ;
 split_col = 2 ;
@@ -1281,7 +802,7 @@
 split_feature_value = 0.994286124455373455 ;
 after_split_error = 0.31500238638171546 ;
 missing_node = *0 ;
-missing_leave = *93 ->RegressionTreeLeave(
+missing_leave = *48 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1293,20 +814,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *94 ->RegressionTreeNode(
+left_node = *49 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *95 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 109 ;
-weights_sum = 0.630057803468209054 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.31500238638171546 0 0.31500238638171546 ] ;
 split_col = 4 ;
@@ -1314,7 +824,7 @@
 split_feature_value = 8.32667268468867405e-16 ;
 after_split_error = 0.24550237059167368 ;
 missing_node = *0 ;
-missing_leave = *96 ->RegressionTreeLeave(
+missing_leave = *50 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1326,20 +836,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *97 ->RegressionTreeNode(
+left_node = *51 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *98 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.115606936416184927 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -1347,7 +846,7 @@
 split_feature_value = 0.180062798802320762 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *99 ->RegressionTreeLeave(
+missing_leave = *52 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1360,47 +859,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *100 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080924306 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *101 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.109826589595375682 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *98  ;
-right_node = *102 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *53 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *103 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.514450867052023031 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.24550237059167368 0 0.24550237059167368 ] ;
 split_col = 0 ;
@@ -1408,7 +874,7 @@
 split_feature_value = 0.242853945270868343 ;
 after_split_error = 0.19876061032130396 ;
 missing_node = *0 ;
-missing_leave = *104 ->RegressionTreeLeave(
+missing_leave = *54 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1420,20 +886,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *105 ->RegressionTreeNode(
+left_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *106 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 15 ;
-weights_sum = 0.0867052023121386989 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0115606936416184965 ;
-weighted_squared_targets_sum = 0.0115606936416184965 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0200385356454720609 0 0.0200385356454720609 ] ;
 split_col = 1 ;
@@ -1441,7 +896,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0107349298100743173 ;
 missing_node = *0 ;
-missing_leave = *107 ->RegressionTreeLeave(
+missing_leave = *56 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1454,47 +909,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *108 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080924306 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *109 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 14 ;
-weights_sum = 0.0809248554913294532 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0115606936416184965 ;
-weighted_squared_targets_sum = 0.0115606936416184965 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *106  ;
-right_node = *110 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *57 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *111 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 74 ;
-weights_sum = 0.427745664739884235 ;
-targets_sum = 52 ;
-weighted_targets_sum = 0.30057803468208083 ;
-weighted_squared_targets_sum = 0.30057803468208083 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.178722074675831843 0 0.178722074675831843 ] ;
 split_col = 2 ;
@@ -1502,7 +924,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.165519772456188596 ;
 missing_node = *0 ;
-missing_leave = *112 ->RegressionTreeLeave(
+missing_leave = *58 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1515,51 +937,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *113 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0057803468208092847 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0057803468208092847 ;
-weighted_squared_targets_sum = 0.0057803468208092847 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *114 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 73 ;
-weights_sum = 0.421965317919074989 ;
-targets_sum = 51 ;
-weighted_targets_sum = 0.294797687861271585 ;
-weighted_squared_targets_sum = 0.294797687861271585 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *111   )
+right_leave = *0  )
 ;
-right_leave = *103   )
+right_leave = *0  )
 ;
-left_leave = *95  ;
-right_node = *115 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *59 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *116 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 27 ;
-weights_sum = 0.156069364161849661 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
@@ -1567,7 +956,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *117 ->RegressionTreeLeave(
+missing_leave = *60 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1580,49 +969,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *118 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080925694 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *119 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 26 ;
-weights_sum = 0.150289017341040415 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *116   )
+right_leave = *0  )
 ;
-left_leave = *92  ;
-right_node = *120 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *61 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *121 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.213872832369942117 ;
-targets_sum = 37 ;
-weighted_targets_sum = 0.213872832369942117 ;
-weighted_squared_targets_sum = 0.213872832369942117 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -1630,7 +986,7 @@
 split_feature_value = 0.893509913423101043 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *122 ->RegressionTreeLeave(
+missing_leave = *62 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1643,43 +999,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *123 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080925694 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00578034682080925694 ;
-weighted_squared_targets_sum = 0.00578034682080925694 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *124 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 36 ;
-weights_sum = 0.208092485549132872 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.208092485549132872 ;
-weighted_squared_targets_sum = 0.208092485549132872 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *121   )
+right_leave = *0  )
 ;
-priority_queue = *125 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 4 ;
-nodes = 5 [ *110  *105  *97  *120  *0 ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *89  ;
-split_cols = 4 [ 2 2 4 0 ] ;
-split_values = 4 [ 0.995245802370935517 0.994286124455373455 8.32667268468867405e-16 0.242853945270868343 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -1695,7 +1026,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*126 ->RegressionTree(
+*63 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1703,32 +1034,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *127 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *128 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *64 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *129 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.999999999999998779 ;
-targets_sum = 58 ;
-weighted_targets_sum = 0.389261744966442613 ;
-weighted_squared_targets_sum = 0.389261744966442613 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.475474077744245216 0 0.475474077744245216 ] ;
 split_col = 1 ;
@@ -1736,7 +1045,7 @@
 split_feature_value = 0.575210824891620121 ;
 after_split_error = 0.29407734793231588 ;
 missing_node = *0 ;
-missing_leave = *130 ->RegressionTreeLeave(
+missing_leave = *65 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1748,20 +1057,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *131 ->RegressionTreeNode(
+left_node = *66 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *132 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 108 ;
-weights_sum = 0.724832214765099847 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.147651006711409377 ;
-weighted_squared_targets_sum = 0.147651006711409377 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.235147899577429681 0 0.235147899577429681 ] ;
 split_col = 3 ;
@@ -1769,7 +1067,7 @@
 split_feature_value = 0.861099804177139827 ;
 after_split_error = 0.181076457428609006 ;
 missing_node = *0 ;
-missing_leave = *133 ->RegressionTreeLeave(
+missing_leave = *67 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1781,20 +1079,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *134 ->RegressionTreeNode(
+left_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *135 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 102 ;
-weights_sum = 0.684563758389260979 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.107382550335570481 ;
-weighted_squared_targets_sum = 0.107382550335570481 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.181076457428609006 0 0.181076457428609006 ] ;
 split_col = 0 ;
@@ -1802,7 +1089,7 @@
 split_feature_value = 0.591195353843563365 ;
 after_split_error = 0.158960974397215959 ;
 missing_node = *0 ;
-missing_leave = *136 ->RegressionTreeLeave(
+missing_leave = *69 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1814,20 +1101,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *137 ->RegressionTreeNode(
+left_node = *70 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *138 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.322147651006711166 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.0939597315436241642 ;
-weighted_squared_targets_sum = 0.0939597315436241642 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.133109619686800851 0 0.133109619686800851 ] ;
 split_col = 3 ;
@@ -1835,7 +1111,7 @@
 split_feature_value = 0.187798288368589333 ;
 after_split_error = 0.111558538404175941 ;
 missing_node = *0 ;
-missing_leave = *139 ->RegressionTreeLeave(
+missing_leave = *71 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1848,47 +1124,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *140 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *141 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 47 ;
-weights_sum = 0.315436241610738022 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.0939597315436241642 ;
-weighted_squared_targets_sum = 0.0939597315436241642 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *138  ;
-right_node = *142 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *72 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *143 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 54 ;
-weights_sum = 0.362416107382550035 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0134228187919463084 ;
-weighted_squared_targets_sum = 0.0134228187919463084 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0258513547104151122 0 0.0258513547104151122 ] ;
 split_col = 1 ;
@@ -1896,7 +1139,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0131695580600227936 ;
 missing_node = *0 ;
-missing_leave = *144 ->RegressionTreeLeave(
+missing_leave = *73 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1909,49 +1152,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *145 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *146 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 53 ;
-weights_sum = 0.35570469798657689 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0134228187919463084 ;
-weighted_squared_targets_sum = 0.0134228187919463084 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *143   )
+right_leave = *0  )
 ;
-left_leave = *135  ;
-right_node = *147 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *74 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *148 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0402684563758389236 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0402684563758389236 ;
-weighted_squared_targets_sum = 0.0402684563758389236 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -1959,7 +1169,7 @@
 split_feature_value = 0.999999941905052481 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *149 ->RegressionTreeLeave(
+missing_leave = *75 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1972,49 +1182,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *150 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315248 ;
-weighted_squared_targets_sum = 0.00671140939597315248 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *151 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.033557046979865772 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.033557046979865772 ;
-weighted_squared_targets_sum = 0.033557046979865772 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *148   )
+right_leave = *0  )
 ;
-left_leave = *132  ;
-right_node = *152 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *76 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *153 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.275167785234899154 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.241610738255033403 ;
-weighted_squared_targets_sum = 0.241610738255033403 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0589294483548861714 0 0.0589294483548861714 ] ;
 split_col = 3 ;
@@ -2022,7 +1199,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0130600399056774452 ;
 missing_node = *0 ;
-missing_leave = *154 ->RegressionTreeLeave(
+missing_leave = *77 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2034,20 +1211,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *155 ->RegressionTreeNode(
+left_node = *78 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *156 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0268456375838926169 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
@@ -2055,7 +1221,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *157 ->RegressionTreeLeave(
+missing_leave = *79 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2068,47 +1234,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *158 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *159 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0201342281879194618 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *156  ;
-right_node = *160 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *80 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *161 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.248322147651006547 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.241610738255033403 ;
-weighted_squared_targets_sum = 0.241610738255033403 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0130600399056774452 0 0.0130600399056774452 ] ;
 split_col = 1 ;
@@ -2116,7 +1249,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0123042505592841078 ;
 missing_node = *0 ;
-missing_leave = *162 ->RegressionTreeLeave(
+missing_leave = *81 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2129,45 +1262,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *163 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0067114093959731386 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0067114093959731386 ;
-weighted_squared_targets_sum = 0.0067114093959731386 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *164 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 36 ;
-weights_sum = 0.241610738255033403 ;
-targets_sum = 35 ;
-weighted_targets_sum = 0.234899328859060258 ;
-weighted_squared_targets_sum = 0.234899328859060258 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *161   )
+right_leave = *0  )
 ;
-right_leave = *153   )
+right_leave = *0  )
 ;
-priority_queue = *165 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 4 ;
-nodes = 5 [ *137  *142  *160  *147  *0 ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *129  ;
-split_cols = 4 [ 1 3 3 0 ] ;
-split_values = 4 [ 0.575210824891620121 0.861099804177139827 0.0434674056274751697 0.591195353843563365 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -2183,7 +1291,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*166 ->RegressionTree(
+*82 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -2191,32 +1299,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *167 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *168 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *83 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *169 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 148 ;
-weights_sum = 1.00000000000000222 ;
-targets_sum = 95 ;
-weighted_targets_sum = 0.641891891891891997 ;
-weighted_squared_targets_sum = 0.641891891891891997 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.459733382030681148 0 0.459733382030681148 ] ;
 split_col = 2 ;
@@ -2224,7 +1310,7 @@
 split_feature_value = 0.00132212183813046336 ;
 after_split_error = 0.430920430920432751 ;
 missing_node = *0 ;
-missing_leave = *170 ->RegressionTreeLeave(
+missing_leave = *84 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2236,20 +1322,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *171 ->RegressionTreeNode(
+left_node = *85 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *172 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0337837837837837857 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -2257,7 +1332,7 @@
 split_feature_value = 0.0480606101777627803 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *173 ->RegressionTreeLeave(
+missing_leave = *86 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2270,47 +1345,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *174 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *175 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0270270270270270285 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *172  ;
-right_node = *176 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *87 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *177 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 143 ;
-weights_sum = 0.966216216216218338 ;
-targets_sum = 95 ;
-weighted_targets_sum = 0.641891891891891997 ;
-weighted_squared_targets_sum = 0.641891891891891997 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.430920430920432751 0 0.430920430920432751 ] ;
 split_col = 2 ;
@@ -2318,7 +1360,7 @@
 split_feature_value = 0.0173550052261667587 ;
 after_split_error = 0.387008857597093914 ;
 missing_node = *0 ;
-missing_leave = *178 ->RegressionTreeLeave(
+missing_leave = *88 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2330,20 +1372,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *179 ->RegressionTreeNode(
+left_node = *89 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *180 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.162162162162162116 ;
-targets_sum = 24 ;
-weighted_targets_sum = 0.162162162162162116 ;
-weighted_squared_targets_sum = 0.162162162162162116 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -2351,7 +1382,7 @@
 split_feature_value = 0.135512975844814643 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *181 ->RegressionTreeLeave(
+missing_leave = *90 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2364,47 +1395,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *182 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675677101 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675677101 ;
-weighted_squared_targets_sum = 0.00675675675675677101 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *183 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.155405405405405372 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.155405405405405372 ;
-weighted_squared_targets_sum = 0.155405405405405372 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *180  ;
-right_node = *184 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *91 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *185 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 119 ;
-weights_sum = 0.804054054054055167 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.479729729729729049 ;
-weighted_squared_targets_sum = 0.479729729729729049 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.387008857597093914 0 0.387008857597093914 ] ;
 split_col = 2 ;
@@ -2412,7 +1410,7 @@
 split_feature_value = 0.0689879291310910303 ;
 after_split_error = 0.351230694980696034 ;
 missing_node = *0 ;
-missing_leave = *186 ->RegressionTreeLeave(
+missing_leave = *92 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2424,20 +1422,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *187 ->RegressionTreeNode(
+left_node = *93 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *188 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0472972972972972999 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -2445,7 +1432,7 @@
 split_feature_value = 0.510088881577538289 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *189 ->RegressionTreeLeave(
+missing_leave = *94 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2458,47 +1445,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *190 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *191 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0405405405405405428 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *188  ;
-right_node = *192 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *95 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *193 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 112 ;
-weights_sum = 0.756756756756757576 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.479729729729729049 ;
-weighted_squared_targets_sum = 0.479729729729729049 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.351230694980696034 0 0.351230694980696034 ] ;
 split_col = 3 ;
@@ -2506,7 +1460,7 @@
 split_feature_value = 0.141930657011306749 ;
 after_split_error = 0.314935988620199336 ;
 missing_node = *0 ;
-missing_leave = *194 ->RegressionTreeLeave(
+missing_leave = *96 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2518,20 +1472,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *195 ->RegressionTreeNode(
+left_node = *97 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *196 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.114864864864864871 ;
-targets_sum = 17 ;
-weighted_targets_sum = 0.114864864864864871 ;
-weighted_squared_targets_sum = 0.114864864864864871 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 2 ;
@@ -2539,7 +1482,7 @@
 split_feature_value = 0.122353510242232788 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *197 ->RegressionTreeLeave(
+missing_leave = *98 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2552,47 +1495,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *198 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675675713 ;
-weighted_squared_targets_sum = 0.00675675675675675713 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *199 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 16 ;
-weights_sum = 0.108108108108108114 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.108108108108108114 ;
-weighted_squared_targets_sum = 0.108108108108108114 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *196  ;
-right_node = *200 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *99 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *201 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 95 ;
-weights_sum = 0.641891891891891997 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.364864864864864413 ;
-weighted_squared_targets_sum = 0.364864864864864413 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.314935988620199336 0 0.314935988620199336 ] ;
 split_col = 3 ;
@@ -2600,7 +1510,7 @@
 split_feature_value = 0.16348885181551448 ;
 after_split_error = 0.249176005273566148 ;
 missing_node = *0 ;
-missing_leave = *202 ->RegressionTreeLeave(
+missing_leave = *100 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2613,49 +1523,24 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *203 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675677101 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675677101 ;
-weighted_squared_targets_sum = 0.00675675675675677101 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *204 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.635135135135135198 ;
-targets_sum = 53 ;
-weighted_targets_sum = 0.35810810810810767 ;
-weighted_squared_targets_sum = 0.35810810810810767 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *201   )
+right_leave = *0  )
 ;
-right_leave = *193   )
+right_leave = *0  )
 ;
-right_leave = *185   )
+right_leave = *0  )
 ;
-right_leave = *177   )
+right_leave = *0  )
 ;
-priority_queue = *205 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *200  *171  *187  *179  *195  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *169  ;
-split_cols = 4 [ 2 2 2 3 ] ;
-split_values = 4 [ 0.00132212183813046336 0.0173550052261667587 0.0689879291310910303 0.141930657011306749 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -2674,10 +1559,10 @@
 ] ;
 voting_weights = 5 [ 1.13671112060546875 1.15143966674804688 0.711639404296875 0.9248046875 0.414886474609375 ] ;
 sum_voting_weights = 4.33948135375976562 ;
-initial_sum_weights = 1 ;
-example_weights = 150 [ 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00506463385355691766 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0137597517091007961 0.000521450777360935947 0.00496250851640710373 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.116156001516510912 0.0331601320626456475 0.000521450777360935947 0.00331503764440590045 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0481987748833352658 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00521604231874619236 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0760294877744166486 0.0011955813513476177 0.000521450777360935947 0.0005214507773609359!
 47 0.000521450777360935947 0.00521604231874619236 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590045 0.0011955813513476177 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.00506463385355691766 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0011955813513476177 0.0011955813513476177 0.000521450777360935947 0.000521450777360935947 0.00496250851640710373 0.00216438966752365231 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.00216438966752365231 0.000521450777360935947 0.0011955813513476177 0.000521450777360935947 0.000521450777360935947 0.0321975776211230677 0.0481987748833352658 0.000521450777360935947 0.00216438966752365231 0.0321975776211230677 0.!
 0216502661232893662 0.000521450777360935947 0.0005214507773609!
 35947 0.
000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00216438966752365231 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590045 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0481987748833352658 0.000521450777360935947 0.000521450777360935947 0.0481987748833352658 0.0210218138668945981 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590045 0.000521450777360935947 0.000521450777360935947 0.0481987748833352658 0.00331503764440590045 0.00521604231874619236 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00216438966752365231 0.00506463385355691766 0.0210218138668945981 0.0331601320626456475 0.0321975776211230677 0.0496!
 396890224618387 0.000521450777360935947 0.00521604231874619236 0.00521604231874619236 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00521604231874619236 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0011955813513476177 0.000521450777360935947 ] ;
-learners_error = 5 [ 0.0933333333333333376 0.090863047965243568 0.194138154481255015 0.135911099894630238 0.303623351488696736 ] ;
-weak_learner_template = *206 ->RegressionTree(
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00506463385355691853 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0137597517091007995 0.000521450777360935947 0.00496250851640710373 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.116156001516510926 0.0331601320626456475 0.000521450777360935947 0.00331503764440590001 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0760294877744166625 0.00119558135134761748 0.000521450777360935947 0.000521450777360935!
 947 0.000521450777360935947 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590001 0.00119558135134761748 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.00506463385355691853 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00119558135134761748 0.00119558135134761748 0.000521450777360935947 0.000521450777360935947 0.00496250851640710373 0.00216438966752365231 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.00216438966752365231 0.000521450777360935947 0.00119558135134761748 0.000521450777360935947 0.000521450777360935947 0.0321975776211230746 0.0481987748833352936 0.000521450777360935947 0.00216438966752365231 0.03219757762112307!
 46 0.0216502661232893662 0.000521450777360935947 0.00052145077!
 73609359
47 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00216438966752365231 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590001 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.0210218138668946085 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00331503764440590001 0.000521450777360935947 0.000521450777360935947 0.0481987748833352936 0.00331503764440590001 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00216438966752365231 0.00506463385355691853 0.0210218138668946085 0.0331601320626456475 0.0321975776211230746 0!
 .0496396890224618387 0.000521450777360935947 0.00521604231874619323 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.0331601320626456475 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00521604231874619323 0.000521450777360935947 0.000521450777360935947 0.000521450777360935947 0.00119558135134761748 0.000521450777360935947 ] ;
+learners_error = 5 [ 0.0933333333333331433 0.0908630479652438872 0.194138154481255237 0.135911099894630349 0.303623351488696902 ] ;
+weak_learner_template = *101 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -2685,7 +1570,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *207 ->RegressionTreeLeave(
+leave_template = *102 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -2710,7 +1595,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 5 ;
 report_progress = 0 ;
@@ -2730,7 +1615,7 @@
 save_often = 1 ;
 compute_training_error = 1 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 0 ;
+modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
@@ -2738,7 +1623,7 @@
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 5 ;
 report_progress = 0 ;
@@ -2774,10 +1659,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *208 ->HyperOptimize(
+strategy = 1 [ *103 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *209 ->EarlyStoppingOracle(
+oracle = *104 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_conf_rated_adaboost/expected_results/expdir/metainfos.txt	2008-12-10 14:58:55 UTC (rev 9768)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9611"
+__REVISION__ = "PL9752"
 conf                                          = True
 pseudo                                        = False

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/final_learner.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -239,32 +239,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *7 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *8 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *7 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *9 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 159 ;
-weights_sum = 0.999999999999999889 ;
-targets_sum = 99 ;
-weighted_targets_sum = 0.622641509433962237 ;
-weighted_squared_targets_sum = 0.622641509433962237 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.469918120327518563 0 0.469918120327518563 ] ;
 split_col = 2 ;
@@ -272,7 +250,7 @@
 split_feature_value = 0.188677163392186542 ;
 after_split_error = 0.165982151128678812 ;
 missing_node = *0 ;
-missing_leave = *10 ->RegressionTreeLeave(
+missing_leave = *8 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -284,20 +262,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *11 ->RegressionTreeNode(
+left_node = *9 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *12 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 65 ;
-weights_sum = 0.408805031446540845 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.0628930817610062892 ;
-weighted_squared_targets_sum = 0.0628930817610062892 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.106434446057087573 0 0.106434446057087573 ] ;
 split_col = 4 ;
@@ -305,7 +272,7 @@
 split_feature_value = 8.88178419700125232e-16 ;
 after_split_error = 0.0733752620545073397 ;
 missing_node = *0 ;
-missing_leave = *13 ->RegressionTreeLeave(
+missing_leave = *10 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -317,20 +284,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *14 ->RegressionTreeNode(
+left_node = *11 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *15 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.257861635220125784 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -338,7 +294,7 @@
 split_feature_value = 0.185353117285409069 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *16 ->RegressionTreeLeave(
+missing_leave = *12 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -351,47 +307,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *17 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610067958 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *18 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.251572327044025157 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *15  ;
-right_node = *19 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *13 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *20 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.150943396226415089 ;
-targets_sum = 10 ;
-weighted_targets_sum = 0.0628930817610062892 ;
-weighted_squared_targets_sum = 0.0628930817610062892 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0733752620545073397 0 0.0733752620545073397 ] ;
 split_col = 4 ;
@@ -399,7 +322,7 @@
 split_feature_value = 3.44335671087492301e-13 ;
 after_split_error = 0.0528301886792452852 ;
 missing_node = *0 ;
-missing_leave = *21 ->RegressionTreeLeave(
+missing_leave = *14 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -411,20 +334,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *22 ->RegressionTreeNode(
+left_node = *15 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *23 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0251572327044025171 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0251572327044025171 ;
-weighted_squared_targets_sum = 0.0251572327044025171 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -432,7 +344,7 @@
 split_feature_value = 2.63677968348474678e-15 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *24 ->RegressionTreeLeave(
+missing_leave = *16 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -445,55 +357,22 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *25 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.006289308176100631 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.006289308176100631 ;
-weighted_squared_targets_sum = 0.006289308176100631 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *26 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0188679245283018895 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0188679245283018895 ;
-weighted_squared_targets_sum = 0.0188679245283018895 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *23  ;
-right_node = *27 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *17 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *28 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.125786163522012578 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0377358490566037721 ;
-weighted_squared_targets_sum = 0.0377358490566037721 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0528301886792452852 0 0.0528301886792452852 ] ;
 split_col = 2 ;
 split_balance = 10 ;
 split_feature_value = 0.0166672042033834122 ;
-after_split_error = 0.045283018867924546 ;
+after_split_error = 0.0452830188679245321 ;
 missing_node = *0 ;
-missing_leave = *29 ->RegressionTreeLeave(
+missing_leave = *18 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -506,51 +385,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *30 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610065182 ;
-targets_sum = 0 ;
-weighted_targets_sum = 1.73472347597680709e-18 ;
-weighted_squared_targets_sum = 1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *31 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.119496855345911937 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0377358490566037721 ;
-weighted_squared_targets_sum = 0.0377358490566037721 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *28   )
+right_leave = *0  )
 ;
-right_leave = *20   )
+right_leave = *0  )
 ;
-left_leave = *12  ;
-right_node = *32 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *19 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *33 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.591194968553459099 ;
-targets_sum = 89 ;
-weighted_targets_sum = 0.559748427672955962 ;
-weighted_squared_targets_sum = 0.559748427672955962 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0595477050715911282 0 0.0595477050715911282 ] ;
 split_col = 3 ;
@@ -558,7 +404,7 @@
 split_feature_value = 0.170219082142077621 ;
 after_split_error = 0.0448775543115166875 ;
 missing_node = *0 ;
-missing_leave = *34 ->RegressionTreeLeave(
+missing_leave = *20 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -570,20 +416,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *35 ->RegressionTreeNode(
+left_node = *21 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *36 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0188679245283018895 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610062927 ;
-weighted_squared_targets_sum = 0.00628930817610062927 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.00838574423480083903 0 0.00838574423480083903 ] ;
 split_col = 3 ;
@@ -591,7 +426,7 @@
 split_feature_value = 0.141699018667860999 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *37 ->RegressionTreeLeave(
+missing_leave = *22 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -604,47 +439,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *38 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.006289308176100631 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *39 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 2 ;
-weights_sum = 0.0125786163522012585 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610062927 ;
-weighted_squared_targets_sum = 0.00628930817610062927 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *36  ;
-right_node = *40 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *23 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *41 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 91 ;
-weights_sum = 0.572327044025157217 ;
-targets_sum = 88 ;
-weighted_targets_sum = 0.553459119496855334 ;
-weighted_squared_targets_sum = 0.553459119496855334 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0364918100767157583 0 0.0364918100767157583 ] ;
 split_col = 1 ;
@@ -652,7 +454,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0332075471698113078 ;
 missing_node = *0 ;
-missing_leave = *42 ->RegressionTreeLeave(
+missing_leave = *24 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -665,45 +467,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *43 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00628930817610073509 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00628930817610073509 ;
-weighted_squared_targets_sum = 0.00628930817610073509 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *44 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 90 ;
-weights_sum = 0.566037735849056589 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.547169811320754707 ;
-weighted_squared_targets_sum = 0.547169811320754707 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *41   )
+right_leave = *0  )
 ;
-right_leave = *33   )
+right_leave = *0  )
 ;
-priority_queue = *45 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *35  *27  *22  *14  *40  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *9  ;
-split_cols = 4 [ 2 4 4 3 ] ;
-split_values = 4 [ 0.188677163392186542 8.88178419700125232e-16 3.44335671087492301e-13 0.170219082142077621 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -719,7 +496,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*46 ->RegressionTree(
+*25 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -727,32 +504,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *47 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *48 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *26 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *49 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.999999999999998779 ;
-targets_sum = 93 ;
-weighted_targets_sum = 0.624161073825502677 ;
-weighted_squared_targets_sum = 0.624161073825502677 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.469168055492995228 0 0.469168055492995228 ] ;
 split_col = 1 ;
@@ -760,7 +515,7 @@
 split_feature_value = 0.482293993618237549 ;
 after_split_error = 0.260686628807433929 ;
 missing_node = *0 ;
-missing_leave = *50 ->RegressionTreeLeave(
+missing_leave = *27 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -772,20 +527,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *51 ->RegressionTreeNode(
+left_node = *28 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *52 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 45 ;
-weights_sum = 0.302013422818791732 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0402684563758389236 ;
-weighted_squared_targets_sum = 0.0402684563758389236 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.069798657718120799 0 0.069798657718120799 ] ;
 split_col = 2 ;
@@ -793,7 +537,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.0130872483221476481 ;
 missing_node = *0 ;
-missing_leave = *53 ->RegressionTreeLeave(
+missing_leave = *29 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -805,20 +549,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *54 ->RegressionTreeNode(
+left_node = *30 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *55 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 40 ;
-weights_sum = 0.268456375838926009 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315422 ;
-weighted_squared_targets_sum = 0.00671140939597315422 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0130872483221476498 0 0.0130872483221476498 ] ;
 split_col = 1 ;
@@ -826,7 +559,7 @@
 split_feature_value = 0.414475818795681961 ;
 after_split_error = 0.0120805369127516774 ;
 missing_node = *0 ;
-missing_leave = *56 ->RegressionTreeLeave(
+missing_leave = *31 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -839,47 +572,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *57 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *58 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 39 ;
-weights_sum = 0.261744966442952864 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315422 ;
-weighted_squared_targets_sum = 0.00671140939597315422 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *55  ;
-right_node = *59 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *32 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *60 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.033557046979865772 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.033557046979865772 ;
-weighted_squared_targets_sum = 0.033557046979865772 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -887,7 +587,7 @@
 split_feature_value = 1.66171551518878857e-09 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *61 ->RegressionTreeLeave(
+missing_leave = *33 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -900,49 +600,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *62 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315248 ;
-weighted_squared_targets_sum = 0.00671140939597315248 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *63 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0268456375838926169 ;
-targets_sum = 4 ;
-weighted_targets_sum = 0.0268456375838926169 ;
-weighted_squared_targets_sum = 0.0268456375838926169 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *60   )
+right_leave = *0  )
 ;
-left_leave = *52  ;
-right_node = *64 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *34 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *65 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 104 ;
-weights_sum = 0.697986577181207268 ;
-targets_sum = 87 ;
-weighted_targets_sum = 0.583892617449663809 ;
-weighted_squared_targets_sum = 0.583892617449663809 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.190887971089313102 0 0.190887971089313102 ] ;
 split_col = 4 ;
@@ -950,7 +617,7 @@
 split_feature_value = 0.999999999538717876 ;
 after_split_error = 0.145761307736665707 ;
 missing_node = *0 ;
-missing_leave = *66 ->RegressionTreeLeave(
+missing_leave = *35 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -962,20 +629,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *67 ->RegressionTreeNode(
+left_node = *36 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *68 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.248322147651006547 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.147651006711409377 ;
-weighted_squared_targets_sum = 0.147651006711409377 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.119717032468710211 0 0.119717032468710211 ] ;
 split_col = 4 ;
@@ -983,7 +639,7 @@
 split_feature_value = 0.725372806051693186 ;
 after_split_error = 0.033557046979865772 ;
 missing_node = *0 ;
-missing_leave = *69 ->RegressionTreeLeave(
+missing_leave = *37 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -995,20 +651,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *70 ->RegressionTreeNode(
+left_node = *38 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *71 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.127516778523489943 ;
-targets_sum = 19 ;
-weighted_targets_sum = 0.127516778523489943 ;
-weighted_squared_targets_sum = 0.127516778523489943 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -1016,7 +661,7 @@
 split_feature_value = 0.00609705032829710447 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *72 ->RegressionTreeLeave(
+missing_leave = *39 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1029,47 +674,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *73 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0067114093959731386 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0067114093959731386 ;
-weighted_squared_targets_sum = 0.0067114093959731386 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *74 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 18 ;
-weights_sum = 0.120805369127516798 ;
-targets_sum = 18 ;
-weighted_targets_sum = 0.120805369127516798 ;
-weighted_squared_targets_sum = 0.120805369127516798 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *71  ;
-right_node = *75 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *40 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *76 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 18 ;
-weights_sum = 0.120805369127516798 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0201342281879194618 ;
-weighted_squared_targets_sum = 0.0201342281879194618 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.033557046979865772 0 0.033557046979865772 ] ;
 split_col = 4 ;
@@ -1077,7 +689,7 @@
 split_feature_value = 0.999935792696207582 ;
 after_split_error = 0.0100671140939597309 ;
 missing_node = *0 ;
-missing_leave = *77 ->RegressionTreeLeave(
+missing_leave = *41 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1090,49 +702,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *78 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *79 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.11409395973154364 ;
-targets_sum = 3 ;
-weighted_targets_sum = 0.0201342281879194618 ;
-weighted_squared_targets_sum = 0.0201342281879194618 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *76   )
+right_leave = *0  )
 ;
-left_leave = *68  ;
-right_node = *80 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *42 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *81 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 67 ;
-weights_sum = 0.449664429530200915 ;
-targets_sum = 65 ;
-weighted_targets_sum = 0.436241610738254626 ;
-weighted_squared_targets_sum = 0.436241610738254626 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0260442752679554967 0 0.0260442752679554967 ] ;
 split_col = 3 ;
@@ -1140,7 +719,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0132194427496440392 ;
 missing_node = *0 ;
-missing_leave = *82 ->RegressionTreeLeave(
+missing_leave = *43 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1153,45 +732,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *83 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597316636 ;
-weighted_squared_targets_sum = 0.00671140939597316636 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *84 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 66 ;
-weights_sum = 0.442953020134227771 ;
-targets_sum = 64 ;
-weighted_targets_sum = 0.429530201342281481 ;
-weighted_squared_targets_sum = 0.429530201342281481 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *81   )
+right_leave = *0  )
 ;
-right_leave = *65   )
+right_leave = *0  )
 ;
-priority_queue = *85 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *75  *80  *54  *70  *59  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *49  ;
-split_cols = 4 [ 1 2 4 4 ] ;
-split_values = 4 [ 0.482293993618237549 0.885239426681956321 0.999999999538717876 0.725372806051693186 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -1207,7 +761,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*86 ->RegressionTree(
+*44 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1215,32 +769,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *87 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *88 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *45 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *89 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 173 ;
-weights_sum = 1.00000000000000422 ;
-targets_sum = 91 ;
-weighted_targets_sum = 0.526011560693641633 ;
-weighted_squared_targets_sum = 0.526011560693641633 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.498646797420564281 0 0.498646797420564281 ] ;
 split_col = 2 ;
@@ -1248,7 +780,7 @@
 split_feature_value = 0.995245802370935517 ;
 after_split_error = 0.376402584155050013 ;
 missing_node = *0 ;
-missing_leave = *90 ->RegressionTreeLeave(
+missing_leave = *46 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1260,20 +792,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *91 ->RegressionTreeNode(
+left_node = *47 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *92 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 136 ;
-weights_sum = 0.786127167630060186 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.376402584155050013 0 0.376402584155050013 ] ;
 split_col = 2 ;
@@ -1281,7 +802,7 @@
 split_feature_value = 0.994286124455373455 ;
 after_split_error = 0.31500238638171546 ;
 missing_node = *0 ;
-missing_leave = *93 ->RegressionTreeLeave(
+missing_leave = *48 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1293,20 +814,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *94 ->RegressionTreeNode(
+left_node = *49 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *95 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 109 ;
-weights_sum = 0.630057803468209054 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.31500238638171546 0 0.31500238638171546 ] ;
 split_col = 4 ;
@@ -1314,7 +824,7 @@
 split_feature_value = 8.32667268468867405e-16 ;
 after_split_error = 0.24550237059167368 ;
 missing_node = *0 ;
-missing_leave = *96 ->RegressionTreeLeave(
+missing_leave = *50 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1326,20 +836,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *97 ->RegressionTreeNode(
+left_node = *51 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *98 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 20 ;
-weights_sum = 0.115606936416184927 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -1347,7 +846,7 @@
 split_feature_value = 0.180062798802320762 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *99 ->RegressionTreeLeave(
+missing_leave = *52 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1360,47 +859,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *100 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080924306 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *101 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 19 ;
-weights_sum = 0.109826589595375682 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *98  ;
-right_node = *102 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *53 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *103 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 89 ;
-weights_sum = 0.514450867052023031 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.312138728323699322 ;
-weighted_squared_targets_sum = 0.312138728323699322 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.24550237059167368 0 0.24550237059167368 ] ;
 split_col = 0 ;
@@ -1408,7 +874,7 @@
 split_feature_value = 0.242853945270868343 ;
 after_split_error = 0.19876061032130396 ;
 missing_node = *0 ;
-missing_leave = *104 ->RegressionTreeLeave(
+missing_leave = *54 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1420,20 +886,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *105 ->RegressionTreeNode(
+left_node = *55 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *106 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 15 ;
-weights_sum = 0.0867052023121386989 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0115606936416184965 ;
-weighted_squared_targets_sum = 0.0115606936416184965 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0200385356454720609 0 0.0200385356454720609 ] ;
 split_col = 1 ;
@@ -1441,7 +896,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0107349298100743173 ;
 missing_node = *0 ;
-missing_leave = *107 ->RegressionTreeLeave(
+missing_leave = *56 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1454,47 +909,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *108 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080924306 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *109 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 14 ;
-weights_sum = 0.0809248554913294532 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0115606936416184965 ;
-weighted_squared_targets_sum = 0.0115606936416184965 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *106  ;
-right_node = *110 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *57 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *111 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 74 ;
-weights_sum = 0.427745664739884235 ;
-targets_sum = 52 ;
-weighted_targets_sum = 0.30057803468208083 ;
-weighted_squared_targets_sum = 0.30057803468208083 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.178722074675831843 0 0.178722074675831843 ] ;
 split_col = 2 ;
@@ -1502,7 +924,7 @@
 split_feature_value = 0.885239426681956321 ;
 after_split_error = 0.165519772456188596 ;
 missing_node = *0 ;
-missing_leave = *112 ->RegressionTreeLeave(
+missing_leave = *58 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1515,51 +937,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *113 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0057803468208092847 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0057803468208092847 ;
-weighted_squared_targets_sum = 0.0057803468208092847 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *114 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 73 ;
-weights_sum = 0.421965317919074989 ;
-targets_sum = 51 ;
-weighted_targets_sum = 0.294797687861271585 ;
-weighted_squared_targets_sum = 0.294797687861271585 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *111   )
+right_leave = *0  )
 ;
-right_leave = *103   )
+right_leave = *0  )
 ;
-left_leave = *95  ;
-right_node = *115 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *59 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *116 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 27 ;
-weights_sum = 0.156069364161849661 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
@@ -1567,7 +956,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *117 ->RegressionTreeLeave(
+missing_leave = *60 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1580,49 +969,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *118 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080925694 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *119 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 26 ;
-weights_sum = 0.150289017341040415 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *116   )
+right_leave = *0  )
 ;
-left_leave = *92  ;
-right_node = *120 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *61 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *121 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.213872832369942117 ;
-targets_sum = 37 ;
-weighted_targets_sum = 0.213872832369942117 ;
-weighted_squared_targets_sum = 0.213872832369942117 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -1630,7 +986,7 @@
 split_feature_value = 0.893509913423101043 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *122 ->RegressionTreeLeave(
+missing_leave = *62 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1643,43 +999,18 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *123 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00578034682080925694 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00578034682080925694 ;
-weighted_squared_targets_sum = 0.00578034682080925694 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *124 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 36 ;
-weights_sum = 0.208092485549132872 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.208092485549132872 ;
-weighted_squared_targets_sum = 0.208092485549132872 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *121   )
+right_leave = *0  )
 ;
-priority_queue = *125 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 4 ;
-nodes = 5 [ *110  *105  *97  *120  *0 ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *89  ;
-split_cols = 4 [ 2 2 4 0 ] ;
-split_values = 4 [ 0.995245802370935517 0.994286124455373455 8.32667268468867405e-16 0.242853945270868343 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -1695,7 +1026,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*126 ->RegressionTree(
+*63 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -1703,32 +1034,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *127 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *128 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *64 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *129 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 149 ;
-weights_sum = 0.999999999999998779 ;
-targets_sum = 58 ;
-weighted_targets_sum = 0.389261744966442613 ;
-weighted_squared_targets_sum = 0.389261744966442613 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.475474077744245216 0 0.475474077744245216 ] ;
 split_col = 1 ;
@@ -1736,7 +1045,7 @@
 split_feature_value = 0.575210824891620121 ;
 after_split_error = 0.29407734793231588 ;
 missing_node = *0 ;
-missing_leave = *130 ->RegressionTreeLeave(
+missing_leave = *65 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1748,20 +1057,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *131 ->RegressionTreeNode(
+left_node = *66 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *132 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 108 ;
-weights_sum = 0.724832214765099847 ;
-targets_sum = 22 ;
-weighted_targets_sum = 0.147651006711409377 ;
-weighted_squared_targets_sum = 0.147651006711409377 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.235147899577429681 0 0.235147899577429681 ] ;
 split_col = 3 ;
@@ -1769,7 +1067,7 @@
 split_feature_value = 0.861099804177139827 ;
 after_split_error = 0.181076457428609006 ;
 missing_node = *0 ;
-missing_leave = *133 ->RegressionTreeLeave(
+missing_leave = *67 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1781,20 +1079,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *134 ->RegressionTreeNode(
+left_node = *68 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *135 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 102 ;
-weights_sum = 0.684563758389260979 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.107382550335570481 ;
-weighted_squared_targets_sum = 0.107382550335570481 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.181076457428609006 0 0.181076457428609006 ] ;
 split_col = 0 ;
@@ -1802,7 +1089,7 @@
 split_feature_value = 0.591195353843563365 ;
 after_split_error = 0.158960974397215959 ;
 missing_node = *0 ;
-missing_leave = *136 ->RegressionTreeLeave(
+missing_leave = *69 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1814,20 +1101,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *137 ->RegressionTreeNode(
+left_node = *70 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *138 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 48 ;
-weights_sum = 0.322147651006711166 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.0939597315436241642 ;
-weighted_squared_targets_sum = 0.0939597315436241642 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.133109619686800851 0 0.133109619686800851 ] ;
 split_col = 3 ;
@@ -1835,7 +1111,7 @@
 split_feature_value = 0.187798288368589333 ;
 after_split_error = 0.111558538404175941 ;
 missing_node = *0 ;
-missing_leave = *139 ->RegressionTreeLeave(
+missing_leave = *71 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1848,47 +1124,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *140 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = -1.73472347597680709e-18 ;
-weighted_squared_targets_sum = -1.73472347597680709e-18 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *141 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 47 ;
-weights_sum = 0.315436241610738022 ;
-targets_sum = 14 ;
-weighted_targets_sum = 0.0939597315436241642 ;
-weighted_squared_targets_sum = 0.0939597315436241642 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *138  ;
-right_node = *142 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *72 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *143 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 54 ;
-weights_sum = 0.362416107382550035 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0134228187919463084 ;
-weighted_squared_targets_sum = 0.0134228187919463084 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0.0258513547104151122 0 0.0258513547104151122 ] ;
 split_col = 1 ;
@@ -1896,7 +1139,7 @@
 split_feature_value = 0.569140400436275673 ;
 after_split_error = 0.0131695580600227936 ;
 missing_node = *0 ;
-missing_leave = *144 ->RegressionTreeLeave(
+missing_leave = *73 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1909,49 +1152,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *145 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597316636 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *146 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 53 ;
-weights_sum = 0.35570469798657689 ;
-targets_sum = 2 ;
-weighted_targets_sum = 0.0134228187919463084 ;
-weighted_squared_targets_sum = 0.0134228187919463084 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *143   )
+right_leave = *0  )
 ;
-left_leave = *135  ;
-right_node = *147 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *74 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *148 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0402684563758389236 ;
-targets_sum = 6 ;
-weighted_targets_sum = 0.0402684563758389236 ;
-weighted_squared_targets_sum = 0.0402684563758389236 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -1959,7 +1169,7 @@
 split_feature_value = 0.999999941905052481 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *149 ->RegressionTreeLeave(
+missing_leave = *75 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -1972,49 +1182,16 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *150 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00671140939597315248 ;
-weighted_squared_targets_sum = 0.00671140939597315248 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *151 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.033557046979865772 ;
-targets_sum = 5 ;
-weighted_targets_sum = 0.033557046979865772 ;
-weighted_squared_targets_sum = 0.033557046979865772 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *148   )
+right_leave = *0  )
 ;
-left_leave = *132  ;
-right_node = *152 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *76 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *153 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 41 ;
-weights_sum = 0.275167785234899154 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.241610738255033403 ;
-weighted_squared_targets_sum = 0.241610738255033403 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0589294483548861714 0 0.0589294483548861714 ] ;
 split_col = 3 ;
@@ -2022,7 +1199,7 @@
 split_feature_value = 0.0434674056274751697 ;
 after_split_error = 0.0130600399056774452 ;
 missing_node = *0 ;
-missing_leave = *154 ->RegressionTreeLeave(
+missing_leave = *77 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2034,20 +1211,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *155 ->RegressionTreeNode(
+left_node = *78 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *156 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0268456375838926169 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = -1 ;
@@ -2055,7 +1221,7 @@
 split_feature_value = 1.79769313486231571e+308 ;
 after_split_error = 1.79769313486231571e+308 ;
 missing_node = *0 ;
-missing_leave = *157 ->RegressionTreeLeave(
+missing_leave = *79 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2068,47 +1234,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *158 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00671140939597315248 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *159 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 3 ;
-weights_sum = 0.0201342281879194618 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *156  ;
-right_node = *160 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *80 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *161 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 37 ;
-weights_sum = 0.248322147651006547 ;
-targets_sum = 36 ;
-weighted_targets_sum = 0.241610738255033403 ;
-weighted_squared_targets_sum = 0.241610738255033403 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.0130600399056774452 0 0.0130600399056774452 ] ;
 split_col = 1 ;
@@ -2116,7 +1249,7 @@
 split_feature_value = 0.689386369193649928 ;
 after_split_error = 0.0123042505592841078 ;
 missing_node = *0 ;
-missing_leave = *162 ->RegressionTreeLeave(
+missing_leave = *81 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2129,45 +1262,20 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *163 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.0067114093959731386 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.0067114093959731386 ;
-weighted_squared_targets_sum = 0.0067114093959731386 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *164 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 36 ;
-weights_sum = 0.241610738255033403 ;
-targets_sum = 35 ;
-weighted_targets_sum = 0.234899328859060258 ;
-weighted_squared_targets_sum = 0.234899328859060258 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *161   )
+right_leave = *0  )
 ;
-right_leave = *153   )
+right_leave = *0  )
 ;
-priority_queue = *165 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 4 ;
-nodes = 5 [ *137  *142  *160  *147  *0 ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *129  ;
-split_cols = 4 [ 1 3 3 0 ] ;
-split_values = 4 [ 0.575210824891620121 0.861099804177139827 0.0434674056274751697 0.591195353843563365 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -2183,7 +1291,7 @@
 save_trainingset_prefix = "" ;
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
-*166 ->RegressionTree(
+*82 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -2191,32 +1299,10 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *167 ->RegressionTreeLeave(
-id = -1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 0 ;
-weights_sum = 0 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
-root = *168 ->RegressionTreeNode(
+leave_template = *0 ;
+root = *83 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *169 ->RegressionTreeLeave(
-id = 1 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 148 ;
-weights_sum = 1.00000000000000222 ;
-targets_sum = 95 ;
-weighted_targets_sum = 0.641891891891891997 ;
-weighted_squared_targets_sum = 0.641891891891891997 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.459733382030681148 0 0.459733382030681148 ] ;
 split_col = 2 ;
@@ -2224,7 +1310,7 @@
 split_feature_value = 0.00132212183813046336 ;
 after_split_error = 0.430920430920432751 ;
 missing_node = *0 ;
-missing_leave = *170 ->RegressionTreeLeave(
+missing_leave = *84 ->RegressionTreeLeave(
 id = 2 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2236,20 +1322,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *171 ->RegressionTreeNode(
+left_node = *85 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *172 ->RegressionTreeLeave(
-id = 3 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 5 ;
-weights_sum = 0.0337837837837837857 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -2257,7 +1332,7 @@
 split_feature_value = 0.0480606101777627803 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *173 ->RegressionTreeLeave(
+missing_leave = *86 ->RegressionTreeLeave(
 id = 5 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2270,47 +1345,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *174 ->RegressionTreeLeave(
-id = 6 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *175 ->RegressionTreeLeave(
-id = 7 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 4 ;
-weights_sum = 0.0270270270270270285 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *172  ;
-right_node = *176 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *87 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *177 ->RegressionTreeLeave(
-id = 4 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 143 ;
-weights_sum = 0.966216216216218338 ;
-targets_sum = 95 ;
-weighted_targets_sum = 0.641891891891891997 ;
-weighted_squared_targets_sum = 0.641891891891891997 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.430920430920432751 0 0.430920430920432751 ] ;
 split_col = 2 ;
@@ -2318,7 +1360,7 @@
 split_feature_value = 0.0173550052261667587 ;
 after_split_error = 0.387008857597093914 ;
 missing_node = *0 ;
-missing_leave = *178 ->RegressionTreeLeave(
+missing_leave = *88 ->RegressionTreeLeave(
 id = 8 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2330,20 +1372,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *179 ->RegressionTreeNode(
+left_node = *89 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *180 ->RegressionTreeLeave(
-id = 9 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 24 ;
-weights_sum = 0.162162162162162116 ;
-targets_sum = 24 ;
-weighted_targets_sum = 0.162162162162162116 ;
-weighted_squared_targets_sum = 0.162162162162162116 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 3 ;
@@ -2351,7 +1382,7 @@
 split_feature_value = 0.135512975844814643 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *181 ->RegressionTreeLeave(
+missing_leave = *90 ->RegressionTreeLeave(
 id = 11 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2364,47 +1395,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *182 ->RegressionTreeLeave(
-id = 12 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675677101 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675677101 ;
-weighted_squared_targets_sum = 0.00675675675675677101 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *183 ->RegressionTreeLeave(
-id = 13 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 23 ;
-weights_sum = 0.155405405405405372 ;
-targets_sum = 23 ;
-weighted_targets_sum = 0.155405405405405372 ;
-weighted_squared_targets_sum = 0.155405405405405372 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *180  ;
-right_node = *184 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *91 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *185 ->RegressionTreeLeave(
-id = 10 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 119 ;
-weights_sum = 0.804054054054055167 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.479729729729729049 ;
-weighted_squared_targets_sum = 0.479729729729729049 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.387008857597093914 0 0.387008857597093914 ] ;
 split_col = 2 ;
@@ -2412,7 +1410,7 @@
 split_feature_value = 0.0689879291310910303 ;
 after_split_error = 0.351230694980696034 ;
 missing_node = *0 ;
-missing_leave = *186 ->RegressionTreeLeave(
+missing_leave = *92 ->RegressionTreeLeave(
 id = 14 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2424,20 +1422,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *187 ->RegressionTreeNode(
+left_node = *93 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *188 ->RegressionTreeLeave(
-id = 15 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 7 ;
-weights_sum = 0.0472972972972972999 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 0 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 4 ;
@@ -2445,7 +1432,7 @@
 split_feature_value = 0.510088881577538289 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *189 ->RegressionTreeLeave(
+missing_leave = *94 ->RegressionTreeLeave(
 id = 17 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2458,47 +1445,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *190 ->RegressionTreeLeave(
-id = 18 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *191 ->RegressionTreeLeave(
-id = 19 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 6 ;
-weights_sum = 0.0405405405405405428 ;
-targets_sum = 0 ;
-weighted_targets_sum = 0 ;
-weighted_squared_targets_sum = 0 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *188  ;
-right_node = *192 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *95 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *193 ->RegressionTreeLeave(
-id = 16 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 112 ;
-weights_sum = 0.756756756756757576 ;
-targets_sum = 71 ;
-weighted_targets_sum = 0.479729729729729049 ;
-weighted_squared_targets_sum = 0.479729729729729049 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.351230694980696034 0 0.351230694980696034 ] ;
 split_col = 3 ;
@@ -2506,7 +1460,7 @@
 split_feature_value = 0.141930657011306749 ;
 after_split_error = 0.314935988620199336 ;
 missing_node = *0 ;
-missing_leave = *194 ->RegressionTreeLeave(
+missing_leave = *96 ->RegressionTreeLeave(
 id = 20 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2518,20 +1472,9 @@
 weighted_squared_targets_sum = 0 ;
 loss_function_factor = 2  )
 ;
-left_node = *195 ->RegressionTreeNode(
+left_node = *97 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *196 ->RegressionTreeLeave(
-id = 21 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 17 ;
-weights_sum = 0.114864864864864871 ;
-targets_sum = 17 ;
-weighted_targets_sum = 0.114864864864864871 ;
-weighted_squared_targets_sum = 0.114864864864864871 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0 0 0 ] ;
 split_col = 2 ;
@@ -2539,7 +1482,7 @@
 split_feature_value = 0.122353510242232788 ;
 after_split_error = 0 ;
 missing_node = *0 ;
-missing_leave = *197 ->RegressionTreeLeave(
+missing_leave = *98 ->RegressionTreeLeave(
 id = 23 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2552,47 +1495,14 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *198 ->RegressionTreeLeave(
-id = 24 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675675713 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675675713 ;
-weighted_squared_targets_sum = 0.00675675675675675713 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *199 ->RegressionTreeLeave(
-id = 25 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 16 ;
-weights_sum = 0.108108108108108114 ;
-targets_sum = 16 ;
-weighted_targets_sum = 0.108108108108108114 ;
-weighted_squared_targets_sum = 0.108108108108108114 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-left_leave = *196  ;
-right_node = *200 ->RegressionTreeNode(
+left_leave = *0 ;
+right_node = *99 ->RegressionTreeNode(
 missing_is_valid = 0 ;
-leave = *201 ->RegressionTreeLeave(
-id = 22 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 95 ;
-weights_sum = 0.641891891891891997 ;
-targets_sum = 54 ;
-weighted_targets_sum = 0.364864864864864413 ;
-weighted_squared_targets_sum = 0.364864864864864413 ;
-loss_function_factor = 2  )
-;
+leave = *0 ;
 leave_output = 2 [ 1 1 ] ;
 leave_error = 3 [ 0.314935988620199336 0 0.314935988620199336 ] ;
 split_col = 3 ;
@@ -2600,7 +1510,7 @@
 split_feature_value = 0.16348885181551448 ;
 after_split_error = 0.249176005273566148 ;
 missing_node = *0 ;
-missing_leave = *202 ->RegressionTreeLeave(
+missing_leave = *100 ->RegressionTreeLeave(
 id = 26 ;
 missing_leave = 0 ;
 loss_function_weight = 1 ;
@@ -2613,49 +1523,24 @@
 loss_function_factor = 2  )
 ;
 left_node = *0 ;
-left_leave = *203 ->RegressionTreeLeave(
-id = 27 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 1 ;
-weights_sum = 0.00675675675675677101 ;
-targets_sum = 1 ;
-weighted_targets_sum = 0.00675675675675677101 ;
-weighted_squared_targets_sum = 0.00675675675675677101 ;
-loss_function_factor = 2  )
-;
+left_leave = *0 ;
 right_node = *0 ;
-right_leave = *204 ->RegressionTreeLeave(
-id = 28 ;
-missing_leave = 0 ;
-loss_function_weight = 1 ;
-verbosity = 2 ;
-length = 94 ;
-weights_sum = 0.635135135135135198 ;
-targets_sum = 53 ;
-weighted_targets_sum = 0.35810810810810767 ;
-weighted_squared_targets_sum = 0.35810810810810767 ;
-loss_function_factor = 2  )
- )
+right_leave = *0  )
 ;
-right_leave = *201   )
+right_leave = *0  )
 ;
-right_leave = *193   )
+right_leave = *0  )
 ;
-right_leave = *185   )
+right_leave = *0  )
 ;
-right_leave = *177   )
+right_leave = *0  )
 ;
-priority_queue = *205 ->RegressionTreeQueue(
-verbosity = 2 ;
-maximum_number_of_nodes = 5 ;
-next_available_node = 5 ;
-nodes = 5 [ *200  *171  *187  *179  *195  ]  )
+priority_queue = *0 ;
+first_leave = *0 ;
+split_cols = []
 ;
-first_leave = *169  ;
-split_cols = 4 [ 2 2 2 3 ] ;
-split_values = 4 [ 0.00132212183813046336 0.0173550052261667587 0.0689879291310910303 0.141930657011306749 ] ;
+split_values = []
+;
 random_gen = *0 ;
 seed = 1827 ;
 stage = 5 ;
@@ -2672,12 +1557,12 @@
 test_minibatch_size = 1 ;
 use_a_separate_random_generator_for_testing = 1827  )
 ] ;
-voting_weights = 5 [ 1.13679877806039675 1.1515814803392157 0.711694555952663288 0.924853007982333275 0.41501925503263265 ] ;
-sum_voting_weights = 4.33994707736724195 ;
-initial_sum_weights = 1 ;
-example_weights = 150 [ 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00506327259806789703 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0137564885108083946 0.000521219238036400986 0.00496216976946179701 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.116188573346790697 0.0331580145383834832 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0760448245655967192 0.00119536787918377106 0.000521219238036400986 0.000521219238036400!
 986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.00119536787918377106 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.00506327259806789703 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00119536787918377106 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.00496216976946179701 0.00216366726200823561 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.00216366726200823561 0.000521219238036400986 0.00119536787918377106 0.000521219238036400986 0.000521219238036400986 0.0321920345819545276 0.0482039349033431888 0.000521219238036400986 0.00216366726200823561 0.03219203458195452!
 76 0.021649179367261618 0.000521219238036400986 0.000521219238!
 03640098
6 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00216366726200823561 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.0210184819737942973 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00331388591284825871 0.000521219238036400986 0.000521219238036400986 0.0482039349033431888 0.00331388591284825871 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00216366726200823561 0.00506327259806789703 0.0210184819737942973 0.0331580145383834832 0.0321920345819545276 0.!
 0496503807568692487 0.000521219238036400986 0.00521520520832954425 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.0331580145383834832 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00521520520832954425 0.000521219238036400986 0.000521219238036400986 0.000521219238036400986 0.00119536787918377106 0.000521219238036400986 ] ;
-learners_error = 5 [ 0.0933333333333333376 0.0908613445378150392 0.194130827514584187 0.135907417796919922 0.303636927486942487 ] ;
-weak_learner_template = *206 ->RegressionTree(
+voting_weights = 5 [ 1.13679877806039786 1.15158148033921659 0.711694555952663621 0.92485300798233383 0.415019255032632539 ] ;
+sum_voting_weights = 4.33994707736724461 ;
+initial_sum_weights = 1.00000000000000244 ;
+example_weights = 150 [ 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.013756488510808379 0.000521219238036399468 0.00496216976946178487 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.116188573346790752 0.0331580145383834901 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0760448245655967053 0.00119536787918376715 0.000521219238036399468 0.00052121923803639946!
 8 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.00119536787918376715 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.00506327259806789182 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.00496216976946178487 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.00216366726200823171 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 0.000521219238036399468 0.0321920345819545345 0.048203934903343168 0.000521219238036399468 0.00216366726200823171 0.0321920345819545345 !
 0.021649179367261618 0.000521219238036399468 0.000521219238036!
 399468 0
.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.0210184819737942973 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00331388591284825307 0.000521219238036399468 0.000521219238036399468 0.048203934903343168 0.00331388591284825307 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00216366726200823171 0.00506327259806789182 0.0210184819737942973 0.0331580145383834901 0.0321920345819545345 0.049650!
 380756869221 0.000521219238036399468 0.00521520520832954079 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.0331580145383834901 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00521520520832954079 0.000521219238036399468 0.000521219238036399468 0.000521219238036399468 0.00119536787918376715 0.000521219238036399468 ] ;
+learners_error = 5 [ 0.0933333333333331433 0.0908613445378149004 0.194130827514584103 0.135907417796919783 0.303636927486942543 ] ;
+weak_learner_template = *101 ->RegressionTree(
 missing_is_valid = 0 ;
 loss_function_weight = 1 ;
 maximum_number_of_nodes = 5 ;
@@ -2685,7 +1570,7 @@
 complexity_penalty_factor = 0 ;
 output_confidence_target = 0 ;
 multiclass_outputs = 2 [ 0 1 ] ;
-leave_template = *207 ->RegressionTreeLeave(
+leave_template = *102 ->RegressionTreeLeave(
 id = -1 ;
 missing_leave = 0 ;
 loss_function_weight = 0 ;
@@ -2710,7 +1595,7 @@
 n_examples = 200 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 5 ;
 report_progress = 0 ;
@@ -2730,7 +1615,7 @@
 save_often = 1 ;
 compute_training_error = 1 ;
 forward_sub_learner_test_costs = 1 ;
-modif_train_set_weights = 0 ;
+modif_train_set_weights = 1 ;
 found_zero_error_weak_learner = 0 ;
 random_gen = *0 ;
 seed = 1827 ;
@@ -2738,7 +1623,7 @@
 n_examples = 150 ;
 inputsize = 5 ;
 targetsize = 1 ;
-weightsize = 0 ;
+weightsize = 1 ;
 forget_when_training_set_changes = 0 ;
 nstages = 5 ;
 report_progress = 0 ;
@@ -2774,10 +1659,10 @@
 ;
 option_fields = 1 [ "nstages" ] ;
 dont_restart_upon_change = 1 [ "nstages" ] ;
-strategy = 1 [ *208 ->HyperOptimize(
+strategy = 1 [ *103 ->HyperOptimize(
 which_cost = "E[test2.E[class_error]]" ;
 min_n_trials = 0 ;
-oracle = *209 ->EarlyStoppingOracle(
+oracle = *104 ->EarlyStoppingOracle(
 option = "nstages" ;
 values = []
 ;
@@ -2806,7 +1691,7 @@
 auto_save_diff_time = 10800 ;
 auto_save_test = 0 ;
 best_objective = 0.140000000000000013 ;
-best_results = 6 [ 0.0533333333333333368 0.166752838955117694 0.0533333333333333368 0.140000000000000013 3.62727311623549786 0.140000000000000013 ] ;
+best_results = 6 [ 0.0533333333333333368 0.16675283895511761 0.0533333333333333368 0.140000000000000013 3.62727311623550586 0.140000000000000013 ] ;
 best_learner = *5  ;
 trialnum = 5 ;
 option_vals = []

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_stats.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test1_stats.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 23.0573431209043029 ;
-sumsquare_ = 27.5380176375199106 ;
-sumcube_ = 46.5250966576119964 ;
-sumfourth_ = 99.2739794243308324 ;
-min_ = 0.0130372181490889943 ;
-max_ = 2.90621616895834611 ;
+sum_ = 23.0573431209042994 ;
+sumsquare_ = 27.5380176375199071 ;
+sumcube_ = 46.5250966576120177 ;
+sumfourth_ = 99.2739794243309746 ;
+min_ = 0.0130372181490889579 ;
+max_ = 2.90621616895834789 ;
 agmemin_ = 149 ;
 agemax_ = 131 ;
-first_ = 0.0130372181490889943 ;
-last_ = 0.0130372181490889943 ;
+first_ = 0.0130372181490889579 ;
+last_ = 0.0130372181490889579 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -81,12 +81,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00969923368429130919 ;
-max_ = 0.00969923368429130919 ;
+min_ = 0.00969923368429130746 ;
+max_ = 0.00969923368429130746 ;
 agmemin_ = 149 ;
 agemax_ = 149 ;
-first_ = 0.00969923368429130919 ;
-last_ = 0.00969923368429130919 ;
+first_ = 0.00969923368429130746 ;
+last_ = 0.00969923368429130746 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00374842516165699345 ;
-max_ = 0.00374842516165699345 ;
+min_ = 0.00374842516165699042 ;
+max_ = 0.00374842516165699042 ;
 agmemin_ = 149 ;
 agemax_ = 149 ;
-first_ = 0.00374842516165699345 ;
-last_ = 0.00374842516165699345 ;
+first_ = 0.00374842516165699042 ;
+last_ = 0.00374842516165699042 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -119,12 +119,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 67.8772502377077274 ;
-sumsquare_ = 118.14393129077213 ;
-sumcube_ = 231.14082196770616 ;
-sumfourth_ = 482.354506226966521 ;
+sum_ = 67.8772502377077842 ;
+sumsquare_ = 118.14393129077223 ;
+sumcube_ = 231.140821967706728 ;
+sumfourth_ = 482.354506226967544 ;
 min_ = 0 ;
-max_ = 2.70339951343224527 ;
+max_ = 2.70339951343224705 ;
 agmemin_ = 149 ;
 agemax_ = 131 ;
 first_ = 0 ;
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+min_ = 4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 149 ;
 agemax_ = 149 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -161,16 +161,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -135.754500475415483 ;
-sumsquare_ = 472.575725163088521 ;
-sumcube_ = -1849.12657574165019 ;
-sumfourth_ = 7717.67209963146615 ;
-min_ = -1.0668519494972486 ;
-max_ = 4.33994707736724195 ;
+sum_ = -135.754500475415597 ;
+sumsquare_ = 472.575725163088919 ;
+sumcube_ = -1849.12657574165473 ;
+sumfourth_ = 7717.67209963148525 ;
+min_ = -1.06685194949724993 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 131 ;
 agemax_ = 149 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -182,16 +182,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -135.754500475415483 ;
-sumsquare_ = 472.575725163088521 ;
-sumcube_ = -1849.12657574165019 ;
-sumfourth_ = 7717.67209963146615 ;
-min_ = -1.0668519494972486 ;
-max_ = 4.33994707736724195 ;
+sum_ = -135.754500475415597 ;
+sumsquare_ = 472.575725163088919 ;
+sumcube_ = -1849.12657574165473 ;
+sumfourth_ = 7717.67209963148525 ;
+min_ = -1.06685194949724993 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 131 ;
 agemax_ = 149 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -203,12 +203,12 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 70.9371869135480608 ;
-sumsquare_ = 129.839350690236188 ;
-sumcube_ = 269.727712341757183 ;
-sumfourth_ = 608.877705658564992 ;
+sum_ = 70.9371869135481177 ;
+sumsquare_ = 129.839350690236301 ;
+sumcube_ = 269.727712341757695 ;
+sumfourth_ = 608.877705658566242 ;
 min_ = 0 ;
-max_ = 3.41509406938490878 ;
+max_ = 3.41509406938491056 ;
 agmemin_ = 149 ;
 agemax_ = 131 ;
 first_ = 0 ;
@@ -224,16 +224,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -37.5508790328347715 ;
-sumsquare_ = 65.927018120689894 ;
-sumcube_ = -36.38421051855412 ;
-sumfourth_ = 50.6411951502830249 ;
+sum_ = -37.550879032834878 ;
+sumsquare_ = 65.9270181206901213 ;
+sumcube_ = -36.3842105185541556 ;
+sumfourth_ = 50.6411951502831243 ;
 min_ = 0 ;
-max_ = 1.63654756393499667 ;
+max_ = 1.63654756393499756 ;
 agmemin_ = 148 ;
 agemax_ = 142 ;
-first_ = 0.924853007982333275 ;
-last_ = 1.63654756393499667 ;
+first_ = 0.92485300798233383 ;
+last_ = 1.63654756393499756 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -245,16 +245,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 58.1471012748402032 ;
-sumsquare_ = 74.2876873973146559 ;
-sumcube_ = 59.9340515275839181 ;
-sumfourth_ = 94.1517014718979652 ;
-min_ = 2.07643448832154887 ;
-max_ = 5.28966775470349493 ;
+sum_ = 58.1471012748402103 ;
+sumsquare_ = 74.2876873973148406 ;
+sumcube_ = 59.9340515275841312 ;
+sumfourth_ = 94.1517014718984342 ;
+min_ = 2.0764344883215502 ;
+max_ = 5.28966775470349937 ;
 agmemin_ = 84 ;
 agemax_ = 123 ;
-first_ = 3.22801596866076457 ;
-last_ = 3.93971052461342763 ;
+first_ = 3.22801596866076679 ;
+last_ = 3.93971052461343074 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -266,16 +266,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -7.03312503214024254 ;
-sumsquare_ = 133.683529618793727 ;
-sumcube_ = 63.6153416947102102 ;
-sumfourth_ = 303.882312131112542 ;
-min_ = 3.09355109911095827 ;
-max_ = 6.8053204694158973 ;
+sum_ = -7.03312503214035889 ;
+sumsquare_ = 133.683529618793898 ;
+sumcube_ = 63.6153416947098478 ;
+sumfourth_ = 303.882312131113338 ;
+min_ = 3.09355109911095916 ;
+max_ = 6.80532046941589996 ;
 agmemin_ = 148 ;
 agemax_ = 135 ;
-first_ = 4.1267886253375714 ;
-last_ = 5.2635874033979686 ;
+first_ = 4.12678862533757407 ;
+last_ = 5.26358740339797215 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -287,16 +287,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = -18.1733302402189523 ;
-sumsquare_ = 117.707682245042491 ;
-sumcube_ = -130.992227419560692 ;
-sumfourth_ = 307.816160654515727 ;
-min_ = 1.75489151804759858 ;
-max_ = 5.89176511046027152 ;
+sum_ = -18.1733302402190127 ;
+sumsquare_ = 117.707682245042605 ;
+sumcube_ = -130.992227419561146 ;
+sumfourth_ = 307.816160654516693 ;
+min_ = 1.7548915180475988 ;
+max_ = 5.89176511046027418 ;
 agmemin_ = 130 ;
 agemax_ = 25 ;
-first_ = 4.11321860501035896 ;
-last_ = 2.26472527099729914 ;
+first_ = 4.11321860501036163 ;
+last_ = 2.26472527099730003 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -308,16 +308,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 47.8651730026865181 ;
-sumsquare_ = 283.462125116786581 ;
-sumcube_ = 888.350651536423925 ;
-sumfourth_ = 3628.99034759896904 ;
-min_ = 0.711694555952663288 ;
-max_ = 6.85505580812373605 ;
+sum_ = 47.8651730026865323 ;
+sumsquare_ = 283.462125116786979 ;
+sumcube_ = 888.350651536426994 ;
+sumfourth_ = 3628.99034759898177 ;
+min_ = 0.711694555952663621 ;
+max_ = 6.85505580812374138 ;
 agmemin_ = 123 ;
 agemax_ = 66 ;
-first_ = 1.84849333401306004 ;
-last_ = 2.98529211207345657 ;
+first_ = 1.8484933340130616 ;
+last_ = 2.98529211207345924 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_stats.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/test2_stats.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -213.43366710400943 ;
-sumsquare_ = 8732.68373561494445 ;
-sumcube_ = 338561.042816411005 ;
-sumfourth_ = 23429594.2572011389 ;
-min_ = 0.0130372181490889943 ;
-max_ = 76.7034798807809608 ;
+sum_ = -213.433667104009203 ;
+sumsquare_ = 8732.68373561498265 ;
+sumcube_ = 338561.042816414207 ;
+sumfourth_ = 23429594.2572014108 ;
+min_ = 0.0130372181490889579 ;
+max_ = 76.7034798807811597 ;
 agmemin_ = 47 ;
 agemax_ = 38 ;
-first_ = 7.8959464583156862 ;
-last_ = 0.0130372181490889943 ;
+first_ = 7.89594645831568975 ;
+last_ = 0.0130372181490889579 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -81,12 +81,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00969923368429130919 ;
-max_ = 0.00969923368429130919 ;
+min_ = 0.00969923368429130746 ;
+max_ = 0.00969923368429130746 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 0.00969923368429130919 ;
-last_ = 0.00969923368429130919 ;
+first_ = 0.00969923368429130746 ;
+last_ = 0.00969923368429130746 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.00374842516165699345 ;
-max_ = 0.00374842516165699345 ;
+min_ = 0.00374842516165699042 ;
+max_ = 0.00374842516165699042 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 0.00374842516165699345 ;
-last_ = 0.00374842516165699345 ;
+first_ = 0.00374842516165699042 ;
+last_ = 0.00374842516165699042 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -119,15 +119,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -118.974105250205838 ;
-sumsquare_ = 361.718741373110618 ;
-sumcube_ = -1088.54433267569675 ;
-sumfourth_ = 3368.94905180080559 ;
+sum_ = -118.974105250205852 ;
+sumsquare_ = 361.718741373110731 ;
+sumcube_ = -1088.54433267569812 ;
+sumfourth_ = 3368.94905180081196 ;
 min_ = 0 ;
-max_ = 4.33994707736724195 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 47 ;
 agemax_ = 38 ;
-first_ = 3.20314829930684519 ;
+first_ = 3.20314829930684652 ;
 last_ = 0 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+min_ = 4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 49 ;
 agemax_ = 49 ;
-first_ = 4.33994707736724195 ;
-last_ = 4.33994707736724195 ;
+first_ = 4.33994707736724461 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -161,16 +161,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 237.948210500411676 ;
-sumsquare_ = 1446.87496549244247 ;
-sumcube_ = 8708.35466140557401 ;
-sumfourth_ = 53903.1848288128895 ;
-min_ = -4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+sum_ = 237.948210500411705 ;
+sumsquare_ = 1446.87496549244293 ;
+sumcube_ = 8708.35466140558492 ;
+sumfourth_ = 53903.1848288129913 ;
+min_ = -4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 38 ;
 agemax_ = 47 ;
 first_ = -2.06634952124644844 ;
-last_ = 4.33994707736724195 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -182,16 +182,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 237.948210500411676 ;
-sumsquare_ = 1446.87496549244247 ;
-sumcube_ = 8708.35466140557401 ;
-sumfourth_ = 53903.1848288128895 ;
-min_ = -4.33994707736724195 ;
-max_ = 4.33994707736724195 ;
+sum_ = 237.948210500411705 ;
+sumsquare_ = 1446.87496549244293 ;
+sumcube_ = 8708.35466140558492 ;
+sumfourth_ = 53903.1848288129913 ;
+min_ = -4.33994707736724461 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 38 ;
 agemax_ = 47 ;
 first_ = -2.06634952124644844 ;
-last_ = 4.33994707736724195 ;
+last_ = 4.33994707736724461 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -203,15 +203,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -118.974105250205838 ;
-sumsquare_ = 361.718741373110618 ;
-sumcube_ = -1088.54433267569675 ;
-sumfourth_ = 3368.94905180080559 ;
+sum_ = -118.974105250205852 ;
+sumsquare_ = 361.718741373110731 ;
+sumcube_ = -1088.54433267569812 ;
+sumfourth_ = 3368.94905180081196 ;
 min_ = 0 ;
-max_ = 4.33994707736724195 ;
+max_ = 4.33994707736724461 ;
 agmemin_ = 47 ;
 agemax_ = 38 ;
-first_ = 3.20314829930684519 ;
+first_ = 3.20314829930684652 ;
 last_ = 0 ;
 binary_ = 0 ;
 integer_ = 0 ;
@@ -224,16 +224,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -40.346432942149562 ;
-sumsquare_ = 52.2063866773546721 ;
-sumcube_ = -75.0396438545297286 ;
-sumfourth_ = 114.886284738728506 ;
+sum_ = -40.3464329421495762 ;
+sumsquare_ = 52.2063866773547218 ;
+sumcube_ = -75.0396438545297855 ;
+sumfourth_ = 114.886284738728719 ;
 min_ = 0 ;
-max_ = 1.63654756393499667 ;
+max_ = 1.63654756393499756 ;
 agmemin_ = 47 ;
 agemax_ = 49 ;
-first_ = 1.63654756393499667 ;
-last_ = 0.924853007982333275 ;
+first_ = 1.63654756393499756 ;
+last_ = 0.92485300798233383 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -245,16 +245,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -16.1112854278747726 ;
-sumsquare_ = 27.3178799413751463 ;
-sumcube_ = -26.7915520541299621 ;
-sumfourth_ = 49.4897368630278365 ;
-min_ = 2.07643448832154887 ;
-max_ = 5.28966775470349493 ;
+sum_ = -16.1112854278748081 ;
+sumsquare_ = 27.3178799413752031 ;
+sumcube_ = -26.7915520541300438 ;
+sumfourth_ = 49.4897368630280354 ;
+min_ = 2.0764344883215502 ;
+max_ = 5.28966775470349937 ;
 agmemin_ = 33 ;
 agemax_ = 25 ;
-first_ = 3.93971052461342763 ;
-last_ = 4.15286897664309773 ;
+first_ = 3.93971052461343074 ;
+last_ = 4.1528689766431004 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -266,16 +266,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 5.85771568121859509 ;
-sumsquare_ = 49.7800097040063818 ;
-sumcube_ = 62.2050339635074678 ;
-sumfourth_ = 197.71117838999271 ;
-min_ = 3.09355109911095827 ;
-max_ = 6.8053204694158973 ;
+sum_ = 5.85771568121856223 ;
+sumsquare_ = 49.7800097040064387 ;
+sumcube_ = 62.2050339635074039 ;
+sumfourth_ = 197.711178389992853 ;
+min_ = 3.09355109911095916 ;
+max_ = 6.80532046941589996 ;
 agmemin_ = 47 ;
 agemax_ = 39 ;
-first_ = 4.1267886253375714 ;
-last_ = 4.1267886253375714 ;
+first_ = 4.12678862533757407 ;
+last_ = 4.12678862533757407 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -287,16 +287,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 76.2390102335463098 ;
-sumsquare_ = 160.742172097361419 ;
-sumcube_ = 356.830904308486083 ;
-sumfourth_ = 834.777632970863806 ;
-min_ = 1.84970601596466655 ;
-max_ = 5.47674585542763825 ;
+sum_ = 76.2390102335463951 ;
+sumsquare_ = 160.742172097361561 ;
+sumcube_ = 356.830904308487163 ;
+sumfourth_ = 834.777632970866875 ;
+min_ = 1.84970601596466766 ;
+max_ = 5.47674585542764181 ;
 agmemin_ = 30 ;
 agemax_ = 8 ;
-first_ = 2.26472527099729914 ;
-last_ = 3.18836559702802624 ;
+first_ = 2.26472527099730003 ;
+last_ = 3.18836559702802802 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -308,16 +308,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -87.9536558510736057 ;
-sumsquare_ = 275.74828119293727 ;
-sumcube_ = -590.104054700513871 ;
-sumfourth_ = 1937.66870355591777 ;
-min_ = 0.711694555952663288 ;
-max_ = 6.85505580812373605 ;
+sum_ = -87.9536558510736768 ;
+sumsquare_ = 275.748281192937782 ;
+sumcube_ = -590.104054700516031 ;
+sumfourth_ = 1937.66870355592437 ;
+min_ = 0.711694555952663621 ;
+max_ = 6.85505580812374138 ;
 agmemin_ = 25 ;
 agemax_ = 20 ;
-first_ = 4.12209089013385377 ;
-last_ = 1.84849333401306004 ;
+first_ = 4.12209089013385732 ;
+last_ = 1.8484933340130616 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/train_stats.psave	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/Split0/train_stats.psave	2008-12-10 14:58:55 UTC (rev 9768)
@@ -39,12 +39,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.166752838955117694 ;
-max_ = 0.166752838955117694 ;
+min_ = 0.16675283895511761 ;
+max_ = 0.16675283895511761 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.166752838955117694 ;
-last_ = 0.166752838955117694 ;
+first_ = 0.16675283895511761 ;
+last_ = 0.16675283895511761 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 3.62727311623549786 ;
-max_ = 3.62727311623549786 ;
+min_ = 3.62727311623550586 ;
+max_ = 3.62727311623550586 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 3.62727311623549786 ;
-last_ = 3.62727311623549786 ;
+first_ = 3.62727311623550586 ;
+last_ = 3.62727311623550586 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2008-12-09 22:01:49 UTC (rev 9767)
+++ trunk/plearn_learners/meta/test/AdaBoost/.pytest/PL_AdaBoost_pseudo_loss_adaboost/expected_results/expdir/metainfos.txt	2008-12-10 14:58:55 UTC (rev 9768)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL9611"
+__REVISION__ = "PL9752"
 conf                                          = False
 pseudo                                        = True



From plearner at mail.berlios.de  Wed Dec 10 18:08:55 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 10 Dec 2008 18:08:55 +0100
Subject: [Plearn-commits] r9769 - trunk/plearn_learners/generic
Message-ID: <200812101708.mBAH8t2f000658@sheep.berlios.de>

Author: plearner
Date: 2008-12-10 18:08:54 +0100 (Wed, 10 Dec 2008)
New Revision: 9769

Modified:
   trunk/plearn_learners/generic/ChainedLearners.cc
Log:
Fixed computeCostsForm Outputs of a ChainedLearner so that it uses the last learner's previously computed outputs


Modified: trunk/plearn_learners/generic/ChainedLearners.cc
===================================================================
--- trunk/plearn_learners/generic/ChainedLearners.cc	2008-12-10 14:58:55 UTC (rev 9768)
+++ trunk/plearn_learners/generic/ChainedLearners.cc	2008-12-10 17:08:54 UTC (rev 9769)
@@ -197,13 +197,16 @@
             learners[k]->computeOutput(tmp_input, tmp_output);
         }
         learners[nlearners-1]->computeOutput(tmp_output, output);
+        
     }
 }
 
 void ChainedLearners::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
-    return learners.lastElement()->computeCostsFromOutputs(input, output, target, costs);
+    // this is generally called after a computeOutput, so the last learner's input 
+    // we used was stored in tmp_output
+    return learners.lastElement()->computeCostsFromOutputs(tmp_output, output, target, costs);
 }
 
 TVec<string> ChainedLearners::getTestCostNames() const



From plearner at mail.berlios.de  Wed Dec 10 18:12:11 2008
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Wed, 10 Dec 2008 18:12:11 +0100
Subject: [Plearn-commits] r9770 - in trunk: commands/PLearnCommands
	python_modules/plearn/pyplearn
	python_modules/plearn/utilities scripts
Message-ID: <200812101712.mBAHCB7O001184@sheep.berlios.de>

Author: plearner
Date: 2008-12-10 18:12:10 +0100 (Wed, 10 Dec 2008)
New Revision: 9770

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
   trunk/commands/PLearnCommands/HelpCommand.h
   trunk/python_modules/plearn/pyplearn/plargs.py
   trunk/python_modules/plearn/utilities/autoscript.py
   trunk/scripts/pyplearn_driver.py
Log:
plearn help can now be invoked on a .pyplearn script and should output useful help
Also added method optionstring to help generate expdir with parsable explicit arguments.


Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2008-12-10 17:08:54 UTC (rev 9769)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2008-12-10 17:12:10 UTC (rev 9770)
@@ -39,12 +39,12 @@
 /*! \file HelpCommand.cc */
 #include "HelpCommand.h"
 
-#include <iostream>
+#include <plearn/io/PPath.h>
+#include <plearn/sys/Popen.h>
 #include <plearn/db/getDataSet.h>
 #include <plearn/base/general.h>        //!< For prgname().
 #include <plearn/base/stringutils.h>
 #include <plearn/io/fileutils.h>        //!< For isfile().
-
 #include <plearn/base/HelpSystem.h>
 
 namespace PLearn {
@@ -138,15 +138,36 @@
     pout << getDataSetHelp() << endl;
 }
 
-void HelpCommand::helpAboutScript(const string& fname)
+void HelpCommand::helpAboutPLearnScript(const string& fname)
 {
     if(!isfile(fname))
         PLERROR("Could not open script file %s", fname.c_str());
     pout << 
-        "Help about a script file not yet implemented \n"
+        "Help about a .plearn script file not yet implemented \n"
          << endl;
 }
 
+void HelpCommand::helpAboutPyPLearnScript(const string& fname)
+{
+    if(!isfile(fname))
+        PLERROR("Could not open script file %s", fname.c_str());
+    
+    pout << "#######################################################" << endl;
+    pout << "### Help on pyplearn script " << fname << endl;
+    pout << "#######################################################" << endl;
+    string command;
+#ifdef WIN32
+    command = "python.exe "+PPath("PLEARNDIR:scripts/pyplearn_driver.py").absolute();
+#else
+    command = "pyplearn_driver.py";
+#endif
+    vector<string> helptext = execute(command+" "+fname+" --help");
+    vector<string>::const_iterator it = helptext.begin();
+    while(it!=helptext.end())
+        pout << *it++ << endl;
+}
+
+
 //! The actual implementation of the 'HelpCommand' command 
 void HelpCommand::run(const vector<string>& args)
 {
@@ -155,15 +176,20 @@
     else
     {
         string about = args[0];
+        string aboutext = extract_extension(about);
         
         if(args.size() > 1)//is option level present?
             OptionBase::setCurrentOptionLevel(
                 OptionBase::optionLevelFromString(args[1]));
 
-        if(extract_extension(about)==".plearn") // help is asked about a plearn script
-            helpAboutScript(about);//TODO: move to HelpSystem
-        if(about=="scripts")
-            helpScripts();//TODO: move to HelpSystem
+        // Note: some of these functions could be moved to HelpSystem
+
+        if(aboutext==".plearn") // help is asked about a plearn script
+            helpAboutPLearnScript(about);
+        else if(aboutext==".pyplearn") // help is asked about a pyplearn script
+            helpAboutPyPLearnScript(about);
+        else if(about=="scripts")
+            helpScripts();
         else if(about=="commands")
             helpCommands();
         else if(about=="datasets")

Modified: trunk/commands/PLearnCommands/HelpCommand.h
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.h	2008-12-10 17:08:54 UTC (rev 9769)
+++ trunk/commands/PLearnCommands/HelpCommand.h	2008-12-10 17:12:10 UTC (rev 9770)
@@ -51,7 +51,8 @@
 protected:
 
     void helpOverview();
-    void helpAboutScript(const string& fname);
+    void helpAboutPLearnScript(const string& fname);
+    void helpAboutPyPLearnScript(const string& fname);
     void helpScripts();
     void helpCommands();
     void helpDatasets();

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2008-12-10 17:08:54 UTC (rev 9769)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2008-12-10 17:12:10 UTC (rev 9770)
@@ -54,7 +54,7 @@
                                       internal = SomeObject( dataset = dataset ) )
 
 Those command-line arguments are widely refered as L{plargs}, on
-account of this class, troughout the pyplearn mechanism. Note that
+account of this class, throughout the pyplearn mechanism. Note that
 I{unexpected} (see binders and L{namespaces<plnamespace>} hereafter)
 arguments given on the command line are interpreted as strings, so if
 you want to pass integers (int) or floating-point values (float), you
@@ -1035,8 +1035,10 @@
         for opt in plopt.iterator(namespace):
             print >>out, namespace.__name__+'.'+opt.getName()+'='+str(opt.get())
 
+def optionsString(holder, separator="__"):
+    return separator.join( [ opt.getName()+'='+str(opt.get()) for opt in plopt.iterator(holder) ] )
+    
 
-
 #######  For backward compatibily: will be deprecated soon  ###################
 
 class plargs_binder(plargs):
@@ -1081,6 +1083,14 @@
             s+= opt.getName()+': '+opt.getDoc()+'\n\t'+str(opt)+'\n'
     return s
 
+def bindersHelp2():
+    s= ''
+    for binder in plargs.getBinders():
+        for opt in plopt.iterator(binder):
+            s+= opt.getName()+'='+repr(opt.get())+'  : '+opt.getDoc()+'\n'
+    return s
+
+
 def currentNamespacesHelp():
     s= ''
     for namespace in plargs.getNamespaces():

Modified: trunk/python_modules/plearn/utilities/autoscript.py
===================================================================
--- trunk/python_modules/plearn/utilities/autoscript.py	2008-12-10 17:08:54 UTC (rev 9769)
+++ trunk/python_modules/plearn/utilities/autoscript.py	2008-12-10 17:12:10 UTC (rev 9770)
@@ -160,6 +160,16 @@
           "  ".join(quote_if_needed(name+'='+mystr(value)) for name,value in zip(all_argnames[defvalpos:],default_values))
     return txt
 
+def autoexpdir(prefix, locals_dict, separator="__", exclude=["self"]):
+    """Returns a suitable expdir string built from the passed dictionary of arguments (typically locals()).
+    This can typically be called as the first instruction in a function meant to be called by autoscript.
+    Typical example:
+      expdir = autoexpdir("exp/mymodel1",locals())
+
+      This will result in expdir of the form "exp/mymodel1__argname=value__argname=value__argname=value"
+      """
+    return prefix+separator+separator.join([ k+"="+str(v) for k,v in locals_dict.items() if k not in exclude ])
+
 def autoscript(callme, autocomplete_names=False, helptext="", argv=sys.argv):
     """Call autoscript as a one-liner to turn a callable (function, class or object) into a command-line script.
     Typical usage goes as follows:
@@ -228,6 +238,7 @@
 
         kwargs = check_args(args, kwargs, all_argnames, default_values)
         eval_str_argument_values(kwargs, all_argnames, default_values)
+
         print
         print "#"*80
         print "# Calling "+scriptname+" with following arguments: "

Modified: trunk/scripts/pyplearn_driver.py
===================================================================
--- trunk/scripts/pyplearn_driver.py	2008-12-10 17:08:54 UTC (rev 9769)
+++ trunk/scripts/pyplearn_driver.py	2008-12-10 17:12:10 UTC (rev 9770)
@@ -5,6 +5,10 @@
 from plearn.pyplearn.plearn_repr import *
 from plearn.utilities import options_dialog
 
+if len(sys.argv)<=1:
+    print "Wrong Usage: pyplearn_drvier.py must be invoked with a .pyplearn script as argument"
+    sys.exit()
+
 # Add the absolute directory portion of the current script to the path
 sys.path = [os.path.dirname(os.path.abspath(sys.argv[1]))] + sys.path
 
@@ -24,7 +28,12 @@
 
 if len(sys.argv) == 3 and sys.argv[2] == '--help':
     # Simply print the docstring of the pyplearn script
-    lines += 'print __doc__\n'
+    lines += 'if __doc__ is not None: print __doc__\n'
+    lines += 'print "### Possible command-line options with their default value: ###"\n'
+    lines += 'print bindersHelp2()\n'
+    lines += 'print\n'
+    lines += 'print currentNamespacesHelp()\n'
+    lines += 'print "Note: If you want to see the preprocessed output of this script in serialized .plearn format, you can invoke it with pyplearn_driver.py" \n'
 elif len(sys.argv) >= 3 and '--PyPLearnScript' in sys.argv:
     # This is the mecanism used inside the run command to manage logs to
     # expdirs.



From laulysta at mail.berlios.de  Wed Dec 10 19:32:18 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 10 Dec 2008 19:32:18 +0100
Subject: [Plearn-commits] r9771 - trunk/plearn_learners_experimental
Message-ID: <200812101832.mBAIWI0H008530@sheep.berlios.de>

Author: laulysta
Date: 2008-12-10 19:32:13 +0100 (Wed, 10 Dec 2008)
New Revision: 9771

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
hidden reconstruction


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-10 17:12:10 UTC (rev 9770)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-10 18:32:13 UTC (rev 9771)
@@ -672,19 +672,26 @@
                 if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
                 
-                // greedy phase
-                if(input_reconstruction_lr!=0 || hidden_reconstruction_lr!=0){
+                // greedy phase input
+                if(input_reconstruction_lr!=0){
                     setLearningRate( input_reconstruction_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, 0);
+                    recurrentUpdate(input_reconstruction_cost_weight, 0, 0);
                 }
+                
+                // greedy phase hidden
+                if(hidden_reconstruction_lr!=0){
+                    setLearningRate( hidden_reconstruction_lr );
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0);
+                }
 
                 // recurrent noisy phase
                 if(noisy_recurrent_lr!=0)
                 {
                     setLearningRate( noisy_recurrent_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, 1);
+                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1);
                 }
 
                 // recurrent no noise phase
@@ -694,7 +701,7 @@
                         encoded_seq << clean_encoded_seq;                  
                     setLearningRate( recurrent_lr );                    
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0,1);
+                    recurrentUpdate(0,0,1);
                 }
             }
 
@@ -970,8 +977,6 @@
 }
 
 
-
-
 Mat DenoisingRecurrentNet::getInputConnectionsWeightMatrix()
 {
     RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)input_connections);
@@ -980,6 +985,14 @@
     return conn->weights;
 }
 
+Mat DenoisingRecurrentNet::getDynamicConnectionsWeightMatrix()
+{
+    RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)dynamic_connections);
+    if(conn==0)
+        PLERROR("Expecting input connection to be a RBMMatrixConnection. Je sais c'est sale, mais au point ou on est rendu..");
+    return conn->weights;
+}
+
 double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
                                                                        Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
@@ -992,40 +1005,41 @@
 
 //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
 //! Also computes neg log cost and returns it
-double DenoisingRecurrentNet::fpropInputReconstructionFromHidden(Vec hidden, Mat reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
+double DenoisingRecurrentNet::fpropInputReconstructionFromHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_bias, Vec& reconstruction_prob, 
                                                                  Vec clean_input)
 {
     // set appropriate sizes
     int fullinputlength = clean_input.length();
-    if(input_reconstruction_bias.length()==0)
+    Vec reconstruction_activation;
+    if(reconstruction_bias.length()==0)
     {
-        input_reconstruction_bias.resize(fullinputlength);
-        input_reconstruction_bias.clear();
+        reconstruction_bias.resize(fullinputlength);
+        reconstruction_bias.clear();
     }
-    input_reconstruction_activation.resize(fullinputlength);
-    input_reconstruction_prob.resize(fullinputlength);
+    reconstruction_activation.resize(fullinputlength);
+    reconstruction_prob.resize(fullinputlength);
 
     // predict (denoised) input_reconstruction 
-    transposeProduct(input_reconstruction_activation, reconstruction_weights, hidden); 
-    input_reconstruction_activation += input_reconstruction_bias;
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
+    reconstruction_activation += reconstruction_bias;
 
     double result_cost = 0;
     if(encoding=="raw_masked_supervised") // complicated input format... consider it's squared error
     {
         real r;
-        input_reconstruction_prob << input_reconstruction_activation;
-        for(int i=0; i<input_reconstruction_activation.length(); i++)
-            r += input_reconstruction_activation[i] - clean_input[i];
+        reconstruction_prob << reconstruction_activation;
+        for(int i=0; i<reconstruction_activation.length(); i++)
+            r += reconstruction_activation[i] - clean_input[i];
         result_cost = r*r;
     }
     else // suppose it's a multiple softmax
     {
-        applyMultipleSoftmaxToInputWindow(input_reconstruction_activation, input_reconstruction_prob);
+        applyMultipleSoftmaxToInputWindow(reconstruction_activation, reconstruction_prob);
     
         double neg_log_cost = 0; // neg log softmax
-        for(int k=0; k<input_reconstruction_prob.length(); k++)
+        for(int k=0; k<reconstruction_prob.length(); k++)
             if(clean_input[k]!=0)
-                neg_log_cost -= clean_input[k]*safelog(input_reconstruction_prob[k]);
+                neg_log_cost -= clean_input[k]*safelog(reconstruction_prob[k]);
         result_cost = neg_log_cost;
     }
     return result_cost;
@@ -1056,8 +1070,46 @@
 }
 
 
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 
+                                                                 Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
+{
+    // set appropriate sizes
+    int fullinputlength = clean_input.length();
+    Vec reconstruction_activation;
+    /*if(reconstruction_bias.length()==0)
+    {
+        reconstruction_bias.resize(fullinputlength);
+        reconstruction_bias.clear();
+        }*/
+    reconstruction_activation.resize(fullinputlength);
+    reconstruction_prob.resize(fullinputlength);
 
+    // predict (denoised) input_reconstruction 
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
+    //reconstruction_activation += hidden_layer->bias;
+    
+    hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
 
+    /********************************************************************************/
+    Vec hidden_reconstruction_activation_grad;
+    hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
+    hidden_reconstruction_activation_grad << reconstruction_prob;
+    hidden_reconstruction_activation_grad -= clean_input;
+    hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
+
+    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+    /********************************************************************************/
+
+    double result_cost = 0;
+    double neg_log_cost = 0; // neg log softmax
+    for(int k=0; k<reconstruction_prob.length(); k++)
+        if(clean_input[k]!=0)
+            neg_log_cost -= clean_input[k]*safelog(reconstruction_prob[k]);
+    result_cost = neg_log_cost;
+    
+    return result_cost;
+}
+
 /*
 input_list
 targets_list
@@ -1072,6 +1124,7 @@
 */
 
 void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
+                                            real hidden_reconstruction_weight,
                                             real temporal_gradient_contribution)
 {
     hidden_temporal_gradient.resize(hidden_layer->size);
@@ -1126,11 +1179,24 @@
             Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
 
             fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
-                                                     clean_input, hidden_gradient, input_reconstruction_weight, current_learning_rate);
+                                                     clean_input, hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
         }
 
+
         if(i!=0 && dynamic_connections )
         {   
+
+            // Add contribution of hidden reconstruction cost in hidden_gradient
+            if(hidden_reconstruction_weight!=0)
+            {
+                Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+                //Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
+                
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, 
+                                                        hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+            }
+
+
             // add contribution to gradient of next time step hidden layer
             if(temporal_gradient_contribution>0)
             { // add weighted contribution of hidden_temporal gradient to hidden_gradient

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-10 17:12:10 UTC (rev 9770)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-10 18:32:13 UTC (rev 9771)
@@ -267,6 +267,7 @@
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
     void recurrentUpdate(real input_reconstruction_weight,
+                         real hidden_reconstruction_cost_weight,
                          real temporal_gradient_contribution = 1);
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
@@ -361,10 +362,10 @@
     mutable Mat encoded_seq; // contains encoded version of current train or test sequence (possibly corrupted by noise)
     mutable Mat clean_encoded_seq; // copy of clean sequence contains encoded version of current train or test sequence
 
-    mutable Vec input_reconstruction_activation; // temporary Vec to hold input reconstruction activation (before softmax)
+    //mutable Vec input_reconstruction_activation; // temporary Vec to hold input reconstruction activation (before softmax)
     mutable Vec input_reconstruction_prob;       // temporary Vec to hold input reconstruction prob (after applying softmax)
+    mutable Vec hidden_reconstruction_prob;
 
-
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -401,6 +402,8 @@
 
     Mat getInputConnectionsWeightMatrix();
 
+    Mat getDynamicConnectionsWeightMatrix();
+
     //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
     //! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
@@ -420,8 +423,9 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
+    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 
+                                                                          Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-
 private:
     //#####  Private Data Members  ############################################
 



From saintmlx at mail.berlios.de  Thu Dec 11 00:00:38 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 11 Dec 2008 00:00:38 +0100
Subject: [Plearn-commits] r9772 - trunk/python_modules/plearn/pybridge
Message-ID: <200812102300.mBAN0bi2026050@sheep.berlios.de>

Author: saintmlx
Date: 2008-12-11 00:00:34 +0100 (Thu, 11 Dec 2008)
New Revision: 9772

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- bugfix: unpickle used to alloc. a second PLearn object; could cause double delete of python object



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-12-10 18:32:13 UTC (rev 9771)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-12-10 23:00:34 UTC (rev 9772)
@@ -73,7 +73,7 @@
         if '_cptr' in kwargs:
             self._cptr= kwargs['_cptr'] # ptr to c++ obj
         elif hasattr(self,'_cptr'):
-            self.setOptions(kwargs)
+            self.setOptions(kwargs)        
             
     def setOptions(self, kwargs):
         call_build= True
@@ -104,7 +104,7 @@
             raise AttributeError("no attribute "
                                  + attr + " in "
                                  + repr(self))
-        
+
     def __del__(self):
         if hasattr(self, '_cptr'):
             self._unref()
@@ -140,14 +140,14 @@
         for o in self._optionnames:
             d['_cptr'][2][o]= self.getOption(o)
         return d
-        ##### old, deprecated version follows: (for reference only)
-	def old_deprecated___getstate__(self):
-		d= self.__dict__.copy()
-	        if remote_pickle:
-        	    d['_cptr']= self.asStringRemoteTransmit()
-        	else:
-        	    d['_cptr']= self.asString()
-        	return d
+    ##### old, deprecated version follows: (for reference only)
+    def old_deprecated___getstate__(self):
+        d= self.__dict__.copy()
+        if remote_pickle:
+            d['_cptr']= self.asStringRemoteTransmit()
+        else:
+            d['_cptr']= self.asString()
+        return d
     
     def __setstate__(self, dict):
         """
@@ -171,9 +171,7 @@
         PLEARN_PICKLE_PROTOCOL_VERSION= 2
         if d[0] != PLEARN_PICKLE_PROTOCOL_VERSION:
             raise RuntimeError, "PLearn pickle protocol version should be 2"
-        newone= plearn_module.newObjectFromClassname(d[1])
-        self._cptr= newone._cptr
-        self._refCPPObj(self, False)
+        # empty PLearn object already exists (from __new__)
         for k in dict:
             if k != '_cptr':
                 self.__setattr__(k, dict[k])



From nouiz at mail.berlios.de  Thu Dec 11 20:18:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 11 Dec 2008 20:18:45 +0100
Subject: [Plearn-commits] r9773 - trunk/plearn/vmat
Message-ID: <200812111918.mBBJIjl6018332@sheep.berlios.de>

Author: nouiz
Date: 2008-12-11 20:18:44 +0100 (Thu, 11 Dec 2008)
New Revision: 9773

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
Log:
put static fct variable of int type non static as their is no good purpose of them being static and this make it less theard safe.


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-12-10 23:00:34 UTC (rev 9772)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-12-11 19:18:44 UTC (rev 9773)
@@ -123,7 +123,7 @@
                 "requested index: (%d,%d) but width of %d and length of %d",
                 i,j,width_,length_);
 #endif
-    static int col;
+    int col;
     col = sel_indices[j];
     if (col == -1)
         return MISSING_VALUE;
@@ -141,7 +141,7 @@
                 "requested index: (%d,%d) but width of %d and length of %d",
                 i,j,width_,length_);
 #endif
-    static int col;
+    int col;
     for(int jj=0; jj<v.length(); jj++) {
         col = sel_indices[j+jj];
         if (col == -1)
@@ -417,7 +417,7 @@
 // getStringToRealMapping //
 ////////////////////////////
 const map<string,real>& SelectColumnsVMatrix::getStringToRealMapping(int col) const {
-    static int the_col;
+    int the_col;
     static map<string, real> empty_mapping;
     the_col = sel_indices[col];
     if (the_col == -1)
@@ -429,7 +429,7 @@
 // getStringVal //
 //////////////////
 real SelectColumnsVMatrix::getStringVal(int col, const string & str) const {
-    static int the_col;
+    int the_col;
     the_col = sel_indices[col];
     if (the_col == -1)
         return MISSING_VALUE;
@@ -440,7 +440,7 @@
 // getRealToStringMapping //
 ////////////////////////////
 const map<real,string>& SelectColumnsVMatrix::getRealToStringMapping(int col) const {
-    static int the_col;
+    int the_col;
     static map<real, string> empty_mapping;
     the_col = sel_indices[col];
     if (the_col == -1)
@@ -452,7 +452,7 @@
 // getValString //
 //////////////////
 string SelectColumnsVMatrix::getValString(int col, real val) const {
-    static int the_col;
+    int the_col;
     the_col = sel_indices[col];
     if (the_col == -1)
         return "";
@@ -463,7 +463,7 @@
 {
     if(col>=width_)
         PLERROR("access out of bound. Width=%i accessed col=%i",width_,col);
-    static int the_col;
+    int the_col;
     the_col = sel_indices[col];
     if (the_col == -1)
         return 0;
@@ -475,7 +475,7 @@
 {
     if(col>=width_)
         PLERROR("access out of bound. Width=%i accessed col=%i",width_,col);
-    static int the_col;
+    int the_col;
     the_col = sel_indices[col];
     if (the_col == -1)
         values.resize(0);
@@ -487,7 +487,7 @@
 {
     if(col>=width_)
         PLERROR("access out of bound. Width=%i accessed col=%i",width_,col);
-    static int the_col;
+    int the_col;
     the_col = sel_indices[col];
     if (the_col == -1)
         values.resize(0);



From nouiz at mail.berlios.de  Thu Dec 11 22:06:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 11 Dec 2008 22:06:28 +0100
Subject: [Plearn-commits] r9774 - in trunk/plearn: io math var vmat
Message-ID: <200812112106.mBBL6SAc029409@sheep.berlios.de>

Author: nouiz
Date: 2008-12-11 22:06:26 +0100 (Thu, 11 Dec 2008)
New Revision: 9774

Modified:
   trunk/plearn/io/PPath.cc
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/TMat_maths_impl.h
   trunk/plearn/var/Variable.cc
   trunk/plearn/vmat/ConcatRowsVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
Log:
change some static variable to make them thread safe. Many of those static variable have no purpose being static and the other are not in hot code.


Modified: trunk/plearn/io/PPath.cc
===================================================================
--- trunk/plearn/io/PPath.cc	2008-12-11 19:18:44 UTC (rev 9773)
+++ trunk/plearn/io/PPath.cc	2008-12-11 21:06:26 UTC (rev 9774)
@@ -398,7 +398,7 @@
     const string* the_path = &path_;
 #if defined(__CYGWIN__) || defined(_MINGW_)
 #ifdef __CYGWIN__
-    static char buf[3000];
+    char buf[3000];
 #endif
     string new_path;
     // This is a hack to try to get the right DOS path from Cygwin.

Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2008-12-11 19:18:44 UTC (rev 9773)
+++ trunk/plearn/math/StatsCollector.cc	2008-12-11 21:06:26 UTC (rev 9774)
@@ -1130,6 +1130,10 @@
     static bool init = false;
     static map<string,STATFUN> statistics;
     if (!init) {
+        //the two if(!init) is volontary not to acquire a lock at each fct call
+#pragma omp critical
+        if(!init){
+        init = true;
         statistics["E"]           = STATFUN(&StatsCollector::mean);
         statistics["V"]           = STATFUN(&StatsCollector::variance);
         statistics["STDDEV"]      = STATFUN(&StatsCollector::stddev);
@@ -1161,7 +1165,7 @@
         statistics["MEAN_LIFT"]   = STATFUN(&StatsCollector::mean_lift);
         statistics["PRBP"]        = STATFUN(&StatsCollector::prbp);
         statistics["DMODE"]       = STATFUN(&StatsCollector::dmode);
-        init = true;
+        }
     }
 
     // Special case :: interpret the PSEUDOQ(xx) and LIFT(xxx) forms

Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2008-12-11 19:18:44 UTC (rev 9773)
+++ trunk/plearn/math/TMat_maths_impl.h	2008-12-11 21:06:26 UTC (rev 9774)
@@ -1065,7 +1065,6 @@
 T powdistance(const TVec<T>& vec1, const TVec<T>& vec2, double n,
               bool ignore_missing = false)
 {
-    static T result, diff;
 #ifdef BOUNDCHECK
     if(vec1.length() != vec2.length())
         PLERROR("In weighted_powdistance: vec1, vec2 should have the same length (%d!=%d)",
@@ -1074,7 +1073,8 @@
     int length = vec1.length();
     if (length == 0)
         return 0.0;
-    result = 0.0;
+    T result = 0;
+    T diff = 0;
     T* v1 = vec1.data();
     T* v2 = vec2.data();
     if(fast_exact_is_equal(n, 1.0)) // L1 distance
@@ -4497,9 +4497,9 @@
 template<class T>
 void regularizeMatrix(const TMat<T>& mat, T tolerance)
 {
-    static T reg;
-    static T* k;
-    static int shift;
+    T reg;
+    T* k;
+    int shift;
     reg = tolerance * trace(mat);
     k = mat.data();
     shift = mat.mod() + 1;

Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2008-12-11 19:18:44 UTC (rev 9773)
+++ trunk/plearn/var/Variable.cc	2008-12-11 21:06:26 UTC (rev 9774)
@@ -591,12 +591,10 @@
 
 bool Variable::update(real step_size, Vec direction_vec, real coeff, real b)
 {
-    static bool hit;
-    static real full_coeff;
+    bool hit = false;
     if(allows_partial_update)
         PLWARNING("Warning in Variable::update(real,Vec): will update every elements of the Variable");
-    hit = false;
-    full_coeff = step_size * coeff + b;
+    real full_coeff = step_size * coeff + b;
     if(min_value>-FLT_MAX || max_value<FLT_MAX)
         // constrained update
     {

Modified: trunk/plearn/vmat/ConcatRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-12-11 19:18:44 UTC (rev 9773)
+++ trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-12-11 21:06:26 UTC (rev 9774)
@@ -406,13 +406,12 @@
 /////////
 real ConcatRowsVMatrix::get(int i, int j) const
 {
-    static real val;
     int whichvm, rowofvm;
     getpositions(i,whichvm,rowofvm);
     if (!need_fix_mappings || fixed_mappings(whichvm, j).size() == 0)
         return to_concat[whichvm]->get(rowofvm,j);
     else {
-        val = to_concat[whichvm]->get(rowofvm,j);
+        real val = to_concat[whichvm]->get(rowofvm,j);
         if (!is_missing(val)) {
             map<real, real>::iterator it = fixed_mappings(whichvm, j).find(val);
             if (it != fixed_mappings(whichvm, j).end()) {

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-12-11 19:18:44 UTC (rev 9773)
+++ trunk/plearn/vmat/VMatrix.cc	2008-12-11 21:06:26 UTC (rev 9774)
@@ -1198,9 +1198,8 @@
 ///////////////
 string VMatrix::getString(int row,int col) const
 {
-    static string str;
     real val = get(row,col);
-    str = getValString(col, val);
+    string str = getValString(col, val);
     if (str == "")
         // There is no string mapping associated to this value.
         return tostring(val);



From nouiz at mail.berlios.de  Fri Dec 12 22:06:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 12 Dec 2008 22:06:29 +0100
Subject: [Plearn-commits] r9775 - trunk/plearn/io
Message-ID: <200812122106.mBCL6TiU021971@sheep.berlios.de>

Author: nouiz
Date: 2008-12-12 22:06:28 +0100 (Fri, 12 Dec 2008)
New Revision: 9775

Modified:
   trunk/plearn/io/PStream.cc
   trunk/plearn/io/PStream.h
Log:
-put a tmp buffer that was a static class variable as a non-static fct variable as this don't cause performance trouble(the compiler probably put in on the stack directly with building a new object)
-The performance was tested to confirm this.
-This make the PStream a little more thread safe, but their is probably other stuff to look at. Don't assume it is thread safe.


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2008-12-11 21:06:26 UTC (rev 9774)
+++ trunk/plearn/io/PStream.cc	2008-12-12 21:06:28 UTC (rev 9775)
@@ -129,8 +129,6 @@
 
 PStream perr = get_perr();
 
-char PStream::tmpbuf[100];
-
 PStream& flush(PStream& out)
 {
     out.flush();
@@ -481,54 +479,63 @@
 
 void PStream::writeAsciiNum(char x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%d", (int)x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(unsigned char x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%d", (int)x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(signed char x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%d", (int)x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(short x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%hd", x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(unsigned short x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%hu", x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(int x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%d", x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(unsigned int x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%u", x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(long x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%ld", x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
 
 void PStream::writeAsciiNum(unsigned long x)
 {
+    char tmpbuf[PSTREAM_BUF_SIZE];
     snprintf(tmpbuf, sizeof(tmpbuf), "%lu", x);
     write(tmpbuf, streamsize(strlen(tmpbuf)));
 }
@@ -578,6 +585,7 @@
     }
     else
     {
+        char tmpbuf[PSTREAM_BUF_SIZE];
         snprintf(tmpbuf, sizeof(tmpbuf), format_float, x);
         write(tmpbuf, streamsize(strlen(tmpbuf)));
     }
@@ -595,6 +603,7 @@
     }
     else
     {
+        char tmpbuf[PSTREAM_BUF_SIZE];
         snprintf(tmpbuf, sizeof(tmpbuf), format_double, x);
         write(tmpbuf, streamsize(strlen(tmpbuf)));
     }
@@ -743,6 +752,7 @@
 void PStream::readAsciiNum(double &x)
 {
     static const char* error_msg = "Bug while reading file and expecting a double";
+    char tmpbuf[PSTREAM_BUF_SIZE];
     skipBlanks();
     int l=0;
     bool opposite = false;

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2008-12-11 21:06:26 UTC (rev 9774)
+++ trunk/plearn/io/PStream.h	2008-12-12 21:06:28 UTC (rev 9775)
@@ -38,6 +38,11 @@
 #ifndef PStream_INC
 #define PStream_INC
 
+//!The size of the temporary buffer to use inside some class function.
+//!Putting the buffer as a class variable don't give any speedup.
+//!So to remove threads issue, we put them as function variable.
+#define PSTREAM_BUF_SIZE 100
+
 #include <map>
 #include <set>
 #include <sstream>
@@ -156,9 +161,6 @@
     map<void *, unsigned int> copies_map_out;
 
 private:
-    //! Buffer for some formatting operations
-    static char tmpbuf[100];
-
     //! Current format string for floats
     const char* format_float;
 



From nouiz at mail.berlios.de  Mon Dec 15 16:07:01 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 16:07:01 +0100
Subject: [Plearn-commits] r9776 - trunk/plearn/vmat
Message-ID: <200812151507.mBFF71HP009647@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 16:07:00 +0100 (Mon, 15 Dec 2008)
New Revision: 9776

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
moved a static class buffer to multiple non-static fct buffer as this don't change the speed and it make it more thread safe. The TextFileVMatrix clas is not thread safe.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-12-12 21:06:28 UTC (rev 9775)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-12-15 15:07:00 UTC (rev 9776)
@@ -49,8 +49,6 @@
 using namespace std;
 
 
-char TextFilesVMatrix::buf[50000];
-
 TextFilesVMatrix::TextFilesVMatrix():
     idxfile(0),
     delimiter("\t"),
@@ -127,6 +125,7 @@
     fwrite(&length_, 4, 1, idxfile);
 
     TVec<string> fields;
+    char buf[50000];
 
     int lineno = 0;
     for(unsigned char fileno=0; fileno<txtfiles.length(); fileno++)
@@ -224,7 +223,8 @@
     width_ = 0;
     TVec<string> fnames;
     TVec<string> fnames_header;//field names take in the header of source file
-    
+    char buf[50000];
+
     //read the fieldnames from the files.
     for(int i=0; i<txtfiles.size(); i++){
         FILE* f = txtfiles[i];
@@ -464,6 +464,8 @@
     getFileAndPos(i, fileno, pos);
     FILE* f = txtfiles[(int)fileno];
     fseek(f,pos,SEEK_SET);
+    char buf[50000];
+
     if(!fgets(buf, sizeof(buf), f))
         PLERROR("In TextFilesVMatrix::getTextRow - fgets for row %d returned NULL",i);
     return removenewline(buf);
@@ -979,7 +981,7 @@
     inherited::declareOptions(ol);
 }
 
-void TextFilesVMatrix::readAndCheckOptionName(PStream& in, const string& optionname)
+void TextFilesVMatrix::readAndCheckOptionName(PStream& in, const string& optionname, char buf[])
 {
     in.skipBlanksAndComments();
     in.readUntil(buf, sizeof(buf), "= ");

Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2008-12-12 21:06:28 UTC (rev 9775)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2008-12-15 15:07:00 UTC (rev 9776)
@@ -72,8 +72,6 @@
     // * protected options *
     // *********************
 
-    static char buf[];
-
     FILE* idxfile;
     TVec<FILE*> txtfiles;
 
@@ -153,7 +151,7 @@
     void setColumnNamesAndWidth();
     void getFileAndPos(int i, unsigned char& fileno, int& pos) const;
     void buildIdx();
-    static void readAndCheckOptionName(PStream& in, const string& optionname);
+    static void readAndCheckOptionName(PStream& in, const string& optionname, char buf[]);
     void closeCurrentFile();
 
 protected:



From nouiz at mail.berlios.de  Mon Dec 15 18:04:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 18:04:26 +0100
Subject: [Plearn-commits] r9777 - in trunk/plearn_learners: meta regressors
Message-ID: <200812151704.mBFH4QZC021705@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 18:04:26 +0100 (Mon, 15 Dec 2008)
New Revision: 9777

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
optimization, In a RegressionTree, we should not finalize the train_set or the sorted_train_set as an AdaBoost, we will reuse it. AdaBoost will finalize it when needed.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-12-15 15:07:00 UTC (rev 9776)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-12-15 17:04:26 UTC (rev 9777)
@@ -286,7 +286,8 @@
     for(int i=0;i<weak_learners.size();i++){
         weak_learners[i]->finalize();
     }
-
+    if(train_set->classname()=="RegressionTreeRegisters")
+        ((PP<RegressionTreeRegisters>)train_set)->finalize();
 }
 
 void AdaBoost::forget()

Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-15 15:07:00 UTC (rev 9776)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-15 17:04:26 UTC (rev 9777)
@@ -280,10 +280,13 @@
     split_values = Vec();
     leave_template = 0;
     first_leave = 0;
-    if(sorted_train_set)
-        sorted_train_set->finalize();
-    if(train_set->classname()=="RegressionTreeRegisters")
-        ((PP<RegressionTreeRegisters>)train_set)->finalize();
+    //we should not finalize the train_set and the sorted_train_set here 
+    //as AdaBoost share it between different weak_learners!
+    //AdaBoost will finalize.
+//    if(sorted_train_set)
+//        sorted_train_set->finalize();
+//    if(train_set->classname()=="RegressionTreeRegisters")
+//        ((PP<RegressionTreeRegisters>)train_set)->finalize();
 }
 
 void RegressionTree::forget()



From nouiz at mail.berlios.de  Mon Dec 15 18:35:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 18:35:10 +0100
Subject: [Plearn-commits] r9778 - trunk/plearn_learners/regressors
Message-ID: <200812151735.mBFHZA0S029991@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 18:35:06 +0100 (Mon, 15 Dec 2008)
New Revision: 9778

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
optimization, call getOutputAndError only when needed.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 17:04:26 UTC (rev 9777)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 17:35:06 UTC (rev 9778)
@@ -173,8 +173,7 @@
     real target = train_set->getTarget(row);
     removeRow(row, target, weight, output, error);
 }
-void RegressionTreeLeave::removeRow(int row, real target, real weight,
-                                    Vec outputv, Vec errorv)
+void RegressionTreeLeave::removeRow(int row, real target, real weight)
 {
     length -= 1;
     weights_sum -= weight;
@@ -182,6 +181,11 @@
     real squared_target = pow(target, 2);
     weighted_targets_sum -= weight * target;
     weighted_squared_targets_sum -= weight * squared_target; 
+}
+void RegressionTreeLeave::removeRow(int row, real target, real weight,
+                                    Vec outputv, Vec errorv)
+{
+    removeRow(row,target,weight);
     getOutputAndError(outputv, errorv);
 }
 

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 17:04:26 UTC (rev 9777)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 17:35:06 UTC (rev 9778)
@@ -95,6 +95,7 @@
     virtual void         addRow(int row, real target, real weight);
     virtual void         addRow(int row, Vec outputv, Vec errorv);
     virtual void         addRow(int row, real target, real weight, Vec outputv, Vec errorv);
+    virtual void         removeRow(int row, real target, real weight);
     virtual void         removeRow(int row, Vec outputv, Vec errorv);
     virtual void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
     inline void          registerRow(int row)

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-12-15 17:04:26 UTC (rev 9777)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-12-15 17:35:06 UTC (rev 9778)
@@ -178,6 +178,12 @@
 
 void RegressionTreeMulticlassLeave::removeRow(int row, real target, real weight,
                                  Vec outputv, Vec errorv){
+    removeRow(row,target,weight);
+    getOutputAndError(outputv,errorv);
+}
+
+void RegressionTreeMulticlassLeave::removeRow(int row, real target, real weight)
+{
     length -= 1;
     weights_sum -= weight;
     PLASSERT(length>=0);
@@ -195,7 +201,6 @@
         }
     }
     PLASSERT(found);
-    getOutputAndError(outputv,errorv);
 }
 
 void RegressionTreeMulticlassLeave::getOutputAndError(Vec& output, Vec& error)const

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-12-15 17:04:26 UTC (rev 9777)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2008-12-15 17:35:06 UTC (rev 9778)
@@ -81,6 +81,7 @@
     void         addRow(int row, real target, real weight);
     void         addRow(int row, Vec outputv, Vec errorv);
     void         addRow(int row, real target, real weight, Vec outputv, Vec errorv);
+    void         removeRow(int row, real target, real weight);
     void         removeRow(int row, Vec outputv, Vec errorv);
     void         removeRow(int row, real target, real weight, Vec outputv, Vec errorv);
     void         getOutputAndError(Vec& output, Vec& error)const;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 17:04:26 UTC (rev 9777)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 17:35:06 UTC (rev 9778)
@@ -395,17 +395,21 @@
     for(int i=candidates.size()-2;i>=0;i--)
     {
         int next_row = candidates[i];
-        left_leave->removeRow(row, targets[i+1], weights[i+1], tmp, left_error);
-        right_leave->addRow(row, targets[i+1], weights[i+1], tmp, right_error);
-
         real next_feature=values[i];
         real row_feature=values[i+1];
         PLASSERT(train_set->get(next_row, col)==values[i]);
         PLASSERT(train_set->get(row, col)==values[i+1]);
         PLASSERT(next_feature<=row_feature);
+
+
+        left_leave->removeRow(row, targets[i+1], weights[i+1]);
+        right_leave->addRow(row, targets[i+1], weights[i+1]);
         row = next_row;
-
-        if (next_feature >= row_feature) continue;
+        if (next_feature < row_feature){
+            left_leave->getOutputAndError(tmp, left_error);
+            right_leave->getOutputAndError(tmp, right_error);
+        }else
+            continue;
         real work_error = missing_errors + left_error[0]
             + left_error[1] + right_error[0] + right_error[1];
         int work_balance = abs(left_leave->getLength() -



From nouiz at mail.berlios.de  Mon Dec 15 20:04:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 20:04:11 +0100
Subject: [Plearn-commits] r9779 - trunk/plearn_learners/regressors
Message-ID: <200812151904.mBFJ4BeT019865@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 20:04:10 +0100 (Mon, 15 Dec 2008)
New Revision: 9779

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
small refactoring:
- RegressionTreeLeave::initLeave now take in addition as parameter the leave id and if it is a missing leave or not.
- RegressionTreeLeave don't have any friend class now.
- Put a dummy variable as static to save memory. It is used to reload old version.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-15 17:35:06 UTC (rev 9778)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-12-15 19:04:10 UTC (rev 9779)
@@ -317,8 +317,7 @@
     leave_template->initStats();
 
     first_leave = ::PLearn::deepCopy(leave_template);
-    first_leave->id=sorted_train_set->getNextId();
-    first_leave->initLeave(sorted_train_set);
+    first_leave->initLeave(sorted_train_set, sorted_train_set->getNextId());
 
     for (int train_sample_index = 0; train_sample_index < length;
          train_sample_index++)

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 17:35:06 UTC (rev 9778)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 19:04:10 UTC (rev 9779)
@@ -97,9 +97,9 @@
     declareOption(ol, "loss_function_factor", &RegressionTreeLeave::loss_function_factor, OptionBase::learntoption,
                   "2 / pow(loss_function_weight, 2.0).\n");
 
-    declareOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
+    declareStaticOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
-    declareOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
+    declareStaticOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
 
     inherited::declareOptions(ol);
@@ -121,9 +121,12 @@
 {
 }
 
-void RegressionTreeLeave::initLeave(PP<RegressionTreeRegisters> the_train_set)
+void RegressionTreeLeave::initLeave(PP<RegressionTreeRegisters> the_train_set, RTR_type_id the_id, bool the_missing_leave)
 {
     train_set = the_train_set;
+    if(the_id>=0)
+        id = the_id;
+    missing_leave = the_missing_leave;
 }
 
 void RegressionTreeLeave::initStats()

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 17:35:06 UTC (rev 9778)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 19:04:10 UTC (rev 9779)
@@ -51,11 +51,8 @@
 {
     typedef Object inherited;
  
-    friend class RegressionTreeNode;
-    friend class RegressionTree;
+    static Vec dummy_vec;
 
-    Vec dummy_vec;
-
 public:
     bool missing_leave;
     real loss_function_weight;
@@ -89,7 +86,7 @@
     static  void         declareOptions(OptionList& ol);
     virtual void         makeDeepCopyFromShallowCopy(CopiesMap &copies);
     virtual void         build();
-    void         initLeave(PP<RegressionTreeRegisters> the_train_set);
+    void         initLeave(PP<RegressionTreeRegisters> the_train_set, RTR_type_id the_id=-1, bool the_missing_leave = false);
     virtual void         initStats();
     virtual void         addRow(int row);
     virtual void         addRow(int row, real target, real weight);

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 17:35:06 UTC (rev 9778)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 19:04:10 UTC (rev 9779)
@@ -205,17 +205,13 @@
     int right_leave_id =  the_train_set->getNextId();
 
     missing_leave = ::PLearn::deepCopy(leave_template);
-    missing_leave->id=missing_leave_id;
-    missing_leave->missing_leave=missing_is_valid;
-    missing_leave->initLeave(the_train_set);
+    missing_leave->initLeave(the_train_set, missing_leave_id, missing_is_valid);
 
     left_leave = ::PLearn::deepCopy(leave_template);
-    left_leave->id=left_leave_id;
-    left_leave->initLeave(the_train_set);
+    left_leave->initLeave(the_train_set, left_leave_id);
 
     right_leave = ::PLearn::deepCopy(leave_template);
-    right_leave->id=right_leave_id;
-    right_leave->initLeave(the_train_set);
+    right_leave->initLeave(the_train_set, right_leave_id);
 
     leave_output.resize(2);
     leave_error.resize(3);
@@ -246,13 +242,13 @@
 //#define RCMP
 void RegressionTreeNode::lookForBestSplit()
 {
-    if(leave->length<=1)
+    if(leave->getLength()<=1)
         return;
-    TVec<RTR_type> candidate(0, leave->length);//list of candidate row to split
-    TVec<RTR_type> registered_row(0, leave->length);
-    Vec registered_target(0, leave->length); 
-    Vec registered_weight(0, leave->length);
-    Vec registered_value(0, leave->length);
+    TVec<RTR_type> candidate(0, leave->getLength());//list of candidate row to split
+    TVec<RTR_type> registered_row(0, leave->getLength());
+    Vec registered_target(0, leave->getLength()); 
+    Vec registered_weight(0, leave->getLength());
+    Vec registered_value(0, leave->getLength());
    tmp_vec.resize(2);
     Vec left_error(3);
     Vec right_error(3);
@@ -269,7 +265,7 @@
     row_split_value.clear();
     row_split_balance.clear();
 #endif
-    int leave_id = leave->id;
+    int leave_id = leave->getId();
     for (int col = 0; col < inputsize; col++)
     {
         missing_leave->initStats();
@@ -322,7 +318,7 @@
 #ifndef BY_ROW
         //in case of missing value
         if(candidate.size()==0){
-            PLASSERT(missing_leave->length()>0);
+            PLASSERT(missing_leave->getLength()>0);
             continue;
         }
         int row = candidate.pop();
@@ -456,7 +452,7 @@
     right_leave->initStats();
     TVec<RTR_type>registered_row;
     PP<RegressionTreeRegisters> train_set = tree->getSortedTrainingSet();
-    train_set->getAllRegisteredRow(leave->id,split_col,registered_row);
+    train_set->getAllRegisteredRow(leave->getId(),split_col,registered_row);
 
     for (int row_index = 0;row_index<registered_row.size();row_index++)
     {



From nouiz at mail.berlios.de  Mon Dec 15 20:31:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 20:31:29 +0100
Subject: [Plearn-commits] r9780 - trunk/plearn_learners/regressors
Message-ID: <200812151931.mBFJVT9L023273@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 20:31:29 +0100 (Mon, 15 Dec 2008)
New Revision: 9780

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
removed RegressionTreeLeave::getLength() to the more standard RegressionTreeLeave::length()


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 19:04:10 UTC (rev 9779)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 19:31:29 UTC (rev 9780)
@@ -57,7 +57,7 @@
     missing_leave(false),
     loss_function_weight(0),
     id(-1),
-    length(0),
+    length_(0),
     weights_sum(0),
     targets_sum(0),
     weighted_targets_sum(0),
@@ -84,7 +84,7 @@
     declareOption(ol, "train_set", &RegressionTreeLeave::train_set, 
                   OptionBase::buildoption | OptionBase::nosave,
                   "The train set with the sorted row index matrix and the leave id vector\n");
-    declareOption(ol, "length", &RegressionTreeLeave::length, OptionBase::learntoption,
+    declareOption(ol, "length", &RegressionTreeLeave::length_, OptionBase::learntoption,
                   "The number of rows in this leave\n");
     declareOption(ol, "weights_sum", &RegressionTreeLeave::weights_sum, OptionBase::learntoption,
                   "The sum of weights for the samples in this leave\n");
@@ -131,7 +131,7 @@
 
 void RegressionTreeLeave::initStats()
 {
-    length = 0;
+    length_ = 0;
     weights_sum= 0.0;
     targets_sum = 0.0;
     weighted_targets_sum = 0.0;
@@ -143,7 +143,7 @@
 
 void RegressionTreeLeave::addRow(int row, real target, real weight)
 {
-    length += 1;
+    length_ += 1;
     weights_sum += weight;
     targets_sum += target;
     real squared_target = pow(target, 2);
@@ -178,7 +178,7 @@
 }
 void RegressionTreeLeave::removeRow(int row, real target, real weight)
 {
-    length -= 1;
+    length_ -= 1;
     weights_sum -= weight;
     targets_sum -= target;
     real squared_target = pow(target, 2);
@@ -194,7 +194,7 @@
 
 void RegressionTreeLeave::getOutputAndError(Vec& output, Vec& error)const
 {
-    if(length>0){
+    if(length_>0){
         output[0] = weighted_targets_sum / weights_sum;
         if (missing_leave != true)
         {
@@ -226,7 +226,7 @@
 
 void RegressionTreeLeave::printStats()
 {
-    cout << " l " << length;
+    cout << " l " << length_;
     Vec output(2);
     Vec error(3);
     getOutputAndError(output,error);

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 19:04:10 UTC (rev 9779)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 19:31:29 UTC (rev 9780)
@@ -71,7 +71,7 @@
   Learnt options: they are sized and initialized if need be, in initLeave(...)
 */
 
-    int  length;
+    int  length_;
     real weights_sum;
     real targets_sum;
     real weighted_targets_sum;
@@ -98,7 +98,7 @@
     inline void          registerRow(int row)
     {train_set->registerLeave(id, row);}
     inline int           getId()const{return id;}
-    inline int           getLength()const{return length;}
+    inline int           length()const{return length_;}
     virtual void         getOutputAndError(Vec& output, Vec& error)const;
     virtual void         printStats();
   

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-12-15 19:04:10 UTC (rev 9779)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-12-15 19:31:29 UTC (rev 9780)
@@ -112,7 +112,7 @@
 
 void RegressionTreeMulticlassLeave::initStats()
 {
-    length = 0;
+    length_ = 0;
     weights_sum = 0.0;
     if (loss_function_weight != 0.0)
     {
@@ -144,7 +144,7 @@
 
 void RegressionTreeMulticlassLeave::addRow(int row, real target, real weight)
 {
-    length += 1;
+    length_ += 1;
     weights_sum += weight;
     int multiclass_found = 0;
     //if target are 0,1,2,... it can be optimized by multiclass_weights_sum[target]
@@ -184,11 +184,11 @@
 
 void RegressionTreeMulticlassLeave::removeRow(int row, real target, real weight)
 {
-    length -= 1;
+    length_ -= 1;
     weights_sum -= weight;
-    PLASSERT(length>=0);
+    PLASSERT(length_>=0);
     PLASSERT(weights_sum>=0);
-    PLASSERT(length>0 || weights_sum==0);
+    PLASSERT(length_>0 || weights_sum==0);
     bool found=false;
     //can be optimized: see addRow
     for (int mc_ind = 0; mc_ind < multiclass_outputs.length(); mc_ind++)
@@ -210,7 +210,7 @@
         PLERROR("In RegressionTreeMulticlassLeave::getOutputAndError() -"
                 " multiclass_outputs must not be empty");
 #endif
-    if(length==0){        
+    if(length_==0){        
         output.clear();
         output[0]=MISSING_VALUE;
         error.clear();
@@ -242,7 +242,7 @@
                 error[0] += abs(output[0] - multiclass_outputs[mc_ind]) 
                     * multiclass_weights_sum[mc_ind];
             }
-            error[0] *= l1_loss_function_factor * length / weights_sum;
+            error[0] *= l1_loss_function_factor * length_ / weights_sum;
             if (error[0] < 1E-10) error[0] = 0.0;
             if (error[0] > weights_sum * l1_loss_function_factor)
                 error[2] = weights_sum * l1_loss_function_factor;
@@ -255,19 +255,19 @@
                 error[0] += pow(output[0] - multiclass_outputs[mc_ind], 2) 
                     * multiclass_weights_sum[mc_ind];
             }
-            error[0] *= l2_loss_function_factor * length / weights_sum;
+            error[0] *= l2_loss_function_factor * length_ / weights_sum;
             if (error[0] < 1E-10) error[0] = 0.0;
             if (error[0] > weights_sum * l2_loss_function_factor) 
                 error[2] = weights_sum * l2_loss_function_factor; 
             else error[2] = error[0];
         }
-        error[1] = (1.0 - output[1]) * length;
+        error[1] = (1.0 - output[1]) * length_;
     }
 }
 
 void RegressionTreeMulticlassLeave::printStats()
 {
-    cout << " l " << length;
+    cout << " l " << length_;
     Vec output(2);
     Vec error(3);
     getOutputAndError(output,error);

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 19:04:10 UTC (rev 9779)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 19:31:29 UTC (rev 9780)
@@ -242,13 +242,13 @@
 //#define RCMP
 void RegressionTreeNode::lookForBestSplit()
 {
-    if(leave->getLength()<=1)
+    if(leave->length()<=1)
         return;
-    TVec<RTR_type> candidate(0, leave->getLength());//list of candidate row to split
-    TVec<RTR_type> registered_row(0, leave->getLength());
-    Vec registered_target(0, leave->getLength()); 
-    Vec registered_weight(0, leave->getLength());
-    Vec registered_value(0, leave->getLength());
+    TVec<RTR_type> candidate(0, leave->length());//list of candidate row to split
+    TVec<RTR_type> registered_row(0, leave->length());
+    Vec registered_target(0, leave->length()); 
+    Vec registered_weight(0, leave->length());
+    Vec registered_value(0, leave->length());
    tmp_vec.resize(2);
     Vec left_error(3);
     Vec right_error(3);
@@ -318,7 +318,7 @@
 #ifndef BY_ROW
         //in case of missing value
         if(candidate.size()==0){
-            PLASSERT(missing_leave->getLength()>0);
+            PLASSERT(missing_leave->length()>0);
             continue;
         }
         int row = candidate.pop();
@@ -408,8 +408,8 @@
             continue;
         real work_error = missing_errors + left_error[0]
             + left_error[1] + right_error[0] + right_error[1];
-        int work_balance = abs(left_leave->getLength() -
-                               right_leave->getLength());
+        int work_balance = abs(left_leave->length() -
+                               right_leave->length());
         if (fast_is_more(work_error,best_split_error)) continue;
         else if (fast_is_equal(work_error,best_split_error) &&
                  fast_is_more(work_balance,best_balance)) continue;
@@ -429,7 +429,7 @@
     PLASSERT(left_leave_last_feature<=right_leave_first_feature);
     if (left_leave_last_feature >= right_leave_first_feature) return;
     real work_error = missing_error[0] + missing_error[1] + left_error[0] + left_error[1] + right_error[0] + right_error[1];
-    int work_balance = abs(left_leave->getLength() - right_leave->getLength());
+    int work_balance = abs(left_leave->length() - right_leave->length());
     if (fast_is_more(work_error,after_split_error)) return;
     else if (fast_is_equal(work_error,after_split_error) &&
              fast_is_more(work_balance,split_balance)) return;



From nouiz at mail.berlios.de  Mon Dec 15 20:32:12 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 20:32:12 +0100
Subject: [Plearn-commits] r9781 - trunk/plearn_learners/regressors
Message-ID: <200812151932.mBFJWCUi023382@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 20:32:11 +0100 (Mon, 15 Dec 2008)
New Revision: 9781

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
Log:
removing uncommited change.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 19:31:29 UTC (rev 9780)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 19:32:11 UTC (rev 9781)
@@ -97,9 +97,9 @@
     declareOption(ol, "loss_function_factor", &RegressionTreeLeave::loss_function_factor, OptionBase::learntoption,
                   "2 / pow(loss_function_weight, 2.0).\n");
 
-    declareStaticOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
+    declareOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
-    declareStaticOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
+    declareOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
 
     inherited::declareOptions(ol);



From nouiz at mail.berlios.de  Mon Dec 15 20:42:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 20:42:14 +0100
Subject: [Plearn-commits] r9782 - trunk/plearn_learners/regressors
Message-ID: <200812151942.mBFJgEKe027388@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 20:42:13 +0100 (Mon, 15 Dec 2008)
New Revision: 9782

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
removed uncommited change


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 19:32:11 UTC (rev 9781)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 19:42:13 UTC (rev 9782)
@@ -51,7 +51,7 @@
 {
     typedef Object inherited;
  
-    static Vec dummy_vec;
+    Vec dummy_vec;
 
 public:
     bool missing_leave;



From nouiz at mail.berlios.de  Mon Dec 15 20:49:41 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 20:49:41 +0100
Subject: [Plearn-commits] r9783 - trunk/plearn_learners/regressors
Message-ID: <200812151949.mBFJnfua028578@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 20:49:41 +0100 (Mon, 15 Dec 2008)
New Revision: 9783

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
bugfix in dbg mode from last commit


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 19:42:13 UTC (rev 9782)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 19:49:41 UTC (rev 9783)
@@ -477,10 +477,10 @@
         }
     }
 
-    PLASSERT(left_leave->length>0);
-    PLASSERT(right_leave->length>0);
-    PLASSERT(left_leave->length + right_leave->length + missing_leave->length
-             == registered_row.size());
+    PLASSERT(left_leave->length()>0);
+    PLASSERT(right_leave->length()>0);
+    PLASSERT(left_leave->length() + right_leave->length() + 
+             missing_leave->length() == registered_row.size());
 //  leave->printStats();
 //  left_leave->printStats();
 //  right_leave->printStats();



From nouiz at mail.berlios.de  Mon Dec 15 21:00:22 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 21:00:22 +0100
Subject: [Plearn-commits] r9784 - trunk/plearn/base
Message-ID: <200812152000.mBFK0MUQ030569@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 21:00:22 +0100 (Mon, 15 Dec 2008)
New Revision: 9784

Modified:
   trunk/plearn/base/Option.h
Log:
added needed classs and function to allow declareStaticOption() for TVec static class variable.


Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2008-12-15 19:49:41 UTC (rev 9783)
+++ trunk/plearn/base/Option.h	2008-12-15 20:00:22 UTC (rev 9784)
@@ -321,6 +321,37 @@
 };
 
 
+// This is a special version of Option designed for TVec<T>.
+// The only difference is that it supports indexed reads and writes.
+template<class VecElementType>
+class TVecStaticOption : public StaticOption<TVec<VecElementType> >
+{
+    typedef StaticOption<TVec<VecElementType> > inherited;
+  
+public:
+    TVecStaticOption(const string& optionname, TVec<VecElementType> * member_ptr, 
+               OptionBase::flag_t flags, const string& optiontype, const string& defaultval,
+               const string& description, const OptionBase::OptionLevel& level)
+        : inherited(optionname, member_ptr, flags, optiontype, defaultval,
+                    description, level)
+    { }
+
+    virtual void readIntoIndex(Object* o, PStream& in, const string& index)
+    {
+        int i = tolong(index);
+//        in >> (dynamic_cast<ObjectType*>(o)->*(this->ptr))[i];
+        in >> (*(this->ptr))[i];
+    }
+
+    virtual void writeAtIndex(const Object* o, PStream& out, const string& index) const
+    {
+        int i = tolong(index);
+//        out << (dynamic_cast<ObjectType*>(const_cast<Object*>(o))->*(this->ptr))[i];
+        out << (*(this->ptr))[i];
+    }
+
+};
+
 //#####  declareOption and Friends  ###########################################
 
 /**
@@ -385,8 +416,24 @@
                                                     defaultval, description, level));
 }
 
+// Overload for TVec<T>
+template <class VecElementType>//, class VecElementType>
+inline void declareStaticOption(OptionList& ol,                      //!< list to which this option should be appended 
+                          const string& optionname,            //!< the name of this option
+                          TVec<VecElementType> * ptr,                     //!< &YourClass::your_static_field
+                          OptionBase::flag_t flags,            //!< see the flags in OptionBase
+                          const string& description,           //!< a description of the option
+                          const OptionBase::OptionLevel level= OptionBase::default_level, //!< Option level (see OptionBase)
+                          const string& defaultval="")         //!< default value for this option, as set by the default constructor
+{
+    ol.push_back(new TVecStaticOption<VecElementType>(
+                     optionname, ptr, flags, 
+                     TypeTraits< TVec<VecElementType> >::name(), 
+                     defaultval, description, level));
+}
 
 
+
 // Overload for TVec<T>
 template <class ObjectType, class VecElementType>
 inline void declareOption(OptionList& ol,



From nouiz at mail.berlios.de  Mon Dec 15 21:02:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 15 Dec 2008 21:02:13 +0100
Subject: [Plearn-commits] r9785 - trunk/plearn_learners/regressors
Message-ID: <200812152002.mBFK2DBD030705@sheep.berlios.de>

Author: nouiz
Date: 2008-12-15 21:02:13 +0100 (Mon, 15 Dec 2008)
New Revision: 9785

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
Log:
use the new declareStaticOption() for TVec to save memory for a dummy_vec variable used only to load version of the object.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 20:00:22 UTC (rev 9784)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-12-15 20:02:13 UTC (rev 9785)
@@ -52,6 +52,7 @@
     );
 
 int RegressionTreeLeave::verbosity = 0;
+Vec RegressionTreeLeave::dummy_vec;
 
 RegressionTreeLeave::RegressionTreeLeave():
     missing_leave(false),
@@ -97,9 +98,9 @@
     declareOption(ol, "loss_function_factor", &RegressionTreeLeave::loss_function_factor, OptionBase::learntoption,
                   "2 / pow(loss_function_weight, 2.0).\n");
 
-    declareOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
+    declareStaticOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
-    declareOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
+    declareStaticOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
 
     inherited::declareOptions(ol);

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 20:00:22 UTC (rev 9784)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2008-12-15 20:02:13 UTC (rev 9785)
@@ -51,7 +51,7 @@
 {
     typedef Object inherited;
  
-    Vec dummy_vec;
+    static Vec dummy_vec;
 
 public:
     bool missing_leave;



From nouiz at mail.berlios.de  Tue Dec 16 21:45:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 16 Dec 2008 21:45:23 +0100
Subject: [Plearn-commits] r9786 - trunk/plearn_learners/regressors
Message-ID: <200812162045.mBGKjNxC020672@sheep.berlios.de>

Author: nouiz
Date: 2008-12-16 21:45:23 +0100 (Tue, 16 Dec 2008)
New Revision: 9786

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
optimization: don't use vec.append, but use vec[] instead.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-15 20:02:13 UTC (rev 9785)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-12-16 20:45:23 UTC (rev 9786)
@@ -245,7 +245,7 @@
     if(leave->length()<=1)
         return;
     TVec<RTR_type> candidate(0, leave->length());//list of candidate row to split
-    TVec<RTR_type> registered_row(0, leave->length());
+    TVec<RTR_type> registered_row(leave->length());
     Vec registered_target(0, leave->length()); 
     Vec registered_weight(0, leave->length());
     Vec registered_value(0, leave->length());
@@ -271,12 +271,12 @@
         missing_leave->initStats();
         left_leave->initStats();
         right_leave->initStats();
-        registered_row.resize(0);
         
         train_set->getAllRegisteredRow(leave_id, col, registered_row,
                                        registered_target, registered_weight,
                                        registered_value);
-        PLASSERT(registered_row.size()>0);
+
+        PLASSERT(registered_row.size()==leave->length());
         PLASSERT(candidate.size()==0);
 
         //we do this optimization in case their is many row with the same value
@@ -450,7 +450,7 @@
     missing_leave->initStats();
     left_leave->initStats();
     right_leave->initStats();
-    TVec<RTR_type>registered_row;
+    TVec<RTR_type>registered_row(leave->length());
     PP<RegressionTreeRegisters> train_set = tree->getSortedTrainingSet();
     train_set->getAllRegisteredRow(leave->getId(),split_col,registered_row);
 

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-12-15 20:02:13 UTC (rev 9785)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-12-16 20:45:23 UTC (rev 9786)
@@ -211,14 +211,21 @@
             value[i]  = tsource->get(col, reg[i]);
     }
 }
+
+//! reg must already have a size >= the number of row that we will put in it.
+//! Will resize reg to the number of registered leave==leave_id
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg) const
 {
-    for(int i=0;i<length();i++)
+
+    int idx=0;
+    int n=reg.length();
+    int i;
+    for( i=0;i<length() && n> idx;i++)
     {
         int srow = tsorted_row(col, i);
         if ( leave_register[srow] == leave_id)
-            reg.append(srow);
+            reg[idx++]=srow;
     }
 }
 



From nouiz at mail.berlios.de  Tue Dec 16 22:02:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 16 Dec 2008 22:02:08 +0100
Subject: [Plearn-commits] r9787 - trunk/scripts
Message-ID: <200812162102.mBGL28Xf024418@sheep.berlios.de>

Author: nouiz
Date: 2008-12-16 22:02:07 +0100 (Tue, 16 Dec 2008)
New Revision: 9787

Modified:
   trunk/scripts/perlgrep
Log:
will check .C file too.


Modified: trunk/scripts/perlgrep
===================================================================
--- trunk/scripts/perlgrep	2008-12-16 20:45:23 UTC (rev 9786)
+++ trunk/scripts/perlgrep	2008-12-16 21:02:07 UTC (rev 9787)
@@ -103,7 +103,7 @@
                 my @flist = lsdir($fname);
                 
                 @flist = grep { -d $_ or /Makefile|makefile|pytest\.config|
-                                    \.c$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$
+                                    \.c$|\.C$|\.cc$|\.cpp$|\.CC$|\.h$|\.hpp$
                                    |\.plearn$|\.pyplearn$|\.psave$|\.vmat$|\.py$|\.pymat$
                                    |\.txt$|^readme|^Readme|^README/x } @flist;
                 process_list(@flist); 
@@ -129,7 +129,7 @@
 
 Will perform the specified grep operation on every file in the
 list and recursively in directories.  (only certain kinds of files are
-considered when recursing in directories, .c .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README)
+considered when recursing in directories, .c .C .cc .cpp .CC .h .hpp .txt .plearn .pyplearn .psave .py .vmat .pymat Makefile makefile readme Readme README)
 
 Ex: perlgrep '\\bVMatrix\\b' .
 ";



From nouiz at mail.berlios.de  Tue Dec 16 22:27:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 16 Dec 2008 22:27:07 +0100
Subject: [Plearn-commits] r9788 - trunk/plearn/base
Message-ID: <200812162127.mBGLR7KP026584@sheep.berlios.de>

Author: nouiz
Date: 2008-12-16 22:27:07 +0100 (Tue, 16 Dec 2008)
New Revision: 9788

Modified:
   trunk/plearn/base/Option.h
Log:
removed old comment.


Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2008-12-16 21:02:07 UTC (rev 9787)
+++ trunk/plearn/base/Option.h	2008-12-16 21:27:07 UTC (rev 9788)
@@ -339,14 +339,12 @@
     virtual void readIntoIndex(Object* o, PStream& in, const string& index)
     {
         int i = tolong(index);
-//        in >> (dynamic_cast<ObjectType*>(o)->*(this->ptr))[i];
         in >> (*(this->ptr))[i];
     }
 
     virtual void writeAtIndex(const Object* o, PStream& out, const string& index) const
     {
         int i = tolong(index);
-//        out << (dynamic_cast<ObjectType*>(const_cast<Object*>(o))->*(this->ptr))[i];
         out << (*(this->ptr))[i];
     }
 



From nouiz at mail.berlios.de  Tue Dec 16 22:35:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 16 Dec 2008 22:35:15 +0100
Subject: [Plearn-commits] r9789 - in trunk: plearn/base plearn/math
	plearn_learners/distributions plearn_learners/meta
Message-ID: <200812162135.mBGLZFAp027573@sheep.berlios.de>

Author: nouiz
Date: 2008-12-16 22:35:14 +0100 (Tue, 16 Dec 2008)
New Revision: 9789

Modified:
   trunk/plearn/base/ProgressBar.cc
   trunk/plearn/base/ProgressBar.h
   trunk/plearn/math/stats_utils.cc
   trunk/plearn_learners/distributions/PDistribution.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
changed how we use the progress bar with OPENMP to be like with MPI. That make it more consistent and more thread safe. 


Modified: trunk/plearn/base/ProgressBar.cc
===================================================================
--- trunk/plearn/base/ProgressBar.cc	2008-12-16 21:27:07 UTC (rev 9788)
+++ trunk/plearn/base/ProgressBar.cc	2008-12-16 21:35:14 UTC (rev 9789)
@@ -53,6 +53,8 @@
 
 #if USING_MPI
 #include <plearn/sys/PLMPI.h>
+#elif _OPENMP
+#include <omp.h>
 #endif //USING_MPI
 
 namespace PLearn {
@@ -128,22 +130,25 @@
 {
 #if USING_MPI
     if(PLMPI::rank==0)
+#elif _OPENMP
+    if(omp_get_thread_num()==0)
+#endif
     {
-#endif
+
         string fulltitle = string(" ") + pb->title + " (" + tostring(pb->maxpos) + ") ";
         out << "[" + center(fulltitle,width,'-') + "]\n[";
         out.flush();
-#if USING_MPI
     }
-#endif
 }
 
 void TextProgressBarPlugin::update(ProgressBar * pb, uint32_t newpos)
 {
 #if USING_MPI
     if(PLMPI::rank==0)
+#elif _OPENMP
+    if(omp_get_thread_num()==0)
+#endif
     {
-#endif
         // this handles the case where we reuse the same progress bar
         if(newpos < pb->currentpos)
         {
@@ -168,9 +173,7 @@
             out << "]";
             out << endl;
         }
-#if USING_MPI
     }
-#endif
 }
 
 

Modified: trunk/plearn/base/ProgressBar.h
===================================================================
--- trunk/plearn/base/ProgressBar.h	2008-12-16 21:27:07 UTC (rev 9788)
+++ trunk/plearn/base/ProgressBar.h	2008-12-16 21:35:14 UTC (rev 9789)
@@ -168,7 +168,6 @@
     void operator()(uint32_t newpos){plugin->update(this,newpos);}
 
     void update(uint32_t newpos){plugin->update(this,newpos);}
-    void updateone(){plugin->update(this,currentpos+1);}
     // this function assumes plugin is always a valid object (it is created statically in the .cc)
     static void setPlugin(PP<ProgressBarPlugin> plugin_) { plugin = plugin_; }
     static PP<ProgressBarPlugin> getCurrentPlugin();

Modified: trunk/plearn/math/stats_utils.cc
===================================================================
--- trunk/plearn/math/stats_utils.cc	2008-12-16 21:27:07 UTC (rev 9788)
+++ trunk/plearn/math/stats_utils.cc	2008-12-16 21:35:14 UTC (rev 9789)
@@ -329,9 +329,7 @@
         Ds[col]=D;
         p_values[col]=p_value;
         if (report_progress)
-#pragma omp critical
-            pbar->updateone();
-
+            pbar->update(col);
     }
     }
 }

Modified: trunk/plearn_learners/distributions/PDistribution.cc
===================================================================
--- trunk/plearn_learners/distributions/PDistribution.cc	2008-12-16 21:27:07 UTC (rev 9788)
+++ trunk/plearn_learners/distributions/PDistribution.cc	2008-12-16 21:35:14 UTC (rev 9789)
@@ -407,7 +407,7 @@
         v = Y(i);
         generate(v);
         if (pb)
-            pb->updateone();
+            pb->update(i);
     }
 }
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-16 21:27:07 UTC (rev 9788)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-12-16 21:35:14 UTC (rev 9789)
@@ -231,13 +231,9 @@
     if(omp_get_max_threads()>1){
       PLCHECK(learner1->verbosity==0);
       PLCHECK(learner2->verbosity==0);
-      PLCHECK(learner1->report_progress==false);
-      PLCHECK(learner2->report_progress==false);
       
       PLCHECK(learner1->weak_learner_template->verbosity==0);
       PLCHECK(learner2->weak_learner_template->verbosity==0);
-      PLCHECK(learner1->weak_learner_template->report_progress==false);
-      PLCHECK(learner2->weak_learner_template->report_progress==false);
     }
     
     EXTREME_MODULE_LOG<<"train() // start"<<endl;



From nouiz at mail.berlios.de  Wed Dec 17 14:40:56 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 17 Dec 2008 14:40:56 +0100
Subject: [Plearn-commits] r9790 - in trunk/plearn_learners: generic online
Message-ID: <200812171340.mBHDeu6f022871@sheep.berlios.de>

Author: nouiz
Date: 2008-12-17 14:40:55 +0100 (Wed, 17 Dec 2008)
New Revision: 9790

Modified:
   trunk/plearn_learners/generic/NNet.cc
   trunk/plearn_learners/online/RBMModule.cc
Log:
fixed compilation.


Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-12-16 21:35:14 UTC (rev 9789)
+++ trunk/plearn_learners/generic/NNet.cc	2008-12-17 13:40:55 UTC (rev 9790)
@@ -1274,7 +1274,7 @@
                 & SumOverBagsVariable::TARGET_COLUMN_FIRST)
                 n_training_bags++;
             if (pb)
-                pb->updateone();
+                pb->update(i);
         }
     }
     

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-12-16 21:35:14 UTC (rev 9789)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-12-17 13:40:55 UTC (rev 9790)
@@ -1321,12 +1321,13 @@
                 verbosity >= 2 ? new ProgressBar("Gibbs sampling",
                                                  min_n - Gibbs_step)
                                : NULL;
+            int start = Gibbs_step;
             for (;Gibbs_step<min_n;Gibbs_step++)
             {
                 sampleHiddenGivenVisible(visible_layer->samples);
                 sampleVisibleGivenHidden(hidden_layer->samples);
                 if (pb)
-                    pb->updateone();
+                    pb->update(Gibbs_step - start);
             }
             if (pb)
                 pb = NULL;



From tihocan at mail.berlios.de  Wed Dec 17 18:11:51 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 17 Dec 2008 18:11:51 +0100
Subject: [Plearn-commits] r9791 - trunk/scripts/Skeletons
Message-ID: <200812171711.mBHHBpRU013370@sheep.berlios.de>

Author: tihocan
Date: 2008-12-17 18:11:51 +0100 (Wed, 17 Dec 2008)
New Revision: 9791

Modified:
   trunk/scripts/Skeletons/PLearner.cc
   trunk/scripts/Skeletons/PLearner.h
Log:
Moved new finalize() method into 'optional' methods that one can safely omit when writing a new PLearner

Modified: trunk/scripts/Skeletons/PLearner.cc
===================================================================
--- trunk/scripts/Skeletons/PLearner.cc	2008-12-17 13:40:55 UTC (rev 9790)
+++ trunk/scripts/Skeletons/PLearner.cc	2008-12-17 17:11:51 UTC (rev 9791)
@@ -85,13 +85,6 @@
     // may depend on its inputsize(), targetsize() and set options).
 }
 
-void DERIVEDCLASS::finalize()
-{
-    //! When this method is called the learner know it we will never train it again.
-    //! So it can free resources that are needed only during the training.
-    //! The function test()/computeOutputs()/... should continue to work.
-}
-
 void DERIVEDCLASS::forget()
 {
     //! (Re-)initialize the PLearner in its fresh state (that state may depend

Modified: trunk/scripts/Skeletons/PLearner.h
===================================================================
--- trunk/scripts/Skeletons/PLearner.h	2008-12-17 13:40:55 UTC (rev 9790)
+++ trunk/scripts/Skeletons/PLearner.h	2008-12-17 17:11:51 UTC (rev 9791)
@@ -43,12 +43,6 @@
 
 
 
-    //! When this method is called the learner know it we will never train it again.
-    //! So it can free resources that are needed only during the training.
-    //! The function test()/computeOutputs()/... should continue to work.
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void finalize();
-
     //! (Re-)initializes the PLearner in its fresh state (that state may depend
     //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
     //! a fresh learner!).
@@ -95,8 +89,11 @@
     // virtual int nTrainCosts() const;
     // virtual void resetInternalState();
     // virtual bool isStatefulLearner() const;
+    //! When this method is called the learner knows it we will not be trained
+    //! anymore, so it can free resources that are needed only during training.
+    //! Methods test()/computeOutputs()/... should continue to work.
+    // virtual void finalize();
 
-
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From nouiz at mail.berlios.de  Wed Dec 17 23:11:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 17 Dec 2008 23:11:50 +0100
Subject: [Plearn-commits] r9792 - trunk/python_modules/plearn/parallel
Message-ID: <200812172211.mBHMBoOb028602@sheep.berlios.de>

Author: nouiz
Date: 2008-12-17 23:11:50 +0100 (Wed, 17 Dec 2008)
New Revision: 9792

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
bugfix and added in comment a bette ways to generate the kerberos keys if we have the time to implement it.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-12-17 17:11:51 UTC (rev 9791)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-12-17 22:11:50 UTC (rev 9792)
@@ -858,6 +858,11 @@
             #keeps a list of the temporary files created, so that they can be deleted at will
 
     def get_pkdilly_var(self):
+
+#the ssh is to have a renewed and cleaned kerberos ticket
+#the +P is to have only the KRV* var, 
+#the +P don't need a condor_submit_file
+#ssh HOSTNAME pkdilly +P
         cmd="pkdilly -S "+self.condor_submit_file
         self.p = Popen( cmd, shell=True, stdout=PIPE, stderr=PIPE)
         self.p.wait()
@@ -992,9 +997,6 @@
                 fd.close()
 
                 fd = open(self.second_lauch_file,'w')
-                fd.write(dedent('''\
-                    #!/bin/sh
-                    '''))
 
             bash=not self.source_file or not self.source_file.endswith(".cshrc")
             if bash:



From nouiz at mail.berlios.de  Thu Dec 18 17:38:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 17:38:37 +0100
Subject: [Plearn-commits] r9793 - trunk/plearn/vmat
Message-ID: <200812181638.mBIGcbK0026863@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 17:38:36 +0100 (Thu, 18 Dec 2008)
New Revision: 9793

Modified:
   trunk/plearn/vmat/MixtureVMatrix.cc
Log:
removed useless warning.


Modified: trunk/plearn/vmat/MixtureVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MixtureVMatrix.cc	2008-12-17 22:11:50 UTC (rev 9792)
+++ trunk/plearn/vmat/MixtureVMatrix.cc	2008-12-18 16:38:36 UTC (rev 9793)
@@ -180,7 +180,7 @@
     for (int i=0; i<n_sources; i++)
     {
         PLASSERT(period.count(i) == weights[i]);
-        PLASSERT( i==0 && occurrences[0]==0
+        PLASSERT( (i==0 && occurrences[0]==0)
                   || period.subVec(0,i-1).count(period[i]) == occurrences[i]);
     }
 #endif



From nouiz at mail.berlios.de  Thu Dec 18 20:02:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 20:02:06 +0100
Subject: [Plearn-commits] r9794 - trunk/commands
Message-ID: <200812181902.mBIJ26B1024876@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 20:02:05 +0100 (Thu, 18 Dec 2008)
New Revision: 9794

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
add an include.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2008-12-18 16:38:36 UTC (rev 9793)
+++ trunk/commands/plearn_noblas_inc.h	2008-12-18 19:02:05 UTC (rev 9794)
@@ -351,6 +351,7 @@
 #include <plearn/vmat/RandomSamplesVMatrix.h>
 #include <plearn/vmat/RandomSamplesFromVMatrix.h>
 #include <plearn/vmat/RankedVMatrix.h>
+#include <plearn/vmat/RepeatVMatrix.h>
 #include <plearn/vmat/RegularGridVMatrix.h>
 #include <plearn/vmat/RemoveDuplicateVMatrix.h>
 #include <plearn/vmat/ReorderByMissingVMatrix.h>



From nouiz at mail.berlios.de  Thu Dec 18 20:04:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 20:04:32 +0100
Subject: [Plearn-commits] r9795 - trunk/examples/data/test_suite
Message-ID: <200812181904.mBIJ4Wtu025107@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 20:04:32 +0100 (Thu, 18 Dec 2008)
New Revision: 9795

Added:
   trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat
   trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat
Log:
added train/test version of one dataset in the test_suite


Added: trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat
===================================================================
--- trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat	2008-12-18 19:02:05 UTC (rev 9794)
+++ trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat	2008-12-18 19:04:32 UTC (rev 9795)
@@ -0,0 +1 @@
+SelectColumnsVMatrix(source=SubVMatrix(istart=100,source=AutoVMatrix(filename="linear_4x_2y_multi_class.vmat")),indices=[0 1 2 3 5])
\ No newline at end of file

Added: trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat
===================================================================
--- trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat	2008-12-18 19:02:05 UTC (rev 9794)
+++ trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat	2008-12-18 19:04:32 UTC (rev 9795)
@@ -0,0 +1 @@
+SelectColumnsVMatrix(source=SubVMatrix(length=100,source=AutoVMatrix(filename="linear_4x_2y_multi_class.vmat")),indices=[0 1 2 3 5])
\ No newline at end of file



From nouiz at mail.berlios.de  Thu Dec 18 20:06:53 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 20:06:53 +0100
Subject: [Plearn-commits] r9796 - trunk/examples/data/test_suite
Message-ID: <200812181906.mBIJ6rXq025395@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 20:06:53 +0100 (Thu, 18 Dec 2008)
New Revision: 9796

Modified:
   trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat
   trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat
Log:
put the full path to the file.


Modified: trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat
===================================================================
--- trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat	2008-12-18 19:04:32 UTC (rev 9795)
+++ trunk/examples/data/test_suite/linear_4x_2y_multi_class_test.vmat	2008-12-18 19:06:53 UTC (rev 9796)
@@ -1 +1 @@
-SelectColumnsVMatrix(source=SubVMatrix(istart=100,source=AutoVMatrix(filename="linear_4x_2y_multi_class.vmat")),indices=[0 1 2 3 5])
\ No newline at end of file
+SelectColumnsVMatrix(source=SubVMatrix(istart=100,source=AutoVMatrix(filename="PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat")),indices=[0 1 2 3 5])

Modified: trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat
===================================================================
--- trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat	2008-12-18 19:04:32 UTC (rev 9795)
+++ trunk/examples/data/test_suite/linear_4x_2y_multi_class_train.vmat	2008-12-18 19:06:53 UTC (rev 9796)
@@ -1 +1 @@
-SelectColumnsVMatrix(source=SubVMatrix(length=100,source=AutoVMatrix(filename="linear_4x_2y_multi_class.vmat")),indices=[0 1 2 3 5])
\ No newline at end of file
+SelectColumnsVMatrix(source=SubVMatrix(length=100,source=AutoVMatrix(filename="PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class.vmat")),indices=[0 1 2 3 5])



From nouiz at mail.berlios.de  Thu Dec 18 20:37:44 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 20:37:44 +0100
Subject: [Plearn-commits] r9797 - trunk/python_modules/plearn/pytest
Message-ID: <200812181937.mBIJbiV2029614@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 20:37:44 +0100 (Thu, 18 Dec 2008)
New Revision: 9797

Modified:
   trunk/python_modules/plearn/pytest/tests.py
Log:
added a small check that would have helped me save time.


Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2008-12-18 19:06:53 UTC (rev 9796)
+++ trunk/python_modules/plearn/pytest/tests.py	2008-12-18 19:37:44 UTC (rev 9797)
@@ -222,6 +222,9 @@
     def link_resources(self, path_to, resources, target_dir): 
         resources_to_append = []
         for resource in resources:
+            if not resource:
+                print "Empty ressource to link to the target_dir '%s', we skip it."%target_dir
+                continue
             self.single_link( path_to, resource, target_dir )
                         
             if toolkit.isvmat( resource ):



From nouiz at mail.berlios.de  Thu Dec 18 21:39:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 21:39:57 +0100
Subject: [Plearn-commits] r9798 - trunk/plearn_learners/regressors
Message-ID: <200812182039.mBIKdv2C004485@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 21:39:57 +0100 (Thu, 18 Dec 2008)
New Revision: 9798

Modified:
   trunk/plearn_learners/regressors/RegressionTreeQueue.cc
Log:
better error msg.


Modified: trunk/plearn_learners/regressors/RegressionTreeQueue.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-12-18 19:37:44 UTC (rev 9797)
+++ trunk/plearn_learners/regressors/RegressionTreeQueue.cc	2008-12-18 20:39:57 UTC (rev 9798)
@@ -41,6 +41,7 @@
 
 #include "RegressionTreeQueue.h"
 #include "RegressionTreeNode.h"
+#include <plearn/base/tostring.h>
 
 namespace PLearn {
 using namespace std;
@@ -110,7 +111,11 @@
     {
         return;
     }
-    if (next_available_node >= maximum_number_of_nodes) PLERROR("RegressionTreeQueue: maximum number of entries exceeded (400)");
+    if (next_available_node >= maximum_number_of_nodes){
+        string s = "RegressionTreeQueue: maximum number of entries exceeded ("
+            +tostring(maximum_number_of_nodes)+")";
+        PLERROR(s.c_str());
+    }
     nodes[next_available_node] = upHeap(new_node, next_available_node);
     next_available_node += 1;
 }



From saintmlx at mail.berlios.de  Thu Dec 18 22:37:38 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 18 Dec 2008 22:37:38 +0100
Subject: [Plearn-commits] r9799 - trunk/plearn/python
Message-ID: <200812182137.mBILbcph008102@sheep.berlios.de>

Author: saintmlx
Date: 2008-12-18 22:37:38 +0100 (Thu, 18 Dec 2008)
New Revision: 9799

Modified:
   trunk/plearn/python/PythonExtension.cc
Log:
- debug logs
- remote function to redirect pout to perr



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-12-18 20:39:57 UTC (rev 9798)
+++ trunk/plearn/python/PythonExtension.cc	2008-12-18 21:37:38 UTC (rev 9799)
@@ -31,6 +31,8 @@
 // This file is part of the PLearn library. For more information on the PLearn
 // library, go to the PLearn Web site at www.plearn.org
 
+#define PL_LOG_MODULE_NAME "PythonExtension"
+
 #include <plearn/python/PythonExtension.h>
 #include <plearn/python/PythonObjectWrapper.h>
 #include <plearn/base/RemoteDeclareMethod.h>
@@ -39,12 +41,16 @@
 #include <plearn/base/PMemPool.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/vmat/VMatrix.h>
+#include <plearn/io/pl_log.h>
+#include <plearn/sys/procinfo.h>
 
 namespace PLearn {
 
 // Trampoline for global PLearn 'remote' functions
 PyObject* pythonGlobalFuncTramp(PyObject* self, PyObject* args)
 {
+    DBG_MODULE_LOG << "pythonGlobalFuncTramp(" << PythonObjectWrapper(self)
+                   << ", " << PythonObjectWrapper(args) << ')' << endl;
     RemoteTrampoline* t= 
         static_cast<RemoteTrampoline*>(PyCObject_AsVoidPtr(self));
     int nas= PyTuple_GET_SIZE(args);
@@ -268,6 +274,7 @@
             + pyclassname + "(" + actual_wrapper_name + "):\n"
             "  \"\"\" \n" + class_help_text + "\n \"\"\"\n"
             "  def __new__(cls,*args,**kwargs):\n"
+            "    #get_plearn_module().loggingControl(500, ['__ALL__'])"
             "    #print '** "+pyclassname+".__new__',kwargs\n"
             "    obj= object.__new__(cls,*args,**kwargs)\n"
             "    if '_cptr' not in kwargs:\n"
@@ -388,6 +395,11 @@
 
 void createWrappedObjectsSet(PyObject* module)
 {
+    /* can't set logging before this gets called
+    perr << "[pid=" << getPid() << "] "
+         << "createWrappedObjectsSet for module: " << PythonObjectWrapper(module) << endl;
+    */
+
     string code= "";
 #if PL_PYTHON_VERSION <= 230
     code+= "\nfrom sets import Set as set\n";
@@ -405,6 +417,8 @@
 
 void addToWrappedObjectsSet(PyObject* o)
 {
+    DBG_MODULE_LOG << "addToWrappedObjectsSet for module: " << PythonObjectWrapper(the_PLearn_python_module)
+                   << "\tadding object: " << PythonObjectWrapper(o) << endl;
     PLASSERT(the_PLearn_python_module);
     if(-1 == PyObject_SetAttrString(the_PLearn_python_module, const_cast<char*>("_tmp_wrapped_instance"), o))
         PLERROR("in addToWrappedObjectsSet : cannot add wrapped object to module.");
@@ -422,6 +436,8 @@
 }
 void removeFromWrappedObjectsSet(PyObject* o)
 {
+    DBG_MODULE_LOG << "removeFromWrappedObjectsSet for module: " << PythonObjectWrapper(the_PLearn_python_module)
+                   << "\tremoving object: " << PythonObjectWrapper(o) << endl;
     PLASSERT(the_PLearn_python_module);
     if(-1 == PyObject_SetAttrString(the_PLearn_python_module, const_cast<char*>("_tmp_wrapped_instance"), o))
         PLERROR("in removeFromWrappedObjectsSet : cannot add wrapped object to module.");
@@ -442,6 +458,10 @@
 // init module, then inject global funcs
 void initPythonExtensionModule(char const * module_name)
 {
+    /* can't set logging before this gets called
+    perr << "[pid=" << getPid() << "] "
+         << "initPythonExtensionModule name=" << module_name << endl;
+    */
     PythonObjectWrapper::initializePython();
     PyObject* plext= Py_InitModule(const_cast<char*>(module_name), NULL);
     setPythonModuleAndInject(plext);
@@ -449,11 +469,14 @@
 
 void setPythonModuleAndInject(PyObject* module)
 {
+    /* can't set logging before this gets called
+    perr << "[pid=" << getPid() << "] "
+         << "setPythonModuleAndInject for module: " << PythonObjectWrapper(module) << "\tat " << (void*)module << endl;
+    */
     injectPLearnGlobalFunctions(module);
     injectPLearnClasses(module);
     createWrappedObjectsSet(module);
-    the_PLearn_python_module= module;
-    
+    the_PLearn_python_module= module;   
 }
 
 PyObject* the_PLearn_python_module= 0;
@@ -465,12 +488,19 @@
 {
     pout= pnull;
 }
+void setPoutToPerr()
+{
+    pout= perr;
+}
 
 BEGIN_DECLARE_REMOTE_FUNCTIONS
 
     declareFunction("setNullPout", &setNullPout,
                     (BodyDoc("Sets the pout output stream to be null.\n")));
 
+    declareFunction("setPoutToPerr", &setPoutToPerr,
+                    (BodyDoc("Sets the pout output stream to be perr.\n")));
+
 END_DECLARE_REMOTE_FUNCTIONS
         
 



From nouiz at mail.berlios.de  Thu Dec 18 22:42:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 18 Dec 2008 22:42:45 +0100
Subject: [Plearn-commits] r9800 - in trunk: . speedtest2 speedtest2/.pytest
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata
 speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/.plearn
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Spe!
 ed/expected_results
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata
 speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata
 speedtest2/.pytest/PL_RegressionTree_Reg_Speed
 speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results
 speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp
 speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0
 speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir
 speedtest! 2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Spl!
 it0/Lear
Message-ID: <200812182142.mBILgjfx008407@sheep.berlios.de>

Author: nouiz
Date: 2008-12-18 22:42:43 +0100 (Thu, 18 Dec 2008)
New Revision: 9800

Added:
   trunk/speedtest2/
   trunk/speedtest2/.pytest/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/RUN.log
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/test_cost_names.txt
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/train_cost_names.txt
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/.plearn/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/.plearn/ppath.config
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/RUN.log
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/test_cost_names.txt
   trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/train_cost_names.txt
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/RUN.log
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/sizes
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/test_cost_names.txt
   trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/train_cost_names.txt
   trunk/speedtest2/pytest.config
   trunk/speedtest2/statnames.txt
   trunk/speedtest2/test_tree.plearn
   trunk/speedtest2/test_tree_multiclass_leave.plearn
   trunk/speedtest2/test_tree_multiclass_output.plearn
   trunk/speedtest2/test_tree_reg.plearn
   trunk/speedtest2/train.pymat
Log:
Added some test that are used to make speed test. They are not enable by default. To execute them use pytest --category=Speed --showtime.



Property changes on: trunk/speedtest2/.pytest
___________________________________________________________________
Name: svn:ignore
   + *.compilation_log



Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/RUN.log
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/RUN.log	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/RUN.log	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,100 @@
+expdir=exp
+In HyperOptimize::optimize() - We optimize with parameters nstages=1
+split_cols: []
+
+split_values: []
+
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=1 Current value=0.709999999999999964 Best value= 0.709999999999999964
+In HyperOptimize::optimize() - We optimize with parameters nstages=2
+split_cols: 2 
+split_values: 0.107821245374841557 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=2 Current value=0.5 Best value= 0.5
+In HyperOptimize::optimize() - We optimize with parameters nstages=3
+split_cols: 2 2 
+split_values: 0.107821245374841557 0.999815364796868278 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=3 Current value=0.330000000000000016 Best value= 0.330000000000000016
+In HyperOptimize::optimize() - We optimize with parameters nstages=4
+split_cols: 2 2 2 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=4 Current value=0.280000000000000027 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=5
+split_cols: 2 2 2 1 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=5 Current value=0.280000000000000027 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=6
+split_cols: 2 2 2 1 1 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=6 Current value=0.309999999999999998 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=7
+split_cols: 2 2 2 1 1 1 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=7 Current value=0.280000000000000027 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=8
+split_cols: 2 2 2 1 1 1 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=8 Current value=0.340000000000000024 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=9
+split_cols: 2 2 2 1 1 1 3 1 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=9 Current value=0.340000000000000024 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=10
+split_cols: 2 2 2 1 1 1 3 1 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=10 Current value=0.340000000000000024 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=11
+split_cols: 2 2 2 1 1 1 3 1 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=11 Current value=0.340000000000000024 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=12
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=12 Current value=0.359999999999999987 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=13
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=13 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=14
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=14 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=15
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=15 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=16
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=16 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=17
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=17 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=18
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=18 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=19
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=19 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=20
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=20 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=21
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=21 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=22
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=22 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=23
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=23 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=24
+split_cols: 2 2 2 1 1 1 3 1 3 3 3 3 
+split_values: 0.107821245374841557 0.999815364796868278 1.01787407170983446e-05 0.336269621279988551 0.6805672568090122 0.712600088500864981 0.341840952415575328 0.677356639452356912 0.0677860488311545217 0.938795280896405959 0.525998522889180808 0.497062039230372399 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=24 Current value=0.380000000000000004 Best value= 0.280000000000000027
+expdir=exp

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,10 @@
+_trial_	0
+_objective_	0
+nstages	0
+loss_function_weight	0
+E[test1.E[mse]]	0
+E[test2.E[mse]]	0
+E[test2.STDERROR[mse]]	0
+E[test1.E[class_error]]	0
+E[test2.E[class_error]]	0
+E[test2.STDERROR[class_error]]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,6 @@
+E[test1.E[mse]]	0
+E[test2.E[mse]]	0
+E[test2.STDERROR[mse]]	0
+E[test1.E[class_error]]	0
+E[test2.E[class_error]]	0
+E[test2.STDERROR[class_error]]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/global_stats.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,7 @@
+splitnum	0
+test1.E[mse]	0
+test2.E[mse]	0
+test2.STDERROR[mse]	0
+test1.E[class_error]	0
+test2.E[class_error]	0
+test2.STDERROR[class_error]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/split_stats.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/test_cost_names.txt
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/test_cost_names.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/test_cost_names.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,9 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1
+class_error
+SPLIT_VAR_x1
+SPLIT_VAR_x2
+SPLIT_VAR_x3
+SPLIT_VAR_x4

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/train_cost_names.txt
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/train_cost_names.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassLeave_Speed/expected_results/exp/train_cost_names.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,6 @@
+E[test1.E[mse]]
+E[test2.E[mse]]
+E[test2.STDERROR[mse]]
+E[test1.E[class_error]]
+E[test2.E[class_error]]
+E[test2.STDERROR[class_error]]

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/.plearn/ppath.config
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/.plearn/ppath.config	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/.plearn/ppath.config	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,8 @@
+HOME                                              "${HOME}"
+PLEARNDIR                                         "HOME:PLearn"
+PLEARN_LIBDIR                                     "PLEARNDIR:external_libs"
+DBDIR                                             "/u/lisa/Database"
+UCI_MLDB_REP                                      "DBDIR:UCI_MLDB"
+UCI_MLDB                                          "PLEARNDIR:examples/data/uci_mldb"
+CGI_ROOT                                          "/home/fringant2/lisa/chaire_finance/exp_2006/chaire/exp_v2"
+PYTEST__PL_RegressionTree_MulticlassOutput_Speed__RESULTS "HOME:PLearn/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results"

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/RUN.log
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/RUN.log	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/RUN.log	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,100 @@
+expdir=exp
+In HyperOptimize::optimize() - We optimize with parameters nstages=1
+split_cols: []
+
+split_values: []
+
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=1 Current value=0.709999999999999964 Best value= 0.709999999999999964
+In HyperOptimize::optimize() - We optimize with parameters nstages=2
+split_cols: 2 
+split_values: 0.0764507821937972787 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=2 Current value=0.510000000000000009 Best value= 0.510000000000000009
+In HyperOptimize::optimize() - We optimize with parameters nstages=3
+split_cols: 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=3 Current value=0.359999999999999987 Best value= 0.359999999999999987
+In HyperOptimize::optimize() - We optimize with parameters nstages=4
+split_cols: 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=4 Current value=0.309999999999999998 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=5
+split_cols: 2 2 2 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=5 Current value=0.340000000000000024 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=6
+split_cols: 2 2 2 1 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=6 Current value=0.340000000000000024 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=7
+split_cols: 2 2 2 1 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=7 Current value=0.340000000000000024 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=8
+split_cols: 2 2 2 1 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=8 Current value=0.340000000000000024 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=9
+split_cols: 2 2 2 1 2 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=9 Current value=0.340000000000000024 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=10
+split_cols: 2 2 2 1 2 2 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=10 Current value=0.330000000000000016 Best value= 0.309999999999999998
+In HyperOptimize::optimize() - We optimize with parameters nstages=11
+split_cols: 2 2 2 1 2 2 2 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=11 Current value=0.280000000000000027 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=12
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=12 Current value=0.280000000000000027 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=13
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=13 Current value=0.340000000000000024 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=14
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=14 Current value=0.340000000000000024 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=15
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=15 Current value=0.359999999999999987 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=16
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=16 Current value=0.359999999999999987 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=17
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=17 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=18
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=18 Current value=0.380000000000000004 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=19
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=19 Current value=0.390000000000000013 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=20
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=20 Current value=0.390000000000000013 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=21
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=21 Current value=0.390000000000000013 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=22
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 0.594815713007653457 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=22 Current value=0.400000000000000022 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=23
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 0.594815713007653457 0.48305890621271752 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=23 Current value=0.409999999999999976 Best value= 0.280000000000000027
+In HyperOptimize::optimize() - We optimize with parameters nstages=24
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 3 3 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 0.594815713007653457 0.48305890621271752 0.000102092522948626918 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=24 Current value=0.409999999999999976 Best value= 0.280000000000000027
+expdir=exp

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,10 @@
+_trial_	0
+_objective_	0
+nstages	0
+loss_function_weight	0
+E[test1.E[mse]]	0
+E[test2.E[mse]]	0
+E[test2.STDERROR[mse]]	0
+E[test1.E[class_error]]	0
+E[test2.E[class_error]]	0
+E[test2.STDERROR[class_error]]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,6 @@
+E[test1.E[mse]]	0
+E[test2.E[mse]]	0
+E[test2.STDERROR[mse]]	0
+E[test1.E[class_error]]	0
+E[test2.E[class_error]]	0
+E[test2.STDERROR[class_error]]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/global_stats.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,7 @@
+splitnum	0
+test1.E[mse]	0
+test2.E[mse]	0
+test2.STDERROR[mse]	0
+test1.E[class_error]	0
+test2.E[class_error]	0
+test2.STDERROR[class_error]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/split_stats.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/test_cost_names.txt
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/test_cost_names.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/test_cost_names.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,9 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1
+class_error
+SPLIT_VAR_x1
+SPLIT_VAR_x2
+SPLIT_VAR_x3
+SPLIT_VAR_x4

Added: trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/train_cost_names.txt
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/train_cost_names.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_MulticlassOutput_Speed/expected_results/exp/train_cost_names.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,6 @@
+E[test1.E[mse]]
+E[test2.E[mse]]
+E[test2.STDERROR[mse]]
+E[test1.E[class_error]]
+E[test2.E[class_error]]
+E[test2.STDERROR[class_error]]


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results
PSAVEDIFF


Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/RUN.log
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/RUN.log	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/RUN.log	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,100 @@
+expdir=exp
+In HyperOptimize::optimize() - We optimize with parameters nstages=1
+split_cols: []
+
+split_values: []
+
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=1 Current value=1 Best value= 1
+In HyperOptimize::optimize() - We optimize with parameters nstages=2
+split_cols: 2 
+split_values: 0.0764507821937972787 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=2 Current value=1 Best value= 1
+In HyperOptimize::optimize() - We optimize with parameters nstages=3
+split_cols: 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=3 Current value=1 Best value= 1
+In HyperOptimize::optimize() - We optimize with parameters nstages=4
+split_cols: 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=4 Current value=1 Best value= 1
+In HyperOptimize::optimize() - We optimize with parameters nstages=5
+split_cols: 2 2 2 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=5 Current value=0.849999999999999978 Best value= 0.849999999999999978
+In HyperOptimize::optimize() - We optimize with parameters nstages=6
+split_cols: 2 2 2 1 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=6 Current value=0.849999999999999978 Best value= 0.849999999999999978
+In HyperOptimize::optimize() - We optimize with parameters nstages=7
+split_cols: 2 2 2 1 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=7 Current value=0.839999999999999969 Best value= 0.839999999999999969
+In HyperOptimize::optimize() - We optimize with parameters nstages=8
+split_cols: 2 2 2 1 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=8 Current value=0.770000000000000018 Best value= 0.770000000000000018
+In HyperOptimize::optimize() - We optimize with parameters nstages=9
+split_cols: 2 2 2 1 2 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=9 Current value=0.839999999999999969 Best value= 0.770000000000000018
+In HyperOptimize::optimize() - We optimize with parameters nstages=10
+split_cols: 2 2 2 1 2 2 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=10 Current value=0.810000000000000053 Best value= 0.770000000000000018
+In HyperOptimize::optimize() - We optimize with parameters nstages=11
+split_cols: 2 2 2 1 2 2 2 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=11 Current value=0.810000000000000053 Best value= 0.770000000000000018
+In HyperOptimize::optimize() - We optimize with parameters nstages=12
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=12 Current value=0.810000000000000053 Best value= 0.770000000000000018
+In HyperOptimize::optimize() - We optimize with parameters nstages=13
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=13 Current value=0.800000000000000044 Best value= 0.770000000000000018
+In HyperOptimize::optimize() - We optimize with parameters nstages=14
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=14 Current value=0.739999999999999991 Best value= 0.739999999999999991
+In HyperOptimize::optimize() - We optimize with parameters nstages=15
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=15 Current value=0.719999999999999973 Best value= 0.719999999999999973
+In HyperOptimize::optimize() - We optimize with parameters nstages=16
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=16 Current value=0.719999999999999973 Best value= 0.719999999999999973
+In HyperOptimize::optimize() - We optimize with parameters nstages=17
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=17 Current value=0.729999999999999982 Best value= 0.719999999999999973
+In HyperOptimize::optimize() - We optimize with parameters nstages=18
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=18 Current value=0.719999999999999973 Best value= 0.719999999999999973
+In HyperOptimize::optimize() - We optimize with parameters nstages=19
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=19 Current value=0.689999999999999947 Best value= 0.689999999999999947
+In HyperOptimize::optimize() - We optimize with parameters nstages=20
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=20 Current value=0.619999999999999996 Best value= 0.619999999999999996
+In HyperOptimize::optimize() - We optimize with parameters nstages=21
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=21 Current value=0.619999999999999996 Best value= 0.619999999999999996
+In HyperOptimize::optimize() - We optimize with parameters nstages=22
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 0.594815713007653457 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=22 Current value=0.599999999999999978 Best value= 0.599999999999999978
+In HyperOptimize::optimize() - We optimize with parameters nstages=23
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 3 3 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 0.594815713007653457 0.48305890621271752 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=23 Current value=0.599999999999999978 Best value= 0.599999999999999978
+In HyperOptimize::optimize() - We optimize with parameters nstages=24
+split_cols: 2 2 2 1 2 2 2 2 2 2 1 1 3 3 3 1 2 2 2 3 3 3 2 
+split_values: 0.0764507821937972787 0.995245802370935517 1.01787407170983446e-05 0.749620878600561813 0.00124271272713896708 0.000981625552665510437 0.885239426681956321 0.995010648391392527 0.998050581189888319 0.999815364796868278 0.336269621279988551 0.637401879834719631 0.220280532324405781 0.116462646341546128 0.761912243256090083 0.677356639452356912 0.474960290052197642 0.138734617593002846 0.994230029896215006 0.774431772219866144 0.594815713007653457 0.48305890621271752 0.000102092522948626918 
+In HyperOptimize::optimize() - cost=E[test2.E[class_error]] nb of trials=24 Current value=0.520000000000000018 Best value= 0.520000000000000018
+expdir=exp

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,10 @@
+_trial_	0
+_objective_	0
+nstages	0
+loss_function_weight	0
+E[test1.E[mse]]	0
+E[test2.E[mse]]	0
+E[test2.STDERROR[mse]]	0
+E[test1.E[class_error]]	0
+E[test2.E[class_error]]	0
+E[test2.STDERROR[class_error]]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/Split0/LearnerExpdir/Strat0results.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,6 @@
+E[test1.E[mse]]	0
+E[test2.E[mse]]	0
+E[test2.STDERROR[mse]]	0
+E[test1.E[class_error]]	0
+E[test2.E[class_error]]	0
+E[test2.STDERROR[class_error]]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/global_stats.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat
===================================================================
(Binary files differ)


Property changes on: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat
___________________________________________________________________
Name: svn:mime-type
   + application/octet-stream

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/fieldnames	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,7 @@
+splitnum	0
+test1.E[mse]	0
+test2.E[mse]	0
+test2.STDERROR[mse]	0
+test1.E[class_error]	0
+test2.E[class_error]	0
+test2.STDERROR[class_error]	0

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/sizes
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/sizes	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/split_stats.pmat.metadata/sizes	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1 @@
+-1 -1 -1 0 

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/test_cost_names.txt
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/test_cost_names.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/test_cost_names.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,9 @@
+mse
+base_confidence
+base_reward_l2
+base_reward_l1
+class_error
+SPLIT_VAR_x1
+SPLIT_VAR_x2
+SPLIT_VAR_x3
+SPLIT_VAR_x4

Added: trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/train_cost_names.txt
===================================================================
--- trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/train_cost_names.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/.pytest/PL_RegressionTree_Reg_Speed/expected_results/exp/train_cost_names.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,6 @@
+E[test1.E[mse]]
+E[test2.E[mse]]
+E[test2.STDERROR[mse]]
+E[test1.E[class_error]]
+E[test2.E[class_error]]
+E[test2.STDERROR[class_error]]

Added: trunk/speedtest2/pytest.config
===================================================================
--- trunk/speedtest2/pytest.config	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/pytest.config	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,127 @@
+"""Pytest config file.
+
+Test is a class regrouping the elements that define a test for PyTest.
+    
+    For each Test instance you declare in a config file, a test will be ran
+    by PyTest.
+    
+    @ivar name: The name of the Test must uniquely determine the
+    test. Among others, it will be used to identify the test's results
+    (.PyTest/I{name}/*_results/) and to report test informations.
+    @type name: String
+    
+    @ivar description: The description must provide other users an
+    insight of what exactly is the Test testing. You are encouraged
+    to used triple quoted strings for indented multi-lines
+    descriptions.
+    @type description: String
+    
+    @ivar category: The category to which this test belongs. By default, a
+    test is considered a 'General' test.
+    
+    It is not desirable to let an extensive and lengthy test as 'General',
+    while one shall refrain abusive use of categories since it is likely
+    that only 'General' tests will be ran before most commits...
+    
+    @type category: string
+    
+    @ivar program: The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner::
+    
+    1) Look for a local program named PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named PRGNAME
+    3) Call 'which PRGNAME'
+    4) Fail
+    
+    Compilable program should provide the keyword argument 'compiler'
+    mapping to a string interpreted as the compiler name (e.g.
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument.  @type program:
+    L{Program}
+    
+    @ivar arguments: The command line arguments to be passed to the program
+    for the test to proceed.
+    @type arguments: String
+    
+    @ivar resources: A list of resources that are used by your program
+    either in the command line or directly in the code (plearn or pyplearn
+    files, databases, ...).  The elements of the list must be string
+    representations of the path, absolute or relative, to the resource.
+    @type resources: List of Strings
+    
+    @ivar precision: The precision (absolute and relative) used when comparing
+    floating numbers in the test output (default = 1e-6)
+    @type precision: float
+    
+    @ivar pfileprg: The program to be used for comparing files of psave &
+    vmat formats. It can be either::
+    - "__program__": maps to this test's program if its compilable;
+    maps to 'plearn_tests' otherwise (default);
+    - "__plearn__": always maps to 'plearn_tests' (for when the program
+    under test is not a version of PLearn);
+    - A Program (see 'program' option) instance
+    - None: if you are sure no files are to be compared.
+    
+    @ivar ignored_files_re: Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+    @type ignored_files_re: list of regular expressions
+    
+    @ivar disabled: If true, the test will not be ran.
+    @type disabled: bool
+    
+"""
+Test(
+    name = "PL_RegressionTree_MulticlassLeave_Speed",
+    description = "A test for the speed of RegressionTree With RegressionTreeMulticlassLeave",
+    category = "Speed",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "test_tree_multiclass_leave.plearn train=PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class_train.vmat test=PLEARNDIR:examples/data/test_suite/linear_4x_\
+2y_multi_class_test.vmat missing_is_valid=0 dt=0 n=25 epoch=1 ind=True",
+    resources = [ "test_tree_multiclass_leave.plearn", "test_tree.plearn"],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+Test(
+    name = "PL_RegressionTree_MulticlassOutput_Speed",
+    description = "A test for the speed of RegressionTree With RegressionTreeLeave with Output casted.",
+    category = "Speed",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "test_tree_multiclass_output.plearn train=PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class_train.vmat test=PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class_test.vmat missing_is_valid=0 dt=0 n=25 epoch=1 ind=True",
+    resources = [ "test_tree_multiclass_output.plearn", "test_tree.plearn"],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )
+
+
+Test(
+    name = "PL_RegressionTree_Reg_Speed",
+    description = "A test for the speed of RegressionTree With RegressionTreeLeave.",
+    category = "Speed",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "test_tree_reg.plearn train=PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class_train.vmat test=PLEARNDIR:examples/data/test_suite/linear_4x_2y_multi_class_test.vmat missing_is_valid=0 dt=0 n=25 epoch=1 ind=True",
+    resources = [ "test_tree_reg.plearn", "test_tree.plearn"],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False,
+    runtime = None,
+    difftime = None
+    )

Added: trunk/speedtest2/statnames.txt
===================================================================
--- trunk/speedtest2/statnames.txt	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/statnames.txt	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,662 @@
+#"E[train.E[mse]]" "E[train.E[class_error]]" "E[train.E[linear_class_error]]" "E[train.E[square_class_error]]" "E[test1.E[mse]]" "E[test1.STDERROR[mse]]"
+"E[test1.E[class_error]]" "E[test2.E[class_error]]" "E[test2.STDERROR[class_error]]" "E[test3.E[class_error]]"
+#"E[test1.E[linear_class_error]]" "E[test2.E[linear_class_error]]" "E[test2.STDERROR[linear_class_error]]" "E[test3.E[linear_class_error]]"
+#"E[test1.E[square_class_error]]" "E[test2.E[square_class_error]]" "E[test2.STDERROR[square_class_error]]" "E[test3.E[square_class_error]]"
+#"E[test1.E[train_time]]" #"E[test1.LAST[train_time]]"
+
+#"E[test1.E[confusion_matrix_target0_pred0]]" "E[test1.E[confusion_matrix_target0_pred1]]" "E[test1.E[confusion_matrix_target0_pred2]]"
+#"E[test1.E[confusion_matrix_target1_pred0]]" "E[test1.E[confusion_matrix_target1_pred1]]" "E[test1.E[confusion_matrix_target1_pred2]]"
+#"E[test1.E[confusion_matrix_target2_pred0]]" "E[test1.E[confusion_matrix_target2_pred1]]" "E[test1.E[confusion_matrix_target2_pred2]]"
+#"E[test1.SUM[confusion_matrix_target0_pred0]]" "E[test1.SUM[confusion_matrix_target0_pred1]]" "E[test1.SUM[confusion_matrix_target0_pred2]]"
+#"E[test1.SUM[confusion_matrix_target1_pred0]]" "E[test1.SUM[confusion_matrix_target1_pred1]]" "E[test1.SUM[confusion_matrix_target1_pred2]]"
+#"E[test1.SUM[confusion_matrix_target2_pred0]]" "E[test1.SUM[confusion_matrix_target2_pred1]]" "E[test1.SUM[confusion_matrix_target2_pred2]]"
+#"E[test2.E[confusion_matrix_target0_pred0]]" "E[test2.E[confusion_matrix_target0_pred1]]" "E[test2.E[confusion_matrix_target0_pred2]]"
+#"E[test2.E[confusion_matrix_target1_pred0]]" "E[test2.E[confusion_matrix_target1_pred1]]" "E[test2.E[confusion_matrix_target1_pred2]]"
+#"E[test2.E[confusion_matrix_target2_pred0]]" "E[test2.E[confusion_matrix_target2_pred1]]" "E[test2.E[confusion_matrix_target2_pred2]]"
+#"E[test2.SUM[confusion_matrix_target0_pred0]]" "E[test2.SUM[confusion_matrix_target0_pred1]]" "E[test2.SUM[confusion_matrix_target0_pred2]]"
+#"E[test2.SUM[confusion_matrix_target1_pred0]]" "E[test2.SUM[confusion_matrix_target1_pred1]]" "E[test2.SUM[confusion_matrix_target1_pred2]]"
+#"E[test2.SUM[confusion_matrix_target2_pred0]]" "E[test2.SUM[confusion_matrix_target2_pred1]]" "E[test2.SUM[confusion_matrix_target2_pred2]]"
+#"E[test3.E[confusion_matrix_target0_pred0]]" "E[test3.E[confusion_matrix_target0_pred1]]" "E[test3.E[confusion_matrix_target0_pred2]]"
+#"E[test3.E[confusion_matrix_target1_pred0]]" "E[test3.E[confusion_matrix_target1_pred1]]" "E[test3.E[confusion_matrix_target1_pred2]]"
+#"E[test3.E[confusion_matrix_target2_pred0]]" "E[test3.E[confusion_matrix_target2_pred1]]" "E[test3.E[confusion_matrix_target2_pred2]]"
+#"E[test3.SUM[confusion_matrix_target0_pred0]]" "E[test3.SUM[confusion_matrix_target0_pred1]]" "E[test3.SUM[confusion_matrix_target0_pred2]]"
+#"E[test3.SUM[confusion_matrix_target1_pred0]]" "E[test3.SUM[confusion_matrix_target1_pred1]]" "E[test3.SUM[confusion_matrix_target1_pred2]]"
+#"E[test3.SUM[confusion_matrix_target2_pred0]]" "E[test3.SUM[confusion_matrix_target2_pred1]]" "E[test3.SUM[confusion_matrix_target2_pred2]]"
+
+
+"E[test3.SUM[SPLIT_VAR_EPARGNE_FLUCTUANTE]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEPOT]]"
+"E[test3.SUM[SPLIT_VAR_SECONDAIRE]]"
+"E[test3.SUM[SPLIT_VAR_PAIE]]"
+"E[test3.SUM[SPLIT_VAR_PROPRIO]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC]]"
+"E[test3.SUM[SPLIT_VAR_ASSU_PRET]]"
+"E[test3.SUM[SPLIT_VAR_GrandeVille]]"
+"E[test3.SUM[SPLIT_VAR_LET_GAR]]"
+"E[test3.SUM[SPLIT_VAR_MANU]]"
+"E[test3.SUM[SPLIT_VAR_EXTERNE]]"
+"E[test3.SUM[SPLIT_VAR_ASSO]]"
+"E[test3.SUM[SPLIT_VAR_UTIL_MCR50]]"
+"E[test3.SUM[SPLIT_VAR_TRANSPORT]]"
+"E[test3.SUM[SPLIT_VAR_ALIMENT]]"
+"E[test3.SUM[SPLIT_VAR_SVC_PART]]"
+"E[test3.SUM[SPLIT_VAR_PCT_INST_CR]]"
+"E[test3.SUM[SPLIT_VAR_PCT_AGRI_CR]]"
+"E[test3.SUM[SPLIT_VAR_MULTI]]"
+"E[test3.SUM[SPLIT_VAR_CHQ_RET]]"
+"E[test3.SUM[SPLIT_VAR_NTF]]"
+"E[test3.SUM[SPLIT_VAR_PCT_CPT]]"
+"E[test3.SUM[SPLIT_VAR_SIGNATURE1]]"
+"E[test3.SUM[SPLIT_VAR_SIGNATURE2]]"
+"E[test3.SUM[SPLIT_VAR_PROCURATION]]"
+"E[test3.SUM[SPLIT_VAR_MULTITRANSIT]]"
+"E[test3.SUM[SPLIT_VAR_RCMP]]"
+"E[test3.SUM[SPLIT_VAR_PAR_ACTION]]"
+"E[test3.SUM[SPLIT_VAR_CONDO]]"
+"E[test3.SUM[SPLIT_VAR_TRANSIT]]"
+"E[test3.SUM[SPLIT_VAR_MIN_TRANSIT]]"
+"E[test3.SUM[SPLIT_VAR_MAX_TRANSIT]]"
+"E[test3.SUM[SPLIT_VAR_NUM_PTSC_CAIS200709]]"
+"E[test3.SUM[SPLIT_VAR_NUM_PTSC_CAIS200609]]"
+"E[test3.SUM[SPLIT_VAR_NUM_PTSC_CAIS200512]]"
+"E[test3.SUM[SPLIT_VAR_NUM_PTSC_CAIS200412]]"
+"E[test3.SUM[SPLIT_VAR_NUM_PTSC_CAIS200312]]"
+"E[test3.SUM[SPLIT_VAR_NUM_PTSC_CAIS200212]]"
+"E[test3.SUM[SPLIT_VAR_OUVERTURE]]"
+"E[test3.SUM[SPLIT_VAR_OUVERTURE_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_CATG_FOLI]]"
+"E[test3.SUM[SPLIT_VAR_COD_TYP_MEMB_FOLI]]"
+"E[test3.SUM[SPLIT_VAR_NB_TRANSIT_DIFF]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ACDA200609]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ACDA200512]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ACDA200412]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ACDA200312]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ACDA200212]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE200709]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE200609]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE200512]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE200412]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE200312]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE200212]]"
+"E[test3.SUM[SPLIT_VAR_DETENTION_ES]]"
+"E[test3.SUM[SPLIT_VAR_DETENTION_ET]]"
+"E[test3.SUM[SPLIT_VAR_DETENTION_PR]]"
+"E[test3.SUM[SPLIT_VAR_DETENTION_MC]]"
+"E[test3.SUM[SPLIT_VAR_EOP_MOY]]"
+"E[test3.SUM[SPLIT_VAR_EOP_MOY200609]]"
+"E[test3.SUM[SPLIT_VAR_EOP_MOY200512]]"
+"E[test3.SUM[SPLIT_VAR_EOP_MOY200412]]"
+"E[test3.SUM[SPLIT_VAR_EOP_MOY200312]]"
+"E[test3.SUM[SPLIT_VAR_NBR_CHRT_ANPR_ANCO]]"
+"E[test3.SUM[SPLIT_VAR_NBR_CHRT_ANPR_ANCO200609]]"
+"E[test3.SUM[SPLIT_VAR_NBR_CHRT_ANPR_ANCO200512]]"
+"E[test3.SUM[SPLIT_VAR_NBR_CHRT_ANPR_ANCO200412]]"
+"E[test3.SUM[SPLIT_VAR_NBR_CHRT_ANPR_ANCO200312]]"
+"E[test3.SUM[SPLIT_VAR_NBR_CHRT_ANPR_ANCO0212]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLE_200709]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLE_200609]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLE_200512]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLE_200412]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLE_200312]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLE_200212]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLEPLUS_200709]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLEPLUS_200609]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLEPLUS_200512]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLEPLUS_200412]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLEPLUS_200312]]"
+"E[test3.SUM[SPLIT_VAR_PREVISIBLEPLUS_200212]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL10200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL10200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL10200512]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL15200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL15200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL15200512]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL15P200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL15P200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL15P200512]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL25200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL25200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL25200512]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL25P200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL25P200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL25P200512]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL40200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL40200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL40200512]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL40P200709]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL40P200609]]"
+"E[test3.SUM[SPLIT_VAR_SIMPL40P200512]]"
+"E[test3.SUM[SPLIT_VAR_ES_MOY]]"
+"E[test3.SUM[SPLIT_VAR_ES_MOY200609]]"
+"E[test3.SUM[SPLIT_VAR_ES_MOY200512]]"
+"E[test3.SUM[SPLIT_VAR_ES_MOY200412]]"
+"E[test3.SUM[SPLIT_VAR_ES_MOY200312]]"
+"E[test3.SUM[SPLIT_VAR_ET_MOY]]"
+"E[test3.SUM[SPLIT_VAR_ET_MOY200609]]"
+"E[test3.SUM[SPLIT_VAR_ET_MOY200512]]"
+"E[test3.SUM[SPLIT_VAR_ET_MOY200412]]"
+"E[test3.SUM[SPLIT_VAR_ET_MOY200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_LET_CRE]]"
+"E[test3.SUM[SPLIT_VAR_LET_CRE200609]]"
+"E[test3.SUM[SPLIT_VAR_LET_CRE200512]]"
+"E[test3.SUM[SPLIT_VAR_LET_CRE200412]]"
+"E[test3.SUM[SPLIT_VAR_LET_CRE200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_LET_CRE0212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_LET_GAR]]"
+"E[test3.SUM[SPLIT_VAR_GAR200609]]"
+"E[test3.SUM[SPLIT_VAR_GAR200512]]"
+"E[test3.SUM[SPLIT_VAR_GAR200412]]"
+"E[test3.SUM[SPLIT_VAR_GAR200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_LET_GAR0212]]"
+"E[test3.SUM[SPLIT_VAR_EOP_US]]"
+"E[test3.SUM[SPLIT_VAR_EOP_USA200609]]"
+"E[test3.SUM[SPLIT_VAR_EOP_USA200512]]"
+"E[test3.SUM[SPLIT_VAR_EOP_USA200412]]"
+"E[test3.SUM[SPLIT_VAR_EOP_USA200312]]"
+"E[test3.SUM[SPLIT_VAR_EOP_US0212]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_AUTO200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_AUTO200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_AUTO200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_AUTO200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_AUTO200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_AUTO200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_AUTO200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_AUTO200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_AUTO200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_AUTO200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_AUTO200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_AUTO200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_AUTO200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_AUTO200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_AUTO200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_AUTO200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_AUTO200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_AUTO200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_AUTO200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_AUTO200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_COMP200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_COMP200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_COMP200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_COMP200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_COMP200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_COMP200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_COMP200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_COMP200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_COMP200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_COMP200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_COMP200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_COMP200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_COMP200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_COMP200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_COMP200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_COMP200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_COMP200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_COMP200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_COMP200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_COMP200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIREMENT200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIREMENT200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIREMENT200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIREMENT200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIREMENT200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIREMENT200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIREMENT200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIREMENT200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIREMENT200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIREMENT200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_FIN200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_FIN200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_FIN200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_FIN200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_FIN200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_FIN200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_FIN200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_FIN200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_FIN200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_FIN200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_EPA200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_EPA200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_EPA200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_EPA200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_VIR_EPA200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_EPA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_EPA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_EPA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_EPA200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_VIR_EPA200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_FRAIS_FIN200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_FRAIS_FIN200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_FRAIS_FIN200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_FRAIS_FIN200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_FRAIS_FIN200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_GA200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_GA200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_GA200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_GA200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_DEP_GA200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_GA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_GA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_GA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_GA200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEP_GA200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_GA200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_GA200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_GA200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_GA200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_RET_GA200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_GA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_GA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_GA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_GA200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RET_GA200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_REM_GOUV200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_REM_GOUV200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_REM_GOUV200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_REM_GOUV200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_REM_GOUV200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_REM_GOUV200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_REM_GOUV200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_REM_GOUV200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_REM_GOUV200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_REM_GOUV200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_COMP200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_COMP200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_COMP200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_COMP200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_COMP200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_COMP200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_COMP200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_COMP200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_COMP200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_COMP200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_GA200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_GA200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_GA200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_GA200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_GA200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_GA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_GA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_GA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_GA200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_GA200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_WEB200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_WEB200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_WEB200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_WEB200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_PAIE_FACT_WEB200212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_WEB200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_WEB200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_WEB200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_WEB200312]]"
+"E[test3.SUM[SPLIT_VAR_MNT_PAIE_FACT_WEB200212]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TPV]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TPV200609]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TPV200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_TRX_INTERAC]]"
+"E[test3.SUM[SPLIT_VAR_MNT_TRX_INTERAC200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_TRX_INTERAC200512]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TRX_INTERAC]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TRX_INTERAC200609]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TRX_INTERAC200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_TRX_DIRECT]]"
+"E[test3.SUM[SPLIT_VAR_MNT_TRX_DIRECT200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_TRX_DIRECT200512]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TRX_DIRECT]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TRX_DIRECT200609]]"
+"E[test3.SUM[SPLIT_VAR_NBR_TRX_DIRECT200512]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_PR200609]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_PR200512]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_PR200412]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_PR200312]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_PR200212]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_MC200709]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_MC200609]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_MC200512]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_MC200412]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_MC200312]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_MC200212]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_PR200709]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_PR200609]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_PR200512]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_PR200412]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_PR200312]]"
+"E[test3.SUM[SPLIT_VAR_IND_RETA_PR200212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR0212]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR200609]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR200512]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR200412]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR200312]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR200212]]"
+"E[test3.SUM[SPLIT_VAR_NB_PRET200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_PRET200609]]"
+"E[test3.SUM[SPLIT_VAR_NB_PRET200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_PRET200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_PRET200312]]"
+"E[test3.SUM[SPLIT_VAR_NB_PRET200212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI0212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_AGRI]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_AGRI200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_AGRI200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_AGRI200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_AGRI200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_AGRI0212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_INST]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_INST200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_INST200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_INST200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_INST200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_INST0212]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_MC200609]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_MC200512]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_MC200412]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_MC200312]]"
+"E[test3.SUM[SPLIT_VAR_Ind_ASVI_MC200212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC200212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MOY_MC]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MOY_MC200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MOY_MC200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MOY_MC200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MOY_MC200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MOY_MC200212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_AGRI200709]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_AGRI200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_AGRI200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_AGRI200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_AGRI200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_AGRI200212]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_INST200709]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_INST200609]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_INST200512]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_INST200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_INST200312]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_MC_INST200212]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC200609]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC200512]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC200412]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC200312]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC0212]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_INST]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_INST200609]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_INST200512]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_INST200412]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_INST200312]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_INST0212]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AGRI]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AGRI200609]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AGRI200512]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AGRI200412]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AGRI200312]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AGRI0212]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST_FINAL]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST200609]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST200512]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST200412]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST200312]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST200212]]"
+"E[test3.SUM[SPLIT_VAR_Risque]]"
+"E[test3.SUM[SPLIT_VAR_Risque_MI]]"
+"E[test3.SUM[SPLIT_VAR_NB_CARTES_MSD]]"
+"E[test3.SUM[SPLIT_VAR_COD_IMP_EXP_-1001_-1001]]"
+"E[test3.SUM[SPLIT_VAR_COD_IMP_EXP_-1002_-1002]]"
+"E[test3.SUM[SPLIT_VAR_COD_IMP_EXP_-1003_-1003]]"
+"E[test3.SUM[SPLIT_VAR_COD_IMP_EXP_-1004_-1004]]"
+"E[test3.SUM[SPLIT_VAR_COD_IMP_EXP_MI]]"
+"E[test3.SUM[SPLIT_VAR_NUM_SIC_US_1]]"
+"E[test3.SUM[SPLIT_VAR_COD_REL_AFF]]"
+"E[test3.SUM[SPLIT_VAR_VDM_PLEINEX]]"
+"E[test3.SUM[SPLIT_VAR_DAT_DEBU_BD]]"
+"E[test3.SUM[SPLIT_VAR_DAT_DEBU_BD_MI]]"
+"E[test3.SUM[SPLIT_VAR_DAT_CREA_ENTR]]"
+"E[test3.SUM[SPLIT_VAR_DAT_CREA_ENTR_MI]]"
+"E[test3.SUM[SPLIT_VAR_DAT_DEBU_ACTI_ENTR]]"
+"E[test3.SUM[SPLIT_VAR_DAT_DEBU_ACTI_ENTR_MI]]"
+"E[test3.SUM[SPLIT_VAR_DAT_FIN_ACTI_ENTR]]"
+"E[test3.SUM[SPLIT_VAR_DAT_FIN_ACTI_ENTR_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG_MI]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL_DDE]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL_DDE_MI]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_1_1]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_2_2]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_3_3]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_4_4]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_5_5]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_6_6]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_8_8]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_9_9]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_10_10]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_11_11]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_12_12]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_13_13]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_16_16]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_17_17]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_19_19]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_21_21]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_22_22]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_23_23]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_24_24]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_25_25]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_26_26]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_28_28]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_1_30]]"
+"E[test3.SUM[SPLIT_VAR_TYP_ENTR_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG200609]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG200609_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG200512]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG200512_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG200412]]"
+"E[test3.SUM[SPLIT_VAR_COD_LANG200412_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_MODE_SAIS200609]]"
+"E[test3.SUM[SPLIT_VAR_COD_MODE_SAIS200609_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_MODE_SAIS200512]]"
+"E[test3.SUM[SPLIT_VAR_COD_MODE_SAIS200512_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_MODE_SAIS200412]]"
+"E[test3.SUM[SPLIT_VAR_COD_MODE_SAIS200412_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_STAT_CIE200709]]"
+"E[test3.SUM[SPLIT_VAR_COD_STAT_CIE200709_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_STAT_CIE200609]]"
+"E[test3.SUM[SPLIT_VAR_COD_STAT_CIE200609_MI]]"
+"E[test3.SUM[SPLIT_VAR_COD_STAT_CIE200412]]"
+"E[test3.SUM[SPLIT_VAR_COD_STAT_CIE200412_MI]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL_PAIE]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL_PAIE_MI]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_ACTF200609]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_ACTF200512]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_ACTF200412]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_INCT200709]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_INCT200609]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_INCT200512]]"
+"E[test3.SUM[SPLIT_VAR_NBR_EMPE_INCT200412]]"
+"E[test3.SUM[SPLIT_VAR_NOM_INSN_BANC200609]]"
+"E[test3.SUM[SPLIT_VAR_NOM_INSN_BANC200609_MI]]"
+"E[test3.SUM[SPLIT_VAR_NOM_INSN_BANC200512]]"
+"E[test3.SUM[SPLIT_VAR_NOM_INSN_BANC200512_MI]]"
+"E[test3.SUM[SPLIT_VAR_NOM_INSN_BANC200412]]"
+"E[test3.SUM[SPLIT_VAR_NOM_INSN_BANC200412_MI]]"
+"E[test3.SUM[SPLIT_VAR_RETR_ACD_TPV_SPD]]"
+"E[test3.SUM[SPLIT_VAR_RETR_FIDU]]"
+"E[test3.SUM[SPLIT_VAR_RETR_DSF]]"
+"E[test3.SUM[SPLIT_VAR_RETR_DAG]]"
+"E[test3.SUM[SPLIT_VAR_RETR_VISA]]"
+"E[test3.SUM[SPLIT_VAR_RETR_TOUS]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_SLA]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_MARG_SLA200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_MARG_SLA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_MARG_SLA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_MARG_SLA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_MARG_SLA200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_MARG_SLA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_MARG_SLA200412]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_PR_SLA]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_PRET_SLA200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_PRET_SLA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_PRET_SLA200412]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_SLA]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_PRET_SLA200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_PRET_SLA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_PRET_SLA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_CART_SLA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_CART_SLA200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_CART_SLA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_CART_SLA200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_CART_SLA200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_CART_SLA200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_CART_SLA200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_CART_SLA200412]]"
+"E[test3.SUM[SPLIT_VAR_NB_CARTE200709]]"
+"E[test3.SUM[SPLIT_VAR_NB_CARTE200609]]"
+"E[test3.SUM[SPLIT_VAR_NB_CARTE200512]]"
+"E[test3.SUM[SPLIT_VAR_NB_CARTE200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_LIMI_VAFF200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_LIMI_VAFF200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_LIMI_VAFF200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_AUTS_LIMI_VAFF200412]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_VAFF200709]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_VAFF200609]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_VAFF200512]]"
+"E[test3.SUM[SPLIT_VAR_MNT_SLDE_MOY_VAFF200412]]"
+"E[test3.SUM[SPLIT_VAR_COD_GEST_NPDE]]"
+"E[test3.SUM[SPLIT_VAR_SOLDE_FINP_ES]]"
+"E[test3.SUM[SPLIT_VAR_SOLDE_FINP_ET]]"
+"E[test3.SUM[SPLIT_VAR_SOLDE_FINP_EOP]]"
+"E[test3.SUM[SPLIT_VAR_MNT_FRFX_TOTAL]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE_SANS_FRAIS]]"
+"E[test3.SUM[SPLIT_VAR_AUTRE_ENTENTE]]"
+"E[test3.SUM[SPLIT_VAR_ID_REGT_CMPT_MAJR]]"
+"E[test3.SUM[SPLIT_VAR_CAE]]"
+"E[test3.SUM[SPLIT_VAR_SAE_1100_1199]]"
+"E[test3.SUM[SPLIT_VAR_SAE_2100_2199]]"
+"E[test3.SUM[SPLIT_VAR_SAE_2200_2299]]"
+"E[test3.SUM[SPLIT_VAR_SAE_2300_2399]]"
+"E[test3.SUM[SPLIT_VAR_SAE_3100_3199]]"
+"E[test3.SUM[SPLIT_VAR_SAE_4000_4000]]"
+"E[test3.SUM[SPLIT_VAR_ACDA]]"
+"E[test3.SUM[SPLIT_VAR_ENTENTE]]"
+"E[test3.SUM[SPLIT_VAR_ASSUR_PRET0709]]"
+"E[test3.SUM[SPLIT_VAR_ASSUR_PRET]]"
+"E[test3.SUM[SPLIT_VAR_ASSUR_MCR]]"
+"E[test3.SUM[SPLIT_VAR_ASSUR_MCR0709]]"
+"E[test3.SUM[SPLIT_VAR_EP_MOY]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RETRAIT]]"
+"E[test3.SUM[SPLIT_VAR_MNT_RETRAIT0212]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_1_1]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_2_2]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_3_3]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_4_4]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_5_5]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_6_6]]"
+"E[test3.SUM[SPLIT_VAR_COTE_RISQUE_MI]]"
+"E[test3.SUM[SPLIT_VAR_ENGA_CR0709]]"
+"E[test3.SUM[SPLIT_VAR_ENGA_CR]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_COMM]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_MULT]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_PR_COMM]]"
+"E[test3.SUM[SPLIT_VAR_ENG_COMM]]"
+"E[test3.SUM[SPLIT_VAR_ENG_AGRI]]"
+"E[test3.SUM[SPLIT_VAR_ENG_INST]]"
+"E[test3.SUM[SPLIT_VAR_ENG_MULT]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEPOT0212]]"
+"E[test3.SUM[SPLIT_VAR_RISQUE_MOYEN]]"
+"E[test3.SUM[SPLIT_VAR_RTA_0_9]]"
+"E[test3.SUM[SPLIT_VAR_RTA_10_19]]"
+"E[test3.SUM[SPLIT_VAR_RTA_20_29]]"
+"E[test3.SUM[SPLIT_VAR_RTA_30_39]]"
+"E[test3.SUM[SPLIT_VAR_RTA_40_49]]"
+"E[test3.SUM[SPLIT_VAR_RTA_50_59]]"
+"E[test3.SUM[SPLIT_VAR_RTA_60_69]]"
+"E[test3.SUM[SPLIT_VAR_RTA_70_79]]"
+"E[test3.SUM[SPLIT_VAR_RTA_80_89]]"
+"E[test3.SUM[SPLIT_VAR_RTA_90_99]]"
+"E[test3.SUM[SPLIT_VAR_RTA_100_109]]"
+"E[test3.SUM[SPLIT_VAR_RTA_110_119]]"
+"E[test3.SUM[SPLIT_VAR_RTA_120_129]]"
+"E[test3.SUM[SPLIT_VAR_RTA_130_139]]"
+"E[test3.SUM[SPLIT_VAR_RTA_140_149]]"
+"E[test3.SUM[SPLIT_VAR_RTA_150_159]]"
+"E[test3.SUM[SPLIT_VAR_RTA_160_169]]"
+"E[test3.SUM[SPLIT_VAR_RTA_170_179]]"
+"E[test3.SUM[SPLIT_VAR_RTA_0_9999]]"
+"E[test3.SUM[SPLIT_VAR_RTA_MI]]"
+"E[test3.SUM[SPLIT_VAR_AUTS_MC_AVEC_SLA]]"
+"E[test3.SUM[SPLIT_VAR_SOLD_SANS_MULTI_AVEC_SLA]]"
+"E[test3.SUM[SPLIT_VAR_EMPRUNTEUR]]"
+"E[test3.SUM[SPLIT_VAR_EPARGNANT]]"
+"E[test3.SUM[SPLIT_VAR_LET_GAR0212]]"
+"E[test3.SUM[SPLIT_VAR_HYPO]]"
+"E[test3.SUM[SPLIT_VAR_SLA]]"
+"E[test3.SUM[SPLIT_VAR_PCT_UTIL_MCR0709]]"
+"E[test3.SUM[SPLIT_VAR_PCT_UTIL_PR]]"
+"E[test3.SUM[SPLIT_VAR_PCT_MCR]]"
+"E[test3.SUM[SPLIT_VAR_PCT_PR]]"
+"E[test3.SUM[SPLIT_VAR_NB_ANNEE_OUVERTURE]]"
+"E[test3.SUM[SPLIT_VAR_NTF0212]]"
+"E[test3.SUM[SPLIT_VAR_OUVERT10]]"
+"E[test3.SUM[SPLIT_VAR_OUVERT25]]"
+"E[test3.SUM[SPLIT_VAR_OUVERT50]]"
+"E[test3.SUM[SPLIT_VAR_SUCC]]"
+"E[test3.SUM[SPLIT_VAR_UNIQUE]]"
+"E[test3.SUM[SPLIT_VAR_HQ]]"
+"E[test3.SUM[SPLIT_VAR_FRANCHISE]]"
+"E[test3.SUM[SPLIT_VAR_FILIALE]]"
+"E[test3.SUM[SPLIT_VAR_MANUFACTURIER]]"
+"E[test3.SUM[SPLIT_VAR_FOREIGN_PARENT]]"
+"E[test3.SUM[SPLIT_VAR_FOREIGN_HQ]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL_DB]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL_DB_MI]]"
+"E[test3.SUM[SPLIT_VAR_NB_EMPL]]"
+"E[test3.SUM[SPLIT_VAR_EMPLOYE_ME]]"
+"E[test3.SUM[SPLIT_VAR_LOCATION]]"
+"E[test3.SUM[SPLIT_VAR_MNT_NET]]"
+"E[test3.SUM[SPLIT_VAR_MNT_NET0212]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEPOT2]]"
+"E[test3.SUM[SPLIT_VAR_MNT_DEPOT20212]]"
+"E[test3.SUM[SPLIT_VAR_EPARGNE_FLUCTUANTE0212]]"
+"E[test3.SUM[SPLIT_VAR_MARGE_BEN]]"
+"E[test3.SUM[SPLIT_VAR_US]]"
+"E[test3.SUM[SPLIT_VAR_US0212]]"
+"E[test3.SUM[SPLIT_VAR_ES50K]]"
+"E[test3.SUM[SPLIT_VAR_ET50K]]"
+"E[test3.SUM[SPLIT_VAR_TYPE_FIN_-1000_-1000]]"
+"E[test3.SUM[SPLIT_VAR_TYPE_FIN_-1001_-1001]]"
+"E[test3.SUM[SPLIT_VAR_TYPE_FIN_-1002_-1002]]"
+"E[test3.SUM[SPLIT_VAR_TYPE_FIN_-1003_-1003]]"
+"E[test3.SUM[SPLIT_VAR_LG_SOLD_SANS_MULTI0212]]"
+"E[test3.SUM[SPLIT_VAR_PCT_AGRI_CR0212]]"
+"E[test3.SUM[SPLIT_VAR_PCT_INST_CR0212]]"
+"E[test3.SUM[SPLIT_VAR_CHQ_RET0212]]"
+"E[test3.SUM[SPLIT_VAR_SERVICES]]"
+"E[test3.SUM[SPLIT_VAR_DETAIL]]"
+"E[test3.SUM[SPLIT_VAR_AGRICOLE]]"
+"E[test3.SUM[SPLIT_VAR_MSD]]"
+"E[test3.SUM[SPLIT_VAR_TPV]]"
+"E[test3.SUM[SPLIT_VAR_ASSU_PRET0212]]"
+"E[test3.SUM[SPLIT_VAR_PRINCIPAL]]"
+"E[test3.SUM[SPLIT_VAR_SPLIT0709]]"
+"E[test3.SUM[SPLIT_VAR_PCT_UTIL_MCR]]"
+"E[test3.SUM[SPLIT_VAR_UTIL_MCR500212]]"
+"E[test3.SUM[SPLIT_VAR_PCT_MCR0212]]"
+"E[test3.SUM[SPLIT_VAR_EMPRUNTEUR0212]]"
+"E[test3.SUM[SPLIT_VAR_STABLE]]"
+"E[test3.SUM[SPLIT_VAR_SPLIT_1_1]]"
+"E[test3.SUM[SPLIT_VAR_SPLIT_2_2]]"
+"E[test3.SUM[SPLIT_VAR_SPLIT_3_3]]"
+"E[test3.SUM[SPLIT_VAR_SPLIT_4_4]]"
+"E[test3.SUM[SPLIT_VAR_SPLIT_5_5]]"
+"E[test3.SUM[SPLIT_VAR_NUM_INFI_DESJ]]"
+"E[test3.SUM[SPLIT_VAR_CLASSE_REEL]]"

Added: trunk/speedtest2/test_tree.plearn
===================================================================
--- trunk/speedtest2/test_tree.plearn	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/test_tree.plearn	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,146 @@
+# ARGUMENTS:
+# datadir = prefix of ${datadir}/{train,test}_input.vmat files
+# dt = deletion_threshold
+# outputs = [] for regression [ 0 1 2 ... ] for classification
+# missing_is_valid = 0 if missing are not a valid value, 1 otherwise
+# n = number of online training iterations (examples to see)
+# epoch = number of examples forming an "epoch" (when to save model, compute error)
+
+
+$DEFINE{learner}{test_tree}
+$DEFINE{expdir}{exp}
+
+$EVALUATE{expdir}
+ShellScript(
+  shell_commands = [ 
+		     "echo \"expdir=${expdir}\""
+                   ]
+)
+PTester
+  ( 
+  expdir = "${expdir}"  ;
+
+  splitter = ExplicitSplitter( splitsets = 1 3
+                    [ *1->RepeatVMatrix(repeat_n_times=1000000
+                                              source=AutoVMatrix(
+					               filename = "${train}"
+                                                       targetsize = 1;
+                                                       weightsize = 0;
+                                                       ))
+                      *1
+                                RepeatVMatrix(repeat_n_times=1000000
+                                              source=AutoVMatrix(
+					               filename = "${test}"
+                                                       targetsize = 1;
+                                                       weightsize = 0;
+                                                       ))
+				]
+			      )
+
+  statnames = [
+          "E[test1.E[mse]]" "E[test2.E[mse]]" "E[test2.STDERROR[mse]]"
+          "E[test1.E[class_error]]" "E[test2.E[class_error]]" "E[test2.STDERROR[class_error]]"
+          ] ;
+  report_stats = 1  ;
+  save_initial_tester = 0  ;
+  save_stat_collectors = 0  ;
+  save_learners = 0  ;
+  save_initial_learners = 0  ;
+  save_data_sets = 0  ;
+  save_test_outputs = 0  ;
+  save_test_costs = 0  ;
+  provide_learner_expdir = 1  ;
+  save_test_confidence = 0  ;
+  enforce_clean_expdir = 0
+  learner = HyperLearner(
+                        option_fields = [ "nstages" "loss_function_weight" ];
+                        dont_restart_upon_change = [ "nstages" ] ;
+                        provide_strategy_expdir = 1 ;
+                        save_final_learner = 0 ;
+                        provide_learner_expdir = 1 ;
+                        forget_when_training_set_changes = 0 ;
+                        nstages = 1 ;
+                        nservers = 0 ;
+                        report_progress = 1 ;
+                        verbosity = 0 ;
+                        learner = 
+					RegressionTree(
+							# nstages = 15 ;
+                                                        loss_function_weight = 1 ;
+                                                        # sort_train_set = 1 ;
+                                                        missing_is_valid = ${missing_is_valid} ;
+							multiclass_outputs = ${outputs}
+                                                        maximum_number_of_nodes = 500 ;
+                                                        compute_train_stats = 0 ;
+                                                        complexity_penalty_factor = 0.0 ;
+							output_confidence_target = 0
+                                                        verbosity = 2 ;
+                                                        report_progress = 1 ;
+                                                        forget_when_training_set_changes = 1 ;
+                                                        leave_template = ${leave} ; 
+                                                        test_minibatch_size = 32
+                                                        ) ;  # end of RegressionTree
+                        tester = PTester(
+                                        splitter =  ExplicitSplitter( splitsets = 1 3 
+                    [ *1->RepeatVMatrix(repeat_n_times=1000
+                                              source=AutoVMatrix(
+					               filename = "${train}"
+                                                       targetsize = 1;
+                                                       weightsize = 0;
+                                                       ))
+                       *1
+                                RepeatVMatrix(repeat_n_times=1000
+                                              source=AutoVMatrix(
+					               filename = "${test}"
+                                                       targetsize = 1;
+                                                       weightsize = 0;
+                                                       ))
+				]
+			      )
+
+
+
+
+                                        statnames = [
+                                                    "E[test1.E[mse]]" "E[test2.E[mse]]" "E[test2.STDERROR[mse]]"
+                                                    "E[test1.E[class_error]]" "E[test2.E[class_error]]" "E[test2.STDERROR[class_error]]"
+                                                    ] ;
+                                        save_test_outputs = 0 ;
+                                        report_stats = 1  ;
+                                        save_initial_tester = 0 ;
+                                        save_learners = 0 ;                         
+                                        save_initial_learners = 0  ;
+                                        save_data_sets = 0  ;
+                                        save_test_costs = 0  ;
+                                        provide_learner_expdir = 1  ;
+                                        save_test_confidence = 0  ;
+                                        save_test_names = 0;
+                                        enforce_clean_expdir = 0
+                                        ) ; # end of Hyperlearner PTester
+                        strategy = [
+                                   HyperOptimize(
+                                                which_cost = "E[test2.E[class_error]]" ;
+                                                min_n_trials = 1 ;
+                                                provide_tester_expdir = 0 ;
+                                                rerun_after_sub = 0 ;
+                                                provide_sub_expdir = 0 ;
+						verbosity=3
+                                                oracle = EarlyStoppingOracle(
+                                                                                                        option = "nstages" ;
+                                                                                                        range = [ ${epoch} ${n} ${epoch} ]
+                                                                                                        min_value = -3.40282e+38 ;
+                                                                                                        max_value = 3.40282e+38 ;
+                                                                                                        max_degradation = 3.40282e+38 ;
+                                                                                                        relative_max_degradation = -1 ;
+                                                                                                        min_improvement = -3.40282e+38 ;
+                                                                                                        relative_min_improvement = -1 ;
+                                                                                                        max_degraded_steps = 120 ;
+                                                                                                        min_n_steps = 2 ;
+                                                                                                        ) ; # end of EarlyStoppingOracle
+                                                ) ; # end of strategy.HyperOptimize 
+                                   ] ; # end of HyperLearner strategy
+                        ) ; # end of HyperLearner
+        final_commands = [
+			  "echo \"expdir=${expdir}\""
+                          ]
+  ) # end of main Ptester

Added: trunk/speedtest2/test_tree_multiclass_leave.plearn
===================================================================
--- trunk/speedtest2/test_tree_multiclass_leave.plearn	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/test_tree_multiclass_leave.plearn	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,3 @@
+$DEFINE{outputs}{[]}
+$DEFINE{leave}{RegressionTreeMulticlassLeave(multiclass_outputs=[ 0 1 2 3 ])}
+$INCLUDE{test_tree.plearn}

Added: trunk/speedtest2/test_tree_multiclass_output.plearn
===================================================================
--- trunk/speedtest2/test_tree_multiclass_output.plearn	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/test_tree_multiclass_output.plearn	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,3 @@
+$DEFINE{outputs}{[ 0 1 2 3 ]}
+$DEFINE{leave}{RegressionTreeLeave()}
+$INCLUDE{test_tree.plearn}

Added: trunk/speedtest2/test_tree_reg.plearn
===================================================================
--- trunk/speedtest2/test_tree_reg.plearn	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/test_tree_reg.plearn	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,3 @@
+$DEFINE{outputs}{[]}
+$DEFINE{leave}{RegressionTreeLeave()}
+$INCLUDE{test_tree.plearn}

Added: trunk/speedtest2/train.pymat
===================================================================
--- trunk/speedtest2/train.pymat	2008-12-18 21:37:38 UTC (rev 9799)
+++ trunk/speedtest2/train.pymat	2008-12-18 21:42:43 UTC (rev 9800)
@@ -0,0 +1,153 @@
+import os.path,sys
+from plearn.pyplearn import *
+import plearn.utilities.ppath
+from plearn.utilities.ppath import ppath
+
+plarg_defaults.traintest='train'
+plarg_defaults.datadir = "meanmode"
+plarg_defaults.dt = 0.
+plarg_defaults.ind = True
+plarg_defaults.weightsize = 0
+plarg_defaults.memorymatrix = False
+plarg_defaults.memorymatrix_nosave = False
+plarg_defaults.deep_copy_memory_data=False
+plarg_defaults.target=True
+plarg_defaults.length=-1
+plarg_defaults.istart=-1
+
+
+if plargs.memorymatrix and plargs.memorymatrix_nosave:
+       print "ERROR can set only memorymatrix or memorymatrix_nosave"
+       sys.exit(1)
+       
+def getpvmatfilename(file):
+    file=plearn.utilities.ppath.ppath(file)
+    if os.path.exists(file+"pmat") and os.path.exists(file+"vmat"):
+        mtime1=os.path.getmtime(file+"vmat")
+	mtime2=os.path.getmtime(file+"pmat")
+	if mtime1>=mtime2:
+            print "ERROR: "+file+"vmat older then "+file+"pmat, recreate the pmat"
+            sys.exit(2)
+	else:
+            file = file+"pmat"
+    elif os.path.exists(file+"pmat"):
+        file = file+"pmat"
+    else:
+        file = file+"vmat"
+        print >> sys.stderr, "INFO: no pmat file for the file",file
+    return file
+
+def removeIndVMat(original):
+       fd = open('common/missing_indicator_field_names.inc','r')
+       lines = [ s.strip() for s in fd.readlines()]
+       lines = lines[1:-1]
+       return pl.SelectColumnsVMatrix(
+              inputsize=original.inputsize-len(lines),
+              targetsize=original.targetsize,
+              weightsize=original.weightsize,
+              inverse_fields_selection = True,
+              source=original,
+              fields = lines
+              )
+
+inputfile=os.path.join(plargs.datadir,plargs.traintest)+"_input."
+
+if plargs.datadir.startswith("letter"):
+       sizeinput=16
+       sizetarget=1
+       plarg_defaults.target=False
+elif plargs.datadir.find("imputation2007") != -1:
+       sizeinput=-1
+       sizetarget=1
+       inputfile=os.path.join(plargs.datadir,plargs.traintest)+"."
+       plarg_defaults.target=False
+else:
+       sizeinput=500
+       sizetarget=0
+
+
+inputfile=getpvmatfilename(inputfile)
+vmat=pl.AutoVMatrix( specification = inputfile,
+		     inputsize=sizeinput,
+		     targetsize=sizetarget,
+		     weightsize=0
+		     )
+
+if plargs.target:
+	targetfile="noimp/"+plargs.traintest+"_target.vmat"
+	vmat=pl.ConcatColumnsVMatrix(sources=[vmat,
+					      pl.ProcessingVMatrix(source=pl.AutoVMatrix(specification=targetfile),
+								   prg = "@CLASSE_REEL 1 - :CLASSE_REEL")
+					      ],
+				     inputsize=sizeinput,
+				     targetsize=1,
+				     weightsize=0)
+	
+if plargs.length>0 and plargs.istart>0:
+       vmat = pl.SubVMatrix(source=vmat,
+                            length=plargs.length,
+                            istart=plargs.istart,
+                            inputsize=vmat.inputsize,
+                            targetsize=vmat.targetsize,
+                            weightsize=vmat.weightsize)
+elif plargs.length>0:
+       vmat = pl.SubVMatrix(source=vmat,
+                            length=plargs.length,
+                            inputsize=vmat.inputsize,
+                            targetsize=vmat.targetsize,
+                            weightsize=vmat.weightsize)
+elif plargs.istart>0:
+       vmat = pl.SubVMatrix(source=vmat,
+                            istart=plargs.istart,
+                            inputsize=vmat.inputsize,
+                            targetsize=vmat.targetsize,
+                            weightsize=vmat.weightsize)
+       
+if not plargs.ind:
+       vmat = removeIndVMat(vmat)
+       
+if plargs.dt != 0:
+       dtvmat = pl.VariableDeletionVMatrix(source=vmat,
+                                         min_non_missing_threshold = plargs.dt,
+                                         max_constant_threshold = 0,
+#                                        remove_columns_with_constant_value = 0,
+#                                         number_of_train_samples = 30000
+                                         )
+       if not plargs.ind:
+              train_set = pl.AutoVMatrix(specification=getpvmatfilename("noimp/train_input_class_noind."),
+                                         inputsize=500,
+                                         targetsize=1)
+       else:
+           train_set = pl.AutoVMatrix(specification=getpvmatfilename("noimp/train_input_class."),
+                                      inputsize=500,
+                                      targetsize=1)
+
+       dtvmat.train_set = train_set
+       vmat = dtvmat
+       
+if plargs.weightsize != 0:
+       vmat = pl.ConcatColumnsVMatrix(sources=[
+                     vmat,
+                     pl.ConstantVMatrix(length=vmat.length,width=1,
+                                        constant_output=1./vmat.length,
+                                        inputsize=0,targetsize=0)]
+                                      ,weightsize=1,targetsize=1)
+       
+if plargs.memorymatrix:
+       vmat=pl.MemoryVMatrix(source=vmat,
+	deep_copy_memory_data=plargs.deep_copy_memory_data)
+elif plargs.memorymatrix_nosave:
+       vmat=pl.MemoryVMatrixNoSave(source=vmat,
+	deep_copy_memory_data=plargs.deep_copy_memory_data) 
+dir="newcommon/train.pymat.metadata/traintest="+plargs.traintest
+dir+="_datadir="+plargs.datadir
+dir+="_dt="+str(plargs.dt)
+dir+="_ind="+str(plargs.ind)
+dir+="_weightsize="+str(plargs.weightsize)
+dir+="_memorymatrix="+str(plargs.memorymatrix)
+dir+="_target="+str(plargs.target)
+dir+="_length="+str(plargs.length)
+#print 
+vmat.metadatadir=dir
+def main():
+       return vmat



From nouiz at mail.berlios.de  Fri Dec 19 22:44:57 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 19 Dec 2008 22:44:57 +0100
Subject: [Plearn-commits] r9801 - trunk/plearn_learners/regressors
Message-ID: <200812192144.mBJLivXt009817@sheep.berlios.de>

Author: nouiz
Date: 2008-12-19 22:44:56 +0100 (Fri, 19 Dec 2008)
New Revision: 9801

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
don't overwrite a macro if we want to specify it on the command line.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-18 21:42:43 UTC (rev 9800)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-12-19 21:44:56 UTC (rev 9801)
@@ -49,7 +49,9 @@
 
 //!used to limit the memory used by limiting the length of the dataset.
 //!work with unsigned int, uint16_t, but fail with uint8_t???
+#ifndef RTR_type
 #define RTR_type uint32_t
+#endif
 //!The type for the leave id
 #ifndef RTR_type_id
 #define RTR_type_id int16_t



From laulysta at mail.berlios.de  Sat Dec 20 01:43:12 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Sat, 20 Dec 2008 01:43:12 +0100
Subject: [Plearn-commits] r9802 - trunk/plearn_learners_experimental
Message-ID: <200812200043.mBK0hCMw019784@sheep.berlios.de>

Author: laulysta
Date: 2008-12-20 01:43:12 +0100 (Sat, 20 Dec 2008)
New Revision: 9802

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
auto encodeur de la couche cach?\195?\169e



Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-19 21:44:56 UTC (rev 9801)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-20 00:43:12 UTC (rev 9802)
@@ -683,7 +683,7 @@
                 if(hidden_reconstruction_lr!=0){
                     setLearningRate( hidden_reconstruction_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0);
+                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 1);
                 }
 
                 // recurrent noisy phase
@@ -1070,31 +1070,68 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 
-                                                                 Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+                                                                 Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
 {
     // set appropriate sizes
-    int fullinputlength = clean_input.length();
+    int fullinputlength = hidden_target.length();
     Vec reconstruction_activation;
-    /*if(reconstruction_bias.length()==0)
-    {
-        reconstruction_bias.resize(fullinputlength);
-        reconstruction_bias.clear();
-        }*/
+   
     reconstruction_activation.resize(fullinputlength);
     reconstruction_prob.resize(fullinputlength);
 
     // predict (denoised) input_reconstruction 
     transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
+    //product(reconstruction_activation, reconstruction_weights, hidden); 
     //reconstruction_activation += hidden_layer->bias;
     
     hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
 
     /********************************************************************************/
+    // Vec hidden_reconstruction_activation_grad;
+    hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
+    hidden_reconstruction_activation_grad << reconstruction_prob;
+    hidden_reconstruction_activation_grad -= hidden_target;
+    hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
+
+    productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
+
+    // update weight
+    //externalProductScaleAcc(reconstruction_weights, hidden, hidden_reconstruction_activation_grad, -lr);
+    /********************************************************************************/
+
+    double result_cost = 0;
+    double neg_log_cost = 0; // neg log softmax
+    for(int k=0; k<reconstruction_prob.length(); k++)
+        if(hidden_target[k]!=0)
+            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]);
+    result_cost = neg_log_cost;
+    
+    return result_cost;
+}
+
+double DenoisingRecurrentNet::fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 
+                                                                 Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
+{
+    // set appropriate sizes
+    int fullinputlength = hidden_target.length();
+    Vec reconstruction_activation;
+   
+    reconstruction_activation.resize(fullinputlength);
+    reconstruction_prob.resize(fullinputlength);
+
+    // predict (denoised) input_reconstruction 
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //truc de stan
+    //product(reconstruction_activation, reconstruction_weights, hidden); 
+    //reconstruction_activation += hidden_layer->bias;
+    
+    hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
+
+    /********************************************************************************/
     Vec hidden_reconstruction_activation_grad;
     hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
     hidden_reconstruction_activation_grad << reconstruction_prob;
-    hidden_reconstruction_activation_grad -= clean_input;
+    hidden_reconstruction_activation_grad -= hidden_target;
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
 
     productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
@@ -1103,8 +1140,8 @@
     double result_cost = 0;
     double neg_log_cost = 0; // neg log softmax
     for(int k=0; k<reconstruction_prob.length(); k++)
-        if(clean_input[k]!=0)
-            neg_log_cost -= clean_input[k]*safelog(reconstruction_prob[k]);
+        if(hidden_target[k]!=0)
+            neg_log_cost -= hidden_target[k]*safelog(reconstruction_prob[k]);
     result_cost = neg_log_cost;
     
     return result_cost;
@@ -1122,7 +1159,7 @@
 target_prediction_act_no_bias_list
 nll_list
 */
-
+/*
 void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
                                             real hidden_reconstruction_weight,
                                             real temporal_gradient_contribution)
@@ -1186,17 +1223,141 @@
         if(i!=0 && dynamic_connections )
         {   
 
+
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient);
+            input_connections->bpropUpdate(
+                input_list[i],
+                hidden_act_no_bias_list(i), 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+                
+
+
             // Add contribution of hidden reconstruction cost in hidden_gradient
             if(hidden_reconstruction_weight!=0)
             {
                 Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
-                //Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
+                //truc stan
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                //fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+            
+            }
+            // add contribution to gradient of next time step hidden layer
+            if(temporal_gradient_contribution>0)
+            { // add weighted contribution of hidden_temporal gradient to hidden_gradient
+                // It does this: hidden_gradient += temporal_gradient_contribution*hidden_temporal_gradient;
+                multiplyAcc(hidden_gradient, hidden_temporal_gradient, temporal_gradient_contribution);
+            }
+  
+
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient);
                 
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, 
-                                                        hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+            dynamic_connections->bpropUpdate(
+                hidden_list(i-1),
+                hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                hidden_gradient, hidden_temporal_gradient);
+                
+            
+            hidden_temporal_gradient << hidden_gradient;                
+        }
+        else
+        {
+            hidden_layer->bpropUpdate(
+                hidden_act_no_bias_list(i), hidden_list(i),
+                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
+            input_connections->bpropUpdate(
+                input_list[i],
+                hidden_act_no_bias_list(i), 
+                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
+
+        }
+    }
+    
+}
+
+*/
+void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
+                                            real hidden_reconstruction_weight,
+                                            real temporal_gradient_contribution)
+{
+    hidden_temporal_gradient.resize(hidden_layer->size);
+    hidden_temporal_gradient.clear();
+    for(int i=hidden_list.length()-1; i>=0; i--){   
+
+        if( hidden_layer2 )
+            hidden_gradient.resize(hidden_layer2->size);
+        else
+            hidden_gradient.resize(hidden_layer->size);
+        hidden_gradient.clear();
+        if( prediction_cost_weight!=0 )
+        {
+            for( int tar=0; tar<target_layers.length(); tar++)
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                    target_layers[tar]->activation += target_layers[tar]->bias;
+                    target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                    bias_gradient *= prediction_cost_weight;
+                    if(use_target_layers_masks)
+                        bias_gradient *= masks_list[tar](i);
+                    target_layers[tar]->update(bias_gradient);
+                    if( hidden_layer2 )
+                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                    else
+                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                             hidden_gradient, bias_gradient,true);
+                }
             }
 
+            if (hidden_layer2)
+            {
+                hidden_layer2->bpropUpdate(
+                    hidden2_act_no_bias_list(i), hidden2_list(i),
+                    bias_gradient, hidden_gradient);
+                
+                hidden_connections->bpropUpdate(
+                    hidden_list(i),
+                    hidden2_act_no_bias_list(i), 
+                    hidden_gradient, bias_gradient);
+            }
+        }
+            
+        // Add contribution of input reconstruction cost in hidden_gradient
+        if(input_reconstruction_weight!=0)
+        {
+            Mat reconstruction_weights = getInputConnectionsWeightMatrix();
+            Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
 
+            fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
+                                                     clean_input, hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+        }
+
+
+        if(i!=0 && dynamic_connections )
+        {   
+
+            // Add contribution of hidden reconstruction cost in hidden_gradient
+            Vec hidden_reconstruction_activation_grad;
+            hidden_reconstruction_activation_grad.resize(hidden_layer->size);
+            Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+            if(hidden_reconstruction_weight!=0)
+            {
+                //Vec hidden_reconstruction_activation_grad;
+                //Mat reconstruction_weights = getDynamicConnectionsWeightMatrix();
+
+                //truc stan
+                //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, dynamic_gradient_scale_factor*current_learning_rate);
+            
+            }
+
+
             // add contribution to gradient of next time step hidden layer
             if(temporal_gradient_contribution>0)
             { // add weighted contribution of hidden_temporal gradient to hidden_gradient
@@ -1211,7 +1372,13 @@
                 hidden_list(i-1),
                 hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
                 hidden_gradient, hidden_temporal_gradient);
-                
+
+            if(hidden_reconstruction_weight!=0)
+            {
+                // update weight
+                externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -dynamic_gradient_scale_factor*current_learning_rate);
+            }
+
             input_connections->bpropUpdate(
                 input_list[i],
                 hidden_act_no_bias_list(i), 

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-19 21:44:56 UTC (rev 9801)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-20 00:43:12 UTC (rev 9802)
@@ -319,7 +319,7 @@
     
     //! Stores hidden gradient of dynamic connections coming from time t+1
     mutable Vec hidden_temporal_gradient;
-        
+
     //! List of hidden layers values
     // mutable TVec< Vec > hidden_list;
     mutable Mat hidden_list;
@@ -423,9 +423,11 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 
-                                                                          Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
-
+    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+                                                                          Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
+    
+    double fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 
+                                                                          Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
 private:
     //#####  Private Data Members  ############################################
 



From laulysta at mail.berlios.de  Tue Dec 30 06:46:53 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 30 Dec 2008 06:46:53 +0100
Subject: [Plearn-commits] r9803 - trunk/plearn_learners_experimental
Message-ID: <200812300546.mBU5krcK012875@sheep.berlios.de>

Author: laulysta
Date: 2008-12-30 06:46:51 +0100 (Tue, 30 Dec 2008)
New Revision: 9803

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
hidden reconstruction


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-20 00:43:12 UTC (rev 9802)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-30 05:46:51 UTC (rev 9803)
@@ -683,7 +683,7 @@
                 if(hidden_reconstruction_lr!=0){
                     setLearningRate( hidden_reconstruction_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 1);
+                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0);
                 }
 
                 // recurrent noisy phase
@@ -1070,23 +1070,29 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                  Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
 {
     // set appropriate sizes
-    int fullinputlength = hidden_target.length();
+    int fullhiddenlength = hidden_target.length();
     Vec reconstruction_activation;
-   
-    reconstruction_activation.resize(fullinputlength);
-    reconstruction_prob.resize(fullinputlength);
+    if(reconstruction_bias.length()==0)
+    {
+        reconstruction_bias.resize(fullhiddenlength);
+        reconstruction_bias.clear();
+    }
+    reconstruction_activation.resize(fullhiddenlength);
+    reconstruction_prob.resize(fullhiddenlength);
 
     // predict (denoised) input_reconstruction 
     transposeProduct(reconstruction_activation, reconstruction_weights, hidden); 
-    //product(reconstruction_activation, reconstruction_weights, hidden); 
-    //reconstruction_activation += hidden_layer->bias;
-    
-    hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
+    reconstruction_activation += reconstruction_bias;
 
+    for( int j=0 ; j<fullhiddenlength ; j++ )
+        reconstruction_prob[j] = fastsigmoid( reconstruction_activation[j] );
+
+    //hidden_layer->fprop(reconstruction_activation, reconstruction_prob);
+
     /********************************************************************************/
     // Vec hidden_reconstruction_activation_grad;
     hidden_reconstruction_activation_grad.resize(reconstruction_prob.size());
@@ -1341,7 +1347,7 @@
 
         if(i!=0 && dynamic_connections )
         {   
-
+            
             // Add contribution of hidden reconstruction cost in hidden_gradient
             Vec hidden_reconstruction_activation_grad;
             hidden_reconstruction_activation_grad.resize(hidden_layer->size);
@@ -1353,11 +1359,11 @@
 
                 //truc stan
                 //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, dynamic_gradient_scale_factor*current_learning_rate);
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, dynamic_gradient_scale_factor*current_learning_rate);
             
             }
+            
 
-
             // add contribution to gradient of next time step hidden layer
             if(temporal_gradient_contribution>0)
             { // add weighted contribution of hidden_temporal gradient to hidden_gradient
@@ -1375,8 +1381,11 @@
 
             if(hidden_reconstruction_weight!=0)
             {
+                // update bias
+                multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -dynamic_gradient_scale_factor*current_learning_rate);
                 // update weight
                 externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -dynamic_gradient_scale_factor*current_learning_rate);
+                
             }
 
             input_connections->bpropUpdate(

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-20 00:43:12 UTC (rev 9802)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-30 05:46:51 UTC (rev 9803)
@@ -147,6 +147,9 @@
 
     // learnt bias for input reconstruction
     Vec input_reconstruction_bias;
+
+    // learnt bias for hidden reconstruction
+    Vec hidden_reconstruction_bias;
     
     double prediction_cost_weight;
     double input_reconstruction_cost_weight;
@@ -423,7 +426,7 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                           Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
     
     double fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 



From laulysta at mail.berlios.de  Wed Dec 31 05:11:53 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 31 Dec 2008 05:11:53 +0100
Subject: [Plearn-commits] r9804 - trunk/plearn_learners_experimental
Message-ID: <200812310411.mBV4Brqr014278@sheep.berlios.de>

Author: laulysta
Date: 2008-12-31 05:11:52 +0100 (Wed, 31 Dec 2008)
New Revision: 9804

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
reconstruction


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-30 05:46:51 UTC (rev 9803)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-12-31 04:11:52 UTC (rev 9804)
@@ -671,19 +671,29 @@
 
                 if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
                     inject_zero_forcing_noise(encoded_seq, input_noise_prob);
+
+                // recurrent no noise phase
+                if(recurrent_lr!=0)
+                {
+                    if(corrupt_input) // need to recover the clean sequence                        
+                        encoded_seq << clean_encoded_seq;                  
+                    setLearningRate( recurrent_lr );                    
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate(0,0,1, prediction_cost_weight );
+                }
                 
                 // greedy phase input
                 if(input_reconstruction_lr!=0){
                     setLearningRate( input_reconstruction_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, 0, 0);
+                    recurrentUpdate(input_reconstruction_cost_weight, 0, 0, prediction_cost_weight );
                 }
                 
                 // greedy phase hidden
                 if(hidden_reconstruction_lr!=0){
-                    setLearningRate( hidden_reconstruction_lr );
+                    setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0);
+                    recurrentUpdate(0, hidden_reconstruction_cost_weight, 0, 0 );
                 }
 
                 // recurrent noisy phase
@@ -691,18 +701,10 @@
                 {
                     setLearningRate( noisy_recurrent_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1);
+                    recurrentUpdate(input_reconstruction_cost_weight, hidden_reconstruction_cost_weight, 1, prediction_cost_weight );
                 }
 
-                // recurrent no noise phase
-                if(recurrent_lr!=0)
-                {
-                    if(corrupt_input) // need to recover the clean sequence                        
-                        encoded_seq << clean_encoded_seq;                  
-                    setLearningRate( recurrent_lr );                    
-                    recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate(0,0,1);
-                }
+                
             }
 
             if( pb )
@@ -1100,6 +1102,7 @@
     hidden_reconstruction_activation_grad -= hidden_target;
     hidden_reconstruction_activation_grad *= hidden_reconstruction_cost_weight;
 
+
     productAcc(hidden_gradient, reconstruction_weights, hidden_reconstruction_activation_grad);
 
     // update weight
@@ -1287,7 +1290,8 @@
 */
 void DenoisingRecurrentNet::recurrentUpdate(real input_reconstruction_weight,
                                             real hidden_reconstruction_weight,
-                                            real temporal_gradient_contribution)
+                                            real temporal_gradient_contribution,
+                                            real predic_cost_weight)
 {
     hidden_temporal_gradient.resize(hidden_layer->size);
     hidden_temporal_gradient.clear();
@@ -1298,7 +1302,7 @@
         else
             hidden_gradient.resize(hidden_layer->size);
         hidden_gradient.clear();
-        if( prediction_cost_weight!=0 )
+        if( predic_cost_weight!=0 )
         {
             for( int tar=0; tar<target_layers.length(); tar++)
             {
@@ -1308,7 +1312,7 @@
                     target_layers[tar]->activation += target_layers[tar]->bias;
                     target_layers[tar]->setExpectation(target_prediction_list[tar](i));
                     target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
-                    bias_gradient *= prediction_cost_weight;
+                    bias_gradient *= predic_cost_weight;
                     if(use_target_layers_masks)
                         bias_gradient *= masks_list[tar](i);
                     target_layers[tar]->update(bias_gradient);
@@ -1359,7 +1363,7 @@
 
                 //truc stan
                 //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, dynamic_gradient_scale_factor*current_learning_rate);
+                fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconstruction_weights, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
             
             }
             
@@ -1382,9 +1386,9 @@
             if(hidden_reconstruction_weight!=0)
             {
                 // update bias
-                multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -dynamic_gradient_scale_factor*current_learning_rate);
+                multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -current_learning_rate);
                 // update weight
-                externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -dynamic_gradient_scale_factor*current_learning_rate);
+                externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -current_learning_rate);
                 
             }
 
@@ -1743,8 +1747,10 @@
     input_layer->setLearningRate( the_learning_rate );
     hidden_layer->setLearningRate( the_learning_rate );
     input_connections->setLearningRate( the_learning_rate );
-    if( dynamic_connections )
-        dynamic_connections->setLearningRate( dynamic_gradient_scale_factor*the_learning_rate ); 
+    if( dynamic_connections ){
+        //dynamic_connections->setLearningRate( dynamic_gradient_scale_factor*the_learning_rate ); 
+        dynamic_connections->setLearningRate( the_learning_rate ); 
+    }
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-30 05:46:51 UTC (rev 9803)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-12-31 04:11:52 UTC (rev 9804)
@@ -271,7 +271,8 @@
     //! after the visible units have been clamped
     void recurrentUpdate(real input_reconstruction_weight,
                          real hidden_reconstruction_cost_weight,
-                         real temporal_gradient_contribution = 1);
+                         real temporal_gradient_contribution = 1,
+                         real prediction_cost_weight = 1);
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;



