From nouiz at mail.berlios.de  Thu Apr  2 16:43:52 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Apr 2009 16:43:52 +0200
Subject: [Plearn-commits] r10078 - trunk/plearn_learners/regressors
Message-ID: <200904021443.n32Ehqih023495@sheep.berlios.de>

Author: nouiz
Date: 2009-04-02 16:43:51 +0200 (Thu, 02 Apr 2009)
New Revision: 10078

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
store two vector in one vector of pair to be more memory friendly.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-03-30 20:45:25 UTC (rev 10077)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-02 14:43:51 UTC (rev 10078)
@@ -246,10 +246,11 @@
         return;
     TVec<RTR_type> candidate(0, leave->length());//list of candidate row to split
     TVec<RTR_type> registered_row(leave->length());
-    TVec<RTR_target_t> registered_target(0, leave->length()); 
-    TVec<RTR_weight_t> registered_weight(0, leave->length());
+    TVec<pair<RTR_target_t,RTR_weight_t> > registered_target_weight(leave->length());
+    registered_target_weight.resize(leave->length());
+    registered_target_weight.resize(0);
     Vec registered_value(0, leave->length());
-    tmp_vec.resize(2);
+    tmp_vec.resize(leave->outputsize());
     Vec left_error(3);
     Vec right_error(3);
     Vec missing_error(3);
@@ -273,7 +274,7 @@
         right_leave->initStats();
         
         train_set->getAllRegisteredRow(leave_id, col, registered_row,
-                                       registered_target, registered_weight,
+                                       registered_target_weight,
                                        registered_value);
 
         PLASSERT(registered_row.size()==leave->length());
@@ -291,11 +292,11 @@
             prev_row = registered_row[row_idx_end - 1];
             prev_val = registered_value[row_idx_end - 1];
             if (is_missing(val))
-                missing_leave->addRow(row, registered_target[row_idx_end],
-                                      registered_weight[row_idx_end]);
+                missing_leave->addRow(row, registered_target_weight[row_idx_end].first,
+                                      registered_target_weight[row_idx_end].second);
             else if(val==prev_val)
-                right_leave->addRow(row, registered_target[row_idx_end],
-                                    registered_weight[row_idx_end]);
+                right_leave->addRow(row, registered_target_weight[row_idx_end].first,
+                                    registered_target_weight[row_idx_end].second);
             else
                 break;
         }
@@ -304,11 +305,11 @@
         {
             int row=registered_row[row_idx];
             if (is_missing(registered_value[row_idx]))
-                missing_leave->addRow(row, registered_target[row_idx],
-                                      registered_weight[row_idx]);
+                missing_leave->addRow(row, registered_target_weight[row_idx].first,
+                                      registered_target_weight[row_idx].second);
             else {
-                left_leave->addRow(row, registered_target[row_idx],
-                                   registered_weight[row_idx]);
+                left_leave->addRow(row, registered_target_weight[row_idx].first,
+                                   registered_target_weight[row_idx].second);
                 candidate.append(row);
             }
         }
@@ -318,7 +319,7 @@
                                                 right_error, missing_error,
                                                 right_leave, left_leave,
                                                 train_set, registered_value,
-                                                registered_target, registered_weight);
+                                                registered_target_weight);
 #ifdef RCMP
         row_split_err[col] = get<0>(ret);
         row_split_value[col] = get<1>(ret);
@@ -354,7 +355,7 @@
     PP<RegressionTreeLeave> right_leave,
     PP<RegressionTreeLeave> left_leave,
     PP<RegressionTreeRegisters> train_set,
-    Vec values, TVec<RTR_target_t> targets, TVec<RTR_weight_t> weights
+    Vec values,TVec<pair<RTR_target_t,RTR_weight_t> > t_w
     )
 {
     int best_balance=INT_MAX;
@@ -381,8 +382,8 @@
         PLASSERT(row_feature==values[i+1]);
         next_feature=values[i];
 
-        real target=targets[i+1];
-        real weight=weights[i+1];
+        real target=t_w[i+1].first;
+        real weight=t_w[i+1].second;
         PLASSERT(train_set->get(next_row, col)==values[i]);
         PLASSERT(train_set->get(row, col)==values[i+1]);
         PLASSERT(next_feature<=row_feature);

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-03-30 20:45:25 UTC (rev 10077)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2009-04-02 14:43:51 UTC (rev 10078)
@@ -136,7 +136,8 @@
                                                PP<RegressionTreeLeave> right_leave,
                                                PP<RegressionTreeLeave> left_leave,
                                                PP<RegressionTreeRegisters> train_set,
-                                               Vec values, TVec<RTR_target_t> targets, TVec<RTR_weight_t> weights
+                                               Vec values, 
+                                               TVec<pair<RTR_target_t,RTR_weight_t> > t_w
         );
 };
 

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-03-30 20:45:25 UTC (rev 10077)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-02 14:43:51 UTC (rev 10078)
@@ -221,46 +221,64 @@
 }
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg,
-                                                  TVec<RTR_target_t> &target,
-                                                  TVec<RTR_weight_t> &weight,
+                                                  TVec<pair<RTR_target_t,RTR_weight_t> > &t_w,
                                                   Vec &value) const
 {
     PLASSERT(tsource_mat.length()==tsource.length());
 
     getAllRegisteredRow(leave_id,col,reg);
-    target.resize(reg.length());
-    weight.resize(reg.length());
+    t_w.resize(reg.length());
     value.resize(reg.length());
     real * p = tsource_mat[col];
     pair<RTR_target_t,RTR_weight_t> * ptw = target_weight.data();
-    RTR_target_t * pt = target.data();
-    RTR_weight_t * pw = weight.data();
+    pair<RTR_target_t,RTR_weight_t>* ptwd = t_w.data();
     real * pv = value.data();
     RTR_type * preg = reg.data();
 
     if(weightsize() <= 0){
-        weight.fill(1.0 / length());
+        RTR_weight_t w = 1.0 / length();
         for(int i=0;i<reg.length();i++){
             PLASSERT(tsource->get(col, reg[i])==p[reg[i]]);
             int idx = int(preg[i]);
-            pt[i] = ptw[idx].first;
+            ptwd[i].first = ptw[idx].first;
+            ptwd[i].second = w;
             pv[i] = p[idx];
         }
     } else {
         //It is better to do multiple pass for memory access.
         for(int i=0;i<reg.length();i++){
             int idx = int(preg[i]);
-            pt[i] = ptw[idx].first;
-            pw[i] = ptw[idx].second;
+            ptwd[i].first = ptw[idx].first;
+            ptwd[i].second = ptw[idx].second;
+
         }
         for(int i=0;i<reg.length();i++){
             PLASSERT(tsource->get(col, reg[i])==p[reg[i]]);
-            pv[i] = p[preg[i]];
+            int idx = int(preg[i]);
+            pv[i] = p[idx];
         }
     }
 }
 
 //! reg.size() == the number of row that we will put in it.
+void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id,
+                                                  TVec<RTR_type> &reg) const
+{
+    PLASSERT(tsource_mat.length()==tsource.length());
+
+    int idx=0;
+    int n=reg.length();
+    RTR_type* preg = reg.data();
+    RTR_type_id* pleave_register = leave_register.data();
+    for(int i=0;i<length() && n> idx;i++){
+        if (pleave_register[i] == leave_id){
+            preg[idx++]=i;
+        }
+    }
+    PLASSERT(idx==reg->size());
+}
+
+//! reg.size() == the number of row that we will put in it.
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg) const
 {
@@ -349,14 +367,13 @@
         DBG_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix: "<<f<<endl;
         PLearn::save(f,tsorted_row);
     }else{
-        DBG_LOG<<"RegressionTreeRegisters:: can't save the sorted source VMatrix as we don't have a metadatadir"<<endl;
     }
 
 }
   
 void RegressionTreeRegisters::sortEachDim(int dim)
 {
-    PLCHECK(tsource->classname()=="MemoryVMatrixNoSave");
+    PLCHECK_MSG(tsource->classname()=="MemoryVMatrixNoSave",tsource->classname().c_str());
     Mat m = tsource.toMat();
     Vec v = m(dim);
     TVec<int> order = v.sortingPermutation(true, true);

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-03-30 20:45:25 UTC (rev 10077)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-02 14:43:51 UTC (rev 10078)
@@ -137,9 +137,10 @@
     inline RTR_type_id     getNextId(){
         PLCHECK(next_id<std::numeric_limits<RTR_type_id>::max());
         next_id += 1;return next_id;}
+    void         getAllRegisteredRow(RTR_type_id leave_id, TVec<RTR_type> &reg)const;
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg)const;
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
-                                     TVec<RTR_target_t> &target, TVec<RTR_weight_t> &weight, Vec &value)const;
+                                     TVec<pair<RTR_target_t,RTR_weight_t> > &t_w, Vec &value)const;
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
     inline virtual void put(int i, int j, real value)
@@ -152,8 +153,8 @@
     }
     
     //! usefull in MultiClassAdaBoost to save memory
-    TMat<RTR_type> getTSortedRow(){return tsorted_row;}
-    VMat          getTSource(){return tsource;}
+    inline TMat<RTR_type> getTSortedRow(){return tsorted_row;}
+    inline VMat   getTSource(){return tsource;}
     virtual void finalize(){tsorted_row = TMat<RTR_type>();}
 
 private:



From tihocan at mail.berlios.de  Fri Apr  3 17:02:34 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 3 Apr 2009 17:02:34 +0200
Subject: [Plearn-commits] r10079 - trunk/commands
Message-ID: <200904031502.n33F2YV0019210@sheep.berlios.de>

Author: tihocan
Date: 2009-04-03 17:02:34 +0200 (Fri, 03 Apr 2009)
New Revision: 10079

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Removed include of BetaKernel so that plearn_noblas can compile with -noblas

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2009-04-02 14:43:51 UTC (rev 10078)
+++ trunk/commands/plearn_noblas_inc.h	2009-04-03 15:02:34 UTC (rev 10079)
@@ -108,7 +108,9 @@
  * Kernel *
  **********/
 #include <plearn/ker/AdditiveNormalizationKernel.h>
-#include <plearn/ker/BetaKernel.h>
+//! The include of 'BetaKernel' is disabled because it also triggers the
+//! include of 'distr_maths.h', which currently requires BLAS.
+//#include <plearn/ker/BetaKernel.h>
 #include <plearn/ker/CosKernel.h>
 #include <plearn/ker/DistanceKernel.h>
 #include <plearn/ker/DotProductKernel.h>



From nouiz at mail.berlios.de  Fri Apr  3 18:56:41 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 3 Apr 2009 18:56:41 +0200
Subject: [Plearn-commits] r10080 - trunk
Message-ID: <200904031656.n33GufLg020084@sheep.berlios.de>

Author: nouiz
Date: 2009-04-03 18:56:40 +0200 (Fri, 03 Apr 2009)
New Revision: 10080

Modified:
   trunk/pymake.config.model
Log:
new compile option.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-04-03 15:02:34 UTC (rev 10079)
+++ trunk/pymake.config.model	2009-04-03 16:56:40 UTC (rev 10080)
@@ -261,7 +261,7 @@
     'icc8', 'icc9', 'icc10', 'mpi',
     'purify', 'quantify', 'vc++', 'condor' ],
   
-  [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
+  [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbg', 'optdbggprof', 'safegprof', 
     'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg', 'vecgcc' ],
   
   [ 'double', 'float' ],
@@ -745,6 +745,11 @@
               cpp_definitions = ['NDEBUG'],
               linkeroptions = '-pg' )
 
+pymakeOption( name = 'optdbg',
+              description = 'optimized mode WITH DEBUGGING (-g)',
+              compileroptions = '-Wall -O3 -g',
+              cpp_definitions = ['NDEBUG'])
+
 pymakeOption( name = 'safegprof',
               description = 'safe optimized mode with profiler support (-pg)',
               compileroptions = '-Wall -O3 -pg',



From nouiz at mail.berlios.de  Fri Apr  3 19:43:08 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 3 Apr 2009 19:43:08 +0200
Subject: [Plearn-commits] r10081 - trunk/commands/PLearnCommands
Message-ID: <200904031743.n33Hh8O9011498@sheep.berlios.de>

Author: nouiz
Date: 2009-04-03 19:43:08 +0200 (Fri, 03 Apr 2009)
New Revision: 10081

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
added plearn option --profile-wall


Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2009-04-03 16:56:40 UTC (rev 10080)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2009-04-03 17:43:08 UTC (rev 10081)
@@ -70,6 +70,7 @@
         "                 --windows_endl: use Windows end of line\n"
         "                 --profile: print some profiling information\n"
         "                     Must have been compiled with '-logging=dbg-profile'\n"
+        "                 --profile-wall: same as above except we only print the wall time\n"
         "                 --verbosity LEVEL: The level of log to print.\n"
         "                     Must have been compiled with the same level of more\n"
         "                     Available level:\n"

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2009-04-03 16:56:40 UTC (rev 10080)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2009-04-03 17:43:08 UTC (rev 10081)
@@ -175,7 +175,8 @@
     // Note that the findpos function (stringutils.h) returns -1 if the
     // option is not found.
     int profile_pos       = findpos( command_line, "--profile" );
-    if(profile_pos != -1)
+    int profile_wall_pos       = findpos( command_line, "--profile-wall" );
+    if(profile_pos != -1 || profile_wall_pos != -1)
         Profiler::pl_profile_activate();
     // Note that the findpos function (stringutils.h) returns -1 if the
     // option is not found.
@@ -270,6 +271,7 @@
         // Neglecting to copy options
         if ( c != no_version_pos             &&
              c != profile_pos                &&
+             c != profile_wall_pos           &&
              c != no_progress_bars           &&
              c != windows_endl               &&
              c != verbosity_pos              &&
@@ -394,6 +396,10 @@
         Profiler::pl_profile_deactivate();
         Profiler::pl_profile_report(perr);
         Profiler::pl_profile_reportwall(perr);
+    }else if(findpos( command_line_orig, "--profile-wall" )!=-1){
+        Profiler::pl_profile_end("Prog");
+        Profiler::pl_profile_deactivate();
+        Profiler::pl_profile_reportwall(perr);
     }
     return EXIT_CODE;
 }



From nouiz at mail.berlios.de  Fri Apr  3 22:50:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 3 Apr 2009 22:50:35 +0200
Subject: [Plearn-commits] r10082 - trunk/plearn_learners/regressors
Message-ID: <200904032050.n33KoZqX002332@sheep.berlios.de>

Author: nouiz
Date: 2009-04-03 22:50:29 +0200 (Fri, 03 Apr 2009)
New Revision: 10082

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
optimisation that interleave memory access with computation with prefetch


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-03 17:43:08 UTC (rev 10081)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-03 20:50:29 UTC (rev 10082)
@@ -267,12 +267,19 @@
     row_split_balance.clear();
 #endif
     int leave_id = leave->getId();
+   
     for (int col = 0; col < inputsize; col++)
     {
         missing_leave->initStats();
         left_leave->initStats();
         right_leave->initStats();
         
+        PLASSERT(registered_row.size()==leave->length());
+        PLASSERT(candidate.size()==0);
+
+#ifdef NPREFETCH
+        //The ifdef is in case we don't want to use the optimized version with
+        //prefetch of memory. Maybe the optimization is hurtfull for some computer.
         train_set->getAllRegisteredRow(leave_id, col, registered_row,
                                        registered_target_weight,
                                        registered_value);
@@ -314,6 +321,18 @@
             }
         }
 
+#else
+        train_set->getAllRegisteredRowLeave(leave_id, col, registered_row,
+                                            registered_target_weight,
+                                            registered_value,
+                                            missing_leave,
+                                            left_leave,
+                                            right_leave, candidate);
+        PLASSERT(registered_row.size()==leave->length());
+        PLASSERT(candidate.size()>0);
+
+#endif
+
         missing_leave->getOutputAndError(tmp_vec, missing_error);
         tuple<real,real,int> ret=bestSplitInRow(col, candidate, left_error,
                                                 right_error, missing_error,

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-03 17:43:08 UTC (rev 10081)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-03 20:50:29 UTC (rev 10082)
@@ -40,6 +40,7 @@
  ********************************************************************************** */
 
 #include "RegressionTreeRegisters.h"
+#include "RegressionTreeLeave.h"
 #define PL_LOG_MODULE_NAME RegressionTreeRegisters
 #include <plearn/io/pl_log.h>
 #include <plearn/vmat/TransposeVMatrix.h>
@@ -219,6 +220,99 @@
     //in case we don't save the sorted data
     sortRows();
 }
+
+void RegressionTreeRegisters::getAllRegisteredRowLeave(
+    RTR_type_id leave_id, int col,
+    TVec<RTR_type> &reg,
+    TVec<pair<RTR_target_t,RTR_weight_t> > &t_w,
+    Vec &value,
+    PP<RegressionTreeLeave> missing_leave,
+    PP<RegressionTreeLeave> left_leave,
+    PP<RegressionTreeLeave> right_leave,
+    TVec<RTR_type> &candidate) const
+{
+    PLASSERT(tsource_mat.length()==tsource.length());
+
+    getAllRegisteredRow(leave_id,col,reg);
+    t_w.resize(reg.length());
+    value.resize(reg.length());
+    real * p = tsource_mat[col];
+    pair<RTR_target_t,RTR_weight_t> * ptw = target_weight.data();
+    pair<RTR_target_t,RTR_weight_t>* ptwd = t_w.data();
+    real * pv = value.data();
+    RTR_type * preg = reg.data();
+
+    //It is better to do multiple pass for memory access.
+
+    //we do this optimization in case their is many row with the same value
+    //at the end as with binary variable.
+    //we do it here to overlap computation and memory access
+    int row_idx_end = reg.size() - 1;
+    int prev_row=preg[row_idx_end];
+    real prev_val=p[prev_row];
+    PLASSERT(reg.size()>row_idx_end && row_idx_end>=0);
+    PLASSERT(p[prev_row]==tsource(col,prev_row));
+
+    for( ;row_idx_end>0;row_idx_end--)
+    {
+        int futur_row = preg[row_idx_end-8];
+        __builtin_prefetch(&ptw[futur_row],1,2);
+        __builtin_prefetch(&p[futur_row],1,2);
+
+        int row=prev_row;
+        real val=prev_val;
+        prev_row = preg[row_idx_end-1];
+        prev_val = p[prev_row];
+
+        PLASSERT(reg.size()>row_idx_end && row_idx_end>0);
+        PLASSERT(target_weight.size()>row && row>=0);
+        PLASSERT(p[row]==tsource(col,row));
+        RTR_target_t target = ptw[row].first;
+        RTR_weight_t weight = ptw[row].second;
+
+        if (is_missing(val))
+            missing_leave->addRow(row, target, weight);
+        else if(val==prev_val)
+            right_leave->addRow(row, target, weight);
+        else
+            break;
+    }
+
+    //We need the last data for an optimization in RTN
+    {
+        int idx=reg.size()-1;
+        PLASSERT(reg.size()>idx && idx>=0);
+        int row=int(preg[idx]);
+        PLASSERT(target_weight.size()>row && row>=0);
+        PLASSERT(p[row]==tsource(col,row));
+        pv[idx]=p[row];
+    }
+        for(int row_idx = 0;row_idx<=row_idx_end;row_idx++)
+        {
+            int futur_row = preg[row_idx+8];
+            __builtin_prefetch(&ptw[futur_row],1,2);
+            __builtin_prefetch(&p[futur_row],1,2);
+            
+            PLASSERT(reg.size()>row_idx && row_idx>=0);
+            int row=int(preg[row_idx]);
+            real val=p[row];
+            PLASSERT(target_weight.size()>row && row>=0);
+            PLASSERT(p[row]==tsource(col,row));
+            ptwd[row_idx].first=ptw[row].first;
+            ptwd[row_idx].second=ptw[row].second;
+            pv[row_idx]=val;
+            
+            RTR_target_t target = ptw[row].first;
+            RTR_weight_t weight = ptw[row].second;
+            if (is_missing(val)){
+                missing_leave->addRow(row, target, weight);
+            }else {
+                left_leave->addRow(row, target, weight);
+                candidate.append(row);
+            }
+        }
+}
+
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg,
                                                   TVec<pair<RTR_target_t,RTR_weight_t> > &t_w,
@@ -368,9 +462,8 @@
         PLearn::save(f,tsorted_row);
     }else{
     }
-
 }
-  
+                                                  
 void RegressionTreeRegisters::sortEachDim(int dim)
 {
     PLCHECK_MSG(tsource->classname()=="MemoryVMatrixNoSave",tsource->classname().c_str());

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-03 17:43:08 UTC (rev 10081)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-03 20:50:29 UTC (rev 10082)
@@ -68,10 +68,11 @@
 namespace PLearn {
 using namespace std;
 
+class RegressionTreeLeave;
 class RegressionTreeRegisters: public VMatrix
 {
     typedef VMatrix inherited;
-  
+    
 private:
 
 /*
@@ -98,6 +99,7 @@
 
     bool do_sort_rows;
     bool mem_tsource;
+
     mutable vector<bool> compact_reg;
     mutable int compact_reg_leave;
 
@@ -141,6 +143,13 @@
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg)const;
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
                                      TVec<pair<RTR_target_t,RTR_weight_t> > &t_w, Vec &value)const;
+    void         getAllRegisteredRowLeave(
+        RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
+        TVec<pair<RTR_target_t,RTR_weight_t> > &t_w, Vec &value,
+        PP<RegressionTreeLeave> missing_leave,
+        PP<RegressionTreeLeave> left_leave,
+        PP<RegressionTreeLeave> right_leave,
+        TVec<RTR_type> &candidate)const;
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
     inline virtual void put(int i, int j, real value)
@@ -154,7 +163,7 @@
     
     //! usefull in MultiClassAdaBoost to save memory
     inline TMat<RTR_type> getTSortedRow(){return tsorted_row;}
-    inline VMat   getTSource(){return tsource;}
+    inline VMat  getTSource(){return tsource;}
     virtual void finalize(){tsorted_row = TMat<RTR_type>();}
 
 private:



From chapados at mail.berlios.de  Sat Apr  4 22:33:20 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 4 Apr 2009 22:33:20 +0200
Subject: [Plearn-commits] r10083 - trunk/plearn/ker
Message-ID: <200904042033.n34KXK4o017714@sheep.berlios.de>

Author: chapados
Date: 2009-04-04 22:33:18 +0200 (Sat, 04 Apr 2009)
New Revision: 10083

Modified:
   trunk/plearn/ker/Kernel.cc
   trunk/plearn/ker/Kernel.h
   trunk/plearn/ker/LinearARDKernel.cc
   trunk/plearn/ker/LinearARDKernel.h
   trunk/plearn/ker/MemoryCachedKernel.h
   trunk/plearn/ker/NeuralNetworkARDKernel.cc
   trunk/plearn/ker/NeuralNetworkARDKernel.h
   trunk/plearn/ker/SquaredExponentialARDKernel.cc
   trunk/plearn/ker/SquaredExponentialARDKernel.h
Log:
Some updates to Kernel interface to speed up computation of Gram matrix with test data

Modified: trunk/plearn/ker/Kernel.cc
===================================================================
--- trunk/plearn/ker/Kernel.cc	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/Kernel.cc	2009-04-04 20:33:18 UTC (rev 10083)
@@ -128,10 +128,17 @@
 
     declareMethod(
         rmm, "evaluate", &Kernel::evaluate,
-        (BodyDoc("evaluate the kernel on two vectors\n"),
+        (BodyDoc("Evaluate the kernel on two vectors\n"),
          ArgDoc("x1","first vector"),
          ArgDoc("x2","second vector"),
          RetDoc ("K(x1,x2)")));
+
+    declareMethod(
+        rmm, "setDataForKernelMatrix", &Kernel::setDataForKernelMatrix,
+        (BodyDoc("This method sets the data VMat that will be used to define the kernel\n"
+                 "matrix. It may precompute values from this that may later accelerate\n"
+                 "the evaluation of a kernel matrix element\n"),
+         ArgDoc("data", "The data matrix to set into the kernel")));
 }
 
 ///////////
@@ -407,6 +414,93 @@
     computeGramMatrix(K);
     return K;
 }
+
+
+//////////////////////////////
+// computePartialGramMatrix //
+//////////////////////////////
+void Kernel::computePartialGramMatrix(const TVec<int>& subset_indices, Mat K) const
+{
+    if (!data)
+        PLERROR("Kernel::computePartialGramMatrix should be called only after setDataForKernelMatrix");
+    if (!is_symmetric)
+        PLERROR("In Kernel::computePartialGramMatrix - Currently not implemented for non-symmetric kernels");
+    if (K.length() != subset_indices.length() || K.width() != subset_indices.length())
+        PLERROR("Kernel::computePartialGramMatrix: the argument matrix K should be\n"
+                "of size %d x %d (currently of size %d x %d)",
+                subset_indices.length(), subset_indices.length(), K.length(), K.width());
+
+    int l=subset_indices.size();
+    int m=K.mod();
+    PP<ProgressBar> pb;
+    int count = 0;
+    if (report_progress)
+        pb = new ProgressBar("Computing Partial Gram matrix for " + classname(),
+                             (l * (l + 1)) / 2);
+    real Kij;
+    real* Ki;
+    real* Kji_;
+    for (int i=0;i<l;i++)
+    {
+        int index_i = subset_indices[i];
+        Ki = K[i];
+        Kji_ = &K[0][i];
+        for (int j=0; j<=i; j++,Kji_+=m)
+        {
+            int index_j = subset_indices[j];
+            Kij = evaluate_i_j(index_i, index_j);
+            *Ki++ = Kij;
+            if (j<i)
+                *Kji_ = Kij;
+        }
+        if (report_progress) {
+            count += i + 1;
+            PLASSERT( pb.isNotNull() );
+            pb->update(count);
+        }
+    }
+}
+
+///////////////////////////
+// computeTestGramMatrix //
+///////////////////////////
+void Kernel::computeTestGramMatrix(Mat test_elements, Mat K, Vec self_cov) const
+{
+    if (!data)
+        PLERROR("Kernel::computeTestGramMatrix should be called only after setDataForKernelMatrix");
+
+    if (test_elements.width() != data.width())
+        PLERROR("Kernel::computeTestGramMatrix: the input matrix test_elements "
+                "should be of width %d (currently of width %d)",
+                data.width(), test_elements.width());
+    
+    if (K.length() != test_elements.length() || K.width() != data.length())
+        PLERROR("Kernel::computeTestGramMatrix: the output matrix K should be\n"
+                "of size %d x %d (currently of size %d x %d)",
+                test_elements.length(), data.length(), K.length(), K.width());
+
+    if (self_cov.size() != test_elements.length())
+        PLERROR("Kernel::computeTestGramMatrix: the output vector self_cov should be\n"
+                "of length %d (currently of length %d)",
+                test_elements.length(), self_cov.size());
+
+    int n=test_elements.length();
+    PP<ProgressBar> pb = report_progress?
+        new ProgressBar("Computing Test Gram matrix for " + classname(), n)
+        : 0;
+
+    for (int i=0 ; i<n ; ++i)
+    {
+        Vec cur_test_elem = test_elements(i);
+        evaluate_all_i_x(cur_test_elem, K(i));
+        self_cov[i] = evaluate(cur_test_elem, cur_test_elem);
+        
+        if (pb)
+            pb->update(i);
+    }
+}
+
+
 /////////////////////////////
 // computeSparseGramMatrix //
 /////////////////////////////

Modified: trunk/plearn/ker/Kernel.h
===================================================================
--- trunk/plearn/ker/Kernel.h	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/Kernel.h	2009-04-04 20:33:18 UTC (rev 10083)
@@ -160,6 +160,28 @@
     virtual Mat returnComputedGramMatrix() const;
 
     /**
+     *  Compute a partial Gram matrix between all pairs of a subset of elements
+     *  (length M) of the Kernel data matrix.  This is stored in matrix K
+     *  (assumed to be preallocated to M x M).  The subset_indices should be
+     *  SORTED in order of increasing indices.
+     */
+    virtual void computePartialGramMatrix(const TVec<int>& subset_indices,
+                                          Mat K) const;
+    
+    /**
+     *  Compute a cross-covariance matrix between the given test elements
+     *  (length M) and the elements of the Kernel data matrix (length N).
+     *  This is stored in matrix K (assumed to be preallocated to M x N).
+     *  The self-covariance of the test elements is further stored in the
+     *  vector test_cov (assumed to be preallocated to size M).  (This is
+     *  useful for some kernels used in Gaussian Processes, since there is a
+     *  difference between vector equality and vector identity regarding how
+     *  sampling noise is treated.)
+     */
+    virtual void computeTestGramMatrix(Mat test_elements,
+                                       Mat K, Vec self_cov) const;
+    
+    /**
      *  Fill K[i] with the non-zero elements of row i of the Gram matrix.
      *  Specifically, K[i] is a (k_i x 2) matrix where k_i is the number of
      *  non-zero elements in the i-th row of the Gram matrix, and K[i](k) =

Modified: trunk/plearn/ker/LinearARDKernel.cc
===================================================================
--- trunk/plearn/ker/LinearARDKernel.cc	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/LinearARDKernel.cc	2009-04-04 20:33:18 UTC (rev 10083)
@@ -2,7 +2,7 @@
 
 // LinearARDKernel.cc
 //
-// Copyright (C) 2007 Nicolas Chapados
+// Copyright (C) 2007-2009 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -255,6 +255,16 @@
 }
 
 
+//#####  evaluate_all_i_x  ####################################################
+
+void LinearARDKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                       real squared_norm_of_x, int istart) const
+{
+    evaluateAllIXNV<LinearARDKernel>(x, k_xi_x, istart);
+}
+
+
+
 //#####  derivIspSignalSigma  #################################################
 
 real LinearARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const

Modified: trunk/plearn/ker/LinearARDKernel.h
===================================================================
--- trunk/plearn/ker/LinearARDKernel.h	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/LinearARDKernel.h	2009-04-04 20:33:18 UTC (rev 10083)
@@ -2,7 +2,7 @@
 
 // LinearARDKernel.h
 //
-// Copyright (C) 2007 Nicolas Chapados
+// Copyright (C) 2007-2009 Nicolas Chapados
 //
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -107,7 +107,11 @@
     virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
                                              real epsilon=1e-6) const;
     
+    //! Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
 
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn/ker/MemoryCachedKernel.h
===================================================================
--- trunk/plearn/ker/MemoryCachedKernel.h	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/MemoryCachedKernel.h	2009-04-04 20:33:18 UTC (rev 10083)
@@ -173,6 +173,13 @@
               real (DerivedClass::*derivativeFunc)(int, int, int, real) const>
     void computeGramMatrixDerivNV(Mat& KD, const DerivedClass* This, int arg,
                                   bool derivative_func_requires_K = true) const;
+
+    /**
+     *  Interface to ease derived-class implementation of evaluate_all_i_x
+     *  that avoids virtual function calls in kernel evaluation.
+     */
+    template <class DerivedClass>
+    void evaluateAllIXNV(const Vec& x, const Vec& k_xi_x, int istart) const;
     
     
 private:
@@ -314,7 +321,26 @@
 }
 
 
+//#####  evaluateAllIXNV  #####################################################
 
+template <class DerivedClass>
+void MemoryCachedKernel::evaluateAllIXNV(const Vec& x, const Vec& k_xi_x, int istart) const
+{
+    if (!data)
+        PLERROR("Kernel::computeGramMatrix: setDataForKernelMatrix not yet called");
+
+    const DerivedClass* This = static_cast<const DerivedClass*>(this);
+    int l = min(data->length(), k_xi_x.size());
+    Vec row_i;
+    real* k_xi = &k_xi_x[0];
+    
+    for (int i=istart ; i<l ; ++i) {
+        dataRow(i, row_i);
+        *k_xi++ = This->DerivedClass::evaluate(row_i, x);
+    }
+}
+
+
 } // end of namespace PLearn
 
 #endif

Modified: trunk/plearn/ker/NeuralNetworkARDKernel.cc
===================================================================
--- trunk/plearn/ker/NeuralNetworkARDKernel.cc	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/NeuralNetworkARDKernel.cc	2009-04-04 20:33:18 UTC (rev 10083)
@@ -289,6 +289,15 @@
 }
 
 
+//#####  evaluate_all_i_x  ####################################################
+
+void NeuralNetworkARDKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                              real squared_norm_of_x, int istart) const
+{
+    evaluateAllIXNV<NeuralNetworkARDKernel>(x, k_xi_x, istart);
+}
+
+
 //#####  derivIspGlobalSigma  #################################################
 
 real NeuralNetworkARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const

Modified: trunk/plearn/ker/NeuralNetworkARDKernel.h
===================================================================
--- trunk/plearn/ker/NeuralNetworkARDKernel.h	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/NeuralNetworkARDKernel.h	2009-04-04 20:33:18 UTC (rev 10083)
@@ -106,7 +106,11 @@
     virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
                                              real epsilon=1e-6) const;
     
+    //! Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
 
+    
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.cc
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.cc	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.cc	2009-04-04 20:33:18 UTC (rev 10083)
@@ -272,6 +272,15 @@
 }
 
 
+//#####  evaluate_all_i_x  ####################################################
+
+void SquaredExponentialARDKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                                   real squared_norm_of_x, int istart) const
+{
+    evaluateAllIXNV<SquaredExponentialARDKernel>(x, k_xi_x, istart);
+}
+
+
 //#####  derivIspSignalSigma  #################################################
 
 real SquaredExponentialARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const

Modified: trunk/plearn/ker/SquaredExponentialARDKernel.h
===================================================================
--- trunk/plearn/ker/SquaredExponentialARDKernel.h	2009-04-03 20:50:29 UTC (rev 10082)
+++ trunk/plearn/ker/SquaredExponentialARDKernel.h	2009-04-04 20:33:18 UTC (rev 10083)
@@ -112,7 +112,11 @@
     virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
                                              real epsilon=1e-6) const;
     
+    //! Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
 
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From chapados at mail.berlios.de  Sat Apr  4 22:34:03 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 4 Apr 2009 22:34:03 +0200
Subject: [Plearn-commits] r10084 - trunk/plearn/ker
Message-ID: <200904042034.n34KY3vm017752@sheep.berlios.de>

Author: chapados
Date: 2009-04-04 22:34:02 +0200 (Sat, 04 Apr 2009)
New Revision: 10084

Added:
   trunk/plearn/ker/Matern1ARDKernel.cc
   trunk/plearn/ker/Matern1ARDKernel.h
Log:
New Matern kernel with nu=1/2 corresponding to Ornstein-Uhlenbeck process

Added: trunk/plearn/ker/Matern1ARDKernel.cc
===================================================================
--- trunk/plearn/ker/Matern1ARDKernel.cc	2009-04-04 20:33:18 UTC (rev 10083)
+++ trunk/plearn/ker/Matern1ARDKernel.cc	2009-04-04 20:34:02 UTC (rev 10084)
@@ -0,0 +1,403 @@
+// -*- C++ -*-
+
+// Matern1ARDKernel.cc
+//
+// Copyright (C) 2009 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file Matern1ARDKernel.cc */
+
+
+#include "Matern1ARDKernel.h"
+#include <plearn/math/pl_math.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    Matern1ARDKernel,
+    "Matern kernel with nu=1/2 that can be used for Automatic Relevance Determination.",
+    "With nu=1/2, the Matern kernel corresponds to the Ornstein-Uhlenbeck\n"
+    "process.  This function is specified as:\n"
+    "\n"
+    "  k(x,y) = (sf / (2*a)) * exp(-a sum_i |x_i - y_i|/w_i) * k_kron(x,y)\n"
+    "\n"
+    "where sf = softplus(isp_signal_sigma), a = softplus(isp_persistence), w_i =\n"
+    "softplus(isp_global_sigma + isp_input_sigma[i]), and k_kron(x,y) is the\n"
+    "result of the KroneckerBaseKernel evaluation, or 1.0 if there are no\n"
+    "Kronecker terms.  Note that since the Kronecker terms are incorporated\n"
+    "multiplicatively, the very presence of the term associated to this kernel\n"
+    "can be gated by the value of some input variable(s) (that are incorporated\n"
+    "within one or more Kronecker terms).\n"
+    "\n"
+    "Note that to make its operations more robust when used with unconstrained\n"
+    "optimization of hyperparameters, all hyperparameters of this kernel are\n"
+    "specified in the inverse softplus domain.  See IIDNoiseKernel for more\n"
+    "explanations.\n"
+    );
+
+
+Matern1ARDKernel::Matern1ARDKernel()
+    : m_isp_persistence(pl_log(exp(1.0) - 1.)) // inverse-softplus(1.0)
+{ }
+
+
+//#####  declareOptions  ######################################################
+
+void Matern1ARDKernel::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "isp_persistence",
+        &Matern1ARDKernel::m_isp_persistence,
+        OptionBase::buildoption,
+        "Inverse softplus of the O-U persistence parameter.  Default value =\n"
+        "isp(1.0).");
+    
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void Matern1ARDKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void Matern1ARDKernel::build_()
+{
+    // Ensure that we multiply in Kronecker terms
+    inherited::m_default_value = 1.0;
+}
+
+
+//#####  evaluate  ############################################################
+
+real Matern1ARDKernel::evaluate(const Vec& x1, const Vec& x2) const
+{
+    PLASSERT( x1.size() == x2.size() );
+    PLASSERT( !m_isp_input_sigma.size() || x1.size() == m_isp_input_sigma.size() );
+
+    real gating_term = inherited::evaluate(x1,x2);
+    if (fast_is_equal(gating_term, 0.0))
+        return 0.0;
+    
+    if (x1.size() == 0)
+        return softplus(m_isp_signal_sigma) /
+            (2*softplus(m_isp_persistence)) * gating_term;
+    
+    const real* px1 = x1.data();
+    const real* px2 = x2.data();
+    real sf         = softplus(m_isp_signal_sigma);
+    real persistence= softplus(m_isp_persistence);
+    real expval     = 0.0;
+
+    // Case where we have real ARD
+    if (m_isp_input_sigma.size() > 0) {
+        const real* pinpsig = m_isp_input_sigma.data();
+        for (int i=0, n=x1.size() ; i<n ; ++i) {
+            real diff    = *px1++ - *px2++;
+            real absdiff = fabs(diff);
+            expval      += absdiff / softplus(m_isp_global_sigma + *pinpsig++);
+        }
+    }
+    // No ARD
+    else {
+        real global_sigma = softplus(m_isp_global_sigma);
+        for (int i=0, n=x1.size() ; i<n ; ++i) {
+            real diff    = *px1++ - *px2++;
+            real absdiff = fabs(diff);
+            expval      += absdiff / global_sigma;
+        }
+    }
+
+    // Gate by Kronecker term
+    return sf / (2. * persistence) * exp(-persistence * expval) * gating_term;
+}
+
+
+//#####  computeGramMatrix  ###################################################
+
+void Matern1ARDKernel::computeGramMatrix(Mat K) const
+{
+    PLASSERT( !m_isp_input_sigma.size() || dataInputsize() == m_isp_input_sigma.size() );
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+
+    // Compute Kronecker gram matrix
+    inherited::computeGramMatrix(K);
+
+    // Precompute some terms. Make sure that the input sigmas don't get too
+    // small
+    real sf          = softplus(m_isp_signal_sigma);
+    real persistence = softplus(m_isp_persistence);
+    m_input_sigma.resize(dataInputsize());
+    softplusFloor(m_isp_global_sigma, 1e-6);
+    m_input_sigma.fill(m_isp_global_sigma);  // Still in ISP domain
+    for (int i=0, n=m_input_sigma.size() ; i<n ; ++i) {
+        if (m_isp_input_sigma.size() > 0) {
+            softplusFloor(m_isp_input_sigma[i], 1e-6);
+            m_input_sigma[i] += m_isp_input_sigma[i];
+        }
+        m_input_sigma[i] = softplus(m_input_sigma[i]);
+    }
+
+    // Compute Gram Matrix
+    int  l = data->length();
+    int  m = K.mod();
+    int  n = dataInputsize();
+    int  cache_mod = m_data_cache.mod();
+
+    real *data_start = &m_data_cache(0,0);
+    real *Ki = K[0];                         // Start of current row
+    real *Kij;                               // Current element along row
+    real *input_sigma_data = m_input_sigma.data();
+    real *xi = data_start;
+    
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, Ki+=m)
+    {
+        Kij = Ki;
+        real *xj = data_start;
+
+        for (int j=0; j<=i; ++j, xj += cache_mod) {
+            // Kernel evaluation per se
+            real *x1 = xi;
+            real *x2 = xj;
+            real *p_inpsigma = input_sigma_data;
+            real sum_wt = 0.0;
+            int  k = n;
+
+            // Use Duff's device to unroll the following loop:
+            //     while (k--) {
+            //         real diff = *x1++ - *x2++;
+            //         sum_wt += fabs(diff) / *p_inpsigma++;
+            //     }
+            real diff;
+            switch (k % 8) {
+            case 0: do { diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 7:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 6:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 5:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 4:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 3:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 2:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+            case 1:      diff = *x1++ - *x2++; sum_wt += fabs(diff) / *p_inpsigma++;
+                       } while((k -= 8) > 0);
+            }
+
+            // Multiplicatively update kernel matrix (already pre-filled with
+            // Kronecker terms, or 1.0 if no Kronecker terms, as per build_).
+            real Kij_cur = *Kij * sf / (2.*persistence) * exp(-persistence * sum_wt);
+            *Kij++ = Kij_cur;
+        }
+    }
+    if (cache_gram_matrix) {
+        gram_matrix.resize(l,l);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
+    }
+}
+
+
+//#####  computeGramMatrixDerivative  #########################################
+
+void Matern1ARDKernel::computeGramMatrixDerivative(
+    Mat& KD, const string& kernel_param, real epsilon) const
+{
+    static const string ISS("isp_signal_sigma");
+    static const string IGS("isp_global_sigma");
+    static const string IIS("isp_input_sigma[");
+    static const string IPe("isp_persistence");
+
+    if (kernel_param == ISS) {
+        computeGramMatrixDerivIspSignalSigma(KD);
+        
+        // computeGramMatrixDerivNV<
+        //     Matern1ARDKernel,
+        //     &Matern1ARDKernel::derivIspSignalSigma>(KD, this, -1);
+    }
+    /*
+    else if (kernel_param == IGS) {
+        computeGramMatrixDerivNV<
+            Matern1ARDKernel,
+            &Matern1ARDKernel::derivIspGlobalSigma>(KD, this, -1);
+    }
+    else if (string_begins_with(kernel_param, IIS) &&
+             kernel_param[kernel_param.size()-1] == ']')
+    {
+        int arg = tolong(kernel_param.substr(
+                             IIS.size(), kernel_param.size() - IIS.size() - 1));
+        PLASSERT( arg < m_isp_input_sigma.size() );
+
+        computeGramMatrixDerivIspInputSigma(KD, arg);
+
+    }
+    */
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+}
+
+
+//#####  evaluate_all_i_x  ####################################################
+
+void Matern1ARDKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                        real squared_norm_of_x, int istart) const
+{
+    evaluateAllIXNV<Matern1ARDKernel>(x, k_xi_x, istart);
+}
+
+
+//#####  derivIspSignalSigma  #################################################
+
+real Matern1ARDKernel::derivIspSignalSigma(int i, int j, int arg, real K) const
+{
+    // (No longer used; see computeGramMatrixDerivIspInputSigma below)
+    return K*sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
+//#####  derivIspGlobalSigma  #################################################
+
+real Matern1ARDKernel::derivIspGlobalSigma(int i, int j, int arg, real K) const
+{
+    if (fast_is_equal(K,0.))
+        return 0.;
+
+    // The norm term inside the exponential may be accessed as Log(K/sf)
+    real inner = pl_log(K / softplus(m_isp_signal_sigma));
+    return - K * inner * sigmoid(m_isp_global_sigma) / softplus(m_isp_global_sigma);
+
+    // Note: in the above expression for 'inner' there is the implicit
+    // assumption that the input_sigma[i] are zero, which allows the
+    // sigmoid/softplus term to be factored out of the norm summation.
+}
+
+
+//#####  computeGramMatrixDerivIspSignalSigma  ################################
+
+void Matern1ARDKernel::computeGramMatrixDerivIspSignalSigma(Mat& KD) const
+{
+    int l = data->length();
+    KD.resize(l,l);
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_signal_sigma', the\n"
+        "Gram matrix must be precomputed and cached in Matern1ARDKernel.");
+    
+    KD << gram_matrix;
+    KD *= sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+
+//#####  computeGramMatrixDerivIspInputSigma  #################################
+
+void Matern1ARDKernel::computeGramMatrixDerivIspInputSigma(Mat& KD,
+                                                           int arg) const
+{
+    // Precompute some terms
+    real input_sigma_arg = m_input_sigma[arg];
+    real input_sigma_sq  = input_sigma_arg * input_sigma_arg;
+    real input_sigmoid   = sigmoid(m_isp_global_sigma + m_isp_input_sigma[arg]);
+    
+    // Compute Gram Matrix derivative w.r.t. isp_input_sigma[arg]
+    int  l = data->length();
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_input_sigma[i]', the\n"
+        "Gram matrix must be precomputed and cached in Matern1ARDKernel.");
+
+    // Variables that walk over the data matrix
+    int  cache_mod = m_data_cache.mod();
+    real *data_start = &m_data_cache(0,0);
+    real *xi = data_start+arg;               // Iterator on data rows
+
+    // Variables that walk over the gram cache
+    int   gram_cache_mod = gram_matrix.mod();
+    real *gram_cache_row = gram_matrix.data();
+    real *gram_cache_cur;
+    
+    // Variables that walk over the kernel derivative matrix (KD)
+    KD.resize(l,l);
+    real* KDi = KD.data();                   // Start of row i
+    real* KDij;                              // Current element on row i
+    int   KD_mod = KD.mod();
+
+    // Iterate on rows of derivative matrix
+    for (int i=0 ; i<l ; ++i, xi += cache_mod, KDi += KD_mod,
+             gram_cache_row += gram_cache_mod)
+    {
+        KDij = KDi;
+        real *xj  = data_start+arg;           // Inner iterator on data rows
+        gram_cache_cur = gram_cache_row;
+
+        // Iterate on columns of derivative matrix
+        for (int j=0 ; j <= i
+                 ; ++j, xj += cache_mod, ++gram_cache_cur)
+        {
+            real diff    = *xi - *xj;
+            real sq_diff = diff * diff;
+            real KD_cur  = 0.5 * *gram_cache_cur *
+                           input_sigmoid * sq_diff / input_sigma_sq;
+
+            // Set into derivative matrix
+            *KDij++ = KD_cur;
+        }
+    }
+}
+
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void Matern1ARDKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/Matern1ARDKernel.h
===================================================================
--- trunk/plearn/ker/Matern1ARDKernel.h	2009-04-04 20:33:18 UTC (rev 10083)
+++ trunk/plearn/ker/Matern1ARDKernel.h	2009-04-04 20:34:02 UTC (rev 10084)
@@ -0,0 +1,158 @@
+// -*- C++ -*-
+
+// Matern1ARDKernel.h
+//
+// Copyright (C) 2009 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file Matern1ARDKernel.h */
+
+
+#ifndef MATERN1ARDKERNEL_INC
+#define MATERN1ARDKERNEL_INC
+
+#include <plearn/ker/ARDBaseKernel.h>
+
+namespace PLearn {
+
+/**
+ *  Matern kernel with nu=1/2 that can be used for Automatic Relevance
+ *  Determination.
+ *
+ *  With nu=1/2, the Matern kernel corresponds to the Ornstein-Uhlenbeck
+ *  process.  This function is specified as:
+ *
+ *    k(x,y) = (sf / (2*a)) * exp(-a sum_i |x_i - y_i|/w_i) * k_kron(x,y)
+ *
+ *  where sf = softplus(isp_signal_sigma), a = softplus(isp_persistence), w_i =
+ *  softplus(isp_global_sigma + isp_input_sigma[i]), and k_kron(x,y) is the
+ *  result of the KroneckerBaseKernel evaluation, or 1.0 if there are no
+ *  Kronecker terms.  Note that since the Kronecker terms are incorporated
+ *  multiplicatively, the very presence of the term associated to this kernel
+ *  can be gated by the value of some input variable(s) (that are incorporated
+ *  within one or more Kronecker terms).
+ *
+ *  Note that to make its operations more robust when used with unconstrained
+ *  optimization of hyperparameters, all hyperparameters of this kernel are
+ *  specified in the inverse softplus domain.  See IIDNoiseKernel for more
+ *  explanations.
+ */
+class Matern1ARDKernel : public ARDBaseKernel
+{
+    typedef ARDBaseKernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    /// Inverse softplus of the O-U persistence parameter.  Default value =
+    /// isp(1.0).
+    mutable real m_isp_persistence;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    /// Default constructor
+    Matern1ARDKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    /// Compute K(x1,x2).
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+
+    /// Compute the Gram Matrix.
+    virtual void computeGramMatrix(Mat K) const;
+    
+    /// Directly compute the derivative with respect to hyperparameters
+    /// (Faster than finite differences...)
+    virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
+                                             real epsilon=1e-6) const;
+    
+    /// Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(Matern1ARDKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    /// Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    /// Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    /// Derivative function with respect to isp_signal_sigma
+    real derivIspSignalSigma(int i, int j, int arg, real K) const;
+
+    /// Derivative function with respect to isp_global_sigma
+    real derivIspGlobalSigma(int i, int j, int arg, real K) const;
+    
+    /// Compute derivative w.r.t. isp_persistence
+    void derivIspPersistence(int i, int j, int arg, real K) const;
+    
+    /// Compute derivative w.r.t. isp_signal_sigma for WHOLE MATRIX
+    void computeGramMatrixDerivIspSignalSigma(Mat& KD) const;
+    
+    /// Compute derivative w.r.t. isp_input_sigma[arg] for WHOLE MATRIX
+    void computeGramMatrixDerivIspInputSigma(Mat& KD, int arg) const;
+
+private:
+    /// This does the actual building.
+    void build_();
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(Matern1ARDKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From chapados at mail.berlios.de  Sat Apr  4 22:34:57 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 4 Apr 2009 22:34:57 +0200
Subject: [Plearn-commits] r10085 - trunk/plearn_learners/regressors
Message-ID: <200904042034.n34KYvvg017813@sheep.berlios.de>

Author: chapados
Date: 2009-04-04 22:34:56 +0200 (Sat, 04 Apr 2009)
New Revision: 10085

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Implemented the Projected-Process sparse approximation in GaussianProcess -- this allows much larger training sets to be used

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-04 20:34:02 UTC (rev 10084)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-04 20:34:56 UTC (rev 10085)
@@ -2,7 +2,7 @@
 
 // GaussianProcessRegressor.cc
 //
-// Copyright (C) 2006-2007 Nicolas Chapados 
+// Copyright (C) 2006-2009 Nicolas Chapados 
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -52,12 +52,16 @@
 #include <plearn/opt/Optimizer.h>
 #include <plearn/io/pl_log.h>
 
+#ifdef USE_BLAS_SPECIALISATIONS
+#include <plearn/math/plapack.h>
+#endif
+
 namespace PLearn {
 using namespace std;
 
 PLEARN_IMPLEMENT_OBJECT(
     GaussianProcessRegressor,
-    "Kernelized version of linear ridge regression.",
+    "Implements Gaussian Process Regression (GPR) with an arbitrary kernel",
     "Given a kernel K(x,y) = phi(x)'phi(y), where phi(x) is the projection of a\n"
     "vector x into feature space, this class implements a version of the ridge\n"
     "estimator, giving the prediction at x as\n"
@@ -100,6 +104,18 @@
     "number of training examples (due to the matrix inversion).  When saving the\n"
     "learner, the training set inputs must be saved, along with an additional\n"
     "matrix of length number-of-training-examples, and width number-of-targets.\n"
+    "\n"
+    "To alleviate the computational bottleneck of the exact method, the sparse\n"
+    "approximation method of Projected Process is also available.  This method\n"
+    "requires identifying M datapoints in the training set called the active\n"
+    "set, although it makes use of all N training points for computing the\n"
+    "likelihood.  The computational complexity of the approach is then O(NM^2).\n"
+    "Note that in the current implementation, hyperparameter optimization is\n"
+    "performed using ONLY the active set (called the \"Subset of Data\" method in\n"
+    "the Rasmussen & Williams book).  Making use of the full set of datapoints\n"
+    "is more computationally expensive and would require substantial updates to\n"
+    "the PLearn Kernel class (to efficiently support asymmetric kernel-matrix\n"
+    "gradient).  This may come later.\n"
     );
 
 GaussianProcessRegressor::GaussianProcessRegressor() 
@@ -107,7 +123,8 @@
       m_include_bias(true),
       m_compute_confidence(false),
       m_confidence_epsilon(1e-8),
-      m_save_gram_matrix(false)
+      m_save_gram_matrix(false),
+      m_solution_algorithm("exact")
 { }
 
 
@@ -189,14 +206,39 @@
         "It is saved in the current expdir under the names 'gram_matrix_N.pmat'\n"
         "where N is an increasing counter.\n");
 
+    declareOption(
+        ol, "solution_algorithm", &GaussianProcessRegressor::m_solution_algorithm,
+        OptionBase::buildoption,
+        "Solution algorithm used for the regression.  If \"exact\", use the exact\n"
+        "Gaussian process solution (requires O(N^3) computation).  If\n"
+        "\"projected-process\", use the PP approximation, which requires O(MN^2)\n"
+        "computation, where M is given by the size of the active training\n"
+        "examples specified by the \"active-set\" option.  Default=\"exact\".\n");
 
+    declareOption(
+        ol, "active_set_indices", &GaussianProcessRegressor::m_active_set_indices,
+        OptionBase::buildoption,
+        "If a sparse approximation algorithm is used (e.g. projected process),\n"
+        "this specifies the indices of the training-set examples which should be\n"
+        "considered to be part of the active set.  Note that these indices must\n"
+        "be SORTED IN INCREASING ORDER and should not contain duplicates.\n");
+
+    
     //#####  Learnt Options  ##################################################
 
     declareOption(
         ol, "alpha", &GaussianProcessRegressor::m_alpha,
         OptionBase::learntoption,
-        "Vector of learned parameters, determined from the equation\n"
-        "    (M + lambda I)^-1 y");
+        "Matrix of learned parameters, determined from the equation\n"
+        "\n"
+        "  (K + lambda I)^-1 y\n"
+        "\n"
+        "(don't forget that y can be a matrix for multivariate output problems)\n"
+        "\n"
+        "In the case of the projected-process approximation, this contains\n"
+        "the result of the equiation\n"
+        "\n"
+        "  (lambda K_mm + K_mn K_nm)^-1 K_mn y\n");
 
     declareOption(
         ol, "gram_inverse", &GaussianProcessRegressor::m_gram_inverse,
@@ -204,9 +246,18 @@
         "Inverse of the Gram matrix, used to compute confidence intervals (must\n"
         "be saved since the confidence intervals are obtained from the equation\n"
         "\n"
-        "  sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)\n");
+        "  sigma^2 = k(x,x) - k(x)'(K + lambda I)^-1 k(x)\n"
+        "\n"
+        "An adjustment similar to 'alpha' is made for the projected-process\n"
+        "approximation.\n");
 
     declareOption(
+        ol, "subgram_inverse", &GaussianProcessRegressor::m_subgram_inverse,
+        OptionBase::learntoption,
+        "Inverse of the sub-Gram matrix, i.e. K_mm^-1.  Used only with the\n"
+        "projected-process approximation.\n");
+    
+    declareOption(
         ol, "target_mean", &GaussianProcessRegressor::m_target_mean,
         OptionBase::learntoption,
         "Mean of the targets, if the option 'include_bias' is true");
@@ -215,7 +266,9 @@
         ol, "training_inputs", &GaussianProcessRegressor::m_training_inputs,
         OptionBase::learntoption,
         "Saved version of the training set, which must be kept along for\n"
-        "carrying out kernel evaluations with the test point");
+        "carrying out kernel evaluations with the test point.  If using the\n"
+        "projected-process approximation, only the inputs in the active set are\n"
+        "saved.");
 
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
@@ -244,6 +297,16 @@
 
     if (m_confidence_epsilon < 0)
         PLERROR("GaussianProcessRegressor::build_: 'confidence_epsilon' must be non-negative");
+
+    // Cache solution algorithm in quick form
+    if (m_solution_algorithm == "exact")
+        m_algorithm_enum = AlgoExact;
+    else if (m_solution_algorithm == "projected-process")
+        m_algorithm_enum = AlgoProjectedProcess;
+    else
+        PLERROR("GaussianProcessRegressor::build_: the option solution_algorithm=='%s' "
+                "is not supported.  Value must be in {'exact', 'projected-process'}",
+                m_solution_algorithm.c_str());
 }
 
 // ### Nothing to add here, simply calls build_
@@ -261,8 +324,10 @@
     deepCopyField(m_kernel,                     copies);
     deepCopyField(m_hyperparameters,            copies);
     deepCopyField(m_optimizer,                  copies);
+    deepCopyField(m_active_set_indices,         copies);
     deepCopyField(m_alpha,                      copies);
     deepCopyField(m_gram_inverse,               copies);
+    deepCopyField(m_subgram_inverse,            copies);
     deepCopyField(m_target_mean,                copies);
     deepCopyField(m_training_inputs,            copies);
     deepCopyField(m_kernel_evaluations,         copies);
@@ -322,9 +387,29 @@
     if (!initTrain())
         return;
 
+    // If we use the projected process approximation, make sure that the
+    // active-set indices are specified and that they are sorted in increasing
+    // order
+    if (m_algorithm_enum == AlgoProjectedProcess) {
+        if (m_active_set_indices.size() == 0)
+            PLERROR("GaussianProcessRegressor::train: with the projected-process "
+                    "approximation, the active_set_indices option must be specified.");
+        int last_index = -1;
+        for (int i=0, n=m_active_set_indices.size() ; i<n ; ++i) {
+            int cur_index = m_active_set_indices[i];
+            if (cur_index <= last_index)
+                PLERROR("GaussianProcessRegressor::train: the option active_set_indices "
+                        "must be sorted and should not contain duplicates; at index %d, "
+                        "encounted value %d whereas previous value was %d.",
+                        i, cur_index, last_index);
+            last_index = cur_index;
+        }
+    }
+    
     PLASSERT( m_kernel );
     if (! train_set || ! m_training_inputs)
         PLERROR("GaussianProcessRegressor::train: the training set must be specified");
+    int activelength= m_active_set_indices.size();
     int trainlength = train_set->length();
     int inputsize   = train_set->inputsize() ;
     int targetsize  = train_set->targetsize();
@@ -344,10 +429,25 @@
         targets -= m_target_mean;
     }
 
+    // Determine the subset of training inputs and targets to use depending on
+    // the training algorithm
+    Mat sub_training_inputs;
+    Mat sub_training_targets;
+    if (m_algorithm_enum == AlgoExact) {
+        sub_training_inputs = m_training_inputs;
+        sub_training_targets= targets;
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        sub_training_inputs .resize(activelength, inputsize);
+        sub_training_targets.resize(activelength, targetsize);
+        selectRows(m_training_inputs, m_active_set_indices, sub_training_inputs);
+        selectRows(targets,           m_active_set_indices, sub_training_targets);
+    }
+    
     // Optimize hyperparameters
     VarArray hyperparam_vars;
-    PP<GaussianProcessNLLVariable> nll = hyperOptimize(m_training_inputs, targets,
-                                                       hyperparam_vars);
+    PP<GaussianProcessNLLVariable> nll =
+        hyperOptimize(sub_training_inputs, sub_training_targets, hyperparam_vars);
     PLASSERT( nll );
     
     // Compute parameters.  Be careful to also propagate through the
@@ -355,23 +455,34 @@
     // into their respective kernels.
     hyperparam_vars.fprop();
     nll->fprop();
-    m_alpha = nll->alpha();
-    m_gram_inverse = nll->gramInverse();
+    if (m_algorithm_enum == AlgoExact) {
+        m_alpha = nll->alpha();
+        m_gram_inverse = nll->gramInverse();
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        trainProjectedProcess(m_training_inputs, sub_training_inputs, targets);
 
-    // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
-    // *** FIXME: MSE IS NOT COMPUTED CORRECTLY SINCE GRAM MATRIX IS
-    // OVERWRITTEN BY CHOLESKY DECOMPOSITION ***
-    Mat residuals(m_alpha.length(), m_alpha.width());
-    product(residuals, nll->gram(), m_alpha);
-    residuals -= targets;
-    real mse = dot(residuals, residuals) / (2 * trainlength);
+        // Full training set no longer required from now on
+        m_training_inputs = sub_training_inputs;
+        m_kernel->setDataForKernelMatrix(m_training_inputs);
+    }
+
+    if (getTrainStatsCollector()) {
+        // Compute train statistics by running a test over the training set.
+        // This works uniformly for all solution algorithms, albeit with some
+        // performance hit.
+        PP<VecStatsCollector> test_stats = new VecStatsCollector;
+        test(getTrainingSet(), test_stats);
     
-    // And accumulate some statistics
-    Vec costs(2);
-    costs[0] = nll->value[0];
-    costs[1] = mse;
-    getTrainStatsCollector()->update(costs);
-    MODULE_LOG << "Train NLL: " << costs[0] << endl;
+        // And accumulate some statistics.  Note: the NLL corresponds to the
+        // subset-of-data version if the projected-process approximation is
+        // used.  It is the exact NLL if the exact algorithm is used.
+        Vec costs(3);
+        costs.subVec(0,2) << test_stats->getMean();
+        costs[2] = nll->value[0];
+        getTrainStatsCollector()->update(costs);
+    }
+    MODULE_LOG << "Train marginal NLL (subset-of-data): " << nll->value[0] << endl;
 }
 
 
@@ -400,7 +511,9 @@
     
     m_kernel->evaluate_all_i_x(input, kernel_evaluations);
 
-    // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
+    // Finally compute k(x,x_i) * (K + \lambda I)^-1 y.
+    // This expression does not change depending on whether we are using
+    // the exact algorithm or the projected-process approximation.
     product(Mat(1, output.size(), output),
             Mat(1, kernel_evaluations.size(), kernel_evaluations),
             m_alpha);
@@ -461,15 +574,29 @@
         return false;
     }
 
-    // BIG-BIG assumption: assume that computeOutput has just been called and
-    // that m_kernel_evaluations contains the right stuff.
+    // BIG assumption: assume that computeOutput has just been called and that
+    // m_kernel_evaluations contains the right stuff.
     PLASSERT( m_kernel && m_gram_inverse.isNotNull() );
     real base_sigma_sq = m_kernel(input, input);
     m_gram_inverse_product.resize(m_kernel_evaluations.size());
-    product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
-    real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(real(0.), base_sigma_sq - sigma_reductor + m_confidence_epsilon));
 
+    real sigma;
+    if (m_algorithm_enum == AlgoExact) {
+        product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
+        real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
+        sigma = sqrt(max(real(0.),
+                         base_sigma_sq - sigma_reductor + m_confidence_epsilon));
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        // From R&W eq. (8.27).
+        product(m_gram_inverse_product, m_subgram_inverse, m_kernel_evaluations);
+        productScaleAcc(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations,
+                        -1.0, 1.0);
+        real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
+        sigma = sqrt(max(real(0.),
+                         base_sigma_sq - sigma_reductor + m_confidence_epsilon));
+    }
+
     // two-tailed
     const real multiplier = gauss_01_quantile((1+probability)/2);
     real half_width = multiplier * sigma;
@@ -516,7 +643,7 @@
         has_missings = has_missings || inputs(i).hasMissing();
     }
 
-    // If any missings found in the inputs, don't bother with with computing a
+    // If any missings found in the inputs, don't bother with computing a
     // covariance matrix
     if (has_missings) {
         covmat.fill(MISSING_VALUE);
@@ -527,34 +654,42 @@
     // less lifted from Kernel.cc ==> must see with Olivier how to better
     // factor this code
     Mat& K = covmat;
-    PLASSERT( K.width() == N && K.length() == N );
-    const int mod = K.mod();
-    real Kij;
-    real* Ki;
-    real* Kji;
-    for (int i=0 ; i<N ; ++i) {
-        Ki  = K[i];
-        Kji = &K[0][i];
-        const Vec& cur_input_i = inputs(i);
-        for (int j=0 ; j<=i ; ++j, Kji += mod) {
-            Kij = m_kernel->evaluate(cur_input_i, inputs(j));
-            *Ki++ = Kij;
-            if (j<i)
-                *Kji = Kij;    // Assume symmetry, checked at build
-        }
-    }
-
-    // The predictive covariance matrix is (c.f. Rasmussen and Williams):
+    Vec self_cov(N);
+    m_kernel->computeTestGramMatrix(inputs, K, self_cov);
+    
+    // The predictive covariance matrix is for the exact cast(c.f. Rasmussen
+    // and Williams):
     //
     //    cov(f*) = K(X*,X*) - K(X*,X) [K(X,X) + sigma*I]^-1 K(X,X*)
     //
     // where X are the training inputs, and X* are the test inputs.
+    //
+    // For the projected process case, it is:
+    //
+    //    cov(f*) = K(X*,X*) - K(X*,X_m) K_mm^-1 K(X*,X_m)
+    //               + sigma^2 K(X*,X_m) (sigma^2 K_mm + K_mn K_nm)^-1 K(X*,X_m)
+    //
+    // Note that all sigma^2's have been absorbed into their respective
+    // cached terms, and in particular in this context sigma^2 is emphatically
+    // not equal to the weight decay.
     m_gram_inv_traintest_product.resize(T,N);
     m_sigma_reductor.resize(N,N);
-    productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
-                     m_gram_traintest_inputs);
-    product(m_sigma_reductor, m_gram_traintest_inputs,
-            m_gram_inv_traintest_product);
+
+    if (m_algorithm_enum == AlgoExact) {    
+        productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
+                         m_gram_traintest_inputs);
+        product(m_sigma_reductor, m_gram_traintest_inputs,
+                m_gram_inv_traintest_product);
+    }
+    else if (m_algorithm_enum == AlgoProjectedProcess) {
+        productTranspose(m_gram_inv_traintest_product, m_subgram_inverse,
+                         m_gram_traintest_inputs);
+        productTransposeScaleAcc(m_gram_inv_traintest_product, m_gram_inverse,
+                                 m_gram_traintest_inputs, -1.0, 1.0);
+        product(m_sigma_reductor, m_gram_traintest_inputs,
+                m_gram_inv_traintest_product);
+    }
+    
     covmat -= m_sigma_reductor;
 
     // As a preventive measure, never output negative variance, even though
@@ -577,9 +712,10 @@
 
 TVec<string> GaussianProcessRegressor::getTrainCostNames() const
 {
-    TVec<string> c(2);
-    c[0] = "nmll";
+    TVec<string> c(3);
+    c[0] = "nll";
     c[1] = "mse";
+    c[2] = "marginal-nll";
     return c;
 }
 
@@ -669,6 +805,106 @@
     return nll;
 }
 
+
+//#####  trainProjectedProcess (LAPACK)  ######################################
+
+void GaussianProcessRegressor::trainProjectedProcess(
+    const Mat& all_training_inputs, const Mat& sub_training_inputs,
+    const Mat& all_training_targets)
+{
+    PLASSERT( m_kernel );
+    const int activelength= m_active_set_indices.length();
+    const int trainlength = all_training_inputs.length();
+    const int targetsize  = all_training_targets.width();
+    
+    // The RHS matrix (when solving the linear system Gram*Params=RHS) is made
+    // up of two parts: the regression targets themselves, and the identity
+    // matrix if we requested them (for confidence intervals).  After solving
+    // the linear system, set the gram-inverse appropriately.  To interface
+    // nicely with LAPACK, we store this in a transposed format.
+    int rhs_width = targetsize + (m_compute_confidence? activelength : 0);
+    Mat tmp_rhs(rhs_width, activelength);
+    if (m_compute_confidence) {
+        Mat rhs_identity = tmp_rhs.subMatRows(targetsize, activelength);
+        identityMatrix(rhs_identity);
+    }
+
+    // We always need to solve K_mm^-1.  Prepare the RHS with the identity
+    // matrix to be ready to solve with a Cholesky decomposition.
+    m_subgram_inverse.resize(activelength, activelength);
+    Mat gram_cholesky(activelength, activelength);
+    identityMatrix(m_subgram_inverse);
+    
+    // Compute Gram Matrix and add weight decay to diagonal.  This is done in a
+    // few steps: (1) K_mm (using the active-set only), (2) then separately
+    // compute K_mn (active-set by all examples), (3) computing the covariance
+    // matrix of K_mn to give an m x m matrix, (4) and finally add them up.
+    // cf. R&W p. 179, eq. 8.26 :: (sigma_n^2 K_mm + K_mn K_nm)
+    m_kernel->setDataForKernelMatrix(all_training_inputs);
+    Mat gram(activelength, activelength);
+    Mat asym_gram(activelength, trainlength);
+    Vec self_cov(activelength);
+    m_kernel->computeTestGramMatrix(sub_training_inputs, asym_gram, self_cov);
+    // Note: asym_gram contains K_mn without any sampling noise.
+
+    // DBG_MODULE_LOG << "Asym_gram =\n" << asym_gram << endl;
+    
+    // Obtain K_mm, also without self-noise.  Add some jitter as per
+    // the Rasmussen & Williams code
+    selectColumns(asym_gram, m_active_set_indices, gram);
+    real jitter = m_weight_decay * trace(gram);
+    addToDiagonal(gram, jitter);
+
+    // DBG_MODULE_LOG << "Kmm =\n" << gram << endl;
+    
+    // Obtain an estimate of the EFFECTIVE sampling noise from the
+    // difference between self_cov and the diagonal of gram
+    Vec sigma_sq = self_cov - diag(gram);
+    double sigma_sq_est = mean(sigma_sq);
+    // DBG_MODULE_LOG << "Sigma^2 estimate = " << sigma_sq_est << endl;
+
+    // Before clobbering K_mm, compute its inverse.
+    gram_cholesky << gram;
+    lapackCholeskyDecompositionInPlace(gram_cholesky);
+    lapackCholeskySolveInPlace(gram_cholesky, m_subgram_inverse,
+                               true /* column-major */);
+    
+    gram *= sigma_sq_est;                            // sigma_n^2 K_mm
+    productTransposeAcc(gram, asym_gram, asym_gram); // Inner part of eq. 8.26
+
+    // DBG_MODULE_LOG << "Gram =\n" << gram << endl;
+    
+    // Dump a fragment of the Gram Matrix to the debug log
+    DBG_MODULE_LOG << "Projected-process Gram fragment: "
+                   << gram(0,0) << ' '
+                   << gram(1,0) << ' '
+                   << gram(1,1) << endl;
+
+    // The RHS should contain (K_mn*y)' = y'*K_mn'.  Compute it.
+    Mat targets_submat = tmp_rhs.subMatRows(0, targetsize);
+    transposeTransposeProduct(targets_submat, all_training_targets, asym_gram);
+    // DBG_MODULE_LOG << "Projected RHS =\n" << targets_submat << endl;
+    
+    // Compute Cholesky decomposition and solve the linear system.  LAPACK
+    // solves in-place, but luckily we don't need either the Gram and RHS
+    // matrices after solving.
+    lapackCholeskyDecompositionInPlace(gram);
+    lapackCholeskySolveInPlace(gram, tmp_rhs, true /* column-major */);
+
+    // Transpose final result.  LAPACK solved in-place for tmp_rhs.
+    m_alpha.resize(tmp_rhs.width(), tmp_rhs.length());
+    transpose(tmp_rhs, m_alpha);
+    if (m_compute_confidence) {
+        m_gram_inverse = m_alpha.subMatColumns(targetsize, activelength);
+        m_alpha        = m_alpha.subMatColumns(0, targetsize);
+
+        // Absorb sigma^2 into gram_inverse as per eq. 8.27 of R&W
+        m_gram_inverse *= sigma_sq_est;
+    }
+}
+
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2009-04-04 20:34:02 UTC (rev 10084)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2009-04-04 20:34:56 UTC (rev 10085)
@@ -2,7 +2,7 @@
 
 // GaussianProcessRegressor.h
 //
-// Copyright (C) 2006 Nicolas Chapados 
+// Copyright (C) 2006--2009 Nicolas Chapados 
 // 
 // Redistribution and use in source and binary forms, with or without
 // modification, are permitted provided that the following conditions are met:
@@ -98,6 +98,18 @@
  *  number of training examples (due to the matrix inversion).  When saving the
  *  learner, the training set inputs must be saved, along with an additional
  *  matrix of length number-of-training-examples, and width number-of-targets.
+ *
+ *  To alleviate the computational bottleneck of the exact method, the sparse
+ *  approximation method of Projected Process is also available.  This method
+ *  requires identifying M datapoints in the training set called the active
+ *  set, although it makes use of all N training points for computing the
+ *  likelihood.  The computational complexity of the approach is then O(NM^2).
+ *  Note that in the current implementation, hyperparameter optimization is
+ *  performed using ONLY the active set (called the "Subset of Data" method in
+ *  the Rasmussen & Williams book).  Making use of the full set of datapoints
+ *  is more computationally expensive and would require substantial updates to
+ *  the PLearn Kernel class (to efficiently support asymmetric kernel-matrix
+ *  gradient).  This may come later.
  */
 class GaussianProcessRegressor : public PLearner
 {
@@ -179,7 +191,24 @@
      */
     bool m_save_gram_matrix;
 
+    /**
+     *  Solution algorithm used for the regression.  If "exact", use the exact
+     *  Gaussian process solution (requires O(N^3) computation).  If
+     *  "projected-process", use the PP approximation, which requires O(MN^2)
+     *  computation, where M is given by the size of the active training
+     *  examples specified by the "active-set" option.  Default="exact".
+     */
+    string m_solution_algorithm;
 
+    /**
+     *  If a sparse approximation algorithm is used (e.g. projected process),
+     *  this specifies the indices of the training-set examples which should be
+     *  considered to be part of the active set.  Note that these indices must
+     *  be SORTED IN INCREASING ORDER and should not contain duplicates.
+     */
+    TVec<int> m_active_set_indices;
+    
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -262,15 +291,26 @@
     PP<GaussianProcessNLLVariable> hyperOptimize(
         const Mat& inputs, const Mat& targets, VarArray& hyperparam_vars);
 
+    /// Update the parameters required for the Projected Process approximation,
+    /// assuming hyperparameters have already been optimized.
+    void trainProjectedProcess(const Mat& all_training_inputs,
+                               const Mat& sub_training_inputs,
+                               const Mat& all_training_targets);
+    
 protected:
     //#####  Protected Options  ###############################################
 
     /**
      *  Matrix of learned parameters, determined from the equation
      *
-     *    (M + lambda I)^-1 y
+     *    (K + lambda I)^-1 y
      *
      *  (don't forget that y can be a matrix for multivariate output problems)
+     *
+     *  In the case of the projected-process approximation, this contains
+     *  the result of the equiation
+     *
+     *    (lambda K_mm + K_mn K_nm)^-1 K_mn y
      */
     Mat m_alpha;
 
@@ -278,15 +318,26 @@
      *  Inverse of the Gram matrix, used to compute confidence intervals (must
      *  be saved since the confidence intervals are obtained from the equation
      *
-     *    sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)
+     *    sigma^2 = k(x,x) - k(x)'(K + lambda I)^-1 k(x)
+     *
+     *  An adjustment similar to 'alpha' is made for the projected-process
+     *  approximation.
      */
     Mat m_gram_inverse;
 
+    /**
+     *  Inverse of the sub-Gram matrix, i.e. K_mm^-1.  Used only with the
+     *  projected-process approximation.
+     */
+    Mat m_subgram_inverse;
+    
     /// Mean of the targets, if the option 'include_bias' is true
     Vec m_target_mean;
     
     /// Saved version of the training set inputs, which must be kept along for
-    /// carrying out kernel evaluations with the test point
+    /// carrying out kernel evaluations with the test point.  If using the
+    /// projected-process approximation, only the inputs in the active set are
+    /// saved.
     Mat m_training_inputs;
 
     /// Buffer for kernel evaluations at test time
@@ -307,6 +358,13 @@
 
     //! Buffer to hold the sigma reductor for m_gram_inverse_product
     mutable Mat m_sigma_reductor;
+
+    //! Solution algorithm in enum form to avoid lengthy string-compare
+    //! each time we want to compute a confidence interval
+    enum {
+        AlgoExact,
+        AlgoProjectedProcess
+    } m_algorithm_enum;
     
 private: 
     /// This does the actual building. 



From chapados at mail.berlios.de  Sat Apr  4 22:35:46 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 4 Apr 2009 22:35:46 +0200
Subject: [Plearn-commits] r10086 - trunk/plearn/var
Message-ID: <200904042035.n34KZk4Q017888@sheep.berlios.de>

Author: chapados
Date: 2009-04-04 22:35:46 +0200 (Sat, 04 Apr 2009)
New Revision: 10086

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
   trunk/plearn/var/GaussianProcessNLLVariable.h
Log:
Minor updates and adjustment to deepCopyField

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2009-04-04 20:34:56 UTC (rev 10085)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2009-04-04 20:35:46 UTC (rev 10086)
@@ -128,7 +128,8 @@
     deepCopyField(m_alpha_t,         copies);
     deepCopyField(m_alpha_buf,       copies);
     deepCopyField(m_inverse_gram,    copies);
-    deepCopyField(m_cholesky_gram,   copies);
+    deepCopyField(m_cholesky_tmp,    copies);
+    deepCopyField(m_rhs_tmp,         copies);
 }
 
 void GaussianProcessNLLVariable::declareOptions(OptionList& ol)

Modified: trunk/plearn/var/GaussianProcessNLLVariable.h
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.h	2009-04-04 20:34:56 UTC (rev 10085)
+++ trunk/plearn/var/GaussianProcessNLLVariable.h	2009-04-04 20:35:46 UTC (rev 10086)
@@ -98,6 +98,9 @@
      *  @param allow_bprop: if true, assume we will be performing bprops on the
      *                  Variable; if not, only fprops are allowed.  BProps
      *                  involve computing a full inverse of the Gram matrix
+     *  @param save_gram_matrix: whether the Gram matrix should be saved
+     *                           (useful for debugging)
+     *  @param expdir:  where to save the Gram matrix if required
      */
     GaussianProcessNLLVariable(Kernel* kernel, real noise,
                                Mat inputs, Mat targets,



From nouiz at mail.berlios.de  Mon Apr  6 15:44:40 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 6 Apr 2009 15:44:40 +0200
Subject: [Plearn-commits] r10087 - trunk/plearn_learners/regressors
Message-ID: <200904061344.n36Die67021070@sheep.berlios.de>

Author: nouiz
Date: 2009-04-06 15:44:38 +0200 (Mon, 06 Apr 2009)
New Revision: 10087

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
added macro RTR_HAVE_MISSING that are used in RegressionTree for optimisation when RTR_HAVE_MISSING == false. When their is no missing value in the train set, we can disable some path in the code.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-04-04 20:35:46 UTC (rev 10086)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-04-06 13:44:38 UTC (rev 10087)
@@ -205,7 +205,7 @@
     real conf = 0;
     if(length_>0){
         output[0] = weighted_targets_sum / weights_sum;
-        if (missing_leave != true)
+        if (!RTR_HAVE_MISSING || missing_leave != true)
         {
             //we put the most frequent case first as an optimisation
             conf = 1.0;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-04-04 20:35:46 UTC (rev 10086)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-04-06 13:44:38 UTC (rev 10087)
@@ -225,7 +225,7 @@
             mc_winer = mc_ind;
     }
     output[0] = multiclass_outputs[mc_winer];
-    if (missing_leave)
+    if (RTR_HAVE_MISSING && missing_leave)
     {
         error[0] = 0.0;
         error[1] = weights_sum;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-04 20:35:46 UTC (rev 10086)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 13:44:38 UTC (rev 10087)
@@ -276,7 +276,6 @@
         
         PLASSERT(registered_row.size()==leave->length());
         PLASSERT(candidate.size()==0);
-
 #ifdef NPREFETCH
         //The ifdef is in case we don't want to use the optimized version with
         //prefetch of memory. Maybe the optimization is hurtfull for some computer.
@@ -298,7 +297,7 @@
             real val=prev_val;
             prev_row = registered_row[row_idx_end - 1];
             prev_val = registered_value[row_idx_end - 1];
-            if (is_missing(val))
+            if (RTR_HAVE_MISSING && is_missing(val))
                 missing_leave->addRow(row, registered_target_weight[row_idx_end].first,
                                       registered_target_weight[row_idx_end].second);
             else if(val==prev_val)
@@ -311,7 +310,7 @@
         for(int row_idx = 0;row_idx<=row_idx_end;row_idx++)
         {
             int row=registered_row[row_idx];
-            if (is_missing(registered_value[row_idx]))
+            if (RTR_HAVE_MISSING && is_missing(registered_value[row_idx]))
                 missing_leave->addRow(row, registered_target_weight[row_idx].first,
                                       registered_target_weight[row_idx].second);
             else {
@@ -467,7 +466,7 @@
     for (int row_index = 0;row_index<registered_row.size();row_index++)
     {
         int row=registered_row[row_index];
-        if (is_missing(train_set->get(row, split_col)))
+        if (RTR_HAVE_MISSING && is_missing(train_set->get(row, split_col)))
         {
             missing_leave->addRow(row);
             missing_leave->registerRow(row);
@@ -494,7 +493,7 @@
 //  leave->printStats();
 //  left_leave->printStats();
 //  right_leave->printStats();
-    if (missing_is_valid > 0)
+    if (RTR_HAVE_MISSING && missing_is_valid > 0)
     {
         missing_node = new RegressionTreeNode(missing_is_valid);
         missing_node->initNode(tree, missing_leave);
@@ -519,7 +518,7 @@
         outputv << leave_output;
         return;
     }
-    if (is_missing(inputv[split_col]))
+    if (RTR_HAVE_MISSING && is_missing(inputv[split_col]))
     {
         if (missing_is_valid > 0)
         {

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-04 20:35:46 UTC (rev 10086)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 13:44:38 UTC (rev 10087)
@@ -270,7 +270,7 @@
         RTR_target_t target = ptw[row].first;
         RTR_weight_t weight = ptw[row].second;
 
-        if (is_missing(val))
+        if (RTR_HAVE_MISSING && is_missing(val))
             missing_leave->addRow(row, target, weight);
         else if(val==prev_val)
             right_leave->addRow(row, target, weight);
@@ -304,7 +304,7 @@
             
             RTR_target_t target = ptw[row].first;
             RTR_weight_t weight = ptw[row].second;
-            if (is_missing(val)){
+            if (RTR_HAVE_MISSING && is_missing(val)){
                 missing_leave->addRow(row, target, weight);
             }else {
                 left_leave->addRow(row, target, weight);

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-04 20:35:46 UTC (rev 10086)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 13:44:38 UTC (rev 10087)
@@ -65,6 +65,12 @@
 #define RTR_weight_t real
 #endif
 
+//if you are sur the their is no missing value in the training set
+//you can set its value to false for some speed up.
+#ifndef RTR_HAVE_MISSING
+#define RTR_HAVE_MISSING true
+#endif
+
 namespace PLearn {
 using namespace std;
 



From saintmlx at mail.berlios.de  Mon Apr  6 17:27:42 2009
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 6 Apr 2009 17:27:42 +0200
Subject: [Plearn-commits] r10088 - trunk/plearn/python
Message-ID: <200904061527.n36FRg4M004731@sheep.berlios.de>

Author: saintmlx
Date: 2009-04-06 17:27:42 +0200 (Mon, 06 Apr 2009)
New Revision: 10088

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
- bugfix: throw exception when trying to convert a scalar to a Vec



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2009-04-06 13:44:38 UTC (rev 10087)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2009-04-06 15:27:42 UTC (rev 10088)
@@ -230,13 +230,22 @@
     PLASSERT( pyobj );
     PyObject* pyarr0= PyArray_CheckFromAny(pyobj, NULL,
                                            1, 1, NPY_CARRAY_RO, Py_None);
+    if(!pyarr0)
+    {
+        Py_XDECREF(pyarr0);
+        PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj,
+                                print_traceback);
+    }
     PyObject* pyarr= 
         PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
                            PyArray_DescrFromType(PL_NPY_REAL), 0);
     Py_XDECREF(pyarr0);
-    if (! pyarr)
+    if(!pyarr)
+    {
+        Py_XDECREF(pyarr);
         PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj,
                                 print_traceback);
+    }
     v.resize(PyArray_DIM(pyarr,0));
     v.copyFrom((real*)(PyArray_DATA(pyarr)), PyArray_DIM(pyarr,0));
     Py_XDECREF(pyarr);



From nouiz at mail.berlios.de  Mon Apr  6 20:27:12 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 6 Apr 2009 20:27:12 +0200
Subject: [Plearn-commits] r10089 - trunk/plearn_learners/regressors
Message-ID: <200904061827.n36IRCAL029020@sheep.berlios.de>

Author: nouiz
Date: 2009-04-06 20:27:09 +0200 (Mon, 06 Apr 2009)
New Revision: 10089

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
new optimisation with their is no missing value in the trainset for the regression tree. We make only one pass on the dataset. Need to be make more general


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-04-06 15:27:42 UTC (rev 10088)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-04-06 18:27:09 UTC (rev 10089)
@@ -50,7 +50,8 @@
 class RegressionTreeLeave: public Object
 {
     typedef Object inherited;
- 
+    friend class RegressionTreeNode;
+    friend class RegressionTreeRegisters;
     static Vec dummy_vec;
 
 public:

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 15:27:42 UTC (rev 10088)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 18:27:09 UTC (rev 10089)
@@ -255,6 +255,7 @@
     Vec right_error(3);
     Vec missing_error(3);
     missing_error.clear();
+    bool one_pass_on_data=!RTR_HAVE_MISSING && leave->classname()=="RegressionTreeLeave";
     PP<RegressionTreeRegisters> train_set = tree->getSortedTrainingSet();
 
     int inputsize = train_set->inputsize();
@@ -267,7 +268,13 @@
     row_split_balance.clear();
 #endif
     int leave_id = leave->getId();
-   
+    
+    int l_length = 0;
+    real l_weights_sum = 0;
+    real l_targets_sum = 0;
+    real l_weighted_targets_sum = 0;
+    real l_weighted_squared_targets_sum = 0;
+
     for (int col = 0; col < inputsize; col++)
     {
         missing_leave->initStats();
@@ -276,6 +283,7 @@
         
         PLASSERT(registered_row.size()==leave->length());
         PLASSERT(candidate.size()==0);
+        tuple<real,real,int> ret;
 #ifdef NPREFETCH
         //The ifdef is in case we don't want to use the optimized version with
         //prefetch of memory. Maybe the optimization is hurtfull for some computer.
@@ -320,24 +328,56 @@
             }
         }
 
+        missing_leave->getOutputAndError(tmp_vec, missing_error);
+        ret=bestSplitInRow(col, candidate, left_error,
+                           right_error, missing_error,
+                           right_leave, left_leave,
+                           train_set, registered_value,
+                           registered_target_weight);
+
 #else
-        train_set->getAllRegisteredRowLeave(leave_id, col, registered_row,
-                                            registered_target_weight,
-                                            registered_value,
-                                            missing_leave,
-                                            left_leave,
-                                            right_leave, candidate);
+        if(!one_pass_on_data){
+            train_set->getAllRegisteredRowLeave(leave_id, col, registered_row,
+                                                registered_target_weight,
+                                                registered_value,
+                                                missing_leave,
+                                                left_leave,
+                                                right_leave, candidate);
+            PLASSERT(candidate.size()>0);
+            missing_leave->getOutputAndError(tmp_vec, missing_error);
+            ret=bestSplitInRow(col, candidate, left_error,
+                               right_error, missing_error,
+                               right_leave, left_leave,
+                               train_set, registered_value,
+                               registered_target_weight);
+        }else{
+            ret=train_set->bestSplitInRow(leave_id, col, registered_row,
+                                          missing_leave,
+                                          left_leave,
+                                          right_leave, left_error,
+                                          right_error, missing_error);
+        }
         PLASSERT(registered_row.size()==leave->length());
-        PLASSERT(candidate.size()>0);
-
 #endif
 
-        missing_leave->getOutputAndError(tmp_vec, missing_error);
-        tuple<real,real,int> ret=bestSplitInRow(col, candidate, left_error,
-                                                right_error, missing_error,
-                                                right_leave, left_leave,
-                                                train_set, registered_value,
-                                                registered_target_weight);
+        if(col==0){
+            l_length=left_leave->length()+right_leave->length();
+            l_weights_sum=left_leave->weights_sum+right_leave->weights_sum;
+            l_targets_sum=left_leave->targets_sum+right_leave->targets_sum;
+            l_weighted_targets_sum=left_leave->weighted_targets_sum+right_leave->weighted_targets_sum;
+            l_weighted_squared_targets_sum=left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum;
+        }else if(!one_pass_on_data){
+            PLCHECK(l_length==left_leave->length()+right_leave->length());
+            PLCHECK(fast_is_equal(l_weights_sum,
+                                  left_leave->weights_sum+right_leave->weights_sum));
+            PLCHECK(fast_is_equal(l_targets_sum,
+                                  left_leave->targets_sum+right_leave->targets_sum));
+            PLCHECK(fast_is_equal(l_weighted_targets_sum,
+                                  left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
+            PLCHECK(fast_is_equal(l_weighted_squared_targets_sum,
+                                  left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum));
+        }
+
 #ifdef RCMP
         row_split_err[col] = get<0>(ret);
         row_split_value[col] = get<1>(ret);

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 15:27:42 UTC (rev 10088)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 18:27:09 UTC (rev 10089)
@@ -415,6 +415,165 @@
 
 }
 
+tuple<real,real,int> RegressionTreeRegisters::bestSplitInRow(
+    RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
+    PP<RegressionTreeLeave> missing_leave,
+    PP<RegressionTreeLeave> left_leave,
+    PP<RegressionTreeLeave> right_leave,
+    Vec left_error, Vec right_error,
+    Vec missing_error) const
+{
+//    fill reg, t_w et value and candidate
+    PLCHECK(missing_leave->classname()=="RegressionTreeLeave");
+    PLCHECK(RTR_HAVE_MISSING==false);//need to modif the code to make it work with missing value.
+
+    PLASSERT(tsource_mat.length()==tsource.length());
+    getAllRegisteredRow(leave_id,col,reg);
+    real * p = tsource_mat[col];
+    pair<RTR_target_t,RTR_weight_t>* ptw = target_weight.data();
+    RTR_type * preg = reg.data();
+
+    int row_idx_end = reg.size() - 1;
+    int prev_row=preg[row_idx_end];
+    real prev_val=p[prev_row];
+    PLASSERT(reg.size()>row_idx_end && row_idx_end>=0);
+    PLASSERT(p[prev_row]==tsource(col,prev_row));
+    //fill right_leave
+    for( ;row_idx_end>0;row_idx_end--)
+    {
+        int futur_row = preg[row_idx_end-8];
+        __builtin_prefetch(&ptw[futur_row],1,2);
+        __builtin_prefetch(&p[futur_row],1,2);
+
+        int row=prev_row;
+        real val=prev_val;
+        prev_row = preg[row_idx_end-1];
+        prev_val = p[prev_row];
+
+        PLASSERT(reg.size()>row_idx_end && row_idx_end>0);
+        PLASSERT(target_weight.size()>row && row>=0);
+        PLASSERT(p[row]==tsource(col,row));
+        RTR_target_t target = ptw[row].first;
+        RTR_weight_t weight = ptw[row].second;
+
+        if (RTR_HAVE_MISSING && is_missing(val))
+            missing_leave->addRow(row, target, weight);
+        else if(val==prev_val)
+            right_leave->addRow(row, target, weight);
+        else
+            break;
+    }
+
+    if(col==0){//do 2 pass finding of the best split.
+        //fill left_leave
+        for(int row_idx = 0;row_idx<=row_idx_end;row_idx++)
+        {
+            int futur_row = preg[row_idx+8];
+            __builtin_prefetch(&ptw[futur_row],1,2);
+            __builtin_prefetch(&p[futur_row],1,2);
+            
+            PLASSERT(reg.size()>row_idx && row_idx>=0);
+            int row=int(preg[row_idx]);
+            real val=p[row];
+            PLASSERT(target_weight.size()>row && row>=0);
+            PLASSERT(p[row]==tsource(col,row));
+            
+            RTR_target_t target = ptw[row].first;
+            RTR_weight_t weight = ptw[row].second;
+            if (RTR_HAVE_MISSING && is_missing(val)){
+                missing_leave->addRow(row, target, weight);
+            }else {
+                left_leave->addRow(row, target, weight);
+            }
+        }
+
+        l_length=left_leave->length()+right_leave->length();
+        l_weights_sum=left_leave->weights_sum+right_leave->weights_sum;
+        l_targets_sum=left_leave->targets_sum+right_leave->targets_sum;
+        l_weighted_targets_sum=left_leave->weighted_targets_sum+right_leave->weighted_targets_sum;
+        l_weighted_squared_targets_sum=left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum;
+    }else{//do 1 pass finding of the best split.
+
+        //fill left_leave
+        left_leave->length_=l_length-right_leave->length();
+        left_leave->weights_sum=l_weights_sum-right_leave->weights_sum;
+        left_leave->targets_sum=l_targets_sum-right_leave->targets_sum;
+        left_leave->weighted_targets_sum=l_weighted_targets_sum-right_leave->weighted_targets_sum;
+        left_leave->weighted_squared_targets_sum=l_weighted_squared_targets_sum-right_leave->weighted_squared_targets_sum;
+        PLCHECK(l_length==left_leave->length()+right_leave->length());
+        PLCHECK(fast_is_equal(l_weights_sum,left_leave->weights_sum+right_leave->weights_sum));
+        PLCHECK(fast_is_equal(l_targets_sum,left_leave->targets_sum+right_leave->targets_sum));
+        PLCHECK(fast_is_equal(l_weighted_targets_sum,left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
+        PLCHECK(fast_is_equal(l_weighted_squared_targets_sum,left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum));
+    }
+
+    //find best_split
+    int best_balance=INT_MAX;
+    real best_feature_value = REAL_MAX;
+    real best_split_error = REAL_MAX;
+    //in case of only missing value
+    if(left_leave->length()==0)
+        return make_tuple(best_feature_value, best_split_error, best_balance);
+
+
+    real missing_errors = missing_error[0] + missing_error[1];
+
+    Vec tmp(3);
+    int iter=reg.size()-right_leave->length()-1;
+    RTR_type row=reg[iter];
+//    RTR_type next_row = reg[reg.size()-2-right_leave->length()];
+    real first_value=p[preg[0]];
+    real next_feature=p[row];
+
+
+    //next_feature!=first_value is to check if their is more split point
+    // in case of binary variable or variable with few different value,
+    // this give a great speed up.
+    for(int i=iter-1;i>=0&&next_feature!=first_value;i--)
+    {
+        RTR_type next_row = preg[i];
+        real row_feature=next_feature;
+        next_feature=p[next_row];
+        
+        PLASSERT(next_row!=row);
+
+        PLASSERT((i+1)<reg.size() || row==reg[i+1]);
+        PLASSERT(next_row==reg[i]);
+        PLASSERT(get(next_row, col)==next_feature);
+        PLASSERT(get(row, col)==row_feature);
+        PLASSERT(next_feature<=row_feature);
+
+        int futur_row = preg[i+9];
+        __builtin_prefetch(&ptw[futur_row],1,2);
+        __builtin_prefetch(&p[futur_row],1,2);
+
+
+        real target=ptw[row].first;
+        real weight=ptw[row].second;
+
+        left_leave->removeRow(row, target, weight);
+        right_leave->addRow(row, target, weight);
+
+        row = next_row;
+        if (next_feature < row_feature){
+            left_leave->getOutputAndError(tmp, left_error);
+            right_leave->getOutputAndError(tmp, right_error);
+        }else
+            continue;
+        real work_error = missing_errors + left_error[0]
+            + left_error[1] + right_error[0] + right_error[1];
+        int work_balance = abs(left_leave->length() -
+                               right_leave->length());
+        if (fast_is_more(work_error,best_split_error)) continue;
+        else if (fast_is_equal(work_error,best_split_error) &&
+                 fast_is_more(work_balance,best_balance)) continue;
+
+        best_feature_value = 0.5 * (row_feature + next_feature);
+        best_split_error = work_error;
+        best_balance = work_balance;
+    }
+    return make_tuple(best_split_error, best_feature_value, best_balance);
+}
 void RegressionTreeRegisters::sortRows()
 {
     next_id = 0;

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 15:27:42 UTC (rev 10088)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 18:27:09 UTC (rev 10089)
@@ -109,6 +109,12 @@
     mutable vector<bool> compact_reg;
     mutable int compact_reg_leave;
 
+    mutable int l_length;
+    mutable real l_weights_sum;
+    mutable real l_targets_sum;
+    mutable real l_weighted_targets_sum;
+    mutable real l_weighted_squared_targets_sum;
+
 public:
 
     RegressionTreeRegisters();
@@ -149,13 +155,20 @@
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg)const;
     void         getAllRegisteredRow(RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
                                      TVec<pair<RTR_target_t,RTR_weight_t> > &t_w, Vec &value)const;
-    void         getAllRegisteredRowLeave(
+   void          getAllRegisteredRowLeave(
         RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
         TVec<pair<RTR_target_t,RTR_weight_t> > &t_w, Vec &value,
         PP<RegressionTreeLeave> missing_leave,
         PP<RegressionTreeLeave> left_leave,
         PP<RegressionTreeLeave> right_leave,
         TVec<RTR_type> &candidate)const;
+    tuple<real,real,int> bestSplitInRow(
+        RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
+        PP<RegressionTreeLeave> missing_leave,
+        PP<RegressionTreeLeave> left_leave,
+        PP<RegressionTreeLeave> right_leave,
+        Vec left_error, Vec right_error,
+        Vec missing_error)const;
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
     inline virtual void put(int i, int j, real value)



From nouiz at mail.berlios.de  Mon Apr  6 21:05:41 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 6 Apr 2009 21:05:41 +0200
Subject: [Plearn-commits] r10090 - trunk
Message-ID: <200904061905.n36J5fdM007502@sheep.berlios.de>

Author: nouiz
Date: 2009-04-06 21:05:41 +0200 (Mon, 06 Apr 2009)
New Revision: 10090

Modified:
   trunk/pymake.config.model
Log:
added option no-miss that activate one optimisation in RegressionTree


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2009-04-06 18:27:09 UTC (rev 10089)
+++ trunk/pymake.config.model	2009-04-06 19:05:41 UTC (rev 10090)
@@ -283,6 +283,7 @@
   [ '', 'march=native'],
   [ '', 'cygwin-fgets-bugfix'],
   [ '', 'openmpgcc'],
+  [ '', 'no-miss'],
   
   [ '', 'pass']
 ]
@@ -973,7 +974,11 @@
 	      description = 'to remove warning about uninitialized variables',
 	      compileroptions = '-Wno-uninitialized',
               in_output_dirname = False)
-              
+			  
+pymakeOption( name = 'no-miss',
+	          description = 'To use an optimisation in RegressionTree when their is no missing value in the trainset.',
+	          compileroptions = '-DRTR_HAVE_MISSING=false')
+	
 pymakeOption( name = 'nolock',
               description = 'USE WITH CARE: disable vmatrix lock',
               cpp_definitions = ['DISABLE_VMATRIX_LOCK'])
@@ -986,7 +991,7 @@
               description = 'add the cpp definition CYGWIN_FGETS_BUGFIX',
               cpp_definitions = ['CYGWIN_FGETS_BUGFIX'])
 
-cpp_variables += ['DISABLE_VMATRIX_LOCK', 'CYGWIN_FGETS_BUGFIX']
+cpp_variables += ['DISABLE_VMATRIX_LOCK', 'CYGWIN_FGETS_BUGFIX', 'RTR_HAVE_MISSING']
 
 
 #####  Network Setup  #######################################################



From nouiz at mail.berlios.de  Mon Apr  6 21:21:27 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 6 Apr 2009 21:21:27 +0200
Subject: [Plearn-commits] r10091 - trunk/plearn_learners/regressors
Message-ID: <200904061921.n36JLRen009491@sheep.berlios.de>

Author: nouiz
Date: 2009-04-06 21:21:27 +0200 (Mon, 06 Apr 2009)
New Revision: 10091

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
made a little bit more generic last optimisation in RegressionTree.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 19:05:41 UTC (rev 10090)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 19:21:27 UTC (rev 10091)
@@ -255,8 +255,8 @@
     Vec right_error(3);
     Vec missing_error(3);
     missing_error.clear();
-    bool one_pass_on_data=!RTR_HAVE_MISSING && leave->classname()=="RegressionTreeLeave";
     PP<RegressionTreeRegisters> train_set = tree->getSortedTrainingSet();
+    bool one_pass_on_data=!train_set->haveMissing() && leave->classname()=="RegressionTreeLeave";
 
     int inputsize = train_set->inputsize();
 #ifdef RCMP

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 19:05:41 UTC (rev 10090)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 19:21:27 UTC (rev 10091)
@@ -68,6 +68,7 @@
     next_id(0),
     do_sort_rows(true),
     mem_tsource(true),
+    have_missing(true),
     compact_reg_leave(-1)
 {
     build();
@@ -85,6 +86,7 @@
     next_id(0),
     do_sort_rows(do_sort_rows_),
     mem_tsource(mem_tsource_),
+    have_missing(true),
     compact_reg_leave(-1)
 {
     source = source_;
@@ -92,6 +94,7 @@
     if(tsource->classname()=="MemoryVMatrixNoSave")
         tsource_mat = tsource.toMat();
     tsorted_row = tsorted_row_;
+    checkMissing();
     build();
 }
 
@@ -105,6 +108,7 @@
     next_id(0),
     do_sort_rows(do_sort_rows_),
     mem_tsource(mem_tsource_),
+    have_missing(true),
     compact_reg_leave(-1)
 {
     source = source_;
@@ -423,9 +427,7 @@
     Vec left_error, Vec right_error,
     Vec missing_error) const
 {
-//    fill reg, t_w et value and candidate
     PLCHECK(missing_leave->classname()=="RegressionTreeLeave");
-    PLCHECK(RTR_HAVE_MISSING==false);//need to modif the code to make it work with missing value.
 
     PLASSERT(tsource_mat.length()==tsource.length());
     getAllRegisteredRow(leave_id,col,reg);
@@ -589,6 +591,7 @@
     if(isUpToDate(f)){
         DBG_LOG<<"RegressionTreeRegisters:: Reloading the sorted source VMatrix: "<<f<<endl;
         PLearn::load(f,tsorted_row);
+        checkMissing();
         return;
     }
 
@@ -615,6 +618,7 @@
         sortEachDim(sample_dim);
         if (report_progress) pb->update(sample_dim+1);
     }
+    checkMissing();
     if (report_progress) pb->close();//in case of parallel sort.
     if(source->hasMetaDataDir()){
         DBG_LOG<<"RegressionTreeRegisters:: Saving the sorted source VMatrix: "<<f<<endl;
@@ -622,7 +626,21 @@
     }else{
     }
 }
-                                                  
+
+//!check if their is missing in the input value.
+void RegressionTreeRegisters::checkMissing()
+{
+    if(have_missing==false)
+        return;
+    bool found_missing=false;
+    for(int j=0;j<inputsize()&&!found_missing;j++)
+        for(int i=0;i<length()&&!found_missing;i++)
+            if(is_missing(tsource(j,i)))
+                found_missing=true;
+    if(!found_missing)
+        have_missing=false;
+}
+
 void RegressionTreeRegisters::sortEachDim(int dim)
 {
     PLCHECK_MSG(tsource->classname()=="MemoryVMatrixNoSave",tsource->classname().c_str());

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 19:05:41 UTC (rev 10090)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 19:21:27 UTC (rev 10091)
@@ -105,6 +105,7 @@
 
     bool do_sort_rows;
     bool mem_tsource;
+    bool have_missing;
 
     mutable vector<bool> compact_reg;
     mutable int compact_reg_leave;
@@ -148,6 +149,7 @@
     inline void         setWeight(int row,real val){
         target_weight[row].second = val;
     }
+    inline bool         haveMissing(){return have_missing;}
     inline RTR_type_id     getNextId(){
         PLCHECK(next_id<std::numeric_limits<RTR_type_id>::max());
         next_id += 1;return next_id;}
@@ -190,6 +192,7 @@
     void         sortRows();
     void         sortEachDim(int dim);
     void         verbose(string msg, int level);
+    void         checkMissing();
 
 };
 



From nouiz at mail.berlios.de  Mon Apr  6 23:32:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 6 Apr 2009 23:32:30 +0200
Subject: [Plearn-commits] r10092 - trunk/plearn_learners/regressors
Message-ID: <200904062132.n36LWUv9027747@sheep.berlios.de>

Author: nouiz
Date: 2009-04-06 23:32:29 +0200 (Mon, 06 Apr 2009)
New Revision: 10092

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
made the last commited optimisation more generic. Now it work with all type of Leave if the training set don't have missing value.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-04-06 21:32:29 UTC (rev 10092)
@@ -269,6 +269,31 @@
         PLERROR("In RegressionTreeLeave::uniqTarget subclass must reimplement it.");
 }
 
+void RegressionTreeLeave::addLeave(PP<RegressionTreeLeave> leave){
+    if(leave->classname()=="RegressionTreeLeave" && classname()=="RegressionTreeLeave"){
+        length_ += leave->length_;
+        weights_sum += leave->weights_sum;
+        targets_sum += leave->targets_sum;
+        weighted_targets_sum += leave->weighted_targets_sum;
+        weighted_squared_targets_sum += leave->weighted_squared_targets_sum;        
+    }else
+        PLERROR("In RegressionTreeLeave::addLeave subclass %s or %s must reimplement it.",
+                classname().c_str(), leave->classname().c_str());
+}
+
+void RegressionTreeLeave::removeLeave(PP<RegressionTreeLeave> leave){
+    if(leave->classname()=="RegressionTreeLeave" && classname()=="RegressionTreeLeave"){
+        length_ -= leave->length_;
+        weights_sum -= leave->weights_sum;
+        targets_sum -= leave->targets_sum;
+        weighted_targets_sum -= leave->weighted_targets_sum;
+        weighted_squared_targets_sum -= leave->weighted_squared_targets_sum;
+    }else
+        PLERROR("In RegressionTreeLeave::removeLeave subclass %s or %s must reimplement it.",
+                classname().c_str(), leave->classname().c_str());
+}
+
+
 void RegressionTreeLeave::verbose(string the_msg, int the_level)
 {
     if (verbosity >= the_level)

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2009-04-06 21:32:29 UTC (rev 10092)
@@ -107,6 +107,8 @@
     inline real          getWeightsSum(){return weights_sum;}
     inline real          getTargetsSum(){return targets_sum;}
     virtual bool         uniqTarget();
+    virtual void         addLeave(PP<RegressionTreeLeave> leave);
+    virtual void         removeLeave(PP<RegressionTreeLeave> leave);
 
 private:
     void         build_();

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2009-04-06 21:32:29 UTC (rev 10092)
@@ -41,6 +41,7 @@
 
 #include "RegressionTreeMulticlassLeave.h"
 #include "RegressionTreeRegisters.h"
+#include <plearn/math/TMat_maths_impl.h>
 
 namespace PLearn {
 using namespace std;
@@ -275,6 +276,28 @@
     return ret;
 }
 
+void RegressionTreeMulticlassLeave::addLeave(PP<RegressionTreeLeave> leave_){
+    PP<RegressionTreeMulticlassLeave> leave = (PP<RegressionTreeMulticlassLeave>) leave_;
+    if(leave->classname() == classname()){
+        length_ += leave->length_;
+        weights_sum += leave->weights_sum;
+        multiclass_weights_sum += leave->multiclass_weights_sum;
+    }else
+        PLERROR("In %s::addLeave the leave to add should have the same class. It have %s.",
+                classname().c_str(), leave->classname().c_str());
+}
+
+void RegressionTreeMulticlassLeave::removeLeave(PP<RegressionTreeLeave> leave_){
+    PP<RegressionTreeMulticlassLeave> leave = (PP<RegressionTreeMulticlassLeave>) leave_;
+    if(leave->classname() == classname()){
+        length_ -= leave->length_;
+        weights_sum -= leave->weights_sum;
+        multiclass_weights_sum -= leave->multiclass_weights_sum;
+    }else
+        PLERROR("In %s::addLeave the leave to add should have the same class. It have %s.",
+                classname().c_str(), leave->classname().c_str());
+}
+
 void RegressionTreeMulticlassLeave::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2009-04-06 21:32:29 UTC (rev 10092)
@@ -87,7 +87,9 @@
     void         getOutputAndError(Vec& output, Vec& error)const;
     TVec<string> getOutputNames() const;
     void         printStats();
-  
+    virtual void         addLeave(PP<RegressionTreeLeave> leave);
+    virtual void         removeLeave(PP<RegressionTreeLeave> leave);
+
 private:
     void         build_();
 };

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.cc	2009-04-06 21:32:29 UTC (rev 10092)
@@ -41,6 +41,7 @@
 
 #include "RegressionTreeMulticlassLeaveFast.h"
 #include "RegressionTreeRegisters.h"
+#include <plearn/math/TMat_maths_impl.h>
 
 namespace PLearn {
 using namespace std;
@@ -241,6 +242,30 @@
     return ret;
 }
 
+void RegressionTreeMulticlassLeaveFast::addLeave(PP<RegressionTreeLeave> leave_){
+    PP<RegressionTreeMulticlassLeaveFast> leave = (PP<RegressionTreeMulticlassLeaveFast>) leave_;
+
+    if(leave->classname() == classname()){
+        length_ += leave->length_;
+        weights_sum += leave->weights_sum;
+        multiclass_weights_sum += leave->multiclass_weights_sum;
+    }else
+        PLERROR("In %s::addLeave the leave to add should have the same class. It have %s.",
+                classname().c_str(), leave->classname().c_str());
+}
+
+void RegressionTreeMulticlassLeaveFast::removeLeave(PP<RegressionTreeLeave> leave_){
+    PP<RegressionTreeMulticlassLeaveFast> leave = (PP<RegressionTreeMulticlassLeaveFast>) leave_;
+
+    if(leave->classname() == classname()){
+        length_ -= leave->length_;
+        weights_sum -= leave->weights_sum;
+        multiclass_weights_sum -= leave->multiclass_weights_sum;
+    }else
+        PLERROR("In %s::addLeave the leave to add should have the same class. It have %s.",
+                classname().c_str(), leave->classname().c_str());
+}
+
 void RegressionTreeMulticlassLeaveFast::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveFast.h	2009-04-06 21:32:29 UTC (rev 10092)
@@ -85,7 +85,9 @@
     void         getOutputAndError(Vec& output, Vec& error)const;
     TVec<string> getOutputNames() const;
     void         printStats();
-  
+    virtual void         addLeave(PP<RegressionTreeLeave> leave);
+    virtual void         removeLeave(PP<RegressionTreeLeave> leave);
+
 private:
     void         build_();
 };

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.cc	2009-04-06 21:32:29 UTC (rev 10092)
@@ -41,6 +41,7 @@
 
 #include "RegressionTreeMulticlassLeaveProb.h"
 #include "RegressionTreeRegisters.h"
+#include <plearn/math/TMat_maths_impl.h>
 
 namespace PLearn {
 using namespace std;
@@ -244,6 +245,30 @@
     return ret;
 }
 
+void RegressionTreeMulticlassLeaveProb::addLeave(PP<RegressionTreeLeave> leave_){
+    PP<RegressionTreeMulticlassLeaveProb> leave = (PP<RegressionTreeMulticlassLeaveProb>) leave_;
+
+    if(leave->classname() == classname()){
+        length_ += leave->length_;
+        weights_sum += leave->weights_sum;
+        multiclass_weights_sum += leave->multiclass_weights_sum;
+    }else
+        PLERROR("In %s::addLeave the leave to add should have the same class. It have %s.",
+                classname().c_str(), leave->classname().c_str());
+}
+
+void RegressionTreeMulticlassLeaveProb::removeLeave(PP<RegressionTreeLeave> leave_){
+    PP<RegressionTreeMulticlassLeaveProb> leave = (PP<RegressionTreeMulticlassLeaveProb>) leave_;
+
+    if(leave->classname() == classname()){
+        length_ -= leave->length_;
+        weights_sum -= leave->weights_sum;
+        multiclass_weights_sum -= leave->multiclass_weights_sum;
+    }else
+        PLERROR("In %s::addLeave the leave to add should have the same class. It have %s.",
+                classname().c_str(), leave->classname().c_str());
+}
+
 void RegressionTreeMulticlassLeaveProb::printStats()
 {
     cout << " l " << length_;

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeaveProb.h	2009-04-06 21:32:29 UTC (rev 10092)
@@ -86,7 +86,9 @@
     void         getOutputAndError(Vec& output, Vec& error)const;
     TVec<string> getOutputNames() const;
     void         printStats();
-  
+    virtual void         addLeave(PP<RegressionTreeLeave> leave);
+    virtual void         removeLeave(PP<RegressionTreeLeave> leave);
+
 private:
     void         build_();
 };

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-06 21:32:29 UTC (rev 10092)
@@ -256,7 +256,7 @@
     Vec missing_error(3);
     missing_error.clear();
     PP<RegressionTreeRegisters> train_set = tree->getSortedTrainingSet();
-    bool one_pass_on_data=!train_set->haveMissing() && leave->classname()=="RegressionTreeLeave";
+    bool one_pass_on_data=!train_set->haveMissing();
 
     int inputsize = train_set->inputsize();
 #ifdef RCMP
@@ -352,10 +352,9 @@
                                registered_target_weight);
         }else{
             ret=train_set->bestSplitInRow(leave_id, col, registered_row,
-                                          missing_leave,
                                           left_leave,
                                           right_leave, left_error,
-                                          right_error, missing_error);
+                                          right_error);
         }
         PLASSERT(registered_row.size()==leave->length());
 #endif

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 21:32:29 UTC (rev 10092)
@@ -421,14 +421,16 @@
 
 tuple<real,real,int> RegressionTreeRegisters::bestSplitInRow(
     RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
-    PP<RegressionTreeLeave> missing_leave,
     PP<RegressionTreeLeave> left_leave,
     PP<RegressionTreeLeave> right_leave,
-    Vec left_error, Vec right_error,
-    Vec missing_error) const
+    Vec left_error, Vec right_error) const
 {
-    PLCHECK(missing_leave->classname()=="RegressionTreeLeave");
+    PLCHECK(!haveMissing());
 
+    if(!tmp_leave){
+        tmp_leave = ::PLearn::deepCopy(left_leave);
+    }
+
     PLASSERT(tsource_mat.length()==tsource.length());
     getAllRegisteredRow(leave_id,col,reg);
     real * p = tsource_mat[col];
@@ -458,9 +460,7 @@
         RTR_target_t target = ptw[row].first;
         RTR_weight_t weight = ptw[row].second;
 
-        if (RTR_HAVE_MISSING && is_missing(val))
-            missing_leave->addRow(row, target, weight);
-        else if(val==prev_val)
+        if(val==prev_val)
             right_leave->addRow(row, target, weight);
         else
             break;
@@ -472,53 +472,41 @@
         {
             int futur_row = preg[row_idx+8];
             __builtin_prefetch(&ptw[futur_row],1,2);
-            __builtin_prefetch(&p[futur_row],1,2);
             
             PLASSERT(reg.size()>row_idx && row_idx>=0);
             int row=int(preg[row_idx]);
-            real val=p[row];
             PLASSERT(target_weight.size()>row && row>=0);
-            PLASSERT(p[row]==tsource(col,row));
             
             RTR_target_t target = ptw[row].first;
             RTR_weight_t weight = ptw[row].second;
-            if (RTR_HAVE_MISSING && is_missing(val)){
-                missing_leave->addRow(row, target, weight);
-            }else {
-                left_leave->addRow(row, target, weight);
-            }
+            left_leave->addRow(row, target, weight);
         }
+        tmp_leave->initStats();
+        tmp_leave->addLeave(left_leave);
+        tmp_leave->addLeave(right_leave);
 
-        l_length=left_leave->length()+right_leave->length();
-        l_weights_sum=left_leave->weights_sum+right_leave->weights_sum;
-        l_targets_sum=left_leave->targets_sum+right_leave->targets_sum;
-        l_weighted_targets_sum=left_leave->weighted_targets_sum+right_leave->weighted_targets_sum;
-        l_weighted_squared_targets_sum=left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum;
     }else{//do 1 pass finding of the best split.
 
-        //fill left_leave
-        left_leave->length_=l_length-right_leave->length();
-        left_leave->weights_sum=l_weights_sum-right_leave->weights_sum;
-        left_leave->targets_sum=l_targets_sum-right_leave->targets_sum;
-        left_leave->weighted_targets_sum=l_weighted_targets_sum-right_leave->weighted_targets_sum;
-        left_leave->weighted_squared_targets_sum=l_weighted_squared_targets_sum-right_leave->weighted_squared_targets_sum;
-        PLCHECK(l_length==left_leave->length()+right_leave->length());
-        PLCHECK(fast_is_equal(l_weights_sum,left_leave->weights_sum+right_leave->weights_sum));
-        PLCHECK(fast_is_equal(l_targets_sum,left_leave->targets_sum+right_leave->targets_sum));
-        PLCHECK(fast_is_equal(l_weighted_targets_sum,left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
-        PLCHECK(fast_is_equal(l_weighted_squared_targets_sum,left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum));
+        left_leave->initStats();
+        left_leave->addLeave(tmp_leave);
+        left_leave->removeLeave(right_leave);
+
+        PLCHECK(tmp_leave->length()==left_leave->length()+right_leave->length());
+        PLCHECK(fast_is_equal(tmp_leave->weights_sum,left_leave->weights_sum+right_leave->weights_sum));
+        PLCHECK(fast_is_equal(tmp_leave->targets_sum,left_leave->targets_sum+right_leave->targets_sum));
+        PLCHECK(fast_is_equal(tmp_leave->weighted_targets_sum,left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
+        PLCHECK(fast_is_equal(tmp_leave->weighted_squared_targets_sum,
+                              left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum));
     }
 
     //find best_split
     int best_balance=INT_MAX;
     real best_feature_value = REAL_MAX;
     real best_split_error = REAL_MAX;
-    //in case of only missing value
     if(left_leave->length()==0)
         return make_tuple(best_feature_value, best_split_error, best_balance);
 
 
-    real missing_errors = missing_error[0] + missing_error[1];
 
     Vec tmp(3);
     int iter=reg.size()-right_leave->length()-1;
@@ -562,7 +550,7 @@
             right_leave->getOutputAndError(tmp, right_error);
         }else
             continue;
-        real work_error = missing_errors + left_error[0]
+        real work_error = left_error[0]
             + left_error[1] + right_error[0] + right_error[1];
         int work_balance = abs(left_leave->length() -
                                right_leave->length());
@@ -576,6 +564,7 @@
     }
     return make_tuple(best_split_error, best_feature_value, best_balance);
 }
+
 void RegressionTreeRegisters::sortRows()
 {
     next_id = 0;

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 19:21:27 UTC (rev 10091)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-06 21:32:29 UTC (rev 10092)
@@ -110,11 +110,7 @@
     mutable vector<bool> compact_reg;
     mutable int compact_reg_leave;
 
-    mutable int l_length;
-    mutable real l_weights_sum;
-    mutable real l_targets_sum;
-    mutable real l_weighted_targets_sum;
-    mutable real l_weighted_squared_targets_sum;
+    mutable PP<RegressionTreeLeave> tmp_leave;
 
 public:
 
@@ -149,7 +145,7 @@
     inline void         setWeight(int row,real val){
         target_weight[row].second = val;
     }
-    inline bool         haveMissing(){return have_missing;}
+    inline bool         haveMissing()const{return have_missing;}
     inline RTR_type_id     getNextId(){
         PLCHECK(next_id<std::numeric_limits<RTR_type_id>::max());
         next_id += 1;return next_id;}
@@ -166,11 +162,9 @@
         TVec<RTR_type> &candidate)const;
     tuple<real,real,int> bestSplitInRow(
         RTR_type_id leave_id, int col, TVec<RTR_type> &reg,
-        PP<RegressionTreeLeave> missing_leave,
         PP<RegressionTreeLeave> left_leave,
         PP<RegressionTreeLeave> right_leave,
-        Vec left_error, Vec right_error,
-        Vec missing_error)const;
+        Vec left_error, Vec right_error)const;
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
     inline virtual void put(int i, int j, real value)



From nouiz at mail.berlios.de  Tue Apr  7 15:08:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Apr 2009 15:08:45 +0200
Subject: [Plearn-commits] r10093 - trunk/plearn_learners/regressors
Message-ID: <200904071308.n37D8jkQ015955@sheep.berlios.de>

Author: nouiz
Date: 2009-04-07 15:08:45 +0200 (Tue, 07 Apr 2009)
New Revision: 10093

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
more comment and PLASSERT


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-06 21:32:29 UTC (rev 10092)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-07 13:08:45 UTC (rev 10093)
@@ -359,6 +359,7 @@
 }
 
 //! reg.size() == the number of row that we will put in it.
+//! the register are not sorted. They are in increasing order.
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id,
                                                   TVec<RTR_type> &reg) const
 {
@@ -371,12 +372,14 @@
     for(int i=0;i<length() && n> idx;i++){
         if (pleave_register[i] == leave_id){
             preg[idx++]=i;
+            PLASSERT(reg[idx-1]==i);
         }
     }
     PLASSERT(idx==reg->size());
 }
 
 //! reg.size() == the number of row that we will put in it.
+//! the register are sorted by col.
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,
                                                   TVec<RTR_type> &reg) const
 {



From nouiz at mail.berlios.de  Tue Apr  7 15:51:07 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Apr 2009 15:51:07 +0200
Subject: [Plearn-commits] r10094 - trunk/plearn_learners/regressors
Message-ID: <200904071351.n37Dp7IA019523@sheep.berlios.de>

Author: nouiz
Date: 2009-04-07 15:51:06 +0200 (Tue, 07 Apr 2009)
New Revision: 10094

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
small code reorder. reuse a class vector instead of creating it each time. Changed PLCHECK for PLASSERT.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-07 13:08:45 UTC (rev 10093)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-07 13:51:06 UTC (rev 10094)
@@ -214,7 +214,7 @@
         }
     leave_register.resize(length());
     sortRows();
-    compact_reg.resize(length());
+//    compact_reg.resize(length());
 }
 
 void RegressionTreeRegisters::reinitRegisters()
@@ -390,7 +390,17 @@
     RTR_type* preg = reg.data();
     RTR_type* ptsorted_row = tsorted_row[col];
     RTR_type_id* pleave_register = leave_register.data();
-    if(false && compact_reg_leave==leave_id){
+    if(compact_reg.size()==0){
+        for(int i=0;i<length() && n> idx;i++){
+            PLASSERT(ptsorted_row[i]==tsorted_row(col, i));
+            RTR_type srow = ptsorted_row[i];
+            if ( pleave_register[srow] == leave_id){
+                PLASSERT(leave_register[srow] == leave_id);
+                PLASSERT(preg[idx]==reg[idx]);
+                preg[idx++]=srow;
+            }
+        }
+    }else if(compact_reg_leave==leave_id){
         //compact_reg is used as an optimization.
         //as it is more compact in memory then leave_register
         //we are more memory friendly.
@@ -404,6 +414,8 @@
             }
         }
     }else{
+        compact_reg.resize(0);
+        compact_reg.resize(length(),0);
 //        for(uint i=0;i<compact_reg.size();i++)
 //            compact_reg[i]=false;
         for(int i=0;i<length() && n> idx;i++){
@@ -413,7 +425,7 @@
                 PLASSERT(leave_register[srow] == leave_id);
                 PLASSERT(preg[idx]==reg[idx]);
                 preg[idx++]=srow;
-                //compact_reg[srow]=true;
+                compact_reg[srow]=true;
             }
         }
         compact_reg_leave = leave_id;
@@ -432,6 +444,7 @@
 
     if(!tmp_leave){
         tmp_leave = ::PLearn::deepCopy(left_leave);
+        tmp_vec.resize(left_leave->outputsize());
     }
 
     PLASSERT(tsource_mat.length()==tsource.length());
@@ -494,11 +507,11 @@
         left_leave->addLeave(tmp_leave);
         left_leave->removeLeave(right_leave);
 
-        PLCHECK(tmp_leave->length()==left_leave->length()+right_leave->length());
-        PLCHECK(fast_is_equal(tmp_leave->weights_sum,left_leave->weights_sum+right_leave->weights_sum));
-        PLCHECK(fast_is_equal(tmp_leave->targets_sum,left_leave->targets_sum+right_leave->targets_sum));
-        PLCHECK(fast_is_equal(tmp_leave->weighted_targets_sum,left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
-        PLCHECK(fast_is_equal(tmp_leave->weighted_squared_targets_sum,
+        PLASSERT(tmp_leave->length()==left_leave->length()+right_leave->length());
+        PLASSERT(fast_is_equal(tmp_leave->weights_sum,left_leave->weights_sum+right_leave->weights_sum));
+        PLASSERT(fast_is_equal(tmp_leave->targets_sum,left_leave->targets_sum+right_leave->targets_sum));
+        PLASSERT(fast_is_equal(tmp_leave->weighted_targets_sum,left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
+        PLASSERT(fast_is_equal(tmp_leave->weighted_squared_targets_sum,
                               left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum));
     }
 
@@ -509,12 +522,8 @@
     if(left_leave->length()==0)
         return make_tuple(best_feature_value, best_split_error, best_balance);
 
-
-
-    Vec tmp(3);
     int iter=reg.size()-right_leave->length()-1;
     RTR_type row=reg[iter];
-//    RTR_type next_row = reg[reg.size()-2-right_leave->length()];
     real first_value=p[preg[0]];
     real next_feature=p[row];
 
@@ -549,8 +558,8 @@
 
         row = next_row;
         if (next_feature < row_feature){
-            left_leave->getOutputAndError(tmp, left_error);
-            right_leave->getOutputAndError(tmp, right_error);
+            left_leave->getOutputAndError(tmp_vec, left_error);
+            right_leave->getOutputAndError(tmp_vec, right_error);
         }else
             continue;
         real work_error = left_error[0]

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-07 13:08:45 UTC (rev 10093)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2009-04-07 13:51:06 UTC (rev 10094)
@@ -110,7 +110,10 @@
     mutable vector<bool> compact_reg;
     mutable int compact_reg_leave;
 
+    //!used in bestSplitInRow to save data
     mutable PP<RegressionTreeLeave> tmp_leave;
+    //!used in bestSplitInRow to don't allocate a new vector each time.
+    mutable Vec tmp_vec;
 
 public:
 



From nouiz at mail.berlios.de  Tue Apr  7 16:38:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Apr 2009 16:38:45 +0200
Subject: [Plearn-commits] r10095 - trunk/plearn_learners/regressors
Message-ID: <200904071438.n37EcjUo025962@sheep.berlios.de>

Author: nouiz
Date: 2009-04-07 16:38:44 +0200 (Tue, 07 Apr 2009)
New Revision: 10095

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
fixe prefetch.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-07 13:51:06 UTC (rev 10094)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-07 14:38:44 UTC (rev 10095)
@@ -415,7 +415,7 @@
         }
     }else{
         compact_reg.resize(0);
-        compact_reg.resize(length(),0);
+        compact_reg.resize(length(),false);
 //        for(uint i=0;i<compact_reg.size();i++)
 //            compact_reg[i]=false;
         for(int i=0;i<length() && n> idx;i++){
@@ -545,7 +545,7 @@
         PLASSERT(get(row, col)==row_feature);
         PLASSERT(next_feature<=row_feature);
 
-        int futur_row = preg[i+9];
+        int futur_row = preg[i-9];
         __builtin_prefetch(&ptw[futur_row],1,2);
         __builtin_prefetch(&p[futur_row],1,2);
 



From nouiz at mail.berlios.de  Tue Apr  7 16:56:09 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Apr 2009 16:56:09 +0200
Subject: [Plearn-commits] r10096 - trunk/plearn_learners/meta
Message-ID: <200904071456.n37Eu9xl029026@sheep.berlios.de>

Author: nouiz
Date: 2009-04-07 16:56:09 +0200 (Tue, 07 Apr 2009)
New Revision: 10096

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
put some profile info in comment as they are very short.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-04-07 14:38:44 UTC (rev 10095)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-04-07 14:56:09 UTC (rev 10096)
@@ -770,7 +770,7 @@
         my_costs=VMat(new MemoryVMatrix(testset->length(),
 					nTestCosts()));
     }
-    Profiler::pl_profile_start("MultiClassAdaBoost::test() my_outputs");
+//    Profiler::pl_profile_start("MultiClassAdaBoost::test() my_outputs");//cheap
     if(my_outputs){
         for(int row=0;row<testset.length();row++){
             real out1=testoutputs1->get(row,0);
@@ -792,8 +792,8 @@
 	    my_outputs->putOrAppendRow(row,tmp_output);
 	}
     }
-    Profiler::pl_profile_end("MultiClassAdaBoost::test() my_outputs");
-    Profiler::pl_profile_start("MultiClassAdaBoost::test() my_costs");
+//    Profiler::pl_profile_end("MultiClassAdaBoost::test() my_outputs");
+//    Profiler::pl_profile_start("MultiClassAdaBoost::test() my_costs");//cheap
 
     if (my_costs){
         tmp_costs.resize(nTestCosts());
@@ -826,9 +826,8 @@
 	    my_costs->putOrAppendRow(row,tmp_costs);
         }
     }
-    Profiler::pl_profile_end("MultiClassAdaBoost::test() my_costs");
-    Profiler::pl_profile_start("MultiClassAdaBoost::test() test_stats");
-
+//    Profiler::pl_profile_end("MultiClassAdaBoost::test() my_costs");
+//    Profiler::pl_profile_start("MultiClassAdaBoost::test() test_stats");//cheap
     if (test_stats){
 	if(testset->weightsize()==0){
             for(int row=0;row<testset.length();row++){
@@ -845,7 +844,7 @@
             }
 	}
     }
-    Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
+//    Profiler::pl_profile_end("MultiClassAdaBoost::test() test_stats");
     timer->stopTimer("MultiClassAdaBoost::test() current");
     timer->stopTimer("MultiClassAdaBoost::test()");
     Profiler::pl_profile_end("MultiClassAdaBoost::test()");



From nouiz at mail.berlios.de  Tue Apr  7 17:58:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Apr 2009 17:58:06 +0200
Subject: [Plearn-commits] r10097 - trunk/plearn_learners/meta
Message-ID: <200904071558.n37Fw62o002804@sheep.berlios.de>

Author: nouiz
Date: 2009-04-07 17:58:04 +0200 (Tue, 07 Apr 2009)
New Revision: 10097

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
added some profiling and put some function parameter const.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2009-04-07 14:56:09 UTC (rev 10096)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2009-04-07 15:58:04 UTC (rev 10097)
@@ -789,7 +789,9 @@
         for(int row=0;row<testset.length();row++){
             output=old_outputs(row);
             //compute the new testoutputs
+            Profiler::pl_profile_start("AdaBoost::test() getExample" );
             testset.getExample(row, input, target, weight);
+            Profiler::pl_profile_end("AdaBoost::test() getExample" );
             computeOutput_(input, output, stages_done, output[1]);
             computeCostsFromOutputs(input,output,target,costs);
 #ifndef NDEBUG
@@ -809,7 +811,7 @@
 }
 
 void AdaBoost::computeOutput_(const Vec& input, Vec& output,
-                             int start, real sum) const
+                              const int start, real const sum) const
 {
     PLASSERT(weak_learners.size()>0);
     PLASSERT(weak_learner_output.size()==weak_learner_template->outputsize());

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2009-04-07 14:56:09 UTC (rev 10096)
+++ trunk/plearn_learners/meta/AdaBoost.h	2009-04-07 15:58:04 UTC (rev 10097)
@@ -153,7 +153,7 @@
     void computeTrainingError(Vec input, Vec target);
 
     void computeOutput_(const Vec& input, Vec& output,
-                       int start=0, real sum=0.) const;
+                        const int start=0, const real sum=0.) const;
 
 protected: 
     //! Declares this class' options



From tihocan at mail.berlios.de  Tue Apr  7 18:37:10 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 7 Apr 2009 18:37:10 +0200
Subject: [Plearn-commits] r10098 - in trunk/plearn_learners/regressors: .
	test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200904071637.n37GbAWF011498@sheep.berlios.de>

Author: tihocan
Date: 2009-04-07 18:37:02 +0200 (Tue, 07 Apr 2009)
New Revision: 10098

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Log:
Reverted part of a previous commit in order to fix test

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-07 15:58:04 UTC (rev 10097)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-07 16:37:02 UTC (rev 10098)
@@ -654,9 +654,24 @@
     // less lifted from Kernel.cc ==> must see with Olivier how to better
     // factor this code
     Mat& K = covmat;
-    Vec self_cov(N);
-    m_kernel->computeTestGramMatrix(inputs, K, self_cov);
-    
+
+    PLASSERT( K.width() == N && K.length() == N );
+    const int mod = K.mod();
+    real Kij;
+    real* Ki;
+    real* Kji;
+    for (int i=0 ; i<N ; ++i) {
+        Ki  = K[i];
+        Kji = &K[0][i];
+        const Vec& cur_input_i = inputs(i);
+        for (int j=0 ; j<=i ; ++j, Kji += mod) {
+            Kij = m_kernel->evaluate(cur_input_i, inputs(j));
+            *Ki++ = Kij;
+            if (j<i)
+                *Kji = Kij;    // Assume symmetry, checked at build
+        }
+    }
+
     // The predictive covariance matrix is for the exact cast(c.f. Rasmussen
     // and Williams):
     //

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-04-07 15:58:04 UTC (rev 10097)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-04-07 16:37:02 UTC (rev 10098)
@@ -5,10 +5,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 14.8530952695504261 ;
-isp_signal_sigma = 29.6219285856411894 ;
+isp_alpha = 14.8530952689689837 ;
+isp_signal_sigma = 29.6219285851744907 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 22.2544311482102088 ] ;
+isp_input_sigma = 1 [ 22.2544311480311059 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -19,7 +19,7 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.86446658047284264 ;
+isp_noise_sigma = -1.86446658043304136 ;
 isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
@@ -62,22 +62,28 @@
 early_stop = 0  )
 ;
 save_gram_matrix = 0 ;
+solution_algorithm = "exact" ;
+active_set_indices = []
+;
 alpha = 5  1  [ 
--1.11770047926741944 	
--1.44398600683733602 	
-2.34477475115816247 	
-0.359163702825210596 	
--0.273169390815579172 	
+-1.11770047923155547 	
+-1.44398600678466971 	
+2.34477475107078348 	
+0.359163702825774089 	
+-0.273169390819025082 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.1738430348361808 	-0.0428570226562168741 	-2.24278913022280779 	0.21118794494333884 	-0.016000849842130186 	
--0.0428570226562171239 	2.63229682417954214 	-2.36514023152167674 	-0.310986674739582525 	0.0223156036853270311 	
--2.24278913022280779 	-2.3651402315216763 	4.57667695961490306 	0.0265284039064695096 	-8.51433617983321869e-05 	
-0.211187944943338812 	-0.310986674739582469 	0.0265284039064695096 	0.113894354038512086 	-0.0105928862093947491 	
--0.0160008498421301826 	0.0223156036853270276 	-8.51433617983319294e-05 	-0.0105928862093947491 	0.0346336314417334534 	
+2.17384303478444618 	-0.0428570226814908445 	-2.24278913014357606 	0.211187944940308625 	-0.0160008498417097828 	
+-0.0428570226814906294 	2.63229682411057064 	-2.36514023143184327 	-0.310986674734060831 	0.0223156036847275939 	
+-2.24278913014357606 	-2.36514023143184327 	4.57667695944806141 	0.0265284039045036689 	-8.51433617209678838e-05 	
+0.211187944940308597 	-0.310986674734060831 	0.0265284039045036689 	0.113894354038077864 	-0.0105928862092774176 	
+-0.0160008498417097828 	0.0223156036847275904 	-8.51433617209677347e-05 	-0.0105928862092774159 	0.0346336314422349412 	
 ]
 ;
+subgram_inverse = 0  0  [ 
+]
+;
 target_mean = 1 [ 10 ] ;
 training_inputs = 5  1  [ 
 5 	
@@ -105,18 +111,18 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 0  )
 
-!R 1 1 [ 14.9482509617851225 ] 
-!R 1 1 [ 14.4446958356332527 ] 
+!R 1 1 [ 14.9482509617831152 ] 
+!R 1 1 [ 14.444695835622035 ] 
 !R 2 4  1  [ 
-13.4992090127014244 	
-14.4141233331841647 	
-14.9482509617851225 	
-14.4446958356332527 	
+13.4992090127023872 	
+14.4141233331845555 	
+14.9482509617831152 	
+14.444695835622035 	
 ]
 1 [ 4  4  [ 
-0.510839430940516981 	0.283483493893925242 	0.0642765858976659388 	-0.555619592261329842 	
-0.283483493893680105 	0.391896855155454082 	0.103882973848694604 	-0.369897017886295032 	
-0.0642765858979466032 	0.10388297384912093 	0.285799680582960625 	0.185897571271347317 	
--0.555619592261404449 	-0.369897017886252399 	0.185897571271155471 	2.23845483384018262 	
+0.510839430955278506 	0.283483493901130146 	0.0642765859007319307 	-0.555619592268506324 	
+0.283483493901130146 	0.39189685516754752 	0.1038829738530751 	-0.369897017889378787 	
+0.064276585900728378 	0.1038829738530751 	0.285799680593760874 	0.185897571277269691 	
+-0.555619592268506324 	-0.369897017889378787 	0.185897571277269691 	2.23845483387388011 	
 ]
 ] 



From tihocan at mail.berlios.de  Wed Apr  8 16:40:42 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 8 Apr 2009 16:40:42 +0200
Subject: [Plearn-commits] r10099 - trunk/plearn_learners/generic
Message-ID: <200904081440.n38Eeg2a001969@sheep.berlios.de>

Author: tihocan
Date: 2009-04-08 16:40:42 +0200 (Wed, 08 Apr 2009)
New Revision: 10099

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
(Hopefully) fixed the 'class_error' cost function so that it works with one-hot encoding

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2009-04-07 16:37:02 UTC (rev 10098)
+++ trunk/plearn_learners/generic/NNet.cc	2009-04-08 14:40:42 UTC (rev 10099)
@@ -41,6 +41,7 @@
 
 #include <plearn/var/AffineTransformVariable.h>
 #include <plearn/var/AffineTransformWeightPenalty.h>
+#include <plearn/var/ArgmaxVariable.h>
 #include <plearn/var/BinaryClassificationLossVariable.h>
 #include <plearn/var/ClassificationLossVariable.h>
 #include <plearn/var/ConcatColumnsVariable.h>
@@ -1008,8 +1009,14 @@
     {
         if (the_output->width()==1)
             return binary_classification_loss(the_output, the_target);
-        else
-            return classification_loss(the_output, the_target);
+        else {
+            Var targ = the_target;
+            if (targetsize() > 1)
+                // One-hot encoding of target: we need to convert it to an
+                // index in order to be able to use 'classification_loss'.
+                targ = argmax(the_target);
+            return classification_loss(the_output, targ);
+        }
     }
     else if (costname=="binary_class_error")
         return binary_classification_loss(the_output, the_target);



From nouiz at mail.berlios.de  Wed Apr  8 20:44:36 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 8 Apr 2009 20:44:36 +0200
Subject: [Plearn-commits] r10100 - trunk/plearn_learners/cgi
Message-ID: <200904081844.n38IiawW004608@sheep.berlios.de>

Author: nouiz
Date: 2009-04-08 20:44:35 +0200 (Wed, 08 Apr 2009)
New Revision: 10100

Modified:
   trunk/plearn_learners/cgi/ConfigParsing.cc
Log:
moved the metadatadir to another folder.


Modified: trunk/plearn_learners/cgi/ConfigParsing.cc
===================================================================
--- trunk/plearn_learners/cgi/ConfigParsing.cc	2009-04-08 14:40:42 UTC (rev 10099)
+++ trunk/plearn_learners/cgi/ConfigParsing.cc	2009-04-08 18:44:35 UTC (rev 10100)
@@ -79,7 +79,7 @@
     input.reorder_fieldspec_from_headers=1;
     input.txtfilenames.append(args[0]);
     input.partial_match=1;
-    input.setMetaDataDir(args[0]+".metadatadir");
+    input.setMetaDataDir("output.prepro/"+args[0]+".metadatadir");
     input.build();
     bool all_uptodate = true;
     for(int i=1;i<=5;i++)



From nouiz at mail.berlios.de  Wed Apr  8 21:59:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 8 Apr 2009 21:59:35 +0200
Subject: [Plearn-commits] r10101 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200904081959.n38JxZCj012676@sheep.berlios.de>

Author: nouiz
Date: 2009-04-08 21:59:30 +0200 (Wed, 08 Apr 2009)
New Revision: 10101

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
added option for condor --debug and --local_log_file.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 18:44:35 UTC (rev 10100)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 19:59:30 UTC (rev 10101)
@@ -800,9 +800,14 @@
         self.to_all = False
         self.keep_failed_jobs_in_queue = False
         self.clean_up = True
+        self.max_file_size = 10*1024*1024 #in blocks size, here they are 1k each
+        self.debug = False
+        self.local_log_file = False
 
         DBIBase.__init__(self, commands, **args)
-
+        if self.debug:
+            self.condor_submit_exec+=" -debug"
+            self.condor_submit_dag_exec+=" -debug"
         valid_universe = ["standard", "vanilla", "grid", "java", "scheduler", "local", "parallel", "vm"]
         if not self.universe in valid_universe:
             raise DBIError("[DBI] ERROR: the universe option have an invalid value",self.universe,". Valid values are:",valid_universe)
@@ -1183,12 +1188,15 @@
         if self.to_all:
             raise DBIError("[DBI] ERROR: condor backend don't support the option --to_all and a maximum number of process")
         condor_submit_fd = open( self.condor_submit_file, 'w' )
-
-        self.log_file = os.path.join("/tmp/bastienf/dbidispatch",self.log_dir)
+        if os.path.exists("/Tmp"):
+            self.log_file = "/Tmp"
+        else:
+            self.log_file = "/tmp"
+        self.log_file = os.path.join(self.log_file,os.getenv("USER"),"dbidispatch",self.log_dir)
         os.system('mkdir -p ' + self.log_file)
         self.log_file = os.path.join(self.log_file,"condor.log")
         self.print_common_condor_submit(condor_submit_fd, "$(stdout)", "$(stderr)","$(args)")
-
+        
         condor_submit_fd.write("\nqueue\n")
         condor_submit_fd.close()
 
@@ -1243,7 +1251,14 @@
 
 
         condor_submit_fd = open( self.condor_submit_file, 'w' )
-        self.log_file= os.path.join(self.log_dir,"condor.log")
+        self.log_file = os.path.join(self.log_dir,"condor.log")
+        if self.local_log_file:
+            if os.path.exists("/Tmp"):
+                self.log_file = "/Tmp"
+            else:
+                self.log_file = "/tmp"
+            self.log_file = os.path.join(self.log_file,os.getenv("USER"),"dbidispatch",self.log_dir)
+
         self.print_common_condor_submit(condor_submit_fd, self.log_dir+"/$(Process).out", self.log_dir+"/$(Process).error")
 
         if self.pkdilly:

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-08 18:44:35 UTC (rev 10100)
+++ trunk/scripts/dbidispatch	2009-04-08 19:59:30 UTC (rev 10101)
@@ -34,6 +34,8 @@
                               [--machine=HOSTNAME+] [--machines=regex+]
                               [--no_machine=HOSTNAME+]
                               [--[*no_]keep_failed_jobs_in_queue]
+                              [--max_file_size=N][--[no_]debug]
+                              [--[no_]local_log_file]
 
 An * after '[', '{' or ',' signals the default value.
 An + tell that we can put one or more separeted by a comma
@@ -202,6 +204,10 @@
   The '--[no_]keep_failed_jobs_in_queue' option will cause the jobs to stay 
       in the queue in completed status if it failed. You should do condor_rm 
       after to remove it from the queue.
+  The '--max_file_size=N' option tell the maximum file size. Default 10G.
+  The '--[no_]debug' option is forwarded to condor_submit
+  The '--[no_]local_log_file' option tell to put the condor log file on the 
+      local disk. This help to solv a bug with condor and lock on NFS directory.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -294,12 +300,14 @@
     elif argv in  ["--force", "--interruptible", "--long", 
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
                    "--set_special_env", "--abs_path", "--pkdilly", "--to_all",
-                   "--m32G", "--keep_failed_jobs_in_queue", "--restart"]:
+                   "--m32G", "--keep_failed_jobs_in_queue", "--restart",
+                   "--debug", "--local_log_file"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
                   "--no_set_special_env", "--no_abs_path", "--no_pkdilly",
-                  "--no_m32G", "--no_keep_failed_jobs_in_queue", "--no_restart"]:
+                  "--no_m32G", "--no_keep_failed_jobs_in_queue", "--no_restart",
+                  "--no_debug", "--no_local_log_file"]:
         dbi_param[argv[5:]]=False
     elif argv=="--testdbi":
         dbi_param["test"]=True
@@ -316,7 +324,8 @@
                                 "--universe", "--exp_dir", "--machine", "--machines",
                                 "--queue", "--nano", "--submit_options",
                                 "--jobs_name", "--file", "--tasks_filename",
-                                "--only_n_first", "--no_machine" ]:
+                                "--only_n_first", "--no_machine",
+                                "--max_file_size" ]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -380,7 +389,9 @@
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "no_machine","to_all",
-                       "keep_failed_jobs_in_queue", "tasks_filename", "restart"]
+                       "keep_failed_jobs_in_queue", "tasks_filename", "restart",
+                       "max_file_size", "debug", "local_log_file"
+                       ]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
                        "nano", "queue", "raw", "submit_options", "jobs_name",



From nouiz at mail.berlios.de  Wed Apr  8 22:06:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 8 Apr 2009 22:06:45 +0200
Subject: [Plearn-commits] r10102 - trunk/python_modules/plearn/parallel
Message-ID: <200904082006.n38K6juh013600@sheep.berlios.de>

Author: nouiz
Date: 2009-04-08 22:06:45 +0200 (Wed, 08 Apr 2009)
New Revision: 10102

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
small bugfix


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 19:59:30 UTC (rev 10101)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 20:06:45 UTC (rev 10102)
@@ -1193,8 +1193,8 @@
         else:
             self.log_file = "/tmp"
         self.log_file = os.path.join(self.log_file,os.getenv("USER"),"dbidispatch",self.log_dir)
+        self.log_file = os.path.join(self.log_file,"condor.log")
         os.system('mkdir -p ' + self.log_file)
-        self.log_file = os.path.join(self.log_file,"condor.log")
         self.print_common_condor_submit(condor_submit_fd, "$(stdout)", "$(stderr)","$(args)")
         
         condor_submit_fd.write("\nqueue\n")



From nouiz at mail.berlios.de  Wed Apr  8 22:08:52 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 8 Apr 2009 22:08:52 +0200
Subject: [Plearn-commits] r10103 - trunk/python_modules/plearn/parallel
Message-ID: <200904082008.n38K8qJ6013888@sheep.berlios.de>

Author: nouiz
Date: 2009-04-08 22:08:52 +0200 (Wed, 08 Apr 2009)
New Revision: 10103

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
real fix.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 20:06:45 UTC (rev 10102)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 20:08:52 UTC (rev 10103)
@@ -1193,8 +1193,8 @@
         else:
             self.log_file = "/tmp"
         self.log_file = os.path.join(self.log_file,os.getenv("USER"),"dbidispatch",self.log_dir)
+        os.system('mkdir -p ' + self.log_file)
         self.log_file = os.path.join(self.log_file,"condor.log")
-        os.system('mkdir -p ' + self.log_file)
         self.print_common_condor_submit(condor_submit_fd, "$(stdout)", "$(stderr)","$(args)")
         
         condor_submit_fd.write("\nqueue\n")
@@ -1258,7 +1258,8 @@
             else:
                 self.log_file = "/tmp"
             self.log_file = os.path.join(self.log_file,os.getenv("USER"),"dbidispatch",self.log_dir)
-
+            os.system('mkdir -p ' + self.log_file)
+            self.log_file = os.path.join(self.log_file,"condor.log")
         self.print_common_condor_submit(condor_submit_fd, self.log_dir+"/$(Process).out", self.log_dir+"/$(Process).error")
 
         if self.pkdilly:



From nouiz at mail.berlios.de  Wed Apr  8 22:27:21 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 8 Apr 2009 22:27:21 +0200
Subject: [Plearn-commits] r10104 - trunk/python_modules/plearn/parallel
Message-ID: <200904082027.n38KRLfF015710@sheep.berlios.de>

Author: nouiz
Date: 2009-04-08 22:27:21 +0200 (Wed, 08 Apr 2009)
New Revision: 10104

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
set the default to true to put condor.log on the local disk.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 20:08:52 UTC (rev 10103)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-08 20:27:21 UTC (rev 10104)
@@ -802,7 +802,7 @@
         self.clean_up = True
         self.max_file_size = 10*1024*1024 #in blocks size, here they are 1k each
         self.debug = False
-        self.local_log_file = False
+        self.local_log_file = True#by default true as condor can have randomly failure otherwise.
 
         DBIBase.__init__(self, commands, **args)
         if self.debug:



From larocheh at mail.berlios.de  Thu Apr  9 19:20:28 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 9 Apr 2009 19:20:28 +0200
Subject: [Plearn-commits] r10105 - trunk/plearn_learners_experimental
Message-ID: <200904091720.n39HKSFn019714@sheep.berlios.de>

Author: larocheh
Date: 2009-04-09 19:20:25 +0200 (Thu, 09 Apr 2009)
New Revision: 10105

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Added code to compute partition function Z with AIS


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2009-04-08 20:27:21 UTC (rev 10104)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2009-04-09 17:20:25 UTC (rev 10105)
@@ -76,11 +76,18 @@
     n_selected_inputs_cd( -1 ),
     //select_among_k_most_frequent( -1 ),
     compute_input_space_nll( false ),
+    compute_Z_exactly( true ),
+    use_ais_to_compute_Z( false ),
+    n_ais_chains( 100 ),
     pseudolikelihood_context_size ( 0 ),
     pseudolikelihood_context_type( "uniform_random" ),
     k_most_correlated( -1 ),
     generative_learning_weight( 0 ),
     nll_cost_index( -1 ),
+    log_Z_cost_index( -1 ),
+    log_Z_ais_cost_index( -1 ),
+    log_Z_interval_lower_cost_index( -1 ),
+    log_Z_interval_upper_cost_index( -1 ),
     class_cost_index( -1 ),
     training_cpu_time_cost_index ( -1 ),
     cumulative_training_time_cost_index ( -1 ),
@@ -88,7 +95,11 @@
     cumulative_training_time( 0 ),
     //cumulative_testing_time( 0 ),
     log_Z( MISSING_VALUE ),
-    Z_is_up_to_date( false )
+    log_Z_ais( MISSING_VALUE ),
+    log_Z_down( MISSING_VALUE ),
+    log_Z_up( MISSING_VALUE ),
+    Z_is_up_to_date( false ),
+    Z_ais_is_up_to_date( false )
 {
     random_gen = new PRandom();
 }
@@ -205,9 +216,52 @@
                   &PseudolikelihoodRBM::compute_input_space_nll,
                   OptionBase::buildoption,
                   "Indication that the input space NLL should be "
-                  "computed during test.\n"
+                  "computed during test. It will require a procedure to compute\n"
+                  "the partition function Z, which can be exact (see compute_Z_exactly)\n"
+                  "or approximate (see use_ais_to_compute_Z). If both are true,\n"
+                  "exact computation will be used.\n"
                   );
 
+    declareOption(ol, "compute_Z_exactly",
+                  &PseudolikelihoodRBM::compute_Z_exactly,
+                  OptionBase::buildoption,
+                  "Indication that the partition function Z should be computed exactly.\n"
+                  );
+
+    declareOption(ol, "use_ais_to_compute_Z",
+                  &PseudolikelihoodRBM::use_ais_to_compute_Z,
+                  OptionBase::buildoption,
+                  "Whether to use AIS (see Salakhutdinov and Murray ICML2008) to\n"
+                  "compute Z. Assumes the input layer is an RBMBinomialLayer.\n"
+                  );
+
+    declareOption(ol, "n_ais_chains", 
+                  &PseudolikelihoodRBM::n_ais_chains,
+                  OptionBase::buildoption,
+                  "Number of AIS chains.\n"
+                  );
+
+    declareOption(ol, "ais_beta_begin", 
+                  &PseudolikelihoodRBM::ais_beta_begin,
+                  OptionBase::buildoption,
+                  "List of interval beginnings, used to specify the beta schedule.\n"
+                  "Its first element is always set to 0.\n"
+                  );
+
+    declareOption(ol, "ais_beta_end", 
+                  &PseudolikelihoodRBM::ais_beta_end,
+                  OptionBase::buildoption,
+                  "List of interval ends, used to specify the beta schedule.\n"
+                  "Its last element is always set to 1.\n"
+                  );
+
+    declareOption(ol, "ais_beta_n_steps", 
+                  &PseudolikelihoodRBM::ais_beta_n_steps,
+                  OptionBase::buildoption,
+                  "Number of steps in each of the beta interval, used to "
+                  "specify the beta schedule.\n"
+                  );
+
     declareOption(ol, "pseudolikelihood_context_size", 
                   &PseudolikelihoodRBM::pseudolikelihood_context_size,
                   OptionBase::buildoption,
@@ -286,12 +340,30 @@
 
     declareOption(ol, "log_Z", &PseudolikelihoodRBM::log_Z,
                   OptionBase::learntoption,
-                  "Normalisation constant (on log scale).\n");
+                  "Normalisation constant, computed exactly (on log scale).\n");
 
+    declareOption(ol, "log_Z_ais", &PseudolikelihoodRBM::log_Z_ais,
+                  OptionBase::learntoption,
+                  "Normalisation constant, computed by AIS (on log scale).\n");
+
+    declareOption(ol, "log_Z_down", &PseudolikelihoodRBM::log_Z_down,
+                  OptionBase::learntoption,
+                  "Lower bound of confidence interval for log_Z.\n");
+
+    declareOption(ol, "log_Z_up", &PseudolikelihoodRBM::log_Z_up,
+                  OptionBase::learntoption,
+                  "Upper bound of confidence interval for log_Z.\n");
+
     declareOption(ol, "Z_is_up_to_date", &PseudolikelihoodRBM::Z_is_up_to_date,
                   OptionBase::learntoption,
-                  "Indication that the normalisation constant Z is up to date.\n");
+                  "Indication that the normalisation constant Z (computed exactly) "
+                  "is up to date.\n");
 
+    declareOption(ol, "Z_ais_is_up_to_date", &PseudolikelihoodRBM::Z_ais_is_up_to_date,
+                  OptionBase::learntoption,
+                  "Indication that the normalisation constant Z (computed with AIS) "
+                  "is up to date.\n");
+
     declareOption(ol, "persistent_gibbs_chain_is_started", 
                   &PseudolikelihoodRBM::persistent_gibbs_chain_is_started,
                   OptionBase::learntoption,
@@ -345,6 +417,21 @@
                     "pseudolikelihood_context_size should be > 0 "
                     "for \"most_correlated\" context type");        
 
+        if( compute_input_space_nll && use_ais_to_compute_Z )
+        {
+            if( n_ais_chains <= 0 )
+                PLERROR("In PseudolikelihoodRBM::build_(): "
+                        "n_ais_chains should be > 0.");
+            if( ais_beta_n_steps.length() == 0 )
+                PLERROR("In PseudolikelihoodRBM::build_(): "
+                        "AIS schedule should have at least 1 interval of betas.");
+            if( ais_beta_n_steps.length() != ais_beta_begin.length() ||
+                ais_beta_n_steps.length() != ais_beta_end.length() )
+                PLERROR("In PseudolikelihoodRBM::build_(): "
+                        "ais_beta_begin, ais_beta_end and ais_beta_n_steps should "
+                        "all be of the same length.");
+        }
+
         build_layers_and_connections();
         build_costs();
 
@@ -366,6 +453,21 @@
         cost_names.append("NLL");
         nll_cost_index = current_index;
         current_index++;
+        if( compute_Z_exactly )
+        {
+            cost_names.append("log_Z");
+            log_Z_cost_index = current_index++;
+        }
+        
+        if( use_ais_to_compute_Z )
+        {
+            cost_names.append("log_Z_ais");
+            log_Z_ais_cost_index = current_index++;
+            cost_names.append("log_Z_interval_lower");
+            log_Z_interval_lower_cost_index = current_index++;
+            cost_names.append("log_Z_interval_upper");
+            log_Z_interval_upper_cost_index = current_index++;
+        }
     }
     
     if( targetsize() > 0 )
@@ -669,6 +771,7 @@
     cumulative_training_time = 0;
     //cumulative_testing_time = 0;
     Z_is_up_to_date = false;
+    Z_ais_is_up_to_date = false;
 
     persistent_gibbs_chain_is_started.fill( false );
     correlations_per_i.resize(0,0);
@@ -739,6 +842,7 @@
     for( ; stage<nstages ; stage++ )
     {
         Z_is_up_to_date = false;
+        Z_ais_is_up_to_date = false;
         train_set->getExample(stage%nsamples, input, target, weight);
 
         if( pb )
@@ -3505,7 +3609,25 @@
             connection->setAsDownInput( input );
             hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
             costs[nll_cost_index] = hidden_layer->freeEnergyContribution(
-                hidden_layer->activation) - dot(input,input_layer->bias) + log_Z;
+                hidden_layer->activation) - dot(input,input_layer->bias);
+            if( compute_Z_exactly )
+                costs[nll_cost_index] += log_Z;
+            else if( use_ais_to_compute_Z )
+                costs[nll_cost_index] += log_Z_ais;
+            else
+                PLERROR("In PseudolikelihoodRBM::computeCostsFromOutputs(): "
+                    "can't compute NLL without a mean to compute log(Z).");
+
+            if( compute_Z_exactly )
+            {
+                costs[log_Z_cost_index] = log_Z;
+            }
+            if( use_ais_to_compute_Z )
+            {
+                costs[log_Z_ais_cost_index] = log_Z_ais;
+                costs[log_Z_interval_lower_cost_index] = log_Z_down;
+                costs[log_Z_interval_upper_cost_index] = log_Z_up;
+            }
         }
     }
     costs[cumulative_training_time_cost_index] = cumulative_training_time;
@@ -3542,55 +3664,185 @@
 
 void PseudolikelihoodRBM::compute_Z() const
 {
-    if( Z_is_up_to_date ) return;
 
     int input_n_conf = input_layer->getConfigurationCount(); 
     int hidden_n_conf = hidden_layer->getConfigurationCount();
-    if( input_n_conf == RBMLayer::INFINITE_CONFIGURATIONS && 
+    if( !Z_is_up_to_date && compute_Z_exactly &&
+        input_n_conf == RBMLayer::INFINITE_CONFIGURATIONS && 
         hidden_n_conf == RBMLayer::INFINITE_CONFIGURATIONS )
         PLERROR("In PseudolikelihoodRBM::computeCostsFromOutputs: "
                 "RBM's input and hidden layers are too big "
-                "for NLL computations.");
+                "for exact NLL computations.");
 
-    log_Z = 0;
-    if( input_n_conf < hidden_n_conf )
+    if( !Z_ais_is_up_to_date && use_ais_to_compute_Z )
     {
-        conf.resize( input_layer->size );
-        for(int i=0; i<input_n_conf; i++)
+        log_Z_ais = 0;
+        // This AIS code is based on the Matlab code of Russ, on his web page //
+
+        // Compute base-rate RBM biases
+        Vec input( inputsize() );
+        Vec target( targetsize() );
+        real weight;
+        Vec base_rate_rbm_bias( inputsize() );
+        base_rate_rbm_bias.clear();
+        for( int i=0; i<train_set->length(); i++ )
         {
-            input_layer->getConfiguration(i,conf);
-            connection->setAsDownInput( conf );
-            hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
-            if( i == 0 )
-                log_Z = -hidden_layer->freeEnergyContribution(
-                    hidden_layer->activation) + dot(conf,input_layer->bias);
-            else
-                log_Z = logadd(-hidden_layer->freeEnergyContribution(
-                                   hidden_layer->activation) 
-                               + dot(conf,input_layer->bias),
-                               log_Z);
+            train_set->getExample(i, input, target, weight);
+            base_rate_rbm_bias += input;
         }
+        base_rate_rbm_bias += 0.05*train_set->length();
+        base_rate_rbm_bias /= 1.05*train_set->length();
+        for( int j=0; j<inputsize(); j++ )
+            base_rate_rbm_bias[j] = pl_log( base_rate_rbm_bias[j] ) - 
+                pl_log( 1-base_rate_rbm_bias[j] );
+        
+        Mat ais_chain_init_samples( n_ais_chains,inputsize() );
+        Vec ais_weights( n_ais_chains );
+        ais_weights.clear(); // we'll work on log-scale
+        real beg_beta, end_beta, beta, step_beta;
+        int n_beta;
+        
+        // Start chains
+        real p_j;
+        for( int j=0; j<input_layer->size; j++ )
+        {
+            p_j = sigmoid( base_rate_rbm_bias[j] );
+            for( int c=0; c<n_ais_chains; c++ )
+                ais_chain_init_samples(c,j) = random_gen->binomial_sample( p_j );
+        }
+        input_layer->setBatchSize( n_ais_chains );
+        input_layer->samples << ais_chain_init_samples;
+
+        // Add importance weight contribution (denominator)
+        productScaleAcc( ais_weights, input_layer->samples, false,
+                         base_rate_rbm_bias, -1, 0 );
+        ais_weights -= hidden_layer->size * pl_log(2);
+        for( int k=0; k<ais_beta_n_steps.length(); k++ )
+        {
+            beg_beta = (k==0) ? 0 : ais_beta_begin[k];
+            end_beta = (k == ais_beta_end.length()-1) ? 1 : ais_beta_end[k];
+            if( beg_beta >= end_beta )
+                PLERROR("In PseudolikelihoodRBM::compute_Z(): "
+                        "the AIS beta schedule is not monotonically increasing.");
+
+            n_beta = ais_beta_n_steps[k];
+            if( n_beta == 0)
+                PLERROR("In PseudolikelihoodRBM::compute_Z(): "
+                        "one of the beta intervals has 0 steps.");
+            step_beta = (end_beta - beg_beta)/n_beta;
+
+            beta = beg_beta;
+            for( int k_i=0; k_i < n_beta; k_i++ )
+            {
+                beta += step_beta;
+                // Add importance weight contribution (numerator)
+                productScaleAcc( ais_weights, input_layer->samples, false,
+                                 base_rate_rbm_bias, (1-beta), 1 );
+                productScaleAcc( ais_weights, input_layer->samples, false,
+                                 input_layer->bias, beta, 1 );
+                connection->setAsDownInputs(input_layer->samples);
+                hidden_layer->getAllActivations( 
+                    (RBMMatrixConnection *) connection, 0, true );
+                hidden_layer->activations *= beta;
+                for( int c=0; c<n_ais_chains; c++ )
+                    ais_weights[c] -= hidden_layer->freeEnergyContribution( 
+                        hidden_layer->activations(c) );
+                // Get new chain sample
+                hidden_layer->computeExpectations();
+                hidden_layer->generateSamples();
+                connection->setAsUpInputs(hidden_layer->samples);
+                input_layer->getAllActivations( 
+                    (RBMMatrixConnection *) connection, 0, true );
+                for( int c=0; c<n_ais_chains; c++ )
+                    multiplyScaledAdd(base_rate_rbm_bias,beta,
+                                      (1-beta),input_layer->activations(c));
+                input_layer->computeExpectations();
+                input_layer->generateSamples();
+
+                // Add importance weight contribution (denominator)
+                productScaleAcc( ais_weights, input_layer->samples, false,
+                                 base_rate_rbm_bias, -(1-beta), 1 );
+                productScaleAcc( ais_weights, input_layer->samples, false,
+                                 input_layer->bias, -beta, 1 );
+                connection->setAsDownInputs(input_layer->samples);
+                hidden_layer->getAllActivations( 
+                    (RBMMatrixConnection *) connection, 0, true );
+                hidden_layer->activations *= beta;
+                for( int c=0; c<n_ais_chains; c++ )
+                    ais_weights[c] += hidden_layer->freeEnergyContribution( 
+                        hidden_layer->activations(c) );
+            }
+        }
+        // Final importance weight contribution, at beta=1 (numerator)
+        productScaleAcc( ais_weights, input_layer->samples, false,
+                         input_layer->bias, 1, 1 );
+        connection->setAsDownInputs(input_layer->samples);
+        hidden_layer->getAllActivations( 
+            (RBMMatrixConnection *) connection, 0, true );
+        for( int c=0; c<n_ais_chains; c++ )
+            ais_weights[c] -= hidden_layer->freeEnergyContribution( 
+                hidden_layer->activations(c) );
+
+        real log_r_ais = logadd(ais_weights) - pl_log(n_ais_chains);
+        real log_Z_base =  hidden_layer->size * pl_log(2);
+        for( int j=0; j<inputsize(); j++ )
+            log_Z_base += softplus(base_rate_rbm_bias[j]);
+        log_Z_ais = log_r_ais + log_Z_base;
+
+        real offset = mean(ais_weights);
+        PP<StatsCollector> stats = new StatsCollector();
+        stats->forget();
+        for( int c=0; c<n_ais_chains; c++ )
+            stats->update(exp(ais_weights[c]-offset),1.);
+        stats->finalize();
+        real logstd_ais = pl_log(stats->getStat("STDDEV")) + 
+            offset - pl_log(n_ais_chains)/2;
+        log_Z_up = pl_log(exp(log_r_ais)+exp(logstd_ais)*3) + log_Z_base;
+        log_Z_down = pl_log(exp(log_r_ais)-exp(logstd_ais)*3) + log_Z_base;
+
+        Z_ais_is_up_to_date = true;
     }
-    else
+    if( !Z_is_up_to_date && compute_Z_exactly )
     {
-        conf.resize( hidden_layer->size );
-        for(int i=0; i<hidden_n_conf; i++)
+        log_Z = 0;
+        if( input_n_conf < hidden_n_conf )
         {
-            hidden_layer->getConfiguration(i,conf);
-            connection->setAsUpInput( conf );
-            input_layer->getAllActivations( (RBMMatrixConnection *) connection );
-            if( i == 0 )
-                log_Z = -input_layer->freeEnergyContribution(
-                    input_layer->activation) + dot(conf,hidden_layer->bias);
-            else
-                log_Z = logadd(-input_layer->freeEnergyContribution(
-                                   input_layer->activation)
-                               + dot(conf,hidden_layer->bias),
-                               log_Z);
-        }        
+            conf.resize( input_layer->size );
+            for(int i=0; i<input_n_conf; i++)
+            {
+                input_layer->getConfiguration(i,conf);
+                connection->setAsDownInput( conf );
+                hidden_layer->getAllActivations( (RBMMatrixConnection *) connection );
+                if( i == 0 )
+                    log_Z = -hidden_layer->freeEnergyContribution(
+                        hidden_layer->activation) + dot(conf,input_layer->bias);
+                else
+                    log_Z = logadd(-hidden_layer->freeEnergyContribution(
+                                       hidden_layer->activation) 
+                                   + dot(conf,input_layer->bias),
+                                   log_Z);
+            }
+        }
+        else
+        {
+            conf.resize( hidden_layer->size );
+            for(int i=0; i<hidden_n_conf; i++)
+            {
+                hidden_layer->getConfiguration(i,conf);
+                connection->setAsUpInput( conf );
+                input_layer->getAllActivations( (RBMMatrixConnection *) connection );
+                if( i == 0 )
+                    log_Z = -input_layer->freeEnergyContribution(
+                        input_layer->activation) + dot(conf,hidden_layer->bias);
+                else
+                    log_Z = logadd(-input_layer->freeEnergyContribution(
+                                       input_layer->activation)
+                                   + dot(conf,hidden_layer->bias),
+                                   log_Z);
+            }        
+        }
+        Z_is_up_to_date = true;
     }
-    
-    Z_is_up_to_date = true;
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2009-04-08 20:27:21 UTC (rev 10104)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2009-04-09 17:20:25 UTC (rev 10105)
@@ -126,9 +126,40 @@
     //int select_among_k_most_frequent;
 
     //! Indication that the input space NLL should be computed
-    //! during test
+    //! during test. It will require a procedure to compute
+    //! the partition function Z, which can be exact (see compute_Z_exactly)
+    //! or approximate (see use_ais_to_compute_Z). If both are true,
+    //! exact computation will be used.
     bool compute_input_space_nll;
 
+    //! Indication that the partition function should be computed exactly
+    bool compute_Z_exactly;
+
+    //! Whether to use AIS (see Salakhutdinov and Murray ICML2008) to
+    //! compute Z. Assumes the input layer is an RBMBinomialLayer.
+    bool use_ais_to_compute_Z;
+
+    //! Number of AIS chains.
+    int n_ais_chains;
+
+    // Schedule information for the betas in AIS. 
+    //! List of interval beginnings, used to specify the beta schedule.
+    //! Its first element is always set to 0.
+    Vec ais_beta_begin;
+    //! List of interval ends, used to specify the beta schedule.
+    //! Its last element is always set to 1.
+    Vec ais_beta_end;
+    //! Number of steps in each of the beta interval, used to specify the beta schedule
+    TVec<int> ais_beta_n_steps;
+
+    // Each row gives
+    //! the triplet <a_i,b_i,N_i>, which indicate that
+    //! in interval [a_i,b_i], N_i betas should be uniformly
+    //! laid out. The values of a_0 and b_{ais_beta_schedule.length()-1} are
+    //! fixed to 0 and 1 respectively, i.e. the values
+    //! for them as given by the option are ignored.
+    Mat ais_beta_schedule;
+
     //! Number of additional input variables chosen to form the joint
     //! condition likelihoods in generalized pseudolikelihood
     //! (default = 0, which corresponds to standard pseudolikelihood)
@@ -325,6 +356,17 @@
     //! Keeps the index of the NLL cost in train_costs
     int nll_cost_index;
 
+    //! Index of log_Z "cost"
+    int log_Z_cost_index;
+    //! Index of log_Z "cost", computed by AIS
+    int log_Z_ais_cost_index;
+    //! Index of lower bound of confidence interval for log_Z,
+    //! as computed by AIS
+    int log_Z_interval_lower_cost_index;
+    //! Index of upper bound of confidence interval for log_Z,
+    //! as computed by AIS
+    int log_Z_interval_upper_cost_index;
+
     //! Keeps the index of the class_error cost in train_costs
     int class_cost_index;
 
@@ -337,12 +379,22 @@
     real cumulative_training_time;
     //real cumulative_testing_time;
     
-    //! Normalisation constant (on log scale)
+    //! Normalisation constant, computed exactly (on log scale)
     mutable real log_Z;
+    //! Normalisation constant, computed by AIS (on log scale)
+    mutable real log_Z_ais;
 
-    //! Indication that the normalisation constant Z is up to date
+    //! Lower bound of confidence interval for log_Z
+    mutable real log_Z_down;
+    //! Upper bound of confidence interval for log_Z
+    mutable real log_Z_up;
+
+    //! Indication that the normalisation constant Z (computed exactly) is up to date
     mutable bool Z_is_up_to_date;
 
+    //! Indication that the normalisation constant Z (computed with AIS) is up to date
+    mutable bool Z_ais_is_up_to_date;
+
     //! Indication that the prolonged gibbs chain for 
     //! Persistent Consistent Divergence is started, for each chain
     mutable TVec<bool> persistent_gibbs_chain_is_started;



From larocheh at mail.berlios.de  Thu Apr  9 19:23:00 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 9 Apr 2009 19:23:00 +0200
Subject: [Plearn-commits] r10106 - trunk/plearn_learners/online
Message-ID: <200904091723.n39HN0sM020745@sheep.berlios.de>

Author: larocheh
Date: 2009-04-09 19:22:59 +0200 (Thu, 09 Apr 2009)
New Revision: 10106

Modified:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
Implemented mini-batch version of computeExpectation and generateSample


Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2009-04-09 17:20:25 UTC (rev 10105)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2009-04-09 17:22:59 UTC (rev 10106)
@@ -117,7 +117,44 @@
 
     PLASSERT( samples.width() == size && samples.length() == batch_size );
 
-    PLERROR( "RBMWoodsLayer::generateSamples(): not implemented yet" );
+    //PLERROR( "RBMWoodsLayer::generateSamples(): not implemented yet" );
+    samples.clear();
+
+    int n_nodes_per_tree = size / n_trees;
+    int node, depth, node_sample, sub_tree_size;
+    int offset = 0;
+
+    for( int b=0; b<batch_size; b++ )
+    {
+        offset = 0;
+        for( int t=0; t<n_trees; t++ )
+        {
+            depth = 0;
+            node = n_nodes_per_tree / 2;
+            sub_tree_size = node;
+            while( depth < tree_depth )
+            {
+                // HUGO: Note that local_node_expectation is really
+                // used as a probability, even for signed samples.
+                // Sorry for the misleading choice of variable name...
+                node_sample = random_gen->binomial_sample(
+                    local_node_expectations(b, node + offset ) );
+                if( use_signed_samples )
+                    samples(b,node + offset) = 2*node_sample-1;
+                else
+                    samples(b,node + offset) = node_sample;
+                
+                // Descending in the tree
+                sub_tree_size /= 2;
+                if ( node_sample > 0.5 )
+                    node -= sub_tree_size+1;
+                else
+                    node += sub_tree_size+1;
+                depth++;
+            }
+            offset += n_nodes_per_tree;
+        }    
+    }
 }
 
 void RBMWoodsLayer::computeProbabilisticClustering(Vec& prob_clusters)
@@ -316,8 +353,178 @@
     if( expectations_are_up_to_date )
         return;
 
-    PLERROR( "RBMWoodsLayer::computeExpectations(): not implemented yet" );
+    PLASSERT( expectations.width() == size
+              && expectations.length() == batch_size );
+    off_expectations.resize(batch_size,size);
+    local_node_expectations.resize(batch_size,size);
+    on_free_energies.resize(batch_size,size);
+    off_free_energies.resize(batch_size,size);
 
+    int n_nodes_per_tree = size / n_trees;
+    int node, depth, sub_tree_size, grand_parent;
+    int offset = 0;
+    bool left_of_grand_parent;
+    real grand_parent_prob;
+    for( int b=0; b<batch_size; b++ )
+    {
+        offset=0;
+        // Get local expectations at every node
+        
+        // HUGO: Note that local_node_expectations is really
+        // used as a probability, even for signed samples.
+        // Sorry for the misleading choice of variable name...
+        
+        // Divide and conquer computation of local (conditional) free energies
+        for( int t=0; t<n_trees; t++ )
+        {
+            depth = tree_depth-1;
+            sub_tree_size = 0;
+
+            // Initialize last level
+            for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+            {
+                //on_free_energies(b, n + offset ) = safeexp(activations(b,n+offset));
+                //off_free_energies(b, n + offset ) = 1;
+                // Now working in log-domain
+                on_free_energies(b, n + offset ) = activations(b,n+offset);
+                if( use_signed_samples )
+                    off_free_energies(b, n + offset ) = -activations(b,n+offset);
+                else
+                    off_free_energies(b, n + offset ) = 0;
+            }
+
+            depth = tree_depth-2;
+            sub_tree_size = 1;
+
+            while( depth >= 0 )
+            {
+                for( int n=sub_tree_size; n<n_nodes_per_tree; n += 2*sub_tree_size + 2 )
+                {
+                    //on_free_energies(b, n + offset ) = safeexp(activations(b,n+offset)) *
+                    //    ( on_free_energies(b,n + offset - sub_tree_size) + off_free_energies(b,n + offset - sub_tree_size) ) ;
+                    //off_free_energies(b, n + offset ) =
+                    //    ( on_free_energies(b,n + offset + sub_tree_size) + off_free_energies(b,n + offset + sub_tree_size) ) ;
+                    // Now working in log-domain
+                    on_free_energies(b, n + offset ) = activations(b,n+offset) +
+                        logadd( on_free_energies(b,n + offset - (sub_tree_size/2+1)),
+                                off_free_energies(b,n + offset - (sub_tree_size/2+1)) ) ;
+                    if( use_signed_samples )
+                        off_free_energies(b, n + offset ) = -activations(b,n+offset) +
+                            logadd( on_free_energies(b,n + offset + (sub_tree_size/2+1)),
+                                    off_free_energies(b,n + offset + (sub_tree_size/2+1)) ) ;
+                    else
+                        off_free_energies(b, n + offset ) =
+                            logadd( on_free_energies(b,n + offset + (sub_tree_size/2+1)),
+                                    off_free_energies(b,n + offset + (sub_tree_size/2+1)) ) ;
+
+                }
+                sub_tree_size = 2 * ( sub_tree_size + 1 ) - 1;
+                depth--;
+            }
+            offset += n_nodes_per_tree;
+        }
+
+        for( int i=0 ; i<size ; i++ )
+            //local_node_expectations(b,i) = on_free_energies(b,i) / ( on_free_energies(b,i) + off_free_energies(b,i) );
+            // Now working in log-domain
+            local_node_expectations(b,i) = safeexp(on_free_energies(b,i)
+                                                - logadd(on_free_energies(b,i), off_free_energies(b,i)));
+
+        // Compute marginal expectations
+        offset = 0;
+        for( int t=0; t<n_trees; t++ )
+        {
+            // Initialize root
+            node = n_nodes_per_tree / 2;
+            expectations(b, node + offset ) = local_node_expectations(b, node + offset );
+            off_expectations(b, node + offset ) = (1 - local_node_expectations(b, node + offset ));
+            sub_tree_size = node;
+
+            // First level nodes
+            depth = 1;
+            sub_tree_size /= 2;
+
+            // Left child
+            node = sub_tree_size;
+            expectations(b, node + offset ) = local_node_expectations(b, node + offset )
+                * local_node_expectations(b, node + offset + sub_tree_size + 1 );
+            off_expectations(b, node + offset ) = (1 - local_node_expectations(b, node + offset ))
+                * local_node_expectations(b, node + offset + sub_tree_size + 1 );
+
+            // Right child
+            node = 3*sub_tree_size+2;
+            expectations(b, node + offset ) = local_node_expectations(b, node + offset )
+                * (1 - local_node_expectations(b, node + offset - sub_tree_size - 1 ));
+            off_expectations(b, node + offset ) = (1 - local_node_expectations(b, node + offset ))
+                * (1 - local_node_expectations(b, node + offset - sub_tree_size - 1 ));
+
+            // Set other nodes, level-wise
+            depth = 2;
+            sub_tree_size /= 2;
+            while( depth < tree_depth )
+            {
+                // Left child
+                left_of_grand_parent = true;
+                for( int n=sub_tree_size; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+                {
+                    if( left_of_grand_parent )
+                    {
+                        grand_parent = n + offset + 3*sub_tree_size + 3;
+                        grand_parent_prob = expectations(b, grand_parent );
+                        left_of_grand_parent = false;
+                    }
+                    else
+                    {
+                        grand_parent = n + offset - sub_tree_size - 1;
+                        grand_parent_prob = off_expectations(b, grand_parent );
+                        left_of_grand_parent = true;
+                    }
+
+                    expectations(b, n + offset ) = local_node_expectations(b, n + offset )
+                        * local_node_expectations(b, n + offset + sub_tree_size + 1 )
+                        * grand_parent_prob;
+                    off_expectations(b, n + offset ) = (1 - local_node_expectations(b, n + offset ))
+                        * local_node_expectations(b, n + offset + sub_tree_size + 1 )
+                        * grand_parent_prob;
+
+                }
+
+                // Right child
+                left_of_grand_parent = true;
+                for( int n=3*sub_tree_size+2; n<n_nodes_per_tree; n += 4*sub_tree_size + 4 )
+                {
+                    if( left_of_grand_parent )
+                    {
+                        grand_parent = n + offset + sub_tree_size + 1;
+                        grand_parent_prob = expectations(b, grand_parent );
+                        left_of_grand_parent = false;
+                    }
+                    else
+                    {
+                        grand_parent = n + offset - 3*sub_tree_size - 3;
+                        grand_parent_prob = off_expectations(b, grand_parent );
+                        left_of_grand_parent = true;
+                    }
+
+                    expectations(b, n + offset ) = local_node_expectations(b, n + offset )
+                        * (1 - local_node_expectations(b, n + offset - sub_tree_size - 1 ))
+                        * grand_parent_prob;
+                    off_expectations(b, n + offset ) = (1 - local_node_expectations(b, n + offset ))
+                        * (1 - local_node_expectations(b, n + offset - sub_tree_size - 1 ))
+                        * grand_parent_prob;
+                }
+                sub_tree_size /= 2;
+                depth++;
+            }
+            offset += n_nodes_per_tree;
+        }
+    }
+    
+    if( use_signed_samples )
+        for( int b=0; b<batch_size; b++ )
+            for( int i=0; i<expectation.length(); i++ )
+                expectations(b,i) = expectations(b,i) - off_expectations(b,i);
+
     expectations_are_up_to_date = true;
 }
 
@@ -1028,9 +1235,13 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 
     deepCopyField( off_expectation, copies );
+    deepCopyField( off_expectations, copies );
     deepCopyField( local_node_expectation, copies );
+    deepCopyField( local_node_expectations, copies );
     deepCopyField( on_free_energy, copies );
+    deepCopyField( on_free_energies, copies );
     deepCopyField( off_free_energy, copies );
+    deepCopyField( off_free_energies, copies );
     deepCopyField( local_node_expectation_gradient, copies );
     deepCopyField( on_tree_gradient, copies );
     deepCopyField( off_tree_gradient, copies );

Modified: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2009-04-09 17:20:25 UTC (rev 10105)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2009-04-09 17:22:59 UTC (rev 10106)
@@ -161,13 +161,17 @@
 
     // Probability that sampling reaches a node but samples h=0 (expectation is for h=1)
     Vec off_expectation;
+    Mat off_expectations;
     // Ordinary RBMBinomialLayer expectation
     Vec local_node_expectation;
+    Mat local_node_expectations;
 
     // Computations of the local_node_expectation free energies for h = 1
     Vec on_free_energy;
+    Mat on_free_energies;
     // Computations of the local_node_expectation free energies for h = 0
     Vec off_free_energy;
+    Mat off_free_energies;
 
     // Gradient through the local_node_expectations (after sigmoid)
     Vec local_node_expectation_gradient;



From nouiz at mail.berlios.de  Thu Apr  9 19:26:50 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 9 Apr 2009 19:26:50 +0200
Subject: [Plearn-commits] r10107 - trunk/plearn_learners/regressors
Message-ID: <200904091726.n39HQomY022275@sheep.berlios.de>

Author: nouiz
Date: 2009-04-09 19:26:43 +0200 (Thu, 09 Apr 2009)
New Revision: 10107

Modified:
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
removed deep copy of the train_set as this save many times and make use much less memory.


Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-04-09 17:22:59 UTC (rev 10106)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2009-04-09 17:26:43 UTC (rev 10107)
@@ -117,7 +117,9 @@
 void RegressionTreeLeave::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(train_set, copies);
+    //we don't deep copy it as we don't modify it 
+    //and this is a link to the RegressionTree train_set
+//    deepCopyField(train_set, copies);
 }
 
 void RegressionTreeLeave::build()

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-09 17:22:59 UTC (rev 10106)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-09 17:26:43 UTC (rev 10107)
@@ -157,11 +157,11 @@
 void RegressionTreeRegisters::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(tsorted_row, copies);
     deepCopyField(leave_register, copies);
-//tsource should be deep copied, but as currently when it is deep copied
-// the copy is not used anymore to train. To save memory we don't do it.
+//tsource and tsorted_row should be deep copied, but currently when it is deep copied
+// the copy is modified. To save memory we don't do it.
 // It is deep copied eavily by HyperLearner and HyperOptimizer
+//    deepCopyField(tsorted_row, copies);
 //    deepCopyField(tsource,copies);
 //no need to deep copy source as we don't reuse it after initialization
 //    deepCopyField(source,copies);



From laulysta at mail.berlios.de  Fri Apr 10 04:03:01 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 10 Apr 2009 04:03:01 +0200
Subject: [Plearn-commits] r10108 - trunk/plearn_learners_experimental
Message-ID: <200904100203.n3A231ul026284@sheep.berlios.de>

Author: laulysta
Date: 2009-04-10 04:03:00 +0200 (Fri, 10 Apr 2009)
New Revision: 10108

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
auto encoder and expressive timing


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-09 17:26:43 UTC (rev 10107)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-10 02:03:00 UTC (rev 10108)
@@ -90,7 +90,9 @@
     current_learning_rate(0),
     nb_stage_reconstruction(0),
     nb_stage_target(0),
-    noise(false)
+    noise(false),
+    L1_penalty_factor(0),
+    L2_penalty_factor(0)
 {
     random_gen = new PRandom();
 }
@@ -270,6 +272,23 @@
                   OptionBase::learntoption,
                   "The nomber of stage for de target");
 
+    declareOption(ol, "L1_penalty_factor",
+                  &DenoisingRecurrentNet::L1_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L1 regularization term, i.e.\n"
+                  "minimize L1_penalty_factor * sum_{ij} |weights(i,j)| "
+                  "during training.\n");
+
+    declareOption(ol, "L2_penalty_factor",
+                  &DenoisingRecurrentNet::L2_penalty_factor,
+                  OptionBase::buildoption,
+                  "Optional (default=0) factor of L2 regularization term, i.e.\n"
+                  "minimize 0.5 * L2_penalty_factor * sum_{ij} weights(i,j)^2 "
+                  "during training.\n");
+                  
+                  
+                  
+
  /*
     declareOption(ol, "", &DenoisingRecurrentNet::,
                   OptionBase::learntoption,
@@ -379,9 +398,9 @@
             }
         }
 
-        if( tar_layer != target_layers.length() )
-            PLERROR("DenoisingRecurrentNet::build_(): target layers "
-                    "does not cover all targets.");
+        //if( tar_layer != target_layers.length() )
+        //    PLERROR("DenoisingRecurrentNet::build_(): target layers "
+        //            "does not cover all targets.");
 
 
         // Building weights and layers
@@ -510,6 +529,7 @@
     deepCopyField( mean_encoded_vec, copies);
     deepCopyField( input_reconstruction_bias, copies);
     deepCopyField( hidden_reconstruction_bias, copies);
+    deepCopyField( hidden_reconstruction_bias2, copies);
 
     // Protected fields
     deepCopyField( data, copies);
@@ -620,7 +640,7 @@
         for(int i=0; i<nseq; i++)
         {
             getSequence(i, seq);
-            encodeSequenceAndPopulateLists(seq);
+            encodeSequenceAndPopulateLists(seq, false);
             if(i==0)
             {
                 mean_encoded_vec.resize(encoded_seq.width());
@@ -723,25 +743,25 @@
 
                 
                 
-                noise = false;
+                
                 getSequence(i, seq);
-                encodeSequenceAndPopulateLists(seq);
+                encodeSequenceAndPopulateLists(seq, false);
+
+                
               
-                bool corrupt_input = false;//input_noise_prob!=0 && (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
+                //bool corrupt_input = false;//input_noise_prob!=0 && (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
 
-                clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
-                clean_encoded_seq << encoded_seq;
+                //clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
+                //clean_encoded_seq << encoded_seq;
 
-                if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
-                    inject_zero_forcing_noise(encoded_seq, input_noise_prob);
+                //if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
+                      //  inject_zero_forcing_noise(encoded_seq, input_noise_prob);
 
                 // recurrent no noise phase
                 if(stage>=nb_stage_reconstruction){
                     if(recurrent_lr!=0)
                     {
                         
-                        if(noise) // need to recover the clean sequence                        
-                            encoded_seq << clean_encoded_seq;                  
                         setLearningRate( recurrent_lr );                    
                         recurrentFprop(train_costs, train_n_items);
                         recurrentUpdate(0,0,1, prediction_cost_weight,1, train_costs, train_n_items );
@@ -751,20 +771,38 @@
 
                 if(stage<nb_stage_reconstruction || nb_stage_reconstruction == 0 ){
 
+                    // greedy phase hidden
+                    if(hidden_reconstruction_lr!=0){
+                        setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
+                        recurrentFprop(train_costs, train_n_items, true);
+                        //recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
+                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
+                    }
 
+                    /*if(recurrent_lr!=0)
+                    {                 
+                        setLearningRate( recurrent_lr );                    
+                        recurrentFprop(train_costs, train_n_items);
+                        //recurrentUpdate(0,0,1, prediction_cost_weight,0, train_costs, train_n_items );
+                        recurrentUpdate(0,0,0, prediction_cost_weight,0, train_costs, train_n_items );
+                        
+                        }*/
+                    
                     // greedy phase input
                     if(input_reconstruction_lr!=0){
+                        if (noise)
+                            encodeSequenceAndPopulateLists(seq, true);
                         setLearningRate( input_reconstruction_lr );
-                        recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
+                        recurrentFprop(train_costs, train_n_items, false);
+                        if (noise)
+                            encodeSequenceAndPopulateLists(seq, false);
+                        //recurrentUpdate(input_reconstruction_cost_weight, 0, 1, 0,1, train_costs, train_n_items );
+                        recurrentUpdate(input_reconstruction_cost_weight, 0, 0, 0,1, train_costs, train_n_items );
                     }
                     
-                    // greedy phase hidden
-                    if(hidden_reconstruction_lr!=0){
-                        setLearningRate( dynamic_gradient_scale_factor*hidden_reconstruction_lr);
-                        recurrentFprop(train_costs, train_n_items);
-                        recurrentUpdate(0, hidden_reconstruction_cost_weight, 1, 0,1, train_costs, train_n_items );
-                    }
+                    
+                    
+                    
                 }
 
                 // recurrent no noise phase
@@ -838,10 +876,12 @@
 
 
 //! does encoding if needed and populates the list.
-void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
+void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq, bool doNoise) const
 {
     if(encoding=="raw_masked_supervised") // old already encoded format (for backward testing)
-        splitRawMaskedSupervisedSequence(seq);
+        splitRawMaskedSupervisedSequence(seq, doNoise);
+    else if(encoding=="generic")
+        encode_artificialData(seq);
     else
         encodeAndCreateSupervisedSequence(seq);
 }
@@ -882,7 +922,7 @@
 
 
 // For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
-void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq) const
+void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq, bool doNoise) const
 {
     int l = seq.length();
     resize_lists(l);
@@ -893,7 +933,7 @@
     Mat mask_part = seq.subMatColumns(inputsize_without_masks, targetsize());
     Mat target_part = seq.subMatColumns(inputsize_without_masks+targetsize(), targetsize());
 
-    if(noise)
+    if(doNoise)
         inject_zero_forcing_noise(input_part, input_noise_prob);
 
     for(int i=0; i<l; i++)
@@ -915,7 +955,54 @@
     encoded_seq << input_part;
 }
 
+void DenoisingRecurrentNet::encode_artificialData(Mat seq) const
+{
+    int l = seq.length();
+    int theInputsize = inputsize();
+    int theTargetsize = targetsize();
+    resize_lists(l);
+    //int inputsize_without_masks = inputsize-targetsize;
+    Mat input_part;
+    input_part.resize(seq.length(),theInputsize);
+    input_part << seq.subMatColumns(0,theInputsize);
+    //Mat mask_part = seq.subMatColumns(inputsize, targetsize);
+    Mat target_part = seq.subMatColumns(theInputsize, theTargetsize);
 
+    //if(doNoise)
+    //    inject_zero_forcing_noise(input_part, input_noise_prob);
+
+    for(int i=0; i<l; i++)
+        input_list[i] = input_part(i);
+
+    int ntargets = target_layers.length();
+    targets_list.resize(ntargets);
+    //masks_list.resize(ntargets);
+    int startcol = 0; // starting column of next target in target_part and mask_part
+    for(int k=0; k<ntargets; k++)
+    {
+        int targsize = target_layers[k]->size;
+        targets_list[k] = target_part.subMatColumns(startcol, targsize);
+        //masks_list[k] = mask_part.subMatColumns(startcol, targsize);
+        startcol += targsize;
+    }
+
+    encoded_seq.resize(input_part.length(), input_part.width());
+    encoded_seq << input_part;
+
+
+    /*int l = sequence.length();
+ 
+    // reserve one extra bit to mean repetition
+    encoded_sequence.resize(l, 1);
+    encoded_sequence.clear();
+
+    for(int i=0; i<l; i++)
+    {
+        int number = int(sequence(i,0));
+        encoded_sequence(i,0) = number;        
+        }    */
+}    
+
 void DenoisingRecurrentNet::resize_lists(int l) const
 {
     input_list.resize(l);
@@ -987,7 +1074,7 @@
 }
 
 // fprop accumulates costs in costs and counts in n_items
-void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items) const
+void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items, bool useDynamicConnections) const
 {
     int l = input_list.length();
     int ntargets = target_layers.length();
@@ -996,14 +1083,14 @@
     {
         Vec hidden_act_no_bias_i = hidden_act_no_bias_list(i);
         input_connections->fprop( input_list[i], hidden_act_no_bias_i);
-
-        if( i > 0 && dynamic_connections )
-        {
-            Vec hidden_i_prev = hidden_list(i-1);
-            dynamic_connections->fprop(hidden_i_prev,dynamic_act_no_bias_contribution );
-            hidden_act_no_bias_i += dynamic_act_no_bias_contribution;
+        if(useDynamicConnections){
+            if( i > 0 && dynamic_connections )
+            {
+                Vec hidden_i_prev = hidden_list(i-1);
+                dynamic_connections->fprop(hidden_i_prev,dynamic_act_no_bias_contribution );
+                hidden_act_no_bias_i += dynamic_act_no_bias_contribution;
+            }
         }
-        
         Vec hidden_i = hidden_list(i);
         hidden_layer->fprop( hidden_act_no_bias_i, 
                              hidden_i);
@@ -1048,8 +1135,8 @@
             }
         }
     }
-    if(noise)
-        inject_zero_forcing_noise(hidden_list, input_noise_prob);
+    //if(noise)
+    //  inject_zero_forcing_noise(hidden_list, input_noise_prob);
 }
 
 
@@ -1130,7 +1217,8 @@
                                                   int& down_size,
                                                   int& up_size,
                                                   real& lr,
-                                                  bool accumulate)
+                                                  bool accumulate,
+                                                  bool using_penalty_factor)
 {
     PLASSERT( input.size() == down_size );
     PLASSERT( output.size() == up_size );
@@ -1156,7 +1244,8 @@
     //externalProductScaleAcc( weights, output_gradient, input, -lr );
     externalProductScaleAcc( acc_weights_gr, output_gradient, input, -lr );
     
-   
+    if((!fast_exact_is_equal(L1_penalty_factor,0) || !fast_exact_is_equal(L2_penalty_factor,0)) && using_penalty_factor)
+        applyWeightPenalty(weights, acc_weights_gr, down_size, up_size, lr);
 }
 
 void DenoisingRecurrentNet::bpropUpdateHiddenLayer(const Vec& input, 
@@ -1194,6 +1283,45 @@
     //applyBiasDecay();
 }
 
+void DenoisingRecurrentNet::applyWeightPenalty(Mat& weights, Mat& acc_weights_gr, int& down_size, int& up_size, real& lr)
+{
+    // Apply penalty (decay) on weights.
+    real delta_L1 = lr * L1_penalty_factor;
+    real delta_L2 = lr * L2_penalty_factor;
+    /*if (L2_decrease_type == "one_over_t")
+        delta_L2 /= (1 + L2_decrease_constant * L2_n_updates);
+    else if (L2_decrease_type == "sigmoid_like")
+        delta_L2 *= sigmoid((L2_shift - L2_n_updates) * L2_decrease_constant);
+    else
+        PLERROR("In RBMMatrixConnection::applyWeightPenalty - Invalid value "
+                "for L2_decrease_type: %s", L2_decrease_type.c_str());
+    */
+    for( int i=0; i<up_size; i++)
+    {
+        real* w_ = weights[i];
+        real* a_w_g = acc_weights_gr[i];
+        for( int j=0; j<down_size; j++ )
+        {
+            if( delta_L2 != 0. ){
+                //w_[j] *= (1 - delta_L2);
+                a_w_g[j] -= w_[j]*delta_L2;
+            }
+
+            if( delta_L1 != 0. )
+            {
+                if( w_[j] > delta_L1 )
+                    a_w_g[j] -= delta_L1;
+                else if( w_[j] < -delta_L1 )
+                    a_w_g[j] += delta_L1;
+                else
+                    a_w_g[j] = 0.;
+            }
+        }
+    }
+    /*if (delta_L2 > 0)
+      L2_n_updates++;*/
+}
+
 double DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
                                                                        Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr)
 {
@@ -1232,7 +1360,7 @@
     }
 
     double result_cost = 0;
-    if(encoding=="raw_masked_supervised") // complicated input format... consider it's squared error
+    if(encoding=="raw_masked_supervised" || encoding=="generic") // complicated input format... consider it's squared error
     {
         double r = 0;
         double neg_log_cost = 0; // neg log softmax
@@ -1294,22 +1422,47 @@
 }
 
 
-double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+double DenoisingRecurrentNet::fpropHiddenReconstructionFromLastHidden(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec& reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                  Vec hidden_target, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr)
 {
     // set appropriate sizes
     int fullhiddenlength = hidden_target.length();
     Vec reconstruction_activation;
+    Vec hidden_input_noise;
+    Vec hidden_fprop_noise;
+    Vec hidden_act_no_bias;
+    Vec dynamic_act_no_bias_contribution;
     if(reconstruction_bias.length()==0)
     {
         reconstruction_bias.resize(fullhiddenlength);
         reconstruction_bias.clear();
     }
+    if(reconstruction_bias2.length()==0)
+    {
+        reconstruction_bias2.resize(fullhiddenlength);
+        reconstruction_bias2.clear();
+    }
     reconstruction_activation.resize(fullhiddenlength);
     reconstruction_prob.resize(fullhiddenlength);
 
+    hidden_fprop_noise.resize(fullhiddenlength);
+    hidden_input_noise.resize(fullhiddenlength);
+    hidden_act_no_bias.resize(fullhiddenlength);
+    dynamic_act_no_bias_contribution.resize(fullhiddenlength);
+
+    //input_connections->fprop( theInput, hidden_act_no_bias);
+    hidden_input_noise << hidden_target;
+    inject_zero_forcing_noise(hidden_input_noise, input_noise_prob);
+    dynamic_connections->fprop(hidden_input_noise, dynamic_act_no_bias_contribution );
+    hidden_act_no_bias += dynamic_act_no_bias_contribution;
+    //hidden_layer->fprop( hidden_act_no_bias, hidden_fprop_noise);
+    hidden_act_no_bias += reconstruction_bias2;
+    for( int j=0 ; j<fullhiddenlength ; j++ )
+        hidden_fprop_noise[j] = fastsigmoid(hidden_act_no_bias[j] );
+
     // predict (denoised) input_reconstruction 
-    transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
+    transposeProduct(reconstruction_activation, reconstruction_weights, hidden_fprop_noise); //dynamic matrice tied
+    //transposeProduct(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice tied
     //product(reconstruction_activation, reconstruction_weights, hidden); //dynamic matrice not tied
     reconstruction_activation += reconstruction_bias;
 
@@ -1334,7 +1487,8 @@
     //externalProductScaleAcc(acc_weights_gr, hidden, hidden_reconstruction_activation_grad, -lr); //dynamic matrice tied
     //externalProductScaleAcc(acc_weights_gr, hidden_reconstruction_activation_grad, hidden, -lr); //dynamic matrice not tied
                 
-    
+    //update bias2
+    multiplyAcc(reconstruction_bias2, hidden_gradient, -lr);
     /********************************************************************************/
     // Vec hidden_reconstruction_activation_grad;
     /*hidden_reconstruction_activation_grad.clear();
@@ -1601,7 +1755,8 @@
                                               target_connections[tar]->down_size,
                                               target_connections[tar]->up_size,
                                               target_connections[tar]->learning_rate,
-                                              true);
+                                              true,
+                                              false);
                     }
                     else{
                         //target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),hidden_gradient, bias_gradient,true);
@@ -1614,7 +1769,8 @@
                                               target_connections[tar]->down_size,
                                               target_connections[tar]->up_size,
                                               target_connections[tar]->learning_rate,
-                                              true);
+                                              true,
+                                              false);
                     }
                 }
             }
@@ -1637,11 +1793,11 @@
             if(input_reconstruction_weight!=0)
             {
                 //Mat reconstruction_weights = getInputConnectionsWeightMatrix();
-                Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
+                //Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
                 
-                train_costs[4] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), inputWeights, acc_input_connections_gr, input_reconstruction_bias, input_reconstruction_prob, 
-                                                                           clean_input, hidden_gradient, input_reconstruction_weight, current_learning_rate);
-                train_n_items[4]++;
+                train_costs[train_costs.length()-2] += fpropUpdateInputReconstructionFromHidden(hidden_list(i), inputWeights, acc_input_connections_gr, input_reconstruction_bias, input_reconstruction_prob, 
+                                                                           input_list[i], hidden_gradient, input_reconstruction_weight, current_learning_rate);
+                train_n_items[train_costs.length()-2]++;
             }
             
             
@@ -1659,9 +1815,9 @@
                     
                     //truc stan
                     //fpropHiddenSymmetricDynamicMatrix(hidden_list(i-1), reconstruction_weights, hidden_reconstruction_prob, hidden_list(i), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                    train_costs[5] += fpropHiddenReconstructionFromLastHidden(hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
+                    train_costs[train_costs.length()-1] += fpropHiddenReconstructionFromLastHidden(input_list[i], hidden_list(i), dynamicWeights, acc_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_bias2, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
                     //fpropHiddenReconstructionFromLastHidden(hidden_list(i), reconsWeights, acc_reconstruction_dynamic_connections_gr, hidden_reconstruction_bias, hidden_reconstruction_activation_grad, hidden_reconstruction_prob, hidden_list(i-1), hidden_gradient, hidden_reconstruction_weight, current_learning_rate);
-                    train_n_items[5]++;
+                    train_n_items[train_costs.length()-1]++;
                 }
                 
                 
@@ -1673,69 +1829,87 @@
                     
                 }
                 
+                
+                
+                
+               
                 bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
                                        hidden_list(i),
                                        hidden_temporal_gradient, 
                                        hidden_gradient,
                                        hidden_layer->bias, 
                                        hidden_layer->learning_rate );
-                //Dynamic
-                bpropUpdateConnection(hidden_list(i-1),
-                                      hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
-                                      hidden_gradient, 
-                                      hidden_temporal_gradient, 
-                                      dynamicWeights,
-                                      acc_dynamic_connections_gr,
-                                      dynamic_connections->down_size,
-                                      dynamic_connections->up_size,
-                                      dynamic_connections->learning_rate,
-                                      false);
                 
-                /*if(hidden_reconstruction_weight!=0)
-                  {
-                  // update bias
-                  multiplyAcc(hidden_reconstruction_bias, hidden_reconstruction_activation_grad, -current_learning_rate);
-                  // update weight
-                  externalProductScaleAcc(reconstruction_weights, hidden_list(i), hidden_reconstruction_activation_grad, -current_learning_rate);
-                  
-                  }*/
                 
                 //input
-                bpropUpdateConnection(input_list[i],
-                                      hidden_act_no_bias_list(i), 
-                                      visi_bias_gradient, 
-                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                      inputWeights,
-                                      acc_input_connections_gr,
-                                      input_connections->down_size,
-                                      input_connections->up_size,
-                                      input_connections->learning_rate,
-                                      false);
+                if(hidden_reconstruction_weight==0)
+                {
+                   
+                    
+                    bpropUpdateConnection(input_list[i],
+                                          hidden_act_no_bias_list(i), 
+                                          visi_bias_gradient, 
+                                          hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                          inputWeights,
+                                          acc_input_connections_gr,
+                                          input_connections->down_size,
+                                          input_connections->up_size,
+                                          input_connections->learning_rate,
+                                          false,
+                                          true);
+                }
                 
+                //Dynamic
+                if(input_reconstruction_weight==0)
+                {
+                    /*bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                       hidden_list(i),
+                                       hidden_temporal_gradient, 
+                                       hidden_gradient,
+                                       hidden_layer->bias, 
+                                       hidden_layer->learning_rate );*/
+
+                    bpropUpdateConnection(hidden_list(i-1),
+                                          hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
+                                          hidden_gradient, 
+                                          hidden_temporal_gradient, 
+                                          dynamicWeights,
+                                          acc_dynamic_connections_gr,
+                                          dynamic_connections->down_size,
+                                          dynamic_connections->up_size,
+                                          dynamic_connections->learning_rate,
+                                          false,
+                                          false);
+                }
+                
                 hidden_temporal_gradient << hidden_gradient; 
                 //if(hidden_reconstruction_weight!=0)
                 //    hidden_temporal_gradient +=  hidden_reconstruction_activation_grad;
             }
             else
             {
-                bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
-                                       hidden_list(i),
-                                       hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
-                                       hidden_gradient,
-                                       hidden_layer->bias, 
-                                       hidden_layer->learning_rate );
-                
-                //input
-                bpropUpdateConnection(input_list[i],
-                                      hidden_act_no_bias_list(i), 
-                                      visi_bias_gradient, 
-                                      hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
-                                      inputWeights,
-                                      acc_input_connections_gr,
-                                      input_connections->down_size,
-                                      input_connections->up_size,
-                                      input_connections->learning_rate,
-                                      false);
+                if(input_reconstruction_weight==0)
+                {
+                    bpropUpdateHiddenLayer(hidden_act_no_bias_list(i), 
+                                           hidden_list(i),
+                                           hidden_temporal_gradient, // Not really temporal gradient, but this is the final iteration...
+                                           hidden_gradient,
+                                           hidden_layer->bias, 
+                                           hidden_layer->learning_rate );
+                    
+                    //input
+                    bpropUpdateConnection(input_list[i],
+                                          hidden_act_no_bias_list(i), 
+                                          visi_bias_gradient, 
+                                          hidden_temporal_gradient,// Here, it should be activations - cond_bias, but doesn't matter
+                                          inputWeights,
+                                          acc_input_connections_gr,
+                                          input_connections->down_size,
+                                          input_connections->up_size,
+                                          input_connections->learning_rate,
+                                          false,
+                                          true);
+                }
             }
         }
     }
@@ -1842,7 +2016,7 @@
     else if(encoding=="raw_masked_supervised")
         PLERROR("raw_masked_supervised means already encoded! You shouldnt have landed here!!!");
     else if(encoding=="generic")
-        PLERROR("generic encoding not yet implemented");
+        PLERROR("generic means already encoded! You shouldnt have landed here!!!");
     else
         PLERROR("unsupported encoding: %s",encoding.c_str());
 }
@@ -2005,8 +2179,8 @@
             encoded_sequence(k++,nnotes) = 1;            
     }    
 }
-    
 
+
 // input noise injection
 void DenoisingRecurrentNet::inject_zero_forcing_noise(Mat sequence, double noise_prob) const
 {
@@ -2022,6 +2196,19 @@
     }
 }
 
+// input noise injection
+void DenoisingRecurrentNet::inject_zero_forcing_noise(Vec sequence, double noise_prob) const
+{
+    
+    real* p = sequence.data();
+    int n = sequence.size();
+    while(n--)
+    {
+        if(*p!=real(0.) && random_gen->uniform_sample()<noise_prob)
+            *p = real(0.);
+        ++p;
+    }
+}
 
 void DenoisingRecurrentNet::clamp_units(const Vec layer_vector,
                                              PP<RBMLayer> layer,
@@ -2165,7 +2352,7 @@
         int seqlen = end-start; // target_prediction_list[0].length();
         seq.resize(seqlen, w);
         testset->getMat(start,0,seq);
-        encodeSequenceAndPopulateLists(seq);
+        encodeSequenceAndPopulateLists(seq, false);
 
         if(input_window_size==0)
             unconditionalFprop(costs, n_items);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-04-09 17:26:43 UTC (rev 10107)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2009-04-10 02:03:00 UTC (rev 10108)
@@ -70,6 +70,7 @@
 
     ////! Number of epochs for rbm phase
     //int rbm_nstages;
+    //int nCost;
 
     //! The training weights of each target layers
     Vec target_layers_weights;
@@ -126,6 +127,12 @@
     string encoding;
 
     bool noise;
+
+    //! Optional (default=0) factor of L1 regularization term
+    real L1_penalty_factor;
+
+    //! Optional (default=0) factor of L2 regularization term
+    real L2_penalty_factor;
     
     //! Input window size
     int input_window_size;
@@ -154,6 +161,8 @@
 
     // learnt bias for hidden reconstruction
     Vec hidden_reconstruction_bias;
+
+    Vec hidden_reconstruction_bias2;
     
     double prediction_cost_weight;
     double input_reconstruction_cost_weight;
@@ -181,8 +190,9 @@
                                                   bool use_silence, int octav_nbits, int duration_nbits=20);
     
     static void encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, 
-                                        int prepend_zero_rows, bool use_silence=false);    
+                                        int prepend_zero_rows, bool use_silence=false); 
 
+
     static int duration_to_number_of_timeframes(int duration);
     static int getDurationBit(int duration);
 
@@ -190,6 +200,9 @@
     // input noise injection
     void inject_zero_forcing_noise(Mat sequence, double noise_prob) const;
 
+    // noise injection
+    void inject_zero_forcing_noise(Vec sequence, double noise_prob) const;
+
     inline static Vec getInputWindow(Mat sequence, int startpos, int winsize)
     { return sequence.subMatRows(startpos, winsize).toVec(); }
           
@@ -414,17 +427,19 @@
     // note: the following functions are declared const because they have
     // to be called by test (which is const). Similarly, the members they 
     // manipulate are all declared mutable.
-    void recurrentFprop(Vec train_costs, Vec train_n_items) const;
+    void recurrentFprop(Vec train_costs, Vec train_n_items, bool useDynamicConnections=true) const;
 
     //! does encoding if needed and populates the list.
-    void encodeSequenceAndPopulateLists(Mat seq) const;
+    void encodeSequenceAndPopulateLists(Mat seq, bool doNoise) const;
 
     //! encodes seq, then populates: inputslist, targets_list, masks_list
     void encodeAndCreateSupervisedSequence(Mat seq) const;
 
     //! For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
-    void splitRawMaskedSupervisedSequence(Mat seq) const;
+    void splitRawMaskedSupervisedSequence(Mat seq, bool doNoise) const;
 
+    void encode_artificialData(Mat seq) const; 
+
     void resize_lists(int l) const;
 
     void trainUnconditionalPredictor();
@@ -451,7 +466,8 @@
                                int& down_size,
                                int& up_size,
                                real& lr,
-                               bool accumulate);
+                               bool accumulate,
+                               bool using_penalty_factor);
 
     void bpropUpdateHiddenLayer(const Vec& input, 
                                 const Vec& output,
@@ -460,6 +476,9 @@
                                 Vec& bias,
                                 real& lr);
 
+
+    void applyWeightPenalty(Mat& weights, Mat& acc_weights_gr, int& down_size, int& up_size, real& lr);
+
     //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
     //! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
     //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
@@ -479,7 +498,7 @@
     void updateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Mat& acc_weights_gr, Vec& input_reconstruction_bias, Vec input_reconstruction_prob, 
                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_cost_weight, double lr);
 
-    double fpropHiddenReconstructionFromLastHidden(Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
+    double fpropHiddenReconstructionFromLastHidden(Vec theInput, Vec hidden, Mat reconstruction_weights, Mat& acc_weights_gr, Vec& reconstruction_bias, Vec& reconstruction_bias2, Vec hidden_reconstruction_activation_grad, Vec& reconstruction_prob, 
                                                                           Vec clean_input, Vec hidden_gradient, double hidden_reconstruction_cost_weight, double lr);
     
     double fpropHiddenSymmetricDynamicMatrix(Vec hidden, Mat reconstruction_weights, Vec& reconstruction_prob, 



From chapados at mail.berlios.de  Fri Apr 10 15:56:59 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 10 Apr 2009 15:56:59 +0200
Subject: [Plearn-commits] r10109 - in
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor: .
	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200904101356.n3ADuxED010839@sheep.berlios.de>

Author: chapados
Date: 2009-04-10 15:56:59 +0200 (Fri, 10 Apr 2009)
New Revision: 10109

Added:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn
Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
Log:
Now test the sparse approximation to GaussianProcessRegressor

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-04-10 02:03:00 UTC (rev 10108)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-04-10 13:56:59 UTC (rev 10109)
@@ -5,10 +5,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 14.8530952689689837 ;
-isp_signal_sigma = 29.6219285851744907 ;
+isp_alpha = 4137.9189880121985 ;
+isp_signal_sigma = 25.2522546527682366 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 22.2544311480311059 ] ;
+isp_input_sigma = 1 [ 20.5437932346225374 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -19,7 +19,7 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.86446658043304136 ;
+isp_noise_sigma = -1.92253519592071265 ;
 isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
@@ -66,19 +66,19 @@
 active_set_indices = []
 ;
 alpha = 5  1  [ 
--1.11770047923155547 	
--1.44398600678466971 	
-2.34477475107078348 	
-0.359163702825774089 	
--0.273169390819025082 	
+-1.18978376207412251 	
+-1.51050223666584205 	
+2.48105677760051924 	
+0.377930906671194056 	
+-0.305796404967886237 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.17384303478444618 	-0.0428570226814908445 	-2.24278913014357606 	0.211187944940308625 	-0.0160008498417097828 	
--0.0428570226814906294 	2.63229682411057064 	-2.36514023143184327 	-0.310986674734060831 	0.0223156036847275939 	
--2.24278913014357606 	-2.36514023143184327 	4.57667695944806141 	0.0265284039045036689 	-8.51433617209678838e-05 	
-0.211187944940308597 	-0.310986674734060831 	0.0265284039045036689 	0.113894354038077864 	-0.0105928862092774176 	
--0.0160008498417097828 	0.0223156036847275904 	-8.51433617209677347e-05 	-0.0105928862092774159 	0.0346336314422349412 	
+2.31013968379537182 	-0.0536442038211801253 	-2.36779854974030668 	0.22569490209109197 	-0.0147406401474149042 	
+-0.0536442038211802363 	2.78367241543547994 	-2.49816636895636401 	-0.330531908021966248 	0.0204783918621609592 	
+-2.36779854974030668 	-2.49816636895636446 	4.83232378742233681 	0.0288243957181120274 	-0.000396768649140989422 	
+0.22569490209109197 	-0.330531908021966248 	0.0288243957181120274 	0.123220344867242149 	-0.00907667066269107932 	
+-0.0147406401474149077 	0.0204783918621609627 	-0.000396768649140989639 	-0.00907667066269108105 	0.0400706678382072579 	
 ]
 ;
 subgram_inverse = 0  0  [ 
@@ -96,13 +96,13 @@
 expdir = "" ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 10 ;
+stage = 100 ;
 n_examples = 5 ;
 inputsize = 1 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 10 ;
+nstages = 100 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;
@@ -111,18 +111,149 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 0  )
 
-!R 1 1 [ 14.9482509617831152 ] 
-!R 1 1 [ 14.444695835622035 ] 
+!R 1 1 [ 14.948418736085145 ] 
+!R 1 1 [ 14.4380583444163992 ] 
 !R 2 4  1  [ 
-13.4992090127023872 	
-14.4141233331845555 	
-14.9482509617831152 	
-14.444695835622035 	
+13.4975472944014498 	
+14.4134624243872214 	
+14.948418736085145 	
+14.4380583444163992 	
 ]
 1 [ 4  4  [ 
-0.510839430955278506 	0.283483493901130146 	0.0642765859007319307 	-0.555619592268506324 	
-0.283483493901130146 	0.39189685516754752 	0.1038829738530751 	-0.369897017889378787 	
-0.064276585900728378 	0.1038829738530751 	0.285799680593760874 	0.185897571277269691 	
--0.555619592268506324 	-0.369897017889378787 	0.185897571277269691 	2.23845483387388011 	
+0.480063689375973601 	0.265505446046397253 	0.0610095684567859564 	-0.525180535128360049 	
+0.265505446046397253 	0.368355224390657432 	0.0984353299046141217 	-0.345547507812195676 	
+0.0610095684567859564 	0.0984353299046141217 	0.27067133381043279 	0.175932735038120569 	
+-0.525180535128363601 	-0.345547507812199228 	0.175932735038117016 	2.09240713218070962 	
 ]
 ] 
+!R 0 
+!R 0 
+!R 0 
+!R 1 GaussianProcessRegressor(
+kernel = *1 ->SummationKernel(
+terms = 2 [ *2 ->RationalQuadraticARDKernel(
+isp_alpha = 4137.9189880121985 ;
+isp_signal_sigma = 25.2522546527682366 ;
+isp_global_sigma = 0 ;
+isp_input_sigma = 1 [ 20.5437932346225374 ] ;
+kronecker_indexes = []
+;
+cache_threshold = 1000000 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 1 ;
+data_inputsize = 1 ;
+n_examples = 5  )
+*3 ->IIDNoiseKernel(
+isp_noise_sigma = -1.92253519592071265 ;
+isp_kronecker_sigma = -100 ;
+kronecker_indexes = []
+;
+cache_threshold = 1000000 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 1 ;
+data_inputsize = 1 ;
+n_examples = 5  )
+] ;
+input_indexes = []
+;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 0 ;
+data_inputsize = 1 ;
+n_examples = 5  )
+;
+weight_decay = 0 ;
+include_bias = 1 ;
+compute_confidence = 1 ;
+confidence_epsilon = 1.00000000000000002e-08 ;
+hyperparameters = 3 [ ("terms[0].isp_signal_sigma" , "0.0" )("terms[0].isp_alpha" , "0.0" )("terms[1].isp_noise_sigma" , "0.0" )] ;
+ARD_hyperprefix_initval = ("terms[0].isp_input_sigma" , "0.0" );
+optimizer = *4 ->ConjGradientOptimizer(
+verbosity = 1 ;
+expected_red = 1 ;
+no_negative_gamma = 1 ;
+sigma = 0.100000000000000006 ;
+rho = 0.0500000000000000028 ;
+constrain_limit = 0.100000000000000006 ;
+max_extrapolate = 3 ;
+max_eval_per_line_search = 20 ;
+slope_ratio = 10 ;
+minibatch_n_samples = 0 ;
+minibatch_n_line_searches = 3 ;
+nstages = 1 ;
+early_stop = 0  )
+;
+save_gram_matrix = 0 ;
+solution_algorithm = "projected-process" ;
+active_set_indices = 5 [ 0 1 2 3 4 ] ;
+alpha = 5  1  [ 
+-1.18978376907293937 	
+-1.51050224487545104 	
+2.48105679258359446 	
+0.377930906889508422 	
+-0.305796404977936809 	
+]
+;
+gram_inverse = 5  5  [ 
+396.604919126663106 	463.341455779605496 	-847.31996352349006 	-12.1558868422838877 	0.556064853803424519 	
+463.34145577960544 	544.763973219138165 	-993.089361861597467 	-14.5987162672844573 	0.6745157046565875 	
+-847.319963523489832 	-993.089361861597467 	1813.18952986328532 	26.336693514953943 	-1.21091730158664102 	
+-12.1558868422838859 	-14.598716267284459 	26.3366935149539465 	0.41887278420359092 	-0.0199830935494544268 	
+0.556064853803424408 	0.6745157046565875 	-1.21091730158664102 	-0.0199830935494544198 	0.00118061341927770506 	
+]
+;
+subgram_inverse = 5  5  [ 
+398.915056412611648 	463.287808763247369 	-849.687756940042959 	-11.9301918654121035 	0.54132421021357302 	
+463.287808763247483 	547.547642335624914 	-995.587522209615713 	-14.9292480875924749 	0.694994092480937353 	
+-849.687756940042959 	-995.587522209615713 	1818.02184266184099 	26.3655177505849494 	-1.21131406286636301 	
+-11.9301918654121053 	-14.9292480875924749 	26.3655177505849529 	0.542093126738621423 	-0.029059764104784086 	
+0.541324210213573132 	0.694994092480937353 	-1.21131406286636301 	-0.029059764104784086 	0.0412512812525426542 	
+]
+;
+target_mean = 1 [ 10 ] ;
+training_inputs = 5  1  [ 
+5 	
+6 	
+5.5 	
+10 	
+20 	
+]
+;
+expdir = "" ;
+random_gen = *0 ;
+seed = 1827 ;
+stage = 100 ;
+n_examples = 5 ;
+inputsize = 1 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 100 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = "" ;
+test_minibatch_size = 1 ;
+use_a_separate_random_generator_for_testing = 1827 ;
+finalized = 0  )
+
+!R 1 1 [ 14.9484187360853262 ] 
+!R 1 1 [ 14.4380583447243644 ] 
+!R 2 4  1  [ 
+13.4975472943123123 	
+14.413462424307447 	
+14.9484187360853262 	
+14.4380583447243644 	
+]
+1 [ 4  4  [ 
+0.480063689691387074 	0.265505446333293094 	0.0610095684079006162 	-0.525180536496257844 	
+0.265505446332181094 	0.368355224650399882 	0.09843532986322856 	-0.345547509035363021 	
+0.0610095684061207066 	0.0984353298624789375 	0.270671333777680323 	0.175932735023305753 	
+-0.52518053649836105 	-0.345547509036720157 	0.175932735022545472 	2.09240713680968238 	
+]
+] 

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR	2009-04-10 02:03:00 UTC (rev 10108)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR	2009-04-10 13:56:59 UTC (rev 10109)
@@ -1,3 +1,4 @@
+#####  Basic Gaussian Process Regression: exact solution  ###################
 !L 1 learner_hyperopt.plearn
 !M 1 setTrainingSet 2 MemoryVMatrix(data = 5 2 [5 10 6 11 5.5 11 10 15 20 3], inputsize=1, targetsize=1, weightsize=0) 0
 !M 1 train 0
@@ -5,4 +6,13 @@
 !M 1 computeOutput 1 [ 10 ]
 !M 1 computeOutput 1 [ 12 ]
 !M 1 computeOutputCovMat 1 4 1 [8 9 10 12]
+
+#####  Sparse Approximations  ###############################################
+!L 2 learner_hyperopt_sparse.plearn
+!M 2 setTrainingSet 2 MemoryVMatrix(data = 5 2 [5 10 6 11 5.5 11 10 15 20 3], inputsize=1, targetsize=1, weightsize=0) 0
+!M 2 train 0
+!M 2 getObject 0
+!M 2 computeOutput 1 [ 10 ]
+!M 2 computeOutput 1 [ 12 ]
+!M 2 computeOutputCovMat 1 4 1 [8 9 10 12]
 !Q

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2009-04-10 02:03:00 UTC (rev 10108)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2009-04-10 13:56:59 UTC (rev 10109)
@@ -13,5 +13,5 @@
                                                     rho         = 0.05,
                                                     slope_ratio = 10,
                                                     verbosity   = 1),
-    nstages                 = 10
+    nstages                 = 100
     )

Added: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn	2009-04-10 02:03:00 UTC (rev 10108)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn	2009-04-10 13:56:59 UTC (rev 10109)
@@ -0,0 +1,19 @@
+GaussianProcessRegressor(
+    kernel                  = SummationKernel(terms = [ RationalQuadraticARDKernel(),
+                                                        IIDNoiseKernel() ]),
+    weight_decay            = 0,
+    include_bias            = 1,
+    compute_confidence      = 1,
+    ARD_hyperprefix_initval = ("terms[0].isp_input_sigma",    0.0) ,
+    hyperparameters         = [ ("terms[0].isp_signal_sigma", 0.0) ,
+                                ("terms[0].isp_alpha",        0.0) ,
+                                ("terms[1].isp_noise_sigma",  0.0) ],
+    optimizer               = ConjGradientOptimizer(nstages     = 1,
+                                                    sigma       = 0.1,
+                                                    rho         = 0.05,
+                                                    slope_ratio = 10,
+                                                    verbosity   = 1),
+    nstages                 = 100,
+    solution_algorithm      = "projected-process",
+    active_set_indices      = [ 0 1 2 3 4 ]
+    )

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2009-04-10 02:03:00 UTC (rev 10108)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2009-04-10 13:56:59 UTC (rev 10109)
@@ -103,7 +103,8 @@
     arguments = "server < INPUTS_GPR",
     resources = [
         "INPUTS_GPR",
-        "learner_hyperopt.plearn"
+        "learner_hyperopt.plearn",
+        "learner_hyperopt_sparse.plearn"
         ],
     precision = 1e-06,
     pfileprg = "__program__",



From chapados at mail.berlios.de  Sat Apr 11 18:02:25 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 11 Apr 2009 18:02:25 +0200
Subject: [Plearn-commits] r10110 - in
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor: .
	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200904111602.n3BG2PXd011672@sheep.berlios.de>

Author: chapados
Date: 2009-04-11 18:02:24 +0200 (Sat, 11 Apr 2009)
New Revision: 10110

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn
Log:
Try to fix GaussianProcessRegressor test failure -- may be related to subtle differences between Intel and AMD chips

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-04-10 13:56:59 UTC (rev 10109)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2009-04-11 16:02:24 UTC (rev 10110)
@@ -5,10 +5,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 4137.9189880121985 ;
-isp_signal_sigma = 25.2522546527682366 ;
+isp_alpha = 0.41984749161951862 ;
+isp_signal_sigma = 4.41014650900261618 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 20.5437932346225374 ] ;
+isp_input_sigma = 1 [ -0.0767962823590773863 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -19,7 +19,7 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.92253519592071265 ;
+isp_noise_sigma = 4.4261143527445288 ;
 isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
@@ -66,19 +66,19 @@
 active_set_indices = []
 ;
 alpha = 5  1  [ 
--1.18978376207412251 	
--1.51050223666584205 	
-2.48105677760051924 	
-0.377930906671194056 	
--0.305796404967886237 	
+-0.0749902168570145272 	
+0.0727389070777302998 	
+0.0973384096207741134 	
+0.566601377408640694 	
+-0.795210046404519977 	
 ]
 ;
 gram_inverse = 5  5  [ 
-2.31013968379537182 	-0.0536442038211801253 	-2.36779854974030668 	0.22569490209109197 	-0.0147406401474149042 	
--0.0536442038211802363 	2.78367241543547994 	-2.49816636895636401 	-0.330531908021966248 	0.0204783918621609592 	
--2.36779854974030668 	-2.49816636895636446 	4.83232378742233681 	0.0288243957181120274 	-0.000396768649140989422 	
-0.22569490209109197 	-0.330531908021966248 	0.0288243957181120274 	0.123220344867242149 	-0.00907667066269107932 	
--0.0147406401474149077 	0.0204783918621609627 	-0.000396768649140989639 	-0.00907667066269108105 	0.0400706678382072579 	
+0.139498134040858895 	-0.0185812496595273054 	-0.0506909486307591528 	-0.00148370690353891643 	-0.000242930850138073416 	
+-0.0185812496595273123 	0.139598477491035239 	-0.0506561235142563709 	-0.00367467439584157497 	-0.00030998929716561616 	
+-0.0506909486307591528 	-0.0506561235142563779 	0.1554586053803238 	-0.00178948797787016373 	-0.000211909663436789616 	
+-0.00148370690353891643 	-0.0036746743958415754 	-0.00178948797787016394 	0.113134907432459256 	-0.000913000374293741692 	
+-0.000242930850138073416 	-0.00030998929716561616 	-0.000211909663436789616 	-0.000913000374293741692 	0.112874735081778407 	
 ]
 ;
 subgram_inverse = 0  0  [ 
@@ -96,13 +96,13 @@
 expdir = "" ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 100 ;
+stage = 1 ;
 n_examples = 5 ;
 inputsize = 1 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 100 ;
+nstages = 1 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;
@@ -111,19 +111,19 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 0  )
 
-!R 1 1 [ 14.948418736085145 ] 
-!R 1 1 [ 14.4380583444163992 ] 
+!R 1 1 [ 12.4854206721884182 ] 
+!R 1 1 [ 10.5807841904971429 ] 
 !R 2 4  1  [ 
-13.4975472944014498 	
-14.4134624243872214 	
-14.948418736085145 	
-14.4380583444163992 	
+10.7256846235799053 	
+11.4500202213341549 	
+12.4854206721884182 	
+10.5807841904971429 	
 ]
 1 [ 4  4  [ 
-0.480063689375973601 	0.265505446046397253 	0.0610095684567859564 	-0.525180535128360049 	
-0.265505446046397253 	0.368355224390657432 	0.0984353299046141217 	-0.345547507812195676 	
-0.0610095684567859564 	0.0984353299046141217 	0.27067133381043279 	0.175932735038120569 	
--0.525180535128363601 	-0.345547507812199228 	0.175932735038117016 	2.09240713218070962 	
+8.55743635585828066 	2.13398416871230845 	0.546730129377252516 	0.210257960655207327 	
+2.13398416871230845 	8.09984178215855621 	1.25644119210545746 	0.277244774945088146 	
+0.546730129377252516 	1.25644119210545724 	6.64771676644526366 	0.570559458080892479 	
+0.210257960655207327 	0.277244774945088091 	0.570559458080892479 	8.70744953606769378 	
 ]
 ] 
 !R 0 
@@ -132,10 +132,10 @@
 !R 1 GaussianProcessRegressor(
 kernel = *1 ->SummationKernel(
 terms = 2 [ *2 ->RationalQuadraticARDKernel(
-isp_alpha = 4137.9189880121985 ;
-isp_signal_sigma = 25.2522546527682366 ;
+isp_alpha = 0.41984749161951862 ;
+isp_signal_sigma = 4.41014650900261618 ;
 isp_global_sigma = 0 ;
-isp_input_sigma = 1 [ 20.5437932346225374 ] ;
+isp_input_sigma = 1 [ -0.0767962823590773863 ] ;
 kronecker_indexes = []
 ;
 cache_threshold = 1000000 ;
@@ -146,7 +146,7 @@
 data_inputsize = 1 ;
 n_examples = 5  )
 *3 ->IIDNoiseKernel(
-isp_noise_sigma = -1.92253519592071265 ;
+isp_noise_sigma = 4.4261143527445288 ;
 isp_kronecker_sigma = -100 ;
 kronecker_indexes = []
 ;
@@ -192,27 +192,27 @@
 solution_algorithm = "projected-process" ;
 active_set_indices = 5 [ 0 1 2 3 4 ] ;
 alpha = 5  1  [ 
--1.18978376907293937 	
--1.51050224487545104 	
-2.48105679258359446 	
-0.377930906889508422 	
--0.305796404977936809 	
+-0.0749902168570144578 	
+0.0727389070777302721 	
+0.0973384096207740579 	
+0.566601377408640805 	
+-0.795210046404519866 	
 ]
 ;
 gram_inverse = 5  5  [ 
-396.604919126663106 	463.341455779605496 	-847.31996352349006 	-12.1558868422838877 	0.556064853803424519 	
-463.34145577960544 	544.763973219138165 	-993.089361861597467 	-14.5987162672844573 	0.6745157046565875 	
--847.319963523489832 	-993.089361861597467 	1813.18952986328532 	26.336693514953943 	-1.21091730158664102 	
--12.1558868422838859 	-14.598716267284459 	26.3366935149539465 	0.41887278420359092 	-0.0199830935494544268 	
-0.556064853803424408 	0.6745157046565875 	-1.21091730158664102 	-0.0199830935494544198 	0.00118061341927770506 	
+0.835401008104275089 	0.464220454909828939 	-1.14290699683802321 	-0.0099349932796523148 	-0.00123641448702753779 	
+0.464220454909828772 	0.837752895668678055 	-1.14396362373329352 	-0.0225414924388310269 	-0.00150403229671235454 	
+-1.14290699683802299 	-1.14396362373329352 	2.07761325988048373 	0.0174813840793698119 	0.00132346191527563713 	
+-0.00993499327965231133 	-0.0225414924388310269 	0.0174813840793698119 	0.114874942912193342 	-0.00270028943686582371 	
+-0.00123641448702753779 	-0.00150403229671235454 	0.00132346191527563735 	-0.00270028943686582371 	0.113334727004050437 	
 ]
 ;
 subgram_inverse = 5  5  [ 
-398.915056412611648 	463.287808763247369 	-849.687756940042959 	-11.9301918654121035 	0.54132421021357302 	
-463.287808763247483 	547.547642335624914 	-995.587522209615713 	-14.9292480875924749 	0.694994092480937353 	
--849.687756940042959 	-995.587522209615713 	1818.02184266184099 	26.3655177505849494 	-1.21131406286636301 	
--11.9301918654121053 	-14.9292480875924749 	26.3655177505849529 	0.542093126738621423 	-0.029059764104784086 	
-0.541324210213573132 	0.694994092480937353 	-1.21131406286636301 	-0.029059764104784086 	0.0412512812525426542 	
+0.974899142145137176 	0.445639205250304649 	-1.19359794546878795 	-0.0114187001831912976 	-0.00147934533716562004 	
+0.44563920525030476 	0.977351373159716097 	-1.19461974724755526 	-0.0262161668346726703 	-0.00181402159387797856 	
+-1.19359794546878795 	-1.19461974724755504 	2.23307186526081747 	0.0156918961014997707 	0.00111155225183886207 	
+-0.0114187001831912993 	-0.0262161668346726703 	0.0156918961014997707 	0.228009850344652598 	-0.0036132898111595653 	
+-0.00147934533716562004 	-0.00181402159387797834 	0.00111155225183886229 	-0.0036132898111595653 	0.226209462085828844 	
 ]
 ;
 target_mean = 1 [ 10 ] ;
@@ -227,13 +227,13 @@
 expdir = "" ;
 random_gen = *0 ;
 seed = 1827 ;
-stage = 100 ;
+stage = 1 ;
 n_examples = 5 ;
 inputsize = 1 ;
 targetsize = 1 ;
 weightsize = 0 ;
 forget_when_training_set_changes = 0 ;
-nstages = 100 ;
+nstages = 1 ;
 report_progress = 1 ;
 verbosity = 1 ;
 nservers = 0 ;
@@ -242,18 +242,18 @@
 use_a_separate_random_generator_for_testing = 1827 ;
 finalized = 0  )
 
-!R 1 1 [ 14.9484187360853262 ] 
-!R 1 1 [ 14.4380583447243644 ] 
+!R 1 1 [ 12.4854206721884182 ] 
+!R 1 1 [ 10.5807841904971429 ] 
 !R 2 4  1  [ 
-13.4975472943123123 	
-14.413462424307447 	
-14.9484187360853262 	
-14.4380583447243644 	
+10.7256846235799053 	
+11.4500202213341549 	
+12.4854206721884182 	
+10.5807841904971429 	
 ]
 1 [ 4  4  [ 
-0.480063689691387074 	0.265505446333293094 	0.0610095684079006162 	-0.525180536496257844 	
-0.265505446332181094 	0.368355224650399882 	0.09843532986322856 	-0.345547509035363021 	
-0.0610095684061207066 	0.0984353298624789375 	0.270671333777680323 	0.175932735023305753 	
--0.52518053649836105 	-0.345547509036720157 	0.175932735022545472 	2.09240713680968238 	
+8.55743635585828066 	2.133984168712308 	0.546730129377252294 	0.210257960655207271 	
+2.133984168712308 	8.09984178215855621 	1.25644119210545702 	0.277244774945088091 	
+0.546730129377252405 	1.25644119210545746 	6.64771676644526277 	0.570559458080892479 	
+0.210257960655207271 	0.277244774945088146 	0.570559458080892368 	8.70744953606769378 	
 ]
 ] 

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2009-04-10 13:56:59 UTC (rev 10109)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2009-04-11 16:02:24 UTC (rev 10110)
@@ -13,5 +13,6 @@
                                                     rho         = 0.05,
                                                     slope_ratio = 10,
                                                     verbosity   = 1),
-    nstages                 = 100
+    ## NB: too many stages result in differing optima for Intel versus AMD...
+    nstages                 = 1
     )

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn	2009-04-10 13:56:59 UTC (rev 10109)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt_sparse.plearn	2009-04-11 16:02:24 UTC (rev 10110)
@@ -13,7 +13,8 @@
                                                     rho         = 0.05,
                                                     slope_ratio = 10,
                                                     verbosity   = 1),
-    nstages                 = 100,
+    ## NB: too many stages result in differing optima for Intel versus AMD...
+    nstages                 = 1,
     solution_algorithm      = "projected-process",
     active_set_indices      = [ 0 1 2 3 4 ]
     )



From chapados at mail.berlios.de  Mon Apr 13 03:40:01 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 13 Apr 2009 03:40:01 +0200
Subject: [Plearn-commits] r10111 - trunk/plearn/ker
Message-ID: <200904130140.n3D1e1IZ026813@sheep.berlios.de>

Author: chapados
Date: 2009-04-13 03:39:59 +0200 (Mon, 13 Apr 2009)
New Revision: 10111

Added:
   trunk/plearn/ker/PLearnerDiagonalKernel.cc
   trunk/plearn/ker/PLearnerDiagonalKernel.h
Log:
Added a diagonal kernel that can take its output from a PLearner; useful for heteroscedastic gaussian processes

Added: trunk/plearn/ker/PLearnerDiagonalKernel.cc
===================================================================
--- trunk/plearn/ker/PLearnerDiagonalKernel.cc	2009-04-11 16:02:24 UTC (rev 10110)
+++ trunk/plearn/ker/PLearnerDiagonalKernel.cc	2009-04-13 01:39:59 UTC (rev 10111)
@@ -0,0 +1,229 @@
+// -*- C++ -*-
+
+// PLearnerDiagonalKernel.cc
+//
+// Copyright (C) 2009 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file PLearnerDiagonalKernel.cc */
+
+
+#include "PLearnerDiagonalKernel.h"
+#include <plearn/math/pl_math.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PLearnerDiagonalKernel,
+    "Diagonal kernel from the output of a PLearner.",
+    "The output of this kernel is given by:\n"
+    "\n"
+    "  k(x,x) = isp_signal_sigma * exp(learner->computeOutput(x))\n"
+    "\n"
+    "and is 0 for x != y.\n"
+    "\n"
+    "This is useful for representing heteroscedastic noise in Gaussian\n"
+    "Processes, where the log-noise process is the output of another learner\n"
+    "(e.g. another Gaussian Process).\n"
+    );
+
+
+PLearnerDiagonalKernel::PLearnerDiagonalKernel()
+    : m_isp_signal_sigma(0.)
+{ }
+
+
+//#####  declareOptions  ######################################################
+
+void PLearnerDiagonalKernel::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "learner",
+        &PLearnerDiagonalKernel::m_learner,
+        OptionBase::buildoption,
+        "Learner we are taking output from.");
+    
+    declareOption(
+        ol, "isp_signal_sigma",
+        &PLearnerDiagonalKernel::m_isp_signal_sigma,
+        OptionBase::buildoption,
+        "Inverse softplus of the global noise variance.  Default value = 0.0.");
+        
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void PLearnerDiagonalKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void PLearnerDiagonalKernel::build_()
+{
+    if (m_learner.isNull())
+        PLERROR("PLearnerDiagonalKernel::build: the option 'learner' must be specified");
+
+    if (m_learner->outputsize() != 1)
+        PLERROR("PLearnerDiagonalKernel::build: the learner must have an outputsize of 1; "
+                "current outputsize is %d", m_learner->outputsize());
+    
+    m_output_buffer.resize(m_learner->outputsize());
+    
+    // Ensure that we multiply in Kronecker terms
+    inherited::m_default_value = 1.0;
+}
+
+
+//#####  evaluate  ############################################################
+
+real PLearnerDiagonalKernel::evaluate(const Vec& x1, const Vec& x2) const
+{
+    PLASSERT( x1.size() == x2.size() );
+    PLASSERT( ! m_learner.isNull() );
+
+    if (x1 == x2) {
+        real gating_term = inherited::evaluate(x1,x2);
+        real sigma = softplus(m_isp_signal_sigma);
+        m_learner->computeOutput(x1, m_output_buffer);
+        real diag_term = exp(m_output_buffer[0]);
+        return sigma * gating_term * diag_term;
+    }
+    else
+        return 0.0;
+}
+
+
+//#####  computeGramMatrix  ###################################################
+
+void PLearnerDiagonalKernel::computeGramMatrix(Mat K) const
+{
+    PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+
+    // Most elements are zero, except for the diagonal
+    K.fill(0.0);
+
+    real sigma = softplus(m_isp_signal_sigma);
+    int  n = m_data_cache.length();
+    
+    PLASSERT( K.length() == n && K.width() == n );
+    
+    for (int i=0 ; i<n ; ++i) {
+        real gating_term = inherited::evaluate_i_j(i, i);
+        Vec input_i = m_data_cache(i);
+        m_learner->computeOutput(input_i, m_output_buffer);
+        real diag_term = exp(m_output_buffer[0]);
+        K(i,i) = sigma * gating_term * diag_term;
+    }
+
+    if (cache_gram_matrix) {
+        gram_matrix.resize(n,n);
+        gram_matrix << K;
+        gram_matrix_is_cached = true;
+    }
+}
+
+
+//#####  computeGramMatrixDerivative  #########################################
+
+void PLearnerDiagonalKernel::computeGramMatrixDerivative(
+    Mat& KD, const string& kernel_param, real epsilon) const
+{
+    static const string ISS("isp_signal_sigma");
+
+    if (kernel_param == ISS) {
+        computeGramMatrixDerivIspSignalSigma(KD);
+        
+        // computeGramMatrixDerivNV<
+        //     PLearnerDiagonalKernel,
+        //     &PLearnerDiagonalKernel::derivIspSignalSigma>(KD, this, -1);
+    }
+    else
+        inherited::computeGramMatrixDerivative(KD, kernel_param, epsilon);
+}
+
+
+//#####  evaluate_all_i_x  ####################################################
+
+void PLearnerDiagonalKernel::evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                              real squared_norm_of_x, int istart) const
+{
+    evaluateAllIXNV<PLearnerDiagonalKernel>(x, k_xi_x, istart);
+}
+
+
+//#####  computeGramMatrixDerivIspSignalSigma  ################################
+
+void PLearnerDiagonalKernel::computeGramMatrixDerivIspSignalSigma(Mat& KD) const
+{
+    int l = data->length();
+    KD.resize(l,l);
+    PLASSERT_MSG(
+        gram_matrix.width() == l && gram_matrix.length() == l,
+        "To compute the derivative with respect to 'isp_signal_sigma', the\n"
+        "Gram matrix must be precomputed and cached in PLearnerDiagonalKernel.");
+    
+    KD << gram_matrix;
+    KD *= sigmoid(m_isp_signal_sigma)/softplus(m_isp_signal_sigma);
+}
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void PLearnerDiagonalKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    deepCopyField(m_learner, copies);
+    deepCopyField(m_output_buffer, copies);
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/PLearnerDiagonalKernel.h
===================================================================
--- trunk/plearn/ker/PLearnerDiagonalKernel.h	2009-04-11 16:02:24 UTC (rev 10110)
+++ trunk/plearn/ker/PLearnerDiagonalKernel.h	2009-04-13 01:39:59 UTC (rev 10111)
@@ -0,0 +1,144 @@
+// -*- C++ -*-
+
+// PLearnerDiagonalKernel.h
+//
+// Copyright (C) 2009 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file PLearnerDiagonalKernel.h */
+
+
+#ifndef MATERN1ARDKERNEL_INC
+#define MATERN1ARDKERNEL_INC
+
+#include <plearn/ker/KroneckerBaseKernel.h>
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ *  Diagonal kernel from the output of a PLearner.
+ *
+ *  The output of this kernel is given by:
+ *
+ *    k(x,x) = isp_signal_sigma * exp(learner->computeOutput(x))
+ *
+ *  and is 0 for x != y.
+ *
+ *  This is useful for representing heteroscedastic noise in Gaussian
+ *  Processes, where the log-noise process is the output of another learner
+ *  (e.g. another Gaussian Process).
+ */
+class PLearnerDiagonalKernel : public KroneckerBaseKernel
+{
+    typedef KroneckerBaseKernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    /// Learner we are taking output from.
+    PP<PLearner> m_learner;
+    
+    /// Inverse softplus of the global noise variance.  Default value = 0.0.
+    mutable real m_isp_signal_sigma;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    /// Default constructor
+    PLearnerDiagonalKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    /// Compute K(x1,x2).
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+
+    /// Compute the Gram Matrix.
+    virtual void computeGramMatrix(Mat K) const;
+    
+    /// Directly compute the derivative with respect to hyperparameters
+    /// (Faster than finite differences...)
+    virtual void computeGramMatrixDerivative(Mat& KD, const string& kernel_param,
+                                             real epsilon=1e-6) const;
+    
+    /// Fill k_xi_x with K(x_i, x), for all i from istart to istart + k_xi_x.length() - 1.
+    virtual void evaluate_all_i_x(const Vec& x, const Vec& k_xi_x,
+                                  real squared_norm_of_x=-1, int istart = 0) const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(PLearnerDiagonalKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    /// Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    /// Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+    /// Compute derivative w.r.t. isp_signal_sigma for WHOLE MATRIX
+    void computeGramMatrixDerivIspSignalSigma(Mat& KD) const;
+
+private:
+    /// This does the actual building.
+    void build_();
+
+private:
+    /// Buffer for evaluation of computeOutput
+    mutable Vec m_output_buffer;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PLearnerDiagonalKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From chapados at mail.berlios.de  Mon Apr 13 21:05:08 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 13 Apr 2009 21:05:08 +0200
Subject: [Plearn-commits] r10112 - trunk/plearn/ker
Message-ID: <200904131905.n3DJ58dh016095@sheep.berlios.de>

Author: chapados
Date: 2009-04-13 21:05:07 +0200 (Mon, 13 Apr 2009)
New Revision: 10112

Modified:
   trunk/plearn/ker/PLearnerDiagonalKernel.cc
Log:
Some bugfixes to support the case where the underlying learner is not trained on build

Modified: trunk/plearn/ker/PLearnerDiagonalKernel.cc
===================================================================
--- trunk/plearn/ker/PLearnerDiagonalKernel.cc	2009-04-13 01:39:59 UTC (rev 10111)
+++ trunk/plearn/ker/PLearnerDiagonalKernel.cc	2009-04-13 19:05:07 UTC (rev 10112)
@@ -101,12 +101,11 @@
     if (m_learner.isNull())
         PLERROR("PLearnerDiagonalKernel::build: the option 'learner' must be specified");
 
-    if (m_learner->outputsize() != 1)
-        PLERROR("PLearnerDiagonalKernel::build: the learner must have an outputsize of 1; "
-                "current outputsize is %d", m_learner->outputsize());
+    // At build-time, we don't yet know the learner outputsize
+    // if (m_learner->outputsize() != 1)
+    //     PLERROR("PLearnerDiagonalKernel::build: the learner must have an outputsize of 1; "
+    //             "current outputsize is %d", m_learner->outputsize());
     
-    m_output_buffer.resize(m_learner->outputsize());
-    
     // Ensure that we multiply in Kronecker terms
     inherited::m_default_value = 1.0;
 }
@@ -117,8 +116,10 @@
 real PLearnerDiagonalKernel::evaluate(const Vec& x1, const Vec& x2) const
 {
     PLASSERT( x1.size() == x2.size() );
-    PLASSERT( ! m_learner.isNull() );
+    PLASSERT( ! m_learner.isNull() && m_learner->outputsize() == 1);
 
+    m_output_buffer.resize(m_learner->outputsize());
+    
     if (x1 == x2) {
         real gating_term = inherited::evaluate(x1,x2);
         real sigma = softplus(m_isp_signal_sigma);
@@ -136,7 +137,10 @@
 void PLearnerDiagonalKernel::computeGramMatrix(Mat K) const
 {
     PLASSERT( K.size() == 0 || m_data_cache.size() > 0 );  // Ensure data cached OK
+    PLASSERT( ! m_learner.isNull() && m_learner->outputsize() == 1);
 
+    m_output_buffer.resize(m_learner->outputsize());
+    
     // Most elements are zero, except for the diagonal
     K.fill(0.0);
 



From chapados at mail.berlios.de  Mon Apr 13 21:05:56 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 13 Apr 2009 21:05:56 +0200
Subject: [Plearn-commits] r10113 - trunk/plearn_learners/regressors
Message-ID: <200904131905.n3DJ5uEK016182@sheep.berlios.de>

Author: chapados
Date: 2009-04-13 21:05:55 +0200 (Mon, 13 Apr 2009)
New Revision: 10113

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Bugfix in case where active-set indices is not specified

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-13 19:05:07 UTC (rev 10112)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2009-04-13 19:05:55 UTC (rev 10113)
@@ -409,8 +409,9 @@
     PLASSERT( m_kernel );
     if (! train_set || ! m_training_inputs)
         PLERROR("GaussianProcessRegressor::train: the training set must be specified");
-    int activelength= m_active_set_indices.size();
     int trainlength = train_set->length();
+    int activelength= ( m_algorithm_enum == AlgoProjectedProcess?
+                        m_active_set_indices.size() : trainlength );
     int inputsize   = train_set->inputsize() ;
     int targetsize  = train_set->targetsize();
     int weightsize  = train_set->weightsize();



From chapados at mail.berlios.de  Mon Apr 13 21:06:26 2009
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 13 Apr 2009 21:06:26 +0200
Subject: [Plearn-commits] r10114 - trunk/plearn_learners/generic
Message-ID: <200904131906.n3DJ6Q8k016235@sheep.berlios.de>

Author: chapados
Date: 2009-04-13 21:06:24 +0200 (Mon, 13 Apr 2009)
New Revision: 10114

Modified:
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/PLearner.h
Log:
Added new function computeOutputConfMat

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2009-04-13 19:05:55 UTC (rev 10113)
+++ trunk/plearn_learners/generic/PLearner.cc	2009-04-13 19:06:24 UTC (rev 10114)
@@ -433,6 +433,16 @@
          RetDoc ("Matrix holding the inputs+outputs+confidence-low+confidence-high")));
 
     declareMethod(
+        rmm, "computeOutputConfMat", &PLearner::computeOutputConfMat,
+        (BodyDoc("Return a Mat that is the contatenation of outputs, lower confidence\n"
+                 "bound, and upper confidence bound.  If confidence intervals cannot be\n"
+                 "computed for the learner, they are filled with MISSING_VALUE.\n"),
+         ArgDoc ("inputs", "VMatrix containing the inputs"),
+         ArgDoc ("probability", "Level at which the confidence intervals should be computed, "
+                                "e.g. 0.95."),
+         RetDoc ("Matrix holding the outputs+confidence-low+confidence-high")));
+
+    declareMethod(
         rmm, "computeOutputAndCosts", &PLearner::remote_computeOutputAndCosts,
         (BodyDoc("Compute both the output from the input, and the costs associated\n"
                  "with the desired target.  The computed costs\n"
@@ -1334,6 +1344,41 @@
 }
 
 
+//#####  computeOutputConfMat  ################################################
+
+Mat PLearner::computeOutputConfMat(VMat inputs, real probability) const
+{
+    int l = inputs.length();
+    int nin = inputsize();
+    int nout = outputsize();
+    Mat m(l, 3*nout);
+    TVec< pair<real,real> > intervals;
+    Vec invec(nin);
+    for(int i=0; i<l; i++)
+    {
+        Vec v = m(i);
+        Vec outvec  = v.subVec(0, nout);
+        Vec lowconf = v.subVec(nout, nout);
+        Vec hiconf  = v.subVec(2*nout, nout);
+        inputs->getRow(i, invec);
+        computeOutput(invec, outvec);
+        bool conf_avail = computeConfidenceFromOutput(invec, outvec,
+                                                      probability, intervals);
+        if (conf_avail) {
+            for (int j=0, n=intervals.size() ; j<n ; ++j) {
+                lowconf[j] = intervals[j].first;
+                hiconf[j]  = intervals[j].second;
+            }
+        }
+        else {
+            lowconf << MISSING_VALUE;
+            hiconf  << MISSING_VALUE;
+        }
+    }
+    return m;
+}
+
+
 //////////////////////////
 // remote_computeOutput //
 //////////////////////////

Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2009-04-13 19:05:55 UTC (rev 10113)
+++ trunk/plearn_learners/generic/PLearner.h	2009-04-13 19:06:24 UTC (rev 10114)
@@ -569,6 +569,13 @@
      *  cannot be computed for the learner, they are filled with MISSING_VALUE.
      */
     Mat computeInputOutputConfMat(VMat inputs, real probability) const;
+
+    /**
+     *  Return a Mat that is the contatenation of outputs, lower confidence
+     *  bound, and upper confidence bound.  If confidence intervals cannot be
+     *  computed for the learner, they are filled with MISSING_VALUE.
+     */
+    Mat computeOutputConfMat(VMat inputs, real probability) const;
     
     /**
      *  Compute the output on the training set of the learner, and save the



From tihocan at mail.berlios.de  Tue Apr 14 17:23:16 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 14 Apr 2009 17:23:16 +0200
Subject: [Plearn-commits] r10115 - trunk/plearn/misc
Message-ID: <200904141523.n3EFNG0f001792@sheep.berlios.de>

Author: tihocan
Date: 2009-04-14 17:23:15 +0200 (Tue, 14 Apr 2009)
New Revision: 10115

Modified:
   trunk/plearn/misc/viewVMat.cc
Log:
Got rid of Intel Compiler remark

Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2009-04-13 19:06:24 UTC (rev 10114)
+++ trunk/plearn/misc/viewVMat.cc	2009-04-14 15:23:15 UTC (rev 10115)
@@ -778,7 +778,7 @@
 
                     if (fname[0] == '\0')
                         strcpy(fname, "outCol.txt");
-                    string filename = fname;
+                    string fname_str = fname;
 
                     mvprintw(LINES-1,0,"Writing file '%s'...", fname);
                     clrtoeol();
@@ -787,7 +787,7 @@
                     // Save the selected columns to the desired file, keeping
                     // the string values if 'view_strings' is currently true
                     // (can be toggled with 's'/'S' keys).
-                    vm_showed.columns(indexs)->saveAMAT(filename, false,
+                    vm_showed.columns(indexs)->saveAMAT(fname_str, false,
                                                         false, view_strings);
 
                     mvprintw(LINES-1,0,"*** Output written on: %s ***", fname);



From nouiz at mail.berlios.de  Tue Apr 14 18:15:31 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 14 Apr 2009 18:15:31 +0200
Subject: [Plearn-commits] r10116 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200904141615.n3EGFVJi009008@sheep.berlios.de>

Author: nouiz
Date: 2009-04-14 18:15:30 +0200 (Tue, 14 Apr 2009)
New Revision: 10116

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
implemented the --env and --set_special_env for bqtools


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-14 15:23:15 UTC (rev 10115)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-14 16:15:30 UTC (rev 10116)
@@ -584,13 +584,20 @@
         self.submit_options = ""
         self.jobs_name = ""
         self.m32G = False
+        self.set_special_env = True
+        self.env = ""
+        self.cpu = 0
+        DBIBase.__init__(self, commands, **args)
         
-        DBIBase.__init__(self, commands, **args)
-
         self.nb_proc = int(self.nb_proc)
         self.micro = int(self.micro)
         self.nano = int(self.nano)
+        self.cpu = int(self.cpu)
 
+        if self.set_special_env and self.cpu>0:
+            self.env+=' OMP_NUM_THREADS=%d'%self.cpu
+        if self.env:
+            self.env='export '+self.env
 ### We can't accept the symbols "," as this cause trouble with bqtools
         if self.log_dir.find(',')!=-1 or self.log_file.find(',')!=-1:
             raise DBIError("[DBI] ERROR: The log file(%s) and the log dir(%s) should not have the symbol ','"%(self.log_file,self.log_dir))
@@ -646,9 +653,10 @@
                 HOME=%s
                 export HOME
 
+                %s
                 cd ../../../../
                 (%s '~~task~~')'''
-                % (bq_cluster_home, bq_shell_cmd)
+                % (bq_cluster_home, self.env, bq_shell_cmd)
                 ) )
 
         if int(self.file_redirect_stdout):

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-14 15:23:15 UTC (rev 10115)
+++ trunk/scripts/dbidispatch	2009-04-14 16:15:30 UTC (rev 10116)
@@ -16,6 +16,8 @@
                                                  clusterid(condor only),
                                                  processid(condor only))}+]
                               [--raw=STRING[\nSTRING]]
+                              [*--[no_]set_special_env]
+                              [--env=VAR=VALUE[;VAR2=VALUE2]]
     cluster, condor options  :[--32|--64|--3264] [--os=X]
     bqtools options          :[--micro[=nb_batch]] [--[*no_]long]
                               [--queue=X] [--nano=X] [--submit_options=X]
@@ -26,9 +28,7 @@
                               [--[*no_]getenv] [*--[no_]prefserver] 
                               [--rank=RANK_EXPRESSION] 
                               [--files=file1[,file2...]]
-                              [--env=VAR=VALUE[;VAR2=VALUE2]]
                               [*--[no_]abs_path] [--[*no_]pkdilly]
-                              [*--[no_]set_special_env]
                               [--universe={vanilla*, standard, grid, java,
                                            scheduler, local, parallel, vm}]
                               [--machine=HOSTNAME+] [--machines=regex+]
@@ -113,6 +113,12 @@
       - none    : remove all preceding pattern
   The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the submit file.
       if this option appread many time, they will be concatanated with a new line.
+  The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, 
+    MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
+  The '--env=VAR=VALUE' option will set in the environment of the executing
+    jobs the variable VAR with value VALUE. To pass many variable you can:
+      1) use one --env option and separ the pair by ';'(don't forget to quote)
+      2) you can pass many time the --env parameter.
 
 cluster and condor options:
   The '--3264', '--32' or '--64' specify the type of cpu for the execution node.
@@ -186,12 +192,6 @@
      dbidispatch '--req=regexp("computer0*", target.Machine)'
   The '--[no_]nice' option set the nice_user option to condor. 
     If nice, the job(s) will have the lowest possible priority.
-  The '--env=VAR=VALUE' option will set in the environment of the executing
-    jobs the variable VAR with value VALUE. To pass many variable you can:
-      1) use one --env option and separ the pair by ';'(don't forget to quote)
-      2) you can pass many time the --env parameter.
-  The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, 
-    MKL_NUM_THREADS and GOTO_NUM_THREADS to the number of cpus allocated to job.
   The '--[no_]abs_path' option will tell condor to change the path to the 
     executable to the absolute path or not. Default True.
   The '--[no_]pkdilly': will use the pkdilly tool to make condor more 
@@ -395,7 +395,7 @@
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
                        "nano", "queue", "raw", "submit_options", "jobs_name",
-                       "tasks_filename" ]
+                       "tasks_filename", "set_special_env", "env" ]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.



From tihocan at mail.berlios.de  Tue Apr 14 18:40:35 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 14 Apr 2009 18:40:35 +0200
Subject: [Plearn-commits] r10117 - trunk/python_modules/plearn/pytest
Message-ID: <200904141640.n3EGeZkL018560@sheep.berlios.de>

Author: tihocan
Date: 2009-04-14 18:40:34 +0200 (Tue, 14 Apr 2009)
New Revision: 10117

Modified:
   trunk/python_modules/plearn/pytest/modes.py
Log:
Do not bother printing failed compilation logs on disabled tests

Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2009-04-14 16:15:30 UTC (rev 10116)
+++ trunk/python_modules/plearn/pytest/modes.py	2009-04-14 16:40:34 UTC (rev 10117)
@@ -747,13 +747,13 @@
                     else:
                         logging.debug(e)
                         test.setStatus("SKIPPED", core.traceback(e))
-        l=[]
+        l = set()
         for (test_name, test) in test_instances:
-            if not test.compilationSucceeded():
-                f=test.program.getCompilationLogPath()
+            if not test.compilationSucceeded() and not test.is_disabled():
+                f = test.program.getCompilationLogPath()
                 if f not in l:
-                    l+=[f]
-                    logging.info("The failed compile log %s"%f)
+                    l.add(f)
+                    logging.info("Failed compilation log: %s" % f)
 
 class compile(RoutineBasedMode):
     RoutineType = CompilationRoutine



From tihocan at mail.berlios.de  Tue Apr 14 20:21:21 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 14 Apr 2009 20:21:21 +0200
Subject: [Plearn-commits] r10118 - in trunk/python_modules/plearn: pytest
	utilities
Message-ID: <200904141821.n3EILLUt031165@sheep.berlios.de>

Author: tihocan
Date: 2009-04-14 20:21:20 +0200 (Tue, 14 Apr 2009)
New Revision: 10118

Modified:
   trunk/python_modules/plearn/pytest/core.py
   trunk/python_modules/plearn/pytest/programs.py
   trunk/python_modules/plearn/utilities/cvs.py
   trunk/python_modules/plearn/utilities/toolkit.py
Log:
Removed dependency on deprecated modules sets and popen2 to fix some tests with python 2.6

Modified: trunk/python_modules/plearn/pytest/core.py
===================================================================
--- trunk/python_modules/plearn/pytest/core.py	2009-04-14 16:40:34 UTC (rev 10117)
+++ trunk/python_modules/plearn/pytest/core.py	2009-04-14 18:21:20 UTC (rev 10118)
@@ -1,4 +1,4 @@
-import os, sets, sys
+import os, sys
 from plearn.utilities import toolkit
 from plearn.pyplearn.PyPLearnObject import PLOption, PyPLearnObject, PyPLearnSingleton
 
@@ -124,7 +124,7 @@
     "INTERNAL ERROR"     : 64
     }
 
-__exit_flags = sets.Set()
+__exit_flags = set()
 def exitPyTest(flag=""):
     updateExitCode(flag)    
     sys.exit( sum(__exit_flags) )

Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2009-04-14 16:40:34 UTC (rev 10117)
+++ trunk/python_modules/plearn/pytest/programs.py	2009-04-14 18:21:20 UTC (rev 10118)
@@ -1,4 +1,4 @@
-import logging, os, sets, sys, subprocess
+import logging, os, sys, subprocess
 from plearn.utilities import ppath
 from plearn.utilities import moresh
 from plearn.utilities import toolkit
@@ -149,7 +149,7 @@
             
             cmdline_options = self.cmdline_compile_options.split(",")
 
-            options = list( sets.Set(config_options+cmdline_options) )
+            options = list( set(config_options+cmdline_options) )
             options = " -".join([""]+options).strip()
 
             logging.debug("Test %s: Using compile options '%s' instead of '%s'...",

Modified: trunk/python_modules/plearn/utilities/cvs.py
===================================================================
--- trunk/python_modules/plearn/utilities/cvs.py	2009-04-14 16:40:34 UTC (rev 10117)
+++ trunk/python_modules/plearn/utilities/cvs.py	2009-04-14 18:21:20 UTC (rev 10118)
@@ -1,16 +1,23 @@
 __version_id__ = "$Id$"
 
-import os, popen2, string, types
+import os, string, subprocess, types
 
 from plearn.utilities.ppath     import cvs_directory
 from plearn.utilities.verbosity import vprint
 
+def run_cmd(cmd):
+    """
+    Return Popen object that corresponds to running the given command through
+    the shell.
+    """
+    return subprocess.Popen(cmd, shell = True, stdout = subprocess.PIPE,
+            stderr = subprocess.PIPE)
+
 def add( path ):
     addCmd = "cvs add %s" % path 
     vprint("Adding: " + addCmd, 2)
-    process = popen2.Popen4(addCmd)
-
-    errors = process.fromchild.readlines()
+    process = run_cmd(addCmd)
+    errors = process.stderr.readlines()
     vprint("%s" % string.join( errors, '' ), 1)
 
     return True
@@ -27,9 +34,9 @@
         commit_cmd += f + " " 
         
     vprint("\n+++ Commiting (from "+ os.getcwd() +"):\n" + commit_cmd, 1)
-    commit_process = popen2.Popen4(commit_cmd)    
+    commit_process = run_cmd(commit_cmd)
 
-    errors = commit_process.fromchild.readlines()
+    errors = commit_process.stderr.readlines()
     vprint("%s" % string.join( errors, '' ), 1)
 
 def ignore( path, list_of_paths ):
@@ -49,9 +56,9 @@
     return author
 
 def query(option, fname, lookingFor, delim = "\n"):
-    cvs_process = popen2.Popen4("cvs " + option + " " + fname)
+    cvs_process = run_cmd("cvs " + option + " " + fname)
     
-    lines = cvs_process.fromchild.readlines()
+    lines = cvs_process.stdout.readlines()
     for line in lines :
         index = string.find(line, lookingFor)
         if index != -1:

Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2009-04-14 16:40:34 UTC (rev 10117)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2009-04-14 18:21:20 UTC (rev 10118)
@@ -5,7 +5,7 @@
 module seems to manage similar tasks, it is probably time to create a
 I{similar_tasks.py} L{utilities} submodule to move those functions to.
 """
-import inspect, os, shutil, string, sys, time, types
+import inspect, os, shutil, string, subprocess, sys, time, types
 from os.path import exists, join, abspath
 from string import split
 
@@ -52,7 +52,7 @@
 def command_output(command, stderr = True, stdout = True):
     """Returns the output lines of a shell command.    
     
-    @deprecated Please use the subprocess module instead.
+    @deprecated Please use directly the subprocess module instead.
 
     @param command: The shell command to execute.
     @type command: String
@@ -66,16 +66,17 @@
     @return: Output lines.
     @rtype:  Array of strings.
     """
-    import popen2
     if stderr and stdout:
-        (stdout_and_stderr, stdin) = popen2.popen4(command)
-        return stdout_and_stderr.readlines()
+        p = subprocess.Popen(command, stdout = subprocess.PIPE,
+                stderr = subprocess.STDOUT, shell = True)
+        return p.stdout.readlines()
     else:
-        (stdout_only, stdin, stderr_only) = popen2.popen3(command)
+        p = subprocess.Popen(command, stdout = subprocess.PIPE,
+                stderr = subprocess.PIPE, shell = True)
         if stderr:
-            return stderr_only.readlines()
+            return p.stderr.readlines()
         elif stdout:
-            return stdout_only.readlines()
+            return p.stdout.readlines()
         else:
             return ''
 



From laulysta at mail.berlios.de  Wed Apr 15 01:47:49 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 15 Apr 2009 01:47:49 +0200
Subject: [Plearn-commits] r10119 - trunk/plearn_learners_experimental
Message-ID: <200904142347.n3ENlnNt004556@sheep.berlios.de>

Author: laulysta
Date: 2009-04-15 01:47:48 +0200 (Wed, 15 Apr 2009)
New Revision: 10119

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
generate


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-14 18:21:20 UTC (rev 10118)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-14 23:47:48 UTC (rev 10119)
@@ -2413,22 +2413,23 @@
     return getTestCostNames();
 }
 
-
+/*
 void DenoisingRecurrentNet::generate(int t, int n)
 {
     PLERROR("generate not yet implemented");
 }
+*/
 
 
-/*
-void DenoisingRecurrentNet::oldgenerate(int t, int n)
+void DenoisingRecurrentNet::generate(int t, int n)
 {
     //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
     data = new AutoVMatrix();
     //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/listData/target_tm12_input_t_tm12_tp12/scoreGen_tar_tm12__in_tm12_tp12.amat";
-    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/create_data/scoreGenSuitePerf.amat";
+    //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/create_data/scoreGenSuitePerf.amat";
+    data->filename = "/home/stan/cvs/Gamme/expressive_data/dataGen.amat";
 
-    data->defineSizes(208,16,0);
+    data->defineSizes(173,16,0);
     //data->inputsize = 21;
     //data->targetsize = 0;
     //data->weightsize = 0;
@@ -2445,6 +2446,7 @@
     Vec input;
     Vec target;
     real weight;
+    int targsize;
 
     Vec output(outputsize());
     output.clear();
@@ -2454,7 +2456,8 @@
 //     n_items.clear();
 
     int r,r2;
-    
+    use_target_layers_masks = true;
+
     int ith_sample_in_sequence = 0;
     int inputsize_without_masks = inputsize() 
         - ( use_target_layers_masks ? targetsize() : 0 );
@@ -2471,7 +2474,7 @@
                     for( int tar=0; tar < target_layers.length(); tar++ )
                     {
                         
-                        input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+                        input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar](ith_sample_in_sequence-k);
                         partTarSize -= target_layers[tar]->size;
                         
                         
@@ -2487,7 +2490,7 @@
 //             for( int tar=0; tar < target_layers.length(); tar++ )
 //             {
 //                 if(i>=t){
-//                     input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+//                     input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar](ith_sample_in_sequence-k);
 //                     partTarSize -= target_layers[tar]->size;
 //                 }
 //             }
@@ -2513,12 +2516,12 @@
         }
 
         // Resize internal variables
-        hidden_list.resize(ith_sample_in_sequence+1);
-        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        hidden_list.resize(ith_sample_in_sequence+1, hidden_layer->size);
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1, hidden_layer->size);
         if( hidden_layer2 )
         {
-            hidden2_list.resize(ith_sample_in_sequence+1);
-            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
+            hidden2_list.resize(ith_sample_in_sequence+1, hidden_layer2->size);
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1, hidden_layer2->size);
         }
                  
         input_list.resize(ith_sample_in_sequence+1);
@@ -2531,13 +2534,13 @@
         {
             if( !fast_exact_is_equal(target_layers_weights[tar],0) )
             {
-                targets_list[tar].resize( ith_sample_in_sequence+1);
-                targets_list[tar][ith_sample_in_sequence].resize( 
-                    target_layers[tar]->size);
+                targsize = target_layers[tar]->size;
+                targets_list[tar].resize( ith_sample_in_sequence+1, targsize);
+                //targets_list[tar][ith_sample_in_sequence].resize( target_layers[tar]->size);
                 target_prediction_list[tar].resize(
-                    ith_sample_in_sequence+1);
+                    ith_sample_in_sequence+1, targsize);
                 target_prediction_act_no_bias_list[tar].resize(
-                    ith_sample_in_sequence+1);
+                    ith_sample_in_sequence+1, targsize);
             }
         }
         nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
@@ -2546,7 +2549,7 @@
             masks_list.resize( target_layers.length() );
             for( int tar=0; tar < target_layers.length(); tar++ )
                 if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    masks_list[tar].resize( ith_sample_in_sequence+1 );
+                    masks_list[tar].resize( ith_sample_in_sequence+1, target_layers[tar]->size );
         }
 
         // Forward propagation
@@ -2565,6 +2568,7 @@
             {
                 if( use_target_layers_masks )
                 {
+                    Vec masks_list_tar_i = masks_list[tar](ith_sample_in_sequence);
                     clamp_units(target.subVec(
                                     sum_target_elements,
                                     target_layers_n_of_target_elements[tar]),
@@ -2574,7 +2578,7 @@
                                     inputsize_without_masks 
                                     + sum_target_elements, 
                                     target_layers_n_of_target_elements[tar]),
-                                masks_list[tar][ith_sample_in_sequence]
+                                masks_list_tar_i
                         );
                     
                 }
@@ -2586,81 +2590,68 @@
                                 target_layers[tar],
                                 target_symbol_sizes[tar]);
                 }
-                targets_list[tar][ith_sample_in_sequence] << 
+                targets_list[tar](ith_sample_in_sequence) << 
                     target_layers[tar]->expectation;
             }
             sum_target_elements += target_layers_n_of_target_elements[tar];
         }
-                
+        
+        Vec hidden_act_no_bias_i = hidden_act_no_bias_list(ith_sample_in_sequence);
         input_connections->fprop( input_list[ith_sample_in_sequence], 
-                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
+                                  hidden_act_no_bias_i);
                 
         if( ith_sample_in_sequence > 0 && dynamic_connections )
         {
             dynamic_connections->fprop( 
-                hidden_list[ith_sample_in_sequence-1],
+                hidden_list(ith_sample_in_sequence-1),
                 dynamic_act_no_bias_contribution );
 
-            hidden_act_no_bias_list[ith_sample_in_sequence] += 
+            hidden_act_no_bias_list(ith_sample_in_sequence) += 
                 dynamic_act_no_bias_contribution;
         }
+        
+        Vec hidden_i = hidden_list(ith_sample_in_sequence);
+        hidden_layer->fprop( hidden_act_no_bias_i, 
+                             hidden_i );
+
+        Vec last_hidden = hidden_i;
                  
-        hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
-                             hidden_list[ith_sample_in_sequence] );
-                 
         if( hidden_layer2 )
         {
+            Vec hidden2_i = hidden2_list(ith_sample_in_sequence); 
+            Vec hidden2_act_no_bias_i = hidden2_act_no_bias_list(ith_sample_in_sequence);
+
             hidden_connections->fprop( 
-                hidden_list[ith_sample_in_sequence],
-                hidden2_act_no_bias_list[ith_sample_in_sequence]);
+                hidden2_i,
+                hidden2_act_no_bias_i);
 
             hidden_layer2->fprop( 
-                hidden2_act_no_bias_list[ith_sample_in_sequence],
-                hidden2_list[ith_sample_in_sequence] 
+                hidden2_act_no_bias_i,
+                hidden2_i 
                 );
 
-            for( int tar=0; tar < target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_connections[tar]->fprop(
-                        hidden2_list[ith_sample_in_sequence],
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence]
-                        );
-                    target_layers[tar]->fprop(
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence],
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence] );
-                    if( use_target_layers_masks )
-                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                            masks_list[tar][ith_sample_in_sequence];
-                }
-            }
+            last_hidden = hidden2_i; // last hidden layer vec 
         }
-        else
+           
+       
+        for( int tar=0; tar < target_layers.length(); tar++ )
         {
-            for( int tar=0; tar < target_layers.length(); tar++ )
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
             {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_connections[tar]->fprop(
-                        hidden_list[ith_sample_in_sequence],
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence]
-                        );
-                    target_layers[tar]->fprop(
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence],
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence] );
-                    if( use_target_layers_masks )
-                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                            masks_list[tar][ith_sample_in_sequence];
-                }
+                Vec target_prediction_i = target_prediction_list[tar](i);
+                Vec target_prediction_act_no_bias_i = target_prediction_act_no_bias_list[tar](i);
+                target_connections[tar]->fprop(
+                    last_hidden,
+                    target_prediction_act_no_bias_i
+                    );
+                target_layers[tar]->fprop(
+                    target_prediction_act_no_bias_i,
+                    target_prediction_i );
+                if( use_target_layers_masks )
+                    target_prediction_i *= masks_list[tar](ith_sample_in_sequence);
             }
         }
+        
 
         
 
@@ -2670,15 +2661,15 @@
             if( !fast_exact_is_equal(target_layers_weights[tar],0) )
             {
                 target_layers[tar]->activation << 
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence];
+                    target_prediction_act_no_bias_list[tar](
+                        ith_sample_in_sequence);
                 target_layers[tar]->activation += target_layers[tar]->bias;
                 target_layers[tar]->setExpectation(
-                    target_prediction_list[tar][
-                        ith_sample_in_sequence]);
+                    target_prediction_list[tar](
+                        ith_sample_in_sequence));
                 nll_list(ith_sample_in_sequence,tar) = 
                     target_layers[tar]->fpropNLL( 
-                        targets_list[tar][ith_sample_in_sequence] ); 
+                        targets_list[tar](ith_sample_in_sequence) ); 
 //                 costs[tar] += nll_list(ith_sample_in_sequence,tar);
                 
 //                 // Normalize by the number of things to predict
@@ -2726,14 +2717,14 @@
        
         for( int tar=0; tar < target_layers.length(); tar++ )
         {
-            for (int j = 0; j < target_prediction_list[tar][i].length() ; j++ ){
+            for (int j = 0; j < target_prediction_list[tar](i).length() ; j++ ){
                 
-                if(i>n){
-                    myfile << target_prediction_list[tar][i][j] << " ";
-                }
-                else{
-                    myfile << targets_list[tar][i][j] << " ";
-                }
+                //if(i>n){
+                    myfile << target_prediction_list[tar](i)[j] << " ";
+                    // }
+                    //else{
+                    //    myfile << targets_list[tar](i)[j] << " ";
+                    // }
                        
            
             }
@@ -2746,8 +2737,8 @@
 
 }
 
-*/
 
+
 /*
 void DenoisingRecurrentNet::gen()
 {



From nouiz at mail.berlios.de  Wed Apr 15 16:16:54 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Apr 2009 16:16:54 +0200
Subject: [Plearn-commits] r10120 - trunk/plearn/vmat
Message-ID: <200904151416.n3FEGsWT007233@sheep.berlios.de>

Author: nouiz
Date: 2009-04-15 16:16:53 +0200 (Wed, 15 Apr 2009)
New Revision: 10120

Added:
   trunk/plearn/vmat/ValueSelectRowsVMatrix.cc
   trunk/plearn/vmat/ValueSelectRowsVMatrix.h
Log:
Added a VMatrix that select the row by the value in a column instead of the indices.


Added: trunk/plearn/vmat/ValueSelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ValueSelectRowsVMatrix.cc	2009-04-14 23:47:48 UTC (rev 10119)
+++ trunk/plearn/vmat/ValueSelectRowsVMatrix.cc	2009-04-15 14:16:53 UTC (rev 10120)
@@ -0,0 +1,172 @@
+// -*- C++ -*-
+
+// ValueSelectRowsVMatrix.cc
+//
+// Copyright (C) 2009 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file ValueSelectRowsVMatrix.cc */
+
+
+#include "ValueSelectRowsVMatrix.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ValueSelectRowsVMatrix,
+    "Select rows based on an index column. The value should be in another VMatrix.",
+    "MULTI-LINE \nHELP"
+    );
+
+//////////////////
+// ValueSelectRowsVMatrix //
+//////////////////
+ValueSelectRowsVMatrix::ValueSelectRowsVMatrix()
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void ValueSelectRowsVMatrix::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    declareOption(ol, "col_name", &ValueSelectRowsVMatrix::col_name,
+                  OptionBase::buildoption,
+                  "The name of the column to compare values.");
+
+    declareOption(ol, "col_name_sec", &ValueSelectRowsVMatrix::col_name,
+                  OptionBase::buildoption,
+                  "The name of the column to compare values for the second matrix."
+                  " If not provided, will use col_name.");
+
+    declareOption(ol, "second", &ValueSelectRowsVMatrix::second,
+                  OptionBase::buildoption,
+                  "The matrix that have the value that we keep the columns.");
+
+    inherited::declareOptions(ol);
+}
+
+///////////
+// build //
+///////////
+void ValueSelectRowsVMatrix::build()
+{
+    // ### Nothing to add here, simply calls build_
+//    inherited::build();
+    build_();
+    inherited::build();//must recall as we changed selected_indices
+}
+
+////////////
+// build_ //
+////////////
+void ValueSelectRowsVMatrix::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+
+    // ### In a SourceVMatrix, you will typically end build_() with:
+    // setMetaInfoFromSource();
+
+    // ### You should keep the line 'updateMtime(0);' if you do not implement
+    // ### the update of the mtime. Otherwise you can have an mtime != 0 that
+    // ### is not valid.
+    // ### Note that setMetaInfoFromSource() updates the mtime to the same as
+    // ### the source, but this value will be erased with 'updateMtime(0)'.
+    if(col_name_sec.empty())
+        col_name_sec=col_name;
+    Vec values_src(source->length()), values_sec(second->length());
+    int idx = source->getFieldIndex(col_name,false);
+    if(idx<0)
+        PLERROR("In ValueSelectRowsVMatrix::build_ - the matrix source don't have the column %s",
+                col_name.c_str());
+    source->getColumn(idx,values_src);
+    idx=second->getFieldIndex(col_name_sec);
+    if(idx<0)
+        PLERROR("In ValueSelectRowsVMatrix::build_ - the matrix second don't have the column %s",
+                col_name.c_str());
+    second->getColumn(idx,values_sec);
+
+    //sort values_sec as it is shorter.
+    sortElements(values_sec);
+    for(int i=0;i<values_src.size();i++){
+        real val = values_src[i];
+        int idx=values_sec.findSorted(val);
+        if(values_sec[idx]==val){
+            indices.append(i);
+        }
+    }
+    PLCHECK(indices.size()==values_sec.size());
+    updateMtime(source);
+    updateMtime(second);
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void ValueSelectRowsVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(second, copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/ValueSelectRowsVMatrix.h
===================================================================
--- trunk/plearn/vmat/ValueSelectRowsVMatrix.h	2009-04-14 23:47:48 UTC (rev 10119)
+++ trunk/plearn/vmat/ValueSelectRowsVMatrix.h	2009-04-15 14:16:53 UTC (rev 10120)
@@ -0,0 +1,132 @@
+// -*- C++ -*-
+
+// ValueSelectRowsVMatrix.h
+//
+// Copyright (C) 2009 Frederic Bastien
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Frederic Bastien
+
+/*! \file ValueSelectRowsVMatrix.h */
+
+
+#ifndef ValueSelectRowsVMatrix_INC
+#define ValueSelectRowsVMatrix_INC
+
+#include <plearn/vmat/SelectRowsVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class ValueSelectRowsVMatrix : public SelectRowsVMatrix
+{
+    typedef SelectRowsVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    string col_name;
+    string col_name_sec;
+    VMat second;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ValueSelectRowsVMatrix();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(ValueSelectRowsVMatrix);
+
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ValueSelectRowsVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Wed Apr 15 16:27:41 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Apr 2009 16:27:41 +0200
Subject: [Plearn-commits] r10121 - trunk/plearn/io
Message-ID: <200904151427.n3FERf4b008427@sheep.berlios.de>

Author: nouiz
Date: 2009-04-15 16:27:39 +0200 (Wed, 15 Apr 2009)
New Revision: 10121

Modified:
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
Log:
added an optional parameter to rm(). If set to true(default false) will generate an error if the file exist and we fail to remove it.


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2009-04-15 14:16:53 UTC (rev 10120)
+++ trunk/plearn/io/fileutils.cc	2009-04-15 14:27:39 UTC (rev 10121)
@@ -353,10 +353,13 @@
 ////////
 // rm //
 ////////
-bool rm(const PPath& file)
+bool rm(const PPath& file, bool fail_on_error_if_exist)
 {
     // New cross-platform version.
-    return (PR_Delete(file.absolute().c_str()) == PR_SUCCESS);
+    PRStatus ret = PR_Delete(file.absolute().c_str());
+    if(fail_on_error_if_exist && ret != PR_SUCCESS && pathexists(file))
+        PLERROR("Can't delete file %s",file.c_str());
+    return ret == PR_SUCCESS;
     /*
     // TODO Better cross-platform version ?
 #ifdef WIN32

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2009-04-15 14:16:53 UTC (rev 10120)
+++ trunk/plearn/io/fileutils.h	2009-04-15 14:27:39 UTC (rev 10121)
@@ -129,7 +129,7 @@
 void cp(const PPath& srcpath, const PPath& destpath);
 
 //! Remove a file (return 'true' if removed sucessfully).
-bool rm(const PPath& file);
+bool rm(const PPath& file, bool fail_on_error_if_exist = false);
 
 //! Calls system mv command to move the given source file to destination.
 //! It fail if file exist. Use mvforce to force the overwrite existing file.



From nouiz at mail.berlios.de  Wed Apr 15 18:27:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Apr 2009 18:27:06 +0200
Subject: [Plearn-commits] r10122 - trunk/commands
Message-ID: <200904151627.n3FGR656024023@sheep.berlios.de>

Author: nouiz
Date: 2009-04-15 18:27:05 +0200 (Wed, 15 Apr 2009)
New Revision: 10122

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
added import


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2009-04-15 14:27:39 UTC (rev 10121)
+++ trunk/commands/plearn_desjardins.cc	2009-04-15 16:27:05 UTC (rev 10122)
@@ -107,6 +107,7 @@
 #include <plearn/vmat/VariableDeletionVMatrix.h>
 #include <plearn/vmat/MeanMedianModeImputationVMatrix.h>
 #include <plearn/vmat/MissingIndicatorVMatrix.h>
+#include <plearn/vmat/ValueSelectRowsVMatrix.h>
 
 
 #include "PLearnCommands/plearn_main.h"



From nouiz at mail.berlios.de  Wed Apr 15 19:14:31 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Apr 2009 19:14:31 +0200
Subject: [Plearn-commits] r10123 - trunk/plearn_learners/cgi
Message-ID: <200904151714.n3FHEVRV000618@sheep.berlios.de>

Author: nouiz
Date: 2009-04-15 19:14:30 +0200 (Wed, 15 Apr 2009)
New Revision: 10123

Modified:
   trunk/plearn_learners/cgi/StabilisationLearner.cc
Log:
changed the output name


Modified: trunk/plearn_learners/cgi/StabilisationLearner.cc
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-04-15 16:27:05 UTC (rev 10122)
+++ trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-04-15 17:14:30 UTC (rev 10123)
@@ -155,7 +155,7 @@
 TVec<string> StabilisationLearner::getOutputNames() const
 {
     TVec<string> names(1);
-    names[0]="predicted_class";
+    names[0]="SALES_CATEG_STAB";
     return names;
 }
 



From nouiz at mail.berlios.de  Thu Apr 16 17:28:27 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 16 Apr 2009 17:28:27 +0200
Subject: [Plearn-commits] r10124 - trunk/plearn_learners/online
Message-ID: <200904161528.n3GFSRCH027950@sheep.berlios.de>

Author: nouiz
Date: 2009-04-16 17:28:26 +0200 (Thu, 16 Apr 2009)
New Revision: 10124

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
small uptimisation.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-15 17:14:30 UTC (rev 10123)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-16 15:28:26 UTC (rev 10124)
@@ -965,7 +965,7 @@
     if( !train_stats )
     {
         train_stats = new VecStatsCollector();
-        train_stats->setFieldNames(getTrainCostNames());
+        train_stats->setFieldNames(train_cost_names);
     }
 
     // clear stats of previous epoch
@@ -2602,7 +2602,7 @@
 {
     //Assumes that computeOutput has been called
 
-    costs.resize( getTestCostNames().length() );
+    costs.resize( nTestCosts() );
     costs.fill( MISSING_VALUE );
 
     if(compute_all_test_costs)



From nouiz at mail.berlios.de  Thu Apr 16 21:22:03 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 16 Apr 2009 21:22:03 +0200
Subject: [Plearn-commits] r10125 - trunk/plearn/math
Message-ID: <200904161922.n3GJM3QE027633@sheep.berlios.de>

Author: nouiz
Date: 2009-04-16 21:22:03 +0200 (Thu, 16 Apr 2009)
New Revision: 10125

Modified:
   trunk/plearn/math/TMat_maths_impl.h
   trunk/plearn/math/TMat_maths_specialisation.h
Log:
added more profiler


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2009-04-16 15:28:26 UTC (rev 10124)
+++ trunk/plearn/math/TMat_maths_impl.h	2009-04-16 19:22:03 UTC (rev 10125)
@@ -49,6 +49,7 @@
 
 #include <algorithm>
 #include <limits>
+#include <plearn/sys/Profiler.h>
 
 namespace PLearn {
 using namespace std;
@@ -2929,6 +2930,7 @@
 template <class T>
 void transposeProduct(const TVec<T>& result, const TMat<T>& m, const TVec<T>& v)
 {
+    Profiler::pl_profile_start("transposeProduct T");
     int l=m.length();
 #ifdef BOUNDCHECK
     int w=m.width();
@@ -2945,6 +2947,7 @@
         // not empty, is necessarily zero, since R^0 = {0}.
         if (!result.isEmpty())
             result.clear();
+        Profiler::pl_profile_end("transposeProduct T");
         return;
     }
 
@@ -2958,6 +2961,7 @@
         for (int i=0;i<result.length();i++)
             rp[i] += mj[i] * vj;
     }
+    Profiler::pl_profile_end("transposeProduct T");
 }
 
 //!  result[i] += sum_j m[j,i] * v[j]
@@ -3854,6 +3858,8 @@
 template<class T>
 void externalProductScaleAcc(const TMat<T>& mat, const TVec<T>& v1, const TVec<T>& v2, T gamma)
 {
+    Profiler::pl_profile_start("externalProductScaleAcc T");
+
 #ifdef BOUNDCHECK
     if (v1.length()!=mat.length() || mat.width()!=v2.length())
         PLERROR("externalProductScaleAcc(Vec,Vec), incompatible arguments %dx%d= %d times %d",
@@ -3869,12 +3875,15 @@
         for (int j=0;j<w;j++)
             mi[j] += gamma * v1i * v_2[j];
     }
+    Profiler::pl_profile_end("externalProductScaleAcc T");
 }
 
 // mat[i][j] = alpha * mat[i][j] + gamma * v1[i] * v2[j]
 template<class T>
 void externalProductScaleAcc(const TMat<T>& mat, const TVec<T>& v1, const TVec<T>& v2, T gamma, T alpha)
 {
+    Profiler::pl_profile_start("externalProductScaleAcc T");
+
 #ifdef BOUNDCHECK
     if (v1.length()!=mat.length() || mat.width()!=v2.length())
         PLERROR("externalProductScaleAcc(Vec,Vec), incompatible arguments %dx%d= %d times %d",
@@ -3890,6 +3899,7 @@
         for (int j=0;j<w;j++)
             mi[j] = alpha*mi[j] + gamma * v1i * v_2[j];
     }
+    Profiler::pl_profile_end("externalProductScaleAcc T");
 }
 
 // mat[i][j] *= v1[i] * v2[j]

Modified: trunk/plearn/math/TMat_maths_specialisation.h
===================================================================
--- trunk/plearn/math/TMat_maths_specialisation.h	2009-04-16 15:28:26 UTC (rev 10124)
+++ trunk/plearn/math/TMat_maths_specialisation.h	2009-04-16 19:22:03 UTC (rev 10125)
@@ -194,7 +194,7 @@
                             const TMat<double>& B, bool transposeB,
                             double alpha, double beta)
 {
-    Profiler::pl_profile_start("productScaleAcc(dgemm)");
+    Profiler::pl_profile_start("productScaleAcc(dgemm) specialisation");
 #ifdef BOUNDCHECK
     int l2;
 #endif
@@ -253,7 +253,7 @@
 
     dgemm_(&transb, &transa, &w2, &l1, &w1, &alpha, B.data(), &ldb, A.data(),
            &lda, &beta, C.data(), &ldc);
-    Profiler::pl_profile_end("productScaleAcc(dgemm)");
+    Profiler::pl_profile_end("productScaleAcc(dgemm) specialisation");
 }
 
 //! y <- alpha A.x + beta y 
@@ -262,6 +262,7 @@
                             const TMat<double>& A, bool transposeA,
                             const TVec<double>& x, double alpha, double beta)
 {
+    Profiler::pl_profile_start("productScaleAcc(dgemv_) specialisation");
 #ifdef BOUNDCHECK
     if(!transposeA)
     {
@@ -296,6 +297,7 @@
 
     dgemv_(&trans, &m, &n, &alpha, A.data(), &lda, x.data(), &one, &beta,
            y.data(), &one);
+    Profiler::pl_profile_end("productScaleAcc(dgemv_) specialisation");
 }
 
 //! A <- A + alpha x.y'
@@ -303,6 +305,8 @@
                                     const TVec<double>& x,
                                     const TVec<double>& y, double alpha)
 {
+    Profiler::pl_profile_start("externalProductScaleAcc(dger_) double specialisation");
+
 #ifdef BOUNDCHECK
     if(A.length()!=x.length() || A.width()!=y.length())
         PLERROR("In externalProductScaleAcc, incompatible dimensions:\n"
@@ -322,6 +326,7 @@
         return;
 
     dger_(&m, &n, &alpha, y.data(), &one, x.data(), &one, A.data(), &lda);
+    Profiler::pl_profile_end("externalProductScaleAcc(dger_) double specialisation");
 }
 
 inline void externalProductAcc(const TMat<double>& A,
@@ -427,7 +432,7 @@
                             const TMat<float>& B, bool transposeB,
                             float alpha, float beta)
 {
-    Profiler::pl_profile_start("productScaleAcc(sgemm)");
+    Profiler::pl_profile_start("productScaleAcc(sgemm) specialisation");
 
 #ifdef BOUNDCHECK
     int l2;
@@ -481,7 +486,7 @@
 
     sgemm_(&transb, &transa, &w2, &l1, &w1, &alpha, B.data(), &ldb, A.data(),
            &lda, &beta, C.data(), &ldc);
-    Profiler::pl_profile_end("productScaleAcc(sgemm)");
+    Profiler::pl_profile_end("productScaleAcc(sgemm) specialisation");
 }
 
 //! y <- alpha A.x + beta y
@@ -490,6 +495,7 @@
                             const TMat<float>& A, bool transposeA,
                             const TVec<float>& x, float alpha, float beta)
 {
+    Profiler::pl_profile_start("productScaleAcc(sger_) specialisation");
 #ifdef BOUNDCHECK
     if(!transposeA)
     {
@@ -515,12 +521,14 @@
 
       sgemv_(&trans, &m, &n, &alpha, A.data(), &lda, x.data(), &one, &beta,
              y.data(), &one);
+    Profiler::pl_profile_end("productScaleAcc(sger_) specialisation");
 }
 
 //! A <- A + alpha x.y'
 inline void externalProductScaleAcc(const TMat<float>& A, const TVec<float>& x,
                                     const TVec<float>& y, float alpha)
 {
+    Profiler::pl_profile_start("externalProductScaleAcc(sger_) float specialisation");
 #ifdef BOUNDCHECK
     if(A.length()!=x.length() || A.width()!=y.length())
         PLERROR("In externalProductScaleAcc, incompatible dimensions:\n"
@@ -536,6 +544,7 @@
         return;                                 // with actual calculation
 
     sger_(&m, &n, &alpha, y.data(), &one, x.data(), &one, A.data(), &lda);
+    Profiler::pl_profile_end("externalProductScaleAcc(sger_) float specialisation");
 }
 
 inline void externalProductAcc(const TMat<float>& A, const TVec<float>& x,



From nouiz at mail.berlios.de  Thu Apr 16 22:32:08 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 16 Apr 2009 22:32:08 +0200
Subject: [Plearn-commits] r10126 - trunk/plearn_learners/online
Message-ID: <200904162032.n3GKW8qB004119@sheep.berlios.de>

Author: nouiz
Date: 2009-04-16 22:32:08 +0200 (Thu, 16 Apr 2009)
New Revision: 10126

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
implemented a minibatch version of the test for StackedAutoassociatorsNet. To use it, you must set test_minibatch_size>1(I use 128) This don't change the results. This only work when correlation_connections.length() == 0 and currently_trained_layer == n_layers and !compute_all_test_costs. If that is not true, we call the old implementation.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-16 19:22:03 UTC (rev 10125)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-16 20:32:08 UTC (rev 10126)
@@ -41,6 +41,7 @@
 
 #include "StackedAutoassociatorsNet.h"
 #include <plearn/io/pl_log.h>
+#include <plearn/sys/Profiler.h>
 
 #define minibatch_hack 0 // Do we force the minibatch setting? (debug hack)
 
@@ -937,6 +938,7 @@
 
 void StackedAutoassociatorsNet::train()
 {
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::train");
     MODULE_LOG << "train() called " << endl;
     MODULE_LOG << "  training_schedule = " << training_schedule << endl;
 
@@ -976,8 +978,10 @@
 
     if( !online )
     {
+        Profiler::pl_profile_start("StackedAutoassociatorsNet::train !online");
 
         /***** initial greedy training *****/
+        Profiler::pl_profile_start("StackedAutoassociatorsNet::train greedy");
         for( int i=0 ; i<n_layers-1 ; i++ )
         {
             MODULE_LOG << "Training connection weights between layers " << i
@@ -1116,10 +1120,13 @@
                     pb->update( *this_stage - init_stage + 1 );
             }
         }
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::train greedy");
 
         /***** unsupervised fine-tuning by gradient descent *****/
         if( unsupervised_stage < unsupervised_nstages )
         {
+            Profiler::pl_profile_start("StackedAutoassociatorsNet::train unsupervised");
+
 //            if( unsupervised_nstages > 0 && correlation_connections.length() != 0 )
 //                PLERROR("StackedAutoassociatorsNet::train()"
 //                        " - \n"
@@ -1173,11 +1180,13 @@
                 if( pb )
                     pb->update( unsupervised_stage - init_stage + 1 );
             }
+            Profiler::pl_profile_end("StackedAutoassociatorsNet::train unsupervised");
         }
 
         /***** fine-tuning by gradient descent *****/
         if( stage < nstages )
         {
+            Profiler::pl_profile_start("StackedAutoassociatorsNet::train supervised");
 
             MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
             MODULE_LOG << "  stage = " << stage << endl;
@@ -1222,9 +1231,13 @@
                   && greedy_stages[currently_trained_layer-1] <= 0)
                 currently_trained_layer--;
         }
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::train !online");
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::train supervised");
     }
     else // online==true
     {
+        Profiler::pl_profile_start("StackedAutoassociatorsNet::train online");
+
         if( unsupervised_nstages > 0 )
             PLERROR("StackedAutoassociatorsNet::train()"
                     " - \n"
@@ -1290,8 +1303,10 @@
                     pb->update(stage - init_stage + 1);
             }
         }
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::train online");
 
     }
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::train");
 }
 
 void StackedAutoassociatorsNet::corrupt_input(const Vec& input, Vec& corrupted_input, int layer)
@@ -1375,6 +1390,7 @@
 void StackedAutoassociatorsNet::greedyStep(const Vec& input, const Vec& target,
                                            int index, Vec train_costs)
 {
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep");
     PLASSERT( index < n_layers );
 
     expectations[0] << input;
@@ -1464,10 +1480,12 @@
                                         activation_gradients[ index + 1 ],
                                         expectation_gradients[ index + 1 ] );
 
+        Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep bprop connection");
         connections[ index ]->bpropUpdate( corrupted_autoassociator_expectations[index],
                                            activations[ index + 1 ],
                                            expectation_gradients[ index ],
                                            activation_gradients[ index + 1 ] );
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep bprop connection");
     }
 
     reconstruction_connections[ index ]->fprop( expectations[ index + 1],
@@ -1523,6 +1541,7 @@
 
         if(reconstruct_hidden)
         {
+            Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep reconstruct_hidden");
             connections[ index ]->fprop( layers[ index ]->expectation,
                                          hidden_reconstruction_activations );
             layers[ index+1 ]->fprop( hidden_reconstruction_activations,
@@ -1538,17 +1557,20 @@
                                         hidden_reconstruction_activation_gradients);
             layers[ index+1 ]->update(hidden_reconstruction_activation_gradients);
 
+            Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep reconstruct_hidden connection bprop");
             connections[ index ]->bpropUpdate(
                 layers[ index ]->expectation,
                 hidden_reconstruction_activations,
                 reconstruction_expectation_gradients_from_hid_rec,
                 hidden_reconstruction_activation_gradients);
+            Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep reconstruct_hidden connection bprop");
 
             layers[ index ]->bpropUpdate(
                 reconstruction_activations,
                 layers[ index ]->expectation,
                 reconstruction_activation_gradients_from_hid_rec,
                 reconstruction_expectation_gradients_from_hid_rec);
+            Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep reconstruct_hidden");
         }
 
         layers[ index ]->update(reconstruction_activation_gradients);
@@ -1650,6 +1672,7 @@
         }
     }
 
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep");
 }
 
 void StackedAutoassociatorsNet::greedyStep(const Mat& inputs,
@@ -1808,6 +1831,9 @@
                                                const Vec& target,
                                                Vec& train_costs)
 {
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep");
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep fprop");
+
     // fprop
     expectations[0] << input;
 
@@ -1828,11 +1854,14 @@
     {
         for( int i=0 ; i<n_layers-1; i++ )
         {
+            Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep fprop connection");
             connections[i]->fprop( expectations[i], activations[i+1] );
+            Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep fprop connection");
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
     }
 
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep fprop");
     final_module->fprop( expectations[ n_layers-1 ],
                          final_cost_input );
     final_cost->fprop( final_cost_input, target, final_cost_value );
@@ -1849,6 +1878,7 @@
                                expectation_gradients[ n_layers-1 ],
                                final_cost_gradient );
 
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep bpropUpdate");
     if( correlation_connections.length() != 0 )
     {
         for( int i=n_layers-1 ; i>0 ; i-- )
@@ -1885,12 +1915,16 @@
                                     activation_gradients[i],
                                     expectation_gradients[i] );
 
+           Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep bpropUpdate connection");
             connections[i-1]->bpropUpdate( expectations[i-1],
                                            activations[i],
                                            expectation_gradients[i-1],
                                            activation_gradients[i] );
+           Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep bpropUpdate connection");
         }
     }
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep bpropUpdate");
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep");
 }
 
 void StackedAutoassociatorsNet::fineTuningStep(const Mat& inputs,
@@ -2495,6 +2529,7 @@
 
 void StackedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
 {
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutput");
     // fprop
 
     expectations[0] << input;
@@ -2554,8 +2589,64 @@
     else
         final_module->fprop( expectations[ currently_trained_layer - 1],
                              output );
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::computeOutput");
 }
 
+void StackedAutoassociatorsNet::computeOutputs(const Mat& input, Mat& output) const
+{
+    if(correlation_connections.length() != 0
+       || currently_trained_layer!=n_layers
+       || compute_all_test_costs){
+        inherited::computeOutputs(input, output);
+    }else{
+        Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutputs");
+        PLCHECK(correlation_connections.length() == 0);
+        PLCHECK(currently_trained_layer == n_layers);
+        PLCHECK(!compute_all_test_costs);
+
+        expectations_m[0].resize(input.length(), inputsize());
+        Mat m = expectations_m[0];
+        m<<input;
+        for(int i=0 ; i<currently_trained_layer-1 ; i++ )
+        {
+            connections[i]->fprop( expectations_m[i], activations_m[i+1] );
+            layers[i+1]->fprop(activations_m[i+1],expectations_m[i+1]);
+        }
+        final_module->fprop( expectations_m[ currently_trained_layer - 1],
+                             output );
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::computeOutputs");
+    }
+}
+
+void StackedAutoassociatorsNet::computeOutputsAndCosts(const Mat& input, const Mat& target,
+                                                       Mat& output, Mat& costs) const
+{
+    if(correlation_connections.length() != 0 
+       || currently_trained_layer!=n_layers
+       || compute_all_test_costs){
+        inherited::computeOutputsAndCosts(input, target, output, costs);
+    }else{
+        Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutputsAndCosts");
+        PLCHECK(correlation_connections.length() == 0);
+        PLCHECK(currently_trained_layer == n_layers);
+        PLCHECK(!compute_all_test_costs);
+
+        int n=input.length();
+        PLASSERT(target.length()==n);
+        output.resize(n,outputsize());
+        costs.resize(n,nTestCosts());
+        computeOutputs(input, output);
+        for (int i=0;i<n;i++)
+        {
+            Vec in_i = input(i);
+            Vec out_i = output(i); 
+            Vec target_i = target(i);
+            Vec c_i = costs(i);
+            computeCostsFromOutputs(in_i, out_i, target_i, c_i);
+        }
+        Profiler::pl_profile_end("StackedAutoassociatorsNet::computeOutputsAndCosts");
+    }
+}
 void StackedAutoassociatorsNet::computeOutputWithoutCorrelationConnections(const Vec& input, Vec& output) const
 {
     // fprop
@@ -2602,6 +2693,7 @@
 {
     //Assumes that computeOutput has been called
 
+    Profiler::pl_profile_start("StackedAutoassociatorsNet::computeCostsFromOutputs");
     costs.resize( nTestCosts() );
     costs.fill( MISSING_VALUE );
 
@@ -2703,6 +2795,7 @@
                      final_cost_value.length()) <<
             final_cost_value;
     }
+    Profiler::pl_profile_end("StackedAutoassociatorsNet::computeCostsFromOutputs");
 }
 
 TVec<string> StackedAutoassociatorsNet::getTestCostNames() const

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-04-16 19:22:03 UTC (rev 10125)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-04-16 20:32:08 UTC (rev 10126)
@@ -244,10 +244,16 @@
     //! Computes the output from the input.
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
+    //! Computes the output from the input.
+    virtual void computeOutputs(const Mat& input, Mat& output) const;
+
     //! Computes the costs from already computed output.
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
 
+    virtual void computeOutputsAndCosts(const Mat& input, const Mat& target,
+                                        Mat& output, Mat& costs) const;
+
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
     virtual TVec<std::string> getTestCostNames() const;



From tihocan at mail.berlios.de  Fri Apr 17 17:07:49 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 17 Apr 2009 17:07:49 +0200
Subject: [Plearn-commits] r10127 - trunk/python_modules/plearn/utilities
Message-ID: <200904171507.n3HF7nX1032070@sheep.berlios.de>

Author: tihocan
Date: 2009-04-17 17:07:48 +0200 (Fri, 17 Apr 2009)
New Revision: 10127

Modified:
   trunk/python_modules/plearn/utilities/inject_import.py
Log:
Added missing argument to __inject_import__ that was making some tests fail on mammouth

Modified: trunk/python_modules/plearn/utilities/inject_import.py
===================================================================
--- trunk/python_modules/plearn/utilities/inject_import.py	2009-04-16 20:32:08 UTC (rev 10126)
+++ trunk/python_modules/plearn/utilities/inject_import.py	2009-04-17 15:07:48 UTC (rev 10127)
@@ -47,7 +47,7 @@
 # Overriding builtin import function
 import __builtin__  
 __builtin_import__ = __builtin__.__import__
-def __inject_import__(name, globals_arg=None, locals_arg=None, fromlist=[]):
+def __inject_import__(name, globals_arg=None, locals_arg=None, fromlist=[], level=-1):
     """Import function that injects locals in the imported module.
 
     This function is meant to override the builtin I{__import__} function
@@ -69,7 +69,7 @@
 
     # print >>sys.stderr, "Python path is:",sys.path
     # print >>sys.stderr, "Current directory is:",os.getcwd()
-    module = __builtin_import__(name, globals_arg, locals_arg, fromlist)
+    module = __builtin_import__(name, globals_arg, locals_arg, fromlist, level)
     if '__injected__' in globals_arg:
         if hasattr(module, 'injected'):
             module.injected.update(globals_arg['__injected__'])



From nouiz at mail.berlios.de  Mon Apr 20 16:42:30 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 20 Apr 2009 16:42:30 +0200
Subject: [Plearn-commits] r10128 - trunk/plearn_learners/cgi
Message-ID: <200904201442.n3KEgUWR013215@sheep.berlios.de>

Author: nouiz
Date: 2009-04-20 16:42:27 +0200 (Mon, 20 Apr 2009)
New Revision: 10128

Modified:
   trunk/plearn_learners/cgi/StabilisationLearner.cc
Log:
correctly handle missing value.


Modified: trunk/plearn_learners/cgi/StabilisationLearner.cc
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-04-17 15:07:48 UTC (rev 10127)
+++ trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-04-20 14:42:27 UTC (rev 10128)
@@ -99,16 +99,20 @@
 
 void StabilisationLearner::computeOutput(const Vec& input, Vec& output) const
 {
-    real pred=int(input[0]);
+    real pred_=input[0];
+    
     real l1=input[1];
     real l2=input[2];
-    real old_=int(input[3]);
-    real old,ret;
-    if(old_==3) old=2;
-    else old=old_;
+    real old_=input[3];
+    int old,pred;
+    real ret;
 
-//    if not isNaN(real):     ret = real
-    if(old==pred)           ret = pred;
+    pred=int(pred_);
+    old=int(old_);
+    if(old==3)                old=2;
+
+    if (is_missing(old_))       ret=pred;
+    else if(old==pred)          ret = pred;
     else if(old==0 and pred==2) ret = 1;
     else if(old==2 and pred==0) ret = 1;
     else if(old==0 and pred==1)
@@ -119,6 +123,8 @@
         ret = ((l2-threshold)>=0.5)+1;
     else if(old==2 and pred==1)
         ret = ((l2+threshold)>=0.5)+1;
+    else if(is_missing(old))
+        ret = pred;
     else{
         ret = pred;
         NORMAL_LOG<< "We don't know what to do with old="<<old<<" and pred="<<pred<<endl;    



From nouiz at mail.berlios.de  Mon Apr 20 16:45:19 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 20 Apr 2009 16:45:19 +0200
Subject: [Plearn-commits] r10129 - trunk/plearn_learners/cgi
Message-ID: <200904201445.n3KEjJ08015715@sheep.berlios.de>

Author: nouiz
Date: 2009-04-20 16:45:10 +0200 (Mon, 20 Apr 2009)
New Revision: 10129

Modified:
   trunk/plearn_learners/cgi/StabilisationLearner.cc
Log:
removed duplicate code.


Modified: trunk/plearn_learners/cgi/StabilisationLearner.cc
===================================================================
--- trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-04-20 14:42:27 UTC (rev 10128)
+++ trunk/plearn_learners/cgi/StabilisationLearner.cc	2009-04-20 14:45:10 UTC (rev 10129)
@@ -123,8 +123,6 @@
         ret = ((l2-threshold)>=0.5)+1;
     else if(old==2 and pred==1)
         ret = ((l2+threshold)>=0.5)+1;
-    else if(is_missing(old))
-        ret = pred;
     else{
         ret = pred;
         NORMAL_LOG<< "We don't know what to do with old="<<old<<" and pred="<<pred<<endl;    



From nouiz at mail.berlios.de  Mon Apr 20 16:52:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 20 Apr 2009 16:52:45 +0200
Subject: [Plearn-commits] r10130 - trunk/plearn/vmat
Message-ID: <200904201452.n3KEqj98020714@sheep.berlios.de>

Author: nouiz
Date: 2009-04-20 16:52:38 +0200 (Mon, 20 Apr 2009)
New Revision: 10130

Modified:
   trunk/plearn/vmat/ValueSelectRowsVMatrix.cc
Log:
changed an error for a warning in case some example disapear.


Modified: trunk/plearn/vmat/ValueSelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ValueSelectRowsVMatrix.cc	2009-04-20 14:45:10 UTC (rev 10129)
+++ trunk/plearn/vmat/ValueSelectRowsVMatrix.cc	2009-04-20 14:52:38 UTC (rev 10130)
@@ -142,7 +142,9 @@
             indices.append(i);
         }
     }
-    PLCHECK(indices.size()==values_sec.size());
+    if(indices.size()!=values_sec.size())
+        PLWARNING("In ValueSelectRowsVMatrix::build_() - we select less row(%d) then asked(%d)."
+                  " Meaby some disapeared.",indices.size(),values_sec.size());
     updateMtime(source);
     updateMtime(second);
 }



From nouiz at mail.berlios.de  Mon Apr 20 21:18:16 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 20 Apr 2009 21:18:16 +0200
Subject: [Plearn-commits] r10131 - trunk/plearn/io
Message-ID: <200904201918.n3KJIGJZ011312@sheep.berlios.de>

Author: nouiz
Date: 2009-04-20 21:18:15 +0200 (Mon, 20 Apr 2009)
New Revision: 10131

Modified:
   trunk/plearn/io/PyPLearnScript.cc
   trunk/plearn/io/PyPLearnScript.h
Log:
return the date of the file if we can calculate it.


Modified: trunk/plearn/io/PyPLearnScript.cc
===================================================================
--- trunk/plearn/io/PyPLearnScript.cc	2009-04-20 14:52:38 UTC (rev 10130)
+++ trunk/plearn/io/PyPLearnScript.cc	2009-04-20 19:18:15 UTC (rev 10131)
@@ -322,7 +322,7 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
-Object* smartLoadObject(PPath filepath, const vector<string>& args)
+Object* smartLoadObject(PPath filepath, const vector<string>& args, time_t& return_date)
 {
     if (!isfile(filepath))
         PLERROR("Non-existent script file: %s\n",filepath.c_str());
@@ -372,6 +372,7 @@
     else if(extension==".psave") // do not perform plearn macro expansion
     {
         in = openFile(filepath, PStream::plearn_ascii);
+        date=mtime(filepath);
     }
     else
         PLERROR("In smartLoadObject: unsupported file extension. Must be one of .pyplearn .pymat .plearn .vmat .psave");
@@ -379,7 +380,7 @@
     Object* o = readObject(in);
     if(extension==".vmat")
         ((VMatrix*)o)->updateMtime(date);
-
+    return_date=date;
     if ( pyplearn_script.isNotNull() )
         pyplearn_script->close();
 

Modified: trunk/plearn/io/PyPLearnScript.h
===================================================================
--- trunk/plearn/io/PyPLearnScript.h	2009-04-20 14:52:38 UTC (rev 10130)
+++ trunk/plearn/io/PyPLearnScript.h	2009-04-20 19:18:15 UTC (rev 10131)
@@ -191,11 +191,20 @@
 //! .plearn .vmat : perform simple plearn macro processing
 //! .pyplearn .pymat: use python preprocessor
 //! The given args vector can be used to pass string arguments of the form argname=value.
-Object* smartLoadObject(PPath filepath, const vector<string>& args);
+//! The return_date is set to the lastest date of dependence of file. 
+//!   Otherwise return (time_t)0. Work for .vmat, .psave and .plearn file.
+Object* smartLoadObject(PPath filepath, const vector<string>& args, time_t& return_date);
 
-//! Same as smartLoadObject(PPath, vector<string>) but passing an empty vector<string>
+//! Same as smartLoadObject(PPath, vector<string>, time_t) but passing an empty return_date
+inline Object* smartLoadObject(PPath filepath, const vector<string>& args)
+{ time_t d=0; return smartLoadObject(filepath, args, d); }
+//! Same as smartLoadObject(PPath, vector<string>, time_t) but passing an empty vector<string>
+inline Object* smartLoadObject(PPath filepath, time_t& return_date)
+{ vector<string> args; return smartLoadObject(filepath, args,return_date); }
+//! Same as smartLoadObject(PPath, vector<string>, time_t) but passing an empty vector<string> 
+//! and an empty return_date
 inline Object* smartLoadObject(PPath filepath)
-{ vector<string> args; return smartLoadObject(filepath, args); }
+{time_t d=0;return smartLoadObject(filepath, d);}
 
   
 } // end of namespace PLearn



From nouiz at mail.berlios.de  Mon Apr 20 21:36:57 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 20 Apr 2009 21:36:57 +0200
Subject: [Plearn-commits] r10132 - trunk/plearn/vmat
Message-ID: <200904201936.n3KJavVR013983@sheep.berlios.de>

Author: nouiz
Date: 2009-04-20 21:36:56 +0200 (Mon, 20 Apr 2009)
New Revision: 10132

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
print the log error as a warning, we we know that they happened.


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2009-04-20 19:18:15 UTC (rev 10131)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2009-04-20 19:36:56 UTC (rev 10132)
@@ -186,8 +186,10 @@
             {
                 fields = splitIntoFields(buf);
                 int nf = fields.length();
-                if(nf!=fieldspec.size())
+                if(nf!=fieldspec.size()){
                     fprintf(logfile, "ERROR In file %d line %d: Found %d fields (should be %d):\n %s",fileno,lineno,nf,fieldspec.size(),buf);
+                    PLWARNING("In file %d line %d: Found %d fields (should be %d):\n %s",fileno,lineno,nf,fieldspec.size(),buf);
+                }
                 else  // Row OK! append it to index
                 {
                     fputc(fileno, idxfile);



From tihocan at mail.berlios.de  Mon Apr 20 21:42:11 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 20 Apr 2009 21:42:11 +0200
Subject: [Plearn-commits] r10133 - trunk/plearn/vmat
Message-ID: <200904201942.n3KJgBgj014825@sheep.berlios.de>

Author: tihocan
Date: 2009-04-20 21:42:11 +0200 (Mon, 20 Apr 2009)
New Revision: 10133

Modified:
   trunk/plearn/vmat/ValueSelectRowsVMatrix.cc
Log:
Fixed line length and minor typos in warning

Modified: trunk/plearn/vmat/ValueSelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ValueSelectRowsVMatrix.cc	2009-04-20 19:36:56 UTC (rev 10132)
+++ trunk/plearn/vmat/ValueSelectRowsVMatrix.cc	2009-04-20 19:42:11 UTC (rev 10133)
@@ -143,8 +143,9 @@
         }
     }
     if(indices.size()!=values_sec.size())
-        PLWARNING("In ValueSelectRowsVMatrix::build_() - we select less row(%d) then asked(%d)."
-                  " Meaby some disapeared.",indices.size(),values_sec.size());
+        PLWARNING("In ValueSelectRowsVMatrix::build_() - we selected less rows"
+                " (%d) than asked(%d). Maybe some disappeared.",
+                indices.size(), values_sec.size());
     updateMtime(source);
     updateMtime(second);
 }



From nouiz at mail.berlios.de  Mon Apr 20 23:32:40 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 20 Apr 2009 23:32:40 +0200
Subject: [Plearn-commits] r10134 - trunk/plearn_learners/generic
Message-ID: <200904202132.n3KLWer6031674@sheep.berlios.de>

Author: nouiz
Date: 2009-04-20 23:32:40 +0200 (Mon, 20 Apr 2009)
New Revision: 10134

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
bugfix when we only want to record the train time.


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2009-04-20 19:42:11 UTC (rev 10133)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2009-04-20 21:32:40 UTC (rev 10134)
@@ -88,6 +88,8 @@
       total_train_time(0),
       test_time(0),
       total_test_time(0),
+      train_time_b(false),
+      test_time_b(false),
       check_output_consistency(1),
       combine_bag_outputs_method(1),
       compute_costs_on_bags(0),
@@ -214,6 +216,14 @@
                   &AddCostToLearner::total_test_time, OptionBase::learntoption,
                   "The total time spent in the test() function in second.");
 
+    declareOption(ol, "train_time_b",
+                  &AddCostToLearner::train_time, OptionBase::learntoption,
+                  "If we should calculate the time spent in the train.");
+
+    declareOption(ol, "test_time_b",
+                  &AddCostToLearner::test_time, OptionBase::learntoption,
+                  "If we should calculate the time spent in the test.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -238,7 +248,6 @@
     int n = costs.length();
     int min_verb = 2;
     bool display = (verbosity >= min_verb);
-    bool activ_profiler=false;
     int os = learner_->outputsize();
     if (os < 0) {
         // The sub-learner does not know its outputsize yet: we skip the build for
@@ -284,14 +293,13 @@
         } else if (c == "class_error") {
         } else if (c == "binary_class_error") {
         } else if (c == "train_time") {
-            activ_profiler=true;
+            train_time_b=true;
         } else if (c == "total_train_time") {
-            activ_profiler=true;
+            train_time_b=true;
         } else if (c == "test_time") {
-            activ_profiler=true;
-            Profiler::reset("AddCostToLearner::test");
+            test_time_b=true;
         } else if (c == "total_test_time") {
-            activ_profiler=true;
+            test_time_b=true;
         } else if (c == "linear_class_error") {
         } else if (c == "square_class_error") {
         } else if (c == "confusion_matrix") {
@@ -322,7 +330,11 @@
     if (n > 0 && display) {
         cout << endl;
     }
-    if(activ_profiler)
+    
+    if(test_time_b)
+        Profiler::reset("AddCostToLearner::test");
+
+    if(test_time_b || train_time_b)
         Profiler::activate();
 }
 
@@ -795,16 +807,17 @@
 
     }
     Profiler::end("AddCostToLearner::train");
-    if(Profiler::isActive()){
+    if(train_time_b){
         const Profiler::Stats& stats = Profiler::getStats("AddCostToLearner::train");
         real tmp=stats.wall_duration/Profiler::ticksPerSecond();
         train_time=tmp - total_train_time;
         total_train_time=tmp;
-
+    }
+    if(test_time_b){
         //we get the test_time here as we want the test time for all dataset.
         //if we put it in the test function, we would have it for one dataset.
         const Profiler::Stats& stats_test = Profiler::getStats("AddCostToLearner::test");
-        tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
+        real tmp=stats_test.wall_duration/Profiler::ticksPerSecond();
         test_time=tmp-total_test_time;
         total_test_time=tmp;  
     }

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2009-04-20 19:42:11 UTC (rev 10133)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2009-04-20 21:32:40 UTC (rev 10134)
@@ -117,6 +117,9 @@
     //! The total time passed in test()
     real total_test_time;
 
+    bool train_time_b;
+    bool test_time_b;
+
 public:
 
     // ************************



From nouiz at mail.berlios.de  Tue Apr 21 16:23:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Apr 2009 16:23:38 +0200
Subject: [Plearn-commits] r10135 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200904211423.n3LENc8f031444@sheep.berlios.de>

Author: nouiz
Date: 2009-04-21 16:23:38 +0200 (Tue, 21 Apr 2009)
New Revision: 10135

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
generalysed the handling of the log file name to be the same on all backend.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-20 21:32:40 UTC (rev 10134)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:23:38 UTC (rev 10135)
@@ -180,6 +180,8 @@
         self.temp_files = []
         self.arch = 0 # TODO, we should put the local arch: 32,64 or 3264 bits
         self.base_tasks_log_file = []
+        self.stdouts = ''
+        self.stderrs = ''
         self.raw = ''
         self.cpu = 0
         self.mem = 0
@@ -210,6 +212,22 @@
 
     def add_commands(self,commands): raise NotImplementedError, "DBIBase.add_commands()"
 
+    def get_file_redirection(self, task_id):
+        """ Calculate the file to use for stdout/stderr
+        """
+        n=task_id-1
+        base=self.tasks[n].log_file
+        if self.base_tasks_log_file:
+            base = self.base_tasks_log_file[n]
+            base=os.path.join(self.log_dir,base)
+            self.check_path(base)
+        elif self.stdouts and self.stderrs:
+            assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
+            return (self.stdouts[n], self.stderrs[n])
+
+        return (base + '.out',base + '.err')
+            
+
     def get_redirection(self,stdout_file,stderr_file):
         """Compute the needed redirection based of the objects attribute.
         Return a tuple (stdout,stderr) that can be used with popen.
@@ -509,7 +527,7 @@
         task.launch_time = time.time()
         task.set_scheduled_time()
 
-        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
+        (output,error)=self.get_redirection(*self.get_file_redirection(task.id))
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
         task.p_wait_ret=task.p.wait()
         task.dbi_return_status=None
@@ -669,17 +687,15 @@
         # and another one containing the log_file name associated
         tasks_file = open( 'tasks', 'w' )
         logfiles_file = open( 'logfiles', 'w' )
-        if self.base_tasks_log_file:
-            for task,base in zip(self.tasks,self.base_tasks_log_file):
-                #-4 as we will happend .err or .out
-                p=os.path.join(self.log_dir,base)
-                self.check_path(p)
-                tasks_file.write( ';'.join(task.commands) + '\n' )
-                logfiles_file.write( p + '\n' )
-        else:
-            for task in self.tasks:
-                tasks_file.write( ';'.join(task.commands) + '\n' )
-                logfiles_file.write( task.log_file + '\n' )
+        assert len(self.stdouts)==len(self.stderrs)==0
+        for task in self.tasks:
+            #-4 as we will happend .err or .out
+            base=self.get_file_redirection(task.id)[0][:-4]
+            p=os.path.join(self.log_dir,base)
+            self.check_path(p)
+            tasks_file.write( ';'.join(task.commands) + '\n' )
+            logfiles_file.write( p + '\n' )
+
         tasks_file.close()
         logfiles_file.close()
 
@@ -789,8 +805,6 @@
         self.redirect_stderr_to_stdout = False
         self.env = ''
         self.os = ''
-        self.stdouts = ''
-        self.stderrs = ''
         self.abs_path = True
         self.set_special_env = True
         self.nb_proc = -1 # < 0   mean unlimited
@@ -1220,19 +1234,11 @@
             condor_dag_fd.write('VARS %d stdout="%s"\n'%(id,stdout_file))
             condor_dag_fd.write('VARS %d stderr="%s"\n\n'%(id,stderr_file))
 
-        if self.base_tasks_log_file:
-            for i in range(len(self.tasks)):
-                task=self.tasks[i]
-                s=os.path.join(self.log_dir,self.base_tasks_log_file[i])
-                print_task(i,task,s+".out",s+".err")
-        elif self.stdouts and self.stderrs:
-            assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
-            for (i,task,stdout_file,stderr_file) in zip(range(len(self.tasks)),self.tasks,self.stdouts,self.stderrs):
-                print_task(i,task,stdout_file,stderr_file)
-        else:
-            #should not happen
-            raise NotImplementedError()
-                
+            
+        for i in range(len(self.tasks)):
+            task=self.tasks[i]
+            print_task(i,task,*self.get_file_redirection(task.id))
+
         condor_dag_fd.close()
 
         self.make_launch_script('$@')
@@ -1292,19 +1298,13 @@
                     condor_submit_fd.write("error        = %s \nqueue\n" %stderr_file)
                 if req:
                     condor_submit_fd.write("requirements   = %s\n"%(req))
-            if self.base_tasks_log_file:
-                for (task,task_log,req) in zip(self.tasks,self.base_tasks_log_file,
-                                               self.tasks_req):
-                    s=os.path.join(self.log_dir,task_log)
-                    print_task(task,s+".out",s+".err",req)
-            elif self.stdouts and self.stderrs:
-                assert len(self.stdouts)==len(self.stderrs)==len(self.tasks)
-                for (task,stdout_file,stderr_file,req) in zip(self.tasks,self.stdouts,
-                                                              self.stderrs,self.tasks_req):
-                    print_task(task,stdout_file,stderr_file)
-            else:
-                for (task,req) in zip(self.tasks,self.tasks_req):
-                    print_task(task, "", "", req)
+
+            for i in range(len(self.tasks)):
+                task=self.tasks[i]
+                req=self.tasks_req[i]
+                (o,e)=self.get_file_redirection(task.id)
+                print_task(task,o,e,req)
+
         condor_submit_fd.close()
 
         self.make_launch_script('sh -c "$@"')
@@ -1425,7 +1425,6 @@
             
         #launch the jobs
         if self.test == False:
-            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
             print "[DBI] Executing: " + cmd
             for task in self.tasks:
                 task.set_scheduled_time()
@@ -1513,7 +1512,7 @@
             print "[DBI] "+c
             return
 
-        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
+        (output,error)=self.get_redirection(*self.get_file_redirection(task.id))
 
         self.started+=1
         print "[DBI,%d/%d,%s] %s"%(self.started,len(self.tasks),time.ctime(),c)
@@ -1729,9 +1728,8 @@
 
         task.launch_time = time.time()
         task.set_scheduled_time()
+        (output,error)=self.get_redirection(*self.get_file_redirection(task.id))
 
-        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
-
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
         task.p.wait()
         task.status=STATUS_FINISHED
@@ -1753,7 +1751,6 @@
             task.launch_time = time.time()
             task.set_scheduled_time()
 
-###            (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
 
             task.p = Popen(command, shell=True,stdout=PIPE,stderr=PIPE)
             wait = task.p.wait()

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-20 21:32:40 UTC (rev 10134)
+++ trunk/scripts/dbidispatch	2009-04-21 14:23:38 UTC (rev 10135)
@@ -8,14 +8,14 @@
 ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
 
 <back-end parameter>:
+    all                      :[--tasks_filename={compact,explicit,nb0,nb1,
+                                                 sh(condor only),
+                                                 clusterid(condor only),
+                                                 processid(condor only))}+]
     bqtools, cluster, condor option  : [--mem=N]
                               [--cpu=nb_cpu_per_node]
     bqtools, cluster option  :[--duree=X]
-    bqtools, condor options  :[--tasks_filename={compact,explicit,nb0,nb1,
-                                                 sh(condor only),
-                                                 clusterid(condor only),
-                                                 processid(condor only))}+]
-                              [--raw=STRING[\nSTRING]]
+    bqtools, condor options  :[--raw=STRING[\nSTRING]]
                               [*--[no_]set_special_env]
                               [--env=VAR=VALUE[;VAR2=VALUE2]]
     cluster, condor options  :[--32|--64|--3264] [--os=X]
@@ -83,19 +83,6 @@
     line that was executed, all other options are are those passed to 
     dbidispatch this time. Work only with jobs launched with dbidispatch.
   The '--only_n_first=N' option tell to launch only the first N jobs from the list.
-
-bqtools, cluster and condor option:
-  The '--mem=X' speficify the number of ram in meg the program need to execute.
-  The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
-    will be reserved for each job.
-
-bqtools and cluster option:
-  The '--duree' option specifies the maximum duration of the jobs. The syntax 
-    depends on the back-end. For the cluster syntax, see 'cluster --help'. 
-    For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 
-    13 minutes and 15 seconds.
-
-bqtools and condor options:
   The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the
     filename where the stdout, stderr are redirected. We can put many option 
     separated by comma. They will apper in the filename in order separated by a 
@@ -105,12 +92,25 @@
       - explicit: a unic string that represent the full command to execute
       - nb0     : a number from 0 to nb job -1.
       - nb1     : a number from 1 to nb job.
-      - sh      : (condor only)parse the command for > and 2> redirection command.
+      - sh      : parse the command for > and 2> redirection command.
                   If one or both of them are missing, they are redirected
                   to /dev/null
       - clusterid: (condor only)put the cluster id of the jobs.
       - processid: (condor only)put the process id of the jobs. Idem as nb0
       - none    : remove all preceding pattern
+
+bqtools, cluster and condor option:
+  The '--mem=X' speficify the number of ram in meg the program need to execute.
+  The '--cpu=nb_cpu_per_node' option determine the number of cpu(cores) that 
+    will be reserved for each job.
+
+bqtools and cluster option:
+  The '--duree' option specifies the maximum duration of the jobs. The syntax 
+    depends on the back-end. For the cluster syntax, see 'cluster --help'. 
+    For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 
+    13 minutes and 15 seconds.
+
+bqtools and condor options:
   The '--raw=STRING1[\nSTRING2...]' option append all STRINGX in the submit file.
       if this option appread many time, they will be concatanated with a new line.
   The '--[no_]set_special_env' option will set the varialbe OMP_NUM_THREADS, 
@@ -364,8 +364,7 @@
         break
     command_argv.remove(argv)
 
-if launch_cmd in ["Bqtools","Condor"]:
-    dbi_param.setdefault("tasks_filename", ["nb0","compact"])
+dbi_param.setdefault("tasks_filename", ["nb0","compact"])
 
 if len(command_argv) == 0 and not dbi_param.has_key("file"):
     print "No command or file with command to execute!"
@@ -381,7 +380,8 @@
 if not os.path.exists(LOGDIR):
     os.mkdir(LOGDIR)
 
-valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir", "file"]
+valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir", "file",
+                 "tasks_filename"]
 if launch_cmd=="Cluster":
     valid_dbi_param +=["cwait","force","arch","interruptible",
                        "duree","cpu","mem","os"]
@@ -389,13 +389,13 @@
     valid_dbi_param +=["req", "arch", "getenv", "nice", "files", "rank", "env",
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "no_machine","to_all",
-                       "keep_failed_jobs_in_queue", "tasks_filename", "restart",
+                       "keep_failed_jobs_in_queue", "restart",
                        "max_file_size", "debug", "local_log_file"
                        ]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
                        "nano", "queue", "raw", "submit_options", "jobs_name",
-                       "tasks_filename", "set_special_env", "env" ]
+                       "set_special_env", "env" ]
 
 if  launch_cmd == 'Condor' and gethostname().endswith(".iro.umontreal.ca"):
     #default value for pkdilly is true.
@@ -571,10 +571,16 @@
         pass
     elif pattern == "none":
         dbi_param[n]=[""]*len(commands)
-    elif pattern == "clusterid":#$(Cluster)
-        dbi_param[n]=merge_pattern(["$(Cluster)"]*len(dbi_param[n]))
+    elif pattern == "clusterid":#$(Cluster)        
+        if launch_cmd=="Condor":
+            dbi_param[n]=merge_pattern(["$(Cluster)"]*len(dbi_param[n]))
+        else:
+            print "Warning the option tasks_filename=clusterid is only valid for condor"
     elif pattern == "processid":#$(Process)
-        dbi_param[n]=merge_pattern(["$(Process)"]*len(dbi_param[n]))
+        if launch_cmd=="Condor":
+            dbi_param[n]=merge_pattern(["$(Process)"]*len(dbi_param[n]))
+        else:
+            print "Warning the option tasks_filename=processid is only valid for condor"
     elif pattern == "sh":
         stdouts=[]
         stderrs=[]



From nouiz at mail.berlios.de  Tue Apr 21 16:31:58 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Apr 2009 16:31:58 +0200
Subject: [Plearn-commits] r10136 - trunk/python_modules/plearn/parallel
Message-ID: <200904211431.n3LEVwFp032132@sheep.berlios.de>

Author: nouiz
Date: 2009-04-21 16:31:58 +0200 (Tue, 21 Apr 2009)
New Revision: 10136

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
bugfix last commit for bqtools


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:23:38 UTC (rev 10135)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:31:58 UTC (rev 10136)
@@ -691,10 +691,9 @@
         for task in self.tasks:
             #-4 as we will happend .err or .out
             base=self.get_file_redirection(task.id)[0][:-4]
-            p=os.path.join(self.log_dir,base)
-            self.check_path(p)
+            self.check_path(base)
             tasks_file.write( ';'.join(task.commands) + '\n' )
-            logfiles_file.write( p + '\n' )
+            logfiles_file.write( base + '\n' )
 
         tasks_file.close()
         logfiles_file.close()



From nouiz at mail.berlios.de  Tue Apr 21 16:40:32 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Apr 2009 16:40:32 +0200
Subject: [Plearn-commits] r10137 - trunk/python_modules/plearn/parallel
Message-ID: <200904211440.n3LEeWQM000658@sheep.berlios.de>

Author: nouiz
Date: 2009-04-21 16:40:32 +0200 (Tue, 21 Apr 2009)
New Revision: 10137

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
fix the return value on condor with bash


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:31:58 UTC (rev 10136)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:40:32 UTC (rev 10137)
@@ -1132,8 +1132,11 @@
                     pwd 1>&2
                     echo "nb args: $#" 1>&2
                     echo "Running: command: \\"$@\\"" 1>&2
-                    %s
+                    `%s`
+                    ret=$?
                     rm -f echo ${KRB5CCNAME:5}
+                    echo "return value ${ret}"
+                    exit ${ret}
                     '''%(bash_exec)))
             else:
                 fd.write(dedent('''\



From nouiz at mail.berlios.de  Tue Apr 21 16:50:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Apr 2009 16:50:45 +0200
Subject: [Plearn-commits] r10138 - trunk/python_modules/plearn/parallel
Message-ID: <200904211450.n3LEojok002368@sheep.berlios.de>

Author: nouiz
Date: 2009-04-21 16:50:45 +0200 (Tue, 21 Apr 2009)
New Revision: 10138

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
fixed the return value for condor under csh(tcsh)


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:40:32 UTC (rev 10137)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-21 14:50:45 UTC (rev 10138)
@@ -1162,7 +1162,10 @@
                 pwd
                 echo "Running command: $argv"
                 $argv
+                set ret=$?
                 rm -f `echo  $KRB5CCNAME| cut -d':' -f2`
+                echo "return value ${ret}"
+                exit ${ret}
                 '''))
             fd.close()
             if self.pkdilly:



From nouiz at mail.berlios.de  Tue Apr 21 17:22:38 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Apr 2009 17:22:38 +0200
Subject: [Plearn-commits] r10139 - trunk/plearn/base
Message-ID: <200904211522.n3LFMciE007337@sheep.berlios.de>

Author: nouiz
Date: 2009-04-21 17:22:37 +0200 (Tue, 21 Apr 2009)
New Revision: 10139

Modified:
   trunk/plearn/base/CopiesMap.h
Log:
print the name of the type that don't have deepCopy implemented.


Modified: trunk/plearn/base/CopiesMap.h
===================================================================
--- trunk/plearn/base/CopiesMap.h	2009-04-21 14:50:45 UTC (rev 10138)
+++ trunk/plearn/base/CopiesMap.h	2009-04-21 15:22:37 UTC (rev 10139)
@@ -222,8 +222,10 @@
 {
     /*! no op */
     PLWARNING(
-        "In CopiesMap.h - deepCopyField not handled for this type. "
-        "If it actually doesn't need deep copy, edit CopiesMap.h and add NODEEPCOPY(your_type) to remove this warning."
+        "In CopiesMap.h - deepCopyField not handled for the type '%s'. "
+        "If it actually doesn't need deep copy, edit CopiesMap.h and add"
+        " NODEEPCOPY(your_type) to remove this warning.",
+        TypeTraits<T>().name().c_str()
         );
 }
 



From nouiz at mail.berlios.de  Tue Apr 21 21:01:35 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Apr 2009 21:01:35 +0200
Subject: [Plearn-commits] r10140 - trunk/plearn_learners/meta
Message-ID: <200904211901.n3LJ1ZBw006960@sheep.berlios.de>

Author: nouiz
Date: 2009-04-21 21:01:31 +0200 (Tue, 21 Apr 2009)
New Revision: 10140

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
added profiling of train+test


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-04-21 15:22:37 UTC (rev 10139)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2009-04-21 19:01:31 UTC (rev 10140)
@@ -290,7 +290,7 @@
     EXTREME_MODULE_LOG<<"train() start"<<endl;
     timer->startTimer("MultiClassAdaBoost::train");
     Profiler::pl_profile_start("MultiClassAdaBoost::train");
-
+    Profiler::pl_profile_start("MultiClassAdaBoost::train+test");
     learner1->nstages = nstages;
     learner2->nstages = nstages;
 
@@ -331,7 +331,8 @@
         train_stats->append(*(v),"sublearner2.");
     timer->stopTimer("MultiClassAdaBoost::train");
     Profiler::pl_profile_end("MultiClassAdaBoost::train");
-    
+    Profiler::pl_profile_end("MultiClassAdaBoost::train+test");
+
     real tmp = timer->getTimer("MultiClassAdaBoost::train");
     train_time=tmp - total_train_time;
     total_train_time=tmp;
@@ -662,10 +663,14 @@
                               VMat testoutputs, VMat testcosts) const
 {
     Profiler::pl_profile_start("MultiClassAdaBoost::test()");
+    Profiler::pl_profile_start("MultiClassAdaBoost::train+test");
+
     timer->startTimer("MultiClassAdaBoost::test()");
     if(!forward_test){
          inherited::test(testset,test_stats,testoutputs,testcosts);
          Profiler::pl_profile_end("MultiClassAdaBoost::test()");
+         Profiler::pl_profile_end("MultiClassAdaBoost::train+test");
+
          timer->stopTimer("MultiClassAdaBoost::test()");
          return;
     }
@@ -690,6 +695,8 @@
         timer->stopTimer("MultiClassAdaBoost::test() current");
         timer->stopTimer("MultiClassAdaBoost::test()");
         Profiler::pl_profile_end("MultiClassAdaBoost::test()");
+        Profiler::pl_profile_end("MultiClassAdaBoost::train+test");
+
         time_sum += timer->getTimer("MultiClassAdaBoost::test() current");
         last_stage=stage;
         nb_sequential_ft = 0;
@@ -848,7 +855,8 @@
     timer->stopTimer("MultiClassAdaBoost::test() current");
     timer->stopTimer("MultiClassAdaBoost::test()");
     Profiler::pl_profile_end("MultiClassAdaBoost::test()");
-    
+    Profiler::pl_profile_end("MultiClassAdaBoost::train+test");
+
     time_sum_ft +=timer->getTimer("MultiClassAdaBoost::test() current");
 
 



From nouiz at mail.berlios.de  Wed Apr 22 17:51:24 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 22 Apr 2009 17:51:24 +0200
Subject: [Plearn-commits] r10141 - trunk/commands/PLearnCommands
Message-ID: <200904221551.n3MFpOfP025207@sheep.berlios.de>

Author: nouiz
Date: 2009-04-22 17:51:24 +0200 (Wed, 22 Apr 2009)
New Revision: 10141

Modified:
   trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
Log:
add to the read_and_write command an --update option


Modified: trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-04-21 19:01:31 UTC (rev 10140)
+++ trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-04-22 15:51:24 UTC (rev 10141)
@@ -56,11 +56,12 @@
                 
                   "Used to check (debug) the serialization system",
                 
-                  "read_and_write <sourcefile> <destfile> [modification string] ...\n"
+                  "read_and_write <sourcefile> <destfile> [--updaet] [modification string] ...\n"
                   "Reads an Object (in PLearn serialization format) from the <sourcefile> and writes it to the <destfile>\n"
                   "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
                   "Otherwise (ex: .plearn .vmat) it will. \n"
                   "If their is modification string in format option=value, the modification will be made to the object before saving\n"
+                  "The --update option make that we generate the file only if we can calculate the modification time of the sourcefile and it is older then the destfile."
         )
 {}
 
@@ -68,30 +69,42 @@
 void ReadAndWriteCommand::run(const vector<string>& args)
 {
     if(args.size()<2)
-        PLERROR("read_and_write takes 2 or more arguments: <sourcefile> <destfile> [modification string] ...");
+        PLERROR("read_and_write takes 2 or more arguments: <sourcefile> <destfile> [--update] [modification string] ...");
     string source = args[0];
     string dest = args[1];
 
     string ext = extract_extension(source);
     PP<Object> o;
+    time_t date_src=0;
 
     //read the file
     if(ext==".psave") // may be binay. Don't macro-process
     {
         PLearn::load(source,o);
+        date_src=mtime(source);
     }
     else
     {
         map<string, string> vars;
-        string script = readFileAndMacroProcess(source, vars);
+        string script = readFileAndMacroProcess(source, vars, date_src);
         PStream in = openString(script,PStream::plearn_ascii);
         o = readObject(in);
     }
+    int idx_start=2;
+    if(args.size()>2 && args[2]=="--update"){
+        PLCHECK(date_src>0);
+        idx_start++;
+        time_t date_dst=mtime(dest);
+        if((date_dst>date_src) && (date_src>0)){
+            pout << "The file is up to date. We don't regenerate it."<<endl;
+            return;
+        }
+    }
 
     //modif the object
     string left;
     string right;
-    for(uint i=2; i<args.size();i++){
+    for(uint i=idx_start; i<args.size();i++){
         split_on_first(args[i], "=", left, right);
         o->setOption(left, right);
     }



From nouiz at mail.berlios.de  Thu Apr 23 18:20:06 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 23 Apr 2009 18:20:06 +0200
Subject: [Plearn-commits] r10142 - trunk/plearn_learners/regressors
Message-ID: <200904231620.n3NGK6ns007756@sheep.berlios.de>

Author: nouiz
Date: 2009-04-23 18:20:05 +0200 (Thu, 23 Apr 2009)
New Revision: 10142

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
fixed assert for missing value.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-22 15:51:24 UTC (rev 10141)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-23 16:20:05 UTC (rev 10142)
@@ -255,7 +255,7 @@
     int prev_row=preg[row_idx_end];
     real prev_val=p[prev_row];
     PLASSERT(reg.size()>row_idx_end && row_idx_end>=0);
-    PLASSERT(p[prev_row]==tsource(col,prev_row));
+    PLASSERT(is_equal(p[prev_row],tsource(col,prev_row)));
 
     for( ;row_idx_end>0;row_idx_end--)
     {
@@ -270,7 +270,7 @@
 
         PLASSERT(reg.size()>row_idx_end && row_idx_end>0);
         PLASSERT(target_weight.size()>row && row>=0);
-        PLASSERT(p[row]==tsource(col,row));
+        PLASSERT(is_equal(p[row],tsource(col,row)));
         RTR_target_t target = ptw[row].first;
         RTR_weight_t weight = ptw[row].second;
 
@@ -288,7 +288,7 @@
         PLASSERT(reg.size()>idx && idx>=0);
         int row=int(preg[idx]);
         PLASSERT(target_weight.size()>row && row>=0);
-        PLASSERT(p[row]==tsource(col,row));
+        PLASSERT(is_equal(p[row],tsource(col,row)));
         pv[idx]=p[row];
     }
         for(int row_idx = 0;row_idx<=row_idx_end;row_idx++)
@@ -301,7 +301,7 @@
             int row=int(preg[row_idx]);
             real val=p[row];
             PLASSERT(target_weight.size()>row && row>=0);
-            PLASSERT(p[row]==tsource(col,row));
+            PLASSERT(is_equal(p[row],tsource(col,row)));
             ptwd[row_idx].first=ptw[row].first;
             ptwd[row_idx].second=ptw[row].second;
             pv[row_idx]=val;
@@ -657,7 +657,7 @@
         real v1 = tsource(dim,reg1);
         real v2 = tsource(dim,reg2);
 //check that the sort is valid.
-        PLASSERT(v1<=v2);
+        PLASSERT(v1<=v2 || is_missing(v2));
 //check that the sort is stable
         if(v1==v2 && reg1>reg2)
             PLWARNING("In RegressionTreeRegisters::sortEachDim(%d) - "



From nouiz at mail.berlios.de  Thu Apr 23 19:00:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 23 Apr 2009 19:00:29 +0200
Subject: [Plearn-commits] r10143 - trunk/plearn/math
Message-ID: <200904231700.n3NH0TnZ000158@sheep.berlios.de>

Author: nouiz
Date: 2009-04-23 19:00:27 +0200 (Thu, 23 Apr 2009)
New Revision: 10143

Modified:
   trunk/plearn/math/TVec_decl.h
Log:
Added comment to tell that TVec->findSorted(x) return the index of the first time the value appear if present multiple fime. If the value is not present, return the first index with data bigger then the value.

I check all use in PLearn of this fct and they use it correctly. 


Modified: trunk/plearn/math/TVec_decl.h
===================================================================
--- trunk/plearn/math/TVec_decl.h	2009-04-23 16:20:05 UTC (rev 10142)
+++ trunk/plearn/math/TVec_decl.h	2009-04-23 17:00:27 UTC (rev 10143)
@@ -538,6 +538,9 @@
     //! of this vec in ascending order. If stable is true, will return a stable sort
     TVec<int> sortingPermutation(bool stable = false, bool missing = false) const;
     
+    //! Return the first index where the value COULD be.
+    //! If mulitiple value present, return the first index
+    //! If the value is not present, return the first index with data bigger then the value.
     int findSorted(T value) const
     {
         if (isEmpty())



From nouiz at mail.berlios.de  Thu Apr 23 20:18:09 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 23 Apr 2009 20:18:09 +0200
Subject: [Plearn-commits] r10144 - trunk/scripts
Message-ID: <200904231818.n3NII9PR030304@sheep.berlios.de>

Author: nouiz
Date: 2009-04-23 20:18:09 +0200 (Thu, 23 Apr 2009)
New Revision: 10144

Modified:
   trunk/scripts/dbidispatch
Log:
accept to restart jobs if they are in the queue if they are completed.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-23 17:00:27 UTC (rev 10143)
+++ trunk/scripts/dbidispatch	2009-04-23 18:18:09 UTC (rev 10144)
@@ -494,20 +494,26 @@
     assert launch_cmd=="Condor"
     cmds=[]
     for arg in command_argv:
-        p1=Popen("condor_q -l "+arg, shell=True, stdout=PIPE)
+        #We accept to start jobs in the queue if they are completed
+        p1=Popen('condor_q -l -const "JobStatus!=4" '+arg, shell=True, stdout=PIPE)
         p2=Popen("condor_history -l "+arg, shell=True, stdout=PIPE)
-        p1.wait();p2.wait()
+        p3=Popen('condor_q -l -const "JobStatus==4" '+arg, shell=True, stdout=PIPE)
+        p1.wait();p2.wait();p3.wait()
         lines=p1.stdout.readlines()
         for l in lines:
             if l.startswith("Arguments = "):
-                print "We don't accept to restart jobs in the queue:", arg
+                print "We don't accept to restart jobs in the queue that are not completed:", arg
                 sys.exit(1)
+        print
         lines=p2.stdout.readlines()
+        lines+=p3.stdout.readlines()
         for l in lines:
             if l.startswith("Arguments = "):
                 cmd=l[13:-2]
                 cmds.append(cmd.replace("'",""))
     commands=cmds
+    if len(commands)<1:
+         raise Exception("Their is no commands selected to be restarted!")
     choise_args=[]
 else:
     (commands,choise_args)=generate_commands(command_argv)



From nouiz at mail.berlios.de  Thu Apr 23 22:52:01 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 23 Apr 2009 22:52:01 +0200
Subject: [Plearn-commits] r10145 - trunk/plearn_learners/regressors
Message-ID: <200904232052.n3NKq1eO017485@sheep.berlios.de>

Author: nouiz
Date: 2009-04-23 22:52:00 +0200 (Thu, 23 Apr 2009)
New Revision: 10145

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
various fix for missing value.


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-23 18:18:09 UTC (rev 10144)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2009-04-23 20:52:00 UTC (rev 10145)
@@ -343,7 +343,11 @@
                                                 missing_leave,
                                                 left_leave,
                                                 right_leave, candidate);
-            PLASSERT(candidate.size()>0);
+            PLASSERT(registered_target_weight.size()==candidate.size());
+            PLASSERT(registered_value.size()==candidate.size());
+            PLASSERT(left_leave->length()+right_leave->length()
+                     +missing_leave->length()==leave->length());
+            PLASSERT(candidate.size()>0||(left_leave->length()+right_leave->length()==0));
             missing_leave->getOutputAndError(tmp_vec, missing_error);
             ret=bestSplitInRow(col, candidate, left_error,
                                right_error, missing_error,
@@ -360,21 +364,30 @@
 #endif
 
         if(col==0){
-            l_length=left_leave->length()+right_leave->length();
-            l_weights_sum=left_leave->weights_sum+right_leave->weights_sum;
-            l_targets_sum=left_leave->targets_sum+right_leave->targets_sum;
-            l_weighted_targets_sum=left_leave->weighted_targets_sum+right_leave->weighted_targets_sum;
-            l_weighted_squared_targets_sum=left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum;
+            l_length=left_leave->length()+right_leave->length()+missing_leave->length();
+            l_weights_sum=left_leave->weights_sum+right_leave->weights_sum+missing_leave->weights_sum;
+            l_targets_sum=left_leave->targets_sum+right_leave->targets_sum+missing_leave->targets_sum;
+            l_weighted_targets_sum=left_leave->weighted_targets_sum
+                +right_leave->weighted_targets_sum+missing_leave->weighted_targets_sum;
+            l_weighted_squared_targets_sum=left_leave->weighted_squared_targets_sum
+                +right_leave->weighted_squared_targets_sum+missing_leave->weighted_squared_targets_sum;
         }else if(!one_pass_on_data){
-            PLCHECK(l_length==left_leave->length()+right_leave->length());
+            PLCHECK(l_length==left_leave->length()+right_leave->length()
+                    +missing_leave->length());
             PLCHECK(fast_is_equal(l_weights_sum,
-                                  left_leave->weights_sum+right_leave->weights_sum));
+                                  left_leave->weights_sum+right_leave->weights_sum
+                                  +missing_leave->weights_sum));
             PLCHECK(fast_is_equal(l_targets_sum,
-                                  left_leave->targets_sum+right_leave->targets_sum));
+                                  left_leave->targets_sum+right_leave->targets_sum
+                                  +missing_leave->targets_sum));
             PLCHECK(fast_is_equal(l_weighted_targets_sum,
-                                  left_leave->weighted_targets_sum+right_leave->weighted_targets_sum));
+                                  left_leave->weighted_targets_sum
+                                  +right_leave->weighted_targets_sum
+                                  +missing_leave->weighted_targets_sum));
             PLCHECK(fast_is_equal(l_weighted_squared_targets_sum,
-                                  left_leave->weighted_squared_targets_sum+right_leave->weighted_squared_targets_sum));
+                                  left_leave->weighted_squared_targets_sum
+                                  +right_leave->weighted_squared_targets_sum
+                                  +missing_leave->weighted_squared_targets_sum));
         }
 
 #ifdef RCMP
@@ -436,7 +449,8 @@
     {
         int next_row = candidates[i];
         real row_feature=next_feature;
-        PLASSERT(row_feature==values[i+1]);
+        PLASSERT(is_equal(row_feature,values[i+1]));
+//                 ||(is_missing(row_feature)&&is_missing(values[i+1])));
         next_feature=values[i];
 
         real target=t_w[i+1].first;

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-23 18:18:09 UTC (rev 10144)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2009-04-23 20:52:00 UTC (rev 10145)
@@ -212,6 +212,25 @@
             target_weight[i].first=source->get(i,inputsize());
             target_weight[i].second=source->get(i,inputsize()+targetsize());
         }
+#if 0
+    //usefull to weight the dataset to have the sum of weight==1 or ==length()
+    real weights_sum=0;
+    for(int i=0;i<source->length();i++){
+        weights_sum+=target_weight[i].second;
+    }
+    pout<<weights_sum<<endl;
+//    real t=length()/weights_sum;
+    real t=1/weights_sum;
+    for(int i=0;i<source->length();i++){
+        target_weight[i].second*=t;
+    }
+    weights_sum=0;
+    for(int i=0;i<source->length();i++){
+        weights_sum+=target_weight[i].second;
+    }
+    pout<<weights_sum<<endl;
+#endif
+
     leave_register.resize(length());
     sortRows();
 //    compact_reg.resize(length());
@@ -291,30 +310,32 @@
         PLASSERT(is_equal(p[row],tsource(col,row)));
         pv[idx]=p[row];
     }
-        for(int row_idx = 0;row_idx<=row_idx_end;row_idx++)
-        {
-            int futur_row = preg[row_idx+8];
-            __builtin_prefetch(&ptw[futur_row],1,2);
-            __builtin_prefetch(&p[futur_row],1,2);
+    for(int row_idx = 0;row_idx<=row_idx_end;row_idx++)
+    {
+        int futur_row = preg[row_idx+8];
+        __builtin_prefetch(&ptw[futur_row],1,2);
+        __builtin_prefetch(&p[futur_row],1,2);
             
-            PLASSERT(reg.size()>row_idx && row_idx>=0);
-            int row=int(preg[row_idx]);
-            real val=p[row];
-            PLASSERT(target_weight.size()>row && row>=0);
-            PLASSERT(is_equal(p[row],tsource(col,row)));
+        PLASSERT(reg.size()>row_idx && row_idx>=0);
+        int row=int(preg[row_idx]);
+        real val=p[row];
+        PLASSERT(target_weight.size()>row && row>=0);
+        PLASSERT(is_equal(p[row],tsource(col,row)));
+            
+        RTR_target_t target = ptw[row].first;
+        RTR_weight_t weight = ptw[row].second;
+        if (RTR_HAVE_MISSING && is_missing(val)){
+            missing_leave->addRow(row, target, weight);
+        }else {
+            left_leave->addRow(row, target, weight);
+            candidate.append(row);
             ptwd[row_idx].first=ptw[row].first;
             ptwd[row_idx].second=ptw[row].second;
             pv[row_idx]=val;
-            
-            RTR_target_t target = ptw[row].first;
-            RTR_weight_t weight = ptw[row].second;
-            if (RTR_HAVE_MISSING && is_missing(val)){
-                missing_leave->addRow(row, target, weight);
-            }else {
-                left_leave->addRow(row, target, weight);
-                candidate.append(row);
-            }
         }
+    }
+    t_w.resize(candidate.size());
+    value.resize(candidate.size());
 }
 
 void RegressionTreeRegisters::getAllRegisteredRow(RTR_type_id leave_id, int col,



From ducharme at mail.berlios.de  Fri Apr 24 18:15:50 2009
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 24 Apr 2009 18:15:50 +0200
Subject: [Plearn-commits] r10146 - trunk/plearn_learners/regressors
Message-ID: <200904241615.n3OGFoWV000973@sheep.berlios.de>

Author: ducharme
Date: 2009-04-24 18:15:50 +0200 (Fri, 24 Apr 2009)
New Revision: 10146

Modified:
   trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
   trunk/plearn_learners/regressors/BasisSelectionRegressor.h
Log:
setTrainingSet est maintenant appelle sur le learner sous-jacent


Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2009-04-23 20:52:00 UTC (rev 10145)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.cc	2009-04-24 16:15:50 UTC (rev 10146)
@@ -1321,7 +1321,13 @@
     return template_learner->getTrainCostNames();
 }
 
+void BasisSelectionRegressor::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set, call_forget);
+    template_learner->setTrainingSet(training_set, call_forget);
+}
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/BasisSelectionRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2009-04-23 20:52:00 UTC (rev 10145)
+++ trunk/plearn_learners/regressors/BasisSelectionRegressor.h	2009-04-24 16:15:50 UTC (rev 10146)
@@ -143,7 +143,10 @@
     //! and  for which it updates the VecStatsCollector train_stats.
     virtual TVec<std::string> getTrainCostNames() const;
 
+    //! Forwards the call to sub-learner
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
 
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From larocheh at mail.berlios.de  Sun Apr 26 15:26:36 2009
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sun, 26 Apr 2009 15:26:36 +0200
Subject: [Plearn-commits] r10147 - trunk/plearn_learners/online
Message-ID: <200904261326.n3QDQarG022673@sheep.berlios.de>

Author: larocheh
Date: 2009-04-26 15:26:35 +0200 (Sun, 26 Apr 2009)
New Revision: 10147

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Removed computeOutputWithoutCorrelationConnections functions...


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-24 16:15:50 UTC (rev 10146)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-26 13:26:35 UTC (rev 10147)
@@ -363,22 +363,6 @@
     // Insert a backpointer to remote methods; note that this is different from
     // declareOptions().
     rmm.inherited(inherited::_getRemoteMethodMap_());
-
-    declareMethod(
-        rmm, "computeOutputWithoutCorrelationConnections",
-        &StackedAutoassociatorsNet::remote_computeOutputWithoutCorrelationConnections,
-        (BodyDoc("On a trained learner, this computes the output from the input without using the correlation_connections"),
-         ArgDoc ("input", "Input vector (should have width inputsize)"),
-         RetDoc ("Computed output (will have width outputsize)")));
-
-    declareMethod(
-        rmm, "computeOutputsWithoutCorrelationConnections",
-        &StackedAutoassociatorsNet::remote_computeOutputsWithoutCorrelationConnections,
-        (BodyDoc("On a trained learner, this computes the outputs from the inputs without using the correlation_connections"),
-         ArgDoc ("input", "Input matrix (should have width inputsize)"),
-         RetDoc ("Computed outputs (will have width outputsize)")));
-
-
 }
 
 void StackedAutoassociatorsNet::build_()
@@ -2647,47 +2631,7 @@
         Profiler::pl_profile_end("StackedAutoassociatorsNet::computeOutputsAndCosts");
     }
 }
-void StackedAutoassociatorsNet::computeOutputWithoutCorrelationConnections(const Vec& input, Vec& output) const
-{
-    // fprop
 
-    expectations[0] << input;
-
-    for(int i=0 ; i<currently_trained_layer-1 ; i++ )
-    {
-        connections[i]->fprop( expectations[i], activations[i+1] );
-        layers[i+1]->fprop(activations[i+1],expectations[i+1]);
-    }
-
-    if( currently_trained_layer<n_layers )
-    {
-        connections[currently_trained_layer-1]->fprop(
-            expectations[currently_trained_layer-1],
-            activations[currently_trained_layer] );
-        layers[currently_trained_layer]->fprop(
-            activations[currently_trained_layer],
-            output);
-    }
-    else
-        final_module->fprop( expectations[ currently_trained_layer - 1],
-                             output );
-}
-
-void StackedAutoassociatorsNet::computeOutputsWithoutCorrelationConnections(const Mat& inputs, Mat& outputs) const
-{
-
-    int n=inputs.length();
-    PLASSERT(outputs.length()==n);
-    for (int i=0;i<n;i++)
-    {
-        Vec in_i = inputs(i);
-        Vec out_i = outputs(i);
-        computeOutputWithoutCorrelationConnections(in_i,out_i);
-    }
-
-}
-
-
 void StackedAutoassociatorsNet::computeCostsFromOutputs(const Vec& input, const Vec& output,
                                            const Vec& target, Vec& costs) const
 {
@@ -2874,23 +2818,6 @@
     final_module->setLearningRate( the_learning_rate );
 }
 
-//! Version of computeOutputWithoutCorrelationConnections(Vec,Vec) that returns a result by value
-Vec StackedAutoassociatorsNet::remote_computeOutputWithoutCorrelationConnections(const Vec& input) const
-{
-    tmp_output.resize(outputsize());
-    computeOutputWithoutCorrelationConnections(input, tmp_output);
-    return tmp_output;
-}
-
-//! Version of computeOutputsWithoutCorrelationConnections(Mat,Mat) that returns a result by value
-Mat StackedAutoassociatorsNet::remote_computeOutputsWithoutCorrelationConnections(const Mat& inputs) const
-{
-    tmp_output_mat.resize(inputs.length(),outputsize());
-    computeOutputsWithoutCorrelationConnections(inputs, tmp_output_mat);
-    return tmp_output_mat;
-}
-
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-04-24 16:15:50 UTC (rev 10146)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-04-26 13:26:35 UTC (rev 10147)
@@ -235,13 +235,6 @@
     virtual void train();
 
     //! Computes the output from the input.
-    virtual void computeOutputWithoutCorrelationConnections(const Vec& input,
-                                                        Vec& output) const;
-    //! Computes the output from the input
-    virtual void computeOutputsWithoutCorrelationConnections(const Mat& input,
-                                                        Mat& output) const;
-
-    //! Computes the output from the input.
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
     //! Computes the output from the input.



From nouiz at mail.berlios.de  Mon Apr 27 16:03:51 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 27 Apr 2009 16:03:51 +0200
Subject: [Plearn-commits] r10148 - trunk/scripts
Message-ID: <200904271403.n3RE3prg011283@sheep.berlios.de>

Author: nouiz
Date: 2009-04-27 16:03:50 +0200 (Mon, 27 Apr 2009)
New Revision: 10148

Modified:
   trunk/scripts/collectres
Log:
parameter can be in any order.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2009-04-26 13:26:35 UTC (rev 10147)
+++ trunk/scripts/collectres	2009-04-27 14:03:50 UTC (rev 10148)
@@ -292,20 +292,21 @@
   verbose=1
   output_separator = " "
   separator = "_"
-
-  if args[1]=="--no_printcommand":
-    printcommand=False
-    del args[1]
-  if args[1].startswith("--verbose="):
-    verbose=int(args[1].split("=",1)[1])
-    del args[1]
-  if args[1].startswith("--separator=") or args[1].startswith("--sep=") :
-    separator=args[1].split("=",1)[1]
-  if args[1].startswith("--out_sep="):
-    output_separator=args[1].split("=",1)[1]
-    if output_separator == "tab":
-      output_separator = "\t"
-    print "output_separator",output_separator
-    del args[1]
-
+  while True:
+    if args[1]=="--no_printcommand":
+      printcommand=False
+      del args[1]
+    elif args[1].startswith("--verbose="):
+      verbose=int(args[1].split("=",1)[1])
+      del args[1]
+    elif args[1].startswith("--separator=") or args[1].startswith("--sep=") :
+      separator=args[1].split("=",1)[1]
+    elif args[1].startswith("--out_sep="):
+      output_separator=args[1].split("=",1)[1]
+      if output_separator == "tab":
+        output_separator = "\t"
+      print "output_separator: '%s'"%output_separator
+      del args[1]
+    else:
+      break
   collectres(args[1],args[2],args[3:], printcommand, verbose, separator, output_separator)



From tihocan at mail.berlios.de  Mon Apr 27 17:14:49 2009
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 27 Apr 2009 17:14:49 +0200
Subject: [Plearn-commits] r10149 - trunk/commands/PLearnCommands
Message-ID: <200904271514.n3RFEnGU018752@sheep.berlios.de>

Author: tihocan
Date: 2009-04-27 17:14:49 +0200 (Mon, 27 Apr 2009)
New Revision: 10149

Modified:
   trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
Log:
Fixed typo in help

Modified: trunk/commands/PLearnCommands/ReadAndWriteCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-04-27 14:03:50 UTC (rev 10148)
+++ trunk/commands/PLearnCommands/ReadAndWriteCommand.cc	2009-04-27 15:14:49 UTC (rev 10149)
@@ -61,7 +61,7 @@
                   "If the sourcefile ends with a .psave file, then it will not be subjected to macro preprosessing \n"
                   "Otherwise (ex: .plearn .vmat) it will. \n"
                   "If their is modification string in format option=value, the modification will be made to the object before saving\n"
-                  "The --update option make that we generate the file only if we can calculate the modification time of the sourcefile and it is older then the destfile."
+                  "The --update option makes that we generate the file only if we can calculate the modification time of the sourcefile and it is more recent than the destfile."
         )
 {}
 
@@ -96,7 +96,7 @@
         idx_start++;
         time_t date_dst=mtime(dest);
         if((date_dst>date_src) && (date_src>0)){
-            pout << "The file is up to date. We don't regenerate it."<<endl;
+            pout << "The file is up to date. We do not regenerate it."<<endl;
             return;
         }
     }



From nouiz at mail.berlios.de  Mon Apr 27 17:50:45 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 27 Apr 2009 17:50:45 +0200
Subject: [Plearn-commits] r10150 - trunk/scripts
Message-ID: <200904271550.n3RFoj1Q021255@sheep.berlios.de>

Author: nouiz
Date: 2009-04-27 17:50:45 +0200 (Mon, 27 Apr 2009)
New Revision: 10150

Modified:
   trunk/scripts/dbidispatch
Log:
modif to allow longer exp_dir name.
added option [*--[no_]exec_in_exp_dir] that allow to remove the name of the executable in the exp_dir. This allow to have more parameter in the exp_dir name.



Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-27 15:14:49 UTC (rev 10149)
+++ trunk/scripts/dbidispatch	2009-04-27 15:50:45 UTC (rev 10150)
@@ -5,7 +5,7 @@
 from subprocess import Popen,PIPE
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [*--[no_]exec_in_exp_dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
 
 <back-end parameter>:
     all                      :[--tasks_filename={compact,explicit,nb0,nb1,
@@ -98,6 +98,7 @@
       - clusterid: (condor only)put the cluster id of the jobs.
       - processid: (condor only)put the process id of the jobs. Idem as nb0
       - none    : remove all preceding pattern
+  The '*--[no_]exec_in_exp_dir' option remove the executable from the log dir.
 
 bqtools, cluster and condor option:
   The '--mem=X' speficify the number of ram in meg the program need to execute.
@@ -252,7 +253,7 @@
 
 dbi_param={}
 testmode=False
-
+MAX_FILE_NAME_SIZE=255
 PATH=os.getenv('PATH')
 if search_file('condor_submit',PATH):
     launch_cmd = 'Condor'
@@ -301,13 +302,13 @@
                    "--getenv", "--cwait", "--clean_up" ,"--nice",
                    "--set_special_env", "--abs_path", "--pkdilly", "--to_all",
                    "--m32G", "--keep_failed_jobs_in_queue", "--restart",
-                   "--debug", "--local_log_file"]:
+                   "--debug", "--local_log_file", "--exec_in_exp_dir"]:
         dbi_param[argv[2:]]=True
     elif argv in ["--no_force", "--no_interruptible", "--no_long",
                   "--no_getenv", "--no_cwait", "--no_clean_up" , "--no_nice",
                   "--no_set_special_env", "--no_abs_path", "--no_pkdilly",
                   "--no_m32G", "--no_keep_failed_jobs_in_queue", "--no_restart",
-                  "--no_debug", "--no_local_log_file"]:
+                  "--no_debug", "--no_local_log_file", "--no_exec_in_exp_dir"]:
         dbi_param[argv[5:]]=False
     elif argv=="--testdbi":
         dbi_param["test"]=True
@@ -381,7 +382,8 @@
     os.mkdir(LOGDIR)
 
 valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir", "file",
-                 "tasks_filename"]
+                 "tasks_filename", "exec_in_exp_dir"
+                 ]
 if launch_cmd=="Cluster":
     valid_dbi_param +=["cwait","force","arch","interruptible",
                        "duree","cpu","mem","os"]
@@ -390,7 +392,7 @@
                        "raw", "os", "set_special_env", "mem", "cpu", "pkdilly",
                        "universe", "machine", "machines", "no_machine","to_all",
                        "keep_failed_jobs_in_queue", "restart",
-                       "max_file_size", "debug", "local_log_file"
+                       "max_file_size", "debug", "local_log_file",
                        ]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",
@@ -532,7 +534,11 @@
     dbi_param["log_dir"]=os.path.join(LOGDIR,dbi_param["exp_dir"])
 elif not dbi_param.has_key("file"):
     t = [x for x in sys.argv[1:] if not x[:2]=="--"]
-    t[0]=os.path.split(t[0])[1]
+    if dbi_param.get("exec_in_exp_dir",True)==True:
+        t[0]=os.path.split(t[0])[1]#keep only the exec name, not the full path
+    else:
+        t=t[1:]#remove the exec.
+
     tmp="_".join(t)
     tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
     ### We need to remove the symbols "," as this cause trouble with bqtools
@@ -545,12 +551,12 @@
         #bqtools have a limit. It must have a abspath size < max_file_size -16
         #(255 on ext3)
         l=len(os.path.abspath(tmp))
-        l=255-16-len(date_str)-(l-len(tmp))-10 #-10 for dbi.py #-16 for bqtools itself
+        l=MAX_FILE_NAME_SIZE-16-len(date_str)-(l-len(tmp))-10 #-10 for dbi.py #-16 for bqtools itself
         assert(l>0)
         tmp=tmp[:l]
     else:
-        tmp=tmp[:200]
-
+        l=MAX_FILE_NAME_SIZE-len(date_str)-1 #-1 for the '_' before date_str
+        tmp=tmp[:l]
     tmp+='_'+date_str.replace(' ','_')
     dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
 else:



From nouiz at mail.berlios.de  Mon Apr 27 18:08:29 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 27 Apr 2009 18:08:29 +0200
Subject: [Plearn-commits] r10151 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200904271608.n3RG8Tt5022481@sheep.berlios.de>

Author: nouiz
Date: 2009-04-27 18:08:29 +0200 (Mon, 27 Apr 2009)
New Revision: 10151

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-added option --next_job_start_delay for condor.
-small bugfix (changed `%s` for %s)


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2009-04-27 15:50:45 UTC (rev 10150)
+++ trunk/python_modules/plearn/parallel/dbi.py	2009-04-27 16:08:29 UTC (rev 10151)
@@ -824,6 +824,7 @@
         self.max_file_size = 10*1024*1024 #in blocks size, here they are 1k each
         self.debug = False
         self.local_log_file = True#by default true as condor can have randomly failure otherwise.
+        self.next_job_start_delay = -1
 
         DBIBase.__init__(self, commands, **args)
         if self.debug:
@@ -848,6 +849,7 @@
             os.mkdir(self.tmp_dir)
         self.args = args
 
+        self.next_job_start_delay=int(self.next_job_start_delay)
         self.add_commands(commands)
 
     def add_commands(self,commands):
@@ -1129,10 +1131,12 @@
                     echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
                     echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
                     echo "CONDOR_JOB_LOGDIR: $CONDOR_JOB_LOGDIR" 1>&2
+                    echo "HOME: $HOME" 1>&2
                     pwd 1>&2
                     echo "nb args: $#" 1>&2
                     echo "Running: command: \\"$@\\"" 1>&2
-                    `%s`
+                    #[ -x "$1" ];echo $?
+                    %s
                     ret=$?
                     rm -f echo ${KRB5CCNAME:5}
                     echo "return value ${ret}"
@@ -1191,6 +1195,8 @@
             fd.write('arguments      = '+arguments+'\n')
         if self.keep_failed_jobs_in_queue:
             fd.write('leave_in_queue = (ExitCode!=0)\n')
+        if self.next_job_start_delay>0:
+            fd.write('next_job_start_delay = %s\n'%self.next_job_start_delay)
         if self.mem>0:
             #condor need value in Kb
             fd.write('ImageSize      = %d\n'%(self.mem))

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-27 15:50:45 UTC (rev 10150)
+++ trunk/scripts/dbidispatch	2009-04-27 16:08:29 UTC (rev 10151)
@@ -35,7 +35,7 @@
                               [--no_machine=HOSTNAME+]
                               [--[*no_]keep_failed_jobs_in_queue]
                               [--max_file_size=N][--[no_]debug]
-                              [--[no_]local_log_file]
+                              [--[no_]local_log_file][--next_job_start_delay=N]
 
 An * after '[', '{' or ',' signals the default value.
 An + tell that we can put one or more separeted by a comma
@@ -209,6 +209,8 @@
   The '--[no_]debug' option is forwarded to condor_submit
   The '--[no_]local_log_file' option tell to put the condor log file on the 
       local disk. This help to solv a bug with condor and lock on NFS directory.
+  The '--next_job_start_delay=N' option allow to tell condor to wait N second
+      between each job we start. Default 0.
 
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
@@ -326,7 +328,7 @@
                                 "--queue", "--nano", "--submit_options",
                                 "--jobs_name", "--file", "--tasks_filename",
                                 "--only_n_first", "--no_machine",
-                                "--max_file_size" ]:
+                                "--max_file_size", "--next_job_start_delay"]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -393,6 +395,7 @@
                        "universe", "machine", "machines", "no_machine","to_all",
                        "keep_failed_jobs_in_queue", "restart",
                        "max_file_size", "debug", "local_log_file",
+                       "next_job_start_delay"
                        ]
 elif launch_cmd=="Bqtools":
     valid_dbi_param +=["cpu", "duree", "long", "mem", "micro",



From nouiz at mail.berlios.de  Tue Apr 28 19:55:54 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Apr 2009 19:55:54 +0200
Subject: [Plearn-commits] r10152 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200904281755.n3SHts7G012063@sheep.berlios.de>

Author: nouiz
Date: 2009-04-28 19:55:53 +0200 (Tue, 28 Apr 2009)
New Revision: 10152

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:
added in the help one of the cost supported.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2009-04-27 16:08:29 UTC (rev 10151)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2009-04-28 17:55:53 UTC (rev 10152)
@@ -204,7 +204,9 @@
                   &NatGradNNet::output_type,
                   OptionBase::buildoption,
                   "type of output cost: 'cross_entropy' for binary classification,\n"
-                  "'NLL' for classification problems, or 'MSE' for regression.\n");
+                  "'NLL' for classification problems(noutputs>=1),"
+                  " 'cross_entropy' for classification(noutputs==1)"
+                  " or 'MSE' for regression.\n");
 
     declareOption(ol, "input_size_lrate_normalization_power", 
                   &NatGradNNet::input_size_lrate_normalization_power, 



From nouiz at mail.berlios.de  Tue Apr 28 20:48:20 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Apr 2009 20:48:20 +0200
Subject: [Plearn-commits] r10153 - trunk/scripts
Message-ID: <200904281848.n3SImKaD018600@sheep.berlios.de>

Author: nouiz
Date: 2009-04-28 20:48:20 +0200 (Tue, 28 Apr 2009)
New Revision: 10153

Modified:
   trunk/scripts/dbidispatch
Log:
added option --repeat_jobs=N that repeat N time each jobs.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-28 17:55:53 UTC (rev 10152)
+++ trunk/scripts/dbidispatch	2009-04-28 18:48:20 UTC (rev 10153)
@@ -5,7 +5,7 @@
 from subprocess import Popen,PIPE
 
 ScriptName="launchdbi.py"
-ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [*--[no_]exec_in_exp_dir] [--only_n_first=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
+ShortHelp='''Usage: dbidispatch [--help|-h] [--[*no_]dbilog] [--condor[=N]|--bqtools[=N]|--cluster[=N]|--local[=N]|--ssh[=N]] [--[*no_]test] [--[*no_]testdbi] [--[*no_]nb_proc=N] [--exp_dir=dir] [*--[no_]exec_in_exp_dir] [--only_n_first=N] [--repeat_jobs=N] <back-end parameter> {--file=FILEPATH | <command-template>|--[*no_]restart condor_jobs_number... }
 
 <back-end parameter>:
     all                      :[--tasks_filename={compact,explicit,nb0,nb1,
@@ -83,6 +83,7 @@
     line that was executed, all other options are are those passed to 
     dbidispatch this time. Work only with jobs launched with dbidispatch.
   The '--only_n_first=N' option tell to launch only the first N jobs from the list.
+  The '--repeat_jobs=N' option tell that we must repeat N time each jobs.
   The '--tasks_filename={compact,explicit,nb0,nb1,sh}+' option will change the
     filename where the stdout, stderr are redirected. We can put many option 
     separated by comma. They will apper in the filename in order separated by a 
@@ -328,7 +329,8 @@
                                 "--queue", "--nano", "--submit_options",
                                 "--jobs_name", "--file", "--tasks_filename",
                                 "--only_n_first", "--no_machine",
-                                "--max_file_size", "--next_job_start_delay"]:
+                                "--max_file_size", "--next_job_start_delay",
+                                "--repeat_jobs", ]:
         sp = argv.split('=',1)
         param=sp[0][2:]
         val = sp[1]
@@ -384,7 +386,7 @@
     os.mkdir(LOGDIR)
 
 valid_dbi_param=["clean_up", "test", "dolog", "nb_proc", "exp_dir", "file",
-                 "tasks_filename", "exec_in_exp_dir"
+                 "tasks_filename", "exec_in_exp_dir", "repeat_jobs"
                  ]
 if launch_cmd=="Cluster":
     valid_dbi_param +=["cwait","force","arch","interruptible",
@@ -527,6 +529,12 @@
     commands=commands[:int(dbi_param["only_n_first"])]
     del dbi_param["only_n_first"]
 
+if dbi_param.has_key("repeat_jobs"):
+    print commands
+    commands=commands*int(dbi_param["repeat_jobs"])
+    del dbi_param["repeat_jobs"]
+    print commands
+
 #we duplicate the command so that everything else work correctly.
 if dbi_param.has_key("to_all"):
     assert(len(commands)==1)



From nouiz at mail.berlios.de  Tue Apr 28 20:58:18 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Apr 2009 20:58:18 +0200
Subject: [Plearn-commits] r10154 - trunk/scripts
Message-ID: <200904281858.n3SIwIsQ019725@sheep.berlios.de>

Author: nouiz
Date: 2009-04-28 20:58:18 +0200 (Tue, 28 Apr 2009)
New Revision: 10154

Modified:
   trunk/scripts/dbidispatch
Log:
fixed --repeat_jobs and --only_n_first
now by default on condor when their is more then 20 jobs, we wait 1 second between each jobs start.


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2009-04-28 18:48:20 UTC (rev 10153)
+++ trunk/scripts/dbidispatch	2009-04-28 18:58:18 UTC (rev 10154)
@@ -526,15 +526,22 @@
     (commands,choise_args)=generate_commands(command_argv)
 
 if dbi_param.has_key("only_n_first"):
-    commands=commands[:int(dbi_param["only_n_first"])]
+    n=int(dbi_param["only_n_first"])
+    commands=commands[:n]
+    choise_args=choise_args[:n]
     del dbi_param["only_n_first"]
 
 if dbi_param.has_key("repeat_jobs"):
-    print commands
-    commands=commands*int(dbi_param["repeat_jobs"])
+    n=int(dbi_param["repeat_jobs"])
+    commands=commands*n
+    choise_args=choise_args*n
     del dbi_param["repeat_jobs"]
-    print commands
 
+if not dbi_param.has_key("next_job_start_delay") and launch_cmd=="Condor" and len(commands)>20:
+    #by default, if their is more then 20 jobs we make a start delay of 1 second between each jobs start.
+    dbi_param["next_job_start_delay"]=1
+    
+
 #we duplicate the command so that everything else work correctly.
 if dbi_param.has_key("to_all"):
     assert(len(commands)==1)



From nouiz at mail.berlios.de  Tue Apr 28 22:57:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Apr 2009 22:57:26 +0200
Subject: [Plearn-commits] r10155 - trunk/plearn/vmat
Message-ID: <200904282057.n3SKvQXI000520@sheep.berlios.de>

Author: nouiz
Date: 2009-04-28 22:57:25 +0200 (Tue, 28 Apr 2009)
New Revision: 10155

Modified:
   trunk/plearn/vmat/BootstrapVMatrix.cc
Log:
remove a useless warning in a particular case.


Modified: trunk/plearn/vmat/BootstrapVMatrix.cc
===================================================================
--- trunk/plearn/vmat/BootstrapVMatrix.cc	2009-04-28 18:58:18 UTC (rev 10154)
+++ trunk/plearn/vmat/BootstrapVMatrix.cc	2009-04-28 20:57:25 UTC (rev 10155)
@@ -221,6 +221,9 @@
                 indices.append(bag_to_indices[bag_indices[i]]);
         }
 
+        //if we only shuffle the rows, we remove a useless warning.
+        if(frac == 1 && allow_repetitions == 0 && rows_to_remove == 0 && shuffle == 1)
+            warn_if_all_rows_selected=false;
         // Because we changed the indices, a rebuild may be needed.
         inherited::build();
     }



From nouiz at mail.berlios.de  Wed Apr 29 18:42:16 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 29 Apr 2009 18:42:16 +0200
Subject: [Plearn-commits] r10156 - trunk/plearn/vmat
Message-ID: <200904291642.n3TGgGJL020438@sheep.berlios.de>

Author: nouiz
Date: 2009-04-29 18:42:15 +0200 (Wed, 29 Apr 2009)
New Revision: 10156

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
   trunk/plearn/vmat/GaussianizeVMatrix.h
Log:
-remote declare some fct
-added fct to ungauss a value
-added parameter to save the columns that will be gaussianized or load it instead of calculating them.


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-04-28 20:57:25 UTC (rev 10155)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2009-04-29 16:42:15 UTC (rev 10156)
@@ -42,6 +42,7 @@
 #include "VMat_computeStats.h"
 #include <plearn/io/load_and_save.h>
 #include <plearn/io/fileutils.h>
+#include <plearn/base/RemoteDeclareMethod.h>
 
 namespace PLearn {
 using namespace std;
@@ -132,12 +133,32 @@
         "An optional VMat that will be used instead of 'source' to compute\n"
         "the transformation parameters from the distribution statistics.");
 
+    declareOption(ol, "fields_to_gaussianize",
+                  &GaussianizeVMatrix::fields_to_gaussianize,
+                  OptionBase::buildoption,
+                  "The fields that we want to be gaussianized.");
+
     declareOption(ol, "stats_file_to_use",
                   &GaussianizeVMatrix::stats_file_to_use,
                   OptionBase::buildoption,
                   "The filename of the statistics to use instead of the"
                   " train_source.");
 
+    declareOption(ol, "save_fields_gaussianized",
+                  &GaussianizeVMatrix::save_fields_gaussianized,
+                  OptionBase::buildoption,
+                  "A path where we will save the fields selected to be gaussianized.");
+
+    declareOption(ol, "features_to_gaussianize",
+                  &GaussianizeVMatrix::features_to_gaussianize,
+                  OptionBase::learntoption,
+                  "The columsn that will be gaussianized.");
+
+    declareOption(ol, "values",
+                  &GaussianizeVMatrix::values,
+                  OptionBase::learntoption,
+                  "The values used to gaussinaze.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -198,21 +219,51 @@
 
     // Obtain meta information from source.
     setMetaInfoFromSource();
-    if(hasMetaDataDir() && values.size()==0)
+    if((hasMetaDataDir()||!stats_file_to_use.empty()) && values.size()==0)
         setMetaDataDir(getMetaDataDir());
 }
 
+///////////////////////////////
+// append_col_to_gaussianize //
+///////////////////////////////
+void GaussianizeVMatrix::append_col_to_gaussianize(int col, StatsCollector stat){
+    values.append(Vec());
+    Vec& values_j = values.lastElement();
+    features_to_gaussianize.append(col);
+    map<real, StatsCollectorCounts>::const_iterator it, it_dummy;
+    // Note that we obtain the approximate counts, so that almost equal
+    // values have been merged together already.
+    map<real,StatsCollectorCounts>* count_map =
+        stat.getApproximateCounts();
+    values_j.resize(0,count_map->size());
+    // We use a dummy iterator to get rid of the last element in the
+    // map, which is the max real value.
+    it_dummy = count_map->begin();
+    it_dummy++;
+    for (it = count_map->begin(); it_dummy != count_map->end();
+         it++, it_dummy++)
+    {
+        values_j.append(it->first);
+    }
+}
+
 ////////////////////
 // setMetaDataDir //
 ////////////////////
 void GaussianizeVMatrix::setMetaDataDir(const PPath& the_metadatadir){
-    inherited::setMetaDataDir(the_metadatadir);
 
+    if(!the_metadatadir.empty())
+        inherited::setMetaDataDir(the_metadatadir);
+
     if(features_to_gaussianize.size()==0)
         return;
 
     VMat the_source = train_source ? train_source : source;
     
+    if(!the_source->hasMetaDataDir() && stats_file_to_use.empty() )
+        PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "
+                " train_source, source or this VMatrix should have a metadata directory!");
+
     //to save the stats their must be a metadatadir
     if(!the_source->hasMetaDataDir() && hasMetaDataDir()){
         if (train_source)
@@ -221,10 +272,6 @@
             the_source->setMetaDataDir(getMetaDataDir()+"source");
     }
 
-    if(!the_source->hasMetaDataDir() && stats_file_to_use.empty())
-        PLERROR("In GaussianizeVMatrix::setMetaDataDir() - the "
-                " train_source, source or this VMatrix should have a metadata directory!");
-    
     TVec<StatsCollector> stats;
     if(!stats_file_to_use.empty()){
         if(!isfile(stats_file_to_use))
@@ -238,43 +285,56 @@
     else
         stats = PLearn::computeStats(the_source, -1, true);
 
-    // See which dimensions violate the Gaussian assumption and will be
-    // actually Gaussianized, and store the corresponding list of values.
-    TVec<int> candidates = features_to_gaussianize.copy();
-    features_to_gaussianize.resize(0);
-    values.resize(0);
-    for (int i = 0; i < candidates.length(); i++) {
-        int j = candidates[i];
-        StatsCollector& stat = stats[j];
-        if (fast_exact_is_equal(stat.stddev(), 0)){
-            stats[j].forget();//to keep the total memory used lower faster
-            continue;
+    if(fields_to_gaussianize.size()>0){
+        for(int i=0;i<fields_to_gaussianize.size();i++){
+            int field=fields_to_gaussianize[i];
+            if(field>=width() || field<0)
+                PLERROR("In GaussianizeVMatrix::setMetaDataDir() - "
+                        "bad fields number to gaussianize(%d)!",field);
         }
-        if (!gaussianize_binary && stat.isbinary()) {
-            stats[j].forget();//to keep the total memory used lower faster
-            continue;
+        features_to_gaussianize.resize(0,fields_to_gaussianize.length());
+
+        values.resize(0);
+        int last_j=-1;
+        for (int i = 0; i < fields_to_gaussianize.length(); i++) {
+            int j = fields_to_gaussianize[i];
+            StatsCollector& stat = stats[j];
+            if(last_j+1!=j)
+                for(int k=last_j+1;k<j;k++){
+                    //to keep the total memory used lower faster.
+                    stats[k].forget();
+                }
+            append_col_to_gaussianize(j,stat);
+            stats[j].forget();//to keep the total memory used lower.
         }
-        if ((stat.max() - stat.min()) > threshold_ratio * stat.stddev()) {
-            features_to_gaussianize.append(j);
-            values.append(Vec());
-            Vec& values_j = values.lastElement();
-            map<real, StatsCollectorCounts>::const_iterator it, it_dummy;
-            // Note that we obtain the approximate counts, so that almost equal
-            // values have been merged together already.
-            map<real,StatsCollectorCounts>* count_map =
-                                                stat.getApproximateCounts();
-            // We use a dummy iterator to get rid of the last element in the
-            // map, which is the max real value.
-            it_dummy = count_map->begin();
-            it_dummy++;
-            for (it = count_map->begin(); it_dummy != count_map->end();
-                                          it++, it_dummy++)
-            {
-                values_j.append(it->first);
+    }else{
+
+        // See which dimensions violate the Gaussian assumption and will be
+        // actually Gaussianized, and store the corresponding list of values.
+        TVec<int> candidates = features_to_gaussianize.copy();
+        features_to_gaussianize.resize(0);
+        values.resize(0);
+        for (int i = 0; i < candidates.length(); i++) {
+            int j = candidates[i];
+            StatsCollector& stat = stats[j];
+            if (fast_exact_is_equal(stat.stddev(), 0)){
+                //we don't gaussianize
+            }else if (!gaussianize_binary && stat.isbinary()) {
+                //we don't gaussianize
+            }else if ((stat.max() - stat.min()) > threshold_ratio * stat.stddev()) {
+                append_col_to_gaussianize(j,stat);
             }
+
+            stats[j].forget();//to keep the total memory used lower.
         }
+    }
 
-        stats[j].forget();//to keep the total memory used lower.
+    fields_gaussianized.resize(width());
+    fields_gaussianized.fill(-1);
+    for(int i=0;i<features_to_gaussianize.size();i++)
+        fields_gaussianized[features_to_gaussianize[i]]=i;
+    if(!save_fields_gaussianized.empty()){
+        PLearn::save(save_fields_gaussianized,features_to_gaussianize);
     }
     if(features_to_gaussianize.size()==0)
         PLWARNING("GaussianizeVMatrix::build_() 0 variable was gaussianized");
@@ -344,8 +404,85 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
     deepCopyField(train_source, copies);
+    //features_to_gaussianize?
+    //scaling_factor?
+    //values?
 }
 
+
+/////////////
+// unGauss //
+/////////////
+real GaussianizeVMatrix::unGauss(real input, int j) const
+{
+    int k=fields_gaussianized[j];
+    if(k<0)
+        return input;//was not gaussianized
+    
+    real interpol = gauss_01_cum(input);
+    Vec& values_j = values[k];
+    int idx=int(interpol*values_j.length());
+    return values_j[idx];
+}
+
+/////////////
+// unGauss //
+/////////////
+void GaussianizeVMatrix::unGauss(Vec& inputs, Vec& ret, int j) const
+{
+    int k=fields_gaussianized[j];
+    if(k<0)
+        ret<<inputs;//was not gaussianized
+    
+    for(int i=0;i<inputs.size();i++){
+        real value = inputs[i];
+        real interpol = gauss_01_cum(value);
+        Vec& values_j = values[k];
+        int idx=int(interpol*values_j.length());
+        ret[i]=values_j[idx];
+    }
+   
+}
+
+//! Version of unGauss(vec,vec,int) that's called by RMI
+real GaussianizeVMatrix::remote_unGauss(real value, int col) const
+{
+    return unGauss(value,col);
+}
+
+//! Version of unGauss(vec,vec,int) that's called by RMI
+Vec GaussianizeVMatrix::remote_unGauss_vec(Vec values, int col) const
+{
+    Vec outputs(values.length());
+    unGauss(values,outputs,col);
+    return outputs;
+}
+
+////////////////////
+// declareMethods //
+////////////////////
+void GaussianizeVMatrix::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this is different from
+    // declareOptions().
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "unGauss", &GaussianizeVMatrix::remote_unGauss,
+        (BodyDoc("Revert the gaussinisation done."),
+         ArgDoc ("value", "The value to revert."),
+         ArgDoc ("j", "The column of the value.")));
+
+
+    declareMethod(
+        rmm, "unGauss2", &GaussianizeVMatrix::remote_unGauss_vec,
+        (BodyDoc("Revert the gaussinisation done."),
+         ArgDoc ("values", "A vector of values to revert."),
+         ArgDoc ("j", "The column of the value.")));
+
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/GaussianizeVMatrix.h
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.h	2009-04-28 20:57:25 UTC (rev 10155)
+++ trunk/plearn/vmat/GaussianizeVMatrix.h	2009-04-29 16:42:15 UTC (rev 10156)
@@ -73,6 +73,8 @@
     bool save_and_reuse_stats;
     VMat train_source;
     string stats_file_to_use;
+    TVec<int> fields_to_gaussianize;
+    PPath save_fields_gaussianized;
 
 public:
     //#####  Public Member Functions  #########################################
@@ -94,14 +96,20 @@
     virtual void build();
 
     //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
+
+    //! return the approximate value of value before being gaussianized.
+    real unGauss(real value, int col)const;
+    void unGauss(Vec& values, Vec& ret, int col)const;
+
 protected:
 
     //! List of features that need to be Gaussianized.
     TVec<int> features_to_gaussianize;
 
+    TVec<int> fields_gaussianized;
+
     //! Scaling factor to map the rank to [0,1].
     Vec scaling_factor;
 
@@ -113,9 +121,11 @@
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
     //! Fill the vector 'v' with the content of the i-th row.
     //! v is assumed to be the right size.
     //! ### This function must be overridden in your class
@@ -125,10 +135,11 @@
     //#####  Private Member Functions  ########################################
 
     //! This does the actual building.
-    // (PLEASE IMPLEMENT IN .cc)
     void build_();
     virtual void setMetaDataDir(const PPath& the_metadatadir);
-
+    real remote_unGauss(real input, int col)const;
+    Vec remote_unGauss_vec(Vec inputs, int col) const;
+    void append_col_to_gaussianize(int col, StatsCollector stat);
 private:
     //#####  Private Data Members  ############################################
 



From nouiz at mail.berlios.de  Wed Apr 29 21:14:26 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 29 Apr 2009 21:14:26 +0200
Subject: [Plearn-commits] r10157 - trunk/plearn/vmat
Message-ID: <200904291914.n3TJEQ64004808@sheep.berlios.de>

Author: nouiz
Date: 2009-04-29 21:14:26 +0200 (Wed, 29 Apr 2009)
New Revision: 10157

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
print more debug info, allow [END], allow END-N(N an integer) to revert from the end.


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2009-04-29 16:42:15 UTC (rev 10156)
+++ trunk/plearn/vmat/VMatLanguage.cc	2009-04-29 19:14:26 UTC (rev 10157)
@@ -93,7 +93,7 @@
                         "for a 0.5 thresholding: 0.5 < 0 1 ifelse. To copy a single field, use [field].\n"
                         "There is also a special feature available only for single field copies: if you\n"
                         "use the syntax [field?], then VPL will not produce an error if the field cannot\n"
-                        "be found.\n"
+                        "be found. END-N(N an integer) will select a field conting from the END.\n"
                         "\n"
                         "Here's a real-life example of a VPL program:\n"
                         "\n"
@@ -388,7 +388,8 @@
                     // Keyword indicating we go till the end.
                     a = srcfieldnames.length() - 1;
                 else
-                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field (%s)",
+                        token.c_str());
 
                 if (parts[1][0] == '@')
                 {
@@ -404,8 +405,17 @@
                 else if (parts[1] == "END")
                     // Keyword indicating we go till the end.
                     b = srcfieldnames.length() - 1;
+                
+                else if (parts[1].substr(0,3) == "END")
+                {
+                    // Keyword indicating we go till the end.
+                    int v = toint(parts[1].substr(3));
+                    PLCHECK(v<0);
+                    b = srcfieldnames.length() - 1 + v;
+                }
                 else
-                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field (%s)",
+                            token.c_str());
 
                 if (a > b)
                     PLERROR("In copyfield macro '%s', you have specified a "
@@ -450,8 +460,12 @@
                 }
                 else if (parts[0][0] == '%')
                     a = toint(parts[0].substr(1));
+                else if (parts[0] == "END")
+                    // Keyword indicating we go till the end.
+                    a = srcfieldnames.length() - 1;
                 else
-                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
+                    PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%%6]. 'end' must be after 'start'.. OR [field] to copy a single field (%s)",
+                            token.c_str());
 
                 if (a == -1) {
                     if (!ignore_if_missing)



From nouiz at mail.berlios.de  Wed Apr 29 21:36:48 2009
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 29 Apr 2009 21:36:48 +0200
Subject: [Plearn-commits] r10158 - trunk/plearn/vmat
Message-ID: <200904291936.n3TJamkO008623@sheep.berlios.de>

Author: nouiz
Date: 2009-04-29 21:36:47 +0200 (Wed, 29 Apr 2009)
New Revision: 10158

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
more explicit error msg.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2009-04-29 19:14:26 UTC (rev 10157)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2009-04-29 19:36:47 UTC (rev 10158)
@@ -419,7 +419,11 @@
 
     mean_median_mode_file = new FileVMatrix(file_name);
     compatibleSizeError(mean_median_mode_file, "Bad file "+file_name);
-    PLCHECK(mean_median_mode_file->fieldNames()==fieldNames());
+    if(mean_median_mode_file->fieldNames()!=fieldNames())
+      PLERROR("In MeanMedianModeImputationVMatrix::loadMeanMedianModeFile(%s) -"
+	      " The file don't have the same field name as the source. "
+	      "Delete it to have it recreated it automatically.",
+	      file_name.c_str());
     mean_median_mode_file->getRow(0, variable_mean);
     mean_median_mode_file->getRow(1, variable_median);
     mean_median_mode_file->getRow(2, variable_mode);



From laulysta at mail.berlios.de  Wed Apr 29 23:40:29 2009
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 29 Apr 2009 23:40:29 +0200
Subject: [Plearn-commits] r10159 - trunk/plearn_learners_experimental
Message-ID: <200904292140.n3TLeTtj024452@sheep.berlios.de>

Author: laulysta
Date: 2009-04-29 23:40:28 +0200 (Wed, 29 Apr 2009)
New Revision: 10159

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:
generation expressive


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-29 19:36:47 UTC (rev 10158)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2009-04-29 21:40:28 UTC (rev 10159)
@@ -2429,7 +2429,7 @@
     //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/create_data/scoreGenSuitePerf.amat";
     data->filename = "/home/stan/cvs/Gamme/expressive_data/dataGen.amat";
 
-    data->defineSizes(173,16,0);
+    data->defineSizes(163,16,0);
     //data->inputsize = 21;
     //data->targetsize = 0;
     //data->weightsize = 0;



From islaja at mail.berlios.de  Thu Apr 30 01:02:03 2009
From: islaja at mail.berlios.de (islaja at BerliOS)
Date: Thu, 30 Apr 2009 01:02:03 +0200
Subject: [Plearn-commits] r10160 - trunk/plearn_learners/online
Message-ID: <200904292302.n3TN232s028234@sheep.berlios.de>

Author: islaja
Date: 2009-04-30 01:02:00 +0200 (Thu, 30 Apr 2009)
New Revision: 10160

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Different modification were done. Here's a description of most of them in StackedAutoassociatorsNet:

new option : nb_corrupted_layer 
	- Allows to corrupt only the first (=1), first and second (=2), first, second and third layer (=3), and so one during the pre-training phase.

new noise_type: missing data 
	- incorporates the missing data state, that implicates to double the inputs in some way.
	- One way to do so, and it's the only way implemented for the moment ("binomial_complementary"), can be used only if the input are between 0 and 1, and consists of using a double version of the inputs completed with each (1-x)  
	- This doubled version is then corrupted, where a fraction of the original inputs are chosen and remplaced with 0 as well as their complementary input. Missing data is then represented with two consecutives 0 when non-missing data is represented with its value (x) followed bye (1-x).


new masking_type: mask_with_pepper_salt
	- works with probability_of_masked_inputs or fraction_of_masked_inputs
	- a supplementary hyperparam: prob_salt_noise indicates the probability to corrupt an input with 1 instead of 0.

possibility to put emphasis on corrupted (or missing) data when computing the biais gradient during the reconstruction.
 	- to do so, change value of corrupted_data_weight and data_weight (by default, they are both equal to 1 (no emphasis.)
	- bias gradient will then be multiplied by the reconstruction_weights vector, element-wise.

-method fantasizeKTime
	- Supposes the learner already trained.
	- Allows a codage-decodage ktime from a source image. Returns the 'fantasize' image. 
	- You can choose how many layers to use (including raws layer) by defining the size of sample. 
	- You can corrupt layers differently during the codage phase by defining maskNoiseFractOrProb 
	- You can apply a binary sampling (1) or not (0) differently for each layer during the decode phase
	- Lower element in sample and maskNoiseFractOrProb correspond to lower layer.  




Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2009-04-29 21:40:28 UTC (rev 10159)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2009-04-29 23:02:00 UTC (rev 10160)
@@ -184,7 +184,7 @@
 {
     PLASSERT( input.size() == input_size );
     output.resize( output_size );
-
+   
     if( use_signed_samples )
         if (use_fast_approximations)
             for( int i=0 ; i<size ; i++ )
@@ -199,7 +199,6 @@
         else
             for( int i=0 ; i<size ; i++ )
                 output[i] = sigmoid( input[i] + bias[i] );
-
 }
 
 void RBMBinomialLayer::fprop( const Mat& inputs, Mat& outputs )
@@ -514,6 +513,78 @@
     return ret;
 }
 
+real RBMBinomialLayer::fpropNLL(const Vec& target, const Vec& cost_weights)
+{
+    PLASSERT( target.size() == input_size );
+    PLASSERT( target.size() == cost_weights.size() );
+    PLASSERT (cost_weights.size() == size );
+
+    real ret = 0;
+    real target_i, activation_i;
+    if( use_signed_samples )
+    {
+        if(use_fast_approximations){
+            for( int i=0 ; i<size ; i++ )
+            {
+                if(cost_weights[i] != 0)
+                {
+                    target_i = (target[i]+1)/2;
+                    activation_i = 2*activation[i];
+
+                    ret += cost_weights[i]*(tabulated_softplus(activation_i) - target_i * activation_i);
+                }
+                // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+                // but it is numerically unstable, so use instead the following identity:
+                //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+                //     = act + softplus(-act) - target*act
+                //     = softplus(act) - target*act
+            }
+        } else {
+            for( int i=0 ; i<size ; i++ )
+            {
+                if(cost_weights[i] != 0)
+                {
+                    target_i = (target[i]+1)/2;
+                    activation_i = 2*activation[i];
+                    ret += cost_weights[i]*(softplus(activation_i) - target_i * activation_i);
+                }
+            }
+        }
+    }
+    else
+    {
+        if(use_fast_approximations){
+            for( int i=0 ; i<size ; i++ )
+            {
+                if(cost_weights[i] != 0)
+                {
+                    target_i = target[i];
+                    activation_i = activation[i];
+                    ret += cost_weights[i]*(tabulated_softplus(activation_i) - target_i * activation_i);
+                }
+                // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+                // but it is numerically unstable, so use instead the following identity:
+                //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+                //     = act + softplus(-act) - target*act
+                //     = softplus(act) - target*act
+            }
+        } else {
+            for( int i=0 ; i<size ; i++ )
+            {
+                if(cost_weights[i] != 0)
+                {
+                    target_i = target[i];
+                    activation_i = activation[i];
+                    ret += cost_weights[i]*(softplus(activation_i) - target_i * activation_i);
+                }
+            }
+        }
+    }
+
+    return ret;
+}
+
+
 void RBMBinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
     PLASSERT( targets.width() == input_size );

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2009-04-29 21:40:28 UTC (rev 10159)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2009-04-29 23:02:00 UTC (rev 10160)
@@ -66,7 +66,7 @@
     RBMBinomialLayer( real the_learning_rate=0. );
 
     //! Constructor from the number of units
-    RBMBinomialLayer( int the_size, real the_learning_rate=0. );
+    RBMBinomialLayer( int the_size, real the_learning_rate=0.);
 
     //! generate a sample, and update the sample field
     virtual void generateSample() ;
@@ -111,7 +111,11 @@
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
     virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
+    //! Computes the weighted negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec& target, const Vec& weights);
 
+
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
@@ -133,8 +137,7 @@
     virtual void freeEnergyContributionGradient(const Vec& unit_activations,
                                                 Vec& unit_activations_gradient,
                                                 real output_gradient = 1,
-                                                bool accumulate = false)
-        const;
+                                                bool accumulate = false) const;
 
     virtual int getConfigurationCount();
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2009-04-29 21:40:28 UTC (rev 10159)
+++ trunk/plearn_learners/online/RBMLayer.cc	2009-04-29 23:02:00 UTC (rev 10160)
@@ -369,6 +369,14 @@
     return REAL_MAX;
 }
 
+real RBMLayer::fpropNLL(const Vec& target, const Vec& cost_weights)
+{
+    PLERROR("weighted version of RBMLayer::fpropNLL not implemented in subclass %s",
+            this->classname().c_str());
+    return REAL_MAX;
+}
+
+
 void RBMLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
 {
     PLWARNING("batch version of RBMLayer::fpropNLL may not be optimized in subclass %s",

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2009-04-29 21:40:28 UTC (rev 10159)
+++ trunk/plearn_learners/online/RBMLayer.h	2009-04-29 23:02:00 UTC (rev 10160)
@@ -213,6 +213,7 @@
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
     virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
+    virtual real fpropNLL(const Vec& target, const Vec& cost_weights);
 
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-29 21:40:28 UTC (rev 10159)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2009-04-29 23:02:00 UTC (rev 10160)
@@ -67,15 +67,21 @@
     compute_all_test_costs( false ),
     reconstruct_hidden( false ),
     noise_type( "masking_noise" ),
+    missing_data_method( "binomial_complementary"),
+    corrupted_data_weight( 1 ),
+    data_weight( 1 ),
     fraction_of_masked_inputs( 0 ),
     probability_of_masked_inputs( 0 ),
     probability_of_masked_target( 0 ),
     mask_with_mean( false ),
+    mask_with_pepper_salt( false ),
+    prob_salt_noise( 0.5 ),
     gaussian_std( 1. ),
     binary_sampling_noise_parameter( 1. ),
     unsupervised_nstages( 0 ),
     unsupervised_fine_tuning_learning_rate( 0. ),
     unsupervised_fine_tuning_decrease_ct( 0. ),
+    nb_corrupted_layer( -1 ),
     mask_input_layer_only( false ),
     mask_input_layer_only_in_unsupervised_fine_tuning( false ),
     train_stats_window( -1 ),
@@ -137,8 +143,7 @@
                   &StackedAutoassociatorsNet::training_schedule,
                   OptionBase::buildoption,
                   "Number of examples to use during each phase of greedy pre-training.\n"
-                  "The number of fine-tunig steps is defined by nstages.\n"
-        );
+                  "The number of fine-tunig steps is defined by nstages.\n");
 
     declareOption(ol, "layers", &StackedAutoassociatorsNet::layers,
                   OptionBase::buildoption,
@@ -242,12 +247,36 @@
                   OptionBase::buildoption,
                   "Type of noise that corrupts the autoassociators input. "
                   "Choose among:\n"
+                  " - \"missing_data\"\n"
                   " - \"masking_noise\"\n"
                   " - \"binary_sampling\"\n"
                   " - \"gaussian\"\n"
                   " - \"none\"\n"
         );
 
+    declareOption(ol, "missing_data_method",
+                  &StackedAutoassociatorsNet::missing_data_method,
+                  OptionBase::buildoption,
+                  "Method used to fill the double_input vector for missing_data noise type."
+                  "Choose among:\n"
+                  " - \"binomial_complementary\"\n"
+                  " - \"none\"\n"
+        );
+
+    declareOption(ol, "corrupted_data_weight",
+                  &StackedAutoassociatorsNet::corrupted_data_weight,
+                  OptionBase::buildoption,
+                  "Weight owned by a corrupted or missing data when"
+                  "backpropagating the gradient of reconstruction cost.\n"
+        );
+
+    declareOption(ol, "data_weight",
+                  &StackedAutoassociatorsNet::data_weight,
+                  OptionBase::buildoption,
+                  "Weight owned by a data not corrupted when"
+                  "backpropagating the gradient of reconstruction cost.\n"
+        );
+
     declareOption(ol, "fraction_of_masked_inputs",
                   &StackedAutoassociatorsNet::fraction_of_masked_inputs,
                   OptionBase::buildoption,
@@ -276,6 +305,19 @@
                   "training set mean of that component.\n"
         );
 
+    declareOption(ol, "mask_with_pepper_salt",
+                  &StackedAutoassociatorsNet::mask_with_pepper_salt,
+                  OptionBase::buildoption,
+                  "Indication that inputs should be masked with "
+                  "0 or 1 according to prob_salt_noise.\n"
+        );
+
+    declareOption(ol, "prob_salt_noise",
+                  &StackedAutoassociatorsNet::prob_salt_noise,
+                  OptionBase::buildoption,
+                  "Probability that we mask the input by 1 instead of 0.\n"
+        );
+
     declareOption(ol, "gaussian_std",
                   &StackedAutoassociatorsNet::gaussian_std,
                   OptionBase::buildoption,
@@ -306,10 +348,17 @@
                   "The decrease constant of the learning rate used during\n"
                   "unsupervised fine tuning gradient descent.\n");
 
+    declareOption(ol, "nb_corrupted_layer",
+                  &StackedAutoassociatorsNet::nb_corrupted_layer,
+                  OptionBase::buildoption,
+                  "Indicate how many layers should be corrupted,\n"
+                  "starting with the input one,\n"
+                  "during greedy layer-wise learning.\n");
+
     declareOption(ol, "mask_input_layer_only",
                   &StackedAutoassociatorsNet::mask_input_layer_only,
                   OptionBase::buildoption,
-                  "Indication that only the input layer should be masked\n"
+                  "Indication that only the input layer should be corrupted\n"
                   "during greedy layer-wise learning.\n");
 
     declareOption(ol, "mask_input_layer_only_in_unsupervised_fine_tuning",
@@ -363,6 +412,22 @@
     // Insert a backpointer to remote methods; note that this is different from
     // declareOptions().
     rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "fantasizeKTime",
+        &StackedAutoassociatorsNet::fantasizeKTime,
+        (BodyDoc("On a trained learner, computes a codage-decodage (fantasize) through a specified number of hidden layer."),
+         ArgDoc ("kTime", "Number of time we want to fantasize (next source image will be the last fantasize Image, and so on for kTime.)"),
+         ArgDoc ("srcImg", "Source image vector (should have same width as raws layer)"),
+         ArgDoc ("sampling", "Vector of bool indicating whether or not a sampling will be done for each hidden layer\n"
+                "during decodage. Its width indicates how many hidden layer will be used.)\n"
+                " (should have same width as maskNoiseFractOrProb)\n"
+                "smaller element of the vector correspond to lower layer"),
+         ArgDoc ("maskNoiseFractOrProb", "Vector of noise fraction or probability\n"
+                "(according to the one used during the learning stage)\n"
+                "for each layer. (should have same width as sampling or be empty if unuseful.\n"
+                "Smaller element of the vector correspond to lower layer"),
+         RetDoc ("Corresponding fantasize image (will have same width as srcImg)")));
 }
 
 void StackedAutoassociatorsNet::build_()
@@ -374,6 +439,14 @@
         // Initialize some learnt variables
         n_layers = layers.length();
 
+        if(nb_corrupted_layer == -1)
+                nb_corrupted_layer = n_layers-1;
+
+        if( nb_corrupted_layer >= n_layers)
+            PLERROR("StackedAutoassociatorsNet::build_() - \n"
+                    " - \n"
+                    "nb_corrupted_layers should be < %d\n",n_layers);
+
         if( weightsize_ > 0 )
             PLERROR("StackedAutoassociatorsNet::build_() - \n"
                     "usage of weighted samples (weight size > 0) is not\n"
@@ -415,11 +488,26 @@
                     " - \n"
                     "probability_of_masked_inputs should be > or equal to 0.\n");
 
+        if( prob_salt_noise < 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "prob_salt_noise should be > or equal to 0.\n");
+
         if( probability_of_masked_target < 0 )
             PLERROR("StackedAutoassociatorsNet::build_()"
                     " - \n"
                     "probability_of_masked_target should be > or equal to 0.\n");
 
+        if( data_weight < 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "data_weight should be > or equal to 0.\n");
+
+        if( corrupted_data_weight < 0 )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "corrupted_data_weight should be > or equal to 0.\n");
+
         if( online && noise_type != "masking_noise" && batch_size != 1)
             PLERROR("StackedAutoassociatorsNet::build_()"
                     " - \n"
@@ -485,7 +573,25 @@
                 "compute_all_test_costs option is not implemented for\n"
                 "reconstruct_hidden option.");
 
+    if( noise_type == "missing_data")
+    {
+        if( correlation_connections.length() !=0 )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                    "Missing data is not implemented with correlation_connections.\n");
+    
+        if( direct_connections.length() !=0 )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                    "Missing data is not implemented with direct_connections.\n");
+        
+        if( reconstruct_hidden )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                    "Missing data is not implemented with reconstruct_hidden.\n");
 
+        if( online ) 
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                    "Missing data is not implemented in the online setting.\n");
+    }
+
     if(correlation_connections.length() != 0)
     {
         if( compute_all_test_costs )
@@ -529,12 +635,38 @@
 
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
-        if( layers[i]->size != connections[i]->down_size )
+        if( noise_type == "missing_data")
+        {
+            if( layers[i]->size * 2 != connections[i]->down_size )
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                     "- \n"
+                    "When noise_type==%s, connections[%i] should have a down_size "
+                    "2 time the size of layers[%i], i.e: 2 * %d.\n",
+                    noise_type.c_str(), i, i, layers[i]->size);
+
+            if( reconstruction_connections[i]->up_size != layers[i]->size*2 )
             PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
                     "- \n"
+                    "When noise_type==%s, recontruction_connections[%i] should have a up_size "
+                    "2 time the size of layers[%i], i.e: 2 * %d.\n",
+                    noise_type.c_str(), i, i, layers[i]->size);
+        }
+        else
+        {
+            if( layers[i]->size != connections[i]->down_size )
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                     "- \n"
                     "connections[%i] should have a down_size of %d.\n",
                     i, layers[i]->size);
 
+            if( reconstruction_connections[i]->up_size != layers[i]->size )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
+                    "- \n"
+                    "recontruction_connections[%i] should have a up_size of "
+                    "%d.\n",
+                    i, layers[i]->size);
+        }
+
         if( connections[i]->up_size != layers[i+1]->size )
             PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
                     "- \n"
@@ -548,13 +680,6 @@
                     "%d.\n",
                     i, layers[i+1]->size);
 
-        if( reconstruction_connections[i]->up_size != layers[i]->size )
-            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() "
-                    "- \n"
-                    "recontruction_connections[%i] should have a up_size of "
-                    "%d.\n",
-                    i, layers[i]->size);
-
         if(correlation_connections.length() != 0)
         {
             if(reconstruct_hidden)
@@ -665,18 +790,35 @@
     activation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
     expectation_gradients[n_layers-1].resize( layers[n_layers-1]->size );
 
+    reconstruction_weights.resize( layers[0]->size );
+
     // For denoising autoencoders
+    doubled_expectations.resize( n_layers-1 );
+    doubled_expectation_gradients.resize( n_layers-1 );
     corrupted_autoassociator_expectations.resize( n_layers-1 );
     binary_masks.resize( n_layers-1 );
-    if( noise_type == "masking_noise" )
+    
+    if( (noise_type == "masking_noise" || noise_type == "missing_data") && fraction_of_masked_inputs > 0 )
         autoassociator_expectation_indices.resize( n_layers-1 );
-    
+
     for( int i=0 ; i<n_layers-1 ; i++ )
     {
-        corrupted_autoassociator_expectations[i].resize( layers[i]->size );
         binary_masks[i].resize( layers[i]->size ); // For online learning
-        if( noise_type == "masking_noise" )
+        if( noise_type == "missing_data" )
         {
+            corrupted_autoassociator_expectations[i].resize( layers[i]->size * 2 );
+            doubled_expectations[i].resize( layers[i]->size * 2 );
+            doubled_expectation_gradients[i].resize( layers[i]->size * 2 );
+        }
+        else
+        {
+            corrupted_autoassociator_expectations[i].resize( layers[i]->size );
+            doubled_expectations[i].resize( layers[i]->size );
+            doubled_expectation_gradients[i].resize( layers[i]->size );
+        }
+
+        if( (noise_type == "masking_noise" || noise_type == "missing_data") && fraction_of_masked_inputs > 0 )
+        {
             autoassociator_expectation_indices[i].resize( layers[i]->size );
             for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
                 autoassociator_expectation_indices[i][j] = j;
@@ -696,7 +838,6 @@
         }
     }
 }
-
 void StackedAutoassociatorsNet::build_costs()
 {
     MODULE_LOG << "build_final_cost() called" << endl;
@@ -806,9 +947,11 @@
     deepCopyField(activations_m, copies);
     deepCopyField(expectations, copies);
     deepCopyField(expectations_m, copies);
+    deepCopyField(doubled_expectations, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(activation_gradients_m, copies);
     deepCopyField(expectation_gradients, copies);
+    deepCopyField(doubled_expectation_gradients, copies);
     deepCopyField(expectation_gradients_m, copies);
     deepCopyField(reconstruction_activations, copies);
     deepCopyField(reconstruction_activations_m, copies);
@@ -848,6 +991,7 @@
     deepCopyField(final_cost_gradient, copies);
     deepCopyField(final_cost_gradients, copies);
     deepCopyField(corrupted_autoassociator_expectations, copies);
+    deepCopyField(reconstruction_weights, copies);
     deepCopyField(binary_masks, copies);
     deepCopyField(tmp_mask, copies);
     deepCopyField(autoassociator_expectation_indices, copies);
@@ -1013,7 +1157,7 @@
                             layers[j+1]->fprop(activations[j+1],expectations[j+1]);
                         }
                     }
-                    
+
                     expectation_means[i] += expectations[i];
                 }
                 expectation_means[i] /= train_set->length();
@@ -1303,56 +1447,91 @@
 {
     binary_mask.fill(1);
     corrupted_input.resize(input.length());
-    if( mask_input_layer_only && layer != 0 )
+    reconstruction_weights.resize(input.length());
+    reconstruction_weights.fill(1);
+
+    if( (mask_input_layer_only && layer != 0) ||
+         (!mask_input_layer_only && layer > (nb_corrupted_layer-1)) )
     {
-        corrupted_input << input; 
+        corrupted_input << input;
         return;
     }
-    
+
     if( noise_type == "masking_noise" )
     {
         if( probability_of_masked_inputs > 0 )
         {
             if( fraction_of_masked_inputs > 0 )
-                PLERROR("In StackedAutoassociatorsNet::corrupt_input(): fraction_of_masked_inputs and probability_of_masked_inputs can't be both > 0");
-            if( mask_with_mean )
+                PLERROR("In StackedAutoassociatorsNet::corrupt_input():" 
+                        " fraction_of_masked_inputs and probability_of_masked_inputs can't be both > 0");
+            if( mask_with_pepper_salt )
                 for( int j=0 ; j <input.length() ; j++)
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
+                        corrupted_input[ j ] = random_gen->binomial_sample(prob_salt_noise);
+                        reconstruction_weights[j] = corrupted_data_weight;
+                    }
+                    else
+                    {
+                        corrupted_input[ j ] = input[ j ];  
+                        reconstruction_weights[j] = data_weight;
+                    }
+            else if( mask_with_mean )
+                for( int j=0 ; j <input.length() ; j++)
+                    if( random_gen->uniform_sample() < probability_of_masked_inputs )
+                    {                    
                         corrupted_input[ j ] = expectation_means[layer][ j ];
+                        reconstruction_weights[j] = corrupted_data_weight;
                         binary_mask[ j ] = 0;
                     }
                     else
+                    {
                         corrupted_input[ j ] = input[ j ];
+                        reconstruction_weights[j] = data_weight;
+                    }
             else
                 for( int j=0 ; j <input.length() ; j++)
                     if( random_gen->uniform_sample() < probability_of_masked_inputs )
                     {
-                        corrupted_input[ j ] = 0;
+                        corrupted_input[ j ] = 0;   
+                        reconstruction_weights[j] = corrupted_data_weight;
                         binary_mask[ j ] = 0;
                     }
                     else
+                    {
                         corrupted_input[ j ] = input[ j ];
-                
+                        reconstruction_weights[j] = data_weight;
+                    }
         }
         else
         {
-            random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
             corrupted_input << input;
-            if( mask_with_mean )
-                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
-                {
-                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = expectation_means[layer][autoassociator_expectation_indices[layer][j]];
-                    binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
-                }
-            else
-                for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
-                {
-                    corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
-                    binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
-                }
+            reconstruction_weights.fill(data_weight);
+            if( fraction_of_masked_inputs != 0 ) 
+            {
+                random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
+                if( mask_with_pepper_salt )
+                    for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    {
+                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = random_gen->binomial_sample(prob_salt_noise);
+                        reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
+                    }
+                else if( mask_with_mean )
+                    for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    {
+                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = expectation_means[layer][autoassociator_expectation_indices[layer][j]];
+                        reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
+                        binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
+                    }
+                else
+                    for( int j=0 ; j < round(fraction_of_masked_inputs*input.length()) ; j++)
+                    {
+                        corrupted_input[ autoassociator_expectation_indices[layer][j] ] = 0;
+                        reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
+                        binary_mask[ autoassociator_expectation_indices[layer][j] ] = 0;
+                    }
+            }
         }
-
     }
     else if( noise_type == "binary_sampling" )
     {
@@ -1362,15 +1541,124 @@
     else if( noise_type == "gaussian" )
     {
         for( int i=0; i<corrupted_input.length(); i++ )
-            corrupted_input[i] = input[i] + 
+            corrupted_input[i] = input[i] +
                 random_gen->gaussian_01() * gaussian_std;
     }
+    else if( noise_type == "missing_data")
+    {
+        // The entry input is the doubled one according to missing_data_method
+        int original_input_length = input.length() / 2;
+        reconstruction_weights.resize(original_input_length);
+   
+        if(missing_data_method == "binomial_complementary")
+        {
+            if( probability_of_masked_inputs > 0 )
+            {
+                if( fraction_of_masked_inputs > 0 )
+                    PLERROR("In StackedAutoassociatorsNet::corrupt_input():"
+                            " fraction_of_masked_inputs and probability_of_masked_inputs can't be both > 0");
+                for( int j=0 ; j<original_input_length ; j++ )
+                    if( random_gen->uniform_sample() < probability_of_masked_inputs )
+                    {
+                        corrupted_input[ j*2 ] = 0;
+                        corrupted_input[ j*2+1 ] = 0;
+                        reconstruction_weights[j] = corrupted_data_weight;
+                    }
+                    else
+                    {
+                        corrupted_input[ j*2 ] = input[ j*2 ];
+                        corrupted_input[ j*2+1] = input[ j*2+1 ];
+                        reconstruction_weights[j] = data_weight;
+                    }
+            }
+            else
+            {
+                corrupted_input << input;
+                reconstruction_weights.fill(data_weight);
+                if( fraction_of_masked_inputs != 0 )
+                {
+                    random_gen->shuffleElements(autoassociator_expectation_indices[layer]);
+                    for( int j=0 ; j < round(fraction_of_masked_inputs*original_input_length) ; j++)
+                    {
+                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 ] = 0;
+                        corrupted_input[ autoassociator_expectation_indices[layer][j]*2 + 1 ] = 0;
+                        reconstruction_weights[autoassociator_expectation_indices[layer][j]] = corrupted_data_weight;
+                    }
+                }
+            }
+        }
+        else
+            PLERROR("In StackedAutoassociatorsNet::corrupt_input(): "
+                    "missing_data_method %s not valid with noise_type %s",
+                     missing_data_method.c_str(), noise_type.c_str());
+    }
     else if( noise_type == "none" )
-        corrupted_input << input; 
+        corrupted_input << input;
     else
         PLERROR("In StackedAutoassociatorsNet::corrupt_input(): noise_type %s not valid", noise_type.c_str());
 }
 
+
+void StackedAutoassociatorsNet::double_input(const Vec& input, Vec& doubled_input, bool double_grad) const
+{
+    if( noise_type == "missing_data" )
+    {
+        if( missing_data_method == "binomial_complementary" )
+        {
+            // doubled_input[2*i] = input[i] and
+            // doubled input[2*i+1] = 1-input[i]
+            // If input is gradient that we have to double for backpropagation,
+            // (double_grad==true), then:
+            // doubled_input[2*i] = input[i] and
+            // doubled input[2*i+1] = -input[i] 
+            doubled_input.resize(input.length()*2);
+            for( int i=0; i<input.size(); i++ )
+            {
+                doubled_input[i*2] = input[i];
+                // double gradient before backpropagate 
+                // during the pre-training
+                if( double_grad )
+                    doubled_input[i*2+1] = - input[i];
+                // double input of a layer
+                else
+                    doubled_input[i*2+1] = 1 - input[i];
+            }
+        }
+        else
+            PLERROR("In StackedAutoassociatorsNet::double_input(): "
+                    "missing_data_method %s not valid",missing_data_method.c_str());
+    }
+    else
+    {
+        doubled_input.resize(input.length());
+        doubled_input << input;
+    }
+}
+
+void StackedAutoassociatorsNet::divide_input(const Vec& input, Vec& divided_input) const
+{
+    if( noise_type == "missing_data" )
+    {
+        if( missing_data_method == "binomial_complementary" )
+        {
+            // divided_input[i] = input[2*i] - input[2*i+1]
+            // even if input is the doubled_gradient
+            divided_input.resize(input.length()/2);
+            for( int i=0; i<divided_input.size(); i++)  
+                divided_input[i] = input[i*2] - input[i*2+1];
+        }
+        else
+            PLERROR("In StackedAutoassociatorsNet::divide_input(): "
+                    "missing_data_method %s not valid", missing_data_method.c_str());
+    }
+    else
+    {
+        divided_input.resize(input.length());
+        divided_input << input;
+    }
+}
+
+
 void StackedAutoassociatorsNet::greedyStep(const Vec& input, const Vec& target,
                                            int index, Vec train_costs)
 {
@@ -1416,13 +1704,15 @@
     {
         for( int i=0 ; i<index + 1; i++ )
         {
+            double_input(expectations[i], doubled_expectations[i]);
+           
             if( i == index )
             {
-                corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i );
+                corrupt_input( doubled_expectations[i], corrupted_autoassociator_expectations[i], i );
                 connections[i]->fprop( corrupted_autoassociator_expectations[i], activations[i+1] );
             }
             else
-                connections[i]->fprop( expectations[i], activations[i+1] );
+                connections[i]->fprop( doubled_expectations[i], activations[i+1] );
             
             if( i == index && greedy_target_connections.length() && greedy_target_connections[i] )
             {
@@ -1510,19 +1800,36 @@
     }
     else
     {
-        layers[ index ]->fprop( reconstruction_activations,
+        Vec divided_reconstruction_activations(reconstruction_activations.size());
+        Vec divided_reconstruction_activation_gradients(layers[ index ]->size);
+        
+        divide_input(reconstruction_activations, divided_reconstruction_activations);
+
+        layers[ index ]->fprop( divided_reconstruction_activations,
                                 layers[ index ]->expectation);
-
-        layers[ index ]->activation << reconstruction_activations;
+        layers[ index ]->activation << divided_reconstruction_activations;
         layers[ index ]->activation += layers[ index ]->bias;
         //layers[ index ]->expectation_is_up_to_date = true;
         layers[ index ]->setExpectationByRef( layers[ index ]->expectation );
-        real rec_err = layers[ index ]->fpropNLL(expectations[index]);
+        real rec_err;
+
+        // If we want to compute reconstruction error according to reconstruction weights.
+        //   rec_err = layers[ index ]->fpropNLL(expectations[index], reconstruction_weights);
+        
+        rec_err = layers[ index ]->fpropNLL(expectations[index]);
+
         train_costs[index] = rec_err;
+        layers[ index ]->bpropNLL(expectations[index], rec_err, divided_reconstruction_activation_gradients);
 
-        layers[ index ]->bpropNLL(expectations[index], rec_err,
-                                  reconstruction_activation_gradients);
+        // apply reconstruction weights which can be different for corrupted
+        // (or missing) and non corrupted data. 
+        multiply(reconstruction_weights, 
+                divided_reconstruction_activation_gradients, 
+                divided_reconstruction_activation_gradients);
 
+        double_input(divided_reconstruction_activation_gradients, 
+                    reconstruction_activation_gradients, true);   
+ 
         if(reconstruct_hidden)
         {
             Profiler::pl_profile_start("StackedAutoassociatorsNet::greedyStep reconstruct_hidden");
@@ -1557,8 +1864,8 @@
             Profiler::pl_profile_end("StackedAutoassociatorsNet::greedyStep reconstruct_hidden");
         }
 
-        layers[ index ]->update(reconstruction_activation_gradients);
-
+        layers[ index ]->update(divided_reconstruction_activation_gradients);
+        
         if(reconstruct_hidden)
             reconstruction_activation_gradients +=
                 reconstruction_activation_gradients_from_hid_rec;
@@ -1568,13 +1875,11 @@
         //                                   layers[ index ]->expectation,
         //                                   reconstruction_activation_gradients,
         //                                   reconstruction_expectation_gradients);
-
         reconstruction_connections[ index ]->bpropUpdate(
             expectations[ index + 1],
             reconstruction_activations,
             reconstruction_expectation_gradients,
             reconstruction_activation_gradients);
-
     }
 
 
@@ -1678,7 +1983,7 @@
 
     if(correlation_connections.length() != 0)
     {
-        
+
         for( int i=0 ; i<n_layers-1; i++ )
         {
             corrupt_input( expectations[i], corrupted_autoassociator_expectations[i], i);
@@ -1838,13 +2143,13 @@
     {
         for( int i=0 ; i<n_layers-1; i++ )
         {
+            double_input(expectations[i], doubled_expectations[i]);
             Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep fprop connection");
-            connections[i]->fprop( expectations[i], activations[i+1] );
+            connections[i]->fprop( doubled_expectations[i], activations[i+1] );
             Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep fprop connection");
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
     }
-
     Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep fprop");
     final_module->fprop( expectations[ n_layers-1 ],
                          final_cost_input );
@@ -1891,7 +2196,7 @@
         }
     }
     else
-    {
+    {   
         for( int i=n_layers-1 ; i>0 ; i-- )
         {
             layers[i]->bpropUpdate( activations[i],
@@ -1899,12 +2204,14 @@
                                     activation_gradients[i],
                                     expectation_gradients[i] );
 
-           Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep bpropUpdate connection");
-            connections[i-1]->bpropUpdate( expectations[i-1],
+            Profiler::pl_profile_start("StackedAutoassociatorsNet::fineTuningStep bpropUpdate connection");
+            connections[i-1]->bpropUpdate( doubled_expectations[i-1],
                                            activations[i],
-                                           expectation_gradients[i-1],
+                                           doubled_expectation_gradients[i-1],
                                            activation_gradients[i] );
-           Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep bpropUpdate connection");
+
+            Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep bpropUpdate connection");
+            divide_input( doubled_expectation_gradients[i-1], expectation_gradients[i-1] );
         }
     }
     Profiler::pl_profile_end("StackedAutoassociatorsNet::fineTuningStep bpropUpdate");
@@ -2535,7 +2842,8 @@
     {
         for(int i=0 ; i<currently_trained_layer-1 ; i++ )
         {
-            connections[i]->fprop( expectations[i], activations[i+1] );
+            double_input(expectations[i], doubled_expectations[i]);
+            connections[i]->fprop( doubled_expectations[i], activations[i+1] );
             layers[i+1]->fprop(activations[i+1],expectations[i+1]);
         }
     }
@@ -2562,8 +2870,10 @@
         }
         else
         {
+            double_input(expectations[currently_trained_layer-1], 
+                doubled_expectations[currently_trained_layer-1]);
             connections[currently_trained_layer-1]->fprop(
-                expectations[currently_trained_layer-1],
+                doubled_expectations[currently_trained_layer-1],
                 activations[currently_trained_layer] );
             layers[currently_trained_layer]->fprop(
                 activations[currently_trained_layer],
@@ -2580,7 +2890,8 @@
 {
     if(correlation_connections.length() != 0
        || currently_trained_layer!=n_layers
-       || compute_all_test_costs){
+       || compute_all_test_costs
+       || noise_type == "missing_data"){
         inherited::computeOutputs(input, output);
     }else{
         Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutputs");
@@ -2607,7 +2918,8 @@
 {
     if(correlation_connections.length() != 0 
        || currently_trained_layer!=n_layers
-       || compute_all_test_costs){
+       || compute_all_test_costs
+       || noise_type == "missing_data"){
         inherited::computeOutputsAndCosts(input, target, output, costs);
     }else{
         Profiler::pl_profile_start("StackedAutoassociatorsNet::computeOutputsAndCosts");
@@ -2688,17 +3000,23 @@
                 direct_activations );
             reconstruction_activations += direct_activations;
         }
+                
+        Vec divided_reconstruction_activations(reconstruction_activations.size());
+        divide_input(reconstruction_activations, divided_reconstruction_activations);
+
         layers[ currently_trained_layer-1 ]->fprop(
-            reconstruction_activations,
-            layers[ currently_trained_layer-1 ]->expectation);
+          divided_reconstruction_activations,
+          layers[ currently_trained_layer-1 ]->expectation);
+        
+        layers[ currently_trained_layer-1 ]->activation <<
+            divided_reconstruction_activations;
 
-        layers[ currently_trained_layer-1 ]->activation <<
-            reconstruction_activations;
         layers[ currently_trained_layer-1 ]->activation += 
             layers[ currently_trained_layer-1 ]->bias;
         //layers[ currently_trained_layer-1 ]->expectation_is_up_to_date = true;
         layers[ currently_trained_layer-1 ]->setExpectationByRef(
             layers[ currently_trained_layer-1 ]->expectation );
+
         costs[ currently_trained_layer-1 ] =
             layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
@@ -2818,6 +3136,125 @@
     final_module->setLearningRate( the_learning_rate );
 }
 
+Vec StackedAutoassociatorsNet::fantasizeKTime(int KTime, const Vec& srcImg, const Vec& sample, const Vec& maskNoiseFractOrProb)
+{
+    bool bFractOrProbUseful=false;
+
+    // Noise type that needs fraction_of_masked_inputs or prob_masked_inputs
+    if(noise_type == "masking_noise" || noise_type == "missing_data")
+        bFractOrProbUseful=true;
+
+    if(bFractOrProbUseful && maskNoiseFractOrProb.size() == 0)
+        PLERROR("In StackedAutoassociatorsNet::fantasize():"
+        "maskNoiseFractOrProb should be defined because fraction_of_masked_inputs"
+        " or prob_masked_inputs have been used during the learning stage.");
+
+    if(bFractOrProbUseful && maskNoiseFractOrProb.size() != sample.size())
+        PLERROR("In StackedAutoassociatorsNet::fantasize():"
+        "Size of maskNoiseFractOrProb should be equal to sample's size.");
+
+    if(sample.size() > n_layers-1)
+        PLERROR("In StackedAutoassociatorsNet::fantasize():"
+        " Size of sample (%i) should be <= "
+        "number of hidden layer (%i).",sample.size(), n_layers-1);
+
+    bool bFraction_masked_input = true;
+    bool autoassociator_expectation_indices_temp_initialized = false;
+
+    // Number of hidden layer to be 'covered'
+    int n_hlayers_used = sample.size();
+
+    // Save actual value
+    real old_fraction_masked_inputs = fraction_of_masked_inputs;
+    real old_prob_masked_inputs = probability_of_masked_inputs;
+    bool old_mask_input_layer_only = mask_input_layer_only;
+    int  old_nb_corrupted_layer = nb_corrupted_layer;
+
+    // New values for fantasize
+    mask_input_layer_only = false;
+    nb_corrupted_layer = n_hlayers_used;
+
+    if(bFractOrProbUseful)
+    {
+        if(old_prob_masked_inputs != 0)
+            bFraction_masked_input = false;
+        else
+            if(autoassociator_expectation_indices.size() == 0)
+            {
+                autoassociator_expectation_indices.resize( n_hlayers_used );
+                autoassociator_expectation_indices_temp_initialized = true;
+            }
+    }
+
+    expectations[0] << srcImg;
+
+    // Do fantasize k time.
+    for( int k=0 ; k<KTime ; k++ )
+    {
+        for( int i=0 ; i<n_hlayers_used; i++ )
+        {
+            // Initialisation made only at the first loop.
+            if(k == 0)
+            {
+                // initialize autoassociator_expectation_indices if not already done
+                // considering new fraction_of_masked_inputs possibly different (not
+                // equal to zero) from the one used during the training.
+                if(autoassociator_expectation_indices_temp_initialized)
+                {
+                    autoassociator_expectation_indices[i].resize( layers[i]->size );
+                    for( int j=0 ; j < autoassociator_expectation_indices[i].length() ; j++ )
+                         autoassociator_expectation_indices[i][j] = j;
+                }
+            }
+
+            if(bFractOrProbUseful)
+                if(bFraction_masked_input)
+                    fraction_of_masked_inputs = maskNoiseFractOrProb[i];
+                else
+                    probability_of_masked_inputs = maskNoiseFractOrProb[i];
+
+            double_input(expectations[i], doubled_expectations[i]);
+            corrupt_input(
+                doubled_expectations[i],
+                corrupted_autoassociator_expectations[i], i);
+            connections[i]->fprop(
+                corrupted_autoassociator_expectations[i],
+                activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+        }
+
+        for( int i=n_hlayers_used-1 ; i>=0; i-- )
+        {
+            // Binomial sample
+            if(sample[i])
+                for( int j=0; j<expectations[i+1].size(); j++ )
+                    expectations[i+1][j] =
+                    random_gen->binomial_sample(expectations[i+1][j]);
+    
+            reconstruction_connections[i]->fprop(
+                expectations[i+1],
+                reconstruction_activations );
+  
+            Vec divided_reconstruction_activations(reconstruction_activations.size());
+            divide_input(reconstruction_activations, divided_reconstruction_activations);
+
+            layers[i]->fprop(divided_reconstruction_activations, expectations[i]);
+        }
+    }
+
+    if(bFractOrProbUseful)
+    {
+        fraction_of_masked_inputs = old_fraction_masked_inputs;
+        probability_of_masked_inputs = old_prob_masked_inputs;
+    }
+
+    mask_input_layer_only = old_mask_input_layer_only;
+    nb_corrupted_layer = old_nb_corrupted_layer;
+
+    return expectations[0];
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-04-29 21:40:28 UTC (rev 10159)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2009-04-29 23:02:00 UTC (rev 10160)
@@ -157,6 +157,18 @@
     //! Type of noise that corrupts the autoassociators input
     string noise_type;
 
+    //! Method used to fill the double_input vector when using missing_data
+    //| noise type.
+    string missing_data_method;
+
+    //! Weight given to a corrupted (or missing) data when
+    //! backpropagating the gradient of reconstruction cost.
+    real corrupted_data_weight;
+
+    //! Weight given to a data (not corrupted or not missing) when
+    //! backpropagating the gradient of reconstruction cost.
+    real data_weight;
+
     //! Random fraction of the autoassociators' input components that
     //! masked, i.e. unsused to reconstruct the input.
     real fraction_of_masked_inputs;
@@ -165,13 +177,20 @@
     //! or fraction_of_masked_inputs should be > 0.
     real probability_of_masked_inputs;
 
-    //! Probability of masking the target, when using greedy_target_connections
+    //! Probability of masking the target, 
+    //! when using greedy_target_connections.
     real probability_of_masked_target;
 
-    //! Indication that inputs should be masked with the 
     //! training set mean of that component
     bool mask_with_mean;
 
+    //! Indication that inputs should be masked with 
+    //! 0 or 1 according to prop_salt_noise. 
+    bool mask_with_pepper_salt;
+
+    //! Probability that we mask the input by 1 instead of 0.
+    real prob_salt_noise;
+
     //! Standard deviation of Gaussian noise
     real gaussian_std;
 
@@ -190,6 +209,10 @@
     //! unsupervised fine tuning gradient descent
     real unsupervised_fine_tuning_decrease_ct;
 
+    //! Indicates how many layers will be corrupted during
+    //! gready layer-wise learning (starting with input layer)
+    int nb_corrupted_layer;
+
     //! Indication that only the input layer should be masked
     //! during greedy layer-wise learning
     bool mask_input_layer_only;
@@ -217,7 +240,6 @@
     //! Default constructor
     StackedAutoassociatorsNet();
 
-
     //#####  PLearner Member Functions  #######################################
 
     //! Returns the size of this learner's output, (which typically
@@ -276,6 +298,7 @@
     void onlineStep(const Mat& inputs, const Mat& targets,
                     Mat& train_costs);
 
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -305,6 +328,9 @@
     //! (at the output of the layers)
     mutable TVec<Vec> expectations;
     mutable TVec<Mat> expectations_m;
+    //! In case of missing_data: expectations doubled before corruption 
+    //! or before propagation to the next layer.
+    mutable TVec< Vec > doubled_expectations;
 
     //! Stores the gradient of the cost wrt the activations of
     //! the input and hidden layers
@@ -317,6 +343,8 @@
     //! (at the output of the layers)
     mutable TVec<Vec> expectation_gradients;
     mutable TVec<Mat> expectation_gradients_m;
+    //! Stores the gradients of the doubled version of expectations
+    mutable TVec< Vec > doubled_expectation_gradients;
 
     //! Reconstruction activations
     mutable Vec reconstruction_activations;
@@ -407,6 +435,14 @@
     //! Layers randomly masked, for unsupervised fine-tuning.
     TVec< Vec > corrupted_autoassociator_expectations;
 
+    //! Stores the weight of each data used when
+    //! backpropagating the gradient of reconstruction cost.
+    //! The weight is either corrupted_data_weight or data_weight
+    //! if the data has been corrupted or not, respectively.
+    //! Used for example to put emphasis on corrupted/missing data
+    //! during the reconstruction.
+    mutable Vec reconstruction_weights;
+
     //! Layers random binary maske, for online learning.
     TVec< Vec > binary_masks;
 
@@ -456,13 +492,30 @@
 
     void setLearningRate( real the_learning_rate );
 
-    // List of remote methods
-    Vec remote_computeOutputWithoutCorrelationConnections(const Vec& input) const;
+    void corrupt_input(const Vec& input, Vec& corrupted_input, int layer);
 
-    Mat remote_computeOutputsWithoutCorrelationConnections(const Mat& inputs) const;
+    //! Useful in case that noise_type == "missing_data", 
+    //! returns the input if it's not the case.
+    void double_input(const Vec& input, Vec& doubled_input, bool double_grad=false) const;
 
-    void corrupt_input(const Vec& input, Vec& corrupted_input, int layer);
+    //! Useful in case that noise_type == "missing_data",
+    //! returns the input if it's not the case.
+    void divide_input(const Vec& input, Vec& divided_input) const ;
 
+    //! Supposes the learner is already trained.
+    //! Allows a codage-decodage ktime from a source image. Returns the 'fantasize' image. 
+    //! You can choose how many layers to use (including raws layer) by defining the size of sample. 
+    //! You can corrupt layers differently during the codage phase by defining maskNoiseFractOrProb 
+    //! You can apply a binary sampling (1) or not (0) differently for each layer during the decode phase
+    //! Lower element in sample and maskNoiseFractOrProb correspond to lower layer. 
+    //! Example using 3 hidden layers of a learner: 
+    //!     maskNoiseFractOrProb = [0.1,0.25,0]  // noise applied on raws layer
+    //!                                          // and first hidden layer.
+    //!     sample = [1,0,0] // sampling only before reconstruction of the
+    //!                      // raws input.
+    Vec fantasizeKTime(int KTime, const Vec& srcImg, const Vec& sample,
+                        const Vec& maskNoiseFractOrProb);
+
     void corrupt_input(const Vec& input, Vec& corrupted_input, int layer, Vec& binary_mask);
 
     //! Global storage to save memory allocations.



