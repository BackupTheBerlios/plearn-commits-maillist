<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Plearn-commits] r8375 - in trunk: commands/EXPERIMENTAL	plearn/opt/EXPERIMENTAL plearn/var	plearn_learners/classifiers/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/var scripts scripts/EXPERIMENTAL
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/plearn-commits/2008-January/index.html" >
   <LINK REL="made" HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8375%20-%20in%20trunk%3A%20commands/EXPERIMENTAL%0A%09plearn/opt/EXPERIMENTAL%20plearn/var%0A%09plearn_learners/classifiers/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/var%20scripts%20scripts/EXPERIMENTAL&In-Reply-To=%3C200801032201.m03M1qC3020473%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="001822.html">
   <LINK REL="Next"  HREF="001824.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Plearn-commits] r8375 - in trunk: commands/EXPERIMENTAL	plearn/opt/EXPERIMENTAL plearn/var	plearn_learners/classifiers/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/var scripts scripts/EXPERIMENTAL</H1>
    <B>plearner at BerliOS</B> 
    <A HREF="mailto:plearn-commits%40lists.berlios.de?Subject=Re%3A%20%5BPlearn-commits%5D%20r8375%20-%20in%20trunk%3A%20commands/EXPERIMENTAL%0A%09plearn/opt/EXPERIMENTAL%20plearn/var%0A%09plearn_learners/classifiers/EXPERIMENTAL%0A%09plearn_learners/generic/EXPERIMENTAL%0A%09python_modules/plearn/var%20scripts%20scripts/EXPERIMENTAL&In-Reply-To=%3C200801032201.m03M1qC3020473%40sheep.berlios.de%3E"
       TITLE="[Plearn-commits] r8375 - in trunk: commands/EXPERIMENTAL	plearn/opt/EXPERIMENTAL plearn/var	plearn_learners/classifiers/EXPERIMENTAL	plearn_learners/generic/EXPERIMENTAL	python_modules/plearn/var scripts scripts/EXPERIMENTAL">plearner at mail.berlios.de
       </A><BR>
    <I>Thu Jan  3 23:01:52 CET 2008</I>
    <P><UL>
        <LI>Previous message: <A HREF="001822.html">[Plearn-commits] r8374 - trunk/plearn_learners/classifiers
</A></li>
        <LI>Next message: <A HREF="001824.html">[Plearn-commits] r8376 - in trunk: commands plearn/var	python_modules/plearn/pyext python_modules/plearn/pyplearn
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1823">[ date ]</a>
              <a href="thread.html#1823">[ thread ]</a>
              <a href="subject.html#1823">[ subject ]</a>
              <a href="author.html#1823">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: plearner
Date: 2008-01-03 23:01:45 +0100 (Thu, 03 Jan 2008)
New Revision: 8375

Added:
   trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc
   trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h
   trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc
   trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h
Modified:
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/plearn/var/TimesConstantVariable.cc
   trunk/plearn/var/TimesConstantVariable.h
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
   trunk/python_modules/plearn/var/Var.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
   trunk/scripts/pyplot
Log:
Experimental classes for learning with stochastic reconstruciton error (and localgaussian classifier)


Modified: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -119,6 +119,7 @@
 // #include &lt;plearn/opt/AdaptGradientOptimizer.h&gt;
 // #include &lt;plearn/opt/ConjGradientOptimizer.h&gt;
 #include &lt;plearn/opt/GradientOptimizer.h&gt;
+#include &lt;plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h&gt;
 
 // /****************
 //  * OptionOracle *
@@ -294,7 +295,7 @@
 // #include &lt;plearn/vmat/RandomSamplesVMatrix.h&gt;
 // #include &lt;plearn/vmat/RandomSamplesFromVMatrix.h&gt;
 // #include &lt;plearn/vmat/RankedVMatrix.h&gt;
-// #include &lt;plearn/vmat/RegularGridVMatrix.h&gt;
+#include &lt;plearn/vmat/RegularGridVMatrix.h&gt;
 // #include &lt;plearn/vmat/RemoveDuplicateVMatrix.h&gt;
 // #include &lt;plearn/vmat/ReorderByMissingVMatrix.h&gt;
 // //#include &lt;plearn/vmat/SelectAttributsSequenceVMatrix.h&gt;
@@ -382,10 +383,16 @@
 #include &lt;plearn/var/EXPERIMENTAL/RandomForcedValuesVariable.h&gt;
 #include &lt;plearn/var/EXPERIMENTAL/BernoulliSampleVariable.h&gt;
 #include &lt;plearn/var/EXPERIMENTAL/TimesConstantScalarVariable2.h&gt;
+#include &lt;plearn/var/PlusConstantVariable.h&gt;
+#include &lt;plearn/var/TimesConstantVariable.h&gt;
 
 // Stuff used for transformationLearner experiments
 #include &lt;plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h&gt;
 
+// Stuff used for local Gaussian classifier
+#include &lt;plearn_learners/classifiers/KNNClassifier.h&gt;
+#include &lt;plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h&gt;
+
 using namespace PLearn;
 
 int main(int argc, char** argv)

Added: trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,272 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent and Yoshua Bengio
+// Copyright (C) 1999-2002, 2006 University of Montreal
+//
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+ 
+
+/* *******************************************************      
+ * $Id: AutoScaledGradientOptimizer.cc 5852 2006-06-14 14:40:03Z larocheh $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+#define PL_LOG_MODULE_NAME &quot;AutoScaledGradientOptimizer&quot;
+
+#include &quot;AutoScaledGradientOptimizer.h&quot;
+#include &lt;plearn/io/pl_log.h&gt;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &lt;plearn/display/DisplayUtils.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    AutoScaledGradientOptimizer,
+    &quot;Optimization by gradient descent with adapted scaling for each parameter.&quot;, 
+    &quot;This is a simple variation on the basic GradientOptimizer \n&quot;
+    &quot;in which the gradient is scaled elementwise (for each parameter) \n&quot;
+    &quot;by a scaling factor that is 1 over an average of the \n&quot;
+    &quot;absolute value of the gradient plus some small epsilon. \n&quot;
+    &quot;\n&quot;
+);
+
+AutoScaledGradientOptimizer::AutoScaledGradientOptimizer():
+    learning_rate(0.),   
+    start_learning_rate(1e-2),
+    decrease_constant(0),
+    verbosity(0),
+    evaluate_scaling_every(1000),
+    evaluate_scaling_during(1000),
+    epsilon(1e-6),
+    nsteps_remaining_for_evaluation(-1)
+{}
+
+
+void AutoScaledGradientOptimizer::declareOptions(OptionList&amp; ol)
+{
+    declareOption(
+        ol, &quot;start_learning_rate&quot;, &amp;AutoScaledGradientOptimizer::start_learning_rate,
+        OptionBase::buildoption, 
+        &quot;The initial learning rate\n&quot;);
+
+    declareOption(
+        ol, &quot;learning_rate&quot;, &amp;AutoScaledGradientOptimizer::learning_rate,
+        OptionBase::learntoption, 
+        &quot;The current learning rate\n&quot;);
+
+    declareOption(
+        ol, &quot;decrease_constant&quot;, &amp;AutoScaledGradientOptimizer::decrease_constant,
+        OptionBase::buildoption, 
+        &quot;The learning rate decrease constant \n&quot;);
+
+    declareOption(
+        ol, &quot;lr_schedule&quot;, &amp;AutoScaledGradientOptimizer::lr_schedule,
+        OptionBase::buildoption, 
+        &quot;Fixed schedule instead of decrease_constant. This matrix has 2 columns: iteration_threshold \n&quot;
+        &quot;and learning_rate_factor. As soon as the iteration number goes above the iteration_threshold,\n&quot;
+        &quot;the corresponding learning_rate_factor is applied (multiplied) to the start_learning_rate to\n&quot;
+        &quot;obtain the learning_rate.\n&quot;);
+
+    declareOption(
+        ol, &quot;verbosity&quot;, &amp;AutoScaledGradientOptimizer::verbosity,
+        OptionBase::buildoption, 
+        &quot;Controls the amount of output.  If zero, does not print anything.\n&quot;
+        &quot;If 'verbosity'=V, print the current cost and learning rate if\n&quot;
+        &quot;\n&quot;
+        &quot;    stage % V == 0\n&quot;
+        &quot;\n&quot;
+        &quot;i.e. every V stages.  (Default=0)\n&quot;);
+
+    declareOption(
+        ol, &quot;evaluate_scaling_every&quot;, &amp;AutoScaledGradientOptimizer::evaluate_scaling_every,
+        OptionBase::buildoption, 
+        &quot;every how-many steps should the mean and scaling be reevaluated\n&quot;);
+
+    declareOption(
+        ol, &quot;evaluate_scaling_during&quot;, &amp;AutoScaledGradientOptimizer::evaluate_scaling_during,
+        OptionBase::buildoption, 
+        &quot;how many steps should be used to re-evaluate the mean and scaling\n&quot;);
+
+    declareOption(
+        ol, &quot;epsilon&quot;, &amp;AutoScaledGradientOptimizer::epsilon,
+        OptionBase::buildoption, 
+        &quot;scaling will be 1/(mean_abs_grad + epsilon)\n&quot;);
+
+    inherited::declareOptions(ol);
+}
+
+
+void AutoScaledGradientOptimizer::setToOptimize(const VarArray&amp; the_params, Var the_cost, VarArray the_other_costs, TVec&lt;VarArray&gt; the_other_params, real the_other_weight)
+{
+    inherited::setToOptimize(the_params, the_cost, the_other_costs, the_other_params, the_other_weight);
+    int n = params.nelems();
+    param_values = Vec(n);
+    param_gradients = Vec(n);
+    params.makeSharedValue(param_values);
+    params.makeSharedGradient(param_gradients);
+    scaling.resize(n);
+    scaling.clear();
+    if(epsilon&lt;0)
+        scaling.fill(1.0);
+    meanabsgrad.resize(n);
+    meanabsgrad.clear();
+}
+
+
+// static bool displayvg=false;
+
+bool AutoScaledGradientOptimizer::optimizeN(VecStatsCollector&amp; stats_coll) 
+{
+    PLASSERT_MSG(other_costs.length()==0, &quot;gradient on other costs not currently supported&quot;);
+
+    param_gradients.clear();
+
+    int stage_max = stage + nstages; // the stage to reach
+
+    int current_schedule = 0;
+    int n_schedules = lr_schedule.length();
+    if (n_schedules&gt;0)
+        while (current_schedule+1 &lt; n_schedules &amp;&amp; stage &gt; lr_schedule(current_schedule,0))
+            current_schedule++;
+    
+    while (stage &lt; stage_max) 
+    {        
+        if (n_schedules&gt;0)
+        {
+            while (current_schedule+1 &lt; n_schedules &amp;&amp; stage &gt; lr_schedule(current_schedule,0))
+                current_schedule++;
+            learning_rate = start_learning_rate * lr_schedule(current_schedule,1);
+        }
+        else
+            learning_rate = start_learning_rate/(1.0+decrease_constant*stage);
+
+        proppath.clearGradient();
+        cost-&gt;gradient[0] = 1.0;
+
+        static bool display_var_graph_before_fbprop=false;
+        if (display_var_graph_before_fbprop)
+            displayVarGraph(proppath, true, 333);
+        proppath.fbprop(); 
+#ifdef BOUNDCHECK
+        int np = params.size();
+        for(int i=0; i&lt;np; i++)
+            if (params[i]-&gt;value.hasMissing())
+                PLERROR(&quot;parameter updated with NaN&quot;);
+#endif
+        static bool display_var_graph=false;
+        if (display_var_graph)
+            displayVarGraph(proppath, true, 333);
+
+//       // Debugging of negative NLL bug...
+//       if (cost-&gt;value[0] &lt;= 0) {
+//         displayVarGraph(proppath, true, 333);
+//         cerr &lt;&lt; &quot;Negative NLL cost vector = &quot; &lt;&lt; cost &lt;&lt; endl;
+//         PLERROR(&quot;Negative NLL encountered in optimization&quot;);
+//       }
+
+        // set params += -learning_rate * params.gradient * scaling
+        {
+        real* p_val = param_values.data();
+        real* p_grad = param_gradients.data();
+        real* p_scale = scaling.data();
+        real neg_learning_rate = -learning_rate;
+
+        int n = param_values.length();
+        while(n--)
+            *p_val++ += neg_learning_rate*(*p_grad++)*(*p_scale++);
+        }
+
+        if(stage%evaluate_scaling_every==0)
+        {
+            nsteps_remaining_for_evaluation = evaluate_scaling_during;
+            meanabsgrad.clear();
+            if(verbosity&gt;=4)
+                perr &lt;&lt; &quot;At stage &quot; &lt;&lt; stage &lt;&lt; &quot; beginning evaluating meanabsgrad during &quot; &lt;&lt; evaluate_scaling_during &lt;&lt; &quot; stages&quot; &lt;&lt; endl;
+        }
+
+        if(nsteps_remaining_for_evaluation&gt;0)
+        {
+            real* p_grad = param_gradients.data();
+            real* p_mean = meanabsgrad.data();
+            int n = param_gradients.length();
+            while(n--)
+                *p_mean++ += fabs(*p_grad++);
+            --nsteps_remaining_for_evaluation;
+
+            if(nsteps_remaining_for_evaluation==0) // finalize evaluation
+            {
+                int n = param_gradients.length();                
+                for(int i=0; i&lt;n; i++)
+                {
+                    meanabsgrad[i] /= evaluate_scaling_during;
+                    scaling[i] = 1.0/(meanabsgrad[i]+epsilon);
+                }
+                if(verbosity&gt;=4)
+                    perr &lt;&lt; &quot;At stage &quot; &lt;&lt; stage 
+                         &lt;&lt; &quot; finished evaluating meanabsgrad. It's in range: ( &quot; 
+                         &lt;&lt; min(meanabsgrad) &lt;&lt; &quot;,  &quot; &lt;&lt; max(meanabsgrad) &lt;&lt; &quot; )&quot; &lt;&lt; endl;
+                if(verbosity&gt;=5)
+                    perr &lt;&lt; meanabsgrad &lt;&lt; endl;
+
+                if(epsilon&lt;0)
+                    scaling.fill(1.0);
+            }
+        }
+        param_gradients.clear();
+
+        if (verbosity &gt; 0 &amp;&amp; stage % verbosity == 0) {
+            MODULE_LOG &lt;&lt; &quot;Stage &quot; &lt;&lt; stage &lt;&lt; &quot;: &quot; &lt;&lt; cost-&gt;value
+                       &lt;&lt; &quot;\tlr=&quot; &lt;&lt; learning_rate
+                       &lt;&lt; endl;
+        }
+        stats_coll.update(cost-&gt;value);
+        ++stage;
+    }
+
+    return false;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h
===================================================================
--- trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/opt/EXPERIMENTAL/AutoScaledGradientOptimizer.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,141 @@
+// -*- C++ -*-
+
+// PLearn (A C++ Machine Learning Library)
+// Copyright (C) 1998 Pascal Vincent
+// Copyright (C) 1999-2002 Pascal Vincent and Yoshua Bengio
+// Copyright (C) 1999-2002, 2006 University of Montreal
+//
+
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: AutoScaledGradientOptimizer.h 8247 2007-11-12 20:22:12Z nouiz $
+ * This file is part of the PLearn library.
+ ******************************************************* */
+
+
+/*! \file AutoScaledGradientOptimizer.h */
+
+#ifndef AutoScaledGradientOptimizer_INC
+#define AutoScaledGradientOptimizer_INC
+
+#include &lt;plearn/opt/Optimizer.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+
+class AutoScaledGradientOptimizer : public Optimizer
+{
+    typedef Optimizer inherited;
+      
+public:
+
+    //!  gradient descent specific parameters
+    //!  (directly modifiable by the user)
+    real learning_rate; // current learning rate
+
+    // Options (also available through setOption)
+    real start_learning_rate;
+    real decrease_constant;
+
+    // optionally the user can instead of using the decrease_constant
+    // use a fixed schedule. This matrix has 2 columns: iteration_threshold and learning_rate_factor
+    // As soon as the iteration number goes above the iteration_threshold, the corresponding learning_rate_factor
+    // is applied (multiplied) to the start_learning_rate to obtain the learning_rate.
+    Mat lr_schedule;
+
+    int verbosity;
+
+    // every how-many steps should the mean and scaling be reevaluated
+    int evaluate_scaling_every;
+    // how many steps should be used to re-evaluate the mean and scaling
+    int evaluate_scaling_during; 
+    // scaling will be 1/(mean_abs_grad + epsilon)
+    real epsilon;
+
+    AutoScaledGradientOptimizer();
+
+    PLEARN_DECLARE_OBJECT(AutoScaledGradientOptimizer);
+
+    virtual void setToOptimize(const VarArray&amp; the_params, Var the_cost, VarArray the_other_costs = VarArray(0), TVec&lt;VarArray&gt; the_other_params = TVec&lt;VarArray&gt;(0), real the_other_weight = 1);
+
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+    { inherited::makeDeepCopyFromShallowCopy(copies); }
+
+    virtual void build()
+    {
+        inherited::build();
+        build_();
+    }
+
+protected:
+    Vec scaling; // by how much to multiply the gradient before performing an update
+    Vec meanabsgrad; // the mean absolute value of the gradient computed for 
+    int nsteps_remaining_for_evaluation;
+
+    // Vecs pointing to the value and graident of parameters (setup with the makeSharedValue and makeShared Gradient hack)
+    Vec param_values;
+    Vec param_gradients;
+
+private:
+    void build_()
+    {}
+    
+public:
+
+    // virtual void oldwrite(ostream&amp; out) const;
+    // virtual void oldread(istream&amp; in);
+    //virtual real optimize();
+    virtual bool optimizeN(VecStatsCollector&amp; stats_coll);
+
+protected:
+
+    static void declareOptions(OptionList&amp; ol);
+};
+
+DECLARE_OBJECT_PTR(AutoScaledGradientOptimizer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/var/TimesConstantVariable.cc
===================================================================
--- trunk/plearn/var/TimesConstantVariable.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/var/TimesConstantVariable.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -57,6 +57,14 @@
     : inherited(input, input-&gt;length(), input-&gt;width()), cst(c) 
 {}
 
+void
+
+TimesConstantVariable::declareOptions(OptionList &amp;ol)
+{
+    declareOption(ol, &quot;cst&quot;, &amp;TimesConstantVariable::cst, OptionBase::buildoption, &quot;&quot;);
+    inherited::declareOptions(ol);
+}
+
 void TimesConstantVariable::recomputeSize(int&amp; l, int&amp; w) const
 {
     if (input) {

Modified: trunk/plearn/var/TimesConstantVariable.h
===================================================================
--- trunk/plearn/var/TimesConstantVariable.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn/var/TimesConstantVariable.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -69,6 +69,8 @@
 
     PLEARN_DECLARE_OBJECT(TimesConstantVariable);
 
+    static void declareOptions(OptionList &amp;ol);
+
     virtual string info() const
     { return string(&quot;TimesConstant (* &quot;)+tostring(cst)+&quot;)&quot;; }
 

Added: trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,407 @@
+// -*- C++ -*-
+
+// LocalGaussianClassifier.cc
+//
+// Copyright (C) 2007 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file LocalGaussianClassifier.cc */
+
+
+#include &quot;LocalGaussianClassifier.h&quot;
+#include &lt;plearn/math/TMat_maths.h&gt;
+#include &lt;plearn/math/distr_maths.h&gt;
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LocalGaussianClassifier,
+    &quot;ONE LINE DESCRIPTION&quot;,
+    &quot;MULTI-LINE \nHELP&quot;);
+
+LocalGaussianClassifier::LocalGaussianClassifier()
+    :nclasses(-1),
+     computation_neighbors(-1),
+     kernel_sigma(0.1),
+     regularization_sigma(1e-6),
+     ignore_weights_below(1e-8),
+     minus_one_half_over_kernel_sigma_square(0),
+     traintarget_ptr(0),
+     trainweight_ptr(0)
+{
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+
+    // ### If this learner needs to generate random numbers, uncomment the
+    // ### line below to enable the use of the inherited PRandom object.
+    // random_gen = new PRandom();
+}
+
+void LocalGaussianClassifier::declareOptions(OptionList&amp; ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the &quot;flags&quot; of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, &quot;myoption&quot;, &amp;LocalGaussianClassifier::myoption,
+    //               OptionBase::buildoption,
+    //               &quot;Help text describing this option&quot;);
+    // ...
+
+    declareOption(ol, &quot;nclasses&quot;, &amp;LocalGaussianClassifier::nclasses, OptionBase::buildoption,
+                  &quot;The number of different classes.\n&quot;
+                  &quot;Note that the 'target' part of trining set samples must be an integer\n&quot;
+                  &quot;with values between 0 and nclasses-1.\n&quot;);
+
+    declareOption(ol, &quot;computation_neighbors&quot;, &amp;LocalGaussianClassifier::computation_neighbors, OptionBase::buildoption,
+                  &quot;This indicates to how many neighbors we should restrict ourselves for the computation\n&quot;
+                  &quot;of the covariance matrix only (since they are much cheaper, weight and mean are always\n&quot;
+                  &quot;computed using all points.)\n&quot;
+                  &quot;If =0 we do not compute a covariance matrix (i.e. use a spherical cov. of width regularization_sigma).\n&quot;
+                  &quot;If &lt;0 we use all training points (with an appropriate weight).\n&quot;
+                  &quot;If &gt;1 we consider only that many neighbors of the test point;\n&quot;
+                  &quot;If between 0 and 1, it's considered a coefficient by which to multiply\n&quot;
+                  &quot;the square root of the number of training points, to yield the actual \n&quot;
+                  &quot;number of computation neighbors used&quot;);
+
+    declareOption(ol, &quot;kernel_sigma&quot;, &amp;LocalGaussianClassifier::kernel_sigma, OptionBase::buildoption,
+                  &quot;The sigma (standard deviation) of the weighting Gaussian Kernel\n&quot;);
+
+    declareOption(ol, &quot;regularization_sigma&quot;, &amp;LocalGaussianClassifier::regularization_sigma, OptionBase::buildoption,
+                  &quot;This quantity squared is added to the diagonal of the local empirical covariance matrices.\n&quot;);
+
+    declareOption(ol, &quot;ignore_weights_below&quot;, &amp;LocalGaussianClassifier::ignore_weights_below, OptionBase::buildoption,
+                  &quot;minimal weight below which we ignore the point (i.e. consider the weight is 0)\n&quot;);
+
+    declareOption(ol, &quot;train_set&quot;, &amp;LocalGaussianClassifier::train_set, OptionBase::learntoption,
+                  &quot;We need to store the training set, as this learner is memory-based...&quot;);
+
+    /*
+    declareOption(ol, &quot;NN&quot;, &amp;LocalGaussianClassifier::NN, OptionBase::learntoption,
+                  &quot;The nearest neighbor algorithm used to find nearest neighbors&quot;);
+    */
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void LocalGaussianClassifier::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a &quot;reloaded&quot; object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or &quot;re-building&quot; of an object after a few &quot;tuning&quot;
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+    // PLASSERT(weighting_kernel.isNotNull());
+
+    if(train_set.isNotNull())
+        setTrainingSet(train_set, false);
+    
+    minus_one_half_over_kernel_sigma_square = -0.5/(kernel_sigma*kernel_sigma);
+}
+
+// ### Nothing to add here, simply calls build_
+void LocalGaussianClassifier::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void LocalGaussianClassifier::makeDeepCopyFromShallowCopy(CopiesMap&amp; copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all &quot;pointer-like&quot; fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+    // deepCopyField(weighting_kernel, copies);
+    // deepCopyField(NN, copies);
+}
+
+
+int LocalGaussianClassifier::outputsize() const
+{
+    return nclasses;
+}
+
+void LocalGaussianClassifier::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+}
+
+void LocalGaussianClassifier::train()
+{
+    // The role of the train method is to bring the learner up to
+    // stage==nstages, updating train_stats with training costs measured
+    // on-line in the process.
+
+    /* TYPICAL CODE:
+
+    static Vec input;  // static so we don't reallocate memory each time...
+    static Vec target; // (but be careful that static means shared!)
+    input.resize(inputsize());    // the train_set's inputsize()
+    target.resize(targetsize());  // the train_set's targetsize()
+    real weight;
+
+    // This generic PLearner method does a number of standard stuff useful for
+    // (almost) any learner, and return 'false' if no training should take
+    // place. See PLearner.h for more details.
+    if (!initTrain())
+        return;
+
+    while(stage&lt;nstages)
+    {
+        // clear statistics of previous epoch
+        train_stats-&gt;forget();
+
+        //... train for 1 stage, and update train_stats,
+        // using train_set-&gt;getExample(input, target, weight)
+        // and train_stats-&gt;update(train_costs)
+
+        ++stage;
+        train_stats-&gt;finalize(); // finalize statistics for this epoch
+    }
+    */
+}
+
+void LocalGaussianClassifier::setTrainingSet(VMat training_set, bool call_forget)
+{
+    inherited::setTrainingSet(training_set, call_forget);
+    
+    // int l = train_set.length();
+    int is = inputsize();
+    int ts = targetsize();
+    PLASSERT(ts==1);
+    int ws = weightsize();
+    PLASSERT(ws==0 || ws==1);
+    trainsample.resize(is+ts+ws);
+    traininput = trainsample.subVec(0,is);
+    traintarget_ptr = &amp;trainsample[is];
+    trainweight_ptr = NULL;
+    if(ws==1)
+        trainweight_ptr = &amp;trainsample[is+ts];
+    
+    log_counts.resize(nclasses);
+    log_counts2.resize(nclasses);
+    means.resize(nclasses, is);
+    allcovars.resize(nclasses*is, is);
+    covars.resize(nclasses);
+    for(int c=0; c&lt;nclasses; c++)
+        covars[c] = allcovars.subMatRows(c*is, is);
+}
+
+real LocalGaussianClassifier::computeLogWeight(const Vec&amp; input, const Vec&amp; traininput) const
+{
+    return powdistance(input, traininput, 2.0, true)*minus_one_half_over_kernel_sigma_square;
+}
+
+void LocalGaussianClassifier::computeOutput(const Vec&amp; input, Vec&amp; output) const
+{
+    int l = train_set.length();
+    PLASSERT(input.length()==inputsize());
+
+    int K = 0;
+    if(computation_neighbors&gt;1)
+        K = int(computation_neighbors);
+    else if(computation_neighbors&gt;0)
+        K = int(computation_neighbors*sqrt(l));
+    else if(computation_neighbors&lt;0)
+        K = l;
+    if(K&gt;l)
+        K = l;
+
+    pqvec.resize(K+1);
+    pair&lt;real,int&gt;* pq = pqvec.begin();
+    int pqsize = 0;
+    
+    log_counts.fill(-FLT_MAX);
+    if(K&gt;0)
+        log_counts2.fill(-FLT_MAX);
+        
+    if(verbosity&gt;=3)
+        perr &lt;&lt; &quot;______________________________________&quot; &lt;&lt; endl;
+    means.clear();
+    real ignore_log_weights_below = pl_log(ignore_weights_below);
+
+    for(int i=0; i&lt;l; i++)
+    {
+        train_set-&gt;getRow(i,trainsample);
+        real log_w = computeLogWeight(input, traininput);
+        if(trainweight_ptr) 
+            log_w += pl_log(*trainweight_ptr);
+        if(log_w&gt;=ignore_log_weights_below)
+        {
+            if(K&gt;0)
+                {
+                    real d = -log_w;
+                    if(pqsize&lt;K)
+                    {
+                        pq[pqsize++] = pair&lt;real,int&gt;(d,i);
+                        if(K&lt;l) // need to maintain heap structure only if K&lt;l
+                            push_heap(pq,pq+pqsize);
+                    }
+                    else if(d&lt;pq-&gt;first)
+                    {
+                        pop_heap(pq,pq+pqsize);
+                        pq[pqsize-1] = pair&lt;real,int&gt;(d,i);
+                        push_heap(pq,pq+pqsize);
+                    }
+                }
+            int c = int(*traintarget_ptr);
+            real lcc = log_counts[c];
+            log_counts[c] = (lcc&lt;ignore_log_weights_below ?log_w :logadd(lcc, log_w));
+            multiplyAcc(means(c), traininput, exp(log_w));
+        }
+    }
+
+    if(verbosity&gt;=3)
+        perr &lt;&lt; &quot;log_counts: &quot; &lt;&lt; log_counts &lt;&lt; endl;
+
+    for(int c=0; c&lt;nclasses; c++)
+        if(log_counts[c]&gt;=ignore_log_weights_below)
+            means(c) *= exp(-log_counts[c]);
+
+    allcovars.fill(0.);
+    if(K&gt;0) // compute covars?
+    {
+        for(int k=0; k&lt;pqsize; k++)
+        {
+            int i = pq[k].second;
+            real log_w = -pq[k].first;
+            train_set-&gt;getRow(i,trainsample);
+            int c = int(*traintarget_ptr);
+            real lcc = log_counts2[c];
+            log_counts2[c] = (lcc&lt;ignore_log_weights_below ?log_w :logadd(lcc, log_w));
+            traininput -= means(c);
+            externalProductScaleAcc(covars[c], traininput, traininput, exp(log_w));
+        }
+        
+        for(int c=0; c&lt;nclasses; c++)
+            if(log_counts2[c]&gt;=ignore_log_weights_below)
+                covars[c] *= exp(-log_counts2[c]);
+    if(verbosity&gt;=3)
+        perr &lt;&lt; &quot;log_counts2: &quot; &lt;&lt; log_counts2 &lt;&lt; endl;
+    }
+
+    output.resize(nclasses);
+    output.clear();
+
+    for(int c=0; c&lt;nclasses; c++)
+    {
+        if(log_counts[c]&lt;ignore_log_weights_below)
+            output[c] = -FLT_MAX;
+        else
+        {
+            Mat cov = covars[c];
+            addToDiagonal(cov, square(regularization_sigma));
+            real log_p_x = logOfNormal(input, means(c), cov);
+            output[c] = log_p_x + log_counts[c];
+            if(verbosity&gt;=4)
+            {
+                perr &lt;&lt; &quot;** Class &quot; &lt;&lt; c &lt;&lt; &quot; **&quot; &lt;&lt; endl;
+                perr &lt;&lt; &quot;log_p_x: &quot; &lt;&lt; log_p_x &lt;&lt; endl;
+                perr &lt;&lt; &quot;log_count: &quot; &lt;&lt; log_counts[c] &lt;&lt; endl;
+                perr &lt;&lt; &quot;mean: &quot; &lt;&lt; means(c) &lt;&lt; endl;
+                perr &lt;&lt; &quot;regularized covar: \n&quot; &lt;&lt; cov &lt;&lt; endl;
+            }
+        }
+    }
+    if(verbosity&gt;=2)
+    {
+        perr &lt;&lt; &quot;Scores: &quot; &lt;&lt; output &lt;&lt; endl;
+        perr &lt;&lt; &quot;argmax: &quot; &lt;&lt; argmax(output) &lt;&lt; endl;
+    }
+}
+
+void LocalGaussianClassifier::computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                           const Vec&amp; target, Vec&amp; costs) const
+{
+    costs.resize(2);
+    int c = int(target[0]);
+    costs[0] = (argmax(output)==c ?0.0 :1.0);
+    costs[1] = logadd(output)-output[c];
+}
+
+TVec&lt;string&gt; LocalGaussianClassifier::getTestCostNames() const
+{
+    TVec&lt;string&gt; names(2);
+    names[0] = &quot;class_error&quot;;
+    names[1] = &quot;NLL&quot;;
+    return names;
+}
+
+TVec&lt;string&gt; LocalGaussianClassifier::getTrainCostNames() const
+{
+    TVec&lt;string&gt; names;
+    return names;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/LocalGaussianClassifier.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -0,0 +1,231 @@
+// -*- C++ -*-
+
+// LocalGaussianClassifier.h
+//
+// Copyright (C) 2007 Pascal Vincent
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Vincent
+
+/*! \file LocalGaussianClassifier.h */
+
+
+#ifndef LocalGaussianClassifier_INC
+#define LocalGaussianClassifier_INC
+
+#include &lt;plearn_learners/generic/PLearner.h&gt;
+
+// From C++ stdlib
+#include &lt;utility&gt;                           //!&lt; for pair
+#include &lt;algorithm&gt;                         //!&lt; for push_heap
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See <A HREF="http://www.doxygen.org/manual.html">http://www.doxygen.org/manual.html</A>
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class LocalGaussianClassifier : public PLearner
+{
+    typedef PLearner inherited;
+    
+
+protected:
+
+    // Vec emptyvec;
+    // mutable Vec NN_outputs;
+    // mutable Vec NN_costs;
+
+    // *********************
+    // * protected options *
+    // *********************
+
+    // PP&lt;GenericNearestNeighbors&gt; NN;
+
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int nclasses;
+    real computation_neighbors;
+    real kernel_sigma;
+    real regularization_sigma;
+    real ignore_weights_below;
+
+private:
+    real minus_one_half_over_kernel_sigma_square;
+
+    //! Global storage to save memory allocations.
+    Vec trainsample;
+    Vec traininput;
+    real* traintarget_ptr;
+    real* trainweight_ptr;
+    mutable TVec&lt; pair&lt;real,int&gt; &gt; pqvec; // priority queue (heap) vector 
+
+    Vec log_counts;  // the log_counts considering all points
+    Vec log_counts2; // the log_counts considering only the points kept for computing the covariance
+    Mat means;    
+    Mat allcovars;
+    TVec&lt;Mat&gt; covars;
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    LocalGaussianClassifier();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    virtual void setTrainingSet(VMat training_set, bool call_forget=true);
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec&amp; input, Vec&amp; output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec&amp; input, const Vec&amp; output,
+                                         const Vec&amp; target, Vec&amp; costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTestCostNames() const;
+
+    real computeLogWeight(const Vec&amp; input, const Vec&amp; traininput) const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec&lt;std::string&gt; getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec&amp; input, const Vec&amp; target,
+    //                                    Vec&amp; output, Vec&amp; costs) const;
+    // virtual void computeCostsOnly(const Vec&amp; input, const Vec&amp; target,
+    //                               Vec&amp; costs) const;
+    // virtual void test(VMat testset, PP&lt;VecStatsCollector&gt; test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(LocalGaussianClassifier);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap&amp; copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList&amp; ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LocalGaussianClassifier);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:&quot;stroustrup&quot;
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.cc	2008-01-03 22:01:45 UTC (rev 8375)
@@ -69,7 +69,6 @@
     // ### You can also combine flags, for example with OptionBase::nosave:
     // ### (OptionBase::buildoption | OptionBase::nosave)
 
-    
     declareOption(ol, &quot;unsupervised_nepochs&quot;, &amp;DeepReconstructorNet::unsupervised_nepochs,
                   OptionBase::buildoption,
                   &quot;unsupervised_nepochs[k] contains a pair of integers giving the minimum and\n&quot;
@@ -98,6 +97,10 @@
                   OptionBase::buildoption,
                   &quot;recontruction_costs[k] is the reconstruction cost for layer[k]&quot;);
 
+    declareOption(ol, &quot;reconstruction_costs_names&quot;, &amp;DeepReconstructorNet::reconstruction_costs_names,
+                  OptionBase::buildoption,
+                  &quot;The names to be given to each of the elements of a vector cost&quot;);
+
     declareOption(ol, &quot;reconstructed_layers&quot;, &amp;DeepReconstructorNet::reconstructed_layers,
                   OptionBase::buildoption,
                   &quot;reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]&quot;);
@@ -617,13 +620,9 @@
         reconstruction_optimizer-&gt;reset();    
     }
 
-    TVec&lt;string&gt; colnames(4);
-    colnames[0] = &quot;nepochs&quot;;
-    colnames[1] = &quot;reconstr_cost&quot;;
-    colnames[2] = &quot;stderror&quot;;
-    colnames[3] = &quot;relative_improvement&quot;;
-    VMat training_curve = new FileVMatrix(expdir/&quot;training_costs_layer_&quot;+tostring(which_input_layer+1)+&quot;.pmat&quot;,0,colnames);
-    Vec costrow(4);
+    Vec costrow;
+    TVec&lt;string&gt; colnames;
+    VMat training_curve;
 
     VecStatsCollector st;
     real prev_mean = -1;
@@ -641,19 +640,49 @@
             reconstruction_optimizer-&gt;nstages = l/minibatch_size;
             reconstruction_optimizer-&gt;optimizeN(st);
         }        
-        const StatsCollector&amp; s = st.getStats(0);
-        real m = s.mean();
-        real er = s.stderror();
-        perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n+1 &lt;&lt; &quot; mean error: &quot; &lt;&lt; m &lt;&lt; &quot; +- &quot; &lt;&lt; s.stderror() &lt;&lt; endl;
-        if(prev_mean&gt;0)
+        int reconstr_cost_pos = 0;
+
+        Vec means = st.getMean();
+        Vec stderrs = st.getStdError();
+        perr &lt;&lt; &quot;Epoch &quot; &lt;&lt; n+1 &lt;&lt; &quot;: &quot; &lt;&lt; means &lt;&lt; &quot; +- &quot; &lt;&lt; stderrs;
+        real m = means[reconstr_cost_pos];
+        real er = stderrs[reconstr_cost_pos];
+        if(n&gt;0)
         {
-            relative_improvement = (prev_mean-m)/prev_mean;
-            perr &lt;&lt; &quot;Relative improvement: &quot; &lt;&lt; relative_improvement*100 &lt;&lt; &quot; %&quot;&lt;&lt; endl;
+            relative_improvement = (prev_mean-m)/fabs(prev_mean);
+            perr &lt;&lt; &quot;  improvement: &quot; &lt;&lt; relative_improvement*100 &lt;&lt; &quot; %&quot;;
         }
+        perr &lt;&lt; endl;
+
+        int ncosts = means.length();
+        if(reconstruction_costs_names.length()!=ncosts)
+        {
+            reconstruction_costs_names.resize(ncosts);
+            for(int k=0; k&lt;ncosts; k++)
+                reconstruction_costs_names[k] = &quot;cost&quot;+tostring(k);
+        }
+
+        if(colnames.length()==0)
+        {
+            colnames.append(&quot;nepochs&quot;);
+            colnames.append(&quot;relative_improvement&quot;);
+            for(int k=0; k&lt;ncosts; k++)
+            {
+                colnames.append(reconstruction_costs_names[k]+&quot;_mean&quot;);
+                colnames.append(reconstruction_costs_names[k]+&quot;_stderr&quot;);
+            }
+            training_curve = new FileVMatrix(expdir/&quot;training_costs_layer_&quot;+tostring(which_input_layer+1)+&quot;.pmat&quot;,0,colnames);
+        }
+
+        costrow.resize(colnames.length());
+        int k=0;
         costrow[0] = n+1;
-        costrow[1] = m;
-        costrow[2] = er;
-        costrow[3] = relative_improvement*100;
+        costrow[1] = relative_improvement*100;
+        for(int k=0; k&lt;ncosts; k++)
+        {
+            costrow[2+k*2] = means[k];
+            costrow[2+k*2+1] = stderrs[k];
+        }
         training_curve-&gt;appendRow(costrow);
         training_curve-&gt;flush();
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h	2008-01-03 22:01:45 UTC (rev 8375)
@@ -69,7 +69,6 @@
     //! ### declare public option fields (such as build options) here
     //! Start your comments with Doxygen-compatible comments such as //!
     
-
     TVec&lt; pair&lt;int,int&gt; &gt; unsupervised_nepochs;
     Vec unsupervised_min_improvement_rate;
 
@@ -82,6 +81,9 @@
 
     // reconstruction_costs[k] is the reconstruction cost for layers[k]
     VarArray reconstruction_costs;
+    
+    // The names to be given to each of the elements of a vector cost 
+    TVec&lt;string&gt; reconstruction_costs_names;
 
     // reconstructed_layers[k] is the reconstruction of layer k from layers[k+1]
     VarArray reconstructed_layers;

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/python_modules/plearn/var/Var.py	2008-01-03 22:01:45 UTC (rev 8375)
@@ -77,6 +77,12 @@
     def sigmoid(self):
         return Var(pl.SigmoidVariable(input=self.v))
 
+    def sigmoidInRange(self, minp=0.0, maxp=1.0):
+        if minp==0.0 and maxp==1.0:
+            return self.sigmoid()
+        else:
+            return (self*(maxp-minp)+minp).sigmoid()
+
     def tanh(self):
         return Var(pl.TanhVariable(input=self.v))
 
@@ -160,7 +166,10 @@
         return Var(pl.SquareVariable(input=self.v))
 
     def add(self, other):
-        return Var(pl.PlusVariable(input1=self.v, input2=other.v))
+        if type(other) in (int, float):
+            return Var(pl.PlusConstantVariable(input=self.v, cst=other))
+        else:
+            return Var(pl.PlusVariable(input1=self.v, input2=other.v))
 
     def classificationLoss(self, class_index):
         return Var(pl.ClassificationLossVariable(input1=self.v, input2=class_index.v))
@@ -171,6 +180,12 @@
     def __sub__(self, other):
         return Var(pl.MinusVariable(input1=self.v,input2=other.v))
 
+    def __mul__(self, other):
+        if type(other) in (int, float):
+            return Var(pl.TimesConstantVariable(input=self.v, cst=other))
+        else:
+            raise NotImplementedError
+
     def neg(self):
         return Var(pl.NegateElementsVariable(input=self.v))
 

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2008-01-03 22:01:45 UTC (rev 8375)
@@ -310,7 +310,7 @@
             # reconstruction -- r
                 
             elif char == 'r':
-                if i&lt;self.size()-2:
+                if i&lt;self.size()-1:
                     print 'reconstructing...'
                     self.__reconstructLayer(i)
                     self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 

Modified: trunk/scripts/pyplot
===================================================================
--- trunk/scripts/pyplot	2008-01-03 19:24:52 UTC (rev 8374)
+++ trunk/scripts/pyplot	2008-01-03 22:01:45 UTC (rev 8375)
@@ -42,7 +42,11 @@
 import os
 import sys
 import time
-from fpconst import isNaN
+try:
+    from fpconst import isNaN
+except ImportError:
+    pass
+
 from plearn.io.server import *
 from plearn.plotting import *
 


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="001822.html">[Plearn-commits] r8374 - trunk/plearn_learners/classifiers
</A></li>
	<LI>Next message: <A HREF="001824.html">[Plearn-commits] r8376 - in trunk: commands plearn/var	python_modules/plearn/pyext python_modules/plearn/pyplearn
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#1823">[ date ]</a>
              <a href="thread.html#1823">[ thread ]</a>
              <a href="subject.html#1823">[ subject ]</a>
              <a href="author.html#1823">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/plearn-commits">More information about the Plearn-commits
mailing list</a><br>
</body></html>
