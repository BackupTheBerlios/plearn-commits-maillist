From yoshua at mail.berlios.de  Wed Aug  1 00:55:32 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 1 Aug 2007 00:55:32 +0200
Subject: [Plearn-commits] r7887 - in trunk/python_modules/plearn:
	learners/autolr pyext vmat
Message-ID: <200707312255.l6VMtWWL027812@sheep.berlios.de>

Author: yoshua
Date: 2007-08-01 00:55:27 +0200 (Wed, 01 Aug 2007)
New Revision: 7887

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/pyext/__init__.py
   trunk/python_modules/plearn/vmat/__init__.py
Log:
defining len() (i.e. method __len__ for AutoVMatrix class)


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-31 19:51:48 UTC (rev 7886)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-07-31 22:55:27 UTC (rev 7887)
@@ -281,7 +281,9 @@
 learning rate.
 If call_forget then the provided initial_learner is changed (and not
 necessarily the optimal one) upon return. But if not call_forget then the
-initial_learner is unchanged (we make deep copies internally).
+initial_learner is unchanged (we make deep copies internally). However, if
+note call_forget, then deep-copies are made and the best learner is returned
+(3rd element of returned tuple).
 """
     log_initial_lr=log(initial_lr)
     log_steps=log(lr_steps)

Modified: trunk/python_modules/plearn/pyext/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyext/__init__.py	2007-07-31 19:51:48 UTC (rev 7886)
+++ trunk/python_modules/plearn/pyext/__init__.py	2007-07-31 22:55:27 UTC (rev 7887)
@@ -78,6 +78,8 @@
         sys.exit()
     loggingControl(verb, logs)
 
+pl.AutoVMatrix()
+AutoVMatrix.__len__ = lambda self: self.length
 
 if __name__ == "__main__":
     class A(plargs):

Modified: trunk/python_modules/plearn/vmat/__init__.py
===================================================================
--- trunk/python_modules/plearn/vmat/__init__.py	2007-07-31 19:51:48 UTC (rev 7886)
+++ trunk/python_modules/plearn/vmat/__init__.py	2007-07-31 22:55:27 UTC (rev 7887)
@@ -21,3 +21,7 @@
     
         ## For now, consider it as a snippet.
         return vmatrix
+
+
+    
+    



From yoshua at mail.berlios.de  Wed Aug  1 04:15:26 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 1 Aug 2007 04:15:26 +0200
Subject: [Plearn-commits] r7888 - trunk/plearn_learners/online
Message-ID: <200708010215.l712FQSv022054@sheep.berlios.de>

Author: yoshua
Date: 2007-08-01 04:15:25 +0200 (Wed, 01 Aug 2007)
New Revision: 7888

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added untested code in RBMModule to compute gradient of free energy of visible wrt visible input.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-07-31 22:55:27 UTC (rev 7887)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-01 02:15:25 UTC (rev 7888)
@@ -1202,6 +1202,7 @@
     weights = ports_value[getPortIndex("weights")];
     Mat* weights_grad = ports_gradient[getPortIndex("weights")];
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
+    Mat* energy_grad = ports_gradient[getPortIndex("energy")];
     Mat* contrastive_divergence_grad = NULL;
     Mat* contrastive_divergence = NULL;
     if (compute_contrastive_divergence)
@@ -1655,6 +1656,39 @@
         partition_function_is_stale = true;
     }
 
+    if (energy_grad && !energy_grad->isEmpty() &&
+        visible_grad && visible_grad->isEmpty())
+        // compute the gradient of the free-energy wrt input
+    {
+        // very cheap shot, specializing to the common case...
+        PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+        PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+        PLASSERT(connection->classname()=="RBMMatrixConnection");
+        // FE(x) = -b'x - sum_i softplus(hidden_layer->activation[i])        
+        // dFE(x)/dx = -b - sum_i sigmoid(hidden_layer->activation[i]) W_i
+        // dC/dxt = -b dC/dFE - dC/dFE sum_i p_ti W_i
+        int mbs=energy_grad->length();
+        visible_grad->resize(mbs,visible_layer->size);
+        Mat& weights = ((RBMMatrixConnection*)
+                        get_pointer(connection))->weights;
+        bool same_dC_dFE=true;
+        real dC_dFE=(*energy_grad)(0,0);
+        const Mat& p = hidden_layer->getExpectations();
+        for (int t=0;t<mbs;t++)
+        {
+            real new_dC_dFE=(*energy_grad)(t,0);
+            if (new_dC_dFE!=dC_dFE)
+                same_dC_dFE=false;
+            dC_dFE = new_dC_dFE;
+            multiplyAcc((*visible_grad)(t),visible_layer->bias,-dC_dFE);
+        }
+        if (same_dC_dFE)
+            productScaleAcc(*visible_grad,p,false,weights,false,-1,1);
+        else
+            for (int t=0;t<mbs;t++)
+                productScaleAcc((*visible_grad)(t),weights,true,p(t),-dC_dFE,1);
+    }
+
     // Explicit error message in the case of the 'visible' port.
     if (compute_visible_grad && visible_grad->isEmpty())
         PLERROR("In RBMModule::bpropAccUpdate - The gradient with respect "



From louradou at mail.berlios.de  Wed Aug  1 19:53:02 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 1 Aug 2007 19:53:02 +0200
Subject: [Plearn-commits] r7889 - trunk/plearn_learners/online
Message-ID: <200708011753.l71Hr21N022103@sheep.berlios.de>

Author: louradou
Date: 2007-08-01 19:53:00 +0200 (Wed, 01 Aug 2007)
New Revision: 7889

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
contrastive divergence: corrected computation
(= difference of free energies)



Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-01 02:15:25 UTC (rev 7888)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-01 17:53:00 UTC (rev 7889)
@@ -105,6 +105,7 @@
 RBMModule::RBMModule():
     cd_learning_rate(0),
     grad_learning_rate(0),
+    tied_connection_weights(false),
     compute_contrastive_divergence(false),
     n_Gibbs_steps_CD(1),
     min_n_Gibbs_steps(1),
@@ -156,6 +157,10 @@
         "set to 0, the gradient of the contrastive divergence will not be\n"
         "computed at all.");
 
+    declareOption(ol, "tied_connection_weights", &RBMModule::tied_connection_weights,
+                  OptionBase::buildoption,
+        "Whether to keep fixed the connection weights during learning.");
+
     declareOption(ol, "compute_contrastive_divergence", &RBMModule::compute_contrastive_divergence,
                   OptionBase::buildoption,
         "Compute the constrastive divergence in an output port.");
@@ -1136,15 +1141,12 @@
             // compute contrastive divergence itself
             for (int i=0;i<mbs;i++)
             {
-                (*contrastive_divergence)(i,0) =
-                    // positive phase energy
-                    visible_layer->energy((*visible)(i))
-                    - dot((*h)(i),(*h_act)(i))
-                    // minus
-                    -
-                    // negative phase energy
-                    (visible_layer->energy(visible_layer->samples(i))
-                     - dot(hidden_expectations(i),hidden_layer->activations(i)));
+	    	real s = visible_layer->energy((*visible)(i)) - visible_layer->energy(visible_layer->samples(i));
+		Vec a = (*h_act)(i);
+		Vec b = hidden_layer->activations(i); 
+		for (int j=0;j<hidden_layer->size;j++)
+		    s -= tabulated_softplus(a[j]) - tabulated_softplus(b[j]);
+		(*contrastive_divergence)(i,0) = s;
             }
         }
         else
@@ -1244,7 +1246,11 @@
         // Note: we need to perform the following steps even if the gradient
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
-        setAllLearningRates(grad_learning_rate);
+	if (tied_connection_weights)
+           setLearningRatesOnlyForLayers(grad_learning_rate);
+	else
+           setAllLearningRates(grad_learning_rate);
+	
         PLASSERT_MSG( hidden && hidden_act , "To compute gradients in bprop, the hidden_activations.state port must have been filled during fprop");
         // Compute gradient w.r.t. activations of the hidden layer.
         hidden_layer->bpropUpdate(
@@ -1310,7 +1316,10 @@
     if (cd_learning_rate > 0 && minimize_log_likelihood) {
         PLASSERT( visible && !visible->isEmpty() );
         PLASSERT( hidden && !hidden->isEmpty() );
-        setAllLearningRates(cd_learning_rate);
+	if (tied_connection_weights)
+           setLearningRatesOnlyForLayers(cd_learning_rate);
+	else
+           setAllLearningRates(cd_learning_rate);
 
         // positive phase
         visible_layer->accumulatePosStats(*visible);
@@ -1383,7 +1392,10 @@
                            << name << "'" << endl;
         // Perform a step of contrastive divergence.
         PLASSERT( visible && !visible->isEmpty() );
-        setAllLearningRates(cd_learning_rate);
+	if (tied_connection_weights)
+           setLearningRatesOnlyForLayers(cd_learning_rate);
+	else
+           setAllLearningRates(cd_learning_rate);
         Mat* negative_phase_visible_samples =
             computed_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
         const Mat* negative_phase_hidden_expectations =
@@ -1498,7 +1510,7 @@
                     }
                 }
             }
-            if (!standard_cd_grad) {
+            if (!standard_cd_grad && !tied_connection_weights) {
                 // Update connection manually.
                 Mat& weights = ((RBMMatrixConnection*)
                                 get_pointer(connection))->weights;
@@ -1575,7 +1587,10 @@
     }
 
     if (reconstruction_error_grad && !reconstruction_error_grad->isEmpty()) {
-        setAllLearningRates(grad_learning_rate);
+	if (tied_connection_weights)
+           setLearningRatesOnlyForLayers(grad_learning_rate);
+	else
+           setAllLearningRates(grad_learning_rate);
         PLASSERT( reconstruction_connection != 0 );
         // Perform gradient descent on Autoassociator reconstruction cost
         Mat* visible_reconstruction = ports_value[getPortIndex("visible_reconstruction.state")];
@@ -1765,6 +1780,16 @@
         reconstruction_connection->setLearningRate(lr);
 }
 
+void RBMModule::setLearningRatesOnlyForLayers(real lr)
+{
+    hidden_layer->setLearningRate(lr);
+    visible_layer->setLearningRate(lr);
+    connection->setLearningRate(0.);
+    if(reconstruction_connection)
+        reconstruction_connection->setLearningRate(0.);
+}
+
+
 //////////////////////////////
 // sampleHiddenGivenVisible //
 //////////////////////////////



From nouiz at mail.berlios.de  Wed Aug  1 20:04:31 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 1 Aug 2007 20:04:31 +0200
Subject: [Plearn-commits] r7890 - trunk/plearn_learners/regressors
Message-ID: <200708011804.l71I4VQJ022938@sheep.berlios.de>

Author: nouiz
Date: 2007-08-01 20:04:31 +0200 (Wed, 01 Aug 2007)
New Revision: 7890

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
Log:
small optimisation and put class variable local to the function that needed it


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-08-01 17:53:00 UTC (rev 7889)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-08-01 18:04:31 UTC (rev 7890)
@@ -120,7 +120,7 @@
         l2_loss_function_factor = 1.0;
     }
     multiclass_weights_sum.resize(multiclass_outputs.length());
-    for (multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++) multiclass_weights_sum[multiclass_ind] = 0.0;
+    multiclass_weights_sum.fill(0);
 }
 
 void RegressionTreeMulticlassLeave::addRow(int row, Vec outputv, Vec errorv)
@@ -130,7 +130,7 @@
     length += 1;
     weights_sum += weight;
     int multiclass_found = 0;
-    for (multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+    for (int multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {
         if (target == multiclass_outputs[multiclass_ind])
         {
@@ -152,7 +152,7 @@
     target = train_set->getTarget(row);
     length -= 1;
     weights_sum -= weight;
-    for (multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+    for (int multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {
         if (target == multiclass_outputs[multiclass_ind])
         {
@@ -168,7 +168,7 @@
 void RegressionTreeMulticlassLeave::computeOutputAndError()
 {
     multiclass_winer = 0;
-    for (multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+    for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {
         if (multiclass_weights_sum[multiclass_ind] > multiclass_weights_sum[multiclass_winer]) multiclass_winer = multiclass_ind;
     }
@@ -186,7 +186,7 @@
         error[0] = 0.0;
         if (objective_function == "l1")
         {
-            for (multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+            for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
             {
                 error[0] += abs(output[0] - multiclass_outputs[multiclass_ind]) * multiclass_weights_sum[multiclass_ind];
             }
@@ -196,7 +196,7 @@
         }
         else
         {
-            for (multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+            for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
             {
                 error[0] += pow(output[0] - multiclass_outputs[multiclass_ind], 2) * multiclass_weights_sum[multiclass_ind];
             }

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2007-08-01 17:53:00 UTC (rev 7889)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.h	2007-08-01 18:04:31 UTC (rev 7890)
@@ -72,7 +72,6 @@
   Work fields: they are sized and initialized if need be, at buid time
 */  
  
-    int multiclass_ind;
     int multiclass_winer;
   
 public:



From louradou at mail.berlios.de  Wed Aug  1 21:03:42 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 1 Aug 2007 21:03:42 +0200
Subject: [Plearn-commits] r7891 - trunk/plearn_learners/online
Message-ID: <200708011903.l71J3g9H027229@sheep.berlios.de>

Author: louradou
Date: 2007-08-01 21:03:42 +0200 (Wed, 01 Aug 2007)
New Revision: 7891

Modified:
   trunk/plearn_learners/online/RBMModule.h
Log:


Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-01 18:04:31 UTC (rev 7890)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-01 19:03:42 UTC (rev 7891)
@@ -71,6 +71,7 @@
 
     real cd_learning_rate;
     real grad_learning_rate;
+    bool tied_connection_weights;
 
     bool compute_contrastive_divergence;
 
@@ -266,6 +267,10 @@
 
     //! Forward the given learning rate to all elements of this module.
     void setAllLearningRates(real lr);
+        
+    //! Forward the given learning rate to all elements of the layers
+    //! and to the reconstruction connections (NOT of the connection weights).
+    void setLearningRatesOnlyForLayers(real lr);
 
     //! Declares the class options.
     static void declareOptions(OptionList& ol);



From lysiane at mail.berlios.de  Thu Aug  2 03:20:10 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Thu, 2 Aug 2007 03:20:10 +0200
Subject: [Plearn-commits] r7892 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200708020120.l721KAWP001292@sheep.berlios.de>

Author: lysiane
Date: 2007-08-02 03:20:09 +0200 (Thu, 02 Aug 2007)
New Revision: 7892

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
ajout : implantation de MStepBias()


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-01 19:03:42 UTC (rev 7891)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-02 01:20:09 UTC (rev 7892)
@@ -82,9 +82,6 @@
     nbTransforms(2),
     nbNeighbors(2)
 {
-
-    pout << "constructor called" <<endl;
-
 }
 
 
@@ -349,7 +346,6 @@
                    ArgDoc("const Vec source","source data point"),
                    ArgDoc("int transformIdx","index of the transformation (optional)"),
                    RetDoc("Vec")));
-
     declareMethod(rmm,
                   "returnGeneratedSamplesFrom",
                   &TransformationLearner::returnGeneratedSamplesFrom,
@@ -608,13 +604,34 @@
     return inputSpaceDim;
 }
 
+
+/*TVec<string> PDistribution::getTestCostNames() const
+{
+    TVec<string> nll_cost;
+    if (nll_cost.isEmpty())
+        nll_cost.append("NLL");
+    return nll_cost;
+    }*/
+
+///////////////////////
+// getTrainCostNames //
+///////////////////////
+TVec<string> TransformationLearner::getTrainCostNames() const
+{
+    return getTestCostNames();
+}
+
+
+
+
+
+
+
 /////////////////
 // log_density //
 /////////////////
 real TransformationLearner::log_density(const Vec& y) const
 {
- 
-    pout << "in TransformationLearner :: log_density" << endl;
     PLASSERT(y.length() == inputSpaceDim);
     real weight;
     real totalWeight = INIT_weight(0);
@@ -649,9 +666,10 @@
     // ### shallow-copied.
     // ### ex:
     // deepCopyField(trainvec, copies);
+    
 
     // ### Remove this line when you have fully implemented this method.
-    PLERROR("TransformationLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+    //PLERROR("TransformationLearner::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 ////////////////////
@@ -2107,8 +2125,38 @@
  
 
 //TODO
+//!maximization step with respect to transformation bias
+//!(MAP version)
 void TransformationLearner::MStepBias(){
-    
+    Mat newBiasSet;
+    newBiasSet.resize(nbTransforms, inputSpaceDim);
+    Vec norms;
+    norms.resize(nbTransforms);
+    for(int t=0;t<nbTransforms;t++){
+        norms[t] = INIT_weight(0);
+    }
+    int transformIdx;
+    Vec target,neighbor,reconstruction;
+    real proba,weight;
+    for(int idx=0; idx<nbReconstructions; idx++){
+        transformIdx = reconstructionSet[idx].transformIdx;
+        weight = reconstructionSet[idx].weight;
+        proba = PROBA_weight(weight);
+        target.resize(inputSpaceDim);
+        seeTarget(reconstructionSet[idx].targetIdx,target);
+        neighbor.resize(inputSpaceDim);
+        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
+        reconstruction.resize(inputSpaceDim);
+        applyTransformationOn(transformIdx,neighbor, reconstruction);
+        newBiasSet(transformIdx) += proba*(target - reconstruction);
+        norms[transformIdx] = SUM_weights(norms[transformIdx],weight);
+    }
+    for(int t=0; t<nbTransforms ; t++){
+        newBiasSet(t) /= ((noiseVariance/transformsVariance) 
+                       +
+                       PROBA_weight(norms[t]));
+    }
+    biasSet << newBiasSet;   
 }
 
 

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-01 19:03:42 UTC (rev 7891)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-02 01:20:09 UTC (rev 7892)
@@ -304,7 +304,11 @@
 
 
     //#####  PDistribution Member Functions  ##################################
+ 
 
+    virtual TVec<string> getTrainCostNames() const;
+    
+    
     //! Return log of probability density log(p(y | x)).
     virtual real log_density(const Vec& y) const;
 



From lamblin at mail.berlios.de  Thu Aug  2 05:24:19 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 05:24:19 +0200
Subject: [Plearn-commits] r7893 - trunk/plearn_learners/online
Message-ID: <200708020324.l723OJDa012593@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 05:24:17 +0200 (Thu, 02 Aug 2007)
New Revision: 7893

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
Log:
New method to help computing the free energy of a sample


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-08-02 03:24:17 UTC (rev 7893)
@@ -461,6 +461,24 @@
     return -dot(unit_values, bias);
 }
 
+real RBMBinomialLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    PLASSERT( unit_activations.size() == size );
+
+    // result = -\sum_{i=0}^{size-1} softplus(a_i)
+    real result = 0;
+    real* a = unit_activations.data();
+    for (int i=0; i<size, i++)
+    {
+        if (use_fast_approximations)
+            result -= tabulated_softplus(a[i]);
+        else
+            result -= softplus(a[i]);
+    }
+    return result;
+}
+
 int RBMBinomialLayer::getConfigurationCount()
 {
     return size < 31 ? 1<<size : INFINITE_CONFIGURATIONS;
@@ -474,7 +492,7 @@
     for ( int i = 0; i < size; ++i ) {
         output[i] = conf_index & 1;
         conf_index >>= 1;
-    }    
+    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-08-02 03:24:17 UTC (rev 7893)
@@ -120,6 +120,11 @@
     //! compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-08-02 03:24:17 UTC (rev 7893)
@@ -634,6 +634,23 @@
     return en;
 }
 
+real RBMGaussianLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    PLASSERT( unit_activations.size() == size );
+
+    // result = \sum_{i=0}^{size-1} (-(a_i/(2 q_i))^2 + log(q_i)) - n/2 log(Pi)
+    real result = -0.5 * size * LogPi;
+    for (int i=0; i<size; i++)
+    {
+        real a_i = unit_activations[i];
+        real q_i = quad_coeff[i];
+        result += pl_log(q_i);
+        result -= a_i * a_i / (4 * q_i * q_i);
+    }
+    return result;
+}
+
 real RBMGaussianLayer::fpropNLL(const Vec& target)
 {
     PLASSERT( target.size() == input_size );

Modified: trunk/plearn_learners/online/RBMGaussianLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.h	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMGaussianLayer.h	2007-08-02 03:24:17 UTC (rev 7893)
@@ -137,6 +137,11 @@
     //! compute bias' unit_values + min_quad_coeff.^2' unit_values.^2
     virtual real energy(const Vec& unit_values) const;
 
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
     //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-08-02 03:24:17 UTC (rev 7893)
@@ -631,6 +631,12 @@
     return 0;
 }
 
+real RBMLayer::freeEnergyContribution(const Vec& unit_activations) const
+{
+    PLERROR("RBMLayer::freeEnergyContribution(Vec) not implemented in subclass %s\n",classname().c_str());
+    return 0;
+}
+
 int RBMLayer::getConfigurationCount()
 {
     PLERROR("RBMLayer::getConfigurationCount() not implemented in subclass %s\n",classname().c_str());

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-08-02 03:24:17 UTC (rev 7893)
@@ -267,6 +267,11 @@
 
     virtual real energy(const Vec& unit_values) const;
 
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
     //! Returns a number of different configurations the layer can be in.
     virtual int getConfigurationCount();
 

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-08-02 03:24:17 UTC (rev 7893)
@@ -431,6 +431,12 @@
     return -dot(unit_values, bias);
 }
 
+real RBMMultinomialLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    return -pl_log(sum(unit_activations));
+}
+
 int RBMMultinomialLayer::getConfigurationCount()
 {
     return size;

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-08-02 01:20:09 UTC (rev 7892)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-08-02 03:24:17 UTC (rev 7893)
@@ -119,6 +119,11 @@
     // Compute -bias' unit_values
     virtual real energy(const Vec& unit_values) const;
 
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);



From lamblin at mail.berlios.de  Thu Aug  2 05:29:22 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 05:29:22 +0200
Subject: [Plearn-commits] r7894 - trunk/plearn_learners/online
Message-ID: <200708020329.l723TMYY013562@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 05:29:21 +0200 (Thu, 02 Aug 2007)
New Revision: 7894

Modified:
   trunk/plearn_learners/online/RBMGaussianLayer.cc
Log:
Fix case where share_quad_coeff is true


Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-08-02 03:24:17 UTC (rev 7893)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-08-02 03:29:21 UTC (rev 7894)
@@ -644,7 +644,7 @@
     for (int i=0; i<size; i++)
     {
         real a_i = unit_activations[i];
-        real q_i = quad_coeff[i];
+        real q_i = share_quad_coeff ? quad_coeff[i] : quad_coeff[0];
         result += pl_log(q_i);
         result -= a_i * a_i / (4 * q_i * q_i);
     }



From lamblin at mail.berlios.de  Thu Aug  2 05:39:51 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 05:39:51 +0200
Subject: [Plearn-commits] r7895 - in trunk: plearn/math
	plearn_learners/online
Message-ID: <200708020339.l723dpMv015639@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 05:39:50 +0200 (Thu, 02 Aug 2007)
New Revision: 7895

Modified:
   trunk/plearn/math/pl_math.h
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
Compilation fix


Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2007-08-02 03:29:21 UTC (rev 7894)
+++ trunk/plearn/math/pl_math.h	2007-08-02 03:39:50 UTC (rev 7895)
@@ -152,6 +152,10 @@
 #  define Pi 3.141592653589793
 #endif
 
+#if !defined(LogPi)
+#  define LogPi 1.14472988585
+#endif
+
 #if !defined(Log2Pi)
 #  define Log2Pi 1.837877066409
 #endif

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-08-02 03:29:21 UTC (rev 7894)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-08-02 03:39:50 UTC (rev 7895)
@@ -469,7 +469,7 @@
     // result = -\sum_{i=0}^{size-1} softplus(a_i)
     real result = 0;
     real* a = unit_activations.data();
-    for (int i=0; i<size, i++)
+    for (int i=0; i<size; i++)
     {
         if (use_fast_approximations)
             result -= tabulated_softplus(a[i]);



From lamblin at mail.berlios.de  Thu Aug  2 05:52:29 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 05:52:29 +0200
Subject: [Plearn-commits] r7896 - trunk/plearn_learners/online
Message-ID: <200708020352.l723qTFR017680@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 05:52:29 +0200 (Thu, 02 Aug 2007)
New Revision: 7896

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Use new, more generic way to compute free energy.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-02 03:39:50 UTC (rev 7895)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-02 03:52:29 UTC (rev 7896)
@@ -431,9 +431,9 @@
 // FREE-ENERGY(hidden) CASE
 // we know h:
 // free energy = -log sum_x e^{-energy(h,x)}
-//  = -c'h - sum_i log sigmoid(b_i + W_{.i}'h) .... FOR BINOMIAL INPUT LAYER
 // or more robustly,
-//  = hidden_layer->energy(h) - sum_i softplus(visible_layer->activation[i])
+//  = hidden_layer->energy(h)
+//    + visible_layer->freeEnergyContribution(visible_layer->activation)
 void RBMModule::computeFreeEnergyOfHidden(const Mat& hidden, Mat& energy)
 {
     int mbs=hidden.length();
@@ -442,17 +442,13 @@
     else {
         PLASSERT( energy.length() == mbs && energy.width() == 1 );
     }
-    PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+
     computeVisibleActivations(hidden, false);
     for (int i=0;i<mbs;i++)
     {
-        energy(i,0) = hidden_layer->energy(hidden(i));
-        if (use_fast_approximations)
-            for (int j=0;j<visible_layer->size;j++)
-                energy(i,0) -= tabulated_softplus(visible_layer->activations(i,j));
-        else
-            for (int j=0;j<visible_layer->size;j++)
-                energy(i,0) -= softplus(visible_layer->activations(i,j));
+        energy(i,0) = hidden_layer->energy(hidden(i))
+            + visible_layer->freeEnergyContribution(
+                visible_layer->activations(i));
     }
 }
 
@@ -462,9 +458,9 @@
 // FREE-ENERGY(visible) CASE
 // we know x:
 // free energy = -log sum_h e^{-energy(h,x)}
-//  = -b'x - sum_i log sigmoid(c_i + W_i'x) .... FOR BINOMIAL HIDDEN LAYER
 // or more robustly,
-//  = visible_layer->energy(x) - sum_i softplus(hidden_layer->activation[i])
+//  = visible_layer->energy(x)
+//    + hidden_layer->freeEnergyContribution(hidden_layer->activation)
 void RBMModule::computeFreeEnergyOfVisible(const Mat& visible, Mat& energy,
                                            bool positive_phase)
 {
@@ -474,7 +470,7 @@
     else {
         PLASSERT( energy.length() == mbs && energy.width() == 1 );
     }
-    PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
+
     Mat* hidden_activations = NULL;
     if (positive_phase) {
         computePositivePhaseHiddenActivations(visible);
@@ -488,13 +484,8 @@
             && hidden_activations->width() == hidden_layer->size );
     for (int i=0;i<mbs;i++)
     {
-        energy(i,0) = visible_layer->energy(visible(i));
-        if (use_fast_approximations)
-            for (int j=0;j<hidden_layer->size;j++)
-                energy(i,0) -= tabulated_softplus((*hidden_activations)(i,j));
-        else
-            for (int j=0;j<hidden_layer->size;j++)
-                energy(i,0) -= softplus((*hidden_activations)(i,j));
+        energy(i,0) = visible_layer->energy(visible(i))
+            + hidden_layer->freeEnergyContribution((*hidden_activations)(i));
     }
 }
 
@@ -1130,9 +1121,6 @@
                                                       neg_hidden_act.width());
             *negative_phase_hidden_activations << neg_hidden_act;
 
-            // compute the energy (again for now only in the binomial case)
-            PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
-
             // note that h_act and h may point to hidden_act_store and hidden_exp_store
             PLASSERT(h_act && !h_act->isEmpty());
             PLASSERT(h && !h->isEmpty());
@@ -1141,12 +1129,13 @@
             // compute contrastive divergence itself
             for (int i=0;i<mbs;i++)
             {
-	    	real s = visible_layer->energy((*visible)(i)) - visible_layer->energy(visible_layer->samples(i));
-		Vec a = (*h_act)(i);
-		Vec b = hidden_layer->activations(i); 
-		for (int j=0;j<hidden_layer->size;j++)
-		    s -= tabulated_softplus(a[j]) - tabulated_softplus(b[j]);
-		(*contrastive_divergence)(i,0) = s;
+                // + Free energy of positive example
+                // - free energy of negative example
+                (*contrastive_divergence)(i,0) =
+                    visible_layer->energy((*visible)(i))
+                  + hidden_layer->freeEnergyContribution((*h_act)(i))
+                  - visible_layer->energy(visible_layer->samples(i))
+                  - hidden_layer->freeEnergyContribution(hidden_layer->activations(i));
             }
         }
         else



From lamblin at mail.berlios.de  Thu Aug  2 05:55:58 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 05:55:58 +0200
Subject: [Plearn-commits] r7897 - trunk/plearn_learners/online
Message-ID: <200708020355.l723twg1018232@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 05:55:57 +0200 (Thu, 02 Aug 2007)
New Revision: 7897

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Removed tabulations used as intentation


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-02 03:52:29 UTC (rev 7896)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-02 03:55:57 UTC (rev 7897)
@@ -623,53 +623,53 @@
 
 void RBMModule::computePartitionFunction()
 {
-	int hidden_configurations = hidden_layer->getConfigurationCount();
-	int visible_configurations = visible_layer->getConfigurationCount();
-	
-	PLASSERT_MSG(hidden_configurations != RBMLayer::INFINITE_CONFIGURATIONS ||
-                    visible_configurations != RBMLayer::INFINITE_CONFIGURATIONS,
-		"To compute exact log-likelihood of an RBM maximum configurations of hidden "
-				"or visible layer must be less than 2^31.");
-	
-	// Compute partition function
-	if (hidden_configurations > visible_configurations)
-		// do it by log-summing minus-free-energy of visible configurations
-	{
-		energy_inputs.resize(1, visible_layer->size);
-		Vec input = energy_inputs(0);
-		// COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
-		// AT ONCE IN A 'MINIBATCH'
-		Mat free_energy(1, 1);
-		log_partition_function = 0;
-		for (int c = 0; c < visible_configurations; c++)
-		{
-			visible_layer->getConfiguration(c, input);
-			computeFreeEnergyOfVisible(energy_inputs, free_energy, false);
-			if (c==0)
-				log_partition_function = -free_energy(0,0);
-			else
-				log_partition_function = logadd(log_partition_function,-free_energy(0,0));
-		}
-	}
-	else
-		// do it by summing free-energy of hidden configurations
-	{
-		energy_inputs.resize(1, hidden_layer->size);
-		Vec input = energy_inputs(0);
-		// COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
-		// AT ONCE IN A 'MINIBATCH'
-		Mat free_energy(1, 1);
-		log_partition_function = 0;
-		for (int c = 0; c < hidden_configurations; c++)
-		{
-			hidden_layer->getConfiguration(c, input);
-			computeFreeEnergyOfHidden(energy_inputs, free_energy);
-			if (c==0)
-				log_partition_function = -free_energy(0,0);
-			else
-				log_partition_function = logadd(log_partition_function,-free_energy(0,0));
-		}
-	}
+    int hidden_configurations = hidden_layer->getConfigurationCount();
+    int visible_configurations = visible_layer->getConfigurationCount();
+
+    PLASSERT_MSG(hidden_configurations != RBMLayer::INFINITE_CONFIGURATIONS ||
+                 visible_configurations != RBMLayer::INFINITE_CONFIGURATIONS,
+                 "To compute exact log-likelihood of an RBM maximum configurations of hidden "
+                 "or visible layer must be less than 2^31.");
+
+    // Compute partition function
+    if (hidden_configurations > visible_configurations)
+        // do it by log-summing minus-free-energy of visible configurations
+    {
+        energy_inputs.resize(1, visible_layer->size);
+        Vec input = energy_inputs(0);
+        // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+        // AT ONCE IN A 'MINIBATCH'
+        Mat free_energy(1, 1);
+        log_partition_function = 0;
+        for (int c = 0; c < visible_configurations; c++)
+        {
+            visible_layer->getConfiguration(c, input);
+            computeFreeEnergyOfVisible(energy_inputs, free_energy, false);
+            if (c==0)
+                log_partition_function = -free_energy(0,0);
+            else
+                log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+        }
+    }
+    else
+        // do it by summing free-energy of hidden configurations
+    {
+        energy_inputs.resize(1, hidden_layer->size);
+        Vec input = energy_inputs(0);
+        // COULD BE DONE MORE EFFICIENTLY BY DOING MANY CONFIGURATIONS
+        // AT ONCE IN A 'MINIBATCH'
+        Mat free_energy(1, 1);
+        log_partition_function = 0;
+        for (int c = 0; c < hidden_configurations; c++)
+        {
+            hidden_layer->getConfiguration(c, input);
+            computeFreeEnergyOfHidden(energy_inputs, free_energy);
+            if (c==0)
+                log_partition_function = -free_energy(0,0);
+            else
+                log_partition_function = logadd(log_partition_function,-free_energy(0,0));
+        }
+    }
 }
 
 void RBMModule::fprop(const TVec<Mat*>& ports_value)
@@ -1235,11 +1235,11 @@
         // Note: we need to perform the following steps even if the gradient
         // learning rate is equal to 0. This is because we must propagate the
         // gradient to the visible layer, even though no update is required.
-	if (tied_connection_weights)
+        if (tied_connection_weights)
            setLearningRatesOnlyForLayers(grad_learning_rate);
-	else
+        else
            setAllLearningRates(grad_learning_rate);
-	
+
         PLASSERT_MSG( hidden && hidden_act , "To compute gradients in bprop, the hidden_activations.state port must have been filled during fprop");
         // Compute gradient w.r.t. activations of the hidden layer.
         hidden_layer->bpropUpdate(
@@ -1305,9 +1305,9 @@
     if (cd_learning_rate > 0 && minimize_log_likelihood) {
         PLASSERT( visible && !visible->isEmpty() );
         PLASSERT( hidden && !hidden->isEmpty() );
-	if (tied_connection_weights)
+        if (tied_connection_weights)
            setLearningRatesOnlyForLayers(cd_learning_rate);
-	else
+        else
            setAllLearningRates(cd_learning_rate);
 
         // positive phase
@@ -1381,9 +1381,9 @@
                            << name << "'" << endl;
         // Perform a step of contrastive divergence.
         PLASSERT( visible && !visible->isEmpty() );
-	if (tied_connection_weights)
+        if (tied_connection_weights)
            setLearningRatesOnlyForLayers(cd_learning_rate);
-	else
+        else
            setAllLearningRates(cd_learning_rate);
         Mat* negative_phase_visible_samples =
             computed_contrastive_divergence?ports_value[getPortIndex("negative_phase_visible_samples.state")]:0;
@@ -1576,9 +1576,9 @@
     }
 
     if (reconstruction_error_grad && !reconstruction_error_grad->isEmpty()) {
-	if (tied_connection_weights)
+        if (tied_connection_weights)
            setLearningRatesOnlyForLayers(grad_learning_rate);
-	else
+        else
            setAllLearningRates(grad_learning_rate);
         PLASSERT( reconstruction_connection != 0 );
         // Perform gradient descent on Autoassociator reconstruction cost

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-02 03:52:29 UTC (rev 7896)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-02 03:55:57 UTC (rev 7897)
@@ -320,7 +320,7 @@
     void computeEnergy(const Mat& visible, const Mat& hidden, Mat& energy,
                        bool positive_phase = true);
 
-	void computePartitionFunction();
+    void computePartitionFunction();
 
 private:
     //#####  Private Member Functions  ########################################



From lamblin at mail.berlios.de  Thu Aug  2 06:06:08 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 06:06:08 +0200
Subject: [Plearn-commits] r7898 - trunk/plearn_learners/online
Message-ID: <200708020406.l724683b019769@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 06:06:08 +0200 (Thu, 02 Aug 2007)
New Revision: 7898

Added:
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.h
Log:
New subclass of RBMLayer


Added: trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2007-08-02 03:55:57 UTC (rev 7897)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2007-08-02 04:06:08 UTC (rev 7898)
@@ -0,0 +1,858 @@
+// -*- C++ -*-
+
+// RBMLocalMultinomialLayer.cc
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Pascal Lamblin
+
+/*! \file RBMPLayer.cc */
+
+
+
+#include "RBMLocalMultinomialLayer.h"
+#include <plearn/math/TMat_maths.h>
+#include "RBMConnection.h"
+
+namespace PLearn {
+using namespace std;
+
+// Helper functions, like the ones using Vecs, but with Mats
+template <class T>
+void softmax(const TMat<T>& x, const TMat<T>& y)
+{
+    int l = x.length();
+    int w = x.width();
+    PLASSERT( y.length() == l );
+    PLASSERT( y.width() == w );
+
+    if (l*w>0)
+    {
+        TMatElementIterator<real> xp = x.begin();
+        TMatElementIterator<real> yp = y.begin();
+        T maxx = max(x);
+        real s = 0;
+
+        for (int i=0; i<l*w; i++, xp++, yp++)
+            s += ( (*yp) = safeexp((*xp) - maxx) );
+
+        if (s == 0)
+            PLERROR( "Trying to divide by 0 in softmax");
+        s = 1.0 / s;
+
+        for (yp = y.begin(); yp != y.end(); yp++)
+            (*yp) *= s;
+    }
+}
+
+int multinomial_sample(const PP<PRandom>& rg, const Mat& distribution)
+{
+    real u = rg->uniform_sample();
+    TMatElementIterator<real> pi = distribution.begin();
+    real s = *pi;
+    int w = distribution.width();
+    int n = distribution.size();
+    int i = 0;
+
+    while (s<u && i<n)
+    {
+        PLASSERT( *pi == distribution(i / w, i % w) );
+        i++;
+        pi++;
+        s += *pi;
+    }
+    if (i == n)
+        i = n - 1; // Improbable, but...
+    return i;
+}
+
+template<class T>
+void fill_one_hot(const TMat<T>& mat, int hotpos, T coldvalue, T hotvalue)
+{
+    PLASSERT_MSG( mat.isNotEmpty(), "Given mat must not be empty" );
+    PLASSERT_MSG( hotpos >= 0, "hotpos out of mat range" );
+    PLASSERT_MSG( mat.size() > 1 || hotpos <= 1, "hotpos out of mat range" );
+    PLASSERT_MSG( hotpos < mat.size() || mat.size() == 1,
+                  "hotpos out of mat range" );
+
+    if (mat.size() == 1)
+        mat(0,0) = (hotpos == 0 ? coldvalue : hotvalue);
+    else
+    {
+        mat.fill(coldvalue);
+        int w = mat.width();
+        mat(hotpos / w, hotpos % w);
+    }
+}
+
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMLocalMultinomialLayer,
+    "Layer in an RBM, consisting in one multinomial unit",
+    "");
+
+RBMLocalMultinomialLayer::RBMLocalMultinomialLayer( real the_learning_rate ) :
+    inherited( the_learning_rate )
+{
+}
+
+void RBMLocalMultinomialLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectation_is_up_to_date, "Expectation should be computed "
+            "before calling generateSample()");
+
+    for (int l=0; l<n_images; l++)
+    {
+        Mat expectation_image = expectation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat sample_image = sample
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+=area_width)
+            {
+                Mat expectation_area =
+                    expectation_image.subMat(i, j, area_length, area_width);
+                Mat sample_area =
+                    sample_image.subMat(i, j, area_length, area_width);
+                int index = multinomial_sample(random_gen, expectation_area);
+                fill_one_hot(sample_area, index, real(0), real(1));
+            }
+    }
+}
+
+void RBMLocalMultinomialLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectations_are_up_to_date, "Expectations should be computed "
+                        "before calling generateSamples()");
+
+    PLASSERT( samples.width() == size && samples.length() == batch_size );
+
+    for (int k = 0; k < batch_size; k++)
+        for (int l=0; l<n_images; l++)
+        {
+            Mat expectation_image = expectations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat sample_image = samples(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i<images_length; i+=area_length)
+                for (int j=0; j<images_width; j+=area_width)
+                {
+                    Mat expectation_area =
+                        expectation_image.subMat(i, j, area_length, area_width);
+                    Mat sample_area =
+                        sample_image.subMat(i, j, area_length, area_width);
+                    int index = multinomial_sample(random_gen,
+                                                   expectation_area);
+                    fill_one_hot(sample_area, index, real(0), real(1));
+               }
+        }
+}
+
+void RBMLocalMultinomialLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    for (int l=0; l<n_images; l++)
+    {
+        Mat activation_image = activation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat expectation_image = expectation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+=area_width)
+                softmax(
+                    activation_image.subMat(i, j, area_length, area_width),
+                    expectation_image.subMat(i, j, area_length, area_width)
+                    );
+    }
+    expectation_is_up_to_date = true;
+}
+
+void RBMLocalMultinomialLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLASSERT( expectations.width() == size
+              && expectations.length() == batch_size );
+
+    for (int k = 0; k < batch_size; k++)
+        for (int l=0; l<n_images; l++)
+        {
+            Mat activation_image = activations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat expectation_image = expectations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i<images_length; i+=area_length)
+                for (int j=0; j<images_width; j+=area_width)
+                    softmax(
+                        activation_image.subMat(i, j, area_length, area_width),
+                        expectation_image.subMat(i, j, area_length, area_width)
+                        );
+        }
+
+    expectations_are_up_to_date = true;
+}
+
+
+void RBMLocalMultinomialLayer::fprop( const Vec& input, Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+
+    // inefficient
+    Vec input_plus_bias = input + bias;
+    for (int l=0; l<n_images; l++)
+    {
+        Mat input_image = input_plus_bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+=area_width)
+                softmax(
+                    input_image.subMat(i, j, area_length, area_width),
+                    output_image.subMat(i, j, area_length, area_width)
+                    );
+    }
+}
+
+///////////
+// fprop //
+///////////
+void RBMLocalMultinomialLayer::fprop( const Vec& input, const Vec& rbm_bias,
+                                      Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    PLASSERT( rbm_bias.size() == input_size );
+    output.resize( output_size );
+
+    // inefficient
+    Vec input_plus_bias = input + rbm_bias;
+    for (int l=0; l<n_images; l++)
+    {
+        Mat input_image = input_plus_bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+=area_width)
+                softmax(
+                    input_image.subMat(i, j, area_length, area_width),
+                    output_image.subMat(i, j, area_length, area_width)
+                    );
+    }
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMLocalMultinomialLayer::bpropUpdate(const Vec& input, const Vec& output,
+                                           Vec& input_gradient,
+                                           const Vec& output_gradient,
+                                           bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    for (int l=0; l<n_images; l++)
+    {
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat input_grad_image = input_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_grad_image = output_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat bias_image = bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat bias_inc_image;
+        if (momentum != 0)
+            bias_inc_image = bias_inc
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+=area_width)
+            {
+                Mat output_area = output_image
+                    .subMat(i, j, area_length, area_width);
+                Mat input_grad_area = input_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat output_grad_area = output_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat bias_area = bias_image
+                    .subMat(i, j, area_length, area_width);
+                Mat bias_inc_area;
+                if (momentum != 0)
+                    bias_inc_area = bias_inc_image
+                        .subMat(i, j, area_length, area_width);
+
+                real outga_dot_outa = dot(output_grad_area, output_area);
+
+                TMatElementIterator<real> pog = output_grad_area.begin();
+                TMatElementIterator<real> po = output_area.begin();
+                TMatElementIterator<real> pig = input_grad_area.begin();
+                TMatElementIterator<real> pb = bias_area.begin();
+
+                TMatElementIterator<real> pbi = bias_inc_area.begin();
+/*
+                TMatElementIterator<real> pbi;
+                if (momentum != 0)
+                    pbi = bias_inc_area.begin();
+*/
+                for (int m=0; m<area_size; m++, pog++, po++, pig++, pb++)
+                {
+                    real inga_m = (*pog - outga_dot_outa) * (*po);
+                    *pig += inga_m;
+
+                    if (momentum == 0)
+                    {
+                        // update the bias: bias -= learning_rate * input_grad
+                        *pb -= learning_rate * (*pig);
+                    }
+                    else
+                    {
+                        // The update rule becomes:
+                        // bias_inc = momentum * bias_inc
+                        //            - learning_rate * input_grad
+                        *pbi = momentum * (*pbi) - learning_rate * (*pig);
+                        *pb += *pbi;
+                        pbi++;
+                    }
+                }
+            }
+    }
+}
+
+void RBMLocalMultinomialLayer::bpropUpdate(const Mat& inputs,
+                                           const Mat& outputs,
+                                           Mat& input_gradients,
+                                           const Mat& output_gradients,
+                                           bool accumulate)
+{
+    PLASSERT( inputs.width() == size );
+    PLASSERT( outputs.width() == size );
+    PLASSERT( output_gradients.width() == size );
+
+    int mbatch_size = inputs.length();
+    PLASSERT( outputs.length() == mbatch_size );
+    PLASSERT( output_gradients.length() == mbatch_size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradients.width() == size &&
+                input_gradients.length() == inputs.length(),
+                "Cannot resize input_gradient and accumulate into it." );
+    }
+    else
+    {
+        input_gradients.resize(inputs.length(), size);
+        input_gradients.clear();
+    }
+
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+
+    // TODO see if we can have a speed-up by reorganizing the different steps
+
+    // input_gradients[k][i] =
+    //   (output_gradients[k][i]-output_gradients[k].outputs[k]) outputs[k][i]
+    real mean_lr = learning_rate / mbatch_size;
+    for (int l=0; l<n_images; l++)
+    {
+        Mat bias_image = bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat bias_inc_image;
+        if (momentum != 0)
+            bias_inc_image = bias_inc
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+        for( int k=0; k<mbatch_size; k++ )
+        {
+            Mat output_image = outputs(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat input_grad_image = input_gradients(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat output_grad_image = output_gradients(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i<images_length; i+=area_length)
+                for (int j=0; j<images_width; j+=area_width)
+                {
+                    Mat output_area = output_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat input_grad_area = input_grad_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat output_grad_area = output_grad_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat bias_area = bias_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat bias_inc_area;
+                    if (momentum != 0)
+                        bias_inc_area = bias_inc_image
+                            .subMat(i, j, area_length, area_width);
+
+                    real outga_dot_outa = dot(output_grad_area, output_area);
+
+                    TMatElementIterator<real> pog = output_grad_area.begin();
+                    TMatElementIterator<real> po = output_area.begin();
+                    TMatElementIterator<real> pig = input_grad_area.begin();
+                    TMatElementIterator<real> pb = bias_area.begin();
+
+                    if (momentum == 0)
+                    {
+                        for (int i=0; i<area_size; i++, pog++, po++, pig++,
+                                                   pb++)
+                        {
+                            real inga_i = (*pog - outga_dot_outa) * (*po);
+                            *pig += inga_i;
+
+                            // update the bias:
+                            // bias -= learning_rate * input_grad
+                            *pb -= mean_lr * (*pig);
+                        }
+                    }
+                    else
+                        PLCHECK_MSG(false,
+                                    "Momentum and mini-batch not implemented");
+                }
+        }
+    }
+}
+
+//! TODO: add "accumulate" here
+void RBMLocalMultinomialLayer::bpropUpdate(const Vec& input,
+                                           const Vec& rbm_bias,
+                                           const Vec& output,
+                                           Vec& input_gradient,
+                                           Vec& rbm_bias_gradient,
+                                           const Vec& output_gradient)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( rbm_bias.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+    input_gradient.resize( size );
+    rbm_bias_gradient.resize( size );
+
+    for (int l=0; l<n_images; l++)
+    {
+        Mat output_image = output
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat input_grad_image = input_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat output_grad_image = output_gradient
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat rbm_bias_image = rbm_bias
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+=area_width)
+            {
+                Mat output_area = output_image
+                    .subMat(i, j, area_length, area_width);
+                Mat input_grad_area = input_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat output_grad_area = output_grad_image
+                    .subMat(i, j, area_length, area_width);
+                Mat rbm_bias_area = rbm_bias_image
+                    .subMat(i, j, area_length, area_width);
+
+                real outga_dot_outa = dot(output_grad_area, output_area);
+
+                TMatElementIterator<real> pog = output_grad_area.begin();
+                TMatElementIterator<real> po = output_area.begin();
+                TMatElementIterator<real> pig = input_grad_area.begin();
+                TMatElementIterator<real> prb = rbm_bias_area.begin();
+
+                for (int m=0; m<area_size; m++, pog++, po++, pig++, prb++)
+                {
+                    real inga_m = (*pog - outga_dot_outa) * (*po);
+                    *pig += inga_m;
+
+                    // update the bias: bias -= learning_rate * input_grad
+                    *prb -= learning_rate * (*pig);
+                }
+            }
+    }
+
+    rbm_bias_gradient << input_gradient;
+}
+
+//////////////
+// fpropNLL //
+//////////////
+real RBMLocalMultinomialLayer::fpropNLL(const Vec& target)
+{
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+
+    real nll = 0;
+    for (int l=0; l<n_images; l++)
+    {
+        Mat target_image = target
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+        Mat expectation_image = expectation
+            .subVec(l*images_size, images_size)
+            .toMat(images_length, images_width);
+
+        for (int i=0; i<images_length; i+=area_length)
+            for (int j=0; j<images_width; j+= area_width)
+            {
+                Mat target_area = target_image
+                    .subMat(i, j, area_length, area_width);
+                Mat expectation_area = expectation_image
+                    .subMat(i, j, area_length, area_width);
+
+#ifdef BOUNDCHECK
+                if (!target_area.hasMissing())
+                {
+                    PLASSERT_MSG( min(target_area) >= 0.,
+                                  "Elements of \"target_areal\" should be"
+                                  " positive" );
+                    // Ensure the distribution probabilities sum to 1. We relax a
+                    // bit the default tolerance as probabilities using
+                    // exponentials could suffer numerical imprecisions.
+                    if (!is_equal( sum(target_area), 1., 1., 1e-5, 1e-5 ))
+                        PLERROR("In RBMLocalMultinomialLayer::fpropNLL -"
+                                " Elements of \"target_area\" should sum to 1"
+                                " (found a sum = %f)",
+                                sum(target_area));
+                }
+#endif
+                TMatElementIterator<real> p_tgt = target_area.begin();
+                TMatElementIterator<real> p_exp = expectation_area.begin();
+                for (int m=0; m<area_size; m++, p_tgt++, p_exp++)
+                {
+                    if (!fast_exact_is_equal(*p_tgt, 0))
+                        nll -= *p_tgt * pl_log(*p_exp);
+                }
+            }
+    }
+    return nll;
+}
+
+void RBMLocalMultinomialLayer::fpropNLL(const Mat& targets, const Mat& costs_column)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+
+    for (int k=0; k<batch_size; k++) // loop over minibatch
+    {
+        real nll = 0;
+        for (int l=0; l<n_images; l++)
+        {
+            Mat target_image = targets(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+            Mat expectation_image = expectations(k)
+                .subVec(l*images_size, images_size)
+                .toMat(images_length, images_width);
+
+            for (int i=0; i<images_length; i+=area_length)
+                for (int j=0; j<images_width; j+= area_width)
+                {
+                    Mat target_area = target_image
+                        .subMat(i, j, area_length, area_width);
+                    Mat expectation_area = expectation_image
+                        .subMat(i, j, area_length, area_width);
+
+#ifdef BOUNDCHECK
+                    if (!target_area.hasMissing())
+                    {
+                        PLASSERT_MSG( min(target_area) >= 0.,
+                                      "Elements of \"target_areal\" should be"
+                                      " positive" );
+                        // Ensure the distribution probabilities sum to 1. We relax a
+                        // bit the default tolerance as probabilities using
+                        // exponentials could suffer numerical imprecisions.
+                        if (!is_equal( sum(target_area), 1., 1., 1e-5, 1e-5 ))
+                            PLERROR("In RBMLocalMultinomialLayer::fpropNLL -"
+                                    " Elements of \"target_area\" should sum"
+                                    " to 1 (found a sum = %f) at row %d",
+                                    sum(target_area), k);
+                    }
+#endif
+                    TMatElementIterator<real> p_tgt = target_area.begin();
+                    TMatElementIterator<real> p_exp = expectation_area.begin();
+                    for (int m=0; m<area_size; m++, p_tgt++, p_exp++)
+                    {
+                        if (!fast_exact_is_equal(*p_tgt, 0))
+                            nll -= *p_tgt * pl_log(*p_exp);
+                    }
+                }
+        }
+        costs_column(k, 0) = nll;
+    }
+}
+
+void RBMLocalMultinomialLayer::bpropNLL(const Vec& target, real nll,
+                                        Vec& bias_gradient)
+{
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
+}
+
+void RBMLocalMultinomialLayer::bpropNLL(const Mat& targets, const Mat& costs_column,
+                                        Mat& bias_gradients)
+{
+    computeExpectations();
+
+    PLASSERT( targets.width() == input_size );
+    PLASSERT( targets.length() == batch_size );
+    PLASSERT( costs_column.width() == 1 );
+    PLASSERT( costs_column.length() == batch_size );
+    bias_gradients.resize( batch_size, size );
+
+    // bias_gradients = expectations - targets
+    substract(expectations, targets, bias_gradients);
+}
+
+void RBMLocalMultinomialLayer::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "n_images", &RBMLocalMultinomialLayer::n_images,
+                  OptionBase::buildoption,
+                  "Number of images in the layer.");
+
+    declareOption(ol, "images_length",
+                  &RBMLocalMultinomialLayer::images_length,
+                  OptionBase::buildoption,
+                  "Length of the images.");
+
+    declareOption(ol, "images_width",
+                  &RBMLocalMultinomialLayer::images_width,
+                  OptionBase::buildoption,
+                  "Width of the images.");
+
+    declareOption(ol, "images_size",
+                  &RBMLocalMultinomialLayer::images_size,
+                  OptionBase::learntoption,
+                  "images_width ? images_length.");
+
+    declareOption(ol, "area_length",
+                  &RBMLocalMultinomialLayer::area_length,
+                  OptionBase::buildoption,
+                  "Length of the areas over which the multinomial is set.");
+
+    declareOption(ol, "area_width",
+                  &RBMLocalMultinomialLayer::area_width,
+                  OptionBase::buildoption,
+                  "Width of the areas over which the multinomial is set.");
+
+    declareOption(ol, "area_size",
+                  &RBMLocalMultinomialLayer::area_size,
+                  OptionBase::learntoption,
+                  "area_width ? area_length.");
+
+/*
+    declareOption(ol, "size", &RBMLocalMultinomialLayer::size,
+                  OptionBase::buildoption,
+                  "Number of units.");
+*/
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, "size",
+                  &RBMLocalMultinomialLayer::size,
+                  OptionBase::learntoption,
+                  "n_images ? images_width ? images_length.");
+
+}
+
+void RBMLocalMultinomialLayer::build_()
+{
+    PLCHECK_MSG(images_length % area_length == 0,
+                "\"images_length\" should be a multiple of \"area_length\"");
+    PLCHECK_MSG(images_width % area_width == 0,
+                "\"images_width\" should be a multiple of \"area_width\"");
+
+    images_size = images_length * images_width;
+    area_size = area_length * area_width;
+    size = images_size * n_images;
+    n_areas = size / area_size;
+}
+
+void RBMLocalMultinomialLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMLocalMultinomialLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+real RBMLocalMultinomialLayer::energy(const Vec& unit_values) const
+{
+    return -dot(unit_values, bias);
+}
+
+
+real RBMLocalMultinomialLayer::freeEnergyContribution(
+    const Vec& activation_values) const
+{
+    PLASSERT( activation_values.size() == size );
+
+    // result = -\sum_{i=0}^{n_areas-1} log(\sum_{j=0}^{area_size-1} a_{ij})
+    real result = 0;
+    Mat activation_images = activation_values
+        .toMat(n_images*images_length, images_width);
+    for (int i=0; i<n_areas; i++)
+    {
+        Mat activation_area = activation_images
+            .subMat((i/images_width)*area_length,
+                    (i*area_width) % images_width,
+                    area_length,
+                    area_width);
+
+        result -= pl_log(sum(activation_area));
+    }
+    return result;
+}
+
+int RBMLocalMultinomialLayer::getConfigurationCount()
+{
+    real approx_count = pow(real(area_size), n_areas);
+    int count = 1;
+    if (approx_count > 1e30)
+        count = INFINITE_CONFIGURATIONS;
+    else
+        for (int i=0; i<n_areas; i++)
+            count *= area_size;
+
+    return count;
+}
+
+void RBMLocalMultinomialLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLASSERT( output.length() == size );
+    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
+
+    output.clear();
+    Mat output_images = output.toMat(n_images*images_length, images_width);
+    for (int i=0; i<n_areas; i++)
+    {
+        int area_conf_index = conf_index % area_size;
+        conf_index /= area_size;
+
+        Mat output_area = output_images
+            .subMat((i/images_width)*area_length,
+                    (i*area_width) % images_width,
+                    area_length,
+                    area_width );
+
+        output_area(area_conf_index/area_width, area_conf_index%area_width)=1;
+    }
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMLocalMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.h	2007-08-02 03:55:57 UTC (rev 7897)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.h	2007-08-02 04:06:08 UTC (rev 7898)
@@ -0,0 +1,198 @@
+// -*- C++ -*-
+
+// RBMLocalMultinomialLayer.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Pascal Lamblin
+
+/*! \file RBMLocalMultinomialLayer.h */
+
+
+#ifndef RBMLocalMultinomialLayer_INC
+#define RBMLocalMultinomialLayer_INC
+
+#include "RBMLayer.h"
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Multiple multinomial units, each of them seeing an area of nearby pixels
+ *
+ */
+class RBMLocalMultinomialLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Number of images present at the same time in the input vector
+    int n_images;
+
+    //! Length of each of the images
+    int images_length;
+
+    //! Width of each of the images
+    int images_width;
+
+    //! Length of the areas to consider
+    int area_length;
+
+    //! Width of the areas to consider
+    int area_width;
+
+    //#####  Learnt Options  ##################################################
+    int images_size;
+    int area_size;
+    int n_areas;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMLocalMultinomialLayer( real the_learning_rate=0. );
+
+    //! Constructor from the number of units in the multinomial
+    RBMLocalMultinomialLayer( int the_size, real the_learning_rate=0. );
+
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample();
+
+    //! batch version
+    virtual void generateSamples();
+
+    //! compute the expectation
+    virtual void computeExpectation();
+
+    //! batch version
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec& input, Vec& output ) const;
+
+    //! forward propagation with provided bias
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
+                        Vec& output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! Back-propagate the output gradient to the input, and update parameters.
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate = false);
+
+    //! back-propagates the output gradient to the input and the bias
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
+                             const Vec& output,
+                             Vec& input_gradient, Vec& rbm_bias_gradient,
+                             const Vec& output_gradient) ;
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec& target);
+    virtual void fpropNLL(const Mat& targets, const Mat& costs_column);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+    virtual void bpropNLL(const Mat& targets, const Mat& costs_column,
+                          Mat& bias_gradients);
+
+    // Compute -bias' unit_values
+    virtual real energy(const Vec& unit_values) const;
+
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMLocalMultinomialLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMLocalMultinomialLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Thu Aug  2 06:06:51 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 06:06:51 +0200
Subject: [Plearn-commits] r7899 - trunk/commands
Message-ID: <200708020406.l7246pf3020110@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 06:06:50 +0200 (Thu, 02 Aug 2007)
New Revision: 7899

Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Include new class


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-08-02 04:06:08 UTC (rev 7898)
+++ trunk/commands/plearn_noblas_inc.h	2007-08-02 04:06:50 UTC (rev 7899)
@@ -222,6 +222,7 @@
 #include <plearn_learners/online/RBMConv2DConnection.h>
 #include <plearn_learners/online/RBMGaussianLayer.h>
 #include <plearn_learners/online/RBMLayer.h>
+#include <plearn_learners/online/RBMLocalMultinomialLayer.h>
 #include <plearn_learners/online/RBMMatrixConnection.h>
 #include <plearn_learners/online/RBMMatrixTransposeConnection.h>
 #include <plearn_learners/online/RBMMixedConnection.h>



From lamblin at mail.berlios.de  Thu Aug  2 06:26:18 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 06:26:18 +0200
Subject: [Plearn-commits] r7900 -
	trunk/plearn_learners/online/test/ModuleTester
Message-ID: <200708020426.l724QIgO025786@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 06:26:18 +0200 (Thu, 02 Aug 2007)
New Revision: 7900

Modified:
   trunk/plearn_learners/online/test/ModuleTester/pytest.config
Log:
Disable test until we fix gradient computation wrt new contrastive divergence
formula


Modified: trunk/plearn_learners/online/test/ModuleTester/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/pytest.config	2007-08-02 04:06:50 UTC (rev 7899)
+++ trunk/plearn_learners/online/test/ModuleTester/pytest.config	2007-08-02 04:26:18 UTC (rev 7900)
@@ -149,6 +149,6 @@
     resources = [ "PL_ModuleTester_RBM.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = False
+    disabled = True
     )
 



From yoshua at mail.berlios.de  Thu Aug  2 13:30:59 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 2 Aug 2007 13:30:59 +0200
Subject: [Plearn-commits] r7901 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200708021130.l72BUxMw025096@sheep.berlios.de>

Author: yoshua
Date: 2007-08-02 13:30:59 +0200 (Thu, 02 Aug 2007)
New Revision: 7901

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Fixed autolr problem when more than 1 schedule, and when starting from non-zero initial stage


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-02 04:26:18 UTC (rev 7900)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-02 11:30:59 UTC (rev 7901)
@@ -193,14 +193,15 @@
     n_train = len(stages)
     n_schedules = len(lr_options)
     n_tests = len(testsets)
-    results = zeros([n_train, 2 + n_train_costs + n_tests*n_test_costs], Float)
+    results = zeros([n_train, 1 + n_schedules + n_train_costs + n_tests*n_test_costs], Float)
     best_err = 1e10
+    initial_stage=learner.stage
     if plearn.bridgemode.interactive:
         clf()
     colors="bgrcmyk"
     styles=['-', '--', '-.', ':', '.', ',', 'o', '^', 'v', '<', '>', 's', '+', 'x', 'D']
     for i in range(n_train):
-        learner.nstages = int(stages[i])
+        learner.nstages = initial_stage + int(stages[i])
         options = {}
         for s in range(n_schedules):
             for lr_option in lr_options[s]:
@@ -479,6 +480,7 @@
     actives = [0]
     best_candidate = learner
     best_early_stop = stages[0]
+    initial_stage = learner.stage
     if plearn.bridgemode.interactive:
         clf()
         colors="bgrcmyk"
@@ -495,7 +497,7 @@
         for active in actives:
             candidate = all_candidates[active]
             results = all_results[active]
-            candidate.nstages = int(stage)
+            candidate.nstages = initial_stage+int(stage)
             options = {}
             for s in range(n_schedules):
                 for lr_option in lr_options[s]:



From yoshua at mail.berlios.de  Thu Aug  2 13:31:43 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 2 Aug 2007 13:31:43 +0200
Subject: [Plearn-commits] r7902 - trunk/python_modules/plearn/learners/online
Message-ID: <200708021131.l72BVhaL025130@sheep.berlios.de>

Author: yoshua
Date: 2007-08-02 13:31:43 +0200 (Thu, 02 Aug 2007)
New Revision: 7902

Modified:
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
More options in rbm() function of online


Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-08-02 11:30:59 UTC (rev 7901)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-08-02 11:31:43 UTC (rev 7902)
@@ -59,6 +59,7 @@
         compute_nll=False,
         ngcd=1,
         have_reconstruction=True,
+        independent_reconstruction=False,
         compute_contrastive_divergence=True,
         use_Gaussian_inputs=False):
     """Construct an RBMModule"""
@@ -66,6 +67,10 @@
     conx = pl.RBMMatrixConnection(
                 down_size = visible_size,
                 up_size = hidden_size)
+    if independent_reconstruction and have_reconstruction:
+        conx2 = pl.RBMMatrixConnection(
+            down_size = visible_size,
+            up_size = hidden_size)
     return pl.RBMModule(
             name = name,
             cd_learning_rate = cd_learning_rate,
@@ -79,7 +84,9 @@
             hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
             connection = conx,
             reconstruction_connection = ifthenelse(have_reconstruction,
-                                                   conx,
+                                                   ifthenelse(independent_reconstruction,
+                                                              conx2,
+                                                              conx),
                                                    None))
 
 



From yoshua at mail.berlios.de  Thu Aug  2 13:39:21 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 2 Aug 2007 13:39:21 +0200
Subject: [Plearn-commits] r7903 - trunk/python_modules/plearn/learners/online
Message-ID: <200708021139.l72BdLPb025479@sheep.berlios.de>

Author: yoshua
Date: 2007-08-02 13:39:21 +0200 (Thu, 02 Aug 2007)
New Revision: 7903

Modified:
   trunk/python_modules/plearn/learners/online/__init__.py
Log:
Fixed typo


Modified: trunk/python_modules/plearn/learners/online/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/online/__init__.py	2007-08-02 11:31:43 UTC (rev 7902)
+++ trunk/python_modules/plearn/learners/online/__init__.py	2007-08-02 11:39:21 UTC (rev 7903)
@@ -67,10 +67,13 @@
     conx = pl.RBMMatrixConnection(
                 down_size = visible_size,
                 up_size = hidden_size)
-    if independent_reconstruction and have_reconstruction:
-        conx2 = pl.RBMMatrixConnection(
-            down_size = visible_size,
-            up_size = hidden_size)
+    conx2 = None
+    if have_reconstruction:
+        conx2 = conx
+        if independent_reconstruction:
+            conx2 = pl.RBMMatrixConnection(
+                down_size = visible_size,
+                up_size = hidden_size)
     return pl.RBMModule(
             name = name,
             cd_learning_rate = cd_learning_rate,
@@ -83,13 +86,8 @@
                                        pl.RBMBinomialLayer(size = visible_size)),
             hidden_layer = pl.RBMBinomialLayer(size = hidden_size),
             connection = conx,
-            reconstruction_connection = ifthenelse(have_reconstruction,
-                                                   ifthenelse(independent_reconstruction,
-                                                              conx2,
-                                                              conx),
-                                                   None))
+            reconstruction_connection = conx2)
 
-
 def connection(src, dst, propagate_gradient = True):
     """Construct a NetworkConnection"""
     return pl.NetworkConnection(



From nouiz at mail.berlios.de  Thu Aug  2 18:11:41 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Aug 2007 18:11:41 +0200
Subject: [Plearn-commits] r7904 - trunk/plearn/vmat
Message-ID: <200708021611.l72GBfB6012358@sheep.berlios.de>

Author: nouiz
Date: 2007-08-02 18:11:41 +0200 (Thu, 02 Aug 2007)
New Revision: 7904

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
Better error message

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-08-02 11:39:21 UTC (rev 7903)
+++ trunk/plearn/vmat/VMatrix.cc	2007-08-02 16:11:41 UTC (rev 7904)
@@ -1403,7 +1403,7 @@
 /////////
 void VMatrix::put(int i, int j, real value)
 {
-    PLERROR("In VMatrix::put - Method not implemented for this VMat, please implement.");
+    PLERROR("In VMatrix::put - Method not implemented for this VMat(%s), please implement.",classname());
 }
 
 ///////////////



From saintmlx at mail.berlios.de  Thu Aug  2 18:28:29 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 2 Aug 2007 18:28:29 +0200
Subject: [Plearn-commits] r7905 - trunk/plearn/vmat
Message-ID: <200708021628.l72GSTtE013583@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-02 18:28:28 +0200 (Thu, 02 Aug 2007)
New Revision: 7905

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- fixed end-of-file bug for string mappings



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-08-02 16:11:41 UTC (rev 7904)
+++ trunk/plearn/vmat/VMatrix.cc	2007-08-02 16:28:28 UTC (rev 7905)
@@ -1321,6 +1321,7 @@
         {
             map_sr[col][s]   = val;
             map_rs[col][val] = s;
+            f.skipBlanks();
         }
     }
 }



From saintmlx at mail.berlios.de  Thu Aug  2 18:38:16 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 2 Aug 2007 18:38:16 +0200
Subject: [Plearn-commits] r7906 - trunk/plearn/vmat
Message-ID: <200708021638.l72GcGeD001119@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-02 18:38:13 +0200 (Thu, 02 Aug 2007)
New Revision: 7906

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- don't pass string as vararg



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2007-08-02 16:28:28 UTC (rev 7905)
+++ trunk/plearn/vmat/VMatrix.cc	2007-08-02 16:38:13 UTC (rev 7906)
@@ -1404,7 +1404,8 @@
 /////////
 void VMatrix::put(int i, int j, real value)
 {
-    PLERROR("In VMatrix::put - Method not implemented for this VMat(%s), please implement.",classname());
+    PLERROR("In VMatrix::put - Method not implemented for this VMat(%s), please implement.",
+            classname().c_str());
 }
 
 ///////////////



From nouiz at mail.berlios.de  Thu Aug  2 19:49:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Aug 2007 19:49:53 +0200
Subject: [Plearn-commits] r7907 - trunk/plearn/vmat
Message-ID: <200708021749.l72Hnr4x005336@sheep.berlios.de>

Author: nouiz
Date: 2007-08-02 19:49:51 +0200 (Thu, 02 Aug 2007)
New Revision: 7907

Modified:
   trunk/plearn/vmat/ConcatColumnsVMatrix.cc
   trunk/plearn/vmat/ConcatColumnsVMatrix.h
Log:
Added a put() functions that forward the call to the correct sub matrix

Modified: trunk/plearn/vmat/ConcatColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2007-08-02 16:38:13 UTC (rev 7906)
+++ trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2007-08-02 17:49:51 UTC (rev 7907)
@@ -310,6 +310,21 @@
     deepCopyField(sources, copies);
 }
 
+void ConcatColumnsVMatrix::put(int i, int j, real value)
+{
+#ifdef BOUNDCHECK
+    if(j>=width_)
+        PLERROR("access out of bound. Width=%i accessed col=%i",width_,j);
+#endif
+    int pos=0,k=0;
+    while(j>=pos+sources[k]->width())
+    {
+        pos += sources[k]->width();
+        k++;
+    }
+    sources[k]->put(i,j-pos,value);
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/vmat/ConcatColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/ConcatColumnsVMatrix.h	2007-08-02 16:38:13 UTC (rev 7906)
+++ trunk/plearn/vmat/ConcatColumnsVMatrix.h	2007-08-02 17:49:51 UTC (rev 7907)
@@ -102,6 +102,9 @@
     //! Gives the possible values of a certain field (column) given the input
     virtual void getValues(const Vec& input, int col, Vec& values) const;
 
+    //! Sets element (i,j) to value.
+    virtual void put(int i, int j, real value);
+
 private:
     void build_();
 



From yoshua at mail.berlios.de  Thu Aug  2 20:13:23 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 2 Aug 2007 20:13:23 +0200
Subject: [Plearn-commits] r7908 - trunk/plearn_learners/online
Message-ID: <200708021813.l72IDNWd008139@sheep.berlios.de>

Author: yoshua
Date: 2007-08-02 20:13:23 +0200 (Thu, 02 Aug 2007)
New Revision: 7908

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed a bug due to incorrect setting of hidden_activations_are_computed in RBMModule


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-02 17:49:51 UTC (rev 7907)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-02 18:13:23 UTC (rev 7908)
@@ -625,12 +625,12 @@
 {
     int hidden_configurations = hidden_layer->getConfigurationCount();
     int visible_configurations = visible_layer->getConfigurationCount();
-
+	
     PLASSERT_MSG(hidden_configurations != RBMLayer::INFINITE_CONFIGURATIONS ||
                  visible_configurations != RBMLayer::INFINITE_CONFIGURATIONS,
                  "To compute exact log-likelihood of an RBM maximum configurations of hidden "
                  "or visible layer must be less than 2^31.");
-
+	
     // Compute partition function
     if (hidden_configurations > visible_configurations)
         // do it by log-summing minus-free-energy of visible configurations
@@ -650,6 +650,7 @@
             else
                 log_partition_function = logadd(log_partition_function,-free_energy(0,0));
         }
+        hidden_activations_are_computed = false;
     }
     else
         // do it by summing free-energy of hidden configurations
@@ -763,37 +764,6 @@
         }
         found_a_valid_configuration = true;
     }
-    // COMPUTE UNSUPERVISED NLL
-    if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
-    {
-        if (partition_function_is_stale && !during_training)
-        {
-            computePartitionFunction();
-            partition_function_is_stale=false;
-        }
-        if (visible && !visible->isEmpty()
-            && hidden && !hidden->isEmpty())
-        {
-            // neg-log-likelihood(visible,hidden) = energy(visible,hidden) + log(partition_function)
-            computeEnergy(*visible,*hidden,*neg_log_likelihood);
-            *neg_log_likelihood += log_partition_function;
-        }
-        else if (visible && !visible->isEmpty())
-        {
-            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
-            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood,hidden_act);
-            *neg_log_likelihood += log_partition_function;
-        }
-        else if (hidden && !hidden->isEmpty())
-        {
-            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
-            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
-            *neg_log_likelihood += log_partition_function;
-        }
-        else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
-        found_a_valid_configuration = true;
-    }
-
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -891,6 +861,7 @@
             {
                 PLASSERT( reconstruction_error->isEmpty() );
                 reconstruction_error->resize(visible->length(),1);
+                visible_layer->setBatchSize( visible->length() );
                 visible_layer->fpropNLL(*visible,
                                         *reconstruction_error);
             }
@@ -1144,7 +1115,38 @@
         found_a_valid_configuration = true;
     }
 
+    // COMPUTE UNSUPERVISED NLL
+    if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
+    {
+        if (partition_function_is_stale && !during_training)
+        {
+            computePartitionFunction();
+            partition_function_is_stale=false;
+        }
+        if (visible && !visible->isEmpty()
+            && hidden && !hidden->isEmpty())
+        {
+            // neg-log-likelihood(visible,hidden) = energy(visible,hidden) + log(partition_function)
+            computeEnergy(*visible,*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (visible && !visible->isEmpty())
+        {
+            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
+            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood,hidden_act);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (hidden && !hidden->isEmpty())
+        {
+            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
+            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
+        found_a_valid_configuration = true;
+    }
 
+
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;



From yoshua at mail.berlios.de  Thu Aug  2 20:53:20 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 2 Aug 2007 20:53:20 +0200
Subject: [Plearn-commits] r7909 - trunk/plearn_learners/online
Message-ID: <200708021853.l72IrKOi009810@sheep.berlios.de>

Author: yoshua
Date: 2007-08-02 20:53:20 +0200 (Thu, 02 Aug 2007)
New Revision: 7909

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed gradient scaling for free energy gradient in RBMModule


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-02 18:13:23 UTC (rev 7908)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-02 18:53:20 UTC (rev 7909)
@@ -1670,6 +1670,7 @@
         PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
         PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
         PLASSERT(connection->classname()=="RBMMatrixConnection");
+        PLASSERT(hidden && !hidden->isEmpty());
         // FE(x) = -b'x - sum_i softplus(hidden_layer->activation[i])        
         // dFE(x)/dx = -b - sum_i sigmoid(hidden_layer->activation[i]) W_i
         // dC/dxt = -b dC/dFE - dC/dFE sum_i p_ti W_i
@@ -1679,7 +1680,7 @@
                         get_pointer(connection))->weights;
         bool same_dC_dFE=true;
         real dC_dFE=(*energy_grad)(0,0);
-        const Mat& p = hidden_layer->getExpectations();
+        const Mat& p = *hidden;
         for (int t=0;t<mbs;t++)
         {
             real new_dC_dFE=(*energy_grad)(t,0);
@@ -1689,7 +1690,7 @@
             multiplyAcc((*visible_grad)(t),visible_layer->bias,-dC_dFE);
         }
         if (same_dC_dFE)
-            productScaleAcc(*visible_grad,p,false,weights,false,-1,1);
+            productScaleAcc(*visible_grad,p,false,weights,false,-dC_dFE,1);
         else
             for (int t=0;t<mbs;t++)
                 productScaleAcc((*visible_grad)(t),weights,true,p(t),-dC_dFE,1);



From nouiz at mail.berlios.de  Thu Aug  2 21:23:31 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Aug 2007 21:23:31 +0200
Subject: [Plearn-commits] r7910 - trunk/plearn_learners/generic
Message-ID: <200708021923.l72JNVCE011393@sheep.berlios.de>

Author: nouiz
Date: 2007-08-02 21:23:31 +0200 (Thu, 02 Aug 2007)
New Revision: 7910

Modified:
   trunk/plearn_learners/generic/EmbeddedLearner.cc
   trunk/plearn_learners/generic/EmbeddedLearner.h
Log:
added the options forward_nstages that forward the nstages to the sub learner before calling the train from the sublearner

This is needed to use AddCostToLearner with autolr


Modified: trunk/plearn_learners/generic/EmbeddedLearner.cc
===================================================================
--- trunk/plearn_learners/generic/EmbeddedLearner.cc	2007-08-02 18:53:20 UTC (rev 7909)
+++ trunk/plearn_learners/generic/EmbeddedLearner.cc	2007-08-02 19:23:31 UTC (rev 7910)
@@ -63,7 +63,8 @@
     : learner_(0),
       expdir_append(expdir_append_),
       forward_test(false),
-      provide_learner_expdir(true)
+      provide_learner_expdir(true),
+      forward_nstages(false)
 { }
 
 void EmbeddedLearner::declareOptions(OptionList& ol)
@@ -81,6 +82,10 @@
                   OptionBase::buildoption,
                   "A string which should be appended to the expdir for the inner learner;"
                   "default = \"\".");
+    declareOption(ol, "forward_nstages",&EmbeddedLearner::forward_nstages,
+                  OptionBase::buildoption,
+                  "Did we forward our value of nstages to the sublearner before calling "
+                  "the sublearner train()");
 
     // 'forward_test' is set as a 'nosave' option: each subclass should set it
     // to either 'true' or 'false' depending on its specific needs.
@@ -174,6 +179,8 @@
 void EmbeddedLearner::train()
 {
     PLASSERT( learner_ );
+    if(forward_nstages)
+        learner_->nstages = nstages;
     learner_->train();
     stage = learner_->stage;
 }

Modified: trunk/plearn_learners/generic/EmbeddedLearner.h
===================================================================
--- trunk/plearn_learners/generic/EmbeddedLearner.h	2007-08-02 18:53:20 UTC (rev 7909)
+++ trunk/plearn_learners/generic/EmbeddedLearner.h	2007-08-02 19:23:31 UTC (rev 7910)
@@ -63,6 +63,7 @@
     bool forward_test;
     bool provide_learner_expdir;
 
+    bool forward_nstages;
     // ****************
     // * Constructors *
     // ****************



From nouiz at mail.berlios.de  Thu Aug  2 21:24:37 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Aug 2007 21:24:37 +0200
Subject: [Plearn-commits] r7911 - trunk/plearn_learners/generic
Message-ID: <200708021924.l72JObFv011457@sheep.berlios.de>

Author: nouiz
Date: 2007-08-02 21:24:37 +0200 (Thu, 02 Aug 2007)
New Revision: 7911

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
added comment

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-02 19:23:31 UTC (rev 7910)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-02 19:24:37 UTC (rev 7911)
@@ -596,7 +596,6 @@
                 "threshold and no *class_error costs are selected.\n"
                 "We use the first *class_error cost to select the threshold");
     }
-
     inherited::train();
     
     if(-1 != find_threshold){
@@ -670,6 +669,9 @@
 void AddCostToLearner::computeOutputsAndCosts(const Mat& input, const Mat& target,
                                              Mat& output, Mat& costs) const {
     PLASSERT( learner_ );
+    //done this way to use a possibly optimizer version 
+    //of computeOutputsAndCosts from the sub learner as with NatGradNNet
+    //with a minibatch_size>1
     Mat sub_costs = costs.subMatColumns(0, learner_->nTestCosts());
     learner_->computeOutputsAndCosts(input, target, output, sub_costs);
     for (int i=0;i<input.length();i++)



From nouiz at mail.berlios.de  Thu Aug  2 21:53:05 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Aug 2007 21:53:05 +0200
Subject: [Plearn-commits] r7915 - trunk/plearn_learners/regressors
Message-ID: <200708021953.l72Jr5N7013920@sheep.berlios.de>

Author: nouiz
Date: 2007-08-02 21:53:04 +0200 (Thu, 02 Aug 2007)
New Revision: 7915

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
-Made RegressionTreeRegisters look more like a VMatrix for its functions and variable name
-Added VMatrix function to RegressionTreeRegisters
-put global variable name local


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2007-08-02 19:37:12 UTC (rev 7914)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2007-08-02 19:53:04 UTC (rev 7915)
@@ -182,8 +182,8 @@
     missing_leave_id = train_set->getNextId();
     left_leave_id =  train_set->getNextId();
     right_leave_id =  train_set->getNextId();
-    length = train_set->getLength();
-    inputsize = train_set->getInputsize();
+    length = train_set->length();
+    inputsize = train_set->inputsize();
     missing_leave = ::PLearn::deepCopy(leave_template);
     missing_leave->setOption("id", tostring(missing_leave_id));
     if (missing_is_valid > 0) missing_leave->setOption("missing_leave", "0");

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-08-02 19:37:12 UTC (rev 7914)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-08-02 19:53:04 UTC (rev 7915)
@@ -73,15 +73,15 @@
       
     declareOption(ol, "next_id", &RegressionTreeRegisters::next_id, OptionBase::learntoption,
                   "The next id for creating a new leave\n");
-    declareOption(ol, "length", &RegressionTreeRegisters::length, OptionBase::learntoption,
+    declareOption(ol, "length", &RegressionTreeRegisters::length_, OptionBase::learntoption,
                   "The length of the train set\n");
-    declareOption(ol, "width", &RegressionTreeRegisters::width, OptionBase::learntoption,
+    declareOption(ol, "width", &RegressionTreeRegisters::width_, OptionBase::learntoption,
                   "The width of the train set\n");
-    declareOption(ol, "inputsize", &RegressionTreeRegisters::inputsize, OptionBase::learntoption,
+    declareOption(ol, "inputsize", &RegressionTreeRegisters::inputsize_, OptionBase::learntoption,
                   "The input size of the train set\n");
-    declareOption(ol, "targetsize", &RegressionTreeRegisters::targetsize, OptionBase::learntoption,
+    declareOption(ol, "targetsize", &RegressionTreeRegisters::targetsize_, OptionBase::learntoption,
                   "The target size of the train set\n");
-    declareOption(ol, "weightsize", &RegressionTreeRegisters::weightsize, OptionBase::learntoption,
+    declareOption(ol, "weightsize", &RegressionTreeRegisters::weightsize_, OptionBase::learntoption,
                   "The weight of each sample in the train set\n");
     declareOption(ol, "sorted_row", &RegressionTreeRegisters::sorted_row, OptionBase::learntoption,
                   "The matrix holding the sequence of samples in ascending value order for each dimension\n");
@@ -97,15 +97,7 @@
 void RegressionTreeRegisters::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(report_progress, copies);
-    deepCopyField(verbosity, copies);
     deepCopyField(train_set, copies);
-    deepCopyField(next_id, copies);
-    deepCopyField(length, copies);
-    deepCopyField(width, copies);
-    deepCopyField(inputsize, copies);
-    deepCopyField(targetsize, copies);
-    deepCopyField(weightsize, copies);
     deepCopyField(sorted_row, copies);
     deepCopyField(inverted_sorted_row, copies);
     deepCopyField(leave_register, copies);
@@ -125,13 +117,13 @@
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
 {
     train_set = the_train_set;
-    length = train_set->length();
-    width = train_set->width();
-    inputsize = train_set->inputsize();
-    targetsize = train_set->targetsize();
-    weightsize = train_set->weightsize();
-    leave_register.resize(length);
-    leave_candidate.resize(length);
+    length_ = train_set->length();
+    width_ = train_set->width();
+    inputsize_ = train_set->inputsize();
+    targetsize_ = train_set->targetsize();
+    weightsize_ = train_set->weightsize();
+    leave_register.resize(length());
+    leave_candidate.resize(length());
     sortRows();
 }
 
@@ -157,18 +149,19 @@
 
 real RegressionTreeRegisters::getTarget(int row)
 {
-    return train_set->get(row, inputsize);
+    return train_set->get(row, inputsize());
 }
 
 real RegressionTreeRegisters::getWeight(int row)
 {
-    if (weightsize <= 0) return 1.0 / length;
-    else return train_set->get(row, inputsize + 1);
+    if (weightsize() <= 0) return 1.0 / length();
+    else return train_set->get(row, inputsize() + targetsize() );
 }
 
-int RegressionTreeRegisters::getLength()
+void RegressionTreeRegisters::setWeight(int row, real val)
 {
-    return length;
+    PLASSERT(weightsize() > 0);
+    train_set->put(row, inputsize() + targetsize(), val );
 }
 
 int RegressionTreeRegisters::getNextId()
@@ -177,19 +170,15 @@
     return next_id;
 }
 
-int RegressionTreeRegisters::getInputsize()
-{
-    return inputsize;
-}
-
 int RegressionTreeRegisters::getNextRegisteredRow(int leave_id, int col, int previous_row)
 {
-    if (previous_row >= length) return length;
+    if (previous_row >= length()) return length();
+    int each_train_sample_index;
     if (previous_row < 0) each_train_sample_index = 0;
     else each_train_sample_index = inverted_sorted_row(previous_row, col) + 1;
     while (true)
     {
-        if (each_train_sample_index >= length) return length;
+        if (each_train_sample_index >= length()) return length();
         if (leave_register[sorted_row(each_train_sample_index, col)] == leave_id) break;
         each_train_sample_index += 1;
     }
@@ -198,12 +187,13 @@
 
 int RegressionTreeRegisters::getNextCandidateRow(int leave_id, int col, int previous_row)
 {
-    if (previous_row >= length) return length;
+    if (previous_row >= length()) return length();
+    int each_train_sample_index;
     if (previous_row < 0) each_train_sample_index = 0;
     else each_train_sample_index = inverted_sorted_row(previous_row, col) + 1;
     while (true)
     {
-        if (each_train_sample_index >= length) return length;
+        if (each_train_sample_index >= length()) return length();
         if (leave_candidate[sorted_row(each_train_sample_index, col)] == leave_id) break;
         each_train_sample_index += 1;
     }
@@ -213,34 +203,34 @@
 void RegressionTreeRegisters::sortRows()
 {
     next_id = 0;
-    if (sorted_row.length() == length && sorted_row.width() == inputsize)
+    if (sorted_row.length() == length() && sorted_row.width() == inputsize())
     {
         verbose("RegressionTreeRegisters: Sorted train set indices are present, no sort required", 3);
         return;
     }
     verbose("RegressionTreeRegisters: The train set is being sorted", 3);
-    sorted_row.resize(length, inputsize);
+    sorted_row.resize(length(), inputsize());
     PP<ProgressBar> pb;
     if (report_progress)
     {
-        pb = new ProgressBar("RegressionTreeRegisters : sorting the train set on input dimensions: ", inputsize);
+        pb = new ProgressBar("RegressionTreeRegisters : sorting the train set on input dimensions: ", inputsize());
     }
-    for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
+    for (int each_train_sample_index = 0; each_train_sample_index < length(); each_train_sample_index++)
     {
-        for (sample_dim = 0; sample_dim < inputsize; sample_dim++)
+        for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
         {
             sorted_row(each_train_sample_index, sample_dim) = each_train_sample_index;
         }
     }
-    for (sample_dim = 0; sample_dim < inputsize; sample_dim++)
+    for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
     {
         sortEachDim(sample_dim);
         if (report_progress) pb->update(sample_dim);
     }
-    inverted_sorted_row.resize(length, inputsize);
-    for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
+    inverted_sorted_row.resize(length(), inputsize());
+    for (int each_train_sample_index = 0; each_train_sample_index < length(); each_train_sample_index++)
     {
-        for (sample_dim = 0; sample_dim < inputsize; sample_dim++)
+        for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
         {
             inverted_sorted_row(sorted_row(each_train_sample_index, sample_dim), sample_dim) = each_train_sample_index;
         }
@@ -250,7 +240,7 @@
 void RegressionTreeRegisters::sortEachDim(int dim)
 {
     int start_index = 0;
-    int end_index = length - 1;
+    int end_index = length() - 1;
     int forward_index;
     int backward_index;
     int stack_index = -1;
@@ -284,7 +274,7 @@
                 swapIndex(start_index, start_index + 1, dim);
             forward_index = start_index + 1;
             backward_index = end_index;
-            sample_feature = train_set->get(sorted_row(start_index + 1, dim), dim);
+            real sample_feature = train_set->get(sorted_row(start_index + 1, dim), dim);
             for (;;)
             {
                 do forward_index++; while (compare(train_set->get(sorted_row(forward_index, dim), dim), sample_feature) < 0.0);
@@ -317,12 +307,13 @@
   
 void RegressionTreeRegisters::sortSmallSubArray(int the_start_index, int the_end_index, int dim)
 {
-    for (next_train_sample_index = the_start_index + 1;
+    for (int next_train_sample_index = the_start_index + 1;
          next_train_sample_index <= the_end_index;
          next_train_sample_index++)
     {
-        saved_index = sorted_row(next_train_sample_index, dim);
-        sample_feature = train_set->get(saved_index, dim);
+        int saved_index = sorted_row(next_train_sample_index, dim);
+        real sample_feature = train_set->get(saved_index, dim);
+        int each_train_sample_index;
         for (each_train_sample_index = next_train_sample_index - 1;
              each_train_sample_index >= the_start_index;
              each_train_sample_index--)
@@ -339,7 +330,7 @@
 
 void RegressionTreeRegisters::swapIndex(int index_i, int index_j, int dim)
 {
-    saved_index = sorted_row(index_i, dim);
+    int saved_index = sorted_row(index_i, dim);
     sorted_row(index_i, dim) = sorted_row(index_j, dim);
     sorted_row(index_j, dim) = saved_index;
 }
@@ -355,9 +346,9 @@
 void RegressionTreeRegisters::printRegisters()
 {
     cout << "candidate: ";
-    for (int ii = 0; ii < length; ii++) cout << " " << tostring(leave_candidate[ii]);
+    for (int ii = 0; ii < length(); ii++) cout << " " << tostring(leave_candidate[ii]);
     cout << " register:  ";
-    for (int ii = 0; ii < length; ii++) cout << " " << tostring(leave_register[ii]);
+    for (int ii = 0; ii < length(); ii++) cout << " " << tostring(leave_register[ii]);
     cout << endl;
 }
 
@@ -367,6 +358,30 @@
         cout << the_msg << endl;
 }
 
+void RegressionTreeRegisters::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+    if(inputsize_<0)
+        PLERROR("In VMatrix::getExample, inputsize_ not defined for this vmat");
+    input.resize(inputsize_);
+    train_set->getSubRow(i,0,input);
+    if(targetsize_<0)
+        PLERROR("In VMatrix::getExample, targetsize_ not defined for this vmat");
+    target.resize(targetsize_);
+    if (targetsize_ > 0) {
+        train_set->getSubRow(i,inputsize_,target);
+    }
+
+    if(weightsize()==0)
+        weight = 1;
+    else if(weightsize()<0)
+        PLERROR("In VMatrix::getExample, weightsize_ not defined for this vmat");
+    else if(weightsize()>1)
+        PLERROR("In VMatrix::getExample, weightsize_ >1 not supported by this call");
+    else
+        weight = train_set->get(i,inputsize()+targetsize());
+}
+
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2007-08-02 19:37:12 UTC (rev 7914)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2007-08-02 19:53:04 UTC (rev 7915)
@@ -69,26 +69,16 @@
 */
 
     int       next_id;
-    int       length;
-    int       width;
-    int       inputsize;
-    int       targetsize;
-    int       weightsize;  
+    int       length_;
+    int       width_;
+    int       inputsize_;
+    int       targetsize_;
+    int       weightsize_;  
     TMat<int> sorted_row;
     TMat<int> inverted_sorted_row;
     TVec<int> leave_register;
     TVec<int> leave_candidate;
  
-/*
-  Work fields: they are sized and initialized if need be, at buid time
-*/  
- 
-    int  each_train_sample_index;
-    int  next_train_sample_index;
-    int  saved_index;
-    int  sample_dim;
-    real sample_feature;
-  
 public:
     RegressionTreeRegisters();
     virtual              ~RegressionTreeRegisters();
@@ -105,13 +95,18 @@
     real         getFeature(int row, int col);
     real         getTarget(int row);
     real         getWeight(int row);
-    int          getLength();
+    void         setWeight(int row,real val);
+    int          length(){return length_;}
     int          getNextId();
-    int          getInputsize();
+    int          inputsize(){return inputsize_;}
+    int          targetsize(){return targetsize_;}
+    int          weightsize(){return weightsize_;}
     int          getNextRegisteredRow(int leave_id, int col, int previous_row);
     int          getNextCandidateRow(int leave_id, int col, int previous_row);
     void         sortRows();
     void         printRegisters();
+    void         getExample(int i, Vec& input, Vec& target, real& weight);
+
 private:
     void         build_();
     void         sortEachDim(int dim);



From tihocan at mail.berlios.de  Thu Aug  2 21:28:28 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 2 Aug 2007 21:28:28 +0200
Subject: [Plearn-commits] r7913 - trunk/plearn_learners/online
Message-ID: <200708021928.l72JSShk011619@sheep.berlios.de>

Author: tihocan
Date: 2007-08-02 21:28:28 +0200 (Thu, 02 Aug 2007)
New Revision: 7913

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed bug in energy gradient computation

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-02 19:28:27 UTC (rev 7912)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-02 19:28:28 UTC (rev 7913)
@@ -1693,7 +1693,8 @@
             productScaleAcc(*visible_grad,p,false,weights,false,-dC_dFE,1);
         else
             for (int t=0;t<mbs;t++)
-                productScaleAcc((*visible_grad)(t),weights,true,p(t),-dC_dFE,1);
+                productScaleAcc((*visible_grad)(t),weights,true,p(t),
+                        -(*energy_grad)(t, 0),1);
     }
 
     // Explicit error message in the case of the 'visible' port.



From lamblin at mail.berlios.de  Thu Aug  2 21:37:13 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 2 Aug 2007 21:37:13 +0200
Subject: [Plearn-commits] r7914 - trunk/plearn_learners/online
Message-ID: <200708021937.l72JbDxU012408@sheep.berlios.de>

Author: lamblin
Date: 2007-08-02 21:37:12 +0200 (Thu, 02 Aug 2007)
New Revision: 7914

Modified:
   trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
Log:
Fix bug in computation of free energy


Modified: trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2007-08-02 19:28:28 UTC (rev 7913)
+++ trunk/plearn_learners/online/RBMLocalMultinomialLayer.cc	2007-08-02 19:37:12 UTC (rev 7914)
@@ -73,6 +73,21 @@
     }
 }
 
+template <class T>
+T logadd(const TMat<T>& mat)
+{
+    if (mat.isEmpty())
+        return LOG_INIT;
+
+    TMatElementIterator<real> p_mat = mat.begin();
+    T sum = *p_mat++;
+
+    for (int i=1; i<mat.size(); i++, p_mat++)
+        sum = logadd(sum, *p_mat);
+
+    return sum;
+}
+
 int multinomial_sample(const PP<PRandom>& rg, const Mat& distribution)
 {
     real u = rg->uniform_sample();
@@ -789,7 +804,8 @@
 {
     PLASSERT( activation_values.size() == size );
 
-    // result = -\sum_{i=0}^{n_areas-1} log(\sum_{j=0}^{area_size-1} a_{ij})
+    // result =
+    //  -\sum_{i=0}^{n_areas-1} log(\sum_{j=0}^{area_size-1} exp(a_{ij}))
     real result = 0;
     Mat activation_images = activation_values
         .toMat(n_images*images_length, images_width);
@@ -801,7 +817,7 @@
                     area_length,
                     area_width);
 
-        result -= pl_log(sum(activation_area));
+        result -= logadd(activation_area);
     }
     return result;
 }

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-08-02 19:28:28 UTC (rev 7913)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-08-02 19:37:12 UTC (rev 7914)
@@ -434,7 +434,8 @@
 real RBMMultinomialLayer::freeEnergyContribution(const Vec& unit_activations)
     const
 {
-    return -pl_log(sum(unit_activations));
+    // result = -log(\sum_{i=0}^{size-1} exp(a_i))
+    return -logadd(unit_activations);
 }
 
 int RBMMultinomialLayer::getConfigurationCount()



From nouiz at mail.berlios.de  Thu Aug  2 21:28:28 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 2 Aug 2007 21:28:28 +0200
Subject: [Plearn-commits] r7912 - trunk/plearn_learners/regressors
Message-ID: <200708021928.l72JSSp9011609@sheep.berlios.de>

Author: nouiz
Date: 2007-08-02 21:28:27 +0200 (Thu, 02 Aug 2007)
New Revision: 7912

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
Log:
added PLERROR

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-08-02 19:24:37 UTC (rev 7911)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-08-02 19:28:27 UTC (rev 7912)
@@ -139,7 +139,8 @@
             break;      
         }
     }
-    if (multiclass_found < 1) cout << "RegressionTreeMultilassLeave: Unknown target: " << tostring(target) << " row: " << tostring(row) << endl;
+    if (multiclass_found < 1) 
+        PLERROR("RegressionTreeMultilassLeave: Unknown target: %d row: %d\n", target,row);
     computeOutputAndError();
     getOutput(outputv);
     getError(errorv);
@@ -167,6 +168,10 @@
 
 void RegressionTreeMulticlassLeave::computeOutputAndError()
 {
+#ifdef BOUNDCHECK
+    if(multiclass_outputs.length()<=0)
+        PLERROR("In RegressionTreeMulticlassLeave::computeOutputAndError() - multiclass_outputs must not be empty");
+#endif
     multiclass_winer = 0;
     for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
     {



From lamblin at mail.berlios.de  Fri Aug  3 01:04:02 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Fri, 3 Aug 2007 01:04:02 +0200
Subject: [Plearn-commits] r7916 - trunk/plearn/io
Message-ID: <200708022304.l72N42F0009482@sheep.berlios.de>

Author: lamblin
Date: 2007-08-03 01:04:02 +0200 (Fri, 03 Aug 2007)
New Revision: 7916

Modified:
   trunk/plearn/io/PStream.h
Log:
Fix bug in binary format serialization, when trying to write a sequence
of non-base type.


Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-08-02 19:53:04 UTC (rev 7915)
+++ trunk/plearn/io/PStream.h	2007-08-02 23:04:02 UTC (rev 7916)
@@ -1041,7 +1041,7 @@
    operator>> and should not be called by user code directly */
 
 template<class Iterator>
-void binwrite_(PStream& out, Iterator& it, unsigned int n)
+void binwrite_(PStream& out, Iterator it, unsigned int n)
 {
     PStream::mode_t outmode = out.outmode; // store previous outmode
     if(outmode!=PStream::raw_binary && outmode!=PStream::plearn_binary)



From nouiz at mail.berlios.de  Fri Aug  3 16:54:48 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 3 Aug 2007 16:54:48 +0200
Subject: [Plearn-commits] r7917 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200708031454.l73EsmQ1000826@sheep.berlios.de>

Author: nouiz
Date: 2007-08-03 16:54:47 +0200 (Fri, 03 Aug 2007)
New Revision: 7917

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
-Better removing of active candidate\n-Better printing\n-Corrected bug

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-02 23:04:02 UTC (rev 7916)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-03 14:54:47 UTC (rev 7917)
@@ -388,8 +388,12 @@
     if plearn.bridgemode.useserver:
         servers[0][1] = 1 # learner
 
-    # although 1 is probably too small
-    min_epochs_to_delete = max(1, min_epochs_to_delete)
+    train_costnames = learner.getTrainCostNames()
+    if selected_costnames is not None:
+        # Filter out unwanted costnames
+        train_costnames = [ name for name in train_costnames
+                            if name in selected_costnames ]
+    n_train_costs = ifthenelse(get_train_costs,len(train_costnames),0)
 
     def error_curve(active,start_t,current_t):
         delta_t = current_t+1-start_t
@@ -398,29 +402,25 @@
         # cost number 'cost' (index in the selected_costnames list),
         # in testset 'test'.
         # And testset 0 is the one used for selection.
-        return all_results[active][start_t:current_t+1,2+cost_to_select_best]
+        return all_results[active][start_t:current_t+1,
+                                   2+cost_to_select_best+n_train_costs]
 
     def error_curve_dominates(c1,c2,t):
         """curve1 has a lower last error than curve2, but will curve2
         eventually cross curve1? if yes return False o/w return True"""
 
-        start_t = max(all_start[c1],all_start[c2])
-        curve1 = error_curve(c1,start_t,t)
-        curve2 = error_curve(c2,start_t,t)
-        if curve1.shape[0]-1<min_epochs_to_delete or curve1[-1]>=curve2[-1]:
-            return False
-        slope1=curve1[-1]-curve1[-2]
-        slope2=curve2[-1]-curve2[-2]
-        if  slope1 >= slope2:
-            return False
-
         # check to see if c2 is alone with its learning rate (or nearby);
         # if yes keep it
         alone=True
         c2lr=all_lr[c2]
         c2err=all_last_err[c2]
+        start_t = max(all_start[c1],all_start[c2])
+        curve1 = error_curve(c1,start_t,t)#[valid_error]
+        curve2 = error_curve(c2,start_t,t)
+        if curve2.shape[0]-1<min_epochs_to_delete:
+            return False
         for a in actives:
-            if all_lr[a]==c2lr and all_last_err[a]<=c2err:
+            if a!=c2 and all_lr[a]==c2lr and all_last_err[a]<=c2err:
                 # throw it away if worse than other actives of same lr
                 return True
 
@@ -428,24 +428,29 @@
             # and greater lr
             if a!=c2 and all_lr[a]>c2lr and abs(log(all_lr[a]/c2lr))<keep_lr*log(lr_steps):
                 alone=False
+
+        if curve1[-1]>=curve2[-1]:
+            return False
+        slope1=curve1[-1]-curve1[-2]
+        slope2=curve2[-1]-curve2[-2]
+        if  slope1 >= slope2 or all_last_err[c1] >= c2err:
+            return False
+
         c1lr=all_lr[c1]
         if alone and c2lr>c1lr: # and slope2<0:
             # keep if alone and a larger learning rate and improving
             return False
         return True
 
-    train_costnames = learner.getTrainCostNames()
+    # although 1 is probably too small
+    min_epochs_to_delete = max(1, min_epochs_to_delete)
+
     test_costnames = getTestCostNames(learner)
+    n_tests = len(testsets)
+    #n_costs = len(cost_indices)
     if selected_costnames is not None:
-        # Filter out unwanted costnames
-        train_costnames = [ name for name in train_costnames
-                            if name in selected_costnames ]
         test_costnames = [ name for name in test_costnames
                            if name in selected_costnames ]
-
-    n_tests = len(testsets)
-    #n_costs = len(cost_indices)
-    n_train_costs = ifthenelse(get_train_costs,len(train_costnames),0)
     n_test_costs = len(test_costnames)
 
     if schedules:
@@ -488,8 +493,9 @@
     for t in range(n_train):
         stage=stages[t]
         if logfile:
-            print >>logfile, "At stage ", stage
-        print "actives now: ",actives, " with lr=", array(all_lr)[actives]
+            print >>logfile, "After ", stage, "stage"
+        print "After",stage,"stages, actives now: ",actives, " with lr=", array(all_lr)[actives]
+        print "current best actives:",best_active,"best_error:",all_last_err[best_active],"lr:",all_lr[best_active]
         print >>logfile, "actives now: ",actives, " with lr=", array(all_lr)[actives]
 
         threads = []
@@ -591,7 +597,7 @@
         if previous_best_err >= best_err:
             previous_best_err = best_err
             if logfile:
-                print >>logfile,"BEST to now is candidate ",best_active," with err=",best_err
+                print >>logfile,"BEST to now is candidate ",best_active," with err=",best_err,"and lr=",all_lr[best_active]
                 print >>logfile, "stage\tl.rate\t",
                 if get_train_costs:
                     for costname in train_costnames:
@@ -609,7 +615,7 @@
         else:
             if logfile:
                 print >>logfile, "THE BEST ACTIVE HAS GOTTEN WORSE!!!!"
-        if s%(epoch*nskip)==0 and s<nstages:
+        if learner.stage%(epoch*nskip)==0 and learner.stage<nstages:
             best_active = actives[argmin(array(all_last_err)[actives])]
             # remove candidates that are worse and have higher slope
             best_last = all_last_err[best_active]
@@ -638,7 +644,7 @@
                 all_last_err.append(best_last)
                 all_start.append(t)
                 if logfile:
-                    print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at epoch ",s," with lr=",all_lr[new_a]
+                    print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at stage ",learner.stage," with lr=",all_lr[new_a]
                     logfile.flush()
     if return_best_model:
         final_model = best_candidate



From tihocan at mail.berlios.de  Fri Aug  3 17:55:37 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 3 Aug 2007 17:55:37 +0200
Subject: [Plearn-commits] r7918 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200708031555.l73Ftbex006085@sheep.berlios.de>

Author: tihocan
Date: 2007-08-03 17:55:36 +0200 (Fri, 03 Aug 2007)
New Revision: 7918

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
Added learning rate decrease constant mechanism.


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-03 14:54:47 UTC (rev 7917)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-03 15:55:36 UTC (rev 7918)
@@ -706,7 +706,9 @@
     // The other ones are 0/1 values that are initialized with 0, and take 1
     // once the corresponding CPU has finished all updates for this training
     // period.
-    semaphore_id = semget(IPC_PRIVATE, ncpus + 1, 0666 | IPC_CREAT);
+    // Finally, the last value is the current stage, i.e. the number of samples
+    // with which the network has been updated so far.
+    semaphore_id = semget(IPC_PRIVATE, ncpus + 2, 0666 | IPC_CREAT);
     if (semaphore_id == -1)
         PLERROR("In NatGradSMPNNet::train - Could not create semaphore "
                 "(errno = %d)", errno);
@@ -719,6 +721,9 @@
             PLERROR("In NatGradSMPNNet::train - Could not initialize semaphore"
                     " value (errno = %d)", errno);
     }
+    semun_v.val = stage;
+    int success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
+    PLCHECK( success == 0 );
 
     // Fork one process/cpu.
     int iam = 0;
@@ -752,7 +757,11 @@
         //Profiler::pl_profile_end("getting_data");
         if (b+1==minibatch_size) // do also special end-case || stage+1==nstages)
         {
-            onlineStep(stage, targets, train_costs, example_weights );
+            // Read the current stage value (will be used to compute the
+            // current learning rate).
+            int cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
+            PLASSERT( cur_stage >= 0 );
+            onlineStep(cur_stage, targets, train_costs, example_weights );
             nsteps++;
             /*
             for (int i=0;i<minibatch_size;i++)
@@ -772,8 +781,13 @@
                 sem_value = (sem_value + 1) % ncpus;
                 semun_v.val = sem_value;
                 semctl(semaphore_id, 0, SETVAL, semun_v);
+                // Update the current stage.
+                cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
+                PLASSERT( cur_stage >= 0 );
+                semun_v.val = cur_stage + nsteps * minibatch_size;
+                success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
+                PLASSERT( success == 0 );
                 nsteps = 0;
-                // TODO Perform update.
             } else {
 #if 0
                 printf("CPU %d NOT updating (sem_value = %d)\n",
@@ -876,11 +890,17 @@
     }
 
     Profiler::end("Synchronization");
+    /*
     const Profiler::Stats& synch_stats = Profiler::getStats("Synchronization");
     real synch_time = (synch_stats.user_duration + synch_stats.system_duration)
         / real(Profiler::ticksPerSecond());
-    //pout << "Synch time: " << synch_time << endl;
+    pout << "Synch time: " << synch_time << endl;
+    */
 
+    // Get current stage (for debug purpose).
+    int cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
+    PLASSERT( cur_stage >= 0 );
+
     // Free semaphore's ressources.
     if (semaphore_id >= 0) {
         int success = semctl(semaphore_id, 0, IPC_RMID);
@@ -892,6 +912,9 @@
 
     // Update the learner's stage.
     stage = nstages;
+    if (stage != cur_stage)
+        PLWARNING("The target stage (%d) was not reached exactly (actual "
+                "stage: %d", stage, cur_stage);
 
     Profiler::end("training");
     Profiler::pl_profile_end("Totaltraining");
@@ -916,17 +939,13 @@
 
 }
 
-void NatGradSMPNNet::onlineStep(int tutu, const Mat& targets,
+void NatGradSMPNNet::onlineStep(int cur_stage, const Mat& targets,
                              Mat& train_costs, Vec example_weights)
 {
-    // Simply crash right now (easy!) if one tries to use a decrease constant.
-    if (!fast_exact_is_equal(lrate_decay, 0))
-        PLERROR("In NatGradSMPNNet::onlineStep - Learning rate decay not "
-                "implemented");
     // mean gradient over minibatch_size examples has less variance, can afford larger learning rate
     // TODO Note that this scaling formula is disabled to avoid confusion about
     // what learning rates are being used in experiments.
-    real lrate = /*sqrt(real(minibatch_size))* */ init_lrate/(1 + 0*lrate_decay);
+    real lrate = /*sqrt(real(minibatch_size))* */ init_lrate/(1 + cur_stage * lrate_decay);
     PLASSERT(targets.length()==minibatch_size && train_costs.length()==minibatch_size && example_weights.length()==minibatch_size);
     fpropNet(minibatch_size, true);
     fbpropLoss(neuron_outputs_per_layer[n_layers-1],targets,example_weights,train_costs);



From tihocan at mail.berlios.de  Fri Aug  3 18:07:25 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 3 Aug 2007 18:07:25 +0200
Subject: [Plearn-commits] r7919 -
	trunk/plearn_learners/online/test/ModuleTester
Message-ID: <200708031607.l73G7P8E007118@sheep.berlios.de>

Author: tihocan
Date: 2007-08-03 18:07:25 +0200 (Fri, 03 Aug 2007)
New Revision: 7919

Modified:
   trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
Log:
Added test of gradient of energy w.r.t. visible units. Also removed non-working tests due to a change in the way the contrastive divergence is computed

Modified: trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-08-03 15:55:36 UTC (rev 7918)
+++ trunk/plearn_learners/online/test/ModuleTester/PL_ModuleTester_RBM.pyplearn	2007-08-03 16:07:25 UTC (rev 7919)
@@ -53,7 +53,12 @@
          "out_nograd":[ "hidden.state", "hidden_activations.state", "negative_phase_visible_samples.state",
                         "negative_phase_hidden_expectations.state", "negative_phase_hidden_activations.state" ]}
 
+conf_energy = \
+        {"in_grad":[ "visible" ],
+         "out_grad":[ "energy" ],
+         "out_nograd":[ "hidden.state", "hidden_activations.state" ]}
 
+
 testers = [
     # Test a simple RBM that does not update itself (learning rates are set
     # to 0).
@@ -77,11 +82,12 @@
             configurations = [ conf_basic ]),
 
     # Test of gradient of contrastive divergence w.r.t. bias.
-    pl.ModuleTester(
-            module = rbm(1e-3, 0, 10, 20, 'rbm', True, True, False),
-            configurations = [ conf_bias_cd ],
-            min_out_grad = 1,
-            max_out_grad = 1),
+    # Disabled due to change in code computing the contrastive divergence.
+    #pl.ModuleTester(
+    #        module = rbm(1e-3, 0, 10, 20, 'rbm', True, True, False),
+    #        configurations = [ conf_bias_cd ],
+    #        min_out_grad = 1,
+    #        max_out_grad = 1),
 
     # Test of backpropagation gradient w.r.t. bias.
     pl.ModuleTester(
@@ -89,19 +95,26 @@
             configurations = [ conf_bias_grad ]),
 
     # Test of both backpropagation and contrastive divergence gradients w.r.t. bias.
-    pl.ModuleTester(
-            module = rbm(1e-2, 1e-3, 10, 20, 'rbm', True, True, False),
-            configurations = [ conf_bias_both ],
-            min_out_grad = 1,
-            max_out_grad = 1),
+    # Disabled due to change in code computing the contrastive divergence.
+    #pl.ModuleTester(
+    #        module = rbm(1e-2, 1e-3, 10, 20, 'rbm', True, True, False),
+    #        configurations = [ conf_bias_both ],
+    #        min_out_grad = 1,
+    #        max_out_grad = 1),
 
     # Test of gradient of contrastive divergence w.r.t. weights.
+    # Disabled due to change in code computing the contrastive divergence.
+    #pl.ModuleTester(
+    #        default_length = 5,
+    #        module = rbm(1e-3, 0, 10, 20, 'rbm', True, False),
+    #        configurations = [ conf_weights_cd ],
+    #        min_out_grad = 1,
+    #        max_out_grad = 1),
+
+    # Test of gradient of energy w.r.t. visible.
     pl.ModuleTester(
-            default_length = 5,
-            module = rbm(1e-3, 0, 10, 20, 'rbm', True, False),
-            configurations = [ conf_weights_cd ],
-            min_out_grad = 1,
-            max_out_grad = 1),
+            module = rbm(0, 0, 1, 1, 'rbm'),
+            configurations = [ conf_energy ]),
 
     ]
 



From nouiz at mail.berlios.de  Fri Aug  3 19:33:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 3 Aug 2007 19:33:14 +0200
Subject: [Plearn-commits] r7920 - trunk
Message-ID: <200708031733.l73HXEdD028054@sheep.berlios.de>

Author: nouiz
Date: 2007-08-03 19:33:13 +0200 (Fri, 03 Aug 2007)
New Revision: 7920

Modified:
   trunk/pymake.config.model
Log:
Added include dir


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-03 16:07:25 UTC (rev 7919)
+++ trunk/pymake.config.model	2007-08-03 17:33:13 UTC (rev 7920)
@@ -266,7 +266,7 @@
                   'local/lib/python2.4/site-packages/numarray')
             numpy_includedirs = []
         else:
-            numpy_includedirs = [ '/u/lisa/local/' + target_platform + '/include/' ]
+            numpy_includedirs = [ '/u/lisa/local/' + target_platform + '/include/', '/usr/include/python'+python_version ]
             ### NB: The '-lutil' is necessary on i386 LISA computers.
             numpy_site_packages = '/u/lisa/local/' + target_platform + '/lib/python%s/site-packages/numarray -lutil' % pyver
     elif domain_name.endswith('.ms'):



From tihocan at mail.berlios.de  Fri Aug  3 19:48:07 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 3 Aug 2007 19:48:07 +0200
Subject: [Plearn-commits] r7921 - in
	trunk/plearn_learners/online/test/ModuleTester: .
	.pytest/PL_ModuleTester_RBM/expected_results
Message-ID: <200708031748.l73Hm7pG029571@sheep.berlios.de>

Author: tihocan
Date: 2007-08-03 19:48:07 +0200 (Fri, 03 Aug 2007)
New Revision: 7921

Modified:
   trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log
   trunk/plearn_learners/online/test/ModuleTester/pytest.config
Log:
Re-enabled test PL_ModuleTester_RBM since it now passes again

Modified: trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log	2007-08-03 17:33:13 UTC (rev 7920)
+++ trunk/plearn_learners/online/test/ModuleTester/.pytest/PL_ModuleTester_RBM/expected_results/RUN.log	2007-08-03 17:48:07 UTC (rev 7921)
@@ -10,7 +10,3 @@
 All tests passed successfully on module RBMModule
  WARNING: RBMMatrixConnection: cannot forget() without random_gen
 All tests passed successfully on module RBMModule
- WARNING: RBMMatrixConnection: cannot forget() without random_gen
-All tests passed successfully on module RBMModule
- WARNING: RBMMatrixConnection: cannot forget() without random_gen
-All tests passed successfully on module RBMModule

Modified: trunk/plearn_learners/online/test/ModuleTester/pytest.config
===================================================================
--- trunk/plearn_learners/online/test/ModuleTester/pytest.config	2007-08-03 17:33:13 UTC (rev 7920)
+++ trunk/plearn_learners/online/test/ModuleTester/pytest.config	2007-08-03 17:48:07 UTC (rev 7921)
@@ -149,6 +149,6 @@
     resources = [ "PL_ModuleTester_RBM.pyplearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = True
+    disabled = False
     )
 



From manzagop at mail.berlios.de  Fri Aug  3 21:26:11 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Fri, 3 Aug 2007 21:26:11 +0200
Subject: [Plearn-commits] r7922 - trunk/scripts
Message-ID: <200708031926.l73JQBXb002989@sheep.berlios.de>

Author: manzagop
Date: 2007-08-03 21:26:11 +0200 (Fri, 03 Aug 2007)
New Revision: 7922

Added:
   trunk/scripts/plot_mean_stddev_curves.py
Log:
Initial version of plot_mean_stddev_curves.


Added: trunk/scripts/plot_mean_stddev_curves.py
===================================================================
--- trunk/scripts/plot_mean_stddev_curves.py	2007-08-03 17:48:07 UTC (rev 7921)
+++ trunk/scripts/plot_mean_stddev_curves.py	2007-08-03 19:26:11 UTC (rev 7922)
@@ -0,0 +1,99 @@
+#!/usr/bin/env python
+
+# plot_mean_stddev_curves.py
+# Copyright (C) 2007 Pascal Vincent, Pierre-Antoine Manzagol
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Authors: Pascal Vincent, Pierre-Antoine Manzagol
+
+import sys
+import glob
+from numpy import *
+from matplotlib.pylab import *
+from plearn.vmat.smartReadMat import smartReadMat
+
+def extract_x_y(matfilename, xfieldname, yfieldname, exclude_last_line=True):
+    m, fieldnames = smartReadMat(matfilename)
+    xpos = fieldnames.index(xfieldname)
+    ypos = fieldnames.index(yfieldname)
+    x = m[:,xpos]
+    y = m[:,ypos]
+    if exclude_last_line:
+        x = x[0:-1]
+        y = y[0:-1]
+    return x,y
+
+def plotcurves(xfieldname, yfieldname, map):
+    """Takes a map of the form {'curvename1': [matfilename1, ..., matfilenamen ],
+                                'curvename2': [ ... ] }
+       As an alternative, the map may contain in place of list of mat files, a string with wildcard characters.
+       opens each of the files containing curves (with same first coordinate field), computes the mean and stddevor,
+       and plots resulting curves with error bars on a single graph.
+       """
+    for curvename, filenames in map.iteritems():
+        if isinstance(filenames,str): # it's a wildcard specification
+            filenames = glob.glob(filenames)
+        xyvecs = [ extract_x_y(filename, xfieldname, yfieldname) for filename in filenames ]
+        xvec = xyvecs[0][0]
+        yvecs = [ y for x,y in xyvecs ]
+        ymat = array(yvecs)
+        meanvec = mean(ymat,0)
+        stdvec = std(ymat,0)
+        errorbar(xvec, meanvec, stdvec, label=curvename)
+    xlabel(xfieldname)
+    ylabel(yfieldname)
+    legend()
+
+def printHelpAndExit():
+    print """Script to plot mean curves with std error bars computed from several matrix files.
+    It takes three arguments: xfieldname yfieldname map
+    xfieldname is the name of the x field in the matrix files
+    yfieldname is the name of the y field
+    map is a map indicating the matrixfiles to serve as population for each curve and its errorbars.
+    Takes a map of the form {'curvename1': [matfilename1, ..., matfilenamen ],
+                             'curvename2': [ ... ] }
+    As an alternative, the map may contain in place of list of mat files, a string with wildcard characters.
+    You should pass the map as a single argument, for ex. surrounding it with duble quotes
+    Ex: plot_mean_stddev_curves.py "nstages" "E[train.E[class_error]]" "{'group1': '[1234].pmat', 'group2': '[5678].pmat' }"
+    Note: current default behaviour excludes the last line of each loaded matrix (since plearn typically uses it for repeating best result.
+    """
+    sys.exit()
+
+args = sys.argv[1:]
+if len(args)!=3:
+    printHelpAndExit()
+
+xfieldname = args[0]
+yfieldname = args[1]
+map = eval(args[2])
+
+plotcurves(xfieldname, yfieldname, map)
+show()
+


Property changes on: trunk/scripts/plot_mean_stddev_curves.py
___________________________________________________________________
Name: svn:executable
   + *



From nouiz at mail.berlios.de  Fri Aug  3 21:30:24 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 3 Aug 2007 21:30:24 +0200
Subject: [Plearn-commits] r7923 - trunk/plearn_learners/regressors
Message-ID: <200708031930.l73JUOx6003179@sheep.berlios.de>

Author: nouiz
Date: 2007-08-03 21:30:22 +0200 (Fri, 03 Aug 2007)
New Revision: 7923

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.h
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.h
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
Many modification for optimisation

-RegressionTreeRegisters now herit from VMatrix
-RegressionTree now need only a train_set or a sorted_train_set
-Many opt:- affect directly some option
          - some global var are now local
          - fusion of some function used together
          - add parameter to the template instead of setting them each time
          - remove unusued variable
          - ...


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2007-08-03 19:30:22 UTC (rev 7923)
@@ -58,7 +58,7 @@
     );
 
 RegressionTree::RegressionTree()     
-    : missing_is_valid(0),
+    : missing_is_valid(false),
       loss_function_weight(1.0),
       maximum_number_of_nodes(400),
       compute_train_stats(1),
@@ -134,7 +134,21 @@
 
 void RegressionTree::build_()
 {
-    if (train_set)
+    if(sorted_train_set)
+    {
+        length = sorted_train_set->length();
+        
+        if (length < 1) PLERROR("RegressionTree: the training set must contain at least one sample, got %d", length);
+        inputsize = sorted_train_set->inputsize();
+        targetsize = sorted_train_set->targetsize();
+        weightsize = sorted_train_set->weightsize();
+        if (inputsize < 1) PLERROR("RegressionTree: expected  inputsize greater than 0, got %d", inputsize);
+        if (targetsize != 1) PLERROR("RegressionTree: expected targetsize to be 1, got %d", targetsize);
+        if (weightsize != 1 && weightsize != 0)  PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d", weightsize);
+        sample_input.resize(inputsize);
+        sample_target.resize(targetsize);
+    }
+    else if (train_set)
     { 
         length = train_set->length();
         if (length < 1) PLERROR("RegressionTree: the training set must contain at least one sample, got %d", length);
@@ -143,22 +157,23 @@
         weightsize = train_set->weightsize();
         if (inputsize < 1) PLERROR("RegressionTree: expected  inputsize greater than 0, got %d", inputsize);
         if (targetsize != 1) PLERROR("RegressionTree: expected targetsize to be 1, got %d", targetsize);
-        if (weightsize != 1 && weightsize != 0)  PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d", weightsize_);
-        if (loss_function_weight != 0.0)
-        {
-            l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
-            l1_loss_function_factor = 2.0 / loss_function_weight;
-        }
-        else
-        {
-            l2_loss_function_factor = 1.0;
-            l1_loss_function_factor = 1.0;
-        }
+        if (weightsize != 1 && weightsize != 0)  PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d", weightsize);
         sample_input.resize(inputsize);
         sample_target.resize(targetsize);
-        sample_output.resize(2);
-        sample_costs.resize(4);
     }
+
+    sample_output.resize(2);
+    sample_costs.resize(4);
+    if (loss_function_weight != 0.0)
+    {
+        l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
+        l1_loss_function_factor = 2.0 / loss_function_weight;
+    }
+    else
+    {
+        l2_loss_function_factor = 1.0;
+        l1_loss_function_factor = 1.0;
+    }
 }
 
 void RegressionTree::train()
@@ -185,7 +200,7 @@
     train_stats->forget();
     for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
     {  
-        train_set->getExample(each_train_sample_index, sample_input, sample_target, sample_weight);
+        sorted_train_set->getExample(each_train_sample_index, sample_input, sample_target, sample_weight);
         computeOutput(sample_input, sample_output);
         computeCostsFromOutputs(sample_input, sample_output, sample_target, sample_costs); 
         train_stats->update(sample_costs);
@@ -218,13 +233,16 @@
     {
         sorted_train_set->reinitRegisters();
     }
+    //Set value common value of all leave
+    // for optimisation, by default they aren't missing leave
+    leave_template->setOption("missing_leave", "0");
+    leave_template->setOption("loss_function_weight", tostring(loss_function_weight));
+    leave_template->setOption("verbosity", tostring(verbosity));
+
     first_leave_output.resize(2);
     first_leave_error.resize(3);
     first_leave = ::PLearn::deepCopy(leave_template);
     first_leave->setOption("id", tostring(sorted_train_set->getNextId()));
-    first_leave->setOption("missing_leave", "0");
-    first_leave->setOption("loss_function_weight", tostring(loss_function_weight));
-    first_leave->setOption("verbosity", tostring(verbosity));
     first_leave->initLeave(sorted_train_set);
     first_leave->initStats();
     for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
@@ -266,15 +284,17 @@
         verbose("RegressionTree: expand is negative?", 3);
         return -1;
     }
-    priority_queue->addHeap(node->getNodes()[0]); 
-    priority_queue->addHeap(node->getNodes()[1]);
-    if (missing_is_valid > 0) priority_queue->addHeap(node->getNodes()[2]);
+    TVec< PP<RegressionTreeNode> > subnode = node->getNodes();
+    priority_queue->addHeap(subnode[0]); 
+    priority_queue->addHeap(subnode[1]);
+    if (missing_is_valid) priority_queue->addHeap(subnode[2]);
     return 1; 
 }
 
 void RegressionTree::setSortedTrainSet(PP<RegressionTreeRegisters> the_sorted_train_set)
 {
     sorted_train_set = the_sorted_train_set;
+    build();
 }
 
 int RegressionTree::outputsize() const

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2007-08-03 19:30:22 UTC (rev 7923)
@@ -60,7 +60,7 @@
   Build options: they have to be set before training
 */
 
-    int  missing_is_valid;
+    bool  missing_is_valid;
     real loss_function_weight;
     int maximum_number_of_nodes;
     int compute_train_stats;   

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2007-08-03 19:30:22 UTC (rev 7923)
@@ -82,8 +82,6 @@
                   "The sum of weights for the samples in this leave\n");
     declareOption(ol, "targets_sum", &RegressionTreeLeave::targets_sum, OptionBase::learntoption,
                   "The sum of targets for the samples in this leave\n");
-    declareOption(ol, "squared_targets_sum", &RegressionTreeLeave::squared_targets_sum, OptionBase::learntoption,
-                  "The sum of squared target values for the sample in this leave\n");
     declareOption(ol, "weighted_targets_sum", &RegressionTreeLeave::weighted_targets_sum, OptionBase::learntoption,
                   "The sum of weighted targets for the samples in this leave\n");
     declareOption(ol, "weighted_squared_targets_sum", &RegressionTreeLeave::weighted_squared_targets_sum, OptionBase::learntoption,
@@ -96,20 +94,9 @@
 void RegressionTreeLeave::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(id, copies);
-    deepCopyField(missing_leave, copies);
-    deepCopyField(loss_function_weight, copies);
-    deepCopyField(verbosity, copies);
     deepCopyField(train_set, copies);
-    deepCopyField(length, copies);
     deepCopyField(output, copies);
     deepCopyField(error, copies);
-    deepCopyField(weights_sum, copies);
-    deepCopyField(targets_sum, copies);
-    deepCopyField(squared_targets_sum, copies);
-    deepCopyField(weighted_targets_sum, copies);
-    deepCopyField(weighted_squared_targets_sum, copies);
-    deepCopyField(loss_function_factor, copies);
 }
 
 void RegressionTreeLeave::build()
@@ -139,7 +126,6 @@
     error[2] = 0.0;
     weights_sum= 0.0;
     targets_sum = 0.0;
-    squared_targets_sum = 0.0;
     weighted_targets_sum = 0.0;
     weighted_squared_targets_sum = 0.0; 
     if (loss_function_weight != 0.0) loss_function_factor = 2.0 / pow(loss_function_weight, 2);
@@ -148,41 +134,37 @@
 
 void RegressionTreeLeave::addRow(int row, Vec outputv, Vec errorv)
 {
-    weight = train_set->getWeight(row);
-    target = train_set->getTarget(row);
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
     length += 1;
     weights_sum += weight;
     targets_sum += target;
-    squared_target = pow(target, 2);
-    squared_targets_sum += squared_target;
+    real squared_target = pow(target, 2);
     weighted_targets_sum += weight * target;
     weighted_squared_targets_sum += weight * squared_target;  
     computeOutputAndError();
-    getOutput(outputv);
-    getError(errorv);
+    getOutputAndError(outputv,errorv);
     train_set->applyForRow(id, row);
 }
 
 void RegressionTreeLeave::removeRow(int row, Vec outputv, Vec errorv)
 {
-    weight = train_set->getWeight(row);
-    target = train_set->getTarget(row);
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
     length -= 1;
     weights_sum -= weight;
     targets_sum -= target;
-    squared_target = pow(target, 2);
-    squared_targets_sum -= squared_target;
+    real squared_target = pow(target, 2);
     weighted_targets_sum -= weight * target;
     weighted_squared_targets_sum -= weight * squared_target; 
     computeOutputAndError();
-    getOutput(outputv);
-    getError(errorv); 
+    getOutputAndError(outputv,errorv);
 }
 
 void RegressionTreeLeave::computeOutputAndError()
 {
     output[0] = weighted_targets_sum / weights_sum;
-    if (missing_leave >= 1)
+    if (missing_leave == true)
     {
         output[1] = 0.0;
         error[0] = 0.0;
@@ -195,7 +177,8 @@
         error[0] = ((weights_sum * output[0] * output[0]) - (2.0 * weighted_targets_sum * output[0]) + weighted_squared_targets_sum) * loss_function_factor;
         if (error[0] < 1E-10) error[0] = 0.0;
         error[1] = 0.0;
-        if (error[0] > weights_sum * loss_function_factor) error[2] = weights_sum * loss_function_factor; else error[2] = error[0];
+        if (error[0] > weights_sum * loss_function_factor) error[2] = weights_sum * loss_function_factor;
+        else error[2] = error[0];
     }
 }
 
@@ -214,19 +197,15 @@
     return length;
 }
 
-void RegressionTreeLeave::getError(Vec errorv)
+void RegressionTreeLeave::getOutputAndError(Vec outputv, Vec errorv)
 {
+    outputv[0] = output[0];
+    outputv[1] = output[1];
     errorv[0] = error[0];
     errorv[1] = error[1];  
     errorv[2] = error[2]; 
 }
 
-void RegressionTreeLeave::getOutput(Vec outputv)
-{
-    outputv[0] = output[0];
-    outputv[1] = output[1];
-}
-
 void RegressionTreeLeave::printStats()
 {
     cout << " l " << length;
@@ -236,7 +215,6 @@
     cout << " e1 " << error[1];
     cout << " ws " << weights_sum;
     cout << " ts " << targets_sum;
-    cout << " sts " << squared_targets_sum;
     cout << " wts " << weighted_targets_sum;
     cout << " wsts " << weighted_squared_targets_sum; 
     cout << endl;

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.h	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.h	2007-08-03 19:30:22 UTC (rev 7923)
@@ -50,7 +50,8 @@
 class RegressionTreeLeave: public Object
 {
     typedef Object inherited;
-  
+ 
+    friend class RegressionTreeNode;
 protected:
 
 /*
@@ -58,7 +59,7 @@
 */
 
     int  id;
-    int  missing_leave;
+    bool  missing_leave;
     real loss_function_weight;
     int  verbosity;
     PP<RegressionTreeRegisters> train_set;
@@ -72,20 +73,11 @@
     Vec  error;
     real weights_sum;
     real targets_sum;
-    real squared_targets_sum;
     real weighted_targets_sum;
     real weighted_squared_targets_sum;
     real loss_function_factor;
  
-/*
-  Work fields: they are sized and initialized if need be, at buid time
-*/  
- 
-    real weight;
-    real target;
-    real squared_target;
-  
-  
+
 public:
     RegressionTreeLeave();
     virtual              ~RegressionTreeLeave();
@@ -102,8 +94,7 @@
     void         registerRow(int row);
     int          getId();
     int          getLength();
-    void         getError(Vec errorv);
-    void         getOutput(Vec outputv);
+    void         getOutputAndError(Vec outputv, Vec errorv);
     virtual void         printStats();
   
 private:

Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2007-08-03 19:30:22 UTC (rev 7923)
@@ -125,8 +125,8 @@
 
 void RegressionTreeMulticlassLeave::addRow(int row, Vec outputv, Vec errorv)
 {
-    weight = train_set->getWeight(row);
-    target = train_set->getTarget(row);
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
     length += 1;
     weights_sum += weight;
     int multiclass_found = 0;
@@ -142,15 +142,14 @@
     if (multiclass_found < 1) 
         PLERROR("RegressionTreeMultilassLeave: Unknown target: %d row: %d\n", target,row);
     computeOutputAndError();
-    getOutput(outputv);
-    getError(errorv);
+    getOutputAndError(outputv,errorv);
     train_set->applyForRow(id, row);
 }
 
 void RegressionTreeMulticlassLeave::removeRow(int row, Vec outputv, Vec errorv)
 {
-    weight = train_set->getWeight(row);
-    target = train_set->getTarget(row);
+    real weight = train_set->getWeight(row);
+    real target = train_set->getTarget(row);
     length -= 1;
     weights_sum -= weight;
     for (int multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
@@ -162,8 +161,7 @@
         }
     }
     computeOutputAndError();
-    getOutput(outputv);
-    getError(errorv); 
+    getOutputAndError(outputv,errorv);
 }
 
 void RegressionTreeMulticlassLeave::computeOutputAndError()

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2007-08-03 19:30:22 UTC (rev 7923)
@@ -184,25 +184,20 @@
     right_leave_id =  train_set->getNextId();
     length = train_set->length();
     inputsize = train_set->inputsize();
+
     missing_leave = ::PLearn::deepCopy(leave_template);
-    missing_leave->setOption("id", tostring(missing_leave_id));
-    if (missing_is_valid > 0) missing_leave->setOption("missing_leave", "0");
-    else missing_leave->setOption("missing_leave", "1");
-    missing_leave->setOption("loss_function_weight", tostring(loss_function_weight));
-    missing_leave->setOption("verbosity", tostring(verbosity));
+    missing_leave->id=missing_leave_id;
+    missing_leave->missing_leave=missing_is_valid;
     missing_leave->initLeave(train_set);
+
     left_leave = ::PLearn::deepCopy(leave_template);
-    left_leave->setOption("id", tostring(left_leave_id));
-    left_leave->setOption("missing_leave", "0");
-    left_leave->setOption("loss_function_weight", tostring(loss_function_weight));
-    left_leave->setOption("verbosity", tostring(verbosity));
+    left_leave->id=left_leave_id;
     left_leave->initLeave(train_set);
+
     right_leave = ::PLearn::deepCopy(leave_template);
-    right_leave->setOption("id", tostring(right_leave_id));
-    right_leave->setOption("missing_leave", "0");
-    right_leave->setOption("loss_function_weight", tostring(loss_function_weight));
-    right_leave->setOption("verbosity", tostring(verbosity));
+    right_leave->id=right_leave_id;
     right_leave->initLeave(train_set);
+
     leave_output.resize(2);
     leave_error.resize(3);
     missing_output.resize(2);
@@ -211,30 +206,31 @@
     left_error.resize(3);
     right_output.resize(2);
     right_error.resize(3);
-    leave->getOutput(leave_output);
-    leave->getError(leave_error);
+
+    leave->getOutputAndError(leave_output,leave_error);
 }
 
 void RegressionTreeNode::lookForBestSplit()
 {
-    for (col = 0; col < inputsize; col++)
+    for (int col = 0; col < inputsize; col++)
     {
         missing_leave->initStats();
         left_leave->initStats();
         right_leave->initStats();
-        for (row = train_set->getNextRegisteredRow(leave_id, col, -1); row < length; row = train_set->getNextRegisteredRow(leave_id, col, row))
+        for (int row = train_set->getNextRegisteredRow(leave_id, col, -1); row < length; row = train_set->getNextRegisteredRow(leave_id, col, row))
         {
-            if (is_missing(train_set->getFeature(row, col))) missing_leave->addRow(row, missing_output, missing_error);
+            if (is_missing(train_set->get(row, col))) missing_leave->addRow(row, missing_output, missing_error);
             else right_leave->addRow(row, right_output, right_error);
         }
-        row = train_set->getNextCandidateRow(right_leave_id, col, -1);
-        next_row = train_set->getNextCandidateRow(right_leave_id, col, row);
+        int row = train_set->getNextCandidateRow(right_leave_id, col, -1);
+        int next_row = train_set->getNextCandidateRow(right_leave_id, col, row);
+
         while (true)
         {
             if (next_row >= length) break;
             right_leave->removeRow(row, right_output, right_error);
             left_leave->addRow(row, left_output, left_error);
-            compareSplit(col, train_set->getFeature(row, col), train_set->getFeature(next_row, col));
+            compareSplit(col, train_set->get(row, col), train_set->get(next_row, col));
             row = next_row;
             next_row = train_set->getNextCandidateRow(right_leave_id, col, row);
         }
@@ -244,18 +240,11 @@
 void RegressionTreeNode::compareSplit(int col, real left_leave_last_feature, real right_leave_first_feature)
 {
     if (left_leave_last_feature >= right_leave_first_feature) return;
-    work_error = missing_error[0] + missing_error[1] + left_error[0] + left_error[1] + right_error[0] + right_error[1];
-    work_balance = abs(left_leave->getLength() - right_leave->getLength());
-    if (split_col < 0)
-    {
-        split_col = col;
-        split_feature_value = 0.5 * (right_leave_first_feature + left_leave_last_feature);
-        after_split_error = work_error;
-        split_balance = work_balance;
-        return;
-    }
-    if (work_error > after_split_error) return;
-    if (work_error == after_split_error && work_balance > split_balance) return;
+    real work_error = missing_error[0] + missing_error[1] + left_error[0] + left_error[1] + right_error[0] + right_error[1];
+    int work_balance = abs(left_leave->getLength() - right_leave->getLength());
+    if(split_col<0);
+    else if (work_error > after_split_error) return;
+    else if (work_error == after_split_error && work_balance > split_balance) return;
     split_col = col;
     split_feature_value = 0.5 * (right_leave_first_feature + left_leave_last_feature);
     after_split_error = work_error;
@@ -264,8 +253,7 @@
 
 int RegressionTreeNode::expandNode()
 {
-    col = split_col;
-    if (col < 0)
+    if (split_col < 0)
     {
         verbose("RegressionTreeNode: there is no more split candidate", 3);
         return -1;
@@ -273,16 +261,16 @@
     missing_leave->initStats();
     left_leave->initStats();
     right_leave->initStats();
-    for (row = train_set->getNextRegisteredRow(leave_id, col, -1); row < length; row = train_set->getNextRegisteredRow(leave_id, col, row))
+    for (int row = train_set->getNextRegisteredRow(leave_id, split_col, -1); row < length; row = train_set->getNextRegisteredRow(leave_id, split_col, row))
     {
-        if (is_missing(train_set->getFeature(row, col)))
+        if (is_missing(train_set->get(row, split_col)))
         {
             missing_leave->addRow(row, missing_output, missing_error);
             missing_leave->registerRow(row);
         }
         else
         {
-            if (train_set->getFeature(row, col) < split_feature_value)
+            if (train_set->get(row, split_col) < split_feature_value)
             {
                 left_leave->addRow(row, left_output, left_error);
                 left_leave->registerRow(row);    
@@ -300,22 +288,22 @@
     if (missing_is_valid > 0)
     {
         missing_node = new RegressionTreeNode();
-        missing_node->setOption("missing_is_valid", tostring(missing_is_valid));
-        missing_node->setOption("loss_function_weight", tostring(loss_function_weight));
-        missing_node->setOption("verbosity", tostring(verbosity));
+        missing_node->missing_is_valid=missing_is_valid;
+        missing_node->loss_function_weight=loss_function_weight;
+        missing_node->verbosity=verbosity;
         missing_node->initNode(train_set, missing_leave, leave_template);
         missing_node->lookForBestSplit();
     }
     left_node = new RegressionTreeNode();
-    left_node->setOption("missing_is_valid", tostring(missing_is_valid));
-    left_node->setOption("loss_function_weight", tostring(loss_function_weight));
-    left_node->setOption("verbosity", tostring(verbosity));
+    left_node->missing_is_valid=missing_is_valid;
+    left_node->loss_function_weight=loss_function_weight;
+    left_node->verbosity=verbosity;
     left_node->initNode(train_set, left_leave, leave_template);
     left_node->lookForBestSplit();
     right_node = new RegressionTreeNode();
-    right_node->setOption("missing_is_valid", tostring(missing_is_valid));
-    right_node->setOption("loss_function_weight", tostring(loss_function_weight));
-    right_node->setOption("verbosity", tostring(verbosity));
+    right_node->missing_is_valid=missing_is_valid;
+    right_node->loss_function_weight=loss_function_weight;
+    right_node->verbosity=verbosity;
     right_node->initNode(train_set, right_leave, leave_template);
     right_node->lookForBestSplit();
     return +1;

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2007-08-03 19:30:22 UTC (rev 7923)
@@ -94,16 +94,6 @@
     Vec right_output;
     Vec right_error;
  
-/*
-  Work fields: they are sized and initialized if need be, at buid time
-*/  
-
-    int row;
-    int next_row;
-    int col;  
-    int work_balance;
-    real work_error;    
-
 public:  
     RegressionTreeNode();
     virtual              ~RegressionTreeNode();

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-08-03 19:30:22 UTC (rev 7923)
@@ -68,9 +68,9 @@
                   "The indicator to report progress through a progress bar\n");
     declareOption(ol, "verbosity", &RegressionTreeRegisters::verbosity, OptionBase::buildoption,
                   "The desired level of verbosity\n");
-    declareOption(ol, "train_set", &RegressionTreeRegisters::train_set, OptionBase::buildoption,
-                  "The original train set VMatrix to be sorted in each dimensions\n");
-      
+    declareOption(ol, "source", &RegressionTreeRegisters::source, OptionBase::buildoption,
+                  "The source VMatrix");
+
     declareOption(ol, "next_id", &RegressionTreeRegisters::next_id, OptionBase::learntoption,
                   "The next id for creating a new leave\n");
     declareOption(ol, "length", &RegressionTreeRegisters::length_, OptionBase::learntoption,
@@ -97,7 +97,6 @@
 void RegressionTreeRegisters::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(train_set, copies);
     deepCopyField(sorted_row, copies);
     deepCopyField(inverted_sorted_row, copies);
     deepCopyField(leave_register, copies);
@@ -116,12 +115,12 @@
 
 void RegressionTreeRegisters::initRegisters(VMat the_train_set)
 {
-    train_set = the_train_set;
-    length_ = train_set->length();
-    width_ = train_set->width();
-    inputsize_ = train_set->inputsize();
-    targetsize_ = train_set->targetsize();
-    weightsize_ = train_set->weightsize();
+    source = the_train_set;
+    length_ = source->length();
+    width_ = source->width();
+    inputsize_ = source->inputsize();
+    targetsize_ = source->targetsize();
+    weightsize_ = source->weightsize();
     leave_register.resize(length());
     leave_candidate.resize(length());
     sortRows();
@@ -142,26 +141,26 @@
     leave_register[row] = leave_id;
 }
 
-real RegressionTreeRegisters::getFeature(int row, int col)
+real RegressionTreeRegisters::get(int i, int j) const
 {
-    return train_set->get(row, col);
+    return source->get(i,j);
 }
 
 real RegressionTreeRegisters::getTarget(int row)
 {
-    return train_set->get(row, inputsize());
+    return source->get(row, inputsize());
 }
 
 real RegressionTreeRegisters::getWeight(int row)
 {
     if (weightsize() <= 0) return 1.0 / length();
-    else return train_set->get(row, inputsize() + targetsize() );
+    else return source->get(row, inputsize() + targetsize() );
 }
 
 void RegressionTreeRegisters::setWeight(int row, real val)
 {
     PLASSERT(weightsize() > 0);
-    train_set->put(row, inputsize() + targetsize(), val );
+    source->put(row, inputsize() + targetsize(), val );
 }
 
 int RegressionTreeRegisters::getNextId()
@@ -217,10 +216,7 @@
     }
     for (int each_train_sample_index = 0; each_train_sample_index < length(); each_train_sample_index++)
     {
-        for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
-        {
-            sorted_row(each_train_sample_index, sample_dim) = each_train_sample_index;
-        }
+        sorted_row(each_train_sample_index).fill(each_train_sample_index);
     }
     for (int sample_dim = 0; sample_dim < inputsize(); sample_dim++)
     {
@@ -263,22 +259,22 @@
         else
         {
             swapIndex(start_index + 1, (start_index + end_index) / 2, dim);
-            if (compare(train_set->get(sorted_row(start_index, dim), dim),
-                        train_set->get(sorted_row(end_index, dim), dim)) > 0.0)
+            if (compare(source->get(sorted_row(start_index, dim), dim),
+                        source->get(sorted_row(end_index, dim), dim)) > 0.0)
                 swapIndex(start_index, end_index, dim);
-            if (compare(train_set->get(sorted_row(start_index + 1, dim), dim),
-                        train_set->get(sorted_row(end_index, dim), dim)) > 0.0)
+            if (compare(source->get(sorted_row(start_index + 1, dim), dim),
+                        source->get(sorted_row(end_index, dim), dim)) > 0.0)
                 swapIndex(start_index + 1, end_index, dim);
-            if (compare(train_set->get(sorted_row(start_index, dim), dim),
-                        train_set->get(sorted_row(start_index + 1, dim), dim)) > 0.0)
+            if (compare(source->get(sorted_row(start_index, dim), dim),
+                        source->get(sorted_row(start_index + 1, dim), dim)) > 0.0)
                 swapIndex(start_index, start_index + 1, dim);
             forward_index = start_index + 1;
             backward_index = end_index;
-            real sample_feature = train_set->get(sorted_row(start_index + 1, dim), dim);
+            real sample_feature = source->get(sorted_row(start_index + 1, dim), dim);
             for (;;)
             {
-                do forward_index++; while (compare(train_set->get(sorted_row(forward_index, dim), dim), sample_feature) < 0.0);
-                do backward_index--; while (compare(train_set->get(sorted_row(backward_index, dim), dim), sample_feature) > 0.0);
+                do forward_index++; while (compare(source->get(sorted_row(forward_index, dim), dim), sample_feature) < 0.0);
+                do backward_index--; while (compare(source->get(sorted_row(backward_index, dim), dim), sample_feature) > 0.0);
                 if (backward_index < forward_index)
                 {
                     break;
@@ -312,13 +308,13 @@
          next_train_sample_index++)
     {
         int saved_index = sorted_row(next_train_sample_index, dim);
-        real sample_feature = train_set->get(saved_index, dim);
+        real sample_feature = source->get(saved_index, dim);
         int each_train_sample_index;
         for (each_train_sample_index = next_train_sample_index - 1;
              each_train_sample_index >= the_start_index;
              each_train_sample_index--)
         {
-            if (compare(train_set->get(sorted_row(each_train_sample_index, dim), dim), sample_feature) <= 0.0)
+            if (compare(source->get(sorted_row(each_train_sample_index, dim), dim), sample_feature) <= 0.0)
             {
                 break;
             }
@@ -363,12 +359,12 @@
     if(inputsize_<0)
         PLERROR("In VMatrix::getExample, inputsize_ not defined for this vmat");
     input.resize(inputsize_);
-    train_set->getSubRow(i,0,input);
+    source->getSubRow(i,0,input);
     if(targetsize_<0)
         PLERROR("In VMatrix::getExample, targetsize_ not defined for this vmat");
     target.resize(targetsize_);
     if (targetsize_ > 0) {
-        train_set->getSubRow(i,inputsize_,target);
+        source->getSubRow(i,inputsize_,target);
     }
 
     if(weightsize()==0)
@@ -378,7 +374,7 @@
     else if(weightsize()>1)
         PLERROR("In VMatrix::getExample, weightsize_ >1 not supported by this call");
     else
-        weight = train_set->get(i,inputsize()+targetsize());
+        weight = source->get(i,inputsize()+targetsize());
 }
 
 

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2007-08-03 19:26:11 UTC (rev 7922)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2007-08-03 19:30:22 UTC (rev 7923)
@@ -42,7 +42,7 @@
 #ifndef RegressionTreeRegisters_INC
 #define RegressionTreeRegisters_INC
 
-#include <plearn/base/Object.h>
+#include <plearn/vmat/SourceVMatrix.h>
 #include <plearn/base/stringutils.h>
 #include <plearn/math/TMat.h>
 #include <plearn/vmat/VMat.h>
@@ -50,9 +50,9 @@
 namespace PLearn {
 using namespace std;
 
-class RegressionTreeRegisters: public Object
+class RegressionTreeRegisters: public VMatrix
 {
-    typedef Object inherited;
+    typedef VMatrix inherited;
   
 private:
 
@@ -62,24 +62,21 @@
 
     int  report_progress;
     int  verbosity;
-    VMat train_set;
+//    VMat train_set;
   
 /*
   Learnt options: they are sized and initialized if need be, at initRegisters(...) or reinitRegisters()
 */
 
     int       next_id;
-    int       length_;
-    int       width_;
-    int       inputsize_;
-    int       targetsize_;
-    int       weightsize_;  
     TMat<int> sorted_row;
     TMat<int> inverted_sorted_row;
     TVec<int> leave_register;
     TVec<int> leave_candidate;
  
 public:
+
+    VMat source;
     RegressionTreeRegisters();
     virtual              ~RegressionTreeRegisters();
     
@@ -92,20 +89,17 @@
     void         reinitRegisters();
     void         applyForRow(int leave_id, int row);
     void         registerLeave(int leave_id, int row);
-    real         getFeature(int row, int col);
+    virtual real get(int row, int col) const;
     real         getTarget(int row);
     real         getWeight(int row);
     void         setWeight(int row,real val);
-    int          length(){return length_;}
     int          getNextId();
-    int          inputsize(){return inputsize_;}
-    int          targetsize(){return targetsize_;}
-    int          weightsize(){return weightsize_;}
     int          getNextRegisteredRow(int leave_id, int col, int previous_row);
     int          getNextCandidateRow(int leave_id, int col, int previous_row);
     void         sortRows();
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);
+//    virtual void getNewRow(int i, const Vec& v) const;
 
 private:
     void         build_();



From simonl at mail.berlios.de  Fri Aug  3 23:49:02 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Fri, 3 Aug 2007 23:49:02 +0200
Subject: [Plearn-commits] r7924 - in trunk: python_modules/plearn/plotting
	python_modules/plearn/var scripts/EXPERIMENTAL
Message-ID: <200708032149.l73Ln2Cs010741@sheep.berlios.de>

Author: simonl
Date: 2007-08-03 23:49:00 +0200 (Fri, 03 Aug 2007)
New Revision: 7924

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/var/Var.py
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
some functions added in Var.py
some help in deepnetplot.py


Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-08-03 19:30:22 UTC (rev 7923)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-08-03 21:49:00 UTC (rev 7924)
@@ -6,7 +6,7 @@
 ### constants ###
 #################
 
-defaultColorMap = cm.jet
+defaultColorMap = cm.gray#cm.jet
 defaultInterpolation = 'nearest'
 
 ########################
@@ -25,12 +25,14 @@
     M = matrix
     mi = M[0][0]
     ma = M[0][0]
-    for row in M:
-        for el in row:
-            if el > ma:
-                ma = el
-            if el < mi:
-                mi = el
+
+    for i in arange(M.shape[0]):
+        for j in arange(M.shape[1]):
+            if M[i,j] > ma:
+                ma = M[i,j]
+            if M[i,j] < mi:
+                mi = M[i,j]
+    
     return mi,ma
 
 def customColorBar(min, max, (x,y,width,height) = (0.9,0.1,0.1,0.8), nb_ticks = 50., color_map = defaultColorMap):
@@ -91,7 +93,7 @@
             imshow(rowToMatrix(toPlusRow(row),width), interpolation="nearest", cmap = colorMap)
             setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
 
-def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows = None):
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows = None, index_to_plot = [], names = []):
 
     
     #some calculations for plotting
@@ -121,9 +123,14 @@
     
     x,y = sbi,sbi
     toReturn = -1
-    
-    for i in arange(start,length):
-    
+
+    if index_to_plot == []:
+        index_to_plot = arange(start,start+length)
+    print index_to_plot
+
+    j=0
+    for i in index_to_plot:
+           
         #normal
         row = M[i]
         if apply_to_rows != None:
@@ -134,8 +141,11 @@
         
         axes((x, y, plotWidth, plotHeight))
         imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
-        setPlotParams('row_' + str(i), False, True)
-
+        if names == []:
+            setPlotParams('row_' + str(i), False, True)
+        else:
+            setPlotParams(names[j],False,True)
+        j+=1
         
 
         x = x + plotWidth + sbi
@@ -154,7 +164,7 @@
 
 
 
-def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5):
+def plotMatrices(matrices, names = None, ticks = False, same_color_bar = False, space_between_matrices = 5, horizontal=True):
     '''plot matrices from left to right
     TODO : same_color_bar does nothing !!
     '''

Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-08-03 19:30:22 UTC (rev 7923)
+++ trunk/python_modules/plearn/var/Var.py	2007-08-03 21:49:00 UTC (rev 7924)
@@ -37,17 +37,34 @@
     
 class Var:
 
-    def __init__(self, l, w=1, random_type="none", random_a=0., random_b=1., random_clear_first_row=False, varname=""):
+    def __init__(self, l, w=1, random_type="none", random_a=0., random_b=1., random_clear_first_row=False, varname="", min_value = None, max_value = None):
         if isinstance(l, Var):
             self.v = l.v
         elif isinstance(l,int):
-            self.v = pl.SourceVariable(build_length=l,
-                                       build_width=w,
-                                       random_type=random_type,
-                                       random_a=random_a,
-                                       random_b=random_b,
-                                       random_clear_first_row=random_clear_first_row,
-                                       varname=varname)
+            if min_value == None and max_value == None:
+                self.v = pl.SourceVariable(build_length=l,
+                                           build_width=w,
+                                           random_type=random_type,
+                                           random_a=random_a,
+                                           random_b=random_b,
+                                           random_clear_first_row=random_clear_first_row,
+                                           varname=varname)
+            elif min_value == None:
+                pass
+            elif max_value == None:
+                pass
+            else:
+                self.v = pl.SourceVariable(build_length=l,
+                                           build_width=w,
+                                           random_type=random_type,
+                                           random_a=random_a,
+                                           random_b=random_b,
+                                           random_clear_first_row=random_clear_first_row,
+                                           varname=varname,
+                                           min_value = min_value,
+                                           max_value = max_value)
+                
+                
         else: # assume parameter l is a pl. plvar
             self.v = l
 
@@ -127,6 +144,9 @@
     def multiLogSoftMax(self, igs):
         return self.multiMax(igs, 'L')
 
+    def multiSample(self, gs):
+        return Var(pl.MultiSampleVariable(input=self.v, groupsize=gs))
+
     def transposeDoubleProduct(self, W, M):
         return Var(pl.TransposedDoubleProductVariable(varray=[self.v, W, M]))
 
@@ -180,16 +200,23 @@
     cost = reconstr_activation.negCrossEntropySigmoid(input)
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename=""):
+def addMultiSoftMaxDoubleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename="", positive=False):
     """iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output"""
+
     ra = 1./max(iw,ow)
     sqra = sqrt(ra)
-    M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
+
+    if positive:
+        W = Var(ogs, iw, "uniform", 0, sqra, False, varname=basename+"_W", min_value=0, max_value=1e100)
+        M = Var(ow/ogs, iw, "uniform", 0, sqra, False, varname=basename+"_M", min_value=0, max_value=1e100)
+    else:
+        W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
+        M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
+       
     if constrain_mask:
         M = M.sigmoid()
-    W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
         hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
@@ -242,32 +269,51 @@
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
-    ra = 1./max(iw,ow)
-    W = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_W')
+def addMultiSoftMaxSimpleProductTiedRLayer(input, iw, igs, ow, ogs, add_bias=False, basename="", positive=False, stochastic_sample=False):
+    
+    sup = 1./max(iw,ow)
+    if positive:
+        W = Var(ow, iw, "uniform", 0, sup, varname=basename+'_W', min_value=0, max_value=1e100)
+    else:
+        W = Var(ow, iw, "uniform", -sup, sup, varname=basename+'_W')
+
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         br = Var(1,iw,"fill",0, varname=basename+'_br')
         log_reconstructed = hidden.matrixProduct(W).add(br).multiLogSoftMax(igs)
     else:        
         hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         log_reconstructed = hidden.matrixProduct(W).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename=""):
+def addMultiSoftMaxSimpleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, basename="", positive=False, stochastic_sample=False):
     ra = 1./max(iw,ow)
-    W = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_W')
-    Wr = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_Wr')
+
+    if positive:
+        W = Var(ow, iw, "uniform", 0, ra, varname=basename+'_W',min_value=0, max_value=1e100)
+        Wr = Var(ow, iw, "uniform", 0, ra, varname=basename+'_Wr', min_value=0, max_value=1e100)
+    else :            
+        W = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_W')
+        Wr = Var(ow, iw, "uniform", -ra, ra, varname=basename+'_Wr')
+        
     if add_bias:
         b = Var(1,ow,"fill",0, varname=basename+'_b')
         hidden = input.matrixProduct_A_Bt(W).add(b).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         br = Var(1,iw,"fill",0, varname=basename+'_br')
         log_reconstructed = hidden.matrixProduct(Wr).add(br).multiLogSoftMax(igs)
     else:
         hidden = input.matrixProduct_A_Bt(W).multiSoftMax(ogs)
+        if stochastic_sample:
+            hidden = hidden.multiSample(ogs)
         log_reconstructed = hidden.matrixProduct(Wr).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-03 19:30:22 UTC (rev 7923)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-03 21:49:00 UTC (rev 7924)
@@ -6,7 +6,8 @@
 from plearn.pyplearn import *
 from plearn.plotting.netplot import *
 from numarray import *
-from numarray.random_array import *
+# from numpy.numarray import *
+#from numarray.random_array import *
 import numpy.random
 
 
@@ -15,13 +16,49 @@
 ################
 
 def print_usage_and_exit():
-    print "Usage: drnPlot <task> <file> [<other arguments>]",    
-    "drnPlot plotSingleMatrix x.psave ",
-    "drnPlot plotEachRow learner.psave chars.pmat"
-    "drnPlot plotRepAndRec learner.psave chars.pmat"
+    print "Usage :"  
+    print "deepnetplot.py plotSingleMatrix x.psave "
+    print "deepnetplot.py plotEachRow learner.psave chars.pmat"
+    print "deepnetplot.py plotRepAndRec learner.psave chars.pmat"
+    print "deepnetplot.py help"
+    print ""
+    print "where learner.psave is a .psave of a DeepReconstructorNet object"
+    print "and chars.pmat are the training examples"
+    print ""
+    print "exemple:"
+    print ""
+    print "deepnetplot.py plotRepAndRec multimax_tied_small_gs1\=4_ng1\=225/Strat0/Trials0/Split0/final_learner.psave /cluster/pauli/data/mnist/mnist_small/mnist_basic2_train.pmat"
     sys.exit()
 
+def print_usage_repAndRec():
+    print 'putting you mouse OVER a hidden layer an hitting these keyes does... things :'
+    print
+    print 'f : fproping from the last layer'
+    print 'F : fproping form the last layer and for the next ones'
+    print 'r : reconstructing this layer from the next one'
+    print 'R : reconstructing this layer from the next one and also the previous ones'
+    print 'm : set the max of each group of the current hidden layer to 1 and the other elements of the group to 0'
+    print 's : each group of the current layer is sampled (each group has to sum to 1.0)'
+    print 'S : samples the current layer, then reconstruct the previous layer, thant sample this reconstructed layer, and continues until the input'
+    print 'z : set the current pixel (the one the mouse is over) to 0.0'
+    print 'x : set the current pixel to 0.25'
+    print 'c : set the current pixel to 0.5'
+    print 'v : set the current pixel to 0.75'
+    print 'b : set the current pixel to 1.0'
+    print ' space-bar : print value and position of the current pixel'
+    print 'i : print values of the current layer'
+    print 'w : plot the weight matrices associated with the current pixel'
+    print 'W : same as w but for all a group'
+    print 'C : same as w but for the hidden unit that has the highest value in each group'
+    print 'o : set the current hidden layer to its original state'
+    print 'O : same as o but for every layer'
+    print 'right arrow : prints the next character in the dataset and its corresponding hidden layers and reconstructions'
+    print 'left arrow :same but for previous character'
+    print '0,1,2,3...9 : after having pressed a digit, right and left arrows will only find this digit'
+    print '. : cancel the effec of pressing a digit'
+    print 'Control + clic : plots the clicked hidden layer in a new figure'
 
+
 def appendMatrixToFile(file, matrix, matrix_name=""):
     file.write("\n\n" + matrix_name + ' ('+ str(len(matrix)) + 'x' + str(len(matrix[0])) + ')\n\n')
     for i, row in enumerate(matrix):
@@ -32,11 +69,13 @@
         
 
 class HiddenLayer:
+    '''represents a hidden layer which elements are divised in groups
+       and we have some methods to be able to plot it'''
     
     def __init__(self,hidden_Layer, groupsize):
         self.hidden_layer = hidden_Layer
         self.groupsize = groupsize
-
+        
         self.max_height = 150
 
         #if it's too tall, we do this little tweak
@@ -56,7 +95,7 @@
     def matrixToLayer(self, x, y):
         gs = self.groupsize
         x,y = self.correctXY(x,y)
-        return x + gs*self.nbgroups*y
+        return x + gs*self.nbgroups*y        
 
     def correctXY(self,x,y):
         return int(x + .4), int(y + .3)        
@@ -77,10 +116,12 @@
         for i in arange(n*self.groupsize,(n+1)*self.groupsize):
            self.hidden_layer[i] = row[c]
            c+=1
+    def getNGroups(self):
+        return self.hidden_layer.size()/self.groupsize
 
 
-
 class InteractiveRepRecPlotter:
+    '''this class is used to plot representations and reconstructions of a DeepReconstructorNet'''
     
     def __init__(self, learner, vmat, image_width=28, char_indice=0):
         '''constructor'''
@@ -105,7 +146,6 @@
 
         self.plotNext()#starting with a plot...
         self.__linkEvents()
-               
 
 
     def size(self):
@@ -122,7 +162,7 @@
         self.input = row[:-1]
 
     def __getNextChar(self):
-        '''get next input row from the vmat'''        
+        '''get next input row from the vmat'''
         while True:
             self.current+=1
             raw_input = vmat.getRow(self.current)
@@ -132,7 +172,7 @@
         self.__rowToClassInput(raw_input)
 
     def __getPrevChar(self):
-        '''get next input row from the vmat'''        
+        '''get next input row from the vmat'''
         while True:
             self.current-=1
             raw_input = vmat.getRow(self.current)
@@ -209,7 +249,7 @@
 
 
     def __repCommands(self,event):
-
+        
         #met a jour self.current_hl
         self.__findCurrentLayer(event)
         
@@ -258,35 +298,27 @@
             # reconstruction -- r
                 
             elif char == 'r':
-                print 'reconstructing...'
-                self.learner.setMatValue(i+1, reshape(hl2.hidden_layer, (1,-1)))
-                #reconstruct
-                row = self.learner.reconstructOneLayer(i+1)[0]
-                #HACK
-                if len(row) == 28*28*2:
-                    print 'hacking...'
-                    row = array(toMinusRow(row))
-                #print the new layer
-                hl1.hidden_layer = row
-                self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 
-                draw()
-                print '...done'
+                if i<self.size()-2:
+                    print 'reconstructing...'
+                    self.__reconstructLayer(i)
+                    self.rep_axes[i].imshow(hl1.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)                 
+                    draw()
+                    print '...done'
+                else:
+                    print 'invalid layer for reconstruction'
 
             # big-reconstruction -- R
 
             elif char == 'R':
-                print 'big-reconstructing...'
-                for k in arange(i+1, 0, -1):
-                    self.learner.setMatValue(k, reshape(self.hidden_layers[k].hidden_layer, (1,-1)))
-                    row = self.learner.reconstructOneLayer(k)[0]
-                    #HACK
-                    if len(row) == 28*28*2:
-                        print 'hacking...'
-                        row = array(toMinusRow(row))
-                    self.hidden_layers[k-1].hidden_layer = row
-                    self.rep_axes[k-1].imshow(self.hidden_layers[k-1].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                if i<self.size()-2:
+                    print 'big-reconstructing...'
+                    for k in arange(i, -1, -1):
+                        self.__reconstructLayer(k)
+                        self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                     draw()
-                print '...done'                
+                    print '...done'
+                else:
+                    print 'invalid layer for reconstruction'
 
             # max -- m
 
@@ -321,13 +353,22 @@
 
             elif char == 's':
                 print 'sampling...'
-                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
-                    print 'sum', hl.getRow(n).sum()
-                    multi = numpy.random.multinomial(1,hl.getRow(n))                    
-                    hl.setRow(n,multi)                            
+                self.__sampleLayer(i)                
                 self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                 draw()
                 print '...done'
+
+            # sampling + reconstructin -- S
+
+            elif char == 'S':
+                print 'sampling and reconstruction...'
+                for n in arange(i,0,-1):
+                    self.__sampleLayer(n)
+                    self.__reconstructLayer(n-1)
+                for n in arange(i+1):
+                    self.rep_axes[n].imshow(self.hidden_layers[n].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+                print '...done'
                 
             # set pixel -- z,x,c,v,b
 
@@ -355,8 +396,13 @@
                 x,y = event.xdata, event.ydata
                 print
                 print 'Position', hl.matrixToLayer(x,y), '(', x, y, ')'
-                print 'Value', hl.getElement(x,y)                
+                print 'Value', hl.getElement(x,y)
 
+            # more infos -- 'i'
+
+            elif char == 'i':
+                print hl.getMatrix()                
+
             # plot W and M -- w
 
             elif char == 'w':
@@ -375,7 +421,8 @@
                 namesToPlot = []
 
                 x,y = hl.correctXY(event.xdata,event.ydata)
-                n = hl.matrixToLayer(x, y)
+                #n = hl.matrixToLayer(x, y)
+                n = hl.matrixToLayer(event.xdata, event.ydata)               
                 
                 
                 if nameW in listNames and nameM not in listNames:
@@ -405,14 +452,15 @@
                         namesToPlot.append(nameWr)
 
                     figure(3)
+                    
                     clf()
                     plotMatrices(matricesToPlot, namesToPlot)
                     draw()
 
                 if nameW in listNames and nameM in listNames:
 
-                    rowW = learner.getParameterRow(nameW,x)
-                    rowM = learner.getParameterRow(nameM,y)
+                    rowW = learner.getParameterRow(nameW,n%hl.groupsize)
+                    rowM = learner.getParameterRow(nameM,int(n/hl.groupsize))
                     
                     #HACK !!!
                     #print 'just hacked...'
@@ -420,16 +468,21 @@
                     #    row = array(toMinusRow(row))
                     #END OF HACK
                                       
-                    rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
-                    rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
+                    matW1 = reshape(toMinusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
+                    matM1 = reshape(toMinusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
+                    matW2 = reshape(toPlusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
+                    matM2 = reshape(toPlusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
 
                     #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+                    produit = matW1*matM1
+                    produit2 = matW2*matM2
 
-                    produit = rowW*rowM                   
-
+                    
                     figure(3)
+                    ioff()
                     clf()                  
-                    plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
+                    plotMatrices([matW1,matM1,matW2,matM2,produit,produit2], [nameW + "-",nameM+"-", nameW + "+",nameM+"+",'term-to-term product (-)', 'term-to-term product (+)'])
+                    ion()
                     draw()
 
                 #BIAS
@@ -464,8 +517,168 @@
                     clf()
                     plotMatrices(matricesToPlot, namesToPlot)
                     draw()
+            
+            #like 'w' but for an entire row -- 'W'
+            elif char == 'W':
+
+                prefixe = 'Layer' + str(i)
+                nameW = prefixe + '_W'
+                nameWr = nameW + 'r'
+                nameM = prefixe + '_M'
+                nameMr = nameM + 'r'
+                nameB = prefixe + '_b'
+                nameBr = nameB + 'r'
+
+                listNames = learner.listParameterNames()
+
+                matricesToPlot = []
+                namesToPlot = []                
+
+                print 'x,y=', event.xdata, event.ydata
+                n = hl.matrixToLayer(event.xdata, event.ydata)
+                print 'n =',n
+            
                 
+                #                 if nameW in listNames and nameM not in listNames:
+                
+                #                     row_list = []
+                #                     for j in arange(n,n+hl.groupsize):
+                #                         row_list.append(learner.getParameterRow(nameW,j))
+                #                     print row_list
+                    
+                #                     #HACK !!!
+                #                     for row in row_list:
+                #                         if len(row) == 28*28*2:
+                #                             row = array(toMinusRow(list(row)))
+                #                             print 'just hacked...'
+                #                     #END OF HACK
+                
+                #                     for j,row in enumerate(row_list):
+                #                         matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                #                         namesToPlot.append(nameW + '_' + str(n+j))
+                
+                #                     if nameWr in listNames:
+                
+                #                         row = learner.getParameterRow(nameWr,n)
+                
+                #                         #HACK !!!
+                #                         print 'just hacked...'
+                #                         if i==1 and len(row) == 28*28*2:
+                #                             row = array(toMinusRow(list(row)))
+                #                         #END OF HACK 
+                
+                #                         matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                #                         namesToPlot.append(nameWr)
 
+                if nameW in listNames and nameM not in listNames:                
+
+                    n = n - n%hl.groupsize
+                    print 'plotting weigths from',n,'to',n+hl.groupsize-1
+                    print learner.getParameterValue(nameW).shape
+                   
+                    figure(3)
+                    ioff()                    
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.01, toMinusRow)
+                    ion()
+                    draw()
+                    
+                #if nameM in listNames and nameM in listNames:
+                    
+                   # index_w = n%hl.groupsize
+                   # index_m = int(n/hl.groupsize)#
+
+#                    figure(3)
+#                    ioff()                    
+#                    clf()
+#                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.01, toMinusRow)
+#                    ion()
+#                    draw()
+#                    
+#                    figure(4)
+#                    ioff()
+#                    clf()
+#                    plotLayer1(learner.getParameterValue(nameM), 28, .1, n, hl.groupsize,.01, toMinusRow)
+#                    ion()
+#                    draw()
+                    
+                #  if nameW in listNames and nameM in listNames:
+                
+                #                     rowW = learner.getParameterRow(nameW,x)
+                #                     rowM = learner.getParameterRow(nameM,y)
+                
+                #                     #HACK !!!
+                #                     #print 'just hacked...'
+                #                     #if i==1 and len(row) == 28*28*2:
+                #                     #    row = array(toMinusRow(row))
+                #                     #END OF HACK
+                
+                #                     rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
+                #                     rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
+                
+                #                     #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+                
+                #                     produit = rowW*rowM                   
+                
+                #                     figure(3)
+                
+                #                     clf()                  
+                #                     plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
+                #                     draw()
+
+
+            # like 'w' on the max of each row -- 'C'
+
+            elif char == 'C':
+
+                prefixe = 'Layer' + str(i)
+                nameW = prefixe + '_W'
+                nameWr = nameW + 'r'
+                nameM = prefixe + '_M'
+                nameMr = nameM + 'r'
+                nameB = prefixe + '_b'
+                nameBr = nameB + 'r'
+                
+                listNames = learner.listParameterNames()
+                
+                matricesToPlot = []
+                namesToPlot = []
+                
+                print event.xdata,event.ydata
+                
+                indexes = []
+                names = []
+                for j in arange(hl.getNGroups()):
+                    row = hl.getRow(j)
+                    index = 0
+                    for k,el in enumerate(row):
+                        if el > row[index]:
+                            index = k
+
+                    indexes.append(j*hl.groupsize + index)
+                    names.append(str(row[index])[0:8])
+                
+                
+                if nameW in listNames:# and nameM not in listNames:                
+                   
+                    figure(3)
+                    ioff()                    
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.01, toMinusRow, indexes,names)
+                    ion()
+                    draw()
+                    
+                if nameM in listNames:
+                    
+                    figure(4)
+                    ioff()
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameM), 28, .05,0,0,.01,toMinusRow,indexes,names)
+                    ion()
+                    draw()
+                    
+                    
+
             # back to Original -- o
             
             elif char == 'o':
@@ -474,6 +687,9 @@
                 self.rep_axes[i].imshow(self.hidden_layers[i].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                 draw()
                 print '...done'
+
+            else :
+                print_usage_repAndRec()
                 
        
 
@@ -485,28 +701,36 @@
                 self.rep_axes[k].imshow(self.hidden_layers[k].getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
                 draw()
             print '...done'
-        
 
 
+
     def __clicked(self, event):
             
         self.__findCurrentLayer(event)
         
-        print 'current hidden layer is now', self.current_hl
+        if event.key == 'control':
+            print 'control-clic performed'
 
-        if event.key == 'control' and self.current_hl != -1:
+            data = self.hidden_layers[self.current_hl].getMatrix()
+            x,y,s,c = [],[],[],[]
+            for i in arange(data.shape[0]):
+                for j in arange(data.shape[1]):
+                    x.append(j)
+                    y.append(data.shape[1]-i)
+                    s.append(data[i,j]*25.)
+                    if data[i,j]<0 :
+                        c.append(1)
+                    else:
+                        c.append(-1)
+            f = figure(5)
             
-            hidden_layer = self.hidden_layers[self.current_hl]
-            
-            n = hidden_layer.matrixToLayer(x,y)
-           # print 'n', n
-            hidden_layer.hidden_layer[n] = 1 - hidden_layer.hidden_layer[n]
-            axes.imshow(hidden_layer.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)        
-            draw()
+            clf()
+            scatter(x,y,s=s, c=c, marker='s',cmap = cm.gray, norm = Normalize())
+            f.gca().set_axis_bgcolor("0.75")
+            colorbar()
+            draw()            
+        
 
-        if event.button == 3:
-            print 'Layer', self.current_hl, ', position (',x,y,'), value', xself.hidden_layers[self.current_hl].getElement(x,y)
-
     def __findCurrentLayer(self, event):
 
         fig = event.canvas.figure.number
@@ -516,6 +740,7 @@
         if axes != None:
             figure(fig)
             
+            
             #we find to which hidden layer corresponds our axes
             for i,a in enumerate(self.rep_axes):                
                 if a == axes:
@@ -525,11 +750,13 @@
     def __linkEvents(self):
 
         figure(self.fig_rep)
+        
         connect('key_press_event', self.__changeChar)
-        #connect('button_press_event', self.__clicked)
+        connect('button_press_event', self.__clicked)
         connect('key_press_event', self.__repCommands)
 
         figure(self.fig_rec)
+        
         connect('key_press_event', self.__changeChar)        
         #connect('button_press_event', self.__clicked)        
         
@@ -538,23 +765,27 @@
     ###
 
     def __plotReconstructions(self):
-        print 'plotting reconstructions...'
+        print 'plotting reconstructions...'        
         figure(self.fig_rec)
+        ioff()
         clf()
         plotMatrices(self.reconstructions)
+        ion()
         draw()
         print '...done.'
 
     def __plotRepresentations(self):
         print 'plotting representations...'
+        ioff()
         figure(self.fig_rep)
         clf()
         temp = []
         for x in self.hidden_layers:
             temp.append( x.getMatrix() )        
+        #draw()
+        self.rep_axes = plotMatrices(temp)        
         draw()
-
-        self.rep_axes = plotMatrices(temp)
+        ion()
         print '...done.'
 
     def __computeAndPlot(self):        
@@ -639,9 +870,32 @@
             file.write("\n\n\n---------- REC ------------------------------------------------------\n")            
             for i, mat in enumerate(rec):
                 appendMatrixToFile(file,  rowToMatrix(mat[0],28), 'rec of hidden layer ' + str(i+1))
-            
 
+    ###
+    ### utils
+    ###
 
+    def __sampleLayer(self, which_layer):
+        hl = self.hidden_layers[which_layer]
+        for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+            multi = numpy.random.multinomial(1,hl.getRow(n))                    
+            hl.setRow(n,multi)
+
+    def __reconstructLayer(self, which_layer):
+        hl = self.hidden_layers[which_layer+1]
+        
+        self.learner.setMatValue(which_layer+1, reshape(hl.hidden_layer, (1,-1)))
+        #reconstruct
+        row = self.learner.reconstructOneLayer(which_layer+1)[0]
+        #HACK
+        if len(row) == 28*28*2:
+            print 'hacking...'
+            row = array(toMinusRow(row))
+            #print the new layer
+        self.hidden_layers[which_layer].hidden_layer = row
+    
+
+
 class EachRowPlotter:
     
     def __init__(self, matrix, width = 28, plot_width = .1, space_between_images = .01, do_to_rows = None, i=0, nofig=0):
@@ -703,7 +957,7 @@
     names = learner.listParameterNames()
     
     #doToRow = None
-    doToRow = toPlusRow
+    doToRow = toMinusRow
     #doToRow = toMinusRow
 
     matrixName = ''
@@ -780,10 +1034,18 @@
 
 elif task == 'test':
     #jutilise cet endroit pour faire des tests
-
     pass
+elif task == 'help':
+    print 'plotRepAndRec:'
+    print
+    print_usage_repAndRec()
 
 
 
 else:
     print_usage_and_exit()
+
+
+
+
+    



From simonl at mail.berlios.de  Fri Aug  3 23:49:50 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Fri, 3 Aug 2007 23:49:50 +0200
Subject: [Plearn-commits] r7925 - trunk/plearn/var/EXPERIMENTAL
Message-ID: <200708032149.l73Lno5l010780@sheep.berlios.de>

Author: simonl
Date: 2007-08-03 23:49:50 +0200 (Fri, 03 Aug 2007)
New Revision: 7925

Modified:
   trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
Log:
optimized DoubleProductVariable


Modified: trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc
===================================================================
--- trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-08-03 21:49:00 UTC (rev 7924)
+++ trunk/plearn/var/EXPERIMENTAL/DoubleProductVariable.cc	2007-08-03 21:49:50 UTC (rev 7925)
@@ -79,6 +79,7 @@
         d = x.width();// ( = w.width() = m.width() )
 
 
+    /*
     for(int n=0; n<nx; n++)
         for(int i=0; i<nw; i++)        
             for(int j=0; j<nm; j++)
@@ -87,6 +88,25 @@
                 for(int k=0; k<d; k++)
                     matValue(n,i+j*nw) += x(n,k)*w(i,k)*m(j,k);
             }
+    */
+
+    for(int n=0; n<nx; n++)
+    {
+        real* matValue_n = matValue[n];
+        const real* x_n = x[n];
+        for(int j=0; j<nm; j++)
+        {
+            const real* m_j = m[j];
+            for(int i=0; i<nw; i++)
+            {
+                const real* w_i = w[i];
+                real val = 0;
+                for(int k=0; k<d; k++)
+                    val += x_n[k]*w_i[k]*m_j[k];
+                matValue_n[i+j*nw] = val;
+            }
+        }
+    }
 }
 // ### computes varray gradients from gradient
 void DoubleProductVariable::bprop()
@@ -104,18 +124,43 @@
         nm = m.length(),
         d = x.width();// ( = w.width()= m.width() )
 
-    
-    for(int n=0; n<nx; n++)
+    /*
+     for(int n=0; n<nx; n++)
         for(int i=0 ;i<nw; i++)
             for(int j=0; j<nm; j++)
             {
                 for(int k=0; k<d; k++)
-                {             
+                {
                     x_grad(n,k) += s_grad(n,i+j*nw)*w(i,k)*m(j,k);
                     w_grad(i,k) += s_grad(n,i+j*nw)*x(n,k)*m(j,k);
                     m_grad(j,k) += s_grad(n,i+j*nw)*x(n,k)*w(i,k);
+                }
+            }
+    */
+
+    for(int n=0; n<nx; n++)
+    {
+        const real* s_grad_n = s_grad[n];
+        const real* x_n = x[n];
+        real* x_grad_n = x_grad[n];
+        for(int j=0; j<nm; j++)
+        {
+            const real* m_j = m[j];
+            real* m_grad_j = m_grad[j];
+            for(int i=0 ;i<nw; i++)
+            {
+                const real* w_i = w[i];
+                real* w_grad_i = w_grad[i];
+                real s_grad_n_val = s_grad_n[i+j*nw];
+                for(int k=0; k<d; k++)
+                {             
+                    x_grad_n[k] += s_grad_n_val*w_i[k]*m_j[k];
+                    w_grad_i[k] += s_grad_n_val*x_n[k]*m_j[k];
+                    m_grad_j[k] += s_grad_n_val*x_n[k]*w_i[k];
                 }                
             }
+        }
+    }
 }
 // ### You can implement these methods:
 // void DoubleProductVariable::bbprop() {}



From yoshua at mail.berlios.de  Sat Aug  4 22:45:46 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 4 Aug 2007 22:45:46 +0200
Subject: [Plearn-commits] r7926 - trunk/plearn_learners/online
Message-ID: <200708042045.l74Kjkgk012411@sheep.berlios.de>

Author: yoshua
Date: 2007-08-04 22:45:46 +0200 (Sat, 04 Aug 2007)
New Revision: 7926

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed serious bug make neg_log_likelihood port compute incorrectly


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-03 21:49:50 UTC (rev 7925)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-04 20:45:46 UTC (rev 7926)
@@ -764,6 +764,40 @@
         }
         found_a_valid_configuration = true;
     }
+
+
+    // COMPUTE UNSUPERVISED NLL
+    if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
+    {
+        if (partition_function_is_stale && !during_training)
+        {
+            computePartitionFunction();
+            partition_function_is_stale=false;
+        }
+        if (visible && !visible->isEmpty()
+            && hidden && !hidden->isEmpty())
+        {
+            // neg-log-likelihood(visible,hidden) = energy(visible,hidden) + log(partition_function)
+            computeEnergy(*visible,*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (visible && !visible->isEmpty())
+        {
+            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
+            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood,hidden_act);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else if (hidden && !hidden->isEmpty())
+        {
+            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
+            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
+            *neg_log_likelihood += log_partition_function;
+        }
+        else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
+        found_a_valid_configuration = true;
+    }
+
+
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
@@ -1115,38 +1149,6 @@
         found_a_valid_configuration = true;
     }
 
-    // COMPUTE UNSUPERVISED NLL
-    if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
-    {
-        if (partition_function_is_stale && !during_training)
-        {
-            computePartitionFunction();
-            partition_function_is_stale=false;
-        }
-        if (visible && !visible->isEmpty()
-            && hidden && !hidden->isEmpty())
-        {
-            // neg-log-likelihood(visible,hidden) = energy(visible,hidden) + log(partition_function)
-            computeEnergy(*visible,*hidden,*neg_log_likelihood);
-            *neg_log_likelihood += log_partition_function;
-        }
-        else if (visible && !visible->isEmpty())
-        {
-            // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
-            computeFreeEnergyOfVisible(*visible,*neg_log_likelihood,hidden_act);
-            *neg_log_likelihood += log_partition_function;
-        }
-        else if (hidden && !hidden->isEmpty())
-        {
-            // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
-            computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
-            *neg_log_likelihood += log_partition_function;
-        }
-        else PLERROR("RBMModule: neg_log_likelihood currently computable only of the visible as inputs");
-        found_a_valid_configuration = true;
-    }
-
-
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
@@ -1668,7 +1670,8 @@
     {
         // very cheap shot, specializing to the common case...
         PLASSERT(hidden_layer->classname()=="RBMBinomialLayer");
-        PLASSERT(visible_layer->classname()=="RBMBinomialLayer");
+        PLASSERT(visible_layer->classname()=="RBMBinomialLayer" ||
+                 visible_layer->classname()=="RBMGaussianlLayer");
         PLASSERT(connection->classname()=="RBMMatrixConnection");
         PLASSERT(hidden && !hidden->isEmpty());
         // FE(x) = -b'x - sum_i softplus(hidden_layer->activation[i])        



From lamblin at mail.berlios.de  Sun Aug  5 08:34:30 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sun, 5 Aug 2007 08:34:30 +0200
Subject: [Plearn-commits] r7927 - trunk/plearn_learners/online
Message-ID: <200708050634.l756YU3Y005721@sheep.berlios.de>

Author: lamblin
Date: 2007-08-05 08:34:29 +0200 (Sun, 05 Aug 2007)
New Revision: 7927

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fix -noblas compilation


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-04 20:45:46 UTC (rev 7926)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-05 06:34:29 UTC (rev 7927)
@@ -1693,11 +1693,12 @@
             multiplyAcc((*visible_grad)(t),visible_layer->bias,-dC_dFE);
         }
         if (same_dC_dFE)
-            productScaleAcc(*visible_grad,p,false,weights,false,-dC_dFE,1);
+            productScaleAcc(*visible_grad, p, false, weights, false, -dC_dFE,
+                            real(1));
         else
             for (int t=0;t<mbs;t++)
-                productScaleAcc((*visible_grad)(t),weights,true,p(t),
-                        -(*energy_grad)(t, 0),1);
+                productScaleAcc((*visible_grad)(t), weights, true, p(t),
+                        -(*energy_grad)(t, 0), real(1));
     }
 
     // Explicit error message in the case of the 'visible' port.



From lamblin at mail.berlios.de  Sun Aug  5 08:39:28 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sun, 5 Aug 2007 08:39:28 +0200
Subject: [Plearn-commits] r7928 - in trunk: plearn/dict
	plearn_learners/hyper plearn_learners/meta
	plearn_learners/nearest_neighbors plearn_learners/online
	plearn_learners_experimental
Message-ID: <200708050639.l756dSZ1005893@sheep.berlios.de>

Author: lamblin
Date: 2007-08-05 08:39:28 +0200 (Sun, 05 Aug 2007)
New Revision: 7928

Modified:
   trunk/plearn/dict/
   trunk/plearn_learners/hyper/
   trunk/plearn_learners/meta/
   trunk/plearn_learners/nearest_neighbors/
   trunk/plearn_learners/online/
   trunk/plearn_learners_experimental/
Log:
Ignore OBJS directories



Property changes on: trunk/plearn/dict
___________________________________________________________________
Name: svn:ignore
   + OBJS



Property changes on: trunk/plearn_learners/hyper
___________________________________________________________________
Name: svn:ignore
   + OBJS



Property changes on: trunk/plearn_learners/meta
___________________________________________________________________
Name: svn:ignore
   + OBJS



Property changes on: trunk/plearn_learners/nearest_neighbors
___________________________________________________________________
Name: svn:ignore
   + OBJS



Property changes on: trunk/plearn_learners/online
___________________________________________________________________
Name: svn:ignore
   + OBJS



Property changes on: trunk/plearn_learners_experimental
___________________________________________________________________
Name: svn:ignore
   + OBJS




From yoshua at mail.berlios.de  Sun Aug  5 21:44:45 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 5 Aug 2007 21:44:45 +0200
Subject: [Plearn-commits] r7929 - trunk/plearn_learners/online
Message-ID: <200708051944.l75JijD3026111@sheep.berlios.de>

Author: yoshua
Date: 2007-08-05 21:44:44 +0200 (Sun, 05 Aug 2007)
New Revision: 7929

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
locally side-stepped a design problem with state ports in RBMModule


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-05 06:39:28 UTC (rev 7928)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-05 19:44:44 UTC (rev 7929)
@@ -841,8 +841,15 @@
             visible_layer->computeExpectations();
             const Mat expectations=visible_layer->getExpectations();
             visible->resize(expectations.length(),visible_layer->size);
-            *visible_activation << visible_layer->activations;
+            *visible << expectations;
         }
+        if (hidden_act && hidden_act->isEmpty())
+        {
+            // THIS IS STUPID CODE TO HANDLE THE BAD state SYSTEM AND AVOID AN UNNECESSARY ERROR MESSAGE
+            // (hidden_act is a "state" port that must always be produced, even if we don't compute it!)
+            hidden_act->resize(hidden_layer->samples.length(),
+                               hidden_layer->samples.width());
+        }
         found_a_valid_configuration = true;
     }
 



From yoshua at mail.berlios.de  Sun Aug  5 22:08:47 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 5 Aug 2007 22:08:47 +0200
Subject: [Plearn-commits] r7930 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200708052008.l75K8l1c027486@sheep.berlios.de>

Author: yoshua
Date: 2007-08-05 22:08:46 +0200 (Sun, 05 Aug 2007)
New Revision: 7930

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
Added the feature of distinguishing the model used for test from 
the one used for train in autolr (just the fixed-schedule case now).


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-05 19:44:44 UTC (rev 7929)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-05 20:08:46 UTC (rev 7930)
@@ -158,6 +158,7 @@
                         lr_options,
                         schedules,
                         trainset, testsets, expdir,
+                        tester=None,
                         cost_to_select_best=0,
                         selected_costnames = None,
                         get_train_costs = True,
@@ -171,9 +172,13 @@
 The schedules argument is an array with the stages sequence in its first
 column and sequences of learning rates (one sequence per group) in
 each of the other columns (just like the result of the call to merge_schedules).
+Optionally, a different learner can be supplied for training (the learner)
+and testing (the tester).
 """
+    if not tester:
+        tester = learner
     train_costnames = learner.getTrainCostNames()
-    test_costnames = getTestCostNames(learner)
+    test_costnames = getTestCostNames(tester)
 
     # Filter out unwanted costnames
     if selected_costnames is not None:
@@ -237,7 +242,7 @@
 
         # Report error on test sets
         for j in range(n_tests):
-            costs = testlearner(learner,testsets[j])
+            costs = testlearner(tester,testsets[j])
             if logfile:
                 print >>logfile, "At stage ", learner.stage, " test" + str(j+1),": ",
             for k in range(0,n_test_costs):
@@ -249,6 +254,8 @@
                 if k==cost_to_select_best and j==0 and err < best_err:
                     best_err = err
                     learner.save(expdir+"/"+"best_learner.psave","plearn_ascii")
+                    if learner!=tester:
+                        learner.save(expdir+"/"+"best_tester.psave","plearn_ascii")
                 if plearn.bridgemode.interactive:
                     plot(results[0:i+1,0],
                             results[0:i+1, 1+n_schedules+n_train_costs+(j*n_test_costs)+k],



From chapados at mail.berlios.de  Mon Aug  6 13:35:13 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 6 Aug 2007 13:35:13 +0200
Subject: [Plearn-commits] r7931 - trunk/plearn/math
Message-ID: <200708061135.l76BZDS6005280@sheep.berlios.de>

Author: chapados
Date: 2007-08-06 13:35:12 +0200 (Mon, 06 Aug 2007)
New Revision: 7931

Modified:
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/StatsCollector.h
Log:
Added agemax / agemin statistics

Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2007-08-05 20:08:46 UTC (rev 7930)
+++ trunk/plearn/math/StatsCollector.cc	2007-08-06 11:35:12 UTC (rev 7931)
@@ -83,6 +83,8 @@
     "  - KURT         -  Excess Kurtosis == E(X-mu)^4 / sigma^4 - 3\n"
     "  - MIN          -  Minimum value\n"
     "  - MAX          -  Maximum value\n"
+    "  - AGEMIN       -  How many observations ago the min was observed\n"
+    "  - AGEMAX       -  How many observations ago the max was observed\n"
     "  - RANGE        -  The range, i.e. MAX - MIN\n"
     "  - SUM          -  Sum of observations \n"
     "  - SUMSQ        -  Sum of squares\n"
@@ -161,6 +163,8 @@
       sumfourth_(0.),
       min_(MISSING_VALUE),
       max_(MISSING_VALUE),
+      agemin_(MISSING_VALUE),
+      agemax_(MISSING_VALUE),
       first_(MISSING_VALUE),
       last_(MISSING_VALUE),
       more_than_maxnvalues(false),
@@ -270,6 +274,16 @@
         ol, "max_", &StatsCollector::max_,
         OptionBase::learntoption,
         "the max");
+
+    declareOption(
+        ol, "agmemin_", &StatsCollector::agemin_,
+        OptionBase::learntoption,
+        "How many observations ago the min was observed");
+
+    declareOption(
+        ol, "agemax_", &StatsCollector::agemax_,
+        OptionBase::learntoption,
+        "How many observations ago the max was observed");
     
     declareOption(
         ol, "first_", &StatsCollector::first_,
@@ -348,6 +362,8 @@
     sumfourth_ = 0.;
     min_ = MISSING_VALUE;
     max_ = MISSING_VALUE;
+    agemin_ = MISSING_VALUE;
+    agemax_ = MISSING_VALUE;
     first_ = last_ = MISSING_VALUE;
     more_than_maxnvalues = (maxnvalues == 0);
     approximate_counts.clear();
@@ -375,10 +391,18 @@
         last_ = val;
         if(fast_exact_is_equal(nnonmissing_,0))    // first value encountered
             min_ = max_ = first_ = last_ = val;
-        else if(val<min_)
+        else if(val<min_) {
             min_ = val;
-        else if(val>max_)
+            agemin_ = 0;
+        }
+        else if(val>max_) {
             max_ = val;
+            agemax_ = 0;
+        }
+        else {
+            ++agemax_;                       // works even if they are missing
+            ++agemin_;
+        }
         nnonmissing_ += weight;
         sumsquarew_  += weight * weight;
         double sqval = (val-first_)*(val-first_);
@@ -931,6 +955,8 @@
         statistics["KURT"]        = STATFUN(&StatsCollector::kurtosis);
         statistics["MIN"]         = STATFUN(&StatsCollector::min);
         statistics["MAX"]         = STATFUN(&StatsCollector::max);
+        statistics["AGEMIN"]      = STATFUN(&StatsCollector::agemin);
+        statistics["AGEMAX"]      = STATFUN(&StatsCollector::agemax);
         statistics["RANGE"]       = STATFUN(&StatsCollector::range);
         statistics["SUM"]         = STATFUN(&StatsCollector::sum);
         statistics["SUMSQ"]       = STATFUN(&StatsCollector::sumsquare);
@@ -977,7 +1003,7 @@
   
     map<string,STATFUN>::iterator fun = statistics.find(statname);
     if (fun == statistics.end())
-        PLERROR("In StatsCollector::getStat, invalid statname %s",
+        PLERROR("In StatsCollector::getStat, invalid statname '%s'",
                 statname.c_str());
     else
         return (this->*(fun->second))();
@@ -1241,14 +1267,28 @@
     nnonmissing_+= other.nnonmissing_;
     sumsquarew_+= other.sumsquarew_;
 
-    min_= std::min(min_, other.min_);
-    max_= std::max(max_, other.max_);
-    last_= other.last_; //assume this is first and other is last.
+    // In merging first/last/ages, we assume that 'this' comes first, and
+    // 'other' comes last.
+    if (other.min_ < min_) {
+        min_ = other.min_;
+        agemin_ = other.agemin_;
+    }
+    else {
+        agemin_ += other.n();
+    }
+    
+    if (other.max_ > max_) {
+        max_ = other.max_;
+        agemax_ = other.agemax_;
+    }
+    else {
+        agemax_ += other.n();
+    }
+    last_= other.last_; // assume this is first and other is last.
     sorted = false;
 
     if (storeCounts())//now merge counts
-    {
-        
+    {        
         int nextid= 0;
         set<real> already_merged;
         map<real,StatsCollectorCounts>::iterator it;

Modified: trunk/plearn/math/StatsCollector.h
===================================================================
--- trunk/plearn/math/StatsCollector.h	2007-08-05 20:08:46 UTC (rev 7930)
+++ trunk/plearn/math/StatsCollector.h	2007-08-06 11:35:12 UTC (rev 7931)
@@ -182,6 +182,8 @@
     double sumfourth_;     //!< sum of fourth-power of all (values-first_)
     real min_;             //!< the min
     real max_;             //!< the max
+    real agemin_;          //!< how many observations ago the min was observed
+    real agemax_;          //!< how many observations ago the max was observed
     real first_;           //!< first encountered nonmissing observation
     real last_;            //!< last encountered nonmissing observation
     bool more_than_maxnvalues;
@@ -232,6 +234,8 @@
                                                       first_*first_*nnonmissing_); }
     real min() const                    { return min_; }
     real max() const                    { return max_; }
+    real agemin() const                 { return agemin_; }
+    real agemax() const                 { return agemax_; }
     real range() const                  { return max_ - min_; }
     real mean() const                   { return real(sum()/nnonmissing_); }
     //! The normalization for variance (nnonmissing_ - sumsquarew_/nnonmissing_)



From yoshua at mail.berlios.de  Mon Aug  6 14:32:12 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 6 Aug 2007 14:32:12 +0200
Subject: [Plearn-commits] r7932 - trunk/plearn/math
Message-ID: <200708061232.l76CWCwZ010981@sheep.berlios.de>

Author: yoshua
Date: 2007-08-06 14:32:12 +0200 (Mon, 06 Aug 2007)
New Revision: 7932

Modified:
   trunk/plearn/math/pl_math.cc
   trunk/plearn/math/pl_math.h
Log:
Do logadd in double precision instead of single.


Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2007-08-06 11:35:12 UTC (rev 7931)
+++ trunk/plearn/math/pl_math.cc	2007-08-06 12:32:12 UTC (rev 7932)
@@ -155,7 +155,7 @@
 }
 
 // compute log(exp(log_a)+exp(log_b)) without losing too much precision
-real logadd(real log_a, real log_b)
+real logadd(double log_a, double log_b)
 {
     if (log_a < log_b)
     { // swap them
@@ -163,10 +163,10 @@
         log_a = log_b;
         log_b = tmp;
     }
-    real negative_absolute_difference = log_b - log_a;
+    double negative_absolute_difference = log_b - log_a;
     if (negative_absolute_difference < MINUS_LOG_THRESHOLD)
         return log_a;
-    return log_a + log1p(exp(negative_absolute_difference));
+    return (real)(log_a + log1p(exp(negative_absolute_difference)));
 }
 
 real square_f(real x)

Modified: trunk/plearn/math/pl_math.h
===================================================================
--- trunk/plearn/math/pl_math.h	2007-08-06 11:35:12 UTC (rev 7931)
+++ trunk/plearn/math/pl_math.h	2007-08-06 12:32:12 UTC (rev 7932)
@@ -548,8 +548,9 @@
 typedef real (*tRealFunc)(real);
 typedef real (*tRealReadFunc)(real,real);
 
-//!  compute log(exp(log_a)+exp(log_b)) without losing too much precision
-real logadd(real log_a, real log_b);
+//! compute log(exp(log_a)+exp(log_b)) without losing too much precision
+//! (doing the computation in double precision)
+real logadd(double log_a, double log_b);
 
 //!  compute log(exp(log_a)-exp(log_b)) without losing too much precision
 real logsub(real log_a, real log_b);



From chapados at mail.berlios.de  Mon Aug  6 15:20:04 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 6 Aug 2007 15:20:04 +0200
Subject: [Plearn-commits] r7933 - in trunk:
	plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0
	plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0
	plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0
	plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0
	plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0
Message-ID: <200708061320.l76DK4db014553@sheep.berlios.de>

Author: chapados
Date: 2007-08-06 15:20:02 +0200 (Mon, 06 Aug 2007)
New Revision: 7933

Modified:
   trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave
Log:
New test results following new statistics in StatsCollector

Modified: trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,8 +4,11 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
+epsilon = 0 ;
 maxnvalues = 50 ;
 no_removal_warnings = 0 ;
 nmissing_ = 2 ;
@@ -17,6 +20,8 @@
 sumfourth_ = 4474466186.13354397 ;
 min_ = 0.0194221049593962872 ;
 max_ = 265.08702193111759 ;
+agmemin_ = 63 ;
+agemax_ = 83 ;
 first_ = 13.1144255088962556 ;
 last_ = 34.194047475019488 ;
 counts = {0.0194221049593962872 : 1 0 0 0 35 , 0.0325698952931281691 : 1 1 0.0210580598108425186 0.000443441882997020864 25 , 0.061987379823822554 : 1 0 0 0 34 , 0.0786715812781189072 : 1 0 0 0 15 , 0.276547938266577475 : 1 2 0.312045165023246862 0.0543652000143007019 49 , 0.903268077600334784 : 1 2 1.20251619734469539 0.752392985050565066 40 , 0.957582475886613049 : 1 0 0 0 22 , 2.02714634885673828 : 1 4 6.16185831341262613 9.85467870389801881 3 , 2.05231943353131463 : 1 1 2.03043031787215211 4.12264727573440837 44 , 2.55639694673505691 : 1 4 9.56965095026421331 22.9473976535547095 46 , 3.39504868705936369 : 1 1 2.79701999842607396 7.8233208715953948 39 , 4.1429907417257068 : 1 3 11.3023925206681426 42.6903958007932758 6 , 4.67453021446005579 : 1 0 0 0 21 , 5.08242518418548705 : 1 1 4.7761127815638682 22.8112533022177502 13 , 5.25250384797228076 : 1 0 0 0 19 , 5.37207018712550166 : 1 0 0 0 45 , 6.35849467137279323 : 1 1 5.85851516894162305 34.3221999847190915 24 , 7.207466!
 45300661459 : 1 1 6.87072140435881717 47.2068126163143944 29 , 7.96578391954506237 : 1 0 0 0 20 , 8.93298576523078935 : 1 1 8.17089885561959051 66.7635881087655321 14 , 8.98920177002007748 : 1 0 0 0 36 , 12.0085320633282215 : 1 3 31.1542680463670294 326.711585935405083 28 , 12.2468237044022299 : 1 0 0 0 16 , 13.1144255088962556 : 1 0 0 0 1 , 15.2640879450440519 : 1 5 70.5059901992690072 995.965835431366486 42 , 18.0644851071384664 : 1 2 34.6909123848655199 601.832092160831735 7 , 20.2143972777140739 : 1 2 37.4966034917601547 703.430823392461662 5 , 20.7110556243879174 : 1 0 0 0 4 , 22.6057399040152376 : 1 1 21.3536138210659132 455.976823219217181 23 , 23.8020931670862268 : 1 1 23.6527978431544454 559.454845809131598 50 , 23.8229065014076049 : 1 0 0 0 17 , 26.9337117592480837 : 1 0 0 0 27 , 29.789060511597107 : 1 1 27.1438737768138765 736.789883611604068 32 , 33.5218059320941393 : 1 3 95.7311987093098651 3059.48022466103157 9 , 34.3179649564094618 : 1 1 34.194047475019488 11!
 69.23288272388663 41 , 34.5907593027237326 : 1 0 0 0 37 , 37.1!
 23299919
4826334 : 1 0 0 0 47 , 38.8322499582853808 : 1 1 37.9844470884896737 1442.81822061827165 38 , 39.8865512936735271 : 1 0 0 0 26 , 47.911092434272156 : 1 0 0 0 11 , 53.9168908401858928 : 1 1 48.3496423219557911 2337.68791266105836 10 , 54.8916107965928788 : 1 0 0 0 18 , 58.9309522653012152 : 1 2 112.094544383224672 6282.66544835667901 8 , 63.517761056607462 : 1 1 60.6704064161583645 3680.89821470183006 2 , 69.1908667492120912 : 1 0 0 0 48 , 74.3267253554510745 : 1 1 72.476685639701131 5252.86996131605974 31 , 89.8783015490838295 : 1 0 0 0 30 , 98.1643547155170637 : 1 0 0 0 33 , 120.196546132356843 : 1 0 0 0 43 , 265.08702193111759 : 1 1 124.39630041420412 15474.4395567409192 12 , 3.4028234663852886e+38 : 0 0 0 0 0 };

Modified: trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,8 +4,11 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
+window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
+epsilon = 0 ;
 maxnvalues = 50 ;
 no_removal_warnings = 0 ;
 nmissing_ = 0 ;
@@ -17,6 +20,8 @@
 sumfourth_ = 4411195564.83381653 ;
 min_ = 0 ;
 max_ = 242.7039735654227 ;
+agmemin_ = nan ;
+agemax_ = 86 ;
 first_ = 0 ;
 last_ = 34.194047475019488 ;
 counts = {0 : 1 0 0 0 1 , 0.0436211272393489807 : 1 1 0.0417936922830760907 0.00174671271465245406 49 , 0.0780745277513890251 : 1 1 0.0620651721884592222 0.003852085598783092 22 , 0.793490675529345713 : 1 4 1.27267393405984297 0.48054538356323695 21 , 1.13483097908655117 : 1 2 1.82928414820593765 1.69930327063684383 24 , 1.32035787782731528 : 1 0 0 0 19 , 1.37677747033506148 : 1 0 0 0 15 , 1.50017376482361775 : 1 0 0 0 25 , 1.93306322839291611 : 1 0 0 0 36 , 2.04137204438840225 : 1 1 2.00966390728841704 4.03874902025774762 35 , 2.05209475228781768 : 1 0 0 0 13 , 2.38187620766209829 : 1 5 11.1328465930074927 24.8205529574598387 45 , 2.54602178704972104 : 1 1 2.4681971882486935 6.09199736007875625 20 , 3.45634318013221842 : 1 2 6.02305690568915608 18.4868124683608066 34 , 4.04863286819631885 : 1 1 3.90734320361255216 15.2673309108172024 14 , 4.09991346853805272 : 1 1 4.06115618423520708 16.4929895527518688 40 , 4.71739271846963959 : 1 0 0 0 44 , 4.8083034015238173 : 1 0 0 0 6!
  , 5.39038250928048956 : 1 0 0 0 46 , 5.75356403245024417 : 1 0 0 0 3 , 5.97192544261328795 : 1 0 0 0 16 , 7.19336610689008893 : 1 1 6.59960102130673842 43.5547336404329428 4 , 8.81439393732250842 : 1 2 16.2284147850959926 131.748158226717123 39 , 11.1859811965730067 : 1 3 31.3097660888156 327.345353920191315 23 , 11.9885881242943437 : 1 0 0 0 27 , 12.3073646275676634 : 1 0 0 0 7 , 16.3943001305386282 : 1 6 86.733569910522661 1260.83422064745787 32 , 16.6528794576542225 : 1 0 0 0 29 , 21.781422411468963 : 1 3 54.352760502880912 987.728089338036511 41 , 22.2919531921684104 : 1 2 44.1763845837664917 975.789489723103316 26 , 23.7235046550653372 : 1 0 0 0 9 , 24.7669777878771704 : 1 0 0 0 42 , 25.0720218525932417 : 1 0 0 0 28 , 25.9279535636213581 : 1 1 25.2163637439409634 635.865000466740298 5 , 30.2632496601862684 : 1 1 28.1742577680153126 793.788800778571158 50 , 31.2195948795056886 : 1 0 0 0 17 , 33.5889072001640798 : 1 1 32.0996562918163519 1030.38793405274509 2 , 38.53986!
 97538835009 : 1 1 34.194047475019488 1169.23288272388663 11 , !
 40.06837
47231372465 : 1 0 0 0 18 , 44.4985016753966889 : 1 2 84.0812449116471896 3535.98635542121747 47 , 53.0979240795130352 : 1 1 50.2284523822919979 2522.8974287201745 37 , 53.919052283920152 : 1 0 0 0 31 , 54.2632702824460935 : 1 0 0 0 8 , 55.2902654485446163 : 1 0 0 0 10 , 55.2944016290821025 : 1 0 0 0 38 , 57.4415230400068069 : 1 1 56.7000628829983597 3214.89713093596811 48 , 67.9190894734021242 : 1 2 118.175181234439947 6984.53546411977368 33 , 110.543921282626172 : 1 1 69.4053927156164576 4817.1085380089462 30 , 138.13990230238511 : 1 1 127.322264022238727 16210.9589157486662 43 , 242.7039735654227 : 1 0 0 0 12 , 3.4028234663852886e+38 : 0 0 0 0 0 };

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,13 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 48.8719180204968495 ;
-sumsquare_ = 104.363929133442966 ;
-sumcube_ = 318.950374924762059 ;
-sumfourth_ = 1177.77083991773679 ;
-min_ = 17.6042426640295062 ;
-max_ = 22.4027835362180454 ;
-first_ = 17.6199119453043878 ;
+sum_ = 48.8719180204962811 ;
+sumsquare_ = 104.363929133440806 ;
+sumcube_ = 318.950374924751884 ;
+sumfourth_ = 1177.77083991768404 ;
+min_ = 17.6042426640295098 ;
+max_ = 22.4027835362179744 ;
+agmemin_ = 34 ;
+agemax_ = 18 ;
+first_ = 17.6199119453043913 ;
 last_ = 17.6479526969470228 ;
 counts = {};
 more_than_maxnvalues = 1  )

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 9.41750512464377465 ;
-sumsquare_ = 139.406100961585793 ;
-sumcube_ = 289.545736167181076 ;
-sumfourth_ = 1182.45789565909399 ;
+sum_ = 9.41750512464426492 ;
+sumsquare_ = 139.406100961582524 ;
+sumcube_ = 289.545736167174312 ;
+sumfourth_ = 1182.4578956590592 ;
 min_ = 17.600507769412193 ;
-max_ = 23.689855294846268 ;
-first_ = 18.5231445696105474 ;
-last_ = 19.4342267319091135 ;
+max_ = 23.689855294846236 ;
+agmemin_ = 93 ;
+agemax_ = 6 ;
+first_ = 18.5231445696105332 ;
+last_ = 19.4342267319090674 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 300 ;
 sumsquarew_ = 300 ;
-sum_ = 19.3910908077334234 ;
-sumsquare_ = 13.9110920621933758 ;
-sumcube_ = 15.8389851220074238 ;
-sumfourth_ = 22.117074279799926 ;
-min_ = 3.99687001153368061e-05 ;
-max_ = 1.79622562496960114 ;
-first_ = 0.00559245542604428977 ;
-last_ = 0.0703378652397716186 ;
+sum_ = 19.3910908077377719 ;
+sumsquare_ = 13.9110920621987386 ;
+sumcube_ = 15.8389851220146944 ;
+sumfourth_ = 22.1170742798113977 ;
+min_ = 3.99687001153608957e-05 ;
+max_ = 1.79622562497000327 ;
+agmemin_ = 230 ;
+agemax_ = 8 ;
+first_ = 0.00559245542604580592 ;
+last_ = 0.0703378652394482801 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 75 ;
 sumsquarew_ = 75 ;
-sum_ = 144.001912089437212 ;
-sumsquare_ = 10604.5784209182184 ;
-sumcube_ = 1024145.0899799359 ;
-sumfourth_ = 102318280.068502203 ;
-min_ = 5.47450516263987552e-06 ;
-max_ = 100.55774961254312 ;
-first_ = 0.0159720402812144995 ;
-last_ = 0.000951497798096395436 ;
+sum_ = 144.002796681173749 ;
+sumsquare_ = 10604.7679530276328 ;
+sumcube_ = 1024173.84013462893 ;
+sumfourth_ = 102322137.326348484 ;
+min_ = 5.4718757392551664e-06 ;
+max_ = 100.558698546447715 ;
+agmemin_ = 36 ;
+agemax_ = 28 ;
+first_ = 0.0159720327578714814 ;
+last_ = 0.000951494976113702773 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 300 ;
 sumsquarew_ = 300 ;
-sum_ = 148.808352708123778 ;
-sumsquare_ = 10610.4812901734367 ;
-sumcube_ = 1024527.91501712846 ;
-sumfourth_ = 102367281.283304423 ;
-min_ = 1.04669073984214173e-16 ;
-max_ = 100.55774961254312 ;
-first_ = 0.00401338000239141648 ;
-last_ = 0.000951497798096395436 ;
+sum_ = 148.80923780255759 ;
+sumsquare_ = 10610.6708408309078 ;
+sumcube_ = 1024556.67164685845 ;
+sumfourth_ = 102371139.874690771 ;
+min_ = 1.04669084620911105e-16 ;
+max_ = 100.558698546447715 ;
+agmemin_ = 169 ;
+agemax_ = 28 ;
+first_ = 0.00401338269720593124 ;
+last_ = 0.000951494976113702773 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 225 ;
 sumsquarew_ = 225 ;
-sum_ = 3.90954109777482861 ;
-sumsquare_ = 2.44800364618726629 ;
-sumcube_ = 2.31347577141776473 ;
-sumfourth_ = 2.50165010647749542 ;
-min_ = 1.04669073984214173e-16 ;
-max_ = 1.21796818612171132 ;
-first_ = 0.00401338000239141648 ;
-last_ = 0.000157352580241893088 ;
+sum_ = 3.90954236683399037 ;
+sumsquare_ = 2.44800399839949234 ;
+sumcube_ = 2.31347596784497256 ;
+sumfourth_ = 2.50165040817058326 ;
+min_ = 1.04669084620911105e-16 ;
+max_ = 1.21796824903727607 ;
+agmemin_ = 96 ;
+agemax_ = 37 ;
+first_ = 0.00401338269720593124 ;
+last_ = 0.000157352772999446117 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 5 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -4802.11772097782159 ;
-sumsquare_ = 11114205.6583226994 ;
-sumcube_ = 912252339.820312977 ;
-sumfourth_ = 4363965068449.31104 ;
-min_ = 4.49614059257512277 ;
-max_ = 1506.91426994253879 ;
-first_ = 564.628509411856953 ;
-last_ = 13.7380273402322342 ;
+sum_ = -4802.11772097781613 ;
+sumsquare_ = 11114205.6583226975 ;
+sumcube_ = 912252339.820317268 ;
+sumfourth_ = 4363965068449.31055 ;
+min_ = 4.49614059257518761 ;
+max_ = 1506.91426994253925 ;
+agmemin_ = 25 ;
+agemax_ = 39 ;
+first_ = 564.628509411856726 ;
+last_ = 13.7380273402321649 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -30,14 +33,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -4802.11772097782159 ;
-sumsquare_ = 11114205.6583226994 ;
-sumcube_ = 912252339.820312977 ;
-sumfourth_ = 4363965068449.31104 ;
-min_ = 4.49614059257512277 ;
-max_ = 1506.91426994253879 ;
-first_ = 564.628509411856953 ;
-last_ = 13.7380273402322342 ;
+sum_ = -4802.11772097781613 ;
+sumsquare_ = 11114205.6583226975 ;
+sumcube_ = 912252339.820317268 ;
+sumfourth_ = 4363965068449.31055 ;
+min_ = 4.49614059257518761 ;
+max_ = 1506.91426994253925 ;
+agmemin_ = 25 ;
+agemax_ = 39 ;
+first_ = 564.628509411856726 ;
+last_ = 13.7380273402321649 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -51,10 +56,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.23533365254734484 ;
-max_ = 6.23533365254734484 ;
-first_ = 6.23533365254734484 ;
-last_ = 6.23533365254734484 ;
+min_ = 6.23533365254733773 ;
+max_ = 6.23533365254733773 ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = 6.23533365254733773 ;
+last_ = 6.23533365254733773 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -68,10 +75,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.35575906431119542 ;
-max_ = 6.35575906431119542 ;
-first_ = 6.35575906431119542 ;
-last_ = 6.35575906431119542 ;
+min_ = 6.35575906431118831 ;
+max_ = 6.35575906431118831 ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = 6.35575906431118831 ;
+last_ = 6.35575906431118831 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -85,10 +94,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.29554635842927013 ;
-max_ = 6.29554635842927013 ;
-first_ = 6.29554635842927013 ;
-last_ = 6.29554635842927013 ;
+min_ = 6.29554635842926302 ;
+max_ = 6.29554635842926302 ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = 6.29554635842926302 ;
+last_ = 6.29554635842926302 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 5 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -61031.4440677611565 ;
-sumsquare_ = 88239109.550819248 ;
-sumcube_ = -124602854141.718964 ;
-sumfourth_ = 187860766474339.531 ;
-min_ = 1.23065096944085917 ;
-max_ = 2757.59439270702978 ;
-first_ = 1651.65821078803242 ;
-last_ = 892.793570994575475 ;
+sum_ = -61031.3713111458346 ;
+sumsquare_ = 88238920.3267626911 ;
+sumcube_ = -124602455703.620605 ;
+sumfourth_ = 187859983108665.156 ;
+min_ = 1.23064138227748932 ;
+max_ = 2757.59234913165938 ;
+agmemin_ = 22 ;
+agemax_ = 1 ;
+first_ = 1651.65665408713994 ;
+last_ = 892.793588571159603 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -30,14 +33,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = -61031.4440677611565 ;
-sumsquare_ = 88239109.550819248 ;
-sumcube_ = -124602854141.718964 ;
-sumfourth_ = 187860766474339.531 ;
-min_ = 1.23065096944085917 ;
-max_ = 2757.59439270702978 ;
-first_ = 1651.65821078803242 ;
-last_ = 892.793570994575475 ;
+sum_ = -61031.3713111458346 ;
+sumsquare_ = 88238920.3267626911 ;
+sumcube_ = -124602455703.620605 ;
+sumfourth_ = 187859983108665.156 ;
+min_ = 1.23064138227748932 ;
+max_ = 2757.59234913165938 ;
+agmemin_ = 22 ;
+agemax_ = 1 ;
+first_ = 1651.65665408713994 ;
+last_ = 892.793588571159603 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -51,10 +56,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.09268868403331076 ;
-max_ = 6.09268868403331076 ;
-first_ = 6.09268868403331076 ;
-last_ = 6.09268868403331076 ;
+min_ = 6.09268869187651152 ;
+max_ = 6.09268869187651152 ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = 6.09268869187651152 ;
+last_ = 6.09268869187651152 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -68,10 +75,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.2933977036397275 ;
-max_ = 6.2933977036397275 ;
-first_ = 6.2933977036397275 ;
-last_ = 6.2933977036397275 ;
+min_ = 6.29339771148292826 ;
+max_ = 6.29339771148292826 ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = 6.29339771148292826 ;
+last_ = 6.29339771148292826 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -85,10 +94,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.19304319383651958 ;
-max_ = 6.19304319383651958 ;
-first_ = 6.19304319383651958 ;
-last_ = 6.19304319383651958 ;
+min_ = 6.19304320167971944 ;
+max_ = 6.19304320167971944 ;
+agmemin_ = nan ;
+agemax_ = nan ;
+first_ = 6.19304320167971944 ;
+last_ = 6.19304320167971944 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 12:32:12 UTC (rev 7932)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 5 [ StatsCollector(
@@ -14,13 +15,15 @@
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
 sum_ = -49625.3815839320014 ;
-sumsquare_ = 61057483.5670844465 ;
-sumcube_ = -73001204564.4328766 ;
-sumfourth_ = 92944320862419.2188 ;
-min_ = 1.86363769529455059 ;
+sumsquare_ = 61057483.567084454 ;
+sumcube_ = -73001204564.4328918 ;
+sumfourth_ = 92944320862419.2344 ;
+min_ = 1.86363769529453394 ;
 max_ = 2256.46039029148506 ;
+agmemin_ = 22 ;
+agemax_ = 1 ;
 first_ = 1412.74238652622353 ;
-last_ = 782.354451732652365 ;
+last_ = 782.35445173265191 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -31,13 +34,15 @@
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
 sum_ = -49625.3815839320014 ;
-sumsquare_ = 61057483.5670844465 ;
-sumcube_ = -73001204564.4328766 ;
-sumfourth_ = 92944320862419.2188 ;
-min_ = 1.86363769529455059 ;
+sumsquare_ = 61057483.567084454 ;
+sumcube_ = -73001204564.4328918 ;
+sumfourth_ = 92944320862419.2344 ;
+min_ = 1.86363769529453394 ;
 max_ = 2256.46039029148506 ;
+agmemin_ = 22 ;
+agemax_ = 1 ;
 first_ = 1412.74238652622353 ;
-last_ = 782.354451732652365 ;
+last_ = 782.35445173265191 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -53,6 +58,8 @@
 sumfourth_ = 0 ;
 min_ = 6.05769163403522626 ;
 max_ = 6.05769163403522626 ;
+agmemin_ = nan ;
+agemax_ = nan ;
 first_ = 6.05769163403522626 ;
 last_ = 6.05769163403522626 ;
 counts = {};
@@ -70,6 +77,8 @@
 sumfourth_ = 0 ;
 min_ = 6.17811704579907683 ;
 max_ = 6.17811704579907683 ;
+agmemin_ = nan ;
+agemax_ = nan ;
 first_ = 6.17811704579907683 ;
 last_ = 6.17811704579907683 ;
 counts = {};
@@ -87,6 +96,8 @@
 sumfourth_ = 0 ;
 min_ = 6.11790433991715155 ;
 max_ = 6.11790433991715155 ;
+agmemin_ = nan ;
+agemax_ = nan ;
 first_ = 6.11790433991715155 ;
 last_ = 6.11790433991715155 ;
 counts = {};



From chapados at mail.berlios.de  Mon Aug  6 15:37:22 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 6 Aug 2007 15:37:22 +0200
Subject: [Plearn-commits] r7934 -
	trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0
Message-ID: <200708061337.l76DbMnI016397@sheep.berlios.de>

Author: chapados
Date: 2007-08-06 15:37:22 +0200 (Mon, 06 Aug 2007)
New Revision: 7934

Modified:
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
Log:
Missed one file in test result update

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:20:02 UTC (rev 7933)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-06 13:37:22 UTC (rev 7934)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 1 [ StatsCollector(
@@ -13,14 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 75 ;
 sumsquarew_ = 75 ;
-sum_ = 8.32876645342397737 ;
-sumsquare_ = 7.86682227539818513 ;
-sumcube_ = 10.9548142656995928 ;
-sumfourth_ = 17.491486713979409 ;
-min_ = 0.000403386661495011372 ;
-max_ = 1.79622562496960114 ;
-first_ = 0.013327139208753001 ;
-last_ = 0.0703378652397716186 ;
+sum_ = 8.32876645342359012 ;
+sumsquare_ = 7.8668222754000583 ;
+sumcube_ = 10.9548142657038454 ;
+sumfourth_ = 17.4914867139879391 ;
+min_ = 0.000403386661488822746 ;
+max_ = 1.79622562497000327 ;
+agmemin_ = 11 ;
+agemax_ = 8 ;
+first_ = 0.0133271392087507996 ;
+last_ = 0.0703378652394482801 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;



From yoshua at mail.berlios.de  Mon Aug  6 17:28:09 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 6 Aug 2007 17:28:09 +0200
Subject: [Plearn-commits] r7935 - trunk/commands
Message-ID: <200708061528.l76FS9CG026929@sheep.berlios.de>

Author: yoshua
Date: 2007-08-06 17:28:08 +0200 (Mon, 06 Aug 2007)
New Revision: 7935

Modified:
   trunk/commands/plearn_light_inc.h
Log:
Added a few includes in plearn_light to allow RBMs modern-style


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-08-06 13:37:22 UTC (rev 7934)
+++ trunk/commands/plearn_light_inc.h	2007-08-06 15:28:08 UTC (rev 7935)
@@ -206,6 +206,12 @@
 #include <plearn_learners/online/ModuleStackModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMModule.h>
+#include <plearn_learners/online/NetworkModule.h>
+#include <plearn_learners/online/ModuleLearner.h>
+#include <plearn_learners/online/NullModule.h>
+#include <plearn_learners/online/SplitModule.h>
+#include <plearn_learners/online/LinearCombinationModule.h>
 #include <plearn_learners/online/RBMBinomialLayer.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
 #include <plearn_learners/online/RBMConnection.h>



From yoshua at mail.berlios.de  Mon Aug  6 20:30:29 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Mon, 6 Aug 2007 20:30:29 +0200
Subject: [Plearn-commits] r7936 - trunk/plearn_learners/online
Message-ID: <200708061830.l76IUTnn022614@sheep.berlios.de>

Author: yoshua
Date: 2007-08-06 20:30:28 +0200 (Mon, 06 Aug 2007)
New Revision: 7936

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Many changes to try to make RBMModule less stupid, but alas, it is still buggy


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-06 15:28:08 UTC (rev 7935)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-06 18:30:28 UTC (rev 7936)
@@ -682,18 +682,32 @@
     PLASSERT( connection );
 
     Mat* visible = ports_value[getPortIndex("visible")];
+    bool visible_is_output = visible && visible->isEmpty();
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
+    // hidden_is_output is needed in BPROP, which is VERY BAD, VIOLATING OUR DESIGN ASSUMPTIONS
+    hidden_is_output = hidden && hidden->isEmpty();
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
+    bool hidden_act_is_output = hidden_act && hidden_act->isEmpty();
     Mat* visible_sample = ports_value[getPortIndex("visible_sample")];
+    bool visible_sample_is_output = visible_sample && visible_sample->isEmpty();
     Mat* visible_expectation = ports_value[getPortIndex("visible_expectation")];
+    bool visible_expectation_is_output = visible_expectation && visible_expectation->isEmpty();
     Mat* visible_activation = ports_value[getPortIndex("visible_activation")];
+    bool visible_activation_is_output = visible_activation && visible_activation->isEmpty();
     Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
+    bool hidden_sample_is_output = hidden_sample && hidden_sample->isEmpty();
     Mat* energy = ports_value[getPortIndex("energy")];
+    bool energy_is_output = energy && energy->isEmpty();
     Mat* neg_log_likelihood = ports_value[getPortIndex("neg_log_likelihood")];
+    bool neg_log_likelihood_is_output = neg_log_likelihood && neg_log_likelihood->isEmpty();
     Mat* neg_log_phidden = ports_value[getPortIndex("neg_log_phidden")];
+    bool neg_log_phidden_is_output = neg_log_phidden && neg_log_phidden->isEmpty();
     Mat* neg_log_pvisible_given_phidden = ports_value[getPortIndex("neg_log_pvisible_given_phidden")];
+    bool neg_log_pvisible_given_phidden_is_output = neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden->isEmpty();
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
+    //bool hidden_bias_is_output = hidden_bias && hidden_bias->isEmpty();
     weights = ports_value[getPortIndex("weights")];
+    //bool weights_is_output = weights && weights->isEmpty();
     Mat* visible_reconstruction = 0;
     Mat* visible_reconstruction_activations = 0;
     Mat* reconstruction_error = 0;
@@ -706,6 +720,9 @@
         reconstruction_error =
             ports_value[getPortIndex("reconstruction_error.state")];
     }
+    bool visible_reconstruction_is_output = visible_reconstruction && visible_reconstruction->isEmpty();
+    bool visible_reconstruction_activations_is_output = visible_reconstruction_activations && visible_reconstruction_activations->isEmpty();
+    bool reconstruction_error_is_output = reconstruction_error && reconstruction_error->isEmpty();
     Mat* contrastive_divergence = 0;
     Mat* negative_phase_visible_samples = 0;
     Mat* negative_phase_hidden_expectations = 0;
@@ -726,11 +743,16 @@
         negative_phase_hidden_activations =
             ports_value[getPortIndex("negative_phase_hidden_activations.state")];
     }
+    bool contrastive_divergence_is_output = contrastive_divergence && contrastive_divergence->isEmpty();
+    //bool negative_phase_visible_samples_is_output = negative_phase_visible_samples && negative_phase_visible_samples->isEmpty();
+    bool negative_phase_hidden_expectations_is_output = negative_phase_hidden_expectations && negative_phase_hidden_expectations->isEmpty();
+    bool negative_phase_hidden_activations_is_output = negative_phase_hidden_activations && negative_phase_hidden_activations->isEmpty();
+
     bool hidden_expectations_are_computed = false;
     hidden_activations_are_computed = false;
     bool found_a_valid_configuration = false;
 
-    if (visible && !visible->isEmpty())
+    if (visible && !visible_is_output)
     {
         // When an input is provided, that would restart the chain for
         // unconditional sampling, from that example.
@@ -742,18 +764,18 @@
     // COMPUTE ENERGY
     if (energy)
     {
-        PLASSERT_MSG( energy->isEmpty(),
+        PLASSERT_MSG( energy_is_output,
                       "RBMModule: the energy port can only be an output port\n" );
-        if (visible && !visible->isEmpty()
-            && hidden && !hidden->isEmpty())
+        if (visible && !visible_is_output
+            && hidden && !hidden_is_output)
         {
             computeEnergy(*visible, *hidden, *energy);
         }
-        else if (visible && !visible->isEmpty())
+        else if (visible && !visible_is_output)
         {
             computeFreeEnergyOfVisible(*visible,*energy);
         }
-        else if (hidden && !hidden->isEmpty())
+        else if (hidden && !hidden_is_output)
         {
             computeFreeEnergyOfHidden(*hidden,*energy);
         }
@@ -767,27 +789,27 @@
 
 
     // COMPUTE UNSUPERVISED NLL
-    if (neg_log_likelihood && neg_log_likelihood->isEmpty() && compute_log_likelihood)
+    if (neg_log_likelihood && neg_log_likelihood_is_output && compute_log_likelihood)
     {
         if (partition_function_is_stale && !during_training)
         {
             computePartitionFunction();
             partition_function_is_stale=false;
         }
-        if (visible && !visible->isEmpty()
-            && hidden && !hidden->isEmpty())
+        if (visible && !visible_is_output
+            && hidden && !hidden_is_output)
         {
             // neg-log-likelihood(visible,hidden) = energy(visible,hidden) + log(partition_function)
             computeEnergy(*visible,*hidden,*neg_log_likelihood);
             *neg_log_likelihood += log_partition_function;
         }
-        else if (visible && !visible->isEmpty())
+        else if (visible && !visible_is_output)
         {
             // neg-log-likelihood(visible) = free_energy(visible) + log(partition_function)
             computeFreeEnergyOfVisible(*visible,*neg_log_likelihood,hidden_act);
             *neg_log_likelihood += log_partition_function;
         }
-        else if (hidden && !hidden->isEmpty())
+        else if (hidden && !hidden_is_output)
         {
             // neg-log-likelihood(hidden) = free_energy(hidden) + log(partition_function)
             computeFreeEnergyOfHidden(*hidden,*neg_log_likelihood);
@@ -801,21 +823,18 @@
     // REGULAR FPROP
     // we are given the visible units and we want to compute the hidden
     // activation and/or the hidden expectation
-    if ( visible && !visible->isEmpty() &&
-         ((hidden && hidden->isEmpty() ) ||
-          (hidden_act && hidden_act->isEmpty())) )
+    if ( visible && !visible_is_output &&
+         hidden && hidden_is_output )
     {
         computePositivePhaseHiddenActivations(*visible);
-        if (hidden) {
-            PLASSERT( hidden->isEmpty() );
-            PLCHECK_MSG( !hidden_layer->expectations_are_up_to_date, "Safety "
-                    "check: how were expectations computed previously?" );
-            hidden_layer->computeExpectations();
-            hidden_expectations_are_computed=true;
-            const Mat& hidden_out = hidden_layer->getExpectations();
-            hidden->resize(hidden_out.length(), hidden_out.width());
-            *hidden << hidden_out;
-        }
+        PLCHECK_MSG( !hidden_layer->expectations_are_up_to_date, "Safety "
+                     "check: how were expectations computed previously?" );
+        hidden_layer->computeExpectations();
+        hidden_expectations_are_computed=true;
+        const Mat& hidden_out = hidden_layer->getExpectations();
+        hidden->resize(hidden_out.length(), hidden_out.width());
+        *hidden << hidden_out;
+
         // Since we return below, the other ports must be unused.
         //PLASSERT( !visible_sample && !hidden_sample );
         found_a_valid_configuration = true;
@@ -823,27 +842,27 @@
 
     // DOWNWARD FPROP
     // we are given hidden  and we want to compute the visible or visible_activation
-    if ( hidden && !hidden->isEmpty() && 
-         ((visible && visible->isEmpty()) ||
-          (visible_activation && visible_activation->isEmpty())) )
+    if ( hidden && !hidden_is_output && 
+         ((visible && visible_is_output) ||
+          (visible_activation && visible_activation_is_output)) )
     {
         computeVisibleActivations(*hidden,true);
         if (visible_activation)
         {
-            PLASSERT_MSG(visible_activation->isEmpty(),"visible_activation should be an output");
+            PLASSERT_MSG(visible_activation_is_output,"visible_activation should be an output");
             visible_activation->resize(visible_layer->activations.length(),
                                        visible_layer->size);
             *visible_activation << visible_layer->activations;
         }
         if (visible)
         {
-            PLASSERT_MSG(visible->isEmpty(),"visible should be an output");
+            PLASSERT_MSG(visible_is_output,"visible should be an output");
             visible_layer->computeExpectations();
             const Mat expectations=visible_layer->getExpectations();
             visible->resize(expectations.length(),visible_layer->size);
             *visible << expectations;
         }
-        if (hidden_act && hidden_act->isEmpty())
+        if (hidden_act && hidden_act_is_output)
         {
             // THIS IS STUPID CODE TO HANDLE THE BAD state SYSTEM AND AVOID AN UNNECESSARY ERROR MESSAGE
             // (hidden_act is a "state" port that must always be produced, even if we don't compute it!)
@@ -854,17 +873,17 @@
     }
 
     // COMPUTE AUTOASSOCIATOR RECONSTRUCTION ERROR
-    if ( visible && !visible->isEmpty() &&
-         ( ( visible_reconstruction && visible_reconstruction->isEmpty() ) ||
+    if ( visible && !visible_is_output &&
+         ( ( visible_reconstruction && visible_reconstruction_is_output ) ||
            ( visible_reconstruction_activations &&
-             visible_reconstruction_activations->isEmpty() ) ||
-           ( reconstruction_error && reconstruction_error->isEmpty() ) ) )
+             visible_reconstruction_activations_is_output ) ||
+           ( reconstruction_error && reconstruction_error_is_output ) ) )
     {
         // Autoassociator reconstruction cost
         PLASSERT( ports_value.length() == nPorts() );
 
         Mat h;
-        if (hidden && !hidden->isEmpty())
+        if (hidden && !hidden_is_output)
             h = *hidden;
         else {
             if(!hidden_expectations_are_computed)
@@ -881,7 +900,7 @@
         computeVisibleActivations(h, true);
         if(visible_reconstruction_activations)
         {
-            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            PLASSERT( visible_reconstruction_activations_is_output );
             const Mat& to_store = visible_layer->activations;
             visible_reconstruction_activations->resize(to_store.length(),
                                                        to_store.width());
@@ -892,7 +911,7 @@
             visible_layer->computeExpectations();
             if(visible_reconstruction)
             {
-                PLASSERT( visible_reconstruction->isEmpty() );
+                PLASSERT( visible_reconstruction_is_output );
                 const Mat& to_store = visible_layer->getExpectations();
                 visible_reconstruction->resize(to_store.length(),
                                                to_store.width());
@@ -900,7 +919,7 @@
             }
             if(reconstruction_error)
             {
-                PLASSERT( reconstruction_error->isEmpty() );
+                PLASSERT( reconstruction_error_is_output );
                 reconstruction_error->resize(visible->length(),1);
                 visible_layer->setBatchSize( visible->length() );
                 visible_layer->fpropNLL(*visible,
@@ -910,21 +929,21 @@
         found_a_valid_configuration = true;
     }
     // COMPUTE VISIBLE GIVEN HIDDEN
-    else if ( visible_reconstruction && visible_reconstruction->isEmpty()
-         && hidden && !hidden->isEmpty())
+    else if ( visible_reconstruction && visible_reconstruction_is_output
+         && hidden && !hidden_is_output)
     {
         // Don't need to verify if they are asked in a port, this was done previously
         computeVisibleActivations(*hidden,true);
         if(visible_reconstruction_activations)
         {
-            PLASSERT( visible_reconstruction_activations->isEmpty() );
+            PLASSERT( visible_reconstruction_activations_is_output );
             const Mat& to_store = visible_layer->activations;
             visible_reconstruction_activations->resize(to_store.length(),
                                                        to_store.width());
             *visible_reconstruction_activations << to_store;
         }
         visible_layer->computeExpectations();
-        PLASSERT( visible_reconstruction->isEmpty() );
+        PLASSERT( visible_reconstruction_is_output );
         const Mat& to_store = visible_layer->getExpectations();
         visible_reconstruction->resize(to_store.length(),
                                        to_store.width());
@@ -938,13 +957,13 @@
     // neg_log_phidden is an optional column matrix with one element -log P(h) for each 
     // row h of "hidden", used as an input port, with neg_log_pvisible_given_phidden as output. 
     // If neg_log_phidden is provided, it is assumed to be 1/n_h (n_h=h->length()).
-    if (neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden->isEmpty() &&
-        hidden && !hidden->isEmpty() && visible && !visible->isEmpty())
+    if (neg_log_pvisible_given_phidden && neg_log_pvisible_given_phidden_is_output &&
+        hidden && !hidden_is_output && visible && !visible_is_output)
     {
         // estimate P(x) by sum_h P(x|h) P(h) where P(h) is either constant or provided by neg_log_phidden
         if (neg_log_phidden)
         {
-            PLASSERT_MSG(!neg_log_phidden->isEmpty(),"If neg_log_phidden is provided, it must be an input");
+            PLASSERT_MSG(!neg_log_phidden_is_output,"If neg_log_phidden is provided, it must be an input");
             PLASSERT_MSG(neg_log_phidden->length()==hidden->length(),
                      "If neg_log_phidden is provided, it must have the same length as hidden.state");
             PLASSERT_MSG(neg_log_phidden->width()==1,"neg_log_phidden must have width 1 (single column)");
@@ -976,24 +995,24 @@
     }
 
     // SAMPLING
-    if ((visible_sample && visible_sample->isEmpty())               // is asked to sample visible units (discrete)
-        || (visible_expectation && visible_expectation->isEmpty())  //              "                   (continous)
-        || (hidden_sample && hidden_sample->isEmpty()))             // or to sample hidden units
+    if ((visible_sample && visible_sample_is_output)               // is asked to sample visible units (discrete)
+        || (visible_expectation && visible_expectation_is_output)  //              "                   (continous)
+        || (hidden_sample && hidden_sample_is_output))             // or to sample hidden units
     {
-        if (hidden_sample && !hidden_sample->isEmpty()) // sample visible conditionally on hidden
+        if (hidden_sample && !hidden_sample_is_output) // sample visible conditionally on hidden
         {
             sampleVisibleGivenHidden(*hidden_sample);
             Gibbs_step=0;
             //cout << "sampling visible from hidden" << endl;
         }
-        else if (visible_sample && !visible_sample->isEmpty()) // if an input is provided, sample hidden conditionally
+        else if (visible_sample && !visible_sample_is_output) // if an input is provided, sample hidden conditionally
         {
             sampleHiddenGivenVisible(*visible_sample);
             hidden_activations_are_computed = false;
             Gibbs_step=0;
             //cout << "sampling hidden from visible" << endl;
         }
-        else if (visible_expectation && !visible_expectation->isEmpty())
+        else if (visible_expectation && !visible_expectation_is_output)
         {
              PLERROR("In RBMModule::fprop visible_expectation can only be an output port (use visible as input port");
         }
@@ -1003,7 +1022,7 @@
             // start or continue the chain
             if (visible_layer->samples.isEmpty())
             {
-                if (visible && !visible->isEmpty())
+                if (visible && !visible_is_output)
                     visible_layer->samples << *visible;
                 else if (!visible_layer->getExpectations().isEmpty())
                     visible_layer->samples << visible_layer->getExpectations();
@@ -1024,38 +1043,38 @@
             //cout << " -> " << Gibbs_step << endl;
         }
 
-        if ( hidden && hidden->isEmpty())   // fill hidden.state with expectations
+        if ( hidden && hidden_is_output)   // fill hidden.state with expectations
         {
               const Mat& hidden_expect = hidden_layer->getExpectations();
               hidden->resize(hidden_expect.length(), hidden_expect.width());
               *hidden << hidden_expect;
         }
-        if (visible_sample && visible_sample->isEmpty()) // provide sample of the visible units
+        if (visible_sample && visible_sample_is_output) // provide sample of the visible units
         {
             visible_sample->resize(visible_layer->samples.length(),
                                    visible_layer->samples.width());
             *visible_sample << visible_layer->samples;
         }
-        if (hidden_sample && hidden_sample->isEmpty()) // provide sample of the hidden units
+        if (hidden_sample && hidden_sample_is_output) // provide sample of the hidden units
         {
             hidden_sample->resize(hidden_layer->samples.length(),
                                   hidden_layer->samples.width());
             *hidden_sample << hidden_layer->samples;
         }
-        if (visible_expectation && visible_expectation->isEmpty()) // provide expectation of the visible units
+        if (visible_expectation && visible_expectation_is_output) // provide expectation of the visible units
         {
             const Mat& to_store = visible_layer->getExpectations();
             visible_expectation->resize(to_store.length(),
                                         to_store.width());
             *visible_expectation << to_store;
         }
-        if (hidden && hidden->isEmpty())
+        if (hidden && hidden_is_output)
         {
             hidden->resize(hidden_layer->samples.length(),
                            hidden_layer->samples.width());
             *hidden << hidden_layer->samples;
         }
-        if (hidden_act && hidden_act->isEmpty())
+        if (hidden_act && hidden_act_is_output)
         {
             hidden_act->resize(hidden_layer->samples.length(),
                                hidden_layer->samples.width());
@@ -1067,9 +1086,9 @@
     // COMPUTE CONTRASTIVE DIVERGENCE CRITERION
     if (contrastive_divergence)
     {
-        PLASSERT_MSG( contrastive_divergence->isEmpty(),
+        PLASSERT_MSG( contrastive_divergence_is_output,
                       "RBMModule: the contrastive_divergence port can only be an output port\n" );
-        if (visible && !visible->isEmpty())
+        if (visible && !visible_is_output)
         {
             int mbs = visible->length();
             const Mat& hidden_expectations = hidden_layer->getExpectations();
@@ -1120,9 +1139,9 @@
             }
             PLASSERT(negative_phase_visible_samples);
             PLASSERT(negative_phase_hidden_expectations &&
-                     negative_phase_hidden_expectations->isEmpty());
+                     negative_phase_hidden_expectations_is_output);
             PLASSERT(negative_phase_hidden_activations &&
-                     negative_phase_hidden_activations->isEmpty());
+                     negative_phase_hidden_activations_is_output);
             negative_phase_visible_samples->resize(mbs,visible_layer->size);
             *negative_phase_visible_samples << visible_layer->samples;
             negative_phase_hidden_expectations->resize(hidden_expectations.length(),
@@ -1133,10 +1152,6 @@
                                                       neg_hidden_act.width());
             *negative_phase_hidden_activations << neg_hidden_act;
 
-            // note that h_act and h may point to hidden_act_store and hidden_exp_store
-            PLASSERT(h_act && !h_act->isEmpty());
-            PLASSERT(h && !h->isEmpty());
-
             contrastive_divergence->resize(hidden_expectations.length(),1);
             // compute contrastive divergence itself
             for (int i=0;i<mbs;i++)
@@ -1156,6 +1171,25 @@
         found_a_valid_configuration = true;
     }
 
+    // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
+    // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)
+    if (hidden_act && hidden_act->isEmpty())
+        hidden_act->resize(1,1);
+    if (hidden && hidden->isEmpty())
+        hidden->resize(1,1);
+    if (visible_reconstruction && visible_reconstruction->isEmpty())
+        visible_reconstruction->resize(1,1);
+    if (visible_reconstruction_activations && visible_reconstruction_activations->isEmpty())
+        visible_reconstruction_activations->resize(1,1);
+    if (reconstruction_error && reconstruction_error->isEmpty())
+        reconstruction_error->resize(1,1);
+    if (negative_phase_visible_samples && negative_phase_visible_samples->isEmpty())
+        negative_phase_visible_samples->resize(1,1);
+    if (negative_phase_hidden_expectations && negative_phase_hidden_expectations->isEmpty())
+        negative_phase_hidden_expectations->resize(1,1);
+    if (negative_phase_hidden_activations && negative_phase_hidden_activations->isEmpty())
+        negative_phase_hidden_activations->resize(1,1);
+    
     // Reset some class fields to ensure they are not reused by mistake.
     hidden_act = NULL;
     hidden_bias = NULL;
@@ -1163,7 +1197,6 @@
     hidden_activations_are_computed = false;
 
 
-
     if (!found_a_valid_configuration)
     {
         /*
@@ -1636,38 +1669,49 @@
         reconstruction_connection->bpropAccUpdate( rec_ports_value,
                                                    rec_ports_gradient );
 
-        // Hidden layer bias update
-        hidden_layer->bpropUpdate(*hidden_act,
-                                  *hidden, hidden_act_grad,
-                                  hidden_exp_grad, false);
-        if (hidden_bias_grad)
+        // UGLY HACK WHICH BREAKS THE RULE THAT RBMMODULE CAN BE CALLED IN DIFFERENT CONTEXTS AND fprop/bprop ORDERS
+        // BUT NECESSARY WHEN hidden WAS AN INPUT
+        if (hidden_is_output) 
         {
-            if (hidden_bias_grad->isEmpty()) {
-                PLASSERT( hidden_bias_grad->width() == hidden_layer->size );
-                hidden_bias_grad->resize(mbs,hidden_layer->size);
+            // Hidden layer bias update
+            hidden_layer->bpropUpdate(*hidden_act,
+                                      *hidden, hidden_act_grad,
+                                      hidden_exp_grad, false);
+            if (hidden_bias_grad)
+            {
+                if (hidden_bias_grad->isEmpty()) {
+                    PLASSERT( hidden_bias_grad->width() == hidden_layer->size );
+                    hidden_bias_grad->resize(mbs,hidden_layer->size);
+                }
+                *hidden_bias_grad += hidden_act_grad;
             }
-            *hidden_bias_grad += hidden_act_grad;
+            // Connection update
+            if(compute_visible_grad)
+            {
+                // The length of 'visible_grad' must be either 0 (if not computed
+                // previously) or the size of the mini-batches (otherwise).
+                PLASSERT( visible_grad->width() == visible_layer->size &&
+                          visible_grad->length() == 0 ||
+                          visible_grad->length() == mbs );
+                visible_grad->resize(mbs, visible_grad->width());
+                connection->bpropUpdate(
+                    *visible, *hidden_act,
+                    *visible_grad, hidden_act_grad, true);
+            }
+            else
+            {
+                visible_exp_grad.resize(mbs,visible_layer->size);
+                connection->bpropUpdate(
+                    *visible, *hidden_act,
+                    visible_exp_grad, hidden_act_grad, true);
+            }
         }
-        // Connection update
-        if(compute_visible_grad)
+        else if (hidden_grad && hidden_grad->isEmpty()) // copy the hidden gradient
         {
-            // The length of 'visible_grad' must be either 0 (if not computed
-            // previously) or the size of the mini-batches (otherwise).
-            PLASSERT( visible_grad->width() == visible_layer->size &&
-                      visible_grad->length() == 0 ||
-                      visible_grad->length() == mbs );
-            visible_grad->resize(mbs, visible_grad->width());
-            connection->bpropUpdate(
-                *visible, *hidden_act,
-                *visible_grad, hidden_act_grad, true);
+            hidden_grad->resize(mbs,hidden_layer->size);
+            *hidden_grad << hidden_exp_grad;
         }
-        else
-        {
-            visible_exp_grad.resize(mbs,visible_layer->size);
-            connection->bpropUpdate(
-                *visible, *hidden_act,
-                visible_exp_grad, hidden_act_grad, true);
-        }
+
         partition_function_is_stale = true;
     }
 

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-06 15:28:08 UTC (rev 7935)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-06 18:30:28 UTC (rev 7936)
@@ -245,6 +245,8 @@
     Mat* hidden_act;
     bool hidden_activations_are_computed;
 
+    bool hidden_is_output;
+
     //! Used to store the contrastive divergence gradient w.r.t. weights.
     Mat store_weights_grad;
 



From saintmlx at mail.berlios.de  Tue Aug  7 02:25:24 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 7 Aug 2007 02:25:24 +0200
Subject: [Plearn-commits] r7937 - in trunk/python_modules/plearn: plide
	pyplearn
Message-ID: <200708070025.l770PO6i029629@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-07 02:25:22 +0200 (Tue, 07 Aug 2007)
New Revision: 7937

Modified:
   trunk/python_modules/plearn/plide/plide_options.py
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
- plnamespace/plopt gui: avoid setting unchanged options



Modified: trunk/python_modules/plearn/plide/plide_options.py
===================================================================
--- trunk/python_modules/plearn/plide/plide_options.py	2007-08-06 18:30:28 UTC (rev 7936)
+++ trunk/python_modules/plearn/plide/plide_options.py	2007-08-07 00:25:22 UTC (rev 7937)
@@ -353,7 +353,9 @@
         """
         for (widget_getter, group, option_name) in self.widget_map:
             value = widget_getter()
-            if value!='None':
+            opt= group.get_plopt(option_name)
+            dftval= opt.getDefault()
+            if value!='None' and opt.cast(value) != dftval:
                 group.set(option_name, value)
 
         ## Updates from the first page of the dialog

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-08-06 18:30:28 UTC (rev 7936)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-08-07 00:25:22 UTC (rev 7937)
@@ -361,7 +361,8 @@
         elif self._type is list :
             elem_type = self._kwargs.pop("elem_type", None) 
             if elem_type is None:
-                if len(self.get()) > 0:
+                got= self.get()
+                if got!=None and len(got) > 0:
                     elem_type = type(self.get()[0])
                 else:
                     elem_type = str



From saintmlx at mail.berlios.de  Tue Aug  7 02:50:46 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 7 Aug 2007 02:50:46 +0200
Subject: [Plearn-commits] r7938 - trunk/plearn/python
Message-ID: <200708070050.l770okat030687@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-07 02:50:45 +0200 (Tue, 07 Aug 2007)
New Revision: 7938

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- allow conversion of enums from plearn to python



Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-08-07 00:25:22 UTC (rev 7937)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-08-07 00:50:45 UTC (rev 7938)
@@ -461,12 +461,7 @@
 template<typename T>
 struct ConvertToPyObject
 {
-    static PyObject* newPyObject(const T& x)
-    {
-        PLERROR("Cannot convert type %s by value to python.",
-                TypeTraits<T>::name().c_str());
-        return 0;//shut up compiler
-    }
+    static PyObject* newPyObject(const T& x);
 };
 
 // Specialization for Object*
@@ -1086,9 +1081,36 @@
 
 
 //#####  newPyObject Implementations  #########################################
+template<typename T, bool is_enum>
+struct StaticConvertEnumToPyObject
+{
+    static PyObject* newPyObject(const T& x)
+    {
+        PLERROR("Cannot convert type %s by value to python.",
+                TypeTraits<T>::name().c_str());
+        return 0;//shut up compiler
+    }
+};
 
+template<typename T>
+struct StaticConvertEnumToPyObject<T, true>
+{
+    static PyObject* newPyObject(const T& x)
+    {
+        return ConvertToPyObject<int>::newPyObject(x);
+    }
+};
+
+template<typename T>
+PyObject* ConvertToPyObject<T>::newPyObject(const T& x)
+{
+    return StaticConvertEnumToPyObject<T, boost::is_enum<T>::value>
+        ::newPyObject(x);
+}
+
+
 template<size_t N>
-PyObject*  ConvertToPyObject<char[N]>::newPyObject(const char x[N])
+PyObject* ConvertToPyObject<char[N]>::newPyObject(const char x[N])
 {
     return ConvertToPyObject<char*>::newPyObject(x);
 }



From chrish at mail.berlios.de  Tue Aug  7 18:01:28 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 7 Aug 2007 18:01:28 +0200
Subject: [Plearn-commits] r7939 - trunk/python_modules/plearn/pyplearn
Message-ID: <200708071601.l77G1SPN017174@sheep.berlios.de>

Author: chrish
Date: 2007-08-07 18:01:28 +0200 (Tue, 07 Aug 2007)
New Revision: 7939

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Add basic support for dict to plopts.

Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-08-07 00:50:45 UTC (rev 7938)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-08-07 16:01:28 UTC (rev 7939)
@@ -368,6 +368,12 @@
                     elem_type = str
             casted = list_cast(value, elem_type)
 
+        elif self._type is dict:
+            if isinstance(value, str):
+                casted = eval(value)
+            else:
+                casted = value
+            
         # Simple type cast
         else:
             casted = self._type(value)



From nouiz at mail.berlios.de  Tue Aug  7 19:22:27 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 19:22:27 +0200
Subject: [Plearn-commits] r7940 - trunk/scripts
Message-ID: <200708071722.l77HMRoG004837@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 19:22:26 +0200 (Tue, 07 Aug 2007)
New Revision: 7940

Modified:
   trunk/scripts/cdispatch
Log:
Added options --32,--64--3264. Modified so that if we are not in test mode, we launch the job directly


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-08-07 16:01:28 UTC (rev 7939)
+++ trunk/scripts/cdispatch	2007-08-07 17:22:26 UTC (rev 7940)
@@ -1,8 +1,8 @@
 #!/usr/bin/env python
-import sys,os,re
+import sys,os,re,time
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>]'
+ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>] [--32|--64|--3264]'
 LongHelp="""
 Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
 
@@ -59,6 +59,7 @@
 otherargs = []
 FILE = ""
 REQ = ""
+ARCH = ""
 for argv in sys.argv[1:]:
 
     if argv == "--help" or argv == "-h":
@@ -77,6 +78,9 @@
     elif argv[0:7] == "--file=":
         FILE = argv[7:]
         optionargs.append(argv[2:])
+    elif argv == "--32"  or argv == "--64" or argv == "--3264":
+        ARCH=argv[2:]
+        optionargs.append(argv[2:])
     elif argv[0:6] == "--req=":
         REQ = argv[6:]
     elif argv[0:1] == '-':
@@ -95,14 +99,6 @@
     print "--cluster and --local can't be used together"
     sys.exit(1)
 
-
-SCRIPT=open(ScriptName,'w');
-SCRIPT.write(
-"""#! /usr/bin/env python
-#%s
-from plearn.parallel.dbi import DBI
-jobs = DBI([
-"""% " ".join(sys.argv))
 def generate_combination(repl):
     if repl == []:
         return []
@@ -118,7 +114,7 @@
                     res.append(y+" "+r)
         return res
 
-def expendAndPrintArgs(sp):
+def generate_commands(sp):
 ### Find replacement lists in the arguments
     repl = []
     for arg in sp:
@@ -138,56 +134,70 @@
             repl.append([arg])
 #    print "repl: ",repl
     argscombination = generate_combination(repl)
-    for arg in argscombination:
-        cmdstr = "".join(arg);
-	SCRIPT.write("   '%s',\n"%cmdstr)
-    return len(argscombination)
+    return argscombination
 
-nbcommand=0
-#print the command
+#generate the command
 if FILE != "":
     FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
     for line in FD.readlines():
         line = line.rstrip()
 	sp = line.split(" ")
-	nbcommand+=expendAndPrintArgs(sp)
+        commands=generate_commands(sp)
     FD.close
 else:
-    nbcommand=expendAndPrintArgs(otherargs)
+    commands=generate_commands(otherargs)
 
-
 if "cluster" in optionargs:
-    SCRIPT.write("   ],'Cluster'")
+    launch_cmd='Cluster'
 elif "local" in optionargs:
-    SCRIPT.write("   ],'Local'")
+    launch_cmd='Local'
 else:
-    SCRIPT.write("   ],'Condor'")
+    launch_cmd='Condor'
 
-
+dbi_param={}
 if REQ != "":
-    SCRIPT.write(", requirements=\"$REQ\"")
-
+    dbi_param["requirements"]="\"%s\""%REQ
 if "test" in optionargs:
-    SCRIPT.write(", test=True")
-
+    dbi_param["test"]=True
 if "log" in optionargs:
-    SCRIPT.write(", dolog=True")
+    dbi_param["dolog"]=True
 else:
-    SCRIPT.write(", dolog=False")
-
-SCRIPT.write( """)
+    dbi_param["dolog"]=False
+if ARCH!="":
+    dbi_param["arch"]=ARCH
+    
+if "test" in optionargs:
+    print "We generated %s command in the file"% len(commands)
+    print "The script %s was not launched"% ScriptName
+    SCRIPT=open(ScriptName,'w');
+    SCRIPT.write(
+"""#! /usr/bin/env python
+#%s
+from plearn.parallel.dbi import DBI
+jobs = DBI([
+"""% " ".join(sys.argv))
+    for arg in commands:
+        cmdstr = "".join(arg);
+        SCRIPT.write("   '%s',\n"%cmdstr)
+    SCRIPT.write("   ],'%s'"%(launch_cmd))
+    for key in dbi_param.keys():
+        SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
+    SCRIPT.write(
+""")
 jobs.run()
-# There is %d command in the script"""%(nbcommand))
+# There is %d command in the script"""%(len(commands)))
+        
+    SCRIPT.close()
+    os.system("chmod +x %s"%(ScriptName));
 
-SCRIPT.close()
-os.system("chmod +x %s"%(ScriptName));
-
-print "We generated %s command in the file"% nbcommand
-
-if "test" in optionargs:
-    print "The script %s was not launched"% ScriptName
 else:
-    print "Launching the script %s"% ScriptName
-    os.system("./%s"%(ScriptName))
+    print "We generate the DBI object with %s command"%(len(commands))
+    from plearn.parallel.dbi import *
+    t1=time.time()
+    jobs = DBI(commands,launch_cmd,**dbi_param)
+    t2=time.time()
+    print "it took %f s to create the DBI objects"%(t2-t1)
+    jobs.run()
+    t3=time.time()
+    print "it took %f s to launch all the commands"%(t3-t2)
 
-



From nouiz at mail.berlios.de  Tue Aug  7 19:46:24 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 19:46:24 +0200
Subject: [Plearn-commits] r7941 - trunk/python_modules/plearn/parallel
Message-ID: <200708071746.l77HkODn009010@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 19:46:24 +0200 (Tue, 07 Aug 2007)
New Revision: 7941

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-new try for condor with all computer (32 and 64 bits)
-cluster with 32,64 or both computer
-redirect stdout, stderr
-new fct add_command()
-new fct exec_pre_batch()
-new fct exec_post_batch()


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 17:22:26 UTC (rev 7940)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 17:46:24 UTC (rev 7941)
@@ -48,28 +48,29 @@
         self.post_batch = []
 
         # the default directory where to keep all the log files
-        self.log_dir = 'LOGS'
-        self.log_file = os.path.join( self.log_dir, self.unique_id )
+        self.log_dir = os.path.join( 'LOGS', self.unique_id )
+        self.log_file = os.path.join( self.log_dir, 'log' )
 
         # the default directory where file generated by dbi will be stored
         # It should not take the "" or " " value. Use "." instead.
         self.tmp_dir = 'TMP_DBI'
-
         #
-        self.file_redirect_stdout = False
-        self.file_redirect_stderr = False
+        self.file_redirect_stdout = True
+        self.file_redirect_stderr = True
 
         # Initialize the namespace
         self.requirements = ''
         self.test = False
         self.dolog = False
         self.temp_files = []
+        self.arch = 0 # TODO, we should put the local arch: 32,64 or 3264 bits
         for key in args.keys():
             self.__dict__[key] = args[key]
 
         # check if log directory exists, if not create it
-        if self.dolog and not os.path.exists(self.log_dir):
-            os.mkdir(self.log_dir)
+#        if (not os.path.exists(self.log_dir)):
+#            if self.dolog or self.file_redirect_stdout or self.file_redirect_stderr:
+        os.mkdir(self.log_dir)
 
         # If some arguments aren't lists, put them in a list
         if not isinstance(commands, list):
@@ -84,7 +85,35 @@
             self.post_batch = [self.post_batch]
 
     def n_avail_machines(self): raise NotImplementedError, "DBIBase.n_avail_machines()"
-
+    def add_command(self,command): raise NotImplementedError, "DBIBase.add_command()"
+    def exec_pre_batch(self):
+        # Execute pre-batch
+        output = PIPE
+        error = PIPE
+        pre_batch_command = ';'.join( self.pre_batch )
+        if int(self.file_redirect_stdout):
+            output = file(self.log_file + '.pre_batch.out', 'w')
+        if int(self.file_redirect_stderr):
+            error = file(self.log_file + '.pre_batch.err', 'w')
+        if not self.test:
+            self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
+        else:
+            print "[DBI] pre_batch_command:",pre_batch_command
+            
+    def exec_post_batch(self):
+        # Execute post-batch
+        output = PIPE
+        error = PIPE
+        post_batch_command = ';'.join( self.post_batch )
+        if int(self.file_redirect_stdout):
+            output = file(self.log_file + '.post_batch.out', 'w')
+        if int(self.file_redirect_stderr):
+            error = file(self.log_file + '.post_batch.err', 'w')
+        if not self.test:
+            self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
+        else:
+            print "[DBI] post_batch_command:",post_batch_command
+            
     def clean(self):
         pass
 
@@ -208,8 +237,16 @@
 
     def run_one_job(self, task):
         DBIBase.run(self)
-
-        command = "cluster --execute '" + string.join(task.commands,';') + "'"
+        
+        command = "cluster --execute" 
+        if self.arch == 32:
+            command += "--typecpu 32bits"
+        elif self.arch == 64:
+            command += "--typecpu 64bits"
+        if self.arch==3264:
+            command += "--typecpu all"
+        command += " '"+string.join(task.commands,';') + "'"
+        
         print command
 
         task.launch_time = time.time()
@@ -228,33 +265,18 @@
         if self.test:
             print "Test mode, we only print the command to be executed, we don't execute them"
         # Execute pre-batch
-        pre_batch_command = ';'.join( self.pre_batch )
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.pre_batch.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.pre_batch.err', 'w')
-        if self.test:
-            print pre_batch_command
-        else:
-            self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
+        if len(self.pre_batch)>0:
+            exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         for task in self.tasks:
             self.run_one_job(task)
 
         # Execute post-batchs
-        post_batch_command = ";".join( self.post_batch );
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.post_batch.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.post_batch.err', 'w')
-        if self.test:
-            print post_batch_command
-        else:
-            self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
-            
+        if len(self.post_batch)>0:
+            exec_post_batch()
+
+        print "The Log file are under %s"%self.log_dir
     def clean(self):
         #TODO: delete all log files for the current batch
         pass
@@ -397,12 +419,13 @@
                 self.targetcondorplatform='BOTH'
                 self.targetplatform='linux-i386'
                 #newcommand=c+".32"+c2
-                newcommand='if [ $1 == "INTEL" ]; then\n'
-                newcommand+='  '+c+'.32'+c2+'\n'
-                newcommand+='else\n'
-                newcommand+='  '+c+".64"+c2+'\nfi'
+                newcommand='if [ $CPUTYPE == \'x86_64\' ]; then'
+                newcommand+='  '+c+'.64'+c2
+                newcommand+='; else '
+                newcommand+=c+".32"+c2+'; fi'
                 if not os.access(c+".64", os.X_OK):
                     raise Exception("The command '"+c+".64' do not have execution permission!")
+#                newcommand=command
                 c+=".32"
             elif self.cplat=="INTEL" and os.path.exists(c+".32"):
                 self.targetcondorplatform='INTEL'
@@ -438,7 +461,10 @@
             return #no task to run
         # create the bqsubmit.dat, with
         condor_datas = []
-        if len(self.tasks)>1:
+
+        #we supose that each task in tasks have the same number of commands
+        #it should be true.
+        if len(self.tasks[0].commands)>1:
             for task in self.tasks:
                 condor_data = os.path.join(self.tmp_dir,self.unique_id +'.'+ task.unique_id + '.data')
                 condor_datas.append(condor_data)
@@ -483,7 +509,7 @@
 #                preBatch = ''' + pre_batch_command + '''
 #                postBatch = ''' + post_batch_command +'''
 
-        if len(condor_datas)==0:
+        if len(condor_datas)!=0:
             for i in condor_datas:
                 condor_dat.write("arguments      = sh "+i+" $$(Arch) \nqueue\n")
         else:
@@ -567,7 +593,11 @@
 
 
     def run(self):
+        if len(self.pre_batch)>0):
+            exec_pre_batch()
         self.run_all_job()
+        if len(self.post_batch)>0):
+            exec_post_batch()
 
 
 
@@ -581,33 +611,36 @@
         DBIBase.__init__(self, commands, **args)
 
         for command in commands:
-            pos = string.find(command,' ')
-            if pos>=0:
-                c = command[0:pos]
-                c2 = command[pos:]
-            else:
-                c=command
-                c2=""
-
-        # We use the absolute path so that we don't have corner case as with ./ 
-            c = os.path.normpath(os.path.join(os.getcwd(), c))
-            command = c + c2
+            addjobs(command)
             
-            # We will execute the command on the specified architecture
-            # if it is specified. If the executable exist for both
-            # architecture we execute on both. Otherwise we execute on the
-            # same architecture as the architecture of the launch computer
+    def add_command(self,command):
+        pos = string.find(command,' ')
+        if pos>=0:
+            c = command[0:pos]
+            c2 = command[pos:]
+        else:
+            c=command
+            c2=""
             
-            if not os.path.exists(c):
-                raise Exception("The command '"+c+"' do not exist!")
-            elif not os.access(c, os.X_OK):
-                raise Exception("The command '"+c+"' do not have execution permission!")
-            self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
-                                   self.time_format, self.pre_tasks,
-                                   self.post_tasks,self.dolog,args))
+        # We use the absolute path so that we don't have corner case as with ./ 
+        c = os.path.normpath(os.path.join(os.getcwd(), c))
+        command = c + c2
+        
+        # We will execute the command on the specified architecture
+        # if it is specified. If the executable exist for both
+        # architecture we execute on both. Otherwise we execute on the
+        # same architecture as the architecture of the launch computer
+        
+        if not os.path.exists(c):
+            raise Exception("The command '"+c+"' do not exist!")
+        elif not os.access(c, os.X_OK):
+            raise Exception("The command '"+c+"' do not have execution permission!")
+        self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
+                               self.time_format, self.pre_tasks,
+                               self.post_tasks,self.dolog,args))
+        
+        #keeps a list of the temporary files created, so that they can be deleted at will            
 
-            #keeps a list of the temporary files created, so that they can be deleted at will            
-
     def run_one_job(self,task):
         launch_file = os.path.join(self.tmp_dir, 'launch.sh')
 
@@ -623,7 +656,6 @@
         if self.test == False:
             print c
             os.system(c)
-
         else:
             print c
             
@@ -641,9 +673,24 @@
     def run(self):
         if self.test:
             print "Test mode, we only print the command to be executed, we don't execute them"
+        for (task,ind) in zip(self.tasks,range(len(self.tasks))):
+            print "Will execute command %d/%d"%(ind+1,len(self.tasks))
+            self.run_one_job(task)
+
+        # Execute pre-batch
+        output = PIPE
+        error = PIPE
+        if len(self.pre_batch)>0:
+            exec_pre_batch()
+
+        # Execute all Tasks (including pre_tasks and post_tasks if any)
         for task in self.tasks:
             self.run_one_job(task)
 
+        # Execute post-batchs
+        if len(self.post_batch)>0:
+            exec_post_batch()
+            
     def clean(self):
         pass
 
@@ -742,15 +789,8 @@
 
     def run(self):
         # Execute pre-batch
-        pre_batch_command = ';'.join( self.pre_batch )
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.pre_batch.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.pre_batch.err', 'w')
-        self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
-        print 'pre_batch_command =', pre_batch_command
+        if len(self.pre_batch)>0:
+            exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         print "tasks= ", self.tasks
@@ -758,13 +798,8 @@
             self.run_one_job(task)
 
         # Execute post-batchs
-        post_batch_command = ";".join( self.post_batch );
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.post_batch.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.post_batch.err', 'w')
-        self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
-        print 'post_batch_command =', post_batch_command
+        if len(self.post_batch)>0:
+            exec_post_batch()
 
     def clean(self):
         #TODO: delete all log files for the current batch



From nouiz at mail.berlios.de  Tue Aug  7 21:19:58 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 21:19:58 +0200
Subject: [Plearn-commits] r7942 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200708071919.l77JJwcl015135@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 21:19:58 +0200 (Tue, 07 Aug 2007)
New Revision: 7942

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
bugfix and added multithread in the local version


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 17:46:24 UTC (rev 7941)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 19:19:58 UTC (rev 7942)
@@ -12,6 +12,7 @@
 from configobj import ConfigObj
 from textwrap import dedent
 import pdb
+from threading import Thread,BoundedSemaphore
 from time import sleep
 #from plearn.pymake import pymake
 
@@ -593,10 +594,10 @@
 
 
     def run(self):
-        if len(self.pre_batch)>0):
+        if len(self.pre_batch)>0:
             exec_pre_batch()
         self.run_all_job()
-        if len(self.post_batch)>0):
+        if len(self.post_batch)>0:
             exec_post_batch()
 
 
@@ -609,9 +610,11 @@
 
     def __init__( self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
-
+        self.args=args
+        self.threads=[]
+        self.nb_proc=1
         for command in commands:
-            addjobs(command)
+            self.add_command(command)
             
     def add_command(self,command):
         pos = string.find(command,' ')
@@ -637,7 +640,7 @@
             raise Exception("The command '"+c+"' do not have execution permission!")
         self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                self.time_format, self.pre_tasks,
-                               self.post_tasks,self.dolog,args))
+                               self.post_tasks,self.dolog,self.args))
         
         #keeps a list of the temporary files created, so that they can be deleted at will            
 
@@ -647,6 +650,11 @@
         # Launch condor
         output = PIPE
         error = PIPE
+        if not self.file_redirect_stdout and self.nb_proc>1:
+            print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
+        if not self.file_redirect_stderr and self.nb_proc>1:
+            print "[DBI] WARNING: many process but all their stderr are redirected to the parent"
+            
         if int(self.file_redirect_stdout):
             output = file(self.log_file + '.out', 'w')
         if int(self.file_redirect_stderr):
@@ -654,8 +662,11 @@
 
         c = (';'.join(task.commands))
         if self.test == False:
+            self.sema.acquire()
             print c
-            os.system(c)
+            p = Popen(c, shell=True,stdout=output,stderr=error)
+            p.wait()
+            self.sema.release()
         else:
             print c
             
@@ -669,24 +680,26 @@
                 pass
             pass    
 
-
     def run(self):
         if self.test:
             print "Test mode, we only print the command to be executed, we don't execute them"
-        for (task,ind) in zip(self.tasks,range(len(self.tasks))):
-            print "Will execute command %d/%d"%(ind+1,len(self.tasks))
-            self.run_one_job(task)
 
         # Execute pre-batch
         output = PIPE
         error = PIPE
         if len(self.pre_batch)>0:
             exec_pre_batch()
-
+        
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        for task in self.tasks:
-            self.run_one_job(task)
+        self.sema=BoundedSemaphore(int(self.nb_proc))
+        for (task,ind) in zip(self.tasks,range(len(self.tasks))):
+            t=Thread(target=self.run_one_job,args=(task,))
+            t.start()
+            self.threads.append(t)
 
+        for t in self.threads:
+            t.join()
+
         # Execute post-batchs
         if len(self.post_batch)>0:
             exec_post_batch()

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-08-07 17:46:24 UTC (rev 7941)
+++ trunk/scripts/cdispatch	2007-08-07 19:19:58 UTC (rev 7942)
@@ -2,7 +2,7 @@
 import sys,os,re,time
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>] [--32|--64|--3264]'
+ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local[=nb_process]] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>] [--32|--64|--3264]'
 LongHelp="""
 Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
 
@@ -60,6 +60,7 @@
 FILE = ""
 REQ = ""
 ARCH = ""
+NB_PROC=None
 for argv in sys.argv[1:]:
 
     if argv == "--help" or argv == "-h":
@@ -69,10 +70,13 @@
         optionargs.append(argv[2:])
     elif argv == "--log":
         optionargs.append(argv[2:])
-    elif argv == "--cluster" and not "local" in optionargs:
+    elif argv == "--cluster":
         optionargs.append(argv[2:])
-    elif argv == "--local" and not "cluster" in optionargs:
-        optionargs.append(argv[2:])
+    elif argv[0:7] == "--local":
+        optionargs.append(argv[2:7])
+        if len(argv)>7:
+            assert(argv[7]=="=")
+            NB_PROC=argv[8:]
     elif argv == "--test":
         optionargs.append(argv[2:])
     elif argv[0:7] == "--file=":
@@ -165,7 +169,9 @@
     dbi_param["dolog"]=False
 if ARCH!="":
     dbi_param["arch"]=ARCH
-    
+if NB_PROC:
+    dbi_param["nb_proc"]=NB_PROC
+
 if "test" in optionargs:
     print "We generated %s command in the file"% len(commands)
     print "The script %s was not launched"% ScriptName



From nouiz at mail.berlios.de  Tue Aug  7 21:32:38 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 21:32:38 +0200
Subject: [Plearn-commits] r7943 - trunk/plearn_learners/regressors
Message-ID: <200708071932.l77JWcu5015852@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 21:32:38 +0200 (Tue, 07 Aug 2007)
New Revision: 7943

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
Log:
removed compiler warning


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2007-08-07 19:19:58 UTC (rev 7942)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2007-08-07 19:32:38 UTC (rev 7943)
@@ -321,14 +321,15 @@
 {
     root->computeOutput(inputv, outputv);
     if (multiclass_outputs.length() <= 0) return;
-    real closest_value;
-    real margin_to_closest_value;
-    for (int value_ind = 0; value_ind < multiclass_outputs.length(); value_ind++)
+    real closest_value=multiclass_outputs[0];
+    real margin_to_closest_value=abs(outputv[0] - multiclass_outputs[0]);
+    for (int value_ind = 1; value_ind < multiclass_outputs.length(); value_ind++)
     {
-        if (value_ind == 0 || abs(outputv[0] - multiclass_outputs[value_ind]) < margin_to_closest_value)
+        real v=abs(outputv[0] - multiclass_outputs[value_ind]);
+        if (v < margin_to_closest_value)
         {
             closest_value = multiclass_outputs[value_ind];
-            margin_to_closest_value = abs(outputv[0] - multiclass_outputs[value_ind]);
+            margin_to_closest_value = v;
         }
     }
     outputv[0] = closest_value;



From chrish at mail.berlios.de  Tue Aug  7 21:40:48 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 7 Aug 2007 21:40:48 +0200
Subject: [Plearn-commits] r7944 - trunk/python_modules/plearn/plide
Message-ID: <200708071940.l77JemKM016229@sheep.berlios.de>

Author: chrish
Date: 2007-08-07 21:40:48 +0200 (Tue, 07 Aug 2007)
New Revision: 7944

Modified:
   trunk/python_modules/plearn/plide/plide.py
Log:
Remove spurious global declarations to silence SyntaxWarning.


Modified: trunk/python_modules/plearn/plide/plide.py
===================================================================
--- trunk/python_modules/plearn/plide/plide.py	2007-08-07 19:32:38 UTC (rev 7943)
+++ trunk/python_modules/plearn/plide/plide.py	2007-08-07 19:40:48 UTC (rev 7944)
@@ -844,7 +844,6 @@
 
 #####  C++ Functional Interface  ############################################
 
-#global plide_main_window
 plide_main_window = None
 
 def StartPlide(argv = [], streams_to_watch= {}):
@@ -944,8 +943,6 @@
     #class Poubelle:
     #    def __getattr__(self, attr): return None
 
-    global plide_main_window
-    global injected
     injected = Poubelle()
     
     StartPlide(streams_to_watch= {injected.errstm.fileno(): 'injected-stderr'})



From nouiz at mail.berlios.de  Tue Aug  7 21:52:10 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 21:52:10 +0200
Subject: [Plearn-commits] r7945 - trunk/python_modules/plearn/parallel
Message-ID: <200708071952.l77JqATm016995@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 21:52:10 +0200 (Tue, 07 Aug 2007)
New Revision: 7945

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Add dbi backend call exec_pre_batch() and exec_post_batch()


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 19:40:48 UTC (rev 7944)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 19:52:10 UTC (rev 7945)
@@ -37,7 +37,7 @@
         # the( human readable) time format used in log file
         self.time_format = "%Y-%m-%d/%H:%M:%S"
 
-        # Commands to be executed once before the entire batch
+        # Commands to be executed once before the entire batch on the submit node
         self.pre_batch = []
         # Commands to be executed before every task in tasks
         self.pre_tasks = []
@@ -45,7 +45,7 @@
         self.tasks = []
         # Commands to be executed after each task in tasks
         self.post_tasks = []
-        # Commands to be executed once after the entire batch
+        # Commands to be executed once after the entire batch on the submit node
         self.post_batch = []
 
         # the default directory where to keep all the log files
@@ -263,6 +263,7 @@
             task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):
+        print "The Log file are under %s"%self.log_dir
         if self.test:
             print "Test mode, we only print the command to be executed, we don't execute them"
         # Execute pre-batch
@@ -278,6 +279,7 @@
             exec_post_batch()
 
         print "The Log file are under %s"%self.log_dir
+
     def clean(self):
         #TODO: delete all log files for the current batch
         pass
@@ -363,10 +365,12 @@
                 concurrentJobs = 200
 
                 ''') )
-#                preBatch = ''' + pre_batch_command + '''
-#                postBatch = ''' + post_batch_command +'''
         bqsubmit_dat.close()
 
+        # Execute pre-batch
+        if len(self.pre_batch)>0:
+            exec_pre_batch()
+
         # Launch bqsubmit
         output = PIPE
         error = PIPE
@@ -378,6 +382,11 @@
 
         os.chdir('parent')
 
+        # Execute post-batchs
+        if len(self.post_batch)>0:
+            exec_post_batch()
+
+            
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
@@ -507,8 +516,6 @@
                        self.log_dir,self.targetcondorplatform,self.unique_id,
                        self.log_dir,self.targetcondorplatform,self.unique_id,
                        self.log_dir,self.targetcondorplatform)))
-#                preBatch = ''' + pre_batch_command + '''
-#                postBatch = ''' + post_batch_command +'''
 
         if len(condor_datas)!=0:
             for i in condor_datas:
@@ -594,6 +601,8 @@
 
 
     def run(self):
+        print "The Log file are under %s"%self.log_dir
+
         if len(self.pre_batch)>0:
             exec_pre_batch()
         self.run_all_job()
@@ -645,16 +654,9 @@
         #keeps a list of the temporary files created, so that they can be deleted at will            
 
     def run_one_job(self,task):
-        launch_file = os.path.join(self.tmp_dir, 'launch.sh')
-
-        # Launch condor
+            
         output = PIPE
         error = PIPE
-        if not self.file_redirect_stdout and self.nb_proc>1:
-            print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
-        if not self.file_redirect_stderr and self.nb_proc>1:
-            print "[DBI] WARNING: many process but all their stderr are redirected to the parent"
-            
         if int(self.file_redirect_stdout):
             output = file(self.log_file + '.out', 'w')
         if int(self.file_redirect_stderr):
@@ -682,11 +684,14 @@
 
     def run(self):
         if self.test:
-            print "Test mode, we only print the command to be executed, we don't execute them"
+            print "[DBI] Test mode, we only print the command to be executed, we don't execute them"
+        if not self.file_redirect_stdout and self.nb_proc>1:
+            print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
+        if not self.file_redirect_stderr and self.nb_proc>1:
+            print "[DBI] WARNING: many process but all their stderr are redirected to the parent"
+        print "The Log file are under %s"%self.log_dir
 
         # Execute pre-batch
-        output = PIPE
-        error = PIPE
         if len(self.pre_batch)>0:
             exec_pre_batch()
         
@@ -801,6 +806,8 @@
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):
+        print "The Log file are under %s"%self.log_dir
+
         # Execute pre-batch
         if len(self.pre_batch)>0:
             exec_pre_batch()



From nouiz at mail.berlios.de  Tue Aug  7 22:09:21 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 22:09:21 +0200
Subject: [Plearn-commits] r7946 - trunk/plearn_learners/generic
Message-ID: <200708072009.l77K9Lit018074@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 22:09:21 +0200 (Tue, 07 Aug 2007)
New Revision: 7946

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
more debug check


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-07 19:52:10 UTC (rev 7945)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-07 20:09:21 UTC (rev 7946)
@@ -542,9 +542,13 @@
                 PLERROR("In AddCostToLearner::computeCostsFromOutputs - confusion_matrix_target(%d) "
                         "not in the range of target_length(%d)", confusion_matrix_target, target_length);            
             if (sub_learner_output[confusion_matrix_target] >= n_classes
-                || desired_target[confusion_matrix_target] >= n_classes)
-                PLERROR("In AddCostToLearner::computeCostsFromOutputs - confusion_matrix sub_learner_output[i](%f) "
-                        "or desired_target[i](%d) higher or egual to n_classes (%d)", sub_learner_output[confusion_matrix_target],
+                || is_missing(sub_learner_output[confusion_matrix_target]))
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of sub_learner: sub_learner_output[confusion_matrix_target]=%f,  "
+                        " missing or higher or egual to n_classes (%d)",
+                        sub_learner_output[confusion_matrix_target],n_classes);
+            if (desired_target[confusion_matrix_target] >= n_classes
+                ||is_missing(desired_target[confusion_matrix_target]))
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of desired_target[i]=%f, missing or higher or egual to n_classes (%d)",
                         desired_target[confusion_matrix_target], n_classes);
 #endif
             for(int local_ind = ind_cost ; local_ind < (n_classes*n_classes+ind_cost); local_ind++){



From nouiz at mail.berlios.de  Tue Aug  7 22:10:44 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 22:10:44 +0200
Subject: [Plearn-commits] r7947 - trunk/plearn_learners/meta
Message-ID: <200708072010.l77KAiG7018149@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 22:10:44 +0200 (Tue, 07 Aug 2007)
New Revision: 7947

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
-Reuse the sorted train set between the base regressor if it is RegressionTree


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-08-07 20:09:21 UTC (rev 7946)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-08-07 20:10:44 UTC (rev 7947)
@@ -48,6 +48,7 @@
 #include <plearn/math/random.h>
 #include <plearn/io/load_and_save.h>
 #include <plearn/base/stringutils.h>
+#include <plearn_learners/regressors/RegressionTree.h>
 
 namespace PLearn {
 using namespace std;
@@ -191,6 +192,11 @@
                   "Indication that a weak learner with 0 training error"
                   "has been found.\n");
 
+    declareOption(ol, "sorted_train_set",
+                  &AdaBoost::sorted_train_set,
+                  OptionBase::learntoption,
+                  "A sorted train set when using the class RegressionTree as a base regressor\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -300,6 +306,23 @@
         }
         sum_voting_weights = 0;
         voting_weights.resize(0,nstages);
+
+        if (weak_learner_template->classname()=="RegressionTree" && ! weight_by_resampling)
+        {
+            if(train_set->weightsize()<=0)
+            {
+                Mat* m = new Mat(train_set->length(),1);
+                m->fill(1.0/m->length());
+                VMat data_weights = VMat(*m);
+                VMat new_train_set = new ConcatColumnsVMatrix(train_set,data_weights);
+                new_train_set->defineSizes(train_set->inputsize(),train_set->targetsize(),1);
+                train_set = new_train_set;
+            }
+            sorted_train_set = new RegressionTreeRegisters();
+            sorted_train_set->setOption("report_progress", tostring(report_progress));
+            sorted_train_set->setOption("verbosity", tostring(verbosity));
+            sorted_train_set->initRegisters(train_set);
+        }
     }
 
     VMat unweighted_data = train_set.subMatColumns(0, inputsize()+1);
@@ -308,6 +331,7 @@
     for ( ; stage < nstages ; ++stage)
     {
         VMat weak_learner_training_set;
+        PP<RegressionTreeRegisters> weak_learner_sorted_training_set;
         { 
             PP<ProgressBar> pb;
             if(report_progress) pb = new ProgressBar(
@@ -349,6 +373,14 @@
                     new SelectRowsVMatrix(unweighted_data, train_indices);
                 weak_learner_training_set->defineSizes(inputsize(), 1, 0);
             }
+            else if(weak_learner_template->classname()=="RegressionTree" && ! weight_by_resampling)
+            {
+                //No Need for deep copy of the sorted_train_set as after the train it is not used anymore
+                // and the data are not modofied, but we need to change the weight
+                weak_learner_sorted_training_set = sorted_train_set;
+                for(int i=0;i<example_weights.size();i++)
+                    weak_learner_sorted_training_set->setWeight(i,example_weights[i]);
+            }
             else
             {
                 Mat data_weights_column = example_weights.toMat(n,1).copy();
@@ -364,7 +396,10 @@
 
         // Create new weak-learner and train it
         PP<PLearner> new_weak_learner = ::PLearn::deepCopy(weak_learner_template);
-        new_weak_learner->setTrainingSet(weak_learner_training_set);
+        if(weak_learner_template->classname()=="RegressionTree" && ! weight_by_resampling)
+            ((PP<RegressionTree>)(new_weak_learner))->setSortedTrainSet(weak_learner_sorted_training_set);
+        else
+            new_weak_learner->setTrainingSet(weak_learner_training_set);
         new_weak_learner->setTrainStatsCollector(new VecStatsCollector);
         if(expdir!="" && provide_learner_expdir)
             new_weak_learner->setExperimentDirectory( expdir / ("WeakLearner"+tostring(stage)+"Expdir") );

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2007-08-07 20:09:21 UTC (rev 7946)
+++ trunk/plearn_learners/meta/AdaBoost.h	2007-08-07 20:10:44 UTC (rev 7947)
@@ -44,6 +44,7 @@
 #define AdaBoost_INC
 
 #include <plearn_learners/generic/PLearner.h>
+#include <plearn_learners/regressors/RegressionTreeRegisters.h>
 
 namespace PLearn {
 using namespace std;
@@ -76,7 +77,9 @@
 
     //! Indication that a weak learner with 0 training error has been found
     bool found_zero_error_weak_learner;
-  
+
+    //! a sorted train set when using a tree as a base regressor  
+    PP<RegressionTreeRegisters> sorted_train_set;
 public:
 
     // ************************



From nouiz at mail.berlios.de  Tue Aug  7 22:15:28 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 22:15:28 +0200
Subject: [Plearn-commits] r7948 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200708072015.l77KFS32018410@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 22:15:27 +0200 (Tue, 07 Aug 2007)
New Revision: 7948

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
-more log info
-give more info about the early stopping


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-07 20:10:44 UTC (rev 7947)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-07 20:15:27 UTC (rev 7948)
@@ -489,6 +489,7 @@
     all_last_err = [best_err]
     all_lr = [initial_lr]
     all_start = [0]
+    all_end = [-1]
     actives = [0]
     best_candidate = learner
     best_early_stop = stages[0]
@@ -500,10 +501,11 @@
     for t in range(n_train):
         stage=stages[t]
         if logfile:
-            print >>logfile, "After ", stage, "stage"
-        print "After",stage,"stages, actives now: ",actives, " with lr=", array(all_lr)[actives]
+            print >>logfile, "Before epoch",t+1
+            print >>logfile, "Actives now: ",actives
+            print >>logfile, " with lr=", array(all_lr)[actives]
+        print "Before epoch %d/%d"%(t,n_train),"actives now: ",actives, " with lr=", array(all_lr)[actives]
         print "current best actives:",best_active,"best_error:",all_last_err[best_active],"lr:",all_lr[best_active]
-        print >>logfile, "actives now: ",actives, " with lr=", array(all_lr)[actives]
 
         threads = []
         active_stats = []
@@ -593,6 +595,8 @@
                             best_err = err
                             best_active = active
                             best_early_stop = stage
+                            best_early_stop_candidate = active
+                            best_early_stop_epoch = t
                             if return_best_model:
                                 best_candidate = deepcopy(candidate,
                                                           use_threads)
@@ -633,6 +637,7 @@
                         print >>logfile,"REMOVE candidate ",a
                     release_server(all_candidates[a], use_threads)
                     # hopefully this destroys the candidate
+                    all_end[a]=t
                     all_candidates[a]=None
                     del actives[j-ndeleted]
                     ndeleted+=1
@@ -650,6 +655,7 @@
                 all_results.append(all_results[best_active].copy())
                 all_last_err.append(best_last)
                 all_start.append(t)
+                all_end.append(-1)
                 if logfile:
                     print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at stage ",learner.stage," with lr=",all_lr[new_a]
                     logfile.flush()
@@ -661,6 +667,18 @@
         schedules[:,1+optimized_group]=all_results[best_active][:,1]
     if logfile and best_err < all_last_err[best_active]:
         print >>logfile, "WARNING: best performing model would have stopped early at stage ",best_early_stop
+    if logfile:
+        print >>logfile,"When then learner started",all_start
+        print >>logfile,"When then learner ended",all_end
+        l=[]
+        for i in range(len(all_start)):
+            if all_end[i]==-1:
+                l.append(abs(all_start[i]-n_train))
+            else:
+                l.append(abs(all_start[i]-all_end[i]+1))
+        print >>logfile,"Duration of all learner in number of epoch:",l
+        sum=reduce(lambda x,y:x+y, l)
+        print >>logfile,"Their was a total of %d epoch executed for %d asked(%fx more)"%(sum,n_train,float(sum)/n_train)
     return (# Learner
             final_model,
             # Matrix of schedules (including the one that was optimized)
@@ -677,5 +695,5 @@
             all_start,
             # Timestep (in stages) at which early-stopping should have happened
             # (i.e. stage of best error found)
-            best_early_stop)
+            (best_early_stop,best_early_stop_candidate,best_early_stop_epoch))
 



From nouiz at mail.berlios.de  Tue Aug  7 22:27:16 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 7 Aug 2007 22:27:16 +0200
Subject: [Plearn-commits] r7949 - trunk/python_modules/plearn/parallel
Message-ID: <200708072027.l77KRGIa018872@sheep.berlios.de>

Author: nouiz
Date: 2007-08-07 22:27:15 +0200 (Tue, 07 Aug 2007)
New Revision: 7949

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
do not overwire the number of processor


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 20:15:27 UTC (rev 7948)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-07 20:27:15 UTC (rev 7949)
@@ -618,10 +618,10 @@
 class DBILocal(DBIBase):
 
     def __init__( self, commands, **args ):
+        self.nb_proc=1
         DBIBase.__init__(self, commands, **args)
         self.args=args
         self.threads=[]
-        self.nb_proc=1
         for command in commands:
             self.add_command(command)
             



From chrish at mail.berlios.de  Tue Aug  7 22:33:04 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 7 Aug 2007 22:33:04 +0200
Subject: [Plearn-commits] r7950 - in tags: . finlearn3-august2007
	finlearn3-august2007/python_modules/plearn/plide
	finlearn3-august2007/python_modules/plearn/pyplearn
Message-ID: <200708072033.l77KX4jt019199@sheep.berlios.de>

Author: chrish
Date: 2007-08-07 22:33:03 +0200 (Tue, 07 Aug 2007)
New Revision: 7950

Added:
   tags/finlearn3-august2007/
   tags/finlearn3-august2007/python_modules/
   tags/finlearn3-august2007/python_modules/plearn/plide/plide.py
   tags/finlearn3-august2007/python_modules/plearn/pyplearn/plargs.py
Removed:
   tags/finlearn3-august2007/python_modules/
   tags/finlearn3-august2007/python_modules/plearn/plide/plide.py
   tags/finlearn3-august2007/python_modules/plearn/pyplearn/plargs.py
Log:
Tag pour release finlearn3 august2007

Copied: tags/finlearn3-august2007 (from rev 7906, trunk)

Copied: tags/finlearn3-august2007/python_modules (from rev 7938, trunk/python_modules)

Deleted: tags/finlearn3-august2007/python_modules/plearn/plide/plide.py
===================================================================
--- trunk/python_modules/plearn/plide/plide.py	2007-08-07 00:50:45 UTC (rev 7938)
+++ tags/finlearn3-august2007/python_modules/plearn/plide/plide.py	2007-08-07 20:33:03 UTC (rev 7950)
@@ -1,968 +0,0 @@
-#  plide.py
-#  Copyright (C) 2006 by Nicolas Chapados
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-
-#####  Python Imports  ######################################################
-
-import fcntl
-import os, os.path
-import Queue
-import re
-import select, sys
-import threading, time, traceback
-
-
-#####  GTK Imports  #########################################################
-
-import gobject
-import pygtk
-pygtk.require('2.0')
-import gtk, gtk.gdk
-import pango
-from plearn.pl_pygtk import GladeAppWindow, GladeDialog, MessageBox
-
-
-#####  PLearn Imports  ######################################################
-
-from plearn.utilities.metaprog import public_members
-from plearn.pyplearn           import *
-from plearn.utilities.toolkit  import doc as toolkit_doc
-
-
-#####  Plide  ###############################################################
-
-from plide_help    import PlideHelp
-from plide_options import *
-from plide_tabs    import *
-from plide_utils   import *
-
-
-#####  Exports  #############################################################
-
-__all__ = [
-    'StartPlide',
-    'QuitPlide',
-    'GetWork',
-    'PostWorkResults',
-    'LogAppend',
-    'AllocateProgressBar',
-    'ReleaseProgressBar',
-    'ProgressUpdate'
-    ]
-
-
-#####  Global Configuration  ################################################
-
-def gladeFile():
-    import plearn.plide.plide
-    return os.path.join(os.path.dirname(plearn.plide.plide.__file__),
-                        "resources", "plide.glade")
-
-def helpResourcesPath():
-    import plearn.plide.plide
-    return os.path.join(os.path.dirname(plearn.plide.plide.__file__),
-                        "resources")
-
-
-#####  Main Window  #########################################################
-
-class PlideMain( GladeAppWindow ):
-
-    PlideVersion = "0.01"
-
-    def __init__( self, streams_to_watch= {}, *args, **kwargs ):
-        GladeAppWindow.__init__(self, gladeFile())
-
-        ## Forward injected to imported Plide modules
-        PlideHelp.define_injected(injected)
-        PlideTab. define_injected(injected)
-        PyPLearnOptionsDialog.define_injected(injected, gladeFile)
-
-        ## Initialize Members
-        self.untitled_counter  = 1
-        self.work_requests = {}         # Request ids to expdir mapping
-        self.all_plearn_classes = injected.getAllClassnames()
-
-        ## Initialize Display
-        self.setup_statusbar()
-        self.log_filters = [ re.compile("WARNING.*Scintilla.*PosChanged.*deprecated") ]
-        self.log_clear()
-        self.log_hide()
-        welcome_text = kwargs.get("welcome_text",
-                                  "<b>Welcome to Plide %s!</b>" % self.PlideVersion)
-        self.status_display(welcome_text, has_markup=True)
-        self.setup_stdouterr_redirect(streams_to_watch)
-        
-        ## Set up help system
-        injected.helpResourcesPath(helpResourcesPath())
-        self.help_viewer = PlideHelp(self)
-        self.help_viewer.display_page("index.html")
-        self.help_close()
-
-        ## Prepare the work queue
-        self.work_queue = PLearnWorkQueue()
-
-    def quit( self ):
-        ## Minor hack: the main-thread loop is terminated by receiving a
-        ## 'script' whose contents is Quit().  First close all tabs and
-        ## ensure that we stop the process if some tabs won't be closed.
-        n = self.w_plide_notebook.get_n_pages()
-        for i in range(n-1,-1,-1):
-            tab = self.get_nth_tab(i)
-            if not tab.close_tab():
-                return True        # Stop close process if cannot close tab
-        
-        print >>raw_stderr, "Quit message received"
-        raw_stderr.flush()
-        self.work_queue.post_work_request("Quit()","","")
-        GladeAppWindow.quit(self)
-
-    def help_close( self ):
-        """Close the help pane.
-        """
-        self.w_help_frame.hide()
-        self.w_help_frame.set_no_show_all(True)
-
-    def help_show( self ):
-        """Open the help pane.  Bring up context-sensitive help if there is
-        a valid context in the current tab.
-        """
-        self.w_help_frame.set_no_show_all(False)
-        self.w_help_frame.show()
-
-        curtab = self.get_current_tab()
-        if curtab:
-            help_context = curtab.get_help_candidate()
-            if help_context:
-                self.help_viewer.display_page(help_context)
-            
-    def setup_stdouterr_redirect( self, streams_to_watch= {} ):
-        """Redirect standard output and error to be sent to the log pane
-        instead of the console.
-        """
-        ## Redirect standard output and standard error to display to the
-        ## log pane area.  Keep around old stdout/stderr in order to
-        ## display debugging messages.  They are called, respectively,
-        ## raw_stdout and raw_stderr (Python file objects)
-        global raw_stdout, raw_stderr
-        old_stdout_fd = os.dup(sys.stdout.fileno())
-        old_stderr_fd = os.dup(sys.stderr.fileno())
-        raw_stdout    = os.fdopen(old_stdout_fd, 'w')
-        raw_stderr    = os.fdopen(old_stderr_fd, 'w')
-
-        print >>sys.stderr, "Original stderr"
-        print >>raw_stderr, "Redirected stderr"
-        sys.stderr.flush()
-        raw_stderr.flush()
-        
-        (self.stdout_read, self.stdout_write) = os.pipe()
-        (self.stderr_read, self.stderr_write) = os.pipe()
-        os.dup2(self.stdout_write, sys.stdout.fileno())
-        os.dup2(self.stderr_write, sys.stderr.fileno())
-        
-        out_flags = fcntl.fcntl(self.stdout_read, fcntl.F_GETFL)
-        err_flags = fcntl.fcntl(self.stderr_read, fcntl.F_GETFL)
-        fcntl.fcntl(self.stdout_read, fcntl.F_SETFL, out_flags | os.O_NONBLOCK)
-        fcntl.fcntl(self.stderr_read, fcntl.F_SETFL, err_flags | os.O_NONBLOCK)
-
-        streams_to_watch.update({ self.stdout_read:'stdout',
-                                  self.stderr_read:'stderr' })
-
-        def callback( fd, cb_condition ):
-            # print >>raw_stderr, "log_updater: got stuff on fd", fd, \
-            #       "with condition", cb_condition
-            raw_stderr.flush()
-            data = os.read(fd,65536)
-
-            kind = streams_to_watch.get(fd,'')
-            self.log_display(data, kind)
-            return True                 # Ensure it's called again!
-        
-        for s in streams_to_watch:
-            gobject.io_add_watch(s, gobject.IO_IN, callback)
-
-    def setup_statusbar( self ):
-        """Arrange the status bar area to contain both a status line and a progress bar.
-        """
-        ## The GTK StatusBar widget is a pain to use.  Our statusbar is an
-        ## hbox packed at the bottom of main_vbox, and containing:
-        ##
-        ## - A frame with a status label
-        ## - A progress bar
-        self.w_statusframe = gtk.Frame()
-        self.w_statusframe.set_shadow_type(gtk.SHADOW_IN)
-        self.status_display(' ')        # Establish proper height
-        self.w_statusframe.show()
-
-        self.w_progressbar = gtk.ProgressBar()
-        self.w_progressbar.set_ellipsize(pango.ELLIPSIZE_END)
-        self.w_progressbar.show()
-
-        ## This gives a list of the "order" in which progress bars should
-        ## be allocated as well as whether each one has been allocated
-        self.all_progressbars   = [ (self.w_progressbar,False) ]
-
-        self.w_statusbar = gtk.Table(rows=1, columns=4)
-        self.w_statusbar.attach(self.w_statusframe, left_attach=0, right_attach=3,
-                                top_attach=0, bottom_attach=1,
-                                xoptions=gtk.EXPAND | gtk.FILL,
-                                yoptions=gtk.EXPAND | gtk.FILL,
-                                xpadding=0, ypadding=0)
-        self.w_statusbar.attach(self.w_progressbar, left_attach=3, right_attach=4,
-                                top_attach=0, bottom_attach=1,
-                                xoptions=gtk.EXPAND | gtk.FILL,
-                                yoptions=gtk.EXPAND | gtk.FILL,
-                                xpadding=2, ypadding=2)
-        self.w_statusbar.show()
-        self.w_main_vbox.pack_start(self.w_statusbar, expand=False, fill=False)
-
-    def log_clear( self ):
-        """The log is a TreeView widget that contains 3 columns: (1)
-        number, (2) kind, (3) log entry itself.  This sets up a new log or
-        clears an existing log.
-        """
-        self.log_liststore = gtk.ListStore(int, str, str, int)
-
-        ## Create individual columns
-        self.log_columns = [ gtk.TreeViewColumn(x) for x in [ 'No.', 'Kind', 'Message' ] ]
-
-        ## Create cell renderers for displaying the contents
-        self.log_cells = [ gtk.CellRendererText() for i in xrange(len(self.log_columns)) ]
-        for i, (column, cell) in enumerate(zip(self.log_columns, self.log_cells)):
-            column.pack_start(cell, True)
-            column.add_attribute(cell, 'text', i)
-            column.set_sort_column_id(i)
-
-        ## Last column (message) is a bit tricky: we either set the
-        ## attribute 'text' or 'markup' depending on the setting of the
-        ## fourth model column
-        def message_format_func(treeviewcolumn, cell, model, iter):
-            message = model.get(iter,2)[0]
-            markup  = model.get(iter,3)[0]
-            cell.set_property('family-set', True)
-            if markup:
-                cell.set_property('markup', message)
-                cell.set_property('family', 'Helvetica')
-            else:
-                cell.set_property('text', message)
-                cell.set_property('family', 'Monospace')
-        self.log_columns[2].set_cell_data_func(self.log_cells[2], message_format_func)
-            
-        ## Create the TreeView using underlying liststore model and append columns
-        ## and make it searchable on the message
-        self.log_treeview = gtk.TreeView(self.log_liststore)
-        for column in self.log_columns:
-            self.log_treeview.append_column(column)
-        self.log_treeview.set_search_column(2)
-        self.log_treeview.set_rules_hint(True)
-
-        container_remove_children(self.w_plearn_log_scroller)
-        self.w_plearn_log_scroller.add(self.log_treeview)
-        self.log_treeview.show_all()
-
-        ## Reset the next log entry
-        self.log_next_number = 1
-
-    def log_hide( self ):
-        self.w_log_messages_menuitem.set_active(False)
-        self.w_plearn_log_scroller.hide()
-        self.w_plearn_log_scroller.set_no_show_all(True)
-
-    def log_show( self ):
-        self.w_log_messages_menuitem.set_active(True)
-        self.w_plearn_log_scroller.set_no_show_all(False)
-        self.w_plearn_log_scroller.show()
-        
-    def log_display( self, message, kind = "", has_markup = False, log_clear = False ):
-        """Append the given message to the log area of the main window.
-        Thread-safe.
-        """
-        if log_clear:
-            self.log_clear()
-
-        ## If 'data' matches any regular expression in log_filter, skip
-        ## this message.  This is mostly a hack to get around displaying
-        ## known warnings from the Scintilla editor
-        for regex in self.log_filters:
-            if regex.search(message):
-                return
-
-        row = [ self.log_next_number, kind.rstrip(), message.rstrip(), has_markup ]
-        self.log_show()
-        self.log_liststore.append(row)
-        self.log_next_number += 1
-
-    def status_display( self, message, has_markup = False ):
-        """Display the given message in the status bar area of the main window.
-        Thread-safe.
-        """
-        label = gtk.Label(message)
-        if has_markup:
-            label.set_markup(message)
-        label.set_line_wrap(False)
-        label.set_alignment(0,0)
-        label.set_padding(2,2)
-        label.set_single_line_mode(True)
-        label.set_ellipsize(pango.ELLIPSIZE_MIDDLE)
-        label.show()
-        container_remove_children(self.w_statusframe)
-        self.w_statusframe.add(label)
-
-    def cursor_hourglass( self, unsensitize = True ):
-        """Make the cursor an hourglass for the main window.
-
-        In addition, if the argument unsensitize is True, most entry
-        will be disabled for the window.
-        """
-        self.w_root.window.set_cursor(gtk.gdk.Cursor(gtk.gdk.WATCH))
-        if unsensitize:
-            self.w_root.set_sensitive(False)
-
-    def cursor_normal( self, sensitize = True ):
-        """Take back the cursor to normal form for the main window.
-        """
-        self.w_root.window.set_cursor(None)  # Set back to parent window cursor
-        if sensitize:
-            self.w_root.set_sensitive(True)
-
-    def get_nth_tab( self, n ):
-        """Return the PlideTab object corresponding to the n-th tab.
-        Return None if there is no such PlideTab.
-        """
-        notebook_page = self.w_plide_notebook.get_nth_page(n)
-        if notebook_page:
-            return notebook_page.plide_tab_object
-        else:
-            return None
-
-    def get_current_tab( self ):
-        """Return the PlideTab object that's currently selected in the
-        notebook.  Return None if none...
-        """
-        cur_page = self.w_plide_notebook.get_current_page()
-        if cur_page >= 0:
-            return self.w_plide_notebook.get_nth_page(cur_page).plide_tab_object
-        else:
-            return None
-
-    def get_current_tab_directory( self ):
-        """Return the directory associated with the current PlideTab, or
-        '.' if there is no current tab.
-        """
-        cur_tab = self.get_current_tab()
-        dir = None
-        if cur_tab is not None:
-            dir = cur_tab.get_directory()
-        return dir or '.'
-
-
-    #####  Progress Bar Handling  ###########################################
-
-    def reset_progress( self ):
-        """Bring back the all progress bars to an 'available' state,
-        erase their containing text, and bring the fraction to zero.
-        """
-        for i,(pb,alloc) in enumerate(self.all_progressbars):
-            pb.set_text('')
-            pb.set_fraction(0.0)
-            self.all_progressbars[i] = (pb,False)
-
-    def allocate_progress( self ):
-        """Return the ID of an available progress bar, or -1 if none is
-        available.
-        """
-        for i,(pb,alloc) in enumerate(self.all_progressbars):
-            if not alloc:
-                self.all_progressbars[i] = (pb,True)
-                return i
-
-        return -1
-
-    def release_progress( self, progress_id ):
-        """Release an available progress bar and make it available for
-        other uses.
-        """
-        (pb,alloc) = self.all_progressbars[progress_id]
-        self.all_progressbars[progress_id] = (pb,False)
-
-    def get_progress_from_id( self, progress_id ):
-        """Return the gtk.ProgressBar widget corresponding to its id.
-        """
-        return self.all_progressbars[progress_id][0]
-
-
-    #####  Callbacks  #######################################################
-
-    ## General
-    def on_plide_top_delete_event(self, widget, event):
-        return self.quit()
-
-    def on_quit_activate(self, widget):
-        self.quit()
-
-    ## File Menu
-    def on_new_activate(self, widget):
-        self.add_untitled_tab(".pyplearn")
-        
-    def on_new_pyplearn_script_activate(self, widget):
-        self.add_untitled_tab(".pyplearn")
-
-    def on_new_py_script_activate(self, widget):
-        self.add_untitled_tab(".py")
-
-    def on_new_plearn_script_activate(self, widget):
-        self.add_untitled_tab(".plearn")
-
-    def on_new_text_file_activate(self, widget):
-        self.add_untitled_tab("")
-
-    def on_open_activate(self, widget):
-        self.open_file()
-
-    def on_save_activate(self, widget):
-        if self.get_current_tab():
-            self.get_current_tab().on_save_activate()
-
-    def on_save_as_activate(self, widget):
-        if self.get_current_tab():
-            self.get_current_tab().save_as_file()
-
-    def on_close_activate(self, widget):
-        if self.get_current_tab():
-            self.get_current_tab().close_tab(widget)
-
-    def on_browse_expdir_activate(self, widget):
-        self.open_file(action=gtk.FILE_CHOOSER_ACTION_SELECT_FOLDER)
-
-    ## Edit menu
-    def on_undo_activate(self, widget):
-        self.get_current_tab().on_undo_activate()
-
-    def on_redo_activate(self, widget):
-        self.get_current_tab().on_redo_activate()
-
-    def on_cut_activate(self, widget):
-        self.get_current_tab().on_cut_activate()
-
-    def on_copy_activate(self, widget):
-        self.get_current_tab().on_copy_activate()
-
-    def on_paste_activate(self, widget):
-        self.get_current_tab().on_paste_activate()
-
-    ## View menu
-    def on_log_messages_toggled(self, menuitem):
-        if menuitem.get_active():
-            self.log_show()
-        else:
-            self.log_hide()
-
-    ## Help menu
-    def on_about_activate(self, widget):
-        version = injected.versionString().replace("(","\n(")
-        MessageBox("PLearn Integrated Development Environment Version " + self.PlideVersion,
-                   "Running on " + version + "\n" +\
-                   "Copyright (c) 2006 by Nicolas Chapados",
-                   title = "About Plide")
-
-    ## Toolbar
-    def on_toolbutton_new_pyplearn_clicked(self, widget):
-        self.add_untitled_tab(".pyplearn")
-
-    def on_toolbutton_open_clicked(self, widget):
-        self.open_file()
-
-    def on_toolbutton_options_clicked(self, widget):
-        """Display dialog box for establishing script options.
-        """
-        tab = self.get_current_tab()
-        if tab is not None:
-            script      = tab.get_text()
-            name        = tab.get_basename()
-            script_dir  = tab.get_directory()
-
-            while True:                 # Loop to handle script reload
-                options_holder = tab.get_options_holder()
-                if not options_holder:
-                    ## When executing for the first time, run the script
-                    if self.pyplearn_parse( name, script ) is None:
-                        return              # Syntax errors in script
-                    options_holder = PyPLearnOptionsHolder(name, script, script_dir)
-                    tab.set_options_holder(options_holder)
-
-                options_dialog = PyPLearnOptionsDialog(options_holder)
-                result = options_dialog.run()
-                if result == gtk.RESPONSE_OK:
-                    options_dialog.update_options_holder()
-                options_dialog.destroy()
-
-                if result == gtk.RESPONSE_REJECT:
-                    tab.set_options_holder(None)
-                else:
-                    break
-
-    def on_toolbutton_execute_clicked(self, widget):
-        """Launch the execution of the pyplearn script, only if it's indeed
-        such a script.
-        """
-        tab = self.get_current_tab()
-        if tab is not None:
-            if type(tab) == PlideTabPyPLearn:
-                script_name    = tab.get_basename()
-                script_code    = tab.get_text()
-                launch_dir     = tab.get_directory()
-                options_holder = tab.get_options_holder()
-                if options_holder is not None:
-                    launch_dir = options_holder.launch_directory
-                self.pyplearn_executor(script_name, script_code, launch_dir,
-                                       options_holder)
-
-
-    ### Help-related
-    def on_help_activate(self, widget):
-        self.help_show()
-        
-    def on_help_close_clicked(self, widget):
-        self.help_close()
-        
-
-    #####  Tab Handling  ####################################################
-
-    def add_untitled_tab(self, extension):
-        self.add_intelligent_tab("untitled%d%s" % (self.untitled_counter,
-                                                   extension), is_new=True)
-        self.untitled_counter += 1
-
-    def add_intelligent_tab(self, filename, is_new = False):
-        """Create a new tab 'intelligently' depending on the filename type
-        or its extension. (If the file is new, rely on its extension only).
-        """
-        filename = filename.rstrip(os.path.sep)
-        extension = os.path.splitext(filename)[1]
-        new_tab   = None
-        if extension == ".pyplearn" or extension == ".py":
-            new_tab = PlideTabPyPLearn(self.w_plide_notebook, filename, is_new,
-                                       self.all_plearn_classes)
-
-        elif extension == ".pmat":
-            new_tab = PlideTabPMat(self.w_plide_notebook, filename)
-
-        elif os.path.isdir(filename):
-            new_tab = PlideTabExpdir(self.w_plide_notebook, filename)
-
-        elif os.path.exists(filename):
-            new_tab = PlideTabFile(self.w_plide_notebook, filename, is_new)
-
-        elif is_new:
-            new_tab = PlideTabFile(self.w_plide_notebook, filename, is_new)
-            
-        else:
-            MessageBox("File '%s' is of unrecognized type" % filename,
-                       type=gtk.MESSAGE_ERROR)
-
-        self.status_display('')         # Clear the status
-
-    def open_file(self, action=gtk.FILE_CHOOSER_ACTION_OPEN):
-        """Display a file chooser dialog and add a new tab based on selected file.
-        """
-        chooser = gtk.FileChooserDialog(
-            title="Open",
-            action=action,
-            buttons= (gtk.STOCK_CANCEL,gtk.RESPONSE_CANCEL,
-                      gtk.STOCK_OPEN,  gtk.RESPONSE_OK))
-
-        # Add file filters
-        filter = gtk.FileFilter()
-        filter.set_name("All files")
-        filter.add_pattern("*")
-        chooser.add_filter(filter)
-        
-        filter = gtk.FileFilter()
-        filter.set_name("Experiment scripts")
-        filter.add_pattern("*.plearn")
-        filter.add_pattern("*.pyplearn")
-        chooser.add_filter(filter)
-
-        filter = gtk.FileFilter()
-        filter.set_name("Data")
-        filter.add_pattern("*.pmat")
-        filter.add_pattern(".amat")
-        chooser.add_filter(filter)
-
-        chooser.set_default_response(gtk.RESPONSE_OK)
-        chooser.set_current_folder(self.get_current_tab_directory())
-        response = chooser.run()
-
-        ## Add all files selected by the user
-        if response == gtk.RESPONSE_OK:
-            filenames = chooser.get_filenames()
-            for f in filenames:
-                self.add_intelligent_tab(f, is_new=False)
-            
-        chooser.destroy()
-
-
-    #####  PLearn execution  ############################################
-
-    def pyplearn_executor( self, script_name, script_code, launch_directory,
-                           options_holder ):
-        """Execute a pyplearn script within an options context.
-        
-        Operations are as follows:
-        
-        1. We execute the script one more time with the more
-           recent text (may have changed since last time options
-           were set)
-        2. We set the options corresponding each scoped object
-           in their own class
-        3. We parse the manual command-line arguments
-        4. We transform the script to a .plearn
-        5. We grab a hold of the soon-to-be-created expdir
-        6. We hand this script off to PLearn for execution
-        """
-        script_env = self.pyplearn_parse( script_name, script_code )
-        if script_env is not None:
-            if options_holder:
-                options_holder.pyplearn_actualize()
-            else:
-                ## FIXME: Generate a brand-new expdir (minor hack)
-                plargs.parse(["expdir="+generateExpdir()])
-                
-            expdir = plargs.expdir
-            plearn_script = eval('str(PyPLearnScript( main() ))', script_env)
-
-            message = 'Launching script <b>%s</b> in directory <b>%s</b>' % \
-                      (script_name, launch_directory)
-            self.status_display(message, has_markup=True)
-            self.log_display   (message, has_markup=True, log_clear=True)
-            if self.w_dump_plearn_to_log.get_active():
-                self.log_display("Expdir is: %s\n.plearn is:\n%s" %
-                                 (expdir, plearn_script))
-            
-            request_id = self.work_queue.post_work_request(
-                plearn_script, launch_directory, "pyplearn")
-            
-            print >>sys.stderr, "Caller executing request_id", request_id
-            sys.stderr.flush()
-            self.work_requests[request_id] = os.path.join(launch_directory,expdir)
-            self.add_plearn_results_monitor( script_name, request_id )
-
-    def add_plearn_results_monitor( self, script_name, request_id, interval = 100 ):
-        """Add a monitor callback to check for availability of PLearn
-        results every 'interval' milliseconds.
-        """
-        def callback( ):
-            completion_result = \
-                self.work_queue.work_request_completed(request_id)
-
-            if completion_result is not None:
-                gtk.threads_enter()     # This is not a GTK+ callback
-                self.reset_progress()
-                (result_code, result_details) = completion_result
-
-                ## If result_code is "", it means everything is OK.
-                if result_code == "":
-                    message = "<b>%s</b> completed successfully" % script_name
-                    self.status_display(message, has_markup = True)
-                    self.log_display(message, has_markup = True)
-
-                else:
-                    status_msg = "<b>%s</b> terminated due to errors" % script_name
-                    self.status_display(status_msg, has_markup = True)
-                    message = ('A fatal error of kind "%s" was encountered during ' +\
-                               'execution of script "%s".') % (result_code, script_name)
-                    details = "Details:\n" + result_details
-                    self.log_display(message+"\n"+details, has_markup = False)
-                    MessageBox(message, details, type=gtk.MESSAGE_ERROR)
-
-                ## Done: don't call again, and add a new tab corresponding
-                ## to the expdir.  Check for expdir existence first, since
-                ## some experiments don't necessarily leave an expdir around
-                if os.path.isdir(self.work_requests[request_id]):
-                    self.add_intelligent_tab(self.work_requests[request_id])
-                gtk.threads_leave()
-                return False
-            else:
-                ## Call again later
-                return True
-
-        ## Add the callback to the GTK list of callback
-        callback_id = gobject.timeout_add(interval, callback)
-        
-        
-    #####  PyPLearn Parse  ##############################################
-    
-    def pyplearn_parse( self, script_name, script_code ):
-        """Ensure that a pyplearn script parses without error.
-
-        If an error is encountered, a backtrace is emitted to the log aread
-        and a message box is popped to indicate the error.  Return None in
-        this case.  Return the script execution environment if no error is
-        encountered.
-        """
-
-        ## Implementation note: start by compiling the code to catch syntax
-        ## errors in the script.  Then execute with an 'exec' statement and
-        ## separately catch execution errors.
-        compiled_code = None
-        try:
-            self.cursor_hourglass()
-            compiled_code = compile(script_code+'\n', script_name, 'exec')
-        except ValueError:
-            pass
-        except SyntaxError, e:
-            self.cursor_normal()
-            (exc_type, exc_value, tb) = sys.exc_info()
-            self.status_display("Syntax error in script <b>%s</b>" % script_name,
-                                True)
-            self.log_display(''.join(traceback.format_exception_only(exc_type, exc_value)))
-            MessageBox('Syntax error in script "%s".' % script_name,
-                       "Python message: %s\nSee the log area for the detailed traceback." % \
-                       str(exc_value),
-                       title = "PyPLearn Script Error",
-                       type=gtk.MESSAGE_ERROR)
-            return None
-
-        if compiled_code:
-            script_env  = { }
-            try:
-                exec compiled_code in script_env
-            except:
-                self.cursor_normal()
-                (exc_type, exc_value, tb) = sys.exc_info()
-                self.log_display(''.join(traceback.format_tb(tb)))
-                self.status_display("Exception during execution of script <b>%s</b>."
-                                    % script_name, True)
-                MessageBox('Script "%s" raised exception "%s: %s".' \
-                           % (script_name, str(exc_type), str(exc_value)),
-                       "See the log area for the detailed traceback.",
-                       title = "PyPLearn Script Error",
-                       type=gtk.MESSAGE_ERROR)
-                return None
-            else:
-                self.cursor_normal()
-                self.status_display("Script <b>%s</b> parsed successfully."
-                                    % script_name, True)
-                return script_env
-
-        self.cursor_normal()
-
-
-#####  Utility Classes  #####################################################
-
-class PLearnWorkQueue( object ):
-    """Worker thread for PLearn computation.
-
-    Since PLearn is quite highly dependent on the assumption of single
-    threading, this object simply passes work requests between the GUI
-    thread (which runs Plide) and the main thread (which runs PLearn).
-    Work requests are posted from the GUI the post_work_request() method,
-    which returns a work_id.  You can then poll the worker for task
-    completion status, or sleep until the task is completed.
-
-    Likewise, the main thread calls get_work_request(), which blocks until
-    a work request arrives.  The method returns a 4-tuple of strings of the
-    form [ request_id, script, root_directory, script_type ].  The main
-    thread can post results using the post_work_results() method.
-    """
-    def __init__( self ):
-        self.requests_queue     = Queue.Queue()
-        self.results_queue      = Queue.Queue()
-        self.next_request_id    = 1
-        self.completion_results = { }
-
-    def post_work_request( self, script, script_dir, script_kind ):
-        """Post a new work request to PLearn and return immediately.
-        """
-        request_id = self.next_request_id
-        self.completion_results[str(request_id)] = None
-        self.requests_queue.put((str(request_id), script, script_dir, script_kind))
-        self.next_request_id += 1
-        return request_id
-
-    def get_work( self ):
-        """Called by the PLearn main thread to get work to do.  Blocks
-        until there is work.
-        """
-        return self.requests_queue.get()
-
-    def post_work_results( self, request_id, code, results ):
-        """Called by the PLearn main thread to announce that it has finished
-        processing the work request 'request_id'.
-        """
-        self.results_queue.put((request_id, code, results))
-
-    def work_request_completed( self, request_id ):
-        """Return a pair (Code,Results) if the given work request is
-        finished processing by PLearn, and None if it's still being
-        processed.
-        """
-        while not self.results_queue.empty():
-            (cur_id, code, results) = self.results_queue.get()
-            self.completion_results[cur_id] = (code, results)
-            print >>raw_stderr, "work_request_completed:", (cur_id, code, results)
-            raw_stderr.flush()
-
-        return self.completion_results[str(request_id)]
-
-    def wait_for_work_request( self, request_id ):
-        """Wait (block) until the specified work request is finished
-        processing by PLearn.
-        """
-        while self.completion_results[str(request_id)] is None:
-            (cur_id, code, results) = self.results_queue.get()
-            self.completion_results[cur_id] = (code, results)
-
-
-#####  C++ Functional Interface  ############################################
-
-#global plide_main_window
-plide_main_window = None
-
-def StartPlide(argv = [], streams_to_watch= {}):
-    global plide_main_window
-    plide_main_window = PlideMain(streams_to_watch)
-
-    ## Consider each file passed as command-line argument and create a tab
-    ## to view it.
-    for arg in argv:
-        plide_main_window.add_intelligent_tab(arg, not os.path.exists(arg))
-
-    plide_main_window.run()       # Show and start event loop in other thread
-
-def QuitPlide():
-    if not plide_main_window.close_event.isSet():
-        gtk.threads_enter()
-        plide_main_window.quit()
-        gtk.threads_leave()
-
-def GetWork():
-    return plide_main_window.work_queue.get_work()
-
-def PostWorkResults( request_id, code, results ):
-    plide_main_window.work_queue.post_work_results(request_id, code, results)
-
-def LogAppend( kind, severity, message ):
-    gtk.threads_enter()
-    plide_main_window.log_display( message, kind )
-    gtk.threads_leave()
-    
-
-#####  C++ Progress Bar Interface  ##########################################
-
-def AllocateProgressBar( text ):
-    progress_id = plide_main_window.allocate_progress()
-    # print >>raw_stderr, "Allocating progress bar", progress_id
-    raw_stderr.flush()
-    if progress_id >= 0:
-        gtk.threads_enter()
-        pb = plide_main_window.get_progress_from_id(progress_id)
-        pb.set_text(text)
-        pb.set_fraction(0.0)
-        gtk.threads_leave()
-    return progress_id
-
-def ReleaseProgressBar( progress_id ):
-    # print >>raw_stderr, "Releasing progress bar", progress_id
-    raw_stderr.flush()
-    if progress_id >= 0:
-        gtk.threads_enter()
-        pb = plide_main_window.get_progress_from_id(progress_id)
-        pb.set_fraction(1.0)            # Make it complete on screen
-        gtk.threads_leave()
-        plide_main_window.release_progress( progress_id )
-
-def ProgressUpdate( progress_id, fraction ):
-    # print >>raw_stderr, "Updating progress bar", progress_id,"to fraction",fraction
-    raw_stderr.flush()
-    if progress_id >= 0:
-        gtk.threads_enter()
-        pb = plide_main_window.get_progress_from_id(progress_id)
-        pb.set_fraction(fraction)
-        gtk.threads_leave()
-
-
-#####  Standalone Running  ##################################################
-
-from plearn.io.server import *
-
-class Poubelle(RemotePLearnServer):
-    """
-    Patch to test standalone running while 'slave' mode still exists.
-    """
-    def __init__(self):
-        command= 'plearn server'
-        self.errstm= None
-        try:
-            from subprocess import Popen, PIPE
-            p= Popen([command], shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
-            (to_server, from_server, child_pid) = (p.stdin, p.stdout, p.pid)
-            self.errstm= p.stderr
-        except:
-            to_server, from_server = os.popen2(command, 'b')
-            child_pid = -1
-        RemotePLearnServer.__init__(self,from_server, to_server, pid=child_pid)
-
-
-    def getAllClassnames(self): return self.listClasses()
-    def helpResourcesPath(self,path): return self.setResourcesPathHTML(path)
-    def helpIndex(self): return self.helpIndexHTML()
-    def helpClasses(self): return self.helpClassesHTML()
-    def helpCommands(self): return self.helpCommandsHTML()
-    def helpOnCommand(self, command): return self.helpOnCommandHTML(command)
-    def helpOnClass(self, classname): return self.helpOnClassHTML(classname)
-
-if __name__ == "__main__":
-    #class Poubelle:
-    #    def __getattr__(self, attr): return None
-
-    global plide_main_window
-    global injected
-    injected = Poubelle()
-    
-    StartPlide(streams_to_watch= {injected.errstm.fileno(): 'injected-stderr'})
-    # print >>sys.stderr, "Random stuff to stderr"
-    # print >>sys.stdout, "Random stuff to stdout"
-    # sys.stderr.flush()
-    # sys.stdout.flush()
-
-    # prid = AllocateProgressBar("Simple Progress Text")
-    # time.sleep(1)
-    # ProgressUpdate(prid,0.33)
-    # time.sleep(1)
-    # ProgressUpdate(prid,0.66)
-    # time.sleep(1)
-    # ReleaseProgressBar(prid)
-    # time.sleep(1)
-
-    plide_main_window.close_event.wait()  # Wait for window to be closed
-    QuitPlide()
-    

Copied: tags/finlearn3-august2007/python_modules/plearn/plide/plide.py (from rev 7944, trunk/python_modules/plearn/plide/plide.py)

Deleted: tags/finlearn3-august2007/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-08-07 00:50:45 UTC (rev 7938)
+++ tags/finlearn3-august2007/python_modules/plearn/pyplearn/plargs.py	2007-08-07 20:33:03 UTC (rev 7950)
@@ -1,1221 +0,0 @@
-# plargs.py
-# Copyright (C) 2006 Christian Dorion
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-# Author: Christian Dorion
-"""Management of command-line options.
-
-A custom (and encouraged) practice is to write large PyPLearn scripts
-which behaviour can be modified by command-line arguments, e.g.:: 
-        
-    prompt %> plearn command_line.pyplearn on_cmd_line="somefile.pmat" input_size=10
-
-with the command_line.pyplearn script being::
-
-    #
-    # command_line.pyplearn
-    #
-    from plearn.pyplearn import *
-
-    dataset = pl.AutoVMatrix( specification = plargs.on_cmd_line,
-                              inputsize     = int( plargs.input_size ),
-                              targetsize    = 1
-                              )
-
-    def main():
-        return pl.SomeRunnableObject( dataset  = dataset,
-                                      internal = SomeObject( dataset = dataset ) )
-
-Those command-line arguments are widely refered as L{plargs}, on
-account of this class, troughout the pyplearn mechanism. Note that
-I{unexpected} (see binders and L{namespaces<plnamespace>} hereafter)
-arguments given on the command line are interpreted as strings, so if
-you want to pass integers (int) or floating-point values (float), you
-will have to cast them as above.
-
-To set default values for some arguments, one can use
-L{plarg_defaults<_plarg_defaults>}. For instance::
-
-    # 
-    # command_line_with_defaults.pyplearn
-    #
-    from plearn.pyplearn import *
-
-    plarg_defaults.on_cmd_line = "some_default_file.pmat"
-    plarg_defaults.input_size  = 10
-    dataset = pl.AutoVMatrix( specification = plargs.on_cmd_line,
-                              inputsize     = plargs.input_size,
-                              targetsize    = 1
-                              )
-
-    def main( ):
-        return pl.SomeRunnableObject( dataset  = dataset,
-                                      internal = SomeObject( dataset = dataset ) )
-    
-which won't fail and use C{"some_default_file.pmat"} with C{input_size=10} if::
-
-    prompt %> plearn command_line_with_defaults.pyplearn
-
-is entered. Note that since I{input_size} was defined as an int
-(C{plarg_defaults.input_size = 10}). Even if
-L{plarg_defaults<_plarg_defaults>} is still supported, it is preferable
-to define all arguments' default values through some I{binder}. We
-refer to subclasses of L{plargs} as I{binders} since they bind default
-values to expected/possible command-line arguments, e.g::
-
-    # 
-    # my_script.pyplearn
-    #
-    from plearn.pyplearn.plargs import *
-
-    class Misc(plargs):
-        algo      = "classical"
-        ma_len    = plopt([126, 252],
-                          doc="A list of moving average lenghts to be used "
-                          "during the preprocessing.")
-        n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-    print >>sys.stderr, repr(plargs.algo), repr(plargs.n_inputs)
-    assert plargs.n_inputs==Misc.n_inputs
-
-    print >>sys.stderr, Misc.ma_len
-        
-would print (as first line) C{"classical", 10}, a string and an int, if
-no command-line arguments override those L{plargs}. One can always
-access the I{plarg} through C{plargs} or C{Misc} (i.e. the assertion
-never fails).
-
-Note the use of L{plopt} instances. While these are not mandatory, they
-are very useful and powerful tools which one can use to make his
-scripts clearer and more user-friendly when used within
-U{plide<http://plearn.berlios.de/plide>}. Note that list can be
-provided to as command-line argument in the CSV format, that is::
-
-    prompt %> plearn my_script.pyplearn ma_len=22,63,126,252
-    "classical", 10
-    [22, 63, 126, 252]
-
-While I{binders} allow one to define default values for L{plargs},
-these are limited in the sens that clashes can occur::
-
-    # 
-    # complex_script.pyplearn
-    #
-    from plearn.pyplearn.plargs import *
-    
-    class macd(plargs):
-        ma_len = plopt(252,
-                    doc="The moving average length to be used in the macd model.")
-
-    class PCA(plargs):
-        algo      = "classical"
-        ma_len    = plopt([126, 252],
-                          doc="A list of moving average lenghts to be used "
-                          "during the preprocessing.")
-        n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-    ...
-    #####
-    prompt %> plearn complex_script.pyplearn 
-    Traceback (most recent call last):
-    File "TEST.pyplearn", line 7, in ?
-        class PCA(plargs):
-    File "/home/dorionc/PLearn/python_modules/plearn/pyplearn/plargs.py", line 447, in __new__
-        plopt.define(cls, option, value)
-    File "/home/dorionc/PLearn/python_modules/plearn/pyplearn/plargs.py", line 193, in define
-        raise KeyError(
-    KeyError: "A script should not contain two options of the same name. Clashing
-    definition of plarg 'ma_len' in 'macd' and 'PCA'"
-
-To avoid this type of error, one can L{namespace<plnamespace>} the
-arguments of his script::
-
-    # 
-    # namespaced.pyplearn
-    #
-    from plearn.pyplearn.plargs import *
-    
-    class macd(plnamespace):
-        ma_len = plopt(252,
-                    doc="The moving average length to be used in the macd model.")
-
-    class PCA(plnamespace):
-        algo      = "classical"
-        ma_len    = plopt([126, 252],
-                          doc="A list of moving average lenghts to be used "
-                          "during the preprocessing.")
-        n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-    print >>sys.stderr, macd.ma_len, PCA.algo, PCA.ma_len
-
-    try:
-        print >>sys.stderr, plargs.n_inputs
-    except AttributeError:
-        print >>sys.stderr, \
-              "n_inputs is namespaced, you must access it through 'PCA'"
-
-    #####
-    prompt %> plearn namespaced.pyplearn macd.ma_len=126 PCA.algo=weird
-    252 weird [126, 252]
-    n_inputs is namespaced, you must access it through 'PCA'
-
-Finally, B{note that} the value of C{plargs.expdir} is generated automatically and
-B{can not} be assigned a default value through binders. This behaviour
-aims to standardize the naming of experiment directories. For debugging
-purpose, however, one may provide on command-line an override to
-C{plargs.expdir} value. Otherwise, one can also provide the I{expdir_root}
-command-line argument as a (relative) path where the expdir should be
-found, e.g.::
-
-    prompt %> plearn my_script.pyplearn expdir_root=Debug
-
-would cause experiment to use the expdir::
-
-    Debug/expdir_2006_05_10_07_58_10
-    
-for instance.
-"""
-import copy, inspect, logging, new, re, sys
-from plearn.pyplearn.context import *
-from plearn.utilities.Bindings import Bindings
-
-# Helper functions
-
-def neglected_member(name, value):
-    return (name.startswith('_') 
-            or inspect.ismethod(value) or inspect.isfunction(value)
-            or inspect.isroutine(value) or inspect.isclass(value) )
-
-def list_cast(slist, elem_cast):
-    """Intelligently casts I{slist} string to a list.
-
-    The I{slist} argument can have the following forms::
-
-        - CSV::
-            slist = "1,2,3" => casted = [1, 2, 3]
-        - CSV with brackets::
-            slist = "[hello,world]" => casted = ["hello", "world"]
-        - List of strings::
-            slist = [ "100.0", "102.5" ] => casted = [ 100.0, 102.5 ]
-
-    The element cast is made using the I{elem_cast} argument.
-    """
-    # CSV (with or without brackets)
-    slist = slist.strip()
-    #print >>sys.stderr, repr(slist), type(slist)
-    if isinstance(slist,str):
-        if slist=="":
-            slist = []
-        elif slist.startswith("["):
-            assert slist.endswith("]")
-            slist = eval(slist)
-        else:
-            slist = slist.split(",")
-        return [ elem_cast(e) for e in slist ]
-
-    # List of strings
-    elif isinstance(slist, list):
-        return [ elem_cast(e) for e in slist ]
-
-    else:
-        raise ValueError, "Cannot cast '%s' into a list", str(slist)
-
-def warn(message, category=UserWarning, stacklevel=0):
-    import os
-    pytest_state = os.environ.get("PYTEST_STATE", "")
-    if pytest_state!="Active":        
-        from warnings import warn
-        warn(message, category, stacklevel=stacklevel+3)
-
-#######  Classes to Manage Command-Line Arguments  ############################
-
-class plopt(object):
-    """Typed command-line options with constraints for PLearn.
-    
-    This class provides support for default values, strong type checking,
-    conversion from a string representation, documentation, and constraints
-    on the possible values that the option can take.  The syntax to follow
-    is along the lines of::
-    
-        class MyOptions( plargs_namespace ):
-            plot_stuff  = plopt(True, doc='Whether a cute graph should be plotted')
-            K           = plopt(10, min=5, max=25, doc='Number of neighbors to consider')
-            method      = plopt('classical', choices=['new-kind','classical','crazy'])
-            lags        = plopt([1,22,252], doc='Lags to incorporate as inputs, in days')
-            weight_cols = plopt([], elem_type=int)
-
-    The supported keywords are
-
-        - B{doc}: The help string for that option.
-
-        - B{choices}: A list of values accepted for that option. Trying to
-          set this option's value to anything which is not in that list will cause
-          a ValueError to be raised.
-
-        - B{min, max}: Bounds for numeric values. You can specify only one
-          of the two. Trying to set this option to value out of these bounds will
-          cause a ValueError to be raised.
-
-        - B{free_choices}: A list of suggestions as to what the field should contain.
-          Still, this option can be set to whatever value you wish and a ValueError
-          will never be raised.
-
-        - B{type}: This keyword can and must only be used when the default
-          value is None. Otherwise, it is infered using 'type(value)'.
-
-        - B{elem_type}: For list options, the type of the elements is
-          usually infered from the first element of the list. This keyword
-          may however be used to specify the type for the elements of a
-          list which defaults as empty. If the list defaults as empty
-          I{and} 'elem_type' keyword is not provided, then the elements' type is
-          assumed to be 'str'.
-
-    A plopt I{holder} refers to either a plargs binder or a plnamespace.
-    """
-    
-    unnamed = "### UNNAMED ###"
-
-    def __init__(self, value, **kwargs):
-        self._name  = kwargs.pop("name", self.unnamed)
-        self._doc   = kwargs.pop("doc", '')
-        self._gui   = kwargs.pop("gui", True)
-
-        # type: This keyword can and must only be used when the default
-        # value is None. Otherwise, it is infered using 'type(value)'.
-        assert not ("type" in kwargs and value is not None)
-        if value is None:
-            if "type" not in kwargs:
-                warn("When a plopt value defaults to None, a valid type must be "
-                     "provided using the 'type' keyword argument. "
-                     "String is assumed for now...", FutureWarning, 2)
-                kwargs['type'] = str
-            self._type = kwargs.pop("type")
-        else:
-            self._type = type(value)
-
-        # Keep the remaining kwargs for further use.
-        self._kwargs = kwargs
-
-        # Sanity checks
-        self.checkBounds(value)
-        self.checkChoices(value)
-        self.__default_value = value # The check did not raise: 'value' is valid
-
-    def __str__(self):
-        """Short string representation of a plopt instance.
-
-        Either::
-            plopt(value = DEFAULT_VALUE)
-        or::
-            plopt(value = VALUE [default: DEFAULT_VALUE])
-
-        where the second occurs if the current value is not equal to the default value.
-        """
-        value = self.get()
-        value_str = repr(value)
-        if value != self.__default_value:
-            value_str += " [default: %s]"%repr(self.__default_value)
-        return "plopt(value = %s)"%value_str
-
-    def cast(self, value):
-        casted = None
-        
-        # Special string to bool treatment
-        if self._type is bool and isinstance(value, str):
-            if value == "True":
-                casted = True
-            elif value == "False":
-                casted = False
-            else:
-                raise ValueError("Trying to set value %s to bool-typed option %s"
-                                 %(value, self._name))
-
-        # Special treatment for list option
-        elif self._type is list :
-            elem_type = self._kwargs.pop("elem_type", None) 
-            if elem_type is None:
-                got= self.get()
-                if got!=None and len(got) > 0:
-                    elem_type = type(self.get()[0])
-                else:
-                    elem_type = str
-            casted = list_cast(value, elem_type)
-
-        # Simple type cast
-        else:
-            casted = self._type(value)
-
-        return casted
-    
-    def checkBounds(self, value):
-        """Checks that value lies between 'min' and 'max' bounds parsed from self.kwargs."""
-        minimum = self._kwargs.get("min", None)
-        if minimum is not None and value < minimum:
-            raise ValueError("Option %s (=%s) should greater than %s"
-                             %(self._name, repr(value), repr(minimum)))
-
-        maximum = self._kwargs.get("max", None)
-        if maximum is not None and value > maximum:
-            raise ValueError("Option %s (=%s) should lower than %s"
-                             %(self._name, repr(value), repr(minimum)))
-
-    def checkChoices(self, value):
-        """Checks that 'value' is one of the 'choices' parsed from self.kwargs."""
-        choices = self._kwargs.get("choices", None)
-        if choices is not None and value not in choices:
-            name = ""
-            if self._name != self.unnamed:
-                name = self._name+" " 
-            raise ValueError(
-                "Option %sshould be in choices=%s"%(name, choices))
-
-    def get(self):
-        """Returns the current context override, if any, o/w returns the default value."""
-        plopt_overrides = actualContext(plopt).plopt_overrides
-        if self in plopt_overrides:
-            return plopt_overrides[self]
-        return self.__default_value
-
-    def getBounds(self):
-        minimum = self._kwargs.get("min", None)
-        maximum = self._kwargs.get("max", None)
-        return minimum, maximum
-
-    def getChoices(self):
-        return self._kwargs.get("choices", None)
-
-    def getDefault(self):
-        return self.__default_value
-
-    def getFreeChoices(self):
-        return self._kwargs.get("free_choices", None)
-
-    def getName(self):
-        assert self._name != self.unnamed
-        return self._name
-
-    def getType(self):
-        return self._type
-
-    def getGui(self):
-        return self._gui
-
-    def reset(self):
-        """Simply deletes any override for this plopt in the current context.
-        
-        Note that the actual implementation of C{define()} leads to
-        forgetting the command-line override if C{reset()} is called on the
-        plopt instance!
-        """
-        actualContext(plopt).plopt_overrides.pop(self, None)
-        
-    def set(self, value):
-        """Sets an override for this plopt in the current context"""
-        if not isinstance(value, self._type):
-            value = self.cast(value)
-        # Sanity checks 
-        self.checkBounds(value)
-        self.checkChoices(value)
-    
-        # The previous didn't raise exeption, the 'value' is valid
-        actualContext(plopt).plopt_overrides[self] = value
-
-    def setDefault(self, default):
-        self.__default_value = default
-
-    #######  Static methods  ######################################################
-
-    def addCmdLineOverride(holder_name, optname, value):
-        context = actualContext(plopt)
-        hldr_key = (holder_name, optname)
-        # Use the 'unique' key to store command-line override's value
-        context.plopt_cmdline_overrides[hldr_key] = value
-    addCmdLineOverride = staticmethod(addCmdLineOverride)
-
-    def popCmdLineOverride(actual_holder, optname):
-        context = actualContext(plopt)
-        hldr_key = (actual_holder.__name__, optname)
-        override_value = context.plopt_cmdline_overrides.pop(hldr_key, None)
-
-        # If the option is not namespaced, it may have been provided
-        # without the binder name as prefix. Try accessing the key with
-        # None instead of the holder's name
-        if override_value is None \
-           and not issubclass(actual_holder, plnamespace):
-            hldr_key = (None, optname)
-            override_value = context.plopt_cmdline_overrides.pop(hldr_key, None)
-
-        # Will return None if no option named 'optname' were overridden on
-        # the command line
-        return override_value
-    popCmdLineOverride = staticmethod(popCmdLineOverride)
-
-    def buildClassContext(context):
-        assert not hasattr(context, 'plopt_binders')
-        context.plopt_binders = {}
-
-        assert not hasattr(context, 'plopt_namespaces')
-        context.plopt_namespaces = {}
-
-        assert not hasattr(context, 'plopt_overrides')
-        context.plopt_overrides = {}
-
-        assert not hasattr(context, 'plopt_cmdline_overrides')
-        context.plopt_cmdline_overrides = {}
-    buildClassContext = staticmethod(buildClassContext)
-
-    def closeClassContext(context):
-        exceptions = [ 'FILEBASE', 'FILEPATH', 'TIME', 'DATETIME',
-                       'FILEEXT', 'DIRPATH', 'DATE', 'HOME', 'FILENAME' ]
-
-        del context.plopt_binders
-        del context.plopt_namespaces
-        del context.plopt_overrides
-
-        unused = []
-        for (hldr_name, optname), value in context.plopt_cmdline_overrides.iteritems():
-            if optname in exceptions:
-                continue
-            elif hldr_name is None:
-                unused.append("%s=%s"%(optname, value))
-            else:
-                unused.append("%s.%s=%s"%(hldr_name, optname, value))
-                
-        if unused:
-            from plearn.pyplearn import PyPLearnError
-            raise PyPLearnError(
-                "The following command-line arguments were not expected "
-                "(Misspelled? Or namespaced?): %s" % ", ".join(unused))
-
-        # Finally, delete the list
-        del context.plopt_cmdline_overrides
-    closeClassContext = staticmethod(closeClassContext)    
-
-    def define(holder, option, value):
-        """Typical pattern to set a plopt instance member in 'holder' for the first time."""
-        context = actualContext(plopt)
-
-        # Check if an override was defined through parsing before the
-        # holder existed. Returns None in the case no override exists.
-        cmdline_override = plopt.popCmdLineOverride(holder, option)
-
-        ### Holder-type specific management
-        plopt._inner_define(holder, option, value)
-
-        # Command-line MUST override mandatory plopts
-        if isinstance(value, mandatory_plopt) and cmdline_override is None:
-            from plearn.pyplearn import PyPLearnError
-            raise PyPLearnError(
-                "Mandatory argument %s.%s was not received on command line."
-                %(holder.__name__, value.getName()) )
-        
-        # Overrides the default with the command-line override if any. Note
-        # that this way to proceed leads to forgetting the command-line
-        # override if reset() is called on the plopt instance!!!
-        if cmdline_override is not None:
-            plopt.override(holder, option, cmdline_override)            
-    define = staticmethod(define)
-
-    def _inner_define(holder, option, value):
-        """Holder-type specific management."""
-        context = actualContext(plopt)
-            
-        if issubclass(holder, plargs):
-            # A script should not contain two options of the same name
-            if option in context.plopt_binders:
-                raise KeyError(
-                    "A script should not contain two options of the same name. "
-                    "Clashing definition of plarg '%s' in '%s' and '%s'"%
-                    (option,context.plopt_binders[option].__name__,holder.__name__))
-        
-            # Keep a pointer to the binder in which the option is defined
-            context.plopt_binders[option] = holder
-
-        elif issubclass(holder, plnamespace):
-            # Keep a pointer to the namespace in which the option is defined
-            context.plopt_namespaces[option] = holder
-
-        else:
-            raise TypeError("Holder '%s' is of an unknown type: %s"
-                            % (holder.__name__, type(holder)) )
-
-        # Enforce all 'holder' members to be (named) plopt instances
-        if isinstance(value, plopt):
-            value._name = option
-        else:
-            value = plopt(value, name=option)
-
-        # Acutally sets the plopt in the holder
-        type.__setattr__(holder, option, value)        
-    _inner_define = staticmethod(_inner_define)
-
-    def getHolder(plopt_name):        
-        return actualContext(plopt).plopt_binders.get(plopt_name)
-    getHolder = staticmethod(getHolder)
-
-    def iterator(holder):
-        """Returns an iterator over the plopt instances contained in the I{holder}"""
-        keys = holder.__dict__.keys()
-        keys.sort()
-        return iter([ holder.__dict__[key] for key in keys
-                      if isinstance(holder.__dict__[key], plopt) ])
-    iterator = staticmethod(iterator)
-
-    def optdict(holder):
-        return dict([ (opt.getName(), opt.get()) for opt in plopt.iterator(holder) ])
-    optdict = staticmethod(optdict)
-
-    def override(holder, option, value):
-        """Typical pattern to override the value of an existing plopt instance."""
-        plopt_instance = type.__getattribute__(holder, option)
-        plopt_instance.set(value)
-    override = staticmethod(override)
-
-class mandatory_plopt(plopt):
-    def __init__(self, type, **kwargs):
-        self._type  = type
-        self._name  = kwargs.pop("name", self.unnamed)
-        self._doc   = kwargs.pop("doc", '')        
-
-        # Keep the remaining kwargs for further use.
-        self._kwargs = kwargs
-
-        # For __str__ use only
-        self.__default_value = None
-    
-class plargs(object):
-    """Values read from or expected for PLearn command-line variables.
-
-    The core class of this module. See modules documentation for details on
-    possible uses.
-    """
-    _extensible_ = False
-
-    #######  Static methods  ######################################################
-
-    def buildClassContext(context):
-        assert not hasattr(context, 'binders')
-        context.binders = {}
-    buildClassContext = staticmethod(buildClassContext)
-
-    def getBinders():
-        """Returns a list of all binders in the current context."""
-        context = actualContext(plargs)
-        binder_names = context.binders.keys()
-        binder_names.sort()
-
-        binders = []
-        for bname in binder_names:
-            binders.append(context.binders[bname])        
-        return binders
-    getBinders = staticmethod(getBinders)
-
-    def getContextBindings():
-        """Returns a L{Bindings} instance of plopt name-to-value pairs.
-
-        The bindings could thereafter be used to re-create the current
-        context from a command-line::
-
-            bindings = plargs.getContextBindings()
-            cmdline = [ "%s=%s"%(opt, bindings['opt']) for opt in bindings ]
-            actual_command_line = " ".join(cmdline)
-        """
-        context = actualContext(plargs)
-        bindings = Bindings( )
-
-        for binder in plargs.getBinders():
-            for opt in plopt.iterator(binder):
-                bindings[opt.getName()] = opt.get()
-
-        for namespace in plargs.getNamespaces():
-            for opt in plopt.iterator(namespace):
-                key = "%s.%s"%(namespace.__name__, opt.getName())
-                bindings[key] = opt.get()
-
-        return bindings
-    getContextBindings = staticmethod(getContextBindings)
-
-    def getNamespaces():
-        """Returns a list of all namespaces in the current context."""
-        # Note the (hackish?) use of plnamespace. Needed to ensure the
-        # existance of context.namepaces...
-        context = actualContext(plnamespace) 
-        nsp_names = context.namespaces.keys()
-        nsp_names.sort()        
-        return [ context.namespaces[nsp] for nsp in nsp_names ]
-    getNamespaces = staticmethod(getNamespaces)
-
-    def getHolder(holder_name):
-        holder = plnamespace.getHolder(holder_name)
-        if holder is None:
-            holder = actualContext(plargs).binders.get(holder_name)
-        return holder
-    getHolder = staticmethod(getHolder)
-
-    def parse(*args):
-        """Parses a list of argument strings."""
-        if len(args)==1 and isinstance(args[0], list):
-            args = args[0]
-
-        context = actualContext(plargs)
-        for statement in args:
-            assert isinstance(statement, str), statement
-
-            option, value = statement.split('=', 1)
-
-            option = option.strip()
-            value  = value.strip()
-
-            if option=="expdir":
-                context._expdir_ = value
-                continue
-
-            if option=="expdir_root":
-                context._expdir_root_ = value
-                continue            
-
-            try:
-                holder_name, option = option.split('.')
-                holder = plargs.getHolder(holder_name)
-
-            # option.split('.') did not return a pair, i.e. not dot in option
-            except ValueError:
-                holder_name, holder = None, plopt.getHolder(option)
-
-            # Was the holder for that option defined?
-            if holder is None:
-                plopt.addCmdLineOverride(holder_name, option, value)
-            else:
-                setattr(holder, option, value)
-    parse = staticmethod(parse)
-    
-    #######  Metaclass  ###########################################################
-    
-    class __metaclass__(type):
-        """Overrides the attribute management behavior."""
-        def __new__(metacls, clsname, bases, dic):
-            cls = type.__new__(metacls, clsname, bases, dic)
-            plargs = cls
-            if clsname != "plargs":
-                plargs = globals()['plargs']
-
-            context = actualContext(plargs)
-    
-            # Keep track of binder subclass
-            if clsname != "plargs":
-                context.binders[clsname] = cls
-
-            # Introspection of the subclasses
-            if cls is not plargs:
-                for option, value in dic.iteritems():                    
-                    if neglected_member(option, value):
-                        continue
-                    
-                    # Define the plopt instance
-                    plopt.define(cls, option, value)
-
-            return cls
-
-        def __setattr__(cls, option, value):
-            if option == "expdir":
-                raise AttributeError("Cannot modify the value of 'expdir'.")
-
-            plargs = cls
-            if cls.__name__ != "plargs":
-                plargs = globals()['plargs']
-
-            context = actualContext(plargs)
-            if cls is plargs:
-                raise AttributeError(
-                    "Can't set option '%s' directly on plargs. "
-                    "Proceed trough the binder or namespace." % option)
-
-            try:                    
-                plopt.override(cls, option, value)
-            except AttributeError, err:
-                if cls._extensible_:
-                    plopt.define(cls, option, value)
-                else:
-                    raise AttributeError(
-                        "Binder %s does not contain a plopt instance named %s. "
-                        "One can't set a value to an undefined option. (%s)"
-                        %(cls.__name__, option, err))
-
-        def __getattribute__(cls, key):
-            attr = None
-            try:
-                attr = type.__getattribute__(cls, key)
-                if key.startswith('_') or not isinstance(attr, plopt):
-                    return type.__getattribute__(cls, key)
-            except AttributeError:
-                pass
-
-            # The key should map to a plopt instance, we need the context...
-            plargs = cls
-            if cls.__name__ != "plargs":
-                plargs = globals()['plargs']
-
-            # Special management of expdir plarg
-            if key == "expdir":
-                return actualContext(plargs).getExpdir()
-            elif key == "expdir_root":
-                return actualContext(plargs).getExpdirRoot()
-
-            # Find to holder to which option belongs
-            holder = cls
-            if cls is plargs:
-                try:
-                    holder = actualContext(plargs).plopt_binders[key]
-                except KeyError:
-                    raise AttributeError("Unknown option '%s'. Is it namespaced?"%key)
-
-            # We now have a valid holder: dig out the plopt *value*
-            plopt_instance = type.__getattribute__(holder, key)
-            assert isinstance(plopt_instance, plopt)
-            return plopt_instance.get()
-
-# For backward compatibility
-class _plarg_defaults:
-    """Magic class to L{contextualize<context>} I{plarg_defaults}.
-
-    The I{plarg_defaults} object exists mainly for backward compatibility
-    reasons. It was (is) meant to store a default value for some command
-    argument so that::
-
-        plarg_defaults.algo = "classical"
-        print plargs.algo
-
-    would print "classical" even if no command line argument I{algo} is encountered.
-
-    For clarity sakes, we suggest that one now regroup all default values
-    he wishes to set for "plargs" in some L{binder<plargs>}, e.g.::
-
-        class Misc(plargs):
-            algo      = "classical"
-            n_inputs  = 10
-            n_outputs = 2
-
-        print plargs.algo
-
-    The behavior will be the same, but this syntax allows to I{highlight}
-    the definition of default values while being more aligned with the new
-    generation of U{plargs}.
-    """
-    def _getBinder(self):
-        binders = actualContext(plargs).binders
-        if 'plarg_defaults' not in binders:
-            binders['plarg_defaults'] = \
-                new.classobj('plarg_defaults', (plargs,), {'_extensible_' : True})
-        return binders['plarg_defaults']
-
-    def __getattribute__(self, option):
-        return getattr(self._getBinder(), option)
-
-    def __setattr__(self, option, value):
-        defaults = self._getBinder()
-
-        # Say the default was value of a given was set to 'D'. It, the
-        # *default* value, can well be set modified later to 'E'. But we
-        # want this to modify the script behaviour IFF the option's value
-        # was *not* overriden from the command line. A simple call to
-        # 'setattr' would discard the command-line override!
-        if hasattr(defaults, option):
-            attr_plopt = object.__getattribute__(defaults,option)
-            attr_plopt.setDefault(value)
-            return
-
-        setattr(defaults, option, value)
-plarg_defaults = _plarg_defaults()
-        
-class plnamespace:
-    """Avoiding name clashes for L{plargs}.
-
-    Alike binders, L{plnamespace} subclasses allow one to define the value
-    of expected L{plargs}, but plarg names are encapsulated in namespaces. This
-    allows many options to have the same name, as long as they are not in
-    the same namespace::
-
-        # 
-        # namespaced.pyplearn
-        #
-        from plearn.pyplearn.plargs import *
-        
-        class macd(plnamespace):
-            ma_len = plopt(252,
-                        doc="The moving average length to be used in the macd model.")
-
-        class PCA(plnamespace):
-            algo      = "classical"
-            ma_len    = plopt([126, 252],
-                              doc="A list of moving average lenghts to be used "
-                              "during the preprocessing.")
-            n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-        print >>sys.stderr, macd.ma_len, PCA.algo, PCA.ma_len
-
-        try:
-            print >>sys.stderr, plargs.n_inputs
-        except AttributeError:
-            print >>sys.stderr, \
-                  "n_inputs is namespaced, you must access it through 'PCA'"
-
-        #####
-        prompt %> plearn namespaced.pyplearn macd.ma_len=126 PCA.algo=weird
-        252 weird [126, 252]
-        n_inputs is namespaced, you must access it through 'PCA'
-    """    
-    def buildClassContext(context):
-        assert not hasattr(context, 'namespaces')
-        context.namespaces = {}
-    buildClassContext = staticmethod(buildClassContext)
-
-    def getHolder(holder_name):
-        return actualContext(plnamespace).namespaces.get(holder_name)
-    getHolder = staticmethod(getHolder)
-
-    def getPlopt(cls, optname):
-        return super(cls, cls).__getattribute__(cls, optname)
-    getPlopt = classmethod(getPlopt)
-
-    def inherit(namespace):
-        """A deep-copy driven inheritance-like mechanism.
-
-        In the context of plnamespace, usual (Python) inheritance is not
-        satisfactory. Indeed, the options defined in some base class will be
-        shared among subclasses. However, when one would want to subclass a
-        plnamespace, it is more likely the he want the 'subclass' to have
-        options 'of the same name' than the ones in the base-class but
-        still independent.
-
-        This method provide a concise way to enact this inheritance-like
-        relationship between some New and some Existing namespaces
-
-            class New(plnamespace):
-                __metaclass__ = plnamespace.inherit(Existing)
-
-                other_option  = plopt("Other",
-                                      doc="An option that is not in the base class.")
-
-        Note that if 'Existing.some_option=VALUE' is overriden through the command-line,
-        the value of 'New.some_option' will be VALUE unless explicitely overrode. 
-        """
-        META = namespace.__metaclass__
-        class __metaclass__(META):
-            def __new__(metacls, clsname, bases, dic):
-                # Do not use plopt.optdict: the documentation, choices and other
-                # property would be lost...
-                for opt in plopt.iterator(namespace):
-                    if not opt.getName() in dic:
-                        inh_opt = copy.deepcopy(opt)
-                        inh_opt.set( opt.get() )
-                        dic[inh_opt.getName()] = inh_opt
-                    
-                #OLD: optdict = dict([ (
-                #OLD:     opt.getName(), opt) for opt in plopt.iterator(namespace) ])
-                #OLD: dic.update( copy.deepcopy(optdict) )
-                cls = META.__new__(metacls, clsname, bases, dic)
-                return cls        
-        return __metaclass__
-    inherit = staticmethod(inherit)
-        
-    class __metaclass__(type):
-        def __new__(metacls, clsname, bases, dic):
-            cls = type.__new__(metacls, clsname, bases, dic)
-            if clsname != "plnamespace":
-                context = actualContext(globals()["plnamespace"])
-                context.namespaces[clsname] = cls
-
-                for option, value in dic.iteritems():
-                    if neglected_member(option, value):
-                        continue
-                    # Define the plopt instance
-                    plopt.define(cls, option, value)
-            return cls
-
-        def __getattribute__(cls, key):
-            if key.startswith('_'):
-                return type.__getattribute__(cls, key)
-
-            attr = type.__getattribute__(cls, key)
-            try:
-                plopt_instance = attr
-                assert isinstance(plopt_instance, plopt)
-                return plopt_instance.get()
-            except AssertionError:
-                assert inspect.ismethod(attr) or inspect.isfunction(attr)
-                return attr
-
-        def __setattr__(cls, key, value):
-            if key.startswith('_') \
-               or (hasattr(cls, key) and callable(getattr(cls,key))):
-                type.__setattr__(cls,key,value)
-            else:
-                try:
-                    plopt.override(cls, key, value)
-                except AttributeError:
-                    raise AttributeError(
-                        "Namespace %s does not contain a plopt instance named %s. "
-                        "One can't set a value to an undefined option."%(cls.__name__, key))
-
-class _TmpHolder:
-    def __init__(self, holder_name):
-        self._name = holder_name
-
-    def checkConsistency(self, holder):
-        from plearn.pyplearn import PyPLearnError
-        if self._name is None:
-            assert not issubclass(holder, plnamespace)
-        else:
-            assert holder.__name__ == self._name
-
-# class _TmpHolderMap(dict):
-#     def __getitem__(self, key):
-#         if not key in self:
-#             self.__setitem__(key, [])
-#         return super(ListMap, self).__getitem__(key)
-    
-
-#######  For backward compatibily: will be deprecated soon  ###################
-
-class plargs_binder(plargs):
-    class __metaclass__(plargs.__metaclass__):
-        def __new__(metacls, clsname, bases, dic):            
-            if clsname=="plargs_binder":
-                return type.__new__(metacls, clsname, bases, dic)
-            warn("plargs_binder is deprecated. Inherit directly from plargs instead.",
-                 DeprecationWarning)
-            return plargs.__metaclass__.__new__(metacls, clsname, bases, dic)
-
-class plargs_namespace(plnamespace):
-    class __metaclass__(plnamespace.__metaclass__):
-        def __new__(metacls, clsname, bases, dic):
-            if clsname=="plargs_namespace":
-                return type.__new__(metacls, clsname, bases, dic)
-            warn("plargs_namespace is deprecated. Inherit from plnamespace instead.",
-                 DeprecationWarning)
-            return plnamespace.__metaclass__.__new__(metacls, clsname, bases, dic)
-
-
-#######  Unit Tests: invoked from outside  #####################################
-
-def printCurrentContext(out=sys.stdout):
-    print >>out, "Expdir:", plargs.expdir
-    
-    for binder in plargs.getBinders():
-        print >>out, "Binder:", binder.__name__
-        for opt in plopt.iterator(binder):
-            print >>out, '   ',opt.getName()+':', opt
-        print >>out, ''
-    
-    for namespace in plargs.getNamespaces():
-        print >>out, "Namespace:", namespace.__name__
-        for opt in plopt.iterator(namespace):
-            print >>out, '   ',opt.getName()+':', opt
-
-def test_plargs():                
-    print "#######  Binders  #############################################################\n"
-    class binder(plargs):
-        c = "c"
-        d = "d"
-        e = "e"
-        f = "f"
-
-    def bCheck(attr):
-        plargs_attr = getattr(plargs, attr)
-        binder_attr = getattr(binder, attr)
-        assert not isinstance(binder_attr, plopt)
-
-        print "Access through plargs:", plargs_attr        
-        print "Direct access:", binder_attr
-        assert plargs_attr==binder_attr
-
-        # binder_plopt = binder.getPlopt(attr)
-        # print "Access to plopt:", "TO BE DONE" # binder_plopt
-        print 
-        
-
-    ### Untouched
-    print "+++ Untouched plarg\n"
-    bCheck('c')
-
-    ### Standard assignment
-    print "+++ Standard assignment through binder\n"
-    binder.d = "Youppi"
-    bCheck('d')
-
-    ### Subclass propagation
-    print "+++ Setting binded option using 'dotted' key\n"
-    plargs.parse("binder.e = ** E **")
-    bCheck('e')
-
-    print "#######  Namespaces  ##########################################################\n"
-    class n(plnamespace):    
-        namespaced = "within namespace n"
-
-    def nCheck():
-        assert not isinstance(n.namespaced, plopt)
-        print "Direct access:", n.namespaced
-
-        # n_plopt = n.getPlopt('namespaced')
-        # print "Access to plopt:", "TO BE DONE" # n_plopt
-        print 
-
-    ### Untouched
-    nCheck()
-    
-    ### Standard assignments
-    print "+++ Standard assignment through namespace\n"
-    n.namespaced = "WITHIN NAMESPACE n"
-    nCheck()
-    
-    ### Subclass propagation
-    plargs.parse("n.namespaced=FROM_SETATTR")
-    nCheck()    
-
-    print "#######  Contexts Management  #################################################\n"
-
-    print "+++ Context 1"
-    first_context = getCurrentContext()
-    printCurrentContext()
-    print 
-
-    print "+++ Context 2"        
-    second_context = createNewContext()
-    plarg_defaults.algo = "classical"
-
-    print "-- Before creation of the new 'n' plnamespace:"
-    print n.namespaced
-    printCurrentContext()
-    print
-    
-    class n(plnamespace):
-        namespaced = "NEW NAMESPACED ATTR"
-
-    print "-- After creation of the new 'n' plnamespace:"
-    print n.namespaced
-    printCurrentContext()
-    print
-
-
-    print "+++ Back to Context 1"
-    setCurrentContext(first_context)
-    printCurrentContext()
-    print 
-
-def test_plargs_parsing():
-    def readScript(*command_line):
-        context_handle = createNewContext()
-        
-        plargs.parse(*command_line)
-
-        class MyBinder(plargs):
-            binded1 = "binded1"
-            binded2 = "binded2"
-            binded3 = "binded3"
-
-        class MyNamespace(plnamespace):
-            namespaced1 = "namespaced1"
-            namespaced2 = "namespaced2"
-            namespaced3 = "namespaced3"
-
-        header = "Context %d"%context_handle
-        header += '\n'+("="*len(header))
-        print header
-        printCurrentContext()
-        print        
-        return context_handle, header
-    
-    contexts = [ readScript() ]
-    contexts.append(
-        # list argument
-        readScript(["binded1=BINDED1", "binded2=BINDED2",
-                    "MyNamespace.namespaced3=NAMESPACED3"]) )
-    contexts.append(
-          # string arguments
-          readScript("binded3=BINDED3", "MyNamespace.namespaced1=NAMESPACED1") )
-
-    print 
-    print "Reprint all contexts to ensure nothing was lost or corrupted."
-    print "-------------------------------------------------------------"
-    print 
-    for c, header in contexts:
-        setCurrentContext(c)
-        print header
-        printCurrentContext()
-        closeCurrentContext()
-        print 
-
-    print 
-    print "Misspelled command-line argument"
-    print "-------------------------------------------------------------"
-    print
-    from plearn.pyplearn import PyPLearnError
-    try:
-        readScript(["bindd1=BINDED1", "binded2=BINDED2",
-                    "MyNamespace.namespaced3=NAMESPACED3"])
-        closeCurrentContext()        
-    except PyPLearnError, err:
-        print 
-        print 'PyPLearnError:', err
-
-
-def test_mandatory_plargs(*command_line):
-    from plearn.pyplearn import PyPLearnError
-    plargs.parse(*command_line)
-
-    try:
-        class MyBinder(plargs):
-            mandatory = mandatory_plopt(int,
-                                        doc="Some mandatory int command line argument")
-        
-        print MyBinder.mandatory
-
-    except PyPLearnError, err:
-        print err
-
-def test_misspelled_plargs():
-    from plearn.pyplearn import PyPLearnError
-
-    plargs.parse("binded1=[]", "binded2=[ 2 ]", "namespace='misspelled'")
-
-    class Binder(plargs):
-        binded1 = [ 1 ]
-        binded2 = plopt([], elem_type=int)
-
-    class Namespace(plnamespace):
-        namespaced = "N"
-
-    print Binder.binded1, Binder.binded2, Namespace.namespaced
-
-    from plearn.pyplearn import PyPLearnError
-    try:
-        closeCurrentContext()
-    except PyPLearnError, err:
-        print err

Copied: tags/finlearn3-august2007/python_modules/plearn/pyplearn/plargs.py (from rev 7939, trunk/python_modules/plearn/pyplearn/plargs.py)



From yoshua at mail.berlios.de  Wed Aug  8 16:56:28 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 8 Aug 2007 16:56:28 +0200
Subject: [Plearn-commits] r7951 - trunk/plearn_learners/online
Message-ID: <200708081456.l78EuS8H012874@sheep.berlios.de>

Author: yoshua
Date: 2007-08-08 16:56:28 +0200 (Wed, 08 Aug 2007)
New Revision: 7951

Added:
   trunk/plearn_learners/online/VBoundDBN2.cc
   trunk/plearn_learners/online/VBoundDBN2.h
Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Adding a new OnlineLearningModule VBoundDBN2 (in construction)
and changes to RBMModule to make it easier to use its workings.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-07 20:33:03 UTC (rev 7950)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-08 14:56:28 UTC (rev 7951)
@@ -968,29 +968,7 @@
                      "If neg_log_phidden is provided, it must have the same length as hidden.state");
             PLASSERT_MSG(neg_log_phidden->width()==1,"neg_log_phidden must have width 1 (single column)");
         }
-        computeVisibleActivations(*hidden,true);
-        int n_h = hidden->length();
-        int T = visible->length();
-        real default_neg_log_ph = safelog(real(n_h)); // default P(h)=1/Nh: -log(1/Nh) = log(Nh)
-        Vec old_act = visible_layer->activation;
-        neg_log_pvisible_given_phidden->resize(T,1);
-        for (int t=0;t<T;t++)
-        {
-            Vec x_t = (*visible)(t);
-            real log_p_xt=0;
-            for (int i=0;i<n_h;i++)
-            {
-                visible_layer->activation = visible_layer->activations(i);
-                real neg_log_p_xt_given_hi = visible_layer->fpropNLL(x_t);
-                real neg_log_p_hi = neg_log_phidden?(*neg_log_phidden)(i,0):default_neg_log_ph;
-                if (i==0)
-                    log_p_xt = -(neg_log_p_xt_given_hi + neg_log_p_hi);
-                else
-                    log_p_xt = logadd(log_p_xt, -(neg_log_p_xt_given_hi + neg_log_p_hi));
-            }
-            (*neg_log_pvisible_given_phidden)(t,0) = -log_p_xt;
-        }
-        visible_layer->activation = old_act;
+        computeNegLogPVisibleGivenPHidden(*visible,*hidden,neg_log_phidden,*neg_log_pvisible_given_phidden);
         found_a_valid_configuration = true;
     }
 
@@ -1219,6 +1197,33 @@
 
 }
 
+void RBMModule::computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat neg_log_pvisible_given_phidden)
+{
+    computeVisibleActivations(hidden,true);
+    int n_h = hidden->length();
+    int T = visible->length();
+    real default_neg_log_ph = safelog(real(n_h)); // default P(h)=1/Nh: -log(1/Nh) = log(Nh)
+    Vec old_act = visible_layer->activation;
+    neg_log_pvisible_given_phidden.resize(T,1);
+    for (int t=0;t<T;t++)
+    {
+        Vec x_t = visible(t);
+        real log_p_xt=0;
+        for (int i=0;i<n_h;i++)
+        {
+            visible_layer->activation = visible_layer->activations(i);
+            real neg_log_p_xt_given_hi = visible_layer->fpropNLL(x_t);
+            real neg_log_p_hi = neg_log_phidden?(*neg_log_phidden)(i,0):default_neg_log_ph;
+            if (i==0)
+                log_p_xt = -(neg_log_p_xt_given_hi + neg_log_p_hi);
+            else
+                log_p_xt = logadd(log_p_xt, -(neg_log_p_xt_given_hi + neg_log_p_hi));
+        }
+        neg_log_pvisible_given_phidden(t,0) = -log_p_xt;
+    }
+    visible_layer->activation = old_act;
+}
+
 ////////////////////
 // bpropAccUpdate //
 ////////////////////

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-07 20:33:03 UTC (rev 7950)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-08 14:56:28 UTC (rev 7951)
@@ -277,6 +277,7 @@
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
 
+public:
     //! Compute activations on the hidden layer based on the provided
     //! visible input.
     //! If 'hidden_bias' is not null nor empty, then it is used as an
@@ -324,6 +325,8 @@
 
     void computePartitionFunction();
 
+    void computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat neg_log_pvisible_given_phidden);
+
 private:
     //#####  Private Member Functions  ########################################
 

Added: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-07 20:33:03 UTC (rev 7950)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-08 14:56:28 UTC (rev 7951)
@@ -0,0 +1,316 @@
+// -*- C++ -*-
+
+// VBoundDBN2.cc
+//
+// Copyright (C) 2007 yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: yoshua Bengio
+
+/*! \file VBoundDBN2.cc */
+
+
+
+#include "VBoundDBN2.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    VBoundDBN2,
+    "2-RBM DBN trained using Hinton's new variational bound of global likelihood",
+    "The bound that is maximized is the following:\n"
+    " log P(x) >= -FE1(x) + E_{P1(h|x)}[ FE1(h) - FE2(h) ] - log Z2\n"
+    "where P1 and P2 are RBMs with Pi(x) = exp(-FEi(x))/Zi.\n"
+);
+
+//////////////////
+// VBoundDBN2 //
+//////////////////
+VBoundDBN2::VBoundDBN2()
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void VBoundDBN2::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "rbm1", &VBoundDBN2::rbm1,
+                  OptionBase::buildoption,
+                  "First RBM, taking the DBN's input in its visible layer");
+    declareOption(ol, "rbm2", &VBoundDBN2::rbm1,
+                  OptionBase::buildoption,
+                  "Second RBM, producing the DBN's output and generating internal representations.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void VBoundDBN2::build_()
+{
+    if (random_gen)
+    {
+        if (rbm1 && !rbm1->random_gen)
+        {
+            rbm1->random_gen = random_gen;
+            rbm1->build();
+            rbm1->forget();
+        }
+        if (rbm2 && !rbm2->random_gen)
+        {
+            rbm2->random_gen = random_gen;
+            rbm2->build();
+            rbm2->forget();
+        }
+    }
+    ports.append("input"); // 0
+    ports.append("bound"); // 1
+    ports.append("nll"); // 2
+    ports.append("sampled_h"); // 3
+    ports.append("global_improvement"); // 4
+    ports.append("ph_given_v"); // 5
+}
+
+///////////
+// build //
+///////////
+void VBoundDBN2::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void VBoundDBN2::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() && ports_gradient.length() == nPorts());
+    PLASSERT( rbm1 && rbm2);
+
+    Mat* input = ports_value[0];
+    Mat* sampled_h_ = ports_value[3]; // a state if input is given
+    Mat* global_improvement_ = ports_value[4]; // a state if input is given
+    Mat* ph_given_v_ = ports_value[5]; // a state if input is given
+
+
+    // Ensure all required gradients have been computed.
+    checkProp(ports_gradient);
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+// the default implementation returns false
+bool VBoundDBN2::bpropDoesNothing()
+{
+}
+*/
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void VBoundDBN2::finalize()
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void VBoundDBN2::forget()
+{
+    PLASSERT(rbm1 && rbm2);
+    rbm1->forget();
+    rbm2->forget();
+}
+
+///////////
+// fprop //
+///////////
+void VBoundDBN2::fprop(const TVec<Mat*>& ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( rbm1 && rbm2);
+
+    Mat* input = ports_value[0];
+    Mat* bound = ports_value[1];
+    Mat* nll = ports_value[2];
+    Mat* sampled_h_ = ports_value[3]; // a state if input is given
+    Mat* global_improvement_ = ports_value[4]; // a state if input is given
+    Mat* ph_given_v_ = ports_value[5]; // a state if input is given
+
+    // fprop has two modes:
+    //  1) input is given (presumably for learning, or measuring bound or nll)
+    //  2) input is not given and we want to generate one
+    //
+
+    // for learning or testing
+    if (input && !input->isEmpty()) 
+    {
+        int mbs=input->length();
+        FE1v.resize(mbs,1);
+        FE1h.resize(mbs,1);
+        FE2h.resize(mbs,1);
+        Mat* sampled_h = sampled_h_?sampled_h_:&sampled_h_state;
+        Mat* global_improvement = global_improvement_?global_improvement_:&global_improvement_state;
+        Mat* ph_given_v = ph_given_v_?ph_given_v_:&ph_given_v_state;
+        sampled_h->resize(mbs,rbm1->hidden_layer->size);
+        global_improvement->resize(mbs,1);
+        ph_given_v->resize(mbs,rbm1->hidden_layer->size);
+
+        // compute things needed for everything else 
+    
+        rbm1->sampleHiddenGivenVisible(*input);
+        *ph_given_v << rbm1->hidden_layer->getExpectations();
+        *sampled_h << rbm1->hidden_layer->samples;
+        rbm1->visible_layer->fpropNLL(*sampled_h,neglogphsample_given_v);
+        rbm1->computeFreeEnergyOfVisible(*input,FE1v);
+        rbm1->computeFreeEnergyOfHidden(*sampled_h,FE1h);
+        rbm2->computeFreeEnergyOfVisible(*sampled_h,FE2h);
+        substract(FE1h,FE2h,*global_improvement);
+
+        if (bound) // actually minus the bound, to be in same units as nll, only computed exactly during test
+        {
+            PLASSERT(bound->isEmpty());
+            bound->resize(mbs,1);
+
+            if (rbm2->partition_function_is_stale && !during_training)
+                rbm2->computePartitionFunction();
+            *bound << FE1v;
+            *bound -= *global_improvement;
+            *bound += rbm2->log_partition_function;
+        }
+        if (nll) // exact -log P(input) = - log sum_h P2(h) P1(input|h)
+        {
+            int n_h_configurations = 1 << rbm1->hidden_layer->size;
+            if (all_h.length()!=n_h_configurations || all_h.width()!=rbm1->hidden_layer->size)
+            {
+                all_h.resize(n_h_configurations,rbm1->hidden_layer->size);
+                for (int c=0;c<n_h_configurations;c++)
+                {
+                    int N=c;
+                    for (int i=0;i<rbm1->hidden_layer->size;i++)
+                    {
+                        all_h(c,i) = N&1;
+                        N >>= 1;
+                    }
+                }
+            }
+            // compute -log P2(h) for each possible h configuration
+            if (rbm2->partition_function_is_stale && !during_training)
+                rbm2->computePartitionFunction();
+            neglogP2h.resize(n_h_configurations,1);
+            rbm2->computeFreeEnergyOfVisible(all_h,neglogP2h,false);
+            rbm1->computeNegLogPVisibleGivenPHidden(*input,all_h,&neglogP2h,*nll);
+        }
+    }
+    // Ensure all required ports have been computed.
+    checkProp(ports_value);
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+/* Optional
+int VBoundDBN2::getPortIndex(const string& port)
+{}
+*/
+
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& VBoundDBN2::getPorts() {
+    return ports;
+}
+
+//////////////////
+// getPortSizes //
+//////////////////
+const TMat<int>& VBoundDBN2::getPortSizes() {
+    TMat<int> sizes(nPorts(),2);
+    sizes.fill(-1);
+    return sizes;
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void VBoundDBN2::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    deepCopyField(rbm1, copies);
+    deepCopyField(rbm2, copies);
+    deepCopyField(FE1v, copies);
+    deepCopyField(FE1h, copies);
+    deepCopyField(FE2h, copies);
+    deepCopyField(sampled_h_state, copies);
+    deepCopyField(global_improvement_state, copies);
+    deepCopyField(ph_given_v_state, copies);
+    deepCopyField(neglogphsample_given_h, copies);
+    deepCopyField(all_h, copies);
+    deepCopyField(all_h, copies);
+    deepCopyField(neglogP2h, copies);
+    deepCopyField(ports, copies);
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+// The default implementation raises a warning and does not do anything.
+void VBoundDBN2::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
+
+}
+// end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/VBoundDBN2.h
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.h	2007-08-07 20:33:03 UTC (rev 7950)
+++ trunk/plearn_learners/online/VBoundDBN2.h	2007-08-08 14:56:28 UTC (rev 7951)
@@ -0,0 +1,312 @@
+// -*- C++ -*-
+
+// VBoundDBN2.h
+//
+// Copyright (C) 2007 yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: yoshua Bengio
+
+/*! \file VBoundDBN2.h */
+
+
+#ifndef VBoundDBN2_INC
+#define VBoundDBN2_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMModule.h>
+
+namespace PLearn {
+
+/**
+ * 2-RBM DBN trained using Hinton's new variational bound of global likelihood:
+ * 
+ * log P(x) >= -FE1(x) + E_{P1(h|x)}[ FE1(h) - FE2(h) ] - log Z2
+ *
+ * where P1 and P2 are RBMs with Pi(x) = exp(-FEi(x))/Zi.
+ *
+ */
+class VBoundDBN2 : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    PP<RBMModule> rbm1;
+    PP<RBMModule> rbm2;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    VBoundDBN2();
+
+    // Your other public member functions go here
+
+    //! Perform a fprop step.
+    //! Each Mat* pointer in the 'ports_value' vector can be one of:
+    //! - a full matrix: this is data that is provided to the module, and can
+    //!                  be used to compute other ports' values
+    //! - an empty matrix: this is what we want to compute
+    //! - a NULL pointer: this is data that is not available, but whose value
+    //!                   does not need to be returned (or even computed)
+    //! The default version will either:
+    //! - call the mini-batch versions of standard fprop if 'ports_value' has
+    //!   size 2, with the first value being provided (and the second being
+    //!   the desired output)
+    //! - crash otherwise
+    void fprop(const TVec<Mat*>& ports_value);
+
+    //! Perform a back propagation step (also updating parameters according to
+    //! the provided gradient).
+    //! The matrices in 'ports_value' must be the same as the ones given in a
+    //! previous call to 'fprop' (and thus they should in particular contain
+    //! the result of the fprop computation). However, they are not necessarily
+    //! the same as the ones given in the LAST call to 'fprop': if there is a
+    //! need to store an internal module state, this should be done using a
+    //! specific port to store this state.
+    //! Each Mat* pointer in the 'ports_gradient' vector can be one of:
+    //! - a full matrix  : this is the gradient that is provided to the module,
+    //!                    and can be used to compute other ports' gradient.
+    //! - an empty matrix: this is a gradient we want to compute and accumulate
+    //!                    into. This matrix must have length 0 and a width
+    //!                    equal to the width of the corresponding matrix in
+    //!                    the 'ports_value' vector (we can thus accumulate
+    //!                    gradients using PLearn's ability to keep intact
+    //!                    stored values when resizing a matrix' length).
+    //! - a NULL pointer : this is a gradient that is not available, but does
+    //!                    not need to be returned (or even computed).
+    //! The default version tries to use the standard mini-batch bpropUpdate
+    //! method, when possible.
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    /* Optional
+
+    //! Given the input, compute the output (possibly resize it appropriately)
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Given a batch of inputs, compute the outputs
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Mat& inputs, Mat& outputs);
+    */
+
+    /* Optional
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! Batch version
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    //! Batch version
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             const Mat& output_gradients);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    /* Optional
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
+    //! Return the list of ports in the module.
+    //! The default implementation returns a pair ("input", "output") to handle
+    //! the most common case.
+    virtual const TVec<string>& getPorts();
+
+    /* Optional
+    //! Return the size of all ports, in the form of a two-column matrix, where
+    //! each row represents a port, and the two numbers on a row are
+    //! respectively its length and its width (with -1 representing an
+    //! undefined or variable value).
+    //! The default value fills this matrix with:
+    //!     - in the first column (lengths): -1
+    //!     - in the second column (widths):
+    //!         - -1 if nPorts() < 2
+    //!         - 'input_size' for the first row and 'output_size' for the
+    //!           second row if nPorts() >= 2
+    virtual const TMat<int>& getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    //  ### Default implementation performs a simple linear search in
+    //  ### getPorts().
+    virtual int getPortIndex(const string& port);
+    */
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(VBoundDBN2);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+    Mat FE1v, FE1h, FE2h;
+    Mat sampled_h_state;
+    Mat global_improvement_state;
+    Mat ph_given_v_state;
+    Mat neglogphsample_given_v;
+    Mat all_h; // for computing exact likelihood
+    Mat neglogP2h;
+    TVec<string> ports;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(VBoundDBN2);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From yoshua at mail.berlios.de  Wed Aug  8 16:58:45 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 8 Aug 2007 16:58:45 +0200
Subject: [Plearn-commits] r7952 - trunk/plearn_learners/online
Message-ID: <200708081458.l78EwjbO013047@sheep.berlios.de>

Author: yoshua
Date: 2007-08-08 16:58:44 +0200 (Wed, 08 Aug 2007)
New Revision: 7952

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Oops typo corrected in committed RBMModule.cc


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-08 14:56:28 UTC (rev 7951)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-08 14:58:44 UTC (rev 7952)
@@ -1200,8 +1200,8 @@
 void RBMModule::computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat neg_log_pvisible_given_phidden)
 {
     computeVisibleActivations(hidden,true);
-    int n_h = hidden->length();
-    int T = visible->length();
+    int n_h = hidden.length();
+    int T = visible.length();
     real default_neg_log_ph = safelog(real(n_h)); // default P(h)=1/Nh: -log(1/Nh) = log(Nh)
     Vec old_act = visible_layer->activation;
     neg_log_pvisible_given_phidden.resize(T,1);



From yoshua at mail.berlios.de  Wed Aug  8 17:01:48 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 8 Aug 2007 17:01:48 +0200
Subject: [Plearn-commits] r7953 - trunk/plearn_learners/online
Message-ID: <200708081501.l78F1mtg013456@sheep.berlios.de>

Author: yoshua
Date: 2007-08-08 17:01:47 +0200 (Wed, 08 Aug 2007)
New Revision: 7953

Modified:
   trunk/plearn_learners/online/VBoundDBN2.cc
   trunk/plearn_learners/online/VBoundDBN2.h
Log:
Minor compiling problems


Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-08 14:58:44 UTC (rev 7952)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-08 15:01:47 UTC (rev 7953)
@@ -262,8 +262,11 @@
 // getPortSizes //
 //////////////////
 const TMat<int>& VBoundDBN2::getPortSizes() {
-    TMat<int> sizes(nPorts(),2);
-    sizes.fill(-1);
+    if (sizes.width()!=2)
+    {
+        sizes.resize(nPorts(),2);
+        sizes.fill(-1);
+    }
     return sizes;
 }
 
@@ -281,7 +284,7 @@
     deepCopyField(sampled_h_state, copies);
     deepCopyField(global_improvement_state, copies);
     deepCopyField(ph_given_v_state, copies);
-    deepCopyField(neglogphsample_given_h, copies);
+    deepCopyField(neglogphsample_given_v, copies);
     deepCopyField(all_h, copies);
     deepCopyField(all_h, copies);
     deepCopyField(neglogP2h, copies);

Modified: trunk/plearn_learners/online/VBoundDBN2.h
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.h	2007-08-08 14:58:44 UTC (rev 7952)
+++ trunk/plearn_learners/online/VBoundDBN2.h	2007-08-08 15:01:47 UTC (rev 7953)
@@ -229,7 +229,6 @@
     //! the most common case.
     virtual const TVec<string>& getPorts();
 
-    /* Optional
     //! Return the size of all ports, in the form of a two-column matrix, where
     //! each row represents a port, and the two numbers on a row are
     //! respectively its length and its width (with -1 representing an
@@ -242,6 +241,7 @@
     //!           second row if nPorts() >= 2
     virtual const TMat<int>& getPortSizes();
 
+    /* Optional
     //! Return the index (as in the list of ports returned by getPorts()) of
     //! a given port.
     //! If 'port' does not exist, -1 is returned.
@@ -289,6 +289,7 @@
     Mat all_h; // for computing exact likelihood
     Mat neglogP2h;
     TVec<string> ports;
+    TMat<int> sizes;
 };
 
 // Declares a few other classes and functions related to this class



From saintmlx at mail.berlios.de  Wed Aug  8 17:50:18 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 8 Aug 2007 17:50:18 +0200
Subject: [Plearn-commits] r7954 - in trunk: . plearn/base plearn/io
	plearn/python plearn/python/test
	plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results
	plearn_learners/distributions/test
	python_modules/plearn/analysis python_modules/plearn/io
	python_modules/plearn/learners python_modules/plearn/learners/autolr
	python_modules/plearn/math python_modules/plearn/math/stats
	python_modules/plearn/parallel python_modules/plearn/plotting
	python_modules/plearn/pybridge python_modules/plearn/pymake
	python_modules/plearn/pyplearn python_modules/plearn/vmat
Message-ID: <200708081550.l78FoIrq016867@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-08 17:50:15 +0200 (Wed, 08 Aug 2007)
New Revision: 7954

Modified:
   trunk/plearn/base/Object.cc
   trunk/plearn/base/Object.h
   trunk/plearn/io/PStream.h
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonIncludes.h
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log
   trunk/plearn/python/test/BasicIdentityCallsTest.cc
   trunk/plearn/python/test/MemoryStressTest.cc
   trunk/plearn/python/test/test_python_vmatrix.pymat
   trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat
   trunk/pymake.config.model
   trunk/python_modules/plearn/analysis/experiment_results.py
   trunk/python_modules/plearn/io/serialize.py
   trunk/python_modules/plearn/learners/autolr/__init__.py
   trunk/python_modules/plearn/learners/discr_power_SVM.py
   trunk/python_modules/plearn/math/StatsCollector.py
   trunk/python_modules/plearn/math/arrays.py
   trunk/python_modules/plearn/math/missings_robust_covariance.py
   trunk/python_modules/plearn/math/random_processes.py
   trunk/python_modules/plearn/math/statistical_tools.py
   trunk/python_modules/plearn/math/stats/cvx_utils.py
   trunk/python_modules/plearn/parallel/dispatch.py
   trunk/python_modules/plearn/plotting/__init__.py
   trunk/python_modules/plearn/plotting/netplot.py
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
   trunk/python_modules/plearn/pymake/pymake.py
   trunk/python_modules/plearn/pyplearn/__init__.py
   trunk/python_modules/plearn/pyplearn/plearn_repr.py
   trunk/python_modules/plearn/vmat/PMat.py
   trunk/python_modules/plearn/vmat/readAMat.py
   trunk/python_modules/plearn/vmat/smartReadMat.py
Log:
- convert from numarray to numpy



Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/base/Object.cc	2007-08-08 15:50:15 UTC (rev 7954)
@@ -747,7 +747,17 @@
                   (BodyDoc("Returns the refcount of this PPointable"),
                    RetDoc ("Number of references")));
 
+    declareMethod(rmm, "deepCopyNoMap", &Object::deepCopyNoMap,
+                  (BodyDoc("Returns a deep copy of the object"),
+                   RetDoc ("Deep copy.")));
 
+    declareMethod(rmm, "pyDeepCopy", &Object::pyDeepCopy,
+                  (BodyDoc("Returns a pair containing a deep copy of "
+                           "the object and the updated copies map."),
+                   ArgDoc ("copies", "The initial copies map"),
+                   RetDoc ("Deep copy, copies map")));
+
+
 #ifdef PL_PYTHON_VERSION 
     declareMethod(rmm, "setOptionFromPython", &Object::setOptionFromPython,
                   (BodyDoc("Change an option within the object from a PythonObjectWrapper"),
@@ -807,6 +817,21 @@
     PLearn::load(filename, *this);
 }
 
+Object* Object::deepCopyNoMap()
+{
+    CopiesMap cm;
+    return deepCopy(cm);
+}
+
+#ifdef PL_PYTHON_VERSION 
+tuple<Object*, CopiesMap> Object::pyDeepCopy(CopiesMap& copies)
+{
+    Object* o= deepCopy(copies);
+    return make_tuple(o, copies);
+}
+#endif //def PL_PYTHON_VERSION 
+
+
 Object* loadObject(const PPath &filename)
 {
     PStream in = openFile(filename, PStream::plearn_ascii, "r");

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/base/Object.h	2007-08-08 15:50:15 UTC (rev 7954)
@@ -937,7 +937,19 @@
      */
     virtual void load(const PPath& filename);
 
+    /**
+     * deepCopyNoMap: same as deepCopy, but starts with an empty CopiesMap
+     * and returns an Object*.
+     */
+    Object* deepCopyNoMap();
 
+#ifdef PL_PYTHON_VERSION 
+    /**
+     * pyDeepCopy: deep copy called from python
+     */
+    tuple<Object*, CopiesMap> pyDeepCopy(CopiesMap& copies);
+#endif //def PL_PYTHON_VERSION 
+
 protected:
     //#####  Protected Member Functions  ######################################
 

Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/io/PStream.h	2007-08-08 15:50:15 UTC (rev 7954)
@@ -971,6 +971,17 @@
     in.get(); // eat the '}'
 }
 
+inline PStream& operator>>(PStream& in, CopiesMap&)
+{
+    PLERROR("PLearn::CopiesMap cannot be serialized.");
+    return in;
+}
+inline PStream& operator<<(PStream& out, const CopiesMap&)
+{
+    PLERROR("PLearn::CopiesMap cannot be [un]serialized.");
+    return out;
+}
+
 template<class Key, class Value, class Compare, class Alloc>
 inline PStream& operator<<(PStream& out,
                            const map<Key, Value, Compare, Alloc>& m)
@@ -1502,7 +1513,6 @@
 operator<<(PStream &out, const set<T> &v)
 { writeSet(out, v); return out; }
 
-
 /// @deprected Use openFile instead.
 class PIFStream: public PStream
 {

Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-08-08 15:50:15 UTC (rev 7954)
@@ -558,7 +558,7 @@
         if(it == global_map.end())
             PLERROR("in PythonCodeSnippet::compileGlobalCode : "
                     "plearn.pybridge.pl_global_funcs not present in global env!");
-        injectPLearnGlobalFunctions(it->second);//inject in pl_global_funcs module
+        setPythonModuleAndInject(it->second);
         global_funcs_injected= true;
     }
 

Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/PythonExtension.cc	2007-08-08 15:50:15 UTC (rev 7954)
@@ -123,41 +123,34 @@
 }
 
 
-void injectPLearnClasses(PyObject* injenv)
+void injectPLearnClasses(PyObject* module)
 {
     PythonGlobalInterpreterLock gil;         // For thread-safety
     PythonObjectWrapper::initializePython();
 
-    PyObject* module= 0;
-    if(PyModule_Check(injenv))
-        module= injenv;
-
+    if(!PyModule_Check(module))
+        PLERROR("In injectPLearnClasses : "
+                "module param. should be a python module.");
     // import python class for wrapping PLearn objects
     string importcode= "\nfrom plearn.pybridge.wrapped_plearn_object "
         "import *\n";
-    PyObject* pyenv= PyDict_New();
-    PyDict_SetItemString(pyenv, "__builtins__", PyEval_GetBuiltins());
-    PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, pyenv, pyenv);
+    PyObject_SetAttrString(module, "__builtins__", PyEval_GetBuiltins());
+    PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, 
+                                PyModule_GetDict(module), PyModule_GetDict(module));
     if(!res)
     {
-       if(PyErr_Occurred()) PyErr_Print();
-       PLERROR("in injectPLearnClasses : cannot import plearn.pybridge.wrapped_plearn_object.");
+        Py_DECREF(module);
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in injectPLearnClasses : cannot import plearn.pybridge.wrapped_plearn_object.");
     }
-
     Py_DECREF(res);
-    if (PyErr_Occurred()) 
-    {
-        Py_DECREF(pyenv);
-        PyErr_Print();
-        PLERROR("in injectPLearnClasses : error compiling "
-                "WrappedPLearnObject python code.");
-    }
 
     string wrapper_name= "WrappedPLearnObject";
     // now find the class in the env.
     typedef map<string, PyObject*> env_t;
     env_t env= PythonObjectWrapper(
-        pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
+        PyModule_GetDict(module), 
+        PythonObjectWrapper::transfer_ownership).as<env_t>();
     env_t::iterator clit= env.find(wrapper_name);
     if(clit == env.end())
         PLERROR("in injectPLearnClasses : "
@@ -178,7 +171,7 @@
     Py_XDECREF(py_funcobj);
     if(!py_funcobj || !py_methobj) 
     {
-        Py_DECREF(pyenv);
+        Py_DECREF(module);
         Py_XDECREF(py_methobj);
         PLERROR("in injectPLearnClasses : "
                 "can't inject method '%s' (i.e. __del__)", 
@@ -211,7 +204,7 @@
         Py_XDECREF(py_funcobj);
         if(!py_funcobj || !py_methobj) 
         {
-            Py_DECREF(pyenv);
+            Py_DECREF(module);
             Py_XDECREF(py_methobj);
             PLERROR("in injectPLearnClasses : "
                     "can't inject method '%s' (i.e. C++'s new)", 
@@ -223,11 +216,12 @@
         string classmethodname= wrapper_name+"."+py_method->ml_name;
         res= PyRun_String((classmethodname
                            +"= classmethod("+classmethodname+".im_func)").c_str(), 
-                          Py_file_input, pyenv, pyenv);
+                          Py_file_input, 
+                          PyModule_GetDict(module), PyModule_GetDict(module));
         Py_DECREF(res);
         if (PyErr_Occurred()) 
         {
-            Py_DECREF(pyenv);
+            Py_DECREF(module);
             PyErr_Print();
             PLERROR("in injectPLearnClasses : error making "
                     "newCPPObj a classmethod.");
@@ -268,10 +262,12 @@
             "      cls._refCPPObj(obj)\n"
             "    return obj\n";
         PyObject* res= PyRun_String(derivcode.c_str(), 
-                                    Py_file_input, pyenv, pyenv);
+                                    Py_file_input, 
+                                    PyModule_GetDict(module), PyModule_GetDict(module));
         Py_XDECREF(res);
         env= PythonObjectWrapper(
-            pyenv, PythonObjectWrapper::transfer_ownership).as<env_t>();
+            PyModule_GetDict(module), 
+            PythonObjectWrapper::transfer_ownership).as<env_t>();
         clit= env.find(pyclassname);
         if(clit == env.end())
             PLERROR("in injectPLearnClasses : "
@@ -291,7 +287,7 @@
         if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames", 
                                       PythonObjectWrapper(optionnames).getPyObject()))
         {
-            Py_DECREF(pyenv);
+            Py_DECREF(module);
             if (PyErr_Occurred()) PyErr_Print();
             PLERROR("in injectPLearnClasses : "
                     "cannot set attr _optionnames for class %s",
@@ -336,7 +332,7 @@
                 Py_XDECREF(py_funcobj);
                 if(!py_funcobj || !py_methobj) 
                 {
-                    Py_DECREF(pyenv);
+                    Py_DECREF(module);
                     Py_XDECREF(py_methobj);
                     PLERROR("in injectPLearnClasses : "
                             "can't inject method '%s'", py_method->ml_name);

Modified: trunk/plearn/python/PythonIncludes.h
===================================================================
--- trunk/plearn/python/PythonIncludes.h	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/PythonIncludes.h	2007-08-08 15:50:15 UTC (rev 7954)
@@ -74,14 +74,32 @@
 #include <python2.5/Python.h>
 #include <python2.5/compile.h>  // define PyCodeObject
 #include <python2.5/eval.h>     // for accessing PyEval_EvalCode: not included by default
-#include <python2.5/numarray/libnumarray.h>
+#ifdef PL_USE_NUMARRAY
+#  include <python2.5/numarray/libnumarray.h>
+#else
+#  ifdef PL_USE_NUMPY
+#    pragma GCC system_header //suppress all warnings/errors for numpy
+#    include <libnumarray.h>
+#  else
+#    error "should use either NumPy (preferred) or NUMARRAY (deprecated)"
+#  endif //def PL_USE_NUMPY
+#endif //def PL_USE_NUMARRAY
 
 #elif PL_PYTHON_VERSION >= 240
 
 #include <python2.4/Python.h>
 #include <python2.4/compile.h>  // define PyCodeObject
 #include <python2.4/eval.h>     // for accessing PyEval_EvalCode: not included by default
-#include <python2.4/numarray/libnumarray.h>
+#ifdef PL_USE_NUMARRAY
+#  include <python2.4/numarray/libnumarray.h>
+#else
+#  ifdef PL_USE_NUMPY
+#    pragma GCC system_header //suppress all warnings/errors for numpy
+#    include <libnumarray.h>
+#  else
+#    error "should use either NumPy (preferred) or NUMARRAY (deprecated)"
+#  endif //def PL_USE_NUMPY
+#endif //def PL_USE_NUMARRAY
 
 #elif PL_PYTHON_VERSION >= 230
 

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-08-08 15:50:15 UTC (rev 7954)
@@ -256,6 +256,36 @@
     return PythonObjectWrapper(pyobj);
 }
 
+CopiesMap ConvertFromPyObject<CopiesMap>::convert(PyObject* pyobj,
+                                                  bool print_traceback)
+{
+    PLASSERT( pyobj );
+    if (! PyDict_Check(pyobj))
+        PLPythonConversionError("ConvertFromPyObject<CopiesMap>", 
+                                pyobj, print_traceback);
+#if PL_PYTHON_VERSION>=250
+    Py_ssize_t pos = 0;
+#else
+    int pos = 0;
+#endif
+    CopiesMap copies;
+    PyObject *key, *val;
+    while(PyDict_Next(pyobj, &pos, &key, &val)) 
+    {
+        if(!PyCObject_Check(key))
+            PLPythonConversionError("ConvertFromPyObject<CopiesMap> "
+                                    "(key is not a cptr)", 
+                                    key, print_traceback);
+        if(!PyCObject_Check(val))
+            PLPythonConversionError("ConvertFromPyObject<CopiesMap> "
+                                    "(val is not a cptr)", 
+                                    val, print_traceback);
+        copies.insert(make_pair(PyCObject_AsVoidPtr(key),
+                                PyCObject_AsVoidPtr(val)));
+    }
+    return copies;
+}
+
 //#####  Constructors+Destructors  ############################################
 PythonObjectWrapper::PythonObjectWrapper(OwnershipMode o,
                                          // unused in this overload
@@ -433,7 +463,8 @@
         PythonObjectWrapper(args).as<TVec<PyObject*> >();
     PyObject* pyo= args_tvec[1];
     Object* o= PythonObjectWrapper(pyo);
-    o->ref();
+    if(args_tvec.length() < 3 || args_tvec[2]==Py_True)
+        o->ref();
     //perr << "ref o->usage()= " << o->usage() << endl;
     PythonObjectWrapper::m_wrapped_objects[o]= pyo;
     //perr << "refCPPObj: " << (void*)o << " : " << (void*)pyo << endl;
@@ -632,6 +663,24 @@
     return pow.m_object;
 }
 
+PyObject* ConvertToPyObject<CopiesMap>::newPyObject(const CopiesMap& copies)
+{
+    PyObject* pyobj= PyDict_New();
+    for(CopiesMap::const_iterator it= copies.begin();
+        it != copies.end(); ++it)
+    {
+        PyObject* key= PyCObject_FromVoidPtr(const_cast<void*>(it->first), 0);
+        PyObject* val= PyCObject_FromVoidPtr(it->second, 0);
+        int non_success = PyDict_SetItem(pyobj, key, val);
+        Py_XDECREF(key);
+        Py_XDECREF(val);
+        if(non_success)
+            PLERROR("ConvertToPyObject<CopiesMap>::newPyObject: cannot insert element "
+                    "into Python dict");
+    }
+    return pyobj;
+}
+
 PStream& operator>>(PStream& in, PythonObjectWrapper& v)
 {
     PLERROR("Attempting to read a PythonObjectWrapper from a stream : not supported");

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-08-08 15:50:15 UTC (rev 7954)
@@ -61,6 +61,7 @@
 #include <plearn/base/PMemPool.h>
 #include <plearn/base/TypeTraits.h>
 #include <plearn/base/tuple.h>
+#include <plearn/base/CopiesMap.h>
 
 // from boost
 #include <boost/static_assert.hpp>
@@ -429,7 +430,13 @@
     static std::pair<T,U> convert(PyObject*, bool print_traceback);
 };
 
+template <>
+struct ConvertFromPyObject<CopiesMap>
+{
+    static CopiesMap convert(PyObject*, bool print_traceback);
+};
 
+
 //! Used to convert integer values to python, using PyInt if possible
 template <class I>
 PyObject* integerToPyObject(const I& x)
@@ -628,6 +635,8 @@
 template<> struct ConvertToPyObject<PythonObjectWrapper>
 { static PyObject* newPyObject(const PythonObjectWrapper& pow); };
 
+template<> struct ConvertToPyObject<CopiesMap>
+{ static PyObject* newPyObject(const CopiesMap& copies); };
 
 
 struct PLPyClass
@@ -844,17 +853,14 @@
     static PyMethodDef m_refCPPObj_method_def;
     typedef map<const string, PLPyClass> pypl_classes_t;
     static pypl_classes_t m_pypl_classes;
+    typedef map<const Object*, PyObject*> wrapped_objects_t;
+    static wrapped_objects_t m_wrapped_objects; //!< for wrapped PLearn Objects
 
 protected:
     OwnershipMode m_ownership;               //!< Whether we own the PyObject or not
     PyObject* m_object;
-    
-    typedef map<const Object*, PyObject*> wrapped_objects_t;
 
-    static wrapped_objects_t m_wrapped_objects; //!< for wrapped PLearn Objects
-
     template<class T> friend class ConvertToPyObject;
-    friend void printWrappedObjects();
 };
 
 // Specialization for General T*.  Attempt to cast into Object*.  If that works
@@ -1078,8 +1084,6 @@
     return p;
 }
 
-
-
 //#####  newPyObject Implementations  #########################################
 template<typename T, bool is_enum>
 struct StaticConvertEnumToPyObject

Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_BasicIdentityCalls/expected_results/RUN.log	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,6 +1,7 @@
 Python code to be executed: 
 >>>import sys
-from numarray import *
+#from numarray import *
+from numpy import *
 
 def nullary():
     print >>sys.stderr, 'Called nullary()'
@@ -23,13 +24,15 @@
     return x
 
 def unary_vec(x):
-    assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 1
+    #assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 1
+    assert isinstance(x,ndarray) and len(x.shape) == 1
     return x
 
 def unary_mat(x):
     print >>sys.stderr, 'Called unary_mat with:\n',x
     sys.stderr.flush()
-    assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 2
+    #assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 2
+    assert isinstance(x,ndarray) and len(x.shape) == 2
     return x
 
 def unary_list_str(x):

Modified: trunk/plearn/python/test/BasicIdentityCallsTest.cc
===================================================================
--- trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/test/BasicIdentityCallsTest.cc	2007-08-08 15:50:15 UTC (rev 7954)
@@ -134,7 +134,8 @@
 
 string BasicIdentityCallsTest::python_code =
 "import sys\n"
-"from numarray import *\n"
+"#from numarray import *\n"
+"from numpy import *\n"
 "\n"
 "def nullary():\n"
 "    print >>sys.stderr, 'Called nullary()'\n"
@@ -157,13 +158,15 @@
 "    return x\n"
 "\n"
 "def unary_vec(x):\n"
-"    assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 1\n"
+"    #assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 1\n"
+"    assert isinstance(x,ndarray) and len(x.shape) == 1\n"
 "    return x\n"
 "\n"
 "def unary_mat(x):\n"
 "    print >>sys.stderr, 'Called unary_mat with:\\n',x\n"
 "    sys.stderr.flush()\n"
-"    assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 2\n"
+"    #assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 2\n"
+"    assert isinstance(x,ndarray) and len(x.shape) == 2\n"
 "    return x\n"
 "\n"
 "def unary_list_str(x):\n"

Modified: trunk/plearn/python/test/MemoryStressTest.cc
===================================================================
--- trunk/plearn/python/test/MemoryStressTest.cc	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/test/MemoryStressTest.cc	2007-08-08 15:50:15 UTC (rev 7954)
@@ -135,7 +135,8 @@
 
 string MemoryStressTest::python_code =
 "import sys\n"
-"from numarray import *\n"
+"#from numarray import *\n"
+"from numpy import *\n"
 "\n"
 "def nullary():\n"
 "    pass\n"
@@ -157,11 +158,13 @@
 "    return x\n"
 "\n"
 "def unary_vec(x):\n"
-"    assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 1\n"
+"    #assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 1\n"
+"    assert isinstance(x,ndarray) and len(x.shape) == 1\n"
 "    return x\n"
 "\n"
 "def unary_mat(x):\n"
-"    assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 2\n"
+"    #assert isinstance(x,numarraycore.NumArray) and len(x.shape) == 2\n"
+"    assert isinstance(x,ndarray) and len(x.shape) == 2\n"
 "    return x\n"
 "\n"
 "def unary_list_str(x):\n"

Modified: trunk/plearn/python/test/test_python_vmatrix.pymat
===================================================================
--- trunk/plearn/python/test/test_python_vmatrix.pymat	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn/python/test/test_python_vmatrix.pymat	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 from plearn.pyplearn import *
 
 ### This is an inline Matrix that will be processed (transformed) by the
@@ -20,9 +22,10 @@
 ### indentation-sensitive.
 processing_code = """
 import sys
-from numarray import *
+from numpy.numarray import *
 
 def getRow(row_no, source_row):
+    row_no= array([row_no])
     return concatenate((10*row_no, 2*source_row))
 
 def getFieldNames(source_field_names):

Modified: trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat
===================================================================
--- trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/plearn_learners/distributions/test/gaussian_mixtures_generate.pymat	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,7 +1,9 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # Generate data from Gaussian mixtures.
 # Also test the 'expecation' method.
 
-from numarray        import array
+from numpy.numarray        import array
 from plearn.pyplearn import pl
 
 def diagonal(predictor_size):

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/pymake.config.model	2007-08-08 15:50:15 UTC (rev 7954)
@@ -242,15 +242,58 @@
 
 linkeroptions_tail = ''
 
+
+#####  Global List of pymake Options  #######################################
+
+# List of lists of mutually exclusive pymake options.
+# First option that appears in each group is the default, and is assumed if you
+# do not specify any option from that group.
+options_choices = [
+  [ 'g++', 'g++3', 'g++no-cygwin', 'icc', 'icc8', 'icc9', 'mpi',
+    'purify', 'quantify', 'vc++' ],
+  
+  [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
+    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg' ],
+  
+  [ 'double', 'float' ],
+  
+  # [ 'throwerrors', 'exiterrors' ],
+  [ 'autopython', 'python23', 'python24', 'python25', 'nopython' ],
+  
+  [ 'blas', 'noblas' ],
+  [ 'defblas', 'nolibblas', 'p3blas','p4blas','athlonblas','pentiumblas',
+    'mammouthblas', 'veclib', 'scs', 'goto', 'lisa' ],
+  
+  [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal',
+    'logging=extreme', 'logging=dbg-profile' ],
+
+  [ 'numpy', 'numarray' ]
+
+]
+
 ### Using Python code snippets in C++ code
 ### TODO Using /u/lisa/... is quite ugly. But the PLEARN_LIBDIR ppath may also
 ### be problematic (it should be different depending on the platform) => must
 ### find a better way to know where to find the libraries.
 ### TODO Should find a way to automatically find python version ?
+
+
+pyver = sys.version.split()[0][0:3]
+pyoption = 'python%s' % pyver.replace('.', '')
+#verify/set optionargs for python and numpy
+python_choices= [x for x in options_choices if 'autopython' in x][0]
+if [x for x in python_choices if x in optionargs]==[]:
+    optionargs.append('autopython')
+numpy_choices= [x for x in options_choices if 'numpy' in x][0]
+if [x for x in numpy_choices if x in optionargs]==[]:
+    optionargs.append('numpy')
+#auto-detected python version, if needed
+if 'autopython' in optionargs:
+    optionargs.remove('autopython')
+    optionargs += [ pyoption ]
+
 if not 'nopython' in optionargs:
     # First find which version of python is installed.
-    pyver = sys.version.split()[0][0:3]
-    pyoption = 'python%s' % pyver.replace('.', '')
     if domain_name.endswith('iro.umontreal.ca'):
         optionargs += [ pyoption ]
         python_version = pyver
@@ -301,15 +344,40 @@
             python_version = pyver
 
         python_lib_root = '/usr/lib'
-        numpy_site_packages = '/usr/lib/python'+python_version+'/site-packages/numarray'
-        numpy_includedirs   = [ '/usr/include/python'+python_version]
+        numpy_site_packages = '-L/usr/lib/python'+python_version+'/site-packages/numarray'
+        python_includedirs   = [ '/usr/include/python'+python_version]
 
+        numpy_includedirs   = python_includedirs
 
+
+    if python_version != pyver:
+        print '*** WARNING: python version mismatch:'
+        print '\tcompiling for python'+python_version+' using python'+pyver
+
+    if 'numpy' in optionargs:
+        try:
+            numpy_numarray_include, numpy_core_include= \
+              os.popen('python' + python_version + ' ' 
+                       + os.path.join( plearndir,'scripts','get_numpy_includes.py'),
+                       'r').read().splitlines()
+            numpy_site_packages= numpy_numarray_include
+            numpy_includedirs= [numpy_site_packages+'/numpy', numpy_core_include] \
+                               + python_includedirs
+        except ValueError:
+            print 'Cannot find numpy include dirs for python'+python_version+'.',
+            print 'Make sure numpy is installed for this version of python'+\
+                  ' or compile using another version of python.'
+            sys.exit()
+
+    numpy_lib= ' -lnumarray '
+    if 'numpy' in optionargs:
+        numpy_lib= '/_capi.so -lutil '
+
     if platform!='darwin':
         optionalLibrary( name = 'python',
                      triggers = '[Pp]ython*',
                      includedirs = numpy_includedirs,
-                     linkeroptions = ( '-L%s -lnumarray ' % numpy_site_packages + 
+                     linkeroptions = ( '%s%s ' % (numpy_site_packages, numpy_lib) + 
                                        '-L%s/python%s/config -lpython%s ' % (python_lib_root, python_version, python_version) +
                                        '-Xlinker -export-dynamic ' + 
                                        '-Xlinker -rpath -Xlinker %s' % numpy_site_packages )
@@ -411,32 +479,6 @@
 linkeroptions_tail += ' -L' + libdir + ' -lm'
 
 
-#####  Global List of pymake Options  #######################################
-
-# List of lists of mutually exclusive pymake options.
-# First option that appears in each group is the default, and is assumed if you
-# do not specify any option from that group.
-options_choices = [
-  [ 'g++', 'g++3', 'g++no-cygwin', 'icc', 'icc8', 'icc9', 'mpi',
-    'purify', 'quantify', 'vc++' ],
-  
-  [ 'dbg', 'opt', 'pintel', 'gprof', 'optdbggprof', 'safegprof',
-    'safeopt', 'safeoptdbg', 'checkopt', 'genericvc++', 'pydbg' ],
-  
-  [ 'double', 'float' ],
-  
-  # [ 'throwerrors', 'exiterrors' ],
-  [ 'python23', 'python24', 'python25', 'nopython' ],
-  
-  [ 'blas', 'noblas' ],
-  [ 'defblas', 'nolibblas', 'p3blas','p4blas','athlonblas','pentiumblas',
-    'mammouthblas', 'veclib', 'scs', 'goto', 'lisa' ],
-  
-  [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal',
-    'logging=extreme', 'logging=dbg-profile' ]
-
-]
-
 #####  Compiler-Related Options  ############################################
 
 if target_platform=='linux-i386':
@@ -621,6 +663,11 @@
 
 #####  Python version  ######################################################
 
+
+# pymakeOption( name = 'autopython',
+#               description = 'auto-detect python version',
+#               cpp_definitions = ['PL_PYTHON_VERSION='+str(int(float(pyver)*100))])
+
 pymakeOption( name = 'python23',
               description = 'the installed version of python is 2.3.X',
               cpp_definitions = ['PL_PYTHON_VERSION=230'])
@@ -760,6 +807,18 @@
 
 cpp_variables += ['PL_LOG_VERBOSITY', 'PL_PROFILE']
 
+
+#####  Python NUMARRAY vs NumPy  ########################################
+
+pymakeOption( name = 'numarray',
+              description = 'use numarray arrays',
+              cpp_definitions = ['PL_USE_NUMARRAY'] )
+pymakeOption( name = 'numpy',
+              description = 'use numpy arrays',
+              cpp_definitions = ['PL_USE_NUMPY'] )
+
+cpp_variables += ['PL_USE_NUMARRAY', 'PL_USE_NUMPY']
+
 #####  Network Setup  #######################################################
 
 # nprocesses_on_localhost = 1

Modified: trunk/python_modules/plearn/analysis/experiment_results.py
===================================================================
--- trunk/python_modules/plearn/analysis/experiment_results.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/analysis/experiment_results.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -28,9 +28,9 @@
 #
 #  This file is part of the PLearn library. For more information on the PLearn
 #  library, go to the PLearn Web site at www.plearn.org
-import sys, os, os.path, glob, fnmatch, csv, numarray
+import sys, os, os.path, glob, fnmatch, csv, numpy.numarray
 
-import os, glob, fnmatch, numarray
+import os, glob, fnmatch, numpy.numarray
 from plearn.vmat.PMat          import PMat
 from plearn.vmat.smartReadMat  import smartReadMat
 from plearn.utilities.moresh   import relative_path

Modified: trunk/python_modules/plearn/io/serialize.py
===================================================================
--- trunk/python_modules/plearn/io/serialize.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/io/serialize.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # serialize.py
 # Copyright (C) 2005, 2006 Pascal Vincent, Christian Dorion
 #
@@ -36,7 +38,7 @@
 Streams are built using the File objects' specifications
 """
 import logging, sys, string
-import numarray
+import numpy.numarray as numarray
 from plearn.pyplearn import pl
 from plearn.pyplearn.plearn_repr import plearn_repr, clear_ref_map
 import plearn.utilities.pl_struct as struct  ## corrige un bug dans struct
@@ -334,7 +336,7 @@
         ar = numarray.fromstring(data, atype, shape)
         
         if sys.byteorder!=endianness:
-            ar.byteswap()
+            ar.byteswap(True)
 
         if self.return_lists:
             ar = ar.tolist()

Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,5 +1,7 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 from math import *
-from numarray import *
+from numpy.numarray import *
 from plearn import *
 from plearn.bridge import *
 from threading import *
@@ -697,3 +699,4 @@
             # (i.e. stage of best error found)
             (best_early_stop,best_early_stop_candidate,best_early_stop_epoch))
 
+import numpy.numarray as numarray

Modified: trunk/python_modules/plearn/learners/discr_power_SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/learners/discr_power_SVM.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,5 +1,7 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 import sys, os, time
-#from numarray import *
+#from numpy.numarray import *
 from math import *
 from libsvm import *
 

Modified: trunk/python_modules/plearn/math/StatsCollector.py
===================================================================
--- trunk/python_modules/plearn/math/StatsCollector.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/math/StatsCollector.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # readMMat.py
 # Copyright (C) 2006 by Nicolas Chapados
 #
@@ -33,12 +35,12 @@
 # Author: Nicolas Chapados
 
 import os, sys
-import numarray.ieeespecial        as  ieee
-from   plearn.math import isNaN
-from   numarray    import array    as  normal_array
-from   numarray    import sometrue as  normal_sometrue
-from   numarray    import zeros, matrixmultiply, transpose
-from   numarray.ma import *                         # masked array
+from numpy import isnan
+from   numpy.numarray    import array    as  normal_array
+from   numpy.numarray    import sometrue as  normal_sometrue
+from   numpy.numarray    import zeros, matrixmultiply, transpose
+#from   numpy.numarray.ma import *                         # masked array
+from   numpy.numarray.ma import masked_array, argmin, argmax, array, nonzero, outerproduct, diagonal, sqrt, sum
 from   fpconst     import NegInf, PosInf
 
 from   plearn.utilities import ppath
@@ -49,7 +51,7 @@
     If 'pretty' is set to False, the output will not be so nice, but at least
     will not cause test failures due to a zero test blank tolerance.
     """
-    (length, width) = shape(m)
+    (length, width) = m.shape #shape(m)
     assert length == len(rownames)
     assert width  == len(colnames)
     leftlen  = max([len(x) for x in rownames])
@@ -113,15 +115,15 @@
         assume that all observations have the same weight.
         Properly handle missing values.
         """
-        assert self.width() == shape(arr)[1]
+        assert self.width() == arr.shape[1] #shape(arr)[1]
         i = 0
 
         ## Update number of elements counters
-        (length,width)= shape(arr)
-        initial_n     = self.n[:]          # Keep old n for argmin/argmax
+        (length,width)= arr.shape #shape(arr)
+        initial_n     = self.n.copy()#self.n[:]          # Keep old n for argmin/argmax
         n             = zeros(width) + length
-        missings      = isNaN(arr)
-        nnan          = sum(missings)
+        missings      = isnan(arr)
+        nnan          = sum(missings,0)
         self.n       += n
         self.nnan    += nnan
         self.nnonnan += n - nnan
@@ -129,12 +131,12 @@
         ## Create masked version of arr and update accumulators
         ma = masked_array(arr, mask=missings)        # Here, mask missings only
         arr_nomissings = arr[~normal_sometrue(missings,1)]  # Here, strip missing rows
-        self.sum     = self.sum + sum(ma)            # += does not work...
-        self.sum_ssq = self.sum_ssq + sum(ma*ma)     # += does not work...
+        self.sum     = self.sum + sum(ma,0)            # += does not work...
+        self.sum_ssq = self.sum_ssq + sum(ma*ma,0)     # += does not work...
         self.sum_xxt = self.sum_xxt + matrixmultiply(transpose(arr_nomissings),
                                                      arr_nomissings)
-        self.sum_nomi= self.sum_nomi + sum(arr_nomissings)
-        self.nxxt   += shape(arr_nomissings)[0]
+        self.sum_nomi= self.sum_nomi + sum(arr_nomissings,0)
+        self.nxxt   += arr_nomissings.shape[0] #shape(arr_nomissings)[0]
 
         ## Update (arg)min / make sure old argmin is kept if not updated
         ma_argmin  = argmin(ma,0)
@@ -194,7 +196,7 @@
         If 'pretty' is set to False, the output will not be so nice, but at least
         will not cause test failures due to a zero test blank tolerance.
         """
-        if len(nonzero(self.nnonnan)) != len(self.nnonnan):
+        if len(nonzero(self.nnonnan)[0]) != len(self.nnonnan):
             print >>os, "One or more columns in StatsCollector does not contain any data"
             return                      # Nothing accumulated yet
         
@@ -203,7 +205,9 @@
               "V"      , "STDDEV"   , "STDERR"      , "SUM" ,
               "SUMSQ"  , "MIN"      , "ARGMIN"      , "MAX" ,  "ARGMAX" ]
 
+        
         m = array([[stats[k][i] for i in range(self.width())] for k in sk])
+
         _printMatrix(m, sk, self.fieldnames, os, pretty)
         
         print "\nCovariance Matrix:"

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/math/arrays.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # arrays.py
 # Copyright (C) 2005,2006 Christian Dorion
 #
@@ -30,10 +32,13 @@
 #  library, go to the PLearn Web site at www.plearn.org
 
 # Author: Christian Dorion
+import numpy.numarray as numarray
 from math import *
-from numarray import *
-from numarray.linear_algebra import inverse as __numarray_inverse
-from numarray.linear_algebra import determinant
+from numpy.numarray import *
+from numpy.numarray.linear_algebra import inverse as __numarray_inverse
+from numpy.numarray.linear_algebra import determinant
+from numpy import *
+import numpy.numarray as ufunc #most ufuncs are part of the numpy module
 
 def _2D_shape(M):
     if len(M.shape)==1:
@@ -81,7 +86,7 @@
             K[i*l2:(i+1)*l2, j*w2:(j+1)*w2] = M1[i,j] * m2
 
     if w1==1 and w2==1:
-        K = K.getflat()
+        K = K.ravel()
     return K
 
 def lag(series, k=1, fill=(lambda shape : array([]))):
@@ -110,7 +115,7 @@
     assert len(shp)==2
     length = shp[0]*shp[1]
 
-    mat.setshape((length,))
+    mat.shape = (length,)
     return shp
 
 def mmult(*args):
@@ -176,7 +181,7 @@
     """Returns a diagonal matrix with elements in 'a' on the diagonal."""
     assert len(a.shape)==1
     n = len(a)
-    A = zeros(shape=(n,n), type=a.type())
+    A = zeros(shape=(n,n), type=numarray.typefrom(a))
     for i in range(n):
         A[i,i] = a[i]
     return A
@@ -195,17 +200,17 @@
     f = ravel(f)
     if len(f)==0:
         return False
-    f = choose(isNaN(f), (0, 1))
+    f = choose(isnan(f), (0, 1))
     return sum(f)
     
 def isNaN(f):
     """Return 1 where f contains NaN values, 0 elsewhere."""
-    from numarray import ieeespecial
+    import numpy as ieeespecial
     return ieeespecial.mask(f, ieeespecial.NAN)
 
 def isNotNaN(f):
     """Return 0 where f contains NaN values, 1 elsewhere."""
-    return ufunc.equal(isNaN(f), 0.0)
+    return ufunc.equal(isnan(f), 0.0)
 
 def replace_nans(a, repl_with=0.0):
     return choose(isNotNaN(a), (repl_with, a))
@@ -231,11 +236,11 @@
     
     print fast_softmax([ 0, 0, 100 ])
     print
-    print isNaN(float('NaN'))
-    print isNaN(2.0)
-    print isNaN([1.0, float('NaN'), 3.0])
+    print isnan(float('NaN'))
+    print isnan(2.0)
+    print isnan([1.0, float('NaN'), 3.0])
     print hasNaN([1.0, float('NaN'), 3.0])
-    print isNaN([1.0, 2.0, 3.0])
+    print isnan([1.0, 2.0, 3.0])
     print hasNaN([1.0, 2.0, 3.0])
     print
 
@@ -266,7 +271,7 @@
     print a
     a_shape = matrix2vector(a)
     print a
-    a.setshape(a_shape)
+    a.shape = a_shape
     print a
     
     

Modified: trunk/python_modules/plearn/math/missings_robust_covariance.py
===================================================================
--- trunk/python_modules/plearn/math/missings_robust_covariance.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/math/missings_robust_covariance.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # missings_robust_covariance
 # Copyright (C) 2007 by Nicolas Chapados
 #
@@ -35,7 +37,7 @@
 
 import sys
 from fpconst     import NaN
-from numarray.ma import *
+from numpy.numarray.ma import *
 from plearn.math import isNaN
 
 def missingsRobustCovariance(arr, unbiased=False, epsilon=1e-8):
@@ -57,7 +59,7 @@
     the diagonal.
     """
     ## Second-order statistics
-    mask   = isNaN(arr)
+    mask   = isnan(arr)
     arr_ma = array(arr, mask=mask)
     count  = array(1 - mask)
 

Modified: trunk/python_modules/plearn/math/random_processes.py
===================================================================
--- trunk/python_modules/plearn/math/random_processes.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/math/random_processes.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,7 +1,9 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 #!/usr/bin/env python
 from math import ceil, floor
-from numarray import *
-from numarray.random_array import normal
+from numpy.numarray import *
+from numpy.numarray.random_array import normal
 
 #####  Utility Functions  ###################################################
 #####    Instanciate the process' class and extract the process          ####

Modified: trunk/python_modules/plearn/math/statistical_tools.py
===================================================================
--- trunk/python_modules/plearn/math/statistical_tools.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/math/statistical_tools.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # statistical_tools.py
 # Copyright (C) 2006 Christian Dorion
 #
@@ -345,11 +347,11 @@
 
     # for convenience...
     if K==1:
-        beta = beta.getflat()
+        beta = beta.ravel()
     if N==1:
         alpha   = alpha.  getflat()
         beta    = beta.   getflat()
-        epsilon = epsilon.getflat()
+        epsilon = epsilon.ravel()
         
     return alpha, beta, epsilon, sigma
 
@@ -376,7 +378,7 @@
 
     print
     if K==1:
-        X = X.getflat()
+        X = X.ravel()
     scipy = stats.linregress(X, Y)    
     print "Beta SciPy (with intercept %s):"%scipy[1],
     print scipy[0], "(%.3f)"%matrix_distance(array(scipy[0]),beta)
@@ -410,7 +412,7 @@
     __delattr__ = dict.__delitem__
 
 if __name__ == "__main__":        
-    from numarray.random_array import seed, random, normal
+    from numpy.numarray.random_array import seed, random, normal
     seed(02, 25)
     
     # Setting the problem

Modified: trunk/python_modules/plearn/math/stats/cvx_utils.py
===================================================================
--- trunk/python_modules/plearn/math/stats/cvx_utils.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/math/stats/cvx_utils.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # cvx_utils.py
 # Copyright (C) 2007 Christian Dorion
 #
@@ -33,7 +35,7 @@
 # Author: Christian Dorion
 """Provides functions to use CVX transparently where NumPy or NumArray is the standard."""
 import numpy
-import numarray 
+import numpy.numarray as numarray
 from cvxopt import base as cvx
 
 

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -179,14 +179,19 @@
         """
         got_info= False
         while not got_info:
-            s= child_fd.readline()
-            if s=='':
-                print "Cannot get info from server!"
-                sys.exit()
-            ss= s.split(' ')
-            if ss[0] == "PLEARN_SERVER_TCP":
-                info= (ss[1], int(ss[2]), int(ss[3]))
-                got_info= True
+            try:
+                s= child_fd.readline()
+                if s=='':
+                    print "Cannot get info from server!"
+                    sys.exit()
+                ss= s.split(' ')
+                if ss[0] == "PLEARN_SERVER_TCP":
+                    info= (ss[1], int(ss[2]), int(ss[3]))
+                    got_info= True
+            except IOError:
+                print '.',
+                time.sleep(1)
+                
         cls._launched_servers_info+= [info]
         return info
     getLaunchedServerInfo= classmethod(getLaunchedServerInfo)

Modified: trunk/python_modules/plearn/plotting/__init__.py
===================================================================
--- trunk/python_modules/plearn/plotting/__init__.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/plotting/__init__.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # plotting.py
 # Copyright (C) 2005 Pascal Vincent
 #
@@ -33,8 +35,7 @@
 # Author: Pascal Vincent
 
 # from array import *
-import numarray
-
+import numpy.numarray as numarray
 import string
 import matplotlib
 # matplotlib.interactive(True)
@@ -42,7 +43,7 @@
 #matplotlib.use('GTK')
   
 from pylab import *
-from numarray import *
+from numpy.numarray import *
 from mayavi.tools import imv
 
 from plearn.vmat.PMat import *

Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,5 +1,5 @@
 from pylab import *
-from numarray import *
+from numpy.numarray import *
 
 
 #################

Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -69,18 +69,31 @@
                                    + repr(self))
         
     def __del__(self):
-        #from plearn import pyext
-        #print 'del',type(self),self._cptr
-        #pyext.printWrappedObjects()
-        self._unref()
-        #pyext.printWrappedObjects()
+        if hasattr(self, '_cptr'):
+            self._unref()
 
     def __repr__(self):
         return self.asString() #PLearn repr. for now
 
-#from numpy.numarray import *
-from numarray import *
+    def __deepcopy__(self, memo= None):
+        if not memo: memo= {}
+        if 'PLearnCopiesMap' not in memo:
+            memo['PLearnCopiesMap']= {}
+        plnewone, memo['PLearnCopiesMap']= \
+            self.pyDeepCopy(memo['PLearnCopiesMap'])
+        newone= self.__class__(_cptr= plnewone._cptr)
+        memo[id(self)]= newone
+        del plnewone._cptr
+        newone._refCPPObj(newone, False)
+        for k in self.__dict__:
+            if k != '_cptr':
+                newone.__dict__[k]= \
+                    copy.deepcopy(self.__dict__[k], memo)
+        return newone
 
+
+from numpy.numarray import *
+
 class WrappedPLearnVMat(WrappedPLearnObject):
     def __init__(self, cptr):
         WrappedPLearnObject.__init__(self, cptr)

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -2731,6 +2731,7 @@
         (list_of_hosts, nice_values) = get_list_of_hosts()
 
 
+
     ######  The compilation and linking
 
     for target in otherargs:

Modified: trunk/python_modules/plearn/pyplearn/__init__.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/__init__.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/pyplearn/__init__.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # pyplearn/__init__.py
 # Copyright (C) 2004-2006 Christian Dorion
 #
@@ -165,8 +167,7 @@
     @returns: PyPLearn's TMat representation
     @rtype: A numarray or L{ __TMat} wrapper
     """
-    import numarray
-    
+    import numpy.numarray as numarray
     nargs   = len(args)
     is_real = lambda r: isinstance( r, int   ) or isinstance( r, float )
 

Modified: trunk/python_modules/plearn/pyplearn/plearn_repr.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plearn_repr.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/pyplearn/plearn_repr.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # plearn_repr.py
 # Copyright (C) 2005, 2006 Christian Dorion
 #
@@ -38,9 +40,11 @@
 from plearn.utilities.Bindings import Bindings
 
 try:
-    import numarray
+    #import numpy.numarray as numarray
+    import numpy
 except ImportError:
-    numarray = None
+    #numarray = None
+    numpy = None
 
 #
 #  Deprecated functions
@@ -280,8 +284,9 @@
 ##             return '(' + ', '.join( [ inner_repr(elem, indent_level+1) for elem in obj] ) + ') '
 
     # Stands for TMat<real>
-    elif numarray is not None and isinstance( obj, numarray.numarraycore.NumArray ):
-        shape = obj.getshape()
+    #elif numarray is not None and isinstance( obj, numarray.numarraycore.NumArray ):
+    elif numpy is not None and isinstance( obj, numpy.ndarray ):
+        shape = obj.shape
         if len(shape) == 1:
             listrepr = [ elem for elem in obj ]
             return "%d %s" % ( shape[0], inner_repr(listrepr, indent_level+1) )
@@ -290,6 +295,7 @@
             l,w = shape
             listrepr = []
             # we do this explicit for due to a numarray bug for 0x0 matrices...
+            # N.B. is this still true w/ numpy?
             for i in xrange(l):
                 row = obj[i]
                 for elem in row:
@@ -297,7 +303,8 @@
             # listrepr = [ f for row in obj for f in row ]
             return "%d %d %s" % ( l, w, inner_repr(listrepr, indent_level+1) )
 
-        raise ValueError( "Only numarrays of dimension 1 and 2 are understood by plearn_repr." )
+        #raise ValueError( "Only numarrays of dimension 1 and 2 are understood by plearn_repr." )
+        raise ValueError( "Only NumPy arrays of dimension 1 and 2 are understood by plearn_repr." )
             
     
     elif obj is None:
@@ -346,7 +353,8 @@
 
     toplevel2 = pl.TopLevel( name = "Embedded", embedded = toplevel,
                              some_dict = { "a" : 1, "b" : 2 },
-                             some_mat  = numarray.array([[1,2], [3, 4]])
+                             #some_mat  = numarray.array([[1,2], [3, 4]])
+                             some_mat  = numpy.array([[1,2], [3, 4]])
                              )
     print toplevel2 
 

Modified: trunk/python_modules/plearn/vmat/PMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/PMat.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/vmat/PMat.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # PMat.py
 # Copyright (C) 2005 Pascal Vincent
 #
@@ -32,7 +34,8 @@
 
 # Author: Pascal Vincent
 
-import numarray, sys, os, os.path
+#import numarray, sys, os, os.path
+import numpy.numarray, sys, os, os.path
 pyplearn_import_failed = False
 try:
     from plearn.pyplearn.plearn_repr import plearn_repr, format_list_elements
@@ -51,7 +54,7 @@
     else:
         indices = list( cols )            
 
-    return numarray.take(a, indices, axis=1)
+    return numpy.numarray.take(a, indices, axis=1)
 
 def load_pmat_as_array(fname):
     s = file(fname,'rb').read()
@@ -75,15 +78,15 @@
 
     l = int(l)
     w = int(w)
-    X = numarray.fromstring(datastr,elemtype, shape=(l,w) )
+    X = numpy.numarray.fromstring(datastr,elemtype, shape=(l,w) )
     if byteorder!=sys.byteorder:
-        X.byteswap()
+        X.byteswap(True)
     return X
 
 def save_array_as_pmat( fname, ar, fieldnames=[] ):
     s = file(fname,'wb')
     
-    length, width = ar.getshape()
+    length, width = ar.shape
     if fieldnames:
         assert len(fieldnames) == width
         metadatadir = fname+'.metadata'
@@ -96,16 +99,16 @@
         f.close()
     
     header = 'MATRIX ' + str(length) + ' ' + str(width) + ' '
-    if ar.typecode()=='d':
+    if ar.dtype.char=='d':
         header += 'DOUBLE '
         elemsize = 8
 
-    elif ar.typecode()=='f':
+    elif ar.dtype.char=='f':
         header += 'FLOAT '
         elemsize = 4
 
     else:
-        raise TypeError('Unsupported typecode: %s' % ar.typecode())
+        raise TypeError('Unsupported typecode: %s' % ar.dtype.char)
 
     rowsize = elemsize*width
 
@@ -181,7 +184,7 @@
             assert len(key) == 2
             rows = self.__getitem__( key[0] )
 
-            shape = rows.getshape()                       
+            shape = rows.shape                       
             if len(shape) == 1:
                 return rows[ key[1] ]
 
@@ -254,7 +257,7 @@
             raise ValueError("Currently only supported openmodes are 'r', 'w' and 'a': "+repr(openmode)+" is not supported")
 
         if array is not None:
-            shape  = array.getshape()
+            shape  = array.shape
             if len(shape) == 1:
                 row_format = lambda r: [ r ]
             elif len(shape) == 2:
@@ -344,9 +347,9 @@
             raise IndexError('PMat index out of range')
         self.f.seek(64+i*self.rowsize)
         data = self.f.read(self.rowsize)
-        ar = numarray.fromstring(data, self.elemtype, (self.width,))
+        ar = numpy.numarray.fromstring(data, self.elemtype, (self.width,))
         if self.swap_bytes:
-            ar.byteswap()
+            ar.byteswap(True)
         return ar
 
     def getRows(self,i,l):
@@ -354,9 +357,9 @@
             raise IndexError('PMat index out of range')
         self.f.seek(64+i*self.rowsize)
         data = self.f.read(l*self.rowsize)
-        ar = numarray.fromstring(data, self.elemtype, (l,self.width))
+        ar = numpy.numarray.fromstring(data, self.elemtype, (l,self.width))
         if self.swap_bytes:
-            ar.byteswap()
+            ar.byteswap(True)
         return ar
 
     def putRow(self,i,row):
@@ -367,10 +370,10 @@
         if i<0 or i>=self.length:
             raise IndexError
         if self.swap_bytes: # must make a copy and swap bytes
-            ar = numarray.numarray(row,type=self.elemtype)
-            ar.byteswap()
+            ar = numpy.numarray.numarray(row,type=self.elemtype)
+            ar.byteswap(True)
         else: # asarray makes a copy if not already a numarray of the right type
-            ar = numarray.asarray(row,type=self.elemtype)
+            ar = numpy.numarray.asarray(row,type=self.elemtype)
         self.f.seek(64+i*self.rowsize)
         self.f.write(ar.tostring())
 
@@ -378,10 +381,10 @@
         if len(row)!=self.width:
             raise TypeError('length of row ('+str(len(row))+ ') differs from matrix width ('+str(self.width)+')')
         if self.swap_bytes: # must make a copy and swap bytes
-            ar = numarray.numarray(row,type=self.elemtype)
-            ar.byteswap()
+            ar = numpy.numarray.numarray(row,type=self.elemtype)
+            ar.byteswap(True)
         else: # asarray makes a copy if not already a numarray of the right type
-            ar = numarray.asarray(row,type=self.elemtype)
+            ar = numpy.numarray.asarray(row,type=self.elemtype)
 
         self.f.seek(64+self.length*self.rowsize)
         self.f.write(ar.tostring())

Modified: trunk/python_modules/plearn/vmat/readAMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/readAMat.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/vmat/readAMat.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -1,3 +1,5 @@
+## Automatically adapted for numpy.numarray Jun 13, 2007 by python_numarray_to_numpy (-xsm)
+
 # readMMat.py
 # Copyright (C) 2006 by Nicolas Chapados
 #
@@ -32,7 +34,7 @@
 
 # Author: Nicolas Chapados
 
-from numarray import array
+from numpy.numarray import array
 import fpconst
 
 def safefloat(str):

Modified: trunk/python_modules/plearn/vmat/smartReadMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/smartReadMat.py	2007-08-08 15:01:47 UTC (rev 7953)
+++ trunk/python_modules/plearn/vmat/smartReadMat.py	2007-08-08 15:50:15 UTC (rev 7954)
@@ -33,7 +33,7 @@
 # Author: Nicolas Chapados
 
 import csv
-import numarray
+import numpy.numarray
 
 from plearn.vmat.PMat import PMat
 from plearn.vmat.readAMat import readAMat
@@ -162,7 +162,7 @@
         csv_reader = csv.reader(f)
         if has_header:
             fieldnames = csv_reader.next()
-        arr = numarray.array([[float(value) for value in fields] for fields in csv_reader])
+        arr = numpy.numarray.array([[float(value) for value in fields] for fields in csv_reader])
         if not has_header:
             # Generate fake fieldnames
             fieldnames = ['field%d' % (i + 1) for i in range(arr.shape[1])]



From saintmlx at mail.berlios.de  Wed Aug  8 18:35:47 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 8 Aug 2007 18:35:47 +0200
Subject: [Plearn-commits] r7955 - trunk/scripts
Message-ID: <200708081635.l78GZlJM030423@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-08 18:35:46 +0200 (Wed, 08 Aug 2007)
New Revision: 7955

Added:
   trunk/scripts/get_numpy_includes.py
Log:
- new file for numpy



Added: trunk/scripts/get_numpy_includes.py
===================================================================
--- trunk/scripts/get_numpy_includes.py	2007-08-08 15:50:15 UTC (rev 7954)
+++ trunk/scripts/get_numpy_includes.py	2007-08-08 16:35:46 UTC (rev 7955)
@@ -0,0 +1,8 @@
+
+
+try:
+    import numpy
+    print numpy.get_numarray_include()
+    print numpy.get_include()
+except:
+    pass



From saintmlx at mail.berlios.de  Wed Aug  8 18:37:40 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 8 Aug 2007 18:37:40 +0200
Subject: [Plearn-commits] r7956 - trunk
Message-ID: <200708081637.l78GbesG001241@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-08 18:37:40 +0200 (Wed, 08 Aug 2007)
New Revision: 7956

Modified:
   trunk/pymake.config.model
Log:
- compilation should fail when numpy is not available



Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-08 16:35:46 UTC (rev 7955)
+++ trunk/pymake.config.model	2007-08-08 16:37:40 UTC (rev 7956)
@@ -367,7 +367,7 @@
             print 'Cannot find numpy include dirs for python'+python_version+'.',
             print 'Make sure numpy is installed for this version of python'+\
                   ' or compile using another version of python.'
-            sys.exit()
+            sys.exit(100)
 
     numpy_lib= ' -lnumarray '
     if 'numpy' in optionargs:



From saintmlx at mail.berlios.de  Wed Aug  8 19:47:11 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 8 Aug 2007 19:47:11 +0200
Subject: [Plearn-commits] r7957 - trunk/python_modules/plearn/utilities
Message-ID: <200708081747.l78HlBIq006742@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-08 19:47:10 +0200 (Wed, 08 Aug 2007)
New Revision: 7957

Added:
   trunk/python_modules/plearn/utilities/plnumpy.py
Log:
- new file for numpy



Added: trunk/python_modules/plearn/utilities/plnumpy.py
===================================================================
--- trunk/python_modules/plearn/utilities/plnumpy.py	2007-08-08 16:37:40 UTC (rev 7956)
+++ trunk/python_modules/plearn/utilities/plnumpy.py	2007-08-08 17:47:10 UTC (rev 7957)
@@ -0,0 +1,45 @@
+#
+# Copyright (C) 2007 Xavier Saint-Mleux, ApSTAT Technologies inc.
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+import numpy
+
+def getnan(a):
+    """
+    emulate numarray.ieeespecial.getnan on numpy arrays
+    """
+    return numpy.nonzero(numpy.isnan(a))
+
+def stddev(a):
+    import math
+    n= a.size
+    if n==0: return nan
+    if n==1: return 0
+    return math.sqrt(((a - a.mean())**2).sum()/(n-1))



From chapados at mail.berlios.de  Wed Aug  8 21:15:09 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 8 Aug 2007 21:15:09 +0200
Subject: [Plearn-commits] r7958 - trunk/plearn/var
Message-ID: <200708081915.l78JF9DX012189@sheep.berlios.de>

Author: chapados
Date: 2007-08-08 21:15:09 +0200 (Wed, 08 Aug 2007)
New Revision: 7958

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
Log:
Brought the non-lapack version up to date

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-08-08 17:47:10 UTC (rev 7957)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-08-08 19:15:09 UTC (rev 7958)
@@ -295,6 +295,12 @@
     kernel->computeGramMatrix(gram);
     addToDiagonal(gram, noise);
 
+    // The PLearn code relies on the matrix actually being symmetric in memory
+    // (assumption which LAPACK does not make).  Symmetrize the matrix
+    for (int i=1, n=gram.width() ; i<n ; ++i)
+        for (int j=0 ; j<i ; ++j)
+            gram(j,i) = gram(i,j);
+    
     // Save the Gram matrix if requested
     if (save_gram_matrix) {
         static int counter = 1;
@@ -329,6 +335,7 @@
 
 //#####  fbpropFragments (LAPACK)  ############################################
 
+// #if 0
 #ifdef USE_BLAS_SPECIALISATIONS
 void GaussianProcessNLLVariable::fbpropFragments(
     Kernel* kernel, real noise, const Mat& inputs, const Mat& targets,
@@ -390,8 +397,8 @@
     }
 }
 #endif
+// #endif
 
-
 //#####  logVarray  ###########################################################
 
 void GaussianProcessNLLVariable::logVarray(const VarArray& varr,



From yoshua at mail.berlios.de  Wed Aug  8 22:04:54 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Wed, 8 Aug 2007 22:04:54 +0200
Subject: [Plearn-commits] r7959 - trunk/plearn_learners/online
Message-ID: <200708082004.l78K4sTk014624@sheep.berlios.de>

Author: yoshua
Date: 2007-08-08 22:04:51 +0200 (Wed, 08 Aug 2007)
New Revision: 7959

Modified:
   trunk/plearn_learners/online/VBoundDBN2.cc
   trunk/plearn_learners/online/VBoundDBN2.h
Log:
did most of the bprop


Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-08 19:15:09 UTC (rev 7958)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-08 20:04:51 UTC (rev 7959)
@@ -100,6 +100,7 @@
     ports.append("sampled_h"); // 3
     ports.append("global_improvement"); // 4
     ports.append("ph_given_v"); // 5
+    ports.append("p2ph"); // 6
 }
 
 ///////////
@@ -124,8 +125,56 @@
     Mat* sampled_h_ = ports_value[3]; // a state if input is given
     Mat* global_improvement_ = ports_value[4]; // a state if input is given
     Mat* ph_given_v_ = ports_value[5]; // a state if input is given
+    Mat* p2ph_ = ports_value[6]; // same story
+    
+    PLASSERT( input && sampled_h_ && global_improvement_
+              && ph_given_v_ && p2ph_);
 
+    // do CD on rbm2
+    rbm2->setAllLearningRates(rbm2->cd_learning_rate);
+    rbm2->hidden_layer->setExpectations(*p2ph_);
+    rbm2->hidden_layer->generateSamples();
+    rbm2->computeHiddenActivations(rbm2->visible_layer->samples);
+    rbm2->visible_layer->update(*sampled_h_,rbm2->visible_layer->samples);
+    rbm2->connection->update(*sampled_h_,*p2ph_,
+                             rbm2->visible_layer->samples,
+                             rbm2->hidden_layer->getExpectations());
+    rbm2->hidden_layer->update(*p2ph_,rbm2->hidden_layer->getExpectations());
 
+    // for now do the ugly hack, for binomial + MatrixConnection case
+    PLASSERT(rbm1->visible_layer->classname()=="RBMBinomialLayer");
+    PLASSERT(rbm1->hidden_layer->classname()=="RBMBinomialLayer");
+    PLASSERT(rbm1->connection->classname() == "RBMMatrixConnection");
+    Mat& weights = ((RBMMatrixConnection*)
+                    get_pointer(rbm1->connection))->weights;
+    Vec& hidden_bias = rbm1->hidden_layer->bias;    
+    Vec& visible_bias = rbm1->hidden_layer->bias;    
+    static Mat delta_W;
+    static Vec delta_hb;
+    static Vec delta_vb;
+    static Mat delta_h;
+    deltaW.resize(rbm1->hidden_layer->size,rbm1->visible_layer->size);
+    delta_hb.resize(rbm1->hidden_layer->size);
+    delta_vb.resize(rbm1->visible_layer->size);
+    delta_h.resize(mbs,rbm1->hidden_layer->size);
+
+    // reconstruct the input
+    rbm1->computeVisibleActivations(*sampled_h_);
+    rbm1->visible_layer->computeExpectations();
+    Mat reconstructed_v = rbm1->visible_layer->getExpectations();
+
+    // compute RBM1 weight negative gradient
+    substract(*sampled_h_,*p2h_,delta_h);
+    multiply(delta_h, delta_h,global_improvement->toVec());
+    delta_h += *ph_given_v_;
+    productScaleAcc(deltaW, delta_h, true, *input, false, 1, 0);
+    productScaleAcc(deltaW, *sampled_h_, true, reconstructed_v, false, -1, 1);
+    // update the weights
+    multiplyAcc(weights, deltaW, rbm1->cd_learning_rate);
+
+    // do the biases now
+
+
     // Ensure all required gradients have been computed.
     checkProp(ports_gradient);
 }
@@ -173,6 +222,7 @@
     Mat* sampled_h_ = ports_value[3]; // a state if input is given
     Mat* global_improvement_ = ports_value[4]; // a state if input is given
     Mat* ph_given_v_ = ports_value[5]; // a state if input is given
+    Mat* p2ph_ = ports_value[6]; // same story
 
     // fprop has two modes:
     //  1) input is given (presumably for learning, or measuring bound or nll)
@@ -189,6 +239,7 @@
         Mat* sampled_h = sampled_h_?sampled_h_:&sampled_h_state;
         Mat* global_improvement = global_improvement_?global_improvement_:&global_improvement_state;
         Mat* ph_given_v = ph_given_v_?ph_given_v_:&ph_given_v_state;
+        Mat* p2ph = p2ph_?p2ph_:&p2ph_state;
         sampled_h->resize(mbs,rbm1->hidden_layer->size);
         global_improvement->resize(mbs,1);
         ph_given_v->resize(mbs,rbm1->hidden_layer->size);
@@ -199,9 +250,10 @@
         *ph_given_v << rbm1->hidden_layer->getExpectations();
         *sampled_h << rbm1->hidden_layer->samples;
         rbm1->visible_layer->fpropNLL(*sampled_h,neglogphsample_given_v);
-        rbm1->computeFreeEnergyOfVisible(*input,FE1v);
+        rbm1->computeFreeEnergyOfVisible(*input,FE1v,false);
         rbm1->computeFreeEnergyOfHidden(*sampled_h,FE1h);
-        rbm2->computeFreeEnergyOfVisible(*sampled_h,FE2h);
+        rbm2->computeFreeEnergyOfVisible(*sampled_h,FE2h,false);
+        *p2ph << rbm2->hidden_layer->getExpectations();
         substract(FE1h,FE2h,*global_improvement);
 
         if (bound) // actually minus the bound, to be in same units as nll, only computed exactly during test
@@ -284,6 +336,7 @@
     deepCopyField(sampled_h_state, copies);
     deepCopyField(global_improvement_state, copies);
     deepCopyField(ph_given_v_state, copies);
+    deepCopyField(p2ph_state, copies);
     deepCopyField(neglogphsample_given_v, copies);
     deepCopyField(all_h, copies);
     deepCopyField(all_h, copies);

Modified: trunk/plearn_learners/online/VBoundDBN2.h
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.h	2007-08-08 19:15:09 UTC (rev 7958)
+++ trunk/plearn_learners/online/VBoundDBN2.h	2007-08-08 20:04:51 UTC (rev 7959)
@@ -285,6 +285,7 @@
     Mat sampled_h_state;
     Mat global_improvement_state;
     Mat ph_given_v_state;
+    Mat p2ph_state;
     Mat neglogphsample_given_v;
     Mat all_h; // for computing exact likelihood
     Mat neglogP2h;



From simonl at mail.berlios.de  Wed Aug  8 22:36:18 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Wed, 8 Aug 2007 22:36:18 +0200
Subject: [Plearn-commits] r7960 - trunk/scripts/EXPERIMENTAL
Message-ID: <200708082036.l78KaIlH017623@sheep.berlios.de>

Author: simonl
Date: 2007-08-08 22:36:18 +0200 (Wed, 08 Aug 2007)
New Revision: 7960

Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
New cooler version!


Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-08 20:04:51 UTC (rev 7959)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-08 20:36:18 UTC (rev 7960)
@@ -5,7 +5,7 @@
 from plearn.io.server import *
 from plearn.pyplearn import *
 from plearn.plotting.netplot import *
-from numarray import *
+from numpy.numarray import *
 # from numpy.numarray import *
 #from numarray.random_array import *
 import numpy.random
@@ -80,13 +80,13 @@
 
         #if it's too tall, we do this little tweak
         gs = self.groupsize
-        height = self.hidden_layer.size()/gs
+        height = self.hidden_layer.size/gs
         self.nbgroups = 1
         while height > self.max_height:
             self.nbgroups+=1
-            while (self.hidden_layer.size()/gs)%self.nbgroups != 0 :
+            while (self.hidden_layer.size/gs)%self.nbgroups != 0 :
                 self.nbgroups+=1                       
-            height = self.hidden_layer.size()/gs/self.nbgroups
+            height = self.hidden_layer.size/gs/self.nbgroups
         print 'nbgroups', self.nbgroups
         
     def getMatrix(self):        
@@ -117,7 +117,7 @@
            self.hidden_layer[i] = row[c]
            c+=1
     def getNGroups(self):
-        return self.hidden_layer.size()/self.groupsize
+        return self.hidden_layer.size/self.groupsize
 
 
 class InteractiveRepRecPlotter:
@@ -472,8 +472,7 @@
                     matM1 = reshape(toMinusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
                     matW2 = reshape(toPlusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
                     matM2 = reshape(toPlusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
-
-                    #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
+                   
                     produit = matW1*matM1
                     produit2 = matW2*matM2
 
@@ -485,6 +484,29 @@
                     ion()
                     draw()
 
+#                     if nameWr in listNames and nameMr in listNames:
+
+#                         print 'ok'
+#                         rowWr = learner.getParameterRow(nameWr, n%hl.groupsize)
+#                         rowMr = learner.getParameterRow(nameMr, int(n/hl.groupsize))
+#                         print 'ok2'
+                        
+#                         matWr1 = reshape(toMinusRow(rowWr), (-1, self.hidden_layers[i-1].groupsize))
+#                         matMr1 = reshape(toMinusRow(rowMr), (-1, self.hidden_layers[i-1].groupsize))
+#                         matWr2 = reshape(toPlusRow(rowWr), (-1, self.hidden_layers[i-1].groupsize))
+#                         matMr2 = reshape(toPlusRow(rowMr), (-1, self.hidden_layers[i-1].groupsize))
+#                         print 'ok3'
+
+#                         produit = matWr1*matMr1
+#                         produit2 = matWr2*matMr2
+
+#                         figure(4)
+#                         ioff()
+#                         clf()
+#                         plotMatrices([matWr1, matMr1, matWr2, matMr2, produit, produit2], [nameWr + "-", nameMr + "-", nameWr + "+", nameMr + "+", 't.-t.-t. product (-)', 't.-t.-t. product (+)'])
+#                         ion()
+#                         draw()
+
                 #BIAS
 
                 if nameB in listNames:
@@ -538,38 +560,6 @@
                 n = hl.matrixToLayer(event.xdata, event.ydata)
                 print 'n =',n
             
-                
-                #                 if nameW in listNames and nameM not in listNames:
-                
-                #                     row_list = []
-                #                     for j in arange(n,n+hl.groupsize):
-                #                         row_list.append(learner.getParameterRow(nameW,j))
-                #                     print row_list
-                    
-                #                     #HACK !!!
-                #                     for row in row_list:
-                #                         if len(row) == 28*28*2:
-                #                             row = array(toMinusRow(list(row)))
-                #                             print 'just hacked...'
-                #                     #END OF HACK
-                
-                #                     for j,row in enumerate(row_list):
-                #                         matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
-                #                         namesToPlot.append(nameW + '_' + str(n+j))
-                
-                #                     if nameWr in listNames:
-                
-                #                         row = learner.getParameterRow(nameWr,n)
-                
-                #                         #HACK !!!
-                #                         print 'just hacked...'
-                #                         if i==1 and len(row) == 28*28*2:
-                #                             row = array(toMinusRow(list(row)))
-                #                         #END OF HACK 
-                
-                #                         matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
-                #                         namesToPlot.append(nameWr)
-
                 if nameW in listNames and nameM not in listNames:                
 
                     n = n - n%hl.groupsize
@@ -583,48 +573,7 @@
                     ion()
                     draw()
                     
-                #if nameM in listNames and nameM in listNames:
-                    
-                   # index_w = n%hl.groupsize
-                   # index_m = int(n/hl.groupsize)#
 
-#                    figure(3)
-#                    ioff()                    
-#                    clf()
-#                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.01, toMinusRow)
-#                    ion()
-#                    draw()
-#                    
-#                    figure(4)
-#                    ioff()
-#                    clf()
-#                    plotLayer1(learner.getParameterValue(nameM), 28, .1, n, hl.groupsize,.01, toMinusRow)
-#                    ion()
-#                    draw()
-                    
-                #  if nameW in listNames and nameM in listNames:
-                
-                #                     rowW = learner.getParameterRow(nameW,x)
-                #                     rowM = learner.getParameterRow(nameM,y)
-                
-                #                     #HACK !!!
-                #                     #print 'just hacked...'
-                #                     #if i==1 and len(row) == 28*28*2:
-                #                     #    row = array(toMinusRow(row))
-                #                     #END OF HACK
-                
-                #                     rowW = reshape(rowW, (-1,self.hidden_layers[i-1].groupsize))
-                #                     rowM = reshape(rowM,(-1,self.hidden_layers[i-1].groupsize))
-                
-                #                     #TODO: rajouter les deux cas  : juste un Wr et lautre : Mr ET Wr
-                
-                #                     produit = rowW*rowM                   
-                
-                #                     figure(3)
-                
-                #                     clf()                  
-                #                     plotMatrices([rowW,rowM,produit], [nameW,nameM, 'term-to-term product'])
-                #                     draw()
 
 
             # like 'w' on the max of each row -- 'C'
@@ -673,9 +622,27 @@
                     figure(4)
                     ioff()
                     clf()
-                    plotLayer1(learner.getParameterValue(nameM), 28, .05,0,0,.01,toMinusRow,indexes,names)
+                    plotLayer1(learner.getParameterValue(nameM), 28, .056,0,0,.01,toMinusRow,indexes,names)
                     ion()
                     draw()
+
+                if nameWr in listNames:
+
+                    figure(5)
+                    ioff()
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.01,toMinusRow,indexes,names)
+                    ion()
+                    draw()
+
+                if nameMr in listNames:
+
+                    figure(6)
+                    ioff()
+                    clf()
+                    plotLayer1(learner.getParameterValue(nameMr), 28, .056,0,0,.01,toMinusRow,indexes,names)
+                    ion()
+                    draw()
                     
                     
 



From simonl at mail.berlios.de  Wed Aug  8 22:37:04 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Wed, 8 Aug 2007 22:37:04 +0200
Subject: [Plearn-commits] r7961 - trunk/python_modules/plearn/var
Message-ID: <200708082037.l78Kb4x8017662@sheep.berlios.de>

Author: simonl
Date: 2007-08-08 22:37:04 +0200 (Wed, 08 Aug 2007)
New Revision: 7961

Modified:
   trunk/python_modules/plearn/var/Var.py
Log:
Added untied double product


Modified: trunk/python_modules/plearn/var/Var.py
===================================================================
--- trunk/python_modules/plearn/var/Var.py	2007-08-08 20:36:18 UTC (rev 7960)
+++ trunk/python_modules/plearn/var/Var.py	2007-08-08 20:37:04 UTC (rev 7961)
@@ -229,19 +229,41 @@
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input
 
-def addMultiSoftMaxDoubleProductRLayer(input, iw, igs, ow, ogs, basename=""):
+def addMultiSoftMaxDoubleProductRLayer(input, iw, igs, ow, ogs, add_bias=False, constrain_mask=False, basename="", positive=False):
     """iw is the input's width
     igs is the input's group size
     ow and ogs analog but for output"""
+    
+    #TODO: constrain_mask is not used...
+    if constrain_mask:
+        raise Exception("not implemented yet")
+    
     ra = 1./max(iw,ow)
     sqra = sqrt(ra)
-    M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
-    W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
-    hidden = input.doubleProduct(W,M).multiSoftMax(ogs)
-    Mr = Var(iw/igs, ow, "uniform", -sqra, sqra, False, varname=basename+"_Mr")
-    Wr = Var(igs, ow, "uniform", -sqra, sqra, False, varname=basename+"_Wr")
+
+    if positive:
+        W = Var(ogs, iw, "uniform", 0, sqra, False, varname=basename+"_W", min_value=0, max_value=1e100)
+        M = Var(ow/ogs, iw, "uniform", 0, sqra, False, varname=basename+"_M", min_value=0, max_value=1e100)
+    else:           
+        M = Var(ow/ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_M")
+        W = Var(ogs, iw, "uniform", -sqra, sqra, False, varname=basename+"_W")
+    
+    if positive:
+        Mr = Var(iw/igs, ow, "uniform", 0, sqra, False, varname=basename+"_Mr", min_value=0, max_value=1e100)
+        Wr = Var(igs, ow, "uniform", 0, sqra, False, varname=basename+"_Wr", min_value=0, max_value=1e100)
+    else:
+        Mr = Var(iw/igs, ow, "uniform", -sqra, sqra, False, varname=basename+"_Mr")
+        Wr = Var(igs, ow, "uniform", -sqra, sqra, False, varname=basename+"_Wr")
+        
     # TODO: a repenser s'il faut un transpose ou non
-    log_reconstructed = hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs)
+    if add_bias:
+        b = Var(1,ow,"fill",0, varname=basename+'_b')        
+        hidden = input.doubleProduct(W,M).add(b).multiSoftMax(ogs)
+        br = Var(1,iw,"fill",0, varname=basename+'_br')
+        log_reconstructed = hidden.doubleProduct(Wr,Mr).add(br).multiLogSoftMax(igs)
+    else:    
+        hidden = input.doubleProduct(W,M).multiSoftMax(ogs)    
+        log_reconstructed = hidden.doubleProduct(Wr,Mr).multiLogSoftMax(igs)
     reconstructed_input = log_reconstructed.exp()
     cost = log_reconstructed.dot(input).neg()
     return hidden, cost, reconstructed_input



From simonl at mail.berlios.de  Wed Aug  8 22:40:05 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Wed, 8 Aug 2007 22:40:05 +0200
Subject: [Plearn-commits] r7962 - trunk/plearn/base
Message-ID: <200708082040.l78Ke5A8017790@sheep.berlios.de>

Author: simonl
Date: 2007-08-08 22:40:04 +0200 (Wed, 08 Aug 2007)
New Revision: 7962

Modified:
   trunk/plearn/base/Object.cc
Log:
Corrected bug when compiling in -nopython:
Put declareMethod of pyDeepCopy in between #ifdef PL_PYTHON_VERSION 


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-08-08 20:37:04 UTC (rev 7961)
+++ trunk/plearn/base/Object.cc	2007-08-08 20:40:04 UTC (rev 7962)
@@ -751,13 +751,14 @@
                   (BodyDoc("Returns a deep copy of the object"),
                    RetDoc ("Deep copy.")));
 
+#ifdef PL_PYTHON_VERSION 
     declareMethod(rmm, "pyDeepCopy", &Object::pyDeepCopy,
                   (BodyDoc("Returns a pair containing a deep copy of "
                            "the object and the updated copies map."),
                    ArgDoc ("copies", "The initial copies map"),
                    RetDoc ("Deep copy, copies map")));
+#endif //def PL_PYTHON_VERSION 
 
-
 #ifdef PL_PYTHON_VERSION 
     declareMethod(rmm, "setOptionFromPython", &Object::setOptionFromPython,
                   (BodyDoc("Change an option within the object from a PythonObjectWrapper"),



From saintmlx at mail.berlios.de  Wed Aug  8 23:42:59 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 8 Aug 2007 23:42:59 +0200
Subject: [Plearn-commits] r7963 - trunk/plearn/io
Message-ID: <200708082142.l78LgxPH023307@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-08 23:42:59 +0200 (Wed, 08 Aug 2007)
New Revision: 7963

Modified:
   trunk/plearn/io/PStream.h
Log:
- avoid warnings when compiling PStream



Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-08-08 20:40:04 UTC (rev 7962)
+++ trunk/plearn/io/PStream.h	2007-08-08 21:42:59 UTC (rev 7963)
@@ -304,6 +304,10 @@
 
     //! Writes base types to PLearn binary format
     //! TODO: forbid this mechanism for arbitrary classes?
+    template<bool x> struct chkUnsigned 
+    { template<class I> static bool notOk(I& y) { return false; } };
+    struct chkUnsigned<true> 
+    { template<class I> static bool notOk(I& y) { return y<0; } };
     template<class I>
     void writeBinaryNum(I x)
     {
@@ -332,8 +336,10 @@
         x = static_cast<J>(y);
 
         // Check if there was a conversion problem (sign or overflow)
+
         if (static_cast<I>(x) != y
-            || !(numeric_limits<J>::is_signed) && y<0)
+            || !(numeric_limits<J>::is_signed) 
+            && chkUnsigned<numeric_limits<I>::is_signed>::notOk(y))
         {
             std::stringstream error;
             error << "In PStream::readBinaryNumAs, overflow error: " << std::endl



From yoshua at mail.berlios.de  Thu Aug  9 03:40:36 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 9 Aug 2007 03:40:36 +0200
Subject: [Plearn-commits] r7964 - trunk/plearn_learners/online
Message-ID: <200708090140.l791eaWo019603@sheep.berlios.de>

Author: yoshua
Date: 2007-08-09 03:40:27 +0200 (Thu, 09 Aug 2007)
New Revision: 7964

Modified:
   trunk/plearn_learners/online/RBMModule.h
   trunk/plearn_learners/online/VBoundDBN2.cc
Log:
Finished v0 of VBoundDBN2, needed some little changes in RBMModule.h


Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-08 21:42:59 UTC (rev 7963)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-09 01:40:27 UTC (rev 7964)
@@ -267,9 +267,6 @@
     //! Add a new port to the 'portname_to_index' map and 'ports' vector.
     void addPortName(const string& name);
 
-    //! Forward the given learning rate to all elements of this module.
-    void setAllLearningRates(real lr);
-        
     //! Forward the given learning rate to all elements of the layers
     //! and to the reconstruction connections (NOT of the connection weights).
     void setLearningRatesOnlyForLayers(real lr);
@@ -278,6 +275,9 @@
     static void declareOptions(OptionList& ol);
 
 public:
+    //! Forward the given learning rate to all elements of this module.
+    void setAllLearningRates(real lr);
+        
     //! Compute activations on the hidden layer based on the provided
     //! visible input.
     //! If 'hidden_bias' is not null nor empty, then it is used as an

Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-08 21:42:59 UTC (rev 7963)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-09 01:40:27 UTC (rev 7964)
@@ -37,7 +37,7 @@
 /*! \file VBoundDBN2.cc */
 
 
-
+#include <plearn_learners/online/RBMMatrixConnection.h>
 #include "VBoundDBN2.h"
 
 namespace PLearn {
@@ -126,7 +126,7 @@
     Mat* global_improvement_ = ports_value[4]; // a state if input is given
     Mat* ph_given_v_ = ports_value[5]; // a state if input is given
     Mat* p2ph_ = ports_value[6]; // same story
-    
+    int mbs = input->length();
     PLASSERT( input && sampled_h_ && global_improvement_
               && ph_given_v_ && p2ph_);
 
@@ -147,15 +147,15 @@
     PLASSERT(rbm1->connection->classname() == "RBMMatrixConnection");
     Mat& weights = ((RBMMatrixConnection*)
                     get_pointer(rbm1->connection))->weights;
-    Vec& hidden_bias = rbm1->hidden_layer->bias;    
-    Vec& visible_bias = rbm1->hidden_layer->bias;    
     static Mat delta_W;
     static Vec delta_hb;
-    static Vec delta_vb;
+    static Vec delta_vb1;
+    static Vec delta_vb2;
     static Mat delta_h;
-    deltaW.resize(rbm1->hidden_layer->size,rbm1->visible_layer->size);
+    delta_W.resize(rbm1->hidden_layer->size,rbm1->visible_layer->size);
     delta_hb.resize(rbm1->hidden_layer->size);
-    delta_vb.resize(rbm1->visible_layer->size);
+    delta_vb1.resize(rbm1->visible_layer->size);
+    delta_vb2.resize(rbm1->visible_layer->size);
     delta_h.resize(mbs,rbm1->hidden_layer->size);
 
     // reconstruct the input
@@ -164,17 +164,27 @@
     Mat reconstructed_v = rbm1->visible_layer->getExpectations();
 
     // compute RBM1 weight negative gradient
-    substract(*sampled_h_,*p2h_,delta_h);
-    multiply(delta_h, delta_h,global_improvement->toVec());
+    //  dlogbound/dWij sampling approx = (ph_given_v[i] + (h[i]-ph_given_v[i])*global_improvement)*v[j] - h[i]*reconstructed_v[j]
+    substract(*sampled_h_,*ph_given_v_,delta_h);
+    multiply(delta_h, delta_h,global_improvement_->toVec());
     delta_h += *ph_given_v_;
-    productScaleAcc(deltaW, delta_h, true, *input, false, 1, 0);
-    productScaleAcc(deltaW, *sampled_h_, true, reconstructed_v, false, -1, 1);
+    productScaleAcc(delta_W, delta_h, true, *input, false, 1., 0.);
+    productScaleAcc(delta_W, *sampled_h_, true, reconstructed_v, false, -1., 1.);
     // update the weights
-    multiplyAcc(weights, deltaW, rbm1->cd_learning_rate);
+    multiplyAcc(weights, delta_W, rbm1->cd_learning_rate);
 
     // do the biases now
+    //  dlogbound/dbi sampling approx = (ph_given_v[i] + (h[i]-ph_given_v[i])*global_improvement) - h[i]
+    substract(delta_h, *sampled_h_, delta_h);
+    columnSum(delta_h,delta_hb);
+    multiplyAcc(rbm1->hidden_layer->bias,delta_hb,rbm1->cd_learning_rate);
+    
+    //  dlogbound/dji sampling approx = v[j] - reconstructed_v[j]
+    columnSum(reconstructed_v,delta_vb1);
+    columnSum(*input,delta_vb2);
+    substract(delta_vb2,delta_vb1,delta_vb1);
+    multiplyAcc(rbm1->visible_layer->bias,delta_vb1,rbm1->cd_learning_rate);
 
-
     // Ensure all required gradients have been computed.
     checkProp(ports_gradient);
 }



From yoshua at mail.berlios.de  Thu Aug  9 03:41:00 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 9 Aug 2007 03:41:00 +0200
Subject: [Plearn-commits] r7965 - trunk/plearn/math
Message-ID: <200708090141.l791f0a5019639@sheep.berlios.de>

Author: yoshua
Date: 2007-08-09 03:41:00 +0200 (Thu, 09 Aug 2007)
New Revision: 7965

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
Added another multiplication procedure


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-08-09 01:40:27 UTC (rev 7964)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-08-09 01:41:00 UTC (rev 7965)
@@ -4479,31 +4479,47 @@
     }
 }
 
-
-// result[i,j] = x[i,j]*scale;
+// result[i,j] = x[i,j]*y[i] or x[i,j]*y[j] (transpose case)
 template<class T>
-void multiply(const TMat<T>& result, const TMat<T>& x, T scale)
+void multiply(TMat<T>& result, const TMat<T>& x, const TVec<T>& y, bool transpose=false)
 {
-#ifdef BOUNDCHECK
-    if (result.length()!=x.length() || result.width()!=x.width())
-        PLERROR("multiply incompatible dimensions: %dx%d <- %dx%d",
-                result.length(),result.width(),x.length(),x.width());
-#endif
+    PLASSERT_MSG(transpose && x.width()==y.length() ||
+                 !transpose && x.length()==y.length(),
+                 "multiply matrix rows or columns by vector: incompatible dimensions");
+    result.resize(x.length(),x.width());
     if(result.isCompact() && x.isCompact())
     {
         typename TMat<T>::compact_iterator itm = result.compact_begin();
-        typename TMat<T>::compact_iterator itmend = result.compact_end();
         typename TMat<T>::compact_iterator itx = x.compact_begin();
-        for(; itm!=itmend; ++itm, ++itx)
-            *itm = *itx * scale;
+        typename TVec<T>::iterator ity = y.begin();
+        if (transpose)
+            for (int i=0;i<x.length();i++)
+            {
+                ity = y.begin();
+                for (int j=0;j<x.width();j++,++itx,++itm,++ity)
+                    *itm = *itx * *ity;
+            }
+        else
+            for (int i=0;i<x.length();i++,++ity)
+                for (int j=0;j<x.width();j++,++itx,++itm)
+                    *itm = *itx * *ity;
     }
     else // use non-compact iterators
     {
         typename TMat<T>::iterator itm = result.begin();
-        typename TMat<T>::iterator itmend = result.end();
         typename TMat<T>::iterator itx = x.begin();
-        for(; itm!=itmend; ++itm, ++itx)
-            *itm = *itx * scale;
+        typename TVec<T>::iterator ity = y.begin();
+        if (transpose)
+            for (int i=0;i<x.length();i++)
+            {
+                ity = y.begin();
+                for (int j=0;j<x.width();j++,++itx,++itm,++ity)
+                    *itm = *itx * *ity;
+            }
+        else
+            for (int i=0;i<x.length();i++,++ity)
+                for (int j=0;j<x.width();j++,++itx,++itm)
+                    *itm = *itx * *ity;
     }
 }
 



From yoshua at mail.berlios.de  Thu Aug  9 14:18:20 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 9 Aug 2007 14:18:20 +0200
Subject: [Plearn-commits] r7966 - trunk/plearn/math
Message-ID: <200708091218.l79CIKbl021057@sheep.berlios.de>

Author: yoshua
Date: 2007-08-09 14:18:20 +0200 (Thu, 09 Aug 2007)
New Revision: 7966

Modified:
   trunk/plearn/math/TMat_maths_impl.h
Log:
Put back a fn which had been removed by error


Modified: trunk/plearn/math/TMat_maths_impl.h
===================================================================
--- trunk/plearn/math/TMat_maths_impl.h	2007-08-09 01:41:00 UTC (rev 7965)
+++ trunk/plearn/math/TMat_maths_impl.h	2007-08-09 12:18:20 UTC (rev 7966)
@@ -4479,6 +4479,33 @@
     }
 }
 
+// result[i,j] = x[i,j]*scale;
+template<class T>
+void multiply(const TMat<T>& result, const TMat<T>& x, T scale)
+{
+#ifdef BOUNDCHECK
+    if (result.length()!=x.length() || result.width()!=x.width())
+        PLERROR("multiply incompatible dimensions: %dx%d <- %dx%d",
+                result.length(),result.width(),x.length(),x.width());
+#endif
+    if(result.isCompact() && x.isCompact())
+    {
+        typename TMat<T>::compact_iterator itm = result.compact_begin();
+        typename TMat<T>::compact_iterator itmend = result.compact_end();
+        typename TMat<T>::compact_iterator itx = x.compact_begin();
+        for(; itm!=itmend; ++itm, ++itx)
+            *itm = *itx * scale;
+    }
+    else // use non-compact iterators
+    {
+        typename TMat<T>::iterator itm = result.begin();
+        typename TMat<T>::iterator itmend = result.end();
+        typename TMat<T>::iterator itx = x.begin();
+        for(; itm!=itmend; ++itm, ++itx)
+            *itm = *itx * scale;
+    }
+}
+
 // result[i,j] = x[i,j]*y[i] or x[i,j]*y[j] (transpose case)
 template<class T>
 void multiply(TMat<T>& result, const TMat<T>& x, const TVec<T>& y, bool transpose=false)



From lysiane at mail.berlios.de  Thu Aug  9 17:29:10 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Thu, 9 Aug 2007 17:29:10 +0200
Subject: [Plearn-commits] r7967 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200708091529.l79FTAQF000491@sheep.berlios.de>

Author: lysiane
Date: 2007-08-09 17:29:09 +0200 (Thu, 09 Aug 2007)
New Revision: 7967

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
Modifs in methods MStepTransformation, applyTransformationOn, purpose = more rapidity  
experiment : dir = exp2_tiny_10..., profiler file= gprof2.txt
             compilation: without special options (except profiler option)
             time :(real=1m9.334s, user=0m6.872s, sys=0m0.252s)          


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 12:18:20 UTC (rev 7966)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 15:29:09 UTC (rev 7967)
@@ -424,6 +424,9 @@
                    RetDoc("mdXd matrix, m = number of transformations \n"
                           "             d = dimensionality of the input space")));
     
+    declareMethod(rmm,"buildLearnedParameters",
+                  &TransformationLearner::buildLearnedParameters,
+                  (BodyDoc("builds the structures related to learned parameters")));
     declareMethod(rmm,
                   "generatorBuild",
                   &TransformationLearner::generatorBuild,
@@ -593,7 +596,7 @@
     neighborIdx=pickNeighborIdx();
     Vec neighbor;
     neighbor.resize(inputSpaceDim);
-    seeNeighbor(neighborIdx, neighbor);
+    seeTrainingPoint(neighborIdx, neighbor);
     generatePredictedFrom(neighbor, y);
 }
 
@@ -605,28 +608,8 @@
 }
 
 
-/*TVec<string> PDistribution::getTestCostNames() const
-{
-    TVec<string> nll_cost;
-    if (nll_cost.isEmpty())
-        nll_cost.append("NLL");
-    return nll_cost;
-    }*/
 
-///////////////////////
-// getTrainCostNames //
-///////////////////////
-TVec<string> TransformationLearner::getTrainCostNames() const
-{
-    return getTestCostNames();
-}
 
-
-
-
-
-
-
 /////////////////
 // log_density //
 /////////////////
@@ -1029,6 +1012,7 @@
 void TransformationLearner::setTransformsParameters(TVec<Mat> transforms_,
                                                     Mat biasSet_)
 {
+    
     PLASSERT(transforms_.length() == nbTransforms);
     
     int nbRows = inputSpaceDim*nbTransforms;
@@ -1349,7 +1333,7 @@
         transformIdx= reconstructionSet[candidateIdx].transformIdx;
         Vec neighbor;
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(neighborIdx, neighbor);
+        seeTrainingPoint(neighborIdx, neighbor);
         Vec v = reconstructions(i);
         applyTransformationOn(transformIdx, neighbor, v);
         candidateIdx ++;
@@ -1368,7 +1352,7 @@
         neighborIdx = reconstructionSet[candidateIdx].neighborIdx;
         Vec neighbor;
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(neighborIdx, neighbor);
+        seeTrainingPoint(neighborIdx, neighbor);
         neighbors(i) << neighbor;
         candidateIdx++;
     }
@@ -1405,23 +1389,13 @@
                                                        nbTargetReconstructions); 
 }
 
-// stores the "targetIdx"th point in the training set into the variable
-// "target"
-void TransformationLearner::seeTarget(const int targetIdx, Vec & storage)const
-{
-    Vec v;
-    real w;
-    train_set->getExample(targetIdx,storage,v,w);
-    
-}
 
-// stores the "neighborIdx"th input in the training set into the variable
-// "neighbor" 
-void TransformationLearner::seeNeighbor(const int neighborIdx, Vec & neighbor)const
+// stores the 'idx'th training data point into 'dst'
+void TransformationLearner::seeTrainingPoint(const int idx, Vec & dst)const
 {
     Vec v;
     real w;
-    train_set->getExample(neighborIdx, neighbor,v,w);
+    train_set->getExample(idx, dst,v,w);
 }
 
 
@@ -1607,7 +1581,7 @@
 
     Vec target;
     target.resize(inputSpaceDim);
-    seeTarget(targetIdx,target);
+    seeTrainingPoint(targetIdx,target);
     return computeReconstructionWeight(target,
                                        neighborIdx,
                                        transformIdx);
@@ -1618,7 +1592,7 @@
 {
     Vec neighbor;
     neighbor.resize(inputSpaceDim);
-    seeNeighbor(neighborIdx, neighbor);
+    seeTrainingPoint(neighborIdx, neighbor);
     Vec predictedTarget ;
     predictedTarget.resize(inputSpaceDim);
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
@@ -1634,14 +1608,16 @@
 {
     if(transformFamily==TRANSFORM_FAMILY_LINEAR){
         Mat m  = transforms[transformIdx];
-        transposeProduct(dst,m,src); 
+        //transposeProduct(dst,m,src); 
+        product(dst,m,src);
         if(withBias){
             dst += biasSet(transformIdx);
         }
     }
     else{ //transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT
         Mat m = transforms[transformIdx];
-        transposeProduct(dst,m,src);
+        //transposeProduct(dst,m,src);
+        product(dst,m,src);
         dst += src;
         if(withBias){
             dst += biasSet(transformIdx);
@@ -1776,7 +1752,7 @@
     //capture the target from his index in the training set
     Vec target;
     target.resize(inputSpaceDim);
-    seeTarget(targetIdx, target);
+    seeTrainingPoint(targetIdx, target);
     
     //for each potential neighbor,
     real dist;    
@@ -1785,7 +1761,7 @@
             //computes the distance to the target
             Vec neighbor;
             neighbor.resize(inputSpaceDim);
-            seeNeighbor(i, neighbor);
+            seeTrainingPoint(i, neighbor);
             dist = powdistance(target, neighbor); 
             //if the distance is among "nbNeighbors" smallest distances seen,
             //keep it until to see a closer neighbor. 
@@ -2032,8 +2008,7 @@
     if(biasPeriod > 0 && stage % biasPeriod == biasOffset)
         MStepBias();
     if(stage % transformsPeriod == transformsOffset)
-        MStepTransformations();
-    
+        MStepTransformations();    
 }
 
 //!maximization step  with respect to  transformation distribution
@@ -2080,6 +2055,11 @@
 
 //!maximization step with respect to transformation parameters
 //!(MAP version)
+/*-Notation: we will use the symbol _T to indicate the transposition operation
+  -To better understand how the algorithm  is working, 
+   see the NOTE (in comments) placed right after the method.
+   (The method is called often and has to be efficient. Some details
+   of the implantation might be a bit unclear for this reason.) */
 void TransformationLearner::MStepTransformations()
 {
     
@@ -2098,10 +2078,10 @@
         //catch the target and neighbor points from the training set
         Vec target;
         target.resize(inputSpaceDim);
-        seeTarget(reconstructionSet[idx].targetIdx, target);
+        seeTrainingPoint(reconstructionSet[idx].targetIdx, target);
         Vec neighbor;
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
         
         int t = reconstructionSet[idx].transformIdx;
         
@@ -2113,17 +2093,64 @@
         if(withBias){
             v = v - biasSet(t);
         }
+        //at the end, we want that matrix C[t] represents
+        //the matrix ( (NeighborPart(t)_T)W(NeighborPart(t)) + lambdaI ) transposed. 
         externalProductScaleAcc(C[t], neighbor, neighbor, p);
         
-        externalProductScaleAcc(B[t], neighbor, v,p); 
+        //at the end, that matrix B[t] represents
+        //the matrix (NeighborPart(t)_T)W(TargetPart(t)) transposed.
+        //externalProductScaleAcc(B[t], neighbor, v,p);
+        externalProductScaleAcc(B[t],v,neighbor,p);
     }
+    
+    TVec<int> pivots(inputSpaceDim);
     for(int t=0; t<nbTransforms; t++){
         addToDiagonal(C[t],lambda);
-        transforms[t] << solveLinearSystem(C[t], B[t]);  
+        //transforms[t] << solveLinearSystem(C[t], B[t]);  
+        lapackSolveLinearSystem(C[t],B[t],pivots);
+        transforms[t] << B[t];
+        
     }  
 }
+/*NOTE : MStepTransformations() 
+ -Notation: we will use the symbol _T to indicate the transposition operation
+ -The algorithm consist in solving a linear system :
+      for each t, we want to find
+                 transforms(t)=X ,
+                 with X such that : E(t)X_T=D(t)
+                 Here, 
+                 E(t) = (NeighborPart(t)_T)W(NeighborPart(t)) + lambda(I)
+                 D(t) = (NeighborPart(t)_T)W(TargetPart(t))
+ -We will compute E(t)_T = C(t) , and D(t)_T =B(t)
+  in the algorithm. It is necessary to compute directly those transposed 
+  versions of E(t) and D(t) to solve the linear system with efficiency. 
+ -once the computations of C(t) and B(t) are done,
+  we use a method from plapack package to solve our linear system 
+    lapackSolveLinearSystem(A_T,B_T, pivots):
+    Here is a copy of the description of the method: 
+ -------------------------------------------------------------------------------------------------------
+   Solves AX = B
+  This is a simple wrapper over the lapack routine. It expects At and Bt (transposes of A and B) as input, 
+  as well as storage for resulting pivots vector of ints of same length as A.
+  The call overwrites Bt, putting the transposed solution Xt in there,
+  and At is also overwritten to contain the factors L and U from the factorization A = P*L*U; 
+  (the unit diagonal elements of L  are  not stored).
+  The lapack status is returned:
+  = 0:  successful exit
+  < 0:  if INFO = -i, the i-th argument had an illegal value
+  > 0:  if INFO = i, U(i,i) is  exactly  zero.   The factorization has been completed, 
+  but the factor U is exactly singular, so the solution could not be computed.
+--------------------------------------------------------------------------------------------------------
+-Like you can see, we have to transmit the transposed versions of matrices
+ E(t) and D(t) to the procedure, that is, matrices C(t) and B(t)
+ -The matrix transforms(t)=X will be stored in B(t) at the end of the algorithm
+*/
+
  
 
+
+
+
 //TODO
 //!maximization step with respect to transformation bias
 //!(MAP version)
@@ -2143,9 +2170,9 @@
         weight = reconstructionSet[idx].weight;
         proba = PROBA_weight(weight);
         target.resize(inputSpaceDim);
-        seeTarget(reconstructionSet[idx].targetIdx,target);
+        seeTrainingPoint(reconstructionSet[idx].targetIdx,target);
         neighbor.resize(inputSpaceDim);
-        seeNeighbor(reconstructionSet[idx].neighborIdx, neighbor);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
         reconstruction.resize(inputSpaceDim);
         applyTransformationOn(transformIdx,neighbor, reconstruction);
         newBiasSet(transformIdx) += proba*(target - reconstruction);
@@ -2189,10 +2216,10 @@
 real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
     Vec target;
     target.resize(inputSpaceDim);
-    seeTarget(reconstructionSet[candidateIdx].targetIdx, target);
+    seeTrainingPoint(reconstructionSet[candidateIdx].targetIdx, target);
     Vec neighbor;
     neighbor.resize(inputSpaceDim);
-    seeNeighbor(reconstructionSet[candidateIdx].neighborIdx,
+    seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx,
                 neighbor);
     Vec reconstruction;
     reconstruction.resize(inputSpaceDim);

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 12:18:20 UTC (rev 7966)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 15:29:09 UTC (rev 7967)
@@ -306,7 +306,7 @@
     //#####  PDistribution Member Functions  ##################################
  
 
-    virtual TVec<string> getTrainCostNames() const;
+    // virtual TVec<string> getTrainCostNames() const;
     
     
     //! Return log of probability density log(p(y | x)).
@@ -654,14 +654,12 @@
     //! target (into the variable "targetReconstructionSet" )
     void seeTargetReconstructionSet(int targetIdx,
                                     TVec<ReconstructionCandidate> & targetReconstructionSet)const ;
-    // stores the "targetIdx"th point in the training set into the variable
-    // "target"
-    void seeTarget(const int targetIdx, Vec & target) const ;
-    // stores the "neighborIdx"th input in the training set into the variable
-    // "neighbor" 
-    void seeNeighbor(const int neighborIdx, Vec & neighbor)const;
+ 
 
+    //! stores the "idx"th training data point into the variable 'dst'
+    void seeTrainingPoint(const int idx, Vec & dst) const ;
 
+
     //!GENERATE GAMMA RANDOM VARIABLES
     
     //!source of the algorithm: http://oldmill.uchicago.edu/~wilder/Code/random/Papers/Marsaglia_00_SMGGV.pdf



From chrish at mail.berlios.de  Thu Aug  9 17:42:22 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 9 Aug 2007 17:42:22 +0200
Subject: [Plearn-commits] r7968 - in branches: . numarray
	numarray/python_modules/plearn/plide
	numarray/python_modules/plearn/pyplearn
Message-ID: <200708091542.l79FgM2j001243@sheep.berlios.de>

Author: chrish
Date: 2007-08-09 17:42:22 +0200 (Thu, 09 Aug 2007)
New Revision: 7968

Added:
   branches/numarray/
   branches/numarray/python_modules/
   branches/numarray/python_modules/plearn/plide/plide.py
   branches/numarray/python_modules/plearn/pyplearn/plargs.py
Removed:
   branches/numarray/python_modules/
   branches/numarray/python_modules/plearn/plide/plide.py
   branches/numarray/python_modules/plearn/pyplearn/plargs.py
Log:
Branch for numarray version of PLearn, for maintenance.


Copied: branches/numarray (from rev 7906, trunk)

Copied: branches/numarray/python_modules (from rev 7938, trunk/python_modules)

Deleted: branches/numarray/python_modules/plearn/plide/plide.py
===================================================================
--- trunk/python_modules/plearn/plide/plide.py	2007-08-07 00:50:45 UTC (rev 7938)
+++ branches/numarray/python_modules/plearn/plide/plide.py	2007-08-09 15:42:22 UTC (rev 7968)
@@ -1,968 +0,0 @@
-#  plide.py
-#  Copyright (C) 2006 by Nicolas Chapados
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-
-#####  Python Imports  ######################################################
-
-import fcntl
-import os, os.path
-import Queue
-import re
-import select, sys
-import threading, time, traceback
-
-
-#####  GTK Imports  #########################################################
-
-import gobject
-import pygtk
-pygtk.require('2.0')
-import gtk, gtk.gdk
-import pango
-from plearn.pl_pygtk import GladeAppWindow, GladeDialog, MessageBox
-
-
-#####  PLearn Imports  ######################################################
-
-from plearn.utilities.metaprog import public_members
-from plearn.pyplearn           import *
-from plearn.utilities.toolkit  import doc as toolkit_doc
-
-
-#####  Plide  ###############################################################
-
-from plide_help    import PlideHelp
-from plide_options import *
-from plide_tabs    import *
-from plide_utils   import *
-
-
-#####  Exports  #############################################################
-
-__all__ = [
-    'StartPlide',
-    'QuitPlide',
-    'GetWork',
-    'PostWorkResults',
-    'LogAppend',
-    'AllocateProgressBar',
-    'ReleaseProgressBar',
-    'ProgressUpdate'
-    ]
-
-
-#####  Global Configuration  ################################################
-
-def gladeFile():
-    import plearn.plide.plide
-    return os.path.join(os.path.dirname(plearn.plide.plide.__file__),
-                        "resources", "plide.glade")
-
-def helpResourcesPath():
-    import plearn.plide.plide
-    return os.path.join(os.path.dirname(plearn.plide.plide.__file__),
-                        "resources")
-
-
-#####  Main Window  #########################################################
-
-class PlideMain( GladeAppWindow ):
-
-    PlideVersion = "0.01"
-
-    def __init__( self, streams_to_watch= {}, *args, **kwargs ):
-        GladeAppWindow.__init__(self, gladeFile())
-
-        ## Forward injected to imported Plide modules
-        PlideHelp.define_injected(injected)
-        PlideTab. define_injected(injected)
-        PyPLearnOptionsDialog.define_injected(injected, gladeFile)
-
-        ## Initialize Members
-        self.untitled_counter  = 1
-        self.work_requests = {}         # Request ids to expdir mapping
-        self.all_plearn_classes = injected.getAllClassnames()
-
-        ## Initialize Display
-        self.setup_statusbar()
-        self.log_filters = [ re.compile("WARNING.*Scintilla.*PosChanged.*deprecated") ]
-        self.log_clear()
-        self.log_hide()
-        welcome_text = kwargs.get("welcome_text",
-                                  "<b>Welcome to Plide %s!</b>" % self.PlideVersion)
-        self.status_display(welcome_text, has_markup=True)
-        self.setup_stdouterr_redirect(streams_to_watch)
-        
-        ## Set up help system
-        injected.helpResourcesPath(helpResourcesPath())
-        self.help_viewer = PlideHelp(self)
-        self.help_viewer.display_page("index.html")
-        self.help_close()
-
-        ## Prepare the work queue
-        self.work_queue = PLearnWorkQueue()
-
-    def quit( self ):
-        ## Minor hack: the main-thread loop is terminated by receiving a
-        ## 'script' whose contents is Quit().  First close all tabs and
-        ## ensure that we stop the process if some tabs won't be closed.
-        n = self.w_plide_notebook.get_n_pages()
-        for i in range(n-1,-1,-1):
-            tab = self.get_nth_tab(i)
-            if not tab.close_tab():
-                return True        # Stop close process if cannot close tab
-        
-        print >>raw_stderr, "Quit message received"
-        raw_stderr.flush()
-        self.work_queue.post_work_request("Quit()","","")
-        GladeAppWindow.quit(self)
-
-    def help_close( self ):
-        """Close the help pane.
-        """
-        self.w_help_frame.hide()
-        self.w_help_frame.set_no_show_all(True)
-
-    def help_show( self ):
-        """Open the help pane.  Bring up context-sensitive help if there is
-        a valid context in the current tab.
-        """
-        self.w_help_frame.set_no_show_all(False)
-        self.w_help_frame.show()
-
-        curtab = self.get_current_tab()
-        if curtab:
-            help_context = curtab.get_help_candidate()
-            if help_context:
-                self.help_viewer.display_page(help_context)
-            
-    def setup_stdouterr_redirect( self, streams_to_watch= {} ):
-        """Redirect standard output and error to be sent to the log pane
-        instead of the console.
-        """
-        ## Redirect standard output and standard error to display to the
-        ## log pane area.  Keep around old stdout/stderr in order to
-        ## display debugging messages.  They are called, respectively,
-        ## raw_stdout and raw_stderr (Python file objects)
-        global raw_stdout, raw_stderr
-        old_stdout_fd = os.dup(sys.stdout.fileno())
-        old_stderr_fd = os.dup(sys.stderr.fileno())
-        raw_stdout    = os.fdopen(old_stdout_fd, 'w')
-        raw_stderr    = os.fdopen(old_stderr_fd, 'w')
-
-        print >>sys.stderr, "Original stderr"
-        print >>raw_stderr, "Redirected stderr"
-        sys.stderr.flush()
-        raw_stderr.flush()
-        
-        (self.stdout_read, self.stdout_write) = os.pipe()
-        (self.stderr_read, self.stderr_write) = os.pipe()
-        os.dup2(self.stdout_write, sys.stdout.fileno())
-        os.dup2(self.stderr_write, sys.stderr.fileno())
-        
-        out_flags = fcntl.fcntl(self.stdout_read, fcntl.F_GETFL)
-        err_flags = fcntl.fcntl(self.stderr_read, fcntl.F_GETFL)
-        fcntl.fcntl(self.stdout_read, fcntl.F_SETFL, out_flags | os.O_NONBLOCK)
-        fcntl.fcntl(self.stderr_read, fcntl.F_SETFL, err_flags | os.O_NONBLOCK)
-
-        streams_to_watch.update({ self.stdout_read:'stdout',
-                                  self.stderr_read:'stderr' })
-
-        def callback( fd, cb_condition ):
-            # print >>raw_stderr, "log_updater: got stuff on fd", fd, \
-            #       "with condition", cb_condition
-            raw_stderr.flush()
-            data = os.read(fd,65536)
-
-            kind = streams_to_watch.get(fd,'')
-            self.log_display(data, kind)
-            return True                 # Ensure it's called again!
-        
-        for s in streams_to_watch:
-            gobject.io_add_watch(s, gobject.IO_IN, callback)
-
-    def setup_statusbar( self ):
-        """Arrange the status bar area to contain both a status line and a progress bar.
-        """
-        ## The GTK StatusBar widget is a pain to use.  Our statusbar is an
-        ## hbox packed at the bottom of main_vbox, and containing:
-        ##
-        ## - A frame with a status label
-        ## - A progress bar
-        self.w_statusframe = gtk.Frame()
-        self.w_statusframe.set_shadow_type(gtk.SHADOW_IN)
-        self.status_display(' ')        # Establish proper height
-        self.w_statusframe.show()
-
-        self.w_progressbar = gtk.ProgressBar()
-        self.w_progressbar.set_ellipsize(pango.ELLIPSIZE_END)
-        self.w_progressbar.show()
-
-        ## This gives a list of the "order" in which progress bars should
-        ## be allocated as well as whether each one has been allocated
-        self.all_progressbars   = [ (self.w_progressbar,False) ]
-
-        self.w_statusbar = gtk.Table(rows=1, columns=4)
-        self.w_statusbar.attach(self.w_statusframe, left_attach=0, right_attach=3,
-                                top_attach=0, bottom_attach=1,
-                                xoptions=gtk.EXPAND | gtk.FILL,
-                                yoptions=gtk.EXPAND | gtk.FILL,
-                                xpadding=0, ypadding=0)
-        self.w_statusbar.attach(self.w_progressbar, left_attach=3, right_attach=4,
-                                top_attach=0, bottom_attach=1,
-                                xoptions=gtk.EXPAND | gtk.FILL,
-                                yoptions=gtk.EXPAND | gtk.FILL,
-                                xpadding=2, ypadding=2)
-        self.w_statusbar.show()
-        self.w_main_vbox.pack_start(self.w_statusbar, expand=False, fill=False)
-
-    def log_clear( self ):
-        """The log is a TreeView widget that contains 3 columns: (1)
-        number, (2) kind, (3) log entry itself.  This sets up a new log or
-        clears an existing log.
-        """
-        self.log_liststore = gtk.ListStore(int, str, str, int)
-
-        ## Create individual columns
-        self.log_columns = [ gtk.TreeViewColumn(x) for x in [ 'No.', 'Kind', 'Message' ] ]
-
-        ## Create cell renderers for displaying the contents
-        self.log_cells = [ gtk.CellRendererText() for i in xrange(len(self.log_columns)) ]
-        for i, (column, cell) in enumerate(zip(self.log_columns, self.log_cells)):
-            column.pack_start(cell, True)
-            column.add_attribute(cell, 'text', i)
-            column.set_sort_column_id(i)
-
-        ## Last column (message) is a bit tricky: we either set the
-        ## attribute 'text' or 'markup' depending on the setting of the
-        ## fourth model column
-        def message_format_func(treeviewcolumn, cell, model, iter):
-            message = model.get(iter,2)[0]
-            markup  = model.get(iter,3)[0]
-            cell.set_property('family-set', True)
-            if markup:
-                cell.set_property('markup', message)
-                cell.set_property('family', 'Helvetica')
-            else:
-                cell.set_property('text', message)
-                cell.set_property('family', 'Monospace')
-        self.log_columns[2].set_cell_data_func(self.log_cells[2], message_format_func)
-            
-        ## Create the TreeView using underlying liststore model and append columns
-        ## and make it searchable on the message
-        self.log_treeview = gtk.TreeView(self.log_liststore)
-        for column in self.log_columns:
-            self.log_treeview.append_column(column)
-        self.log_treeview.set_search_column(2)
-        self.log_treeview.set_rules_hint(True)
-
-        container_remove_children(self.w_plearn_log_scroller)
-        self.w_plearn_log_scroller.add(self.log_treeview)
-        self.log_treeview.show_all()
-
-        ## Reset the next log entry
-        self.log_next_number = 1
-
-    def log_hide( self ):
-        self.w_log_messages_menuitem.set_active(False)
-        self.w_plearn_log_scroller.hide()
-        self.w_plearn_log_scroller.set_no_show_all(True)
-
-    def log_show( self ):
-        self.w_log_messages_menuitem.set_active(True)
-        self.w_plearn_log_scroller.set_no_show_all(False)
-        self.w_plearn_log_scroller.show()
-        
-    def log_display( self, message, kind = "", has_markup = False, log_clear = False ):
-        """Append the given message to the log area of the main window.
-        Thread-safe.
-        """
-        if log_clear:
-            self.log_clear()
-
-        ## If 'data' matches any regular expression in log_filter, skip
-        ## this message.  This is mostly a hack to get around displaying
-        ## known warnings from the Scintilla editor
-        for regex in self.log_filters:
-            if regex.search(message):
-                return
-
-        row = [ self.log_next_number, kind.rstrip(), message.rstrip(), has_markup ]
-        self.log_show()
-        self.log_liststore.append(row)
-        self.log_next_number += 1
-
-    def status_display( self, message, has_markup = False ):
-        """Display the given message in the status bar area of the main window.
-        Thread-safe.
-        """
-        label = gtk.Label(message)
-        if has_markup:
-            label.set_markup(message)
-        label.set_line_wrap(False)
-        label.set_alignment(0,0)
-        label.set_padding(2,2)
-        label.set_single_line_mode(True)
-        label.set_ellipsize(pango.ELLIPSIZE_MIDDLE)
-        label.show()
-        container_remove_children(self.w_statusframe)
-        self.w_statusframe.add(label)
-
-    def cursor_hourglass( self, unsensitize = True ):
-        """Make the cursor an hourglass for the main window.
-
-        In addition, if the argument unsensitize is True, most entry
-        will be disabled for the window.
-        """
-        self.w_root.window.set_cursor(gtk.gdk.Cursor(gtk.gdk.WATCH))
-        if unsensitize:
-            self.w_root.set_sensitive(False)
-
-    def cursor_normal( self, sensitize = True ):
-        """Take back the cursor to normal form for the main window.
-        """
-        self.w_root.window.set_cursor(None)  # Set back to parent window cursor
-        if sensitize:
-            self.w_root.set_sensitive(True)
-
-    def get_nth_tab( self, n ):
-        """Return the PlideTab object corresponding to the n-th tab.
-        Return None if there is no such PlideTab.
-        """
-        notebook_page = self.w_plide_notebook.get_nth_page(n)
-        if notebook_page:
-            return notebook_page.plide_tab_object
-        else:
-            return None
-
-    def get_current_tab( self ):
-        """Return the PlideTab object that's currently selected in the
-        notebook.  Return None if none...
-        """
-        cur_page = self.w_plide_notebook.get_current_page()
-        if cur_page >= 0:
-            return self.w_plide_notebook.get_nth_page(cur_page).plide_tab_object
-        else:
-            return None
-
-    def get_current_tab_directory( self ):
-        """Return the directory associated with the current PlideTab, or
-        '.' if there is no current tab.
-        """
-        cur_tab = self.get_current_tab()
-        dir = None
-        if cur_tab is not None:
-            dir = cur_tab.get_directory()
-        return dir or '.'
-
-
-    #####  Progress Bar Handling  ###########################################
-
-    def reset_progress( self ):
-        """Bring back the all progress bars to an 'available' state,
-        erase their containing text, and bring the fraction to zero.
-        """
-        for i,(pb,alloc) in enumerate(self.all_progressbars):
-            pb.set_text('')
-            pb.set_fraction(0.0)
-            self.all_progressbars[i] = (pb,False)
-
-    def allocate_progress( self ):
-        """Return the ID of an available progress bar, or -1 if none is
-        available.
-        """
-        for i,(pb,alloc) in enumerate(self.all_progressbars):
-            if not alloc:
-                self.all_progressbars[i] = (pb,True)
-                return i
-
-        return -1
-
-    def release_progress( self, progress_id ):
-        """Release an available progress bar and make it available for
-        other uses.
-        """
-        (pb,alloc) = self.all_progressbars[progress_id]
-        self.all_progressbars[progress_id] = (pb,False)
-
-    def get_progress_from_id( self, progress_id ):
-        """Return the gtk.ProgressBar widget corresponding to its id.
-        """
-        return self.all_progressbars[progress_id][0]
-
-
-    #####  Callbacks  #######################################################
-
-    ## General
-    def on_plide_top_delete_event(self, widget, event):
-        return self.quit()
-
-    def on_quit_activate(self, widget):
-        self.quit()
-
-    ## File Menu
-    def on_new_activate(self, widget):
-        self.add_untitled_tab(".pyplearn")
-        
-    def on_new_pyplearn_script_activate(self, widget):
-        self.add_untitled_tab(".pyplearn")
-
-    def on_new_py_script_activate(self, widget):
-        self.add_untitled_tab(".py")
-
-    def on_new_plearn_script_activate(self, widget):
-        self.add_untitled_tab(".plearn")
-
-    def on_new_text_file_activate(self, widget):
-        self.add_untitled_tab("")
-
-    def on_open_activate(self, widget):
-        self.open_file()
-
-    def on_save_activate(self, widget):
-        if self.get_current_tab():
-            self.get_current_tab().on_save_activate()
-
-    def on_save_as_activate(self, widget):
-        if self.get_current_tab():
-            self.get_current_tab().save_as_file()
-
-    def on_close_activate(self, widget):
-        if self.get_current_tab():
-            self.get_current_tab().close_tab(widget)
-
-    def on_browse_expdir_activate(self, widget):
-        self.open_file(action=gtk.FILE_CHOOSER_ACTION_SELECT_FOLDER)
-
-    ## Edit menu
-    def on_undo_activate(self, widget):
-        self.get_current_tab().on_undo_activate()
-
-    def on_redo_activate(self, widget):
-        self.get_current_tab().on_redo_activate()
-
-    def on_cut_activate(self, widget):
-        self.get_current_tab().on_cut_activate()
-
-    def on_copy_activate(self, widget):
-        self.get_current_tab().on_copy_activate()
-
-    def on_paste_activate(self, widget):
-        self.get_current_tab().on_paste_activate()
-
-    ## View menu
-    def on_log_messages_toggled(self, menuitem):
-        if menuitem.get_active():
-            self.log_show()
-        else:
-            self.log_hide()
-
-    ## Help menu
-    def on_about_activate(self, widget):
-        version = injected.versionString().replace("(","\n(")
-        MessageBox("PLearn Integrated Development Environment Version " + self.PlideVersion,
-                   "Running on " + version + "\n" +\
-                   "Copyright (c) 2006 by Nicolas Chapados",
-                   title = "About Plide")
-
-    ## Toolbar
-    def on_toolbutton_new_pyplearn_clicked(self, widget):
-        self.add_untitled_tab(".pyplearn")
-
-    def on_toolbutton_open_clicked(self, widget):
-        self.open_file()
-
-    def on_toolbutton_options_clicked(self, widget):
-        """Display dialog box for establishing script options.
-        """
-        tab = self.get_current_tab()
-        if tab is not None:
-            script      = tab.get_text()
-            name        = tab.get_basename()
-            script_dir  = tab.get_directory()
-
-            while True:                 # Loop to handle script reload
-                options_holder = tab.get_options_holder()
-                if not options_holder:
-                    ## When executing for the first time, run the script
-                    if self.pyplearn_parse( name, script ) is None:
-                        return              # Syntax errors in script
-                    options_holder = PyPLearnOptionsHolder(name, script, script_dir)
-                    tab.set_options_holder(options_holder)
-
-                options_dialog = PyPLearnOptionsDialog(options_holder)
-                result = options_dialog.run()
-                if result == gtk.RESPONSE_OK:
-                    options_dialog.update_options_holder()
-                options_dialog.destroy()
-
-                if result == gtk.RESPONSE_REJECT:
-                    tab.set_options_holder(None)
-                else:
-                    break
-
-    def on_toolbutton_execute_clicked(self, widget):
-        """Launch the execution of the pyplearn script, only if it's indeed
-        such a script.
-        """
-        tab = self.get_current_tab()
-        if tab is not None:
-            if type(tab) == PlideTabPyPLearn:
-                script_name    = tab.get_basename()
-                script_code    = tab.get_text()
-                launch_dir     = tab.get_directory()
-                options_holder = tab.get_options_holder()
-                if options_holder is not None:
-                    launch_dir = options_holder.launch_directory
-                self.pyplearn_executor(script_name, script_code, launch_dir,
-                                       options_holder)
-
-
-    ### Help-related
-    def on_help_activate(self, widget):
-        self.help_show()
-        
-    def on_help_close_clicked(self, widget):
-        self.help_close()
-        
-
-    #####  Tab Handling  ####################################################
-
-    def add_untitled_tab(self, extension):
-        self.add_intelligent_tab("untitled%d%s" % (self.untitled_counter,
-                                                   extension), is_new=True)
-        self.untitled_counter += 1
-
-    def add_intelligent_tab(self, filename, is_new = False):
-        """Create a new tab 'intelligently' depending on the filename type
-        or its extension. (If the file is new, rely on its extension only).
-        """
-        filename = filename.rstrip(os.path.sep)
-        extension = os.path.splitext(filename)[1]
-        new_tab   = None
-        if extension == ".pyplearn" or extension == ".py":
-            new_tab = PlideTabPyPLearn(self.w_plide_notebook, filename, is_new,
-                                       self.all_plearn_classes)
-
-        elif extension == ".pmat":
-            new_tab = PlideTabPMat(self.w_plide_notebook, filename)
-
-        elif os.path.isdir(filename):
-            new_tab = PlideTabExpdir(self.w_plide_notebook, filename)
-
-        elif os.path.exists(filename):
-            new_tab = PlideTabFile(self.w_plide_notebook, filename, is_new)
-
-        elif is_new:
-            new_tab = PlideTabFile(self.w_plide_notebook, filename, is_new)
-            
-        else:
-            MessageBox("File '%s' is of unrecognized type" % filename,
-                       type=gtk.MESSAGE_ERROR)
-
-        self.status_display('')         # Clear the status
-
-    def open_file(self, action=gtk.FILE_CHOOSER_ACTION_OPEN):
-        """Display a file chooser dialog and add a new tab based on selected file.
-        """
-        chooser = gtk.FileChooserDialog(
-            title="Open",
-            action=action,
-            buttons= (gtk.STOCK_CANCEL,gtk.RESPONSE_CANCEL,
-                      gtk.STOCK_OPEN,  gtk.RESPONSE_OK))
-
-        # Add file filters
-        filter = gtk.FileFilter()
-        filter.set_name("All files")
-        filter.add_pattern("*")
-        chooser.add_filter(filter)
-        
-        filter = gtk.FileFilter()
-        filter.set_name("Experiment scripts")
-        filter.add_pattern("*.plearn")
-        filter.add_pattern("*.pyplearn")
-        chooser.add_filter(filter)
-
-        filter = gtk.FileFilter()
-        filter.set_name("Data")
-        filter.add_pattern("*.pmat")
-        filter.add_pattern(".amat")
-        chooser.add_filter(filter)
-
-        chooser.set_default_response(gtk.RESPONSE_OK)
-        chooser.set_current_folder(self.get_current_tab_directory())
-        response = chooser.run()
-
-        ## Add all files selected by the user
-        if response == gtk.RESPONSE_OK:
-            filenames = chooser.get_filenames()
-            for f in filenames:
-                self.add_intelligent_tab(f, is_new=False)
-            
-        chooser.destroy()
-
-
-    #####  PLearn execution  ############################################
-
-    def pyplearn_executor( self, script_name, script_code, launch_directory,
-                           options_holder ):
-        """Execute a pyplearn script within an options context.
-        
-        Operations are as follows:
-        
-        1. We execute the script one more time with the more
-           recent text (may have changed since last time options
-           were set)
-        2. We set the options corresponding each scoped object
-           in their own class
-        3. We parse the manual command-line arguments
-        4. We transform the script to a .plearn
-        5. We grab a hold of the soon-to-be-created expdir
-        6. We hand this script off to PLearn for execution
-        """
-        script_env = self.pyplearn_parse( script_name, script_code )
-        if script_env is not None:
-            if options_holder:
-                options_holder.pyplearn_actualize()
-            else:
-                ## FIXME: Generate a brand-new expdir (minor hack)
-                plargs.parse(["expdir="+generateExpdir()])
-                
-            expdir = plargs.expdir
-            plearn_script = eval('str(PyPLearnScript( main() ))', script_env)
-
-            message = 'Launching script <b>%s</b> in directory <b>%s</b>' % \
-                      (script_name, launch_directory)
-            self.status_display(message, has_markup=True)
-            self.log_display   (message, has_markup=True, log_clear=True)
-            if self.w_dump_plearn_to_log.get_active():
-                self.log_display("Expdir is: %s\n.plearn is:\n%s" %
-                                 (expdir, plearn_script))
-            
-            request_id = self.work_queue.post_work_request(
-                plearn_script, launch_directory, "pyplearn")
-            
-            print >>sys.stderr, "Caller executing request_id", request_id
-            sys.stderr.flush()
-            self.work_requests[request_id] = os.path.join(launch_directory,expdir)
-            self.add_plearn_results_monitor( script_name, request_id )
-
-    def add_plearn_results_monitor( self, script_name, request_id, interval = 100 ):
-        """Add a monitor callback to check for availability of PLearn
-        results every 'interval' milliseconds.
-        """
-        def callback( ):
-            completion_result = \
-                self.work_queue.work_request_completed(request_id)
-
-            if completion_result is not None:
-                gtk.threads_enter()     # This is not a GTK+ callback
-                self.reset_progress()
-                (result_code, result_details) = completion_result
-
-                ## If result_code is "", it means everything is OK.
-                if result_code == "":
-                    message = "<b>%s</b> completed successfully" % script_name
-                    self.status_display(message, has_markup = True)
-                    self.log_display(message, has_markup = True)
-
-                else:
-                    status_msg = "<b>%s</b> terminated due to errors" % script_name
-                    self.status_display(status_msg, has_markup = True)
-                    message = ('A fatal error of kind "%s" was encountered during ' +\
-                               'execution of script "%s".') % (result_code, script_name)
-                    details = "Details:\n" + result_details
-                    self.log_display(message+"\n"+details, has_markup = False)
-                    MessageBox(message, details, type=gtk.MESSAGE_ERROR)
-
-                ## Done: don't call again, and add a new tab corresponding
-                ## to the expdir.  Check for expdir existence first, since
-                ## some experiments don't necessarily leave an expdir around
-                if os.path.isdir(self.work_requests[request_id]):
-                    self.add_intelligent_tab(self.work_requests[request_id])
-                gtk.threads_leave()
-                return False
-            else:
-                ## Call again later
-                return True
-
-        ## Add the callback to the GTK list of callback
-        callback_id = gobject.timeout_add(interval, callback)
-        
-        
-    #####  PyPLearn Parse  ##############################################
-    
-    def pyplearn_parse( self, script_name, script_code ):
-        """Ensure that a pyplearn script parses without error.
-
-        If an error is encountered, a backtrace is emitted to the log aread
-        and a message box is popped to indicate the error.  Return None in
-        this case.  Return the script execution environment if no error is
-        encountered.
-        """
-
-        ## Implementation note: start by compiling the code to catch syntax
-        ## errors in the script.  Then execute with an 'exec' statement and
-        ## separately catch execution errors.
-        compiled_code = None
-        try:
-            self.cursor_hourglass()
-            compiled_code = compile(script_code+'\n', script_name, 'exec')
-        except ValueError:
-            pass
-        except SyntaxError, e:
-            self.cursor_normal()
-            (exc_type, exc_value, tb) = sys.exc_info()
-            self.status_display("Syntax error in script <b>%s</b>" % script_name,
-                                True)
-            self.log_display(''.join(traceback.format_exception_only(exc_type, exc_value)))
-            MessageBox('Syntax error in script "%s".' % script_name,
-                       "Python message: %s\nSee the log area for the detailed traceback." % \
-                       str(exc_value),
-                       title = "PyPLearn Script Error",
-                       type=gtk.MESSAGE_ERROR)
-            return None
-
-        if compiled_code:
-            script_env  = { }
-            try:
-                exec compiled_code in script_env
-            except:
-                self.cursor_normal()
-                (exc_type, exc_value, tb) = sys.exc_info()
-                self.log_display(''.join(traceback.format_tb(tb)))
-                self.status_display("Exception during execution of script <b>%s</b>."
-                                    % script_name, True)
-                MessageBox('Script "%s" raised exception "%s: %s".' \
-                           % (script_name, str(exc_type), str(exc_value)),
-                       "See the log area for the detailed traceback.",
-                       title = "PyPLearn Script Error",
-                       type=gtk.MESSAGE_ERROR)
-                return None
-            else:
-                self.cursor_normal()
-                self.status_display("Script <b>%s</b> parsed successfully."
-                                    % script_name, True)
-                return script_env
-
-        self.cursor_normal()
-
-
-#####  Utility Classes  #####################################################
-
-class PLearnWorkQueue( object ):
-    """Worker thread for PLearn computation.
-
-    Since PLearn is quite highly dependent on the assumption of single
-    threading, this object simply passes work requests between the GUI
-    thread (which runs Plide) and the main thread (which runs PLearn).
-    Work requests are posted from the GUI the post_work_request() method,
-    which returns a work_id.  You can then poll the worker for task
-    completion status, or sleep until the task is completed.
-
-    Likewise, the main thread calls get_work_request(), which blocks until
-    a work request arrives.  The method returns a 4-tuple of strings of the
-    form [ request_id, script, root_directory, script_type ].  The main
-    thread can post results using the post_work_results() method.
-    """
-    def __init__( self ):
-        self.requests_queue     = Queue.Queue()
-        self.results_queue      = Queue.Queue()
-        self.next_request_id    = 1
-        self.completion_results = { }
-
-    def post_work_request( self, script, script_dir, script_kind ):
-        """Post a new work request to PLearn and return immediately.
-        """
-        request_id = self.next_request_id
-        self.completion_results[str(request_id)] = None
-        self.requests_queue.put((str(request_id), script, script_dir, script_kind))
-        self.next_request_id += 1
-        return request_id
-
-    def get_work( self ):
-        """Called by the PLearn main thread to get work to do.  Blocks
-        until there is work.
-        """
-        return self.requests_queue.get()
-
-    def post_work_results( self, request_id, code, results ):
-        """Called by the PLearn main thread to announce that it has finished
-        processing the work request 'request_id'.
-        """
-        self.results_queue.put((request_id, code, results))
-
-    def work_request_completed( self, request_id ):
-        """Return a pair (Code,Results) if the given work request is
-        finished processing by PLearn, and None if it's still being
-        processed.
-        """
-        while not self.results_queue.empty():
-            (cur_id, code, results) = self.results_queue.get()
-            self.completion_results[cur_id] = (code, results)
-            print >>raw_stderr, "work_request_completed:", (cur_id, code, results)
-            raw_stderr.flush()
-
-        return self.completion_results[str(request_id)]
-
-    def wait_for_work_request( self, request_id ):
-        """Wait (block) until the specified work request is finished
-        processing by PLearn.
-        """
-        while self.completion_results[str(request_id)] is None:
-            (cur_id, code, results) = self.results_queue.get()
-            self.completion_results[cur_id] = (code, results)
-
-
-#####  C++ Functional Interface  ############################################
-
-#global plide_main_window
-plide_main_window = None
-
-def StartPlide(argv = [], streams_to_watch= {}):
-    global plide_main_window
-    plide_main_window = PlideMain(streams_to_watch)
-
-    ## Consider each file passed as command-line argument and create a tab
-    ## to view it.
-    for arg in argv:
-        plide_main_window.add_intelligent_tab(arg, not os.path.exists(arg))
-
-    plide_main_window.run()       # Show and start event loop in other thread
-
-def QuitPlide():
-    if not plide_main_window.close_event.isSet():
-        gtk.threads_enter()
-        plide_main_window.quit()
-        gtk.threads_leave()
-
-def GetWork():
-    return plide_main_window.work_queue.get_work()
-
-def PostWorkResults( request_id, code, results ):
-    plide_main_window.work_queue.post_work_results(request_id, code, results)
-
-def LogAppend( kind, severity, message ):
-    gtk.threads_enter()
-    plide_main_window.log_display( message, kind )
-    gtk.threads_leave()
-    
-
-#####  C++ Progress Bar Interface  ##########################################
-
-def AllocateProgressBar( text ):
-    progress_id = plide_main_window.allocate_progress()
-    # print >>raw_stderr, "Allocating progress bar", progress_id
-    raw_stderr.flush()
-    if progress_id >= 0:
-        gtk.threads_enter()
-        pb = plide_main_window.get_progress_from_id(progress_id)
-        pb.set_text(text)
-        pb.set_fraction(0.0)
-        gtk.threads_leave()
-    return progress_id
-
-def ReleaseProgressBar( progress_id ):
-    # print >>raw_stderr, "Releasing progress bar", progress_id
-    raw_stderr.flush()
-    if progress_id >= 0:
-        gtk.threads_enter()
-        pb = plide_main_window.get_progress_from_id(progress_id)
-        pb.set_fraction(1.0)            # Make it complete on screen
-        gtk.threads_leave()
-        plide_main_window.release_progress( progress_id )
-
-def ProgressUpdate( progress_id, fraction ):
-    # print >>raw_stderr, "Updating progress bar", progress_id,"to fraction",fraction
-    raw_stderr.flush()
-    if progress_id >= 0:
-        gtk.threads_enter()
-        pb = plide_main_window.get_progress_from_id(progress_id)
-        pb.set_fraction(fraction)
-        gtk.threads_leave()
-
-
-#####  Standalone Running  ##################################################
-
-from plearn.io.server import *
-
-class Poubelle(RemotePLearnServer):
-    """
-    Patch to test standalone running while 'slave' mode still exists.
-    """
-    def __init__(self):
-        command= 'plearn server'
-        self.errstm= None
-        try:
-            from subprocess import Popen, PIPE
-            p= Popen([command], shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE, close_fds=True)
-            (to_server, from_server, child_pid) = (p.stdin, p.stdout, p.pid)
-            self.errstm= p.stderr
-        except:
-            to_server, from_server = os.popen2(command, 'b')
-            child_pid = -1
-        RemotePLearnServer.__init__(self,from_server, to_server, pid=child_pid)
-
-
-    def getAllClassnames(self): return self.listClasses()
-    def helpResourcesPath(self,path): return self.setResourcesPathHTML(path)
-    def helpIndex(self): return self.helpIndexHTML()
-    def helpClasses(self): return self.helpClassesHTML()
-    def helpCommands(self): return self.helpCommandsHTML()
-    def helpOnCommand(self, command): return self.helpOnCommandHTML(command)
-    def helpOnClass(self, classname): return self.helpOnClassHTML(classname)
-
-if __name__ == "__main__":
-    #class Poubelle:
-    #    def __getattr__(self, attr): return None
-
-    global plide_main_window
-    global injected
-    injected = Poubelle()
-    
-    StartPlide(streams_to_watch= {injected.errstm.fileno(): 'injected-stderr'})
-    # print >>sys.stderr, "Random stuff to stderr"
-    # print >>sys.stdout, "Random stuff to stdout"
-    # sys.stderr.flush()
-    # sys.stdout.flush()
-
-    # prid = AllocateProgressBar("Simple Progress Text")
-    # time.sleep(1)
-    # ProgressUpdate(prid,0.33)
-    # time.sleep(1)
-    # ProgressUpdate(prid,0.66)
-    # time.sleep(1)
-    # ReleaseProgressBar(prid)
-    # time.sleep(1)
-
-    plide_main_window.close_event.wait()  # Wait for window to be closed
-    QuitPlide()
-    

Copied: branches/numarray/python_modules/plearn/plide/plide.py (from rev 7944, trunk/python_modules/plearn/plide/plide.py)

Deleted: branches/numarray/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-08-07 00:50:45 UTC (rev 7938)
+++ branches/numarray/python_modules/plearn/pyplearn/plargs.py	2007-08-09 15:42:22 UTC (rev 7968)
@@ -1,1221 +0,0 @@
-# plargs.py
-# Copyright (C) 2006 Christian Dorion
-#
-#  Redistribution and use in source and binary forms, with or without
-#  modification, are permitted provided that the following conditions are met:
-#
-#   1. Redistributions of source code must retain the above copyright
-#      notice, this list of conditions and the following disclaimer.
-#
-#   2. Redistributions in binary form must reproduce the above copyright
-#      notice, this list of conditions and the following disclaimer in the
-#      documentation and/or other materials provided with the distribution.
-#
-#   3. The name of the authors may not be used to endorse or promote
-#      products derived from this software without specific prior written
-#      permission.
-#
-#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-#
-#  This file is part of the PLearn library. For more information on the PLearn
-#  library, go to the PLearn Web site at www.plearn.org
-
-# Author: Christian Dorion
-"""Management of command-line options.
-
-A custom (and encouraged) practice is to write large PyPLearn scripts
-which behaviour can be modified by command-line arguments, e.g.:: 
-        
-    prompt %> plearn command_line.pyplearn on_cmd_line="somefile.pmat" input_size=10
-
-with the command_line.pyplearn script being::
-
-    #
-    # command_line.pyplearn
-    #
-    from plearn.pyplearn import *
-
-    dataset = pl.AutoVMatrix( specification = plargs.on_cmd_line,
-                              inputsize     = int( plargs.input_size ),
-                              targetsize    = 1
-                              )
-
-    def main():
-        return pl.SomeRunnableObject( dataset  = dataset,
-                                      internal = SomeObject( dataset = dataset ) )
-
-Those command-line arguments are widely refered as L{plargs}, on
-account of this class, troughout the pyplearn mechanism. Note that
-I{unexpected} (see binders and L{namespaces<plnamespace>} hereafter)
-arguments given on the command line are interpreted as strings, so if
-you want to pass integers (int) or floating-point values (float), you
-will have to cast them as above.
-
-To set default values for some arguments, one can use
-L{plarg_defaults<_plarg_defaults>}. For instance::
-
-    # 
-    # command_line_with_defaults.pyplearn
-    #
-    from plearn.pyplearn import *
-
-    plarg_defaults.on_cmd_line = "some_default_file.pmat"
-    plarg_defaults.input_size  = 10
-    dataset = pl.AutoVMatrix( specification = plargs.on_cmd_line,
-                              inputsize     = plargs.input_size,
-                              targetsize    = 1
-                              )
-
-    def main( ):
-        return pl.SomeRunnableObject( dataset  = dataset,
-                                      internal = SomeObject( dataset = dataset ) )
-    
-which won't fail and use C{"some_default_file.pmat"} with C{input_size=10} if::
-
-    prompt %> plearn command_line_with_defaults.pyplearn
-
-is entered. Note that since I{input_size} was defined as an int
-(C{plarg_defaults.input_size = 10}). Even if
-L{plarg_defaults<_plarg_defaults>} is still supported, it is preferable
-to define all arguments' default values through some I{binder}. We
-refer to subclasses of L{plargs} as I{binders} since they bind default
-values to expected/possible command-line arguments, e.g::
-
-    # 
-    # my_script.pyplearn
-    #
-    from plearn.pyplearn.plargs import *
-
-    class Misc(plargs):
-        algo      = "classical"
-        ma_len    = plopt([126, 252],
-                          doc="A list of moving average lenghts to be used "
-                          "during the preprocessing.")
-        n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-    print >>sys.stderr, repr(plargs.algo), repr(plargs.n_inputs)
-    assert plargs.n_inputs==Misc.n_inputs
-
-    print >>sys.stderr, Misc.ma_len
-        
-would print (as first line) C{"classical", 10}, a string and an int, if
-no command-line arguments override those L{plargs}. One can always
-access the I{plarg} through C{plargs} or C{Misc} (i.e. the assertion
-never fails).
-
-Note the use of L{plopt} instances. While these are not mandatory, they
-are very useful and powerful tools which one can use to make his
-scripts clearer and more user-friendly when used within
-U{plide<http://plearn.berlios.de/plide>}. Note that list can be
-provided to as command-line argument in the CSV format, that is::
-
-    prompt %> plearn my_script.pyplearn ma_len=22,63,126,252
-    "classical", 10
-    [22, 63, 126, 252]
-
-While I{binders} allow one to define default values for L{plargs},
-these are limited in the sens that clashes can occur::
-
-    # 
-    # complex_script.pyplearn
-    #
-    from plearn.pyplearn.plargs import *
-    
-    class macd(plargs):
-        ma_len = plopt(252,
-                    doc="The moving average length to be used in the macd model.")
-
-    class PCA(plargs):
-        algo      = "classical"
-        ma_len    = plopt([126, 252],
-                          doc="A list of moving average lenghts to be used "
-                          "during the preprocessing.")
-        n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-    ...
-    #####
-    prompt %> plearn complex_script.pyplearn 
-    Traceback (most recent call last):
-    File "TEST.pyplearn", line 7, in ?
-        class PCA(plargs):
-    File "/home/dorionc/PLearn/python_modules/plearn/pyplearn/plargs.py", line 447, in __new__
-        plopt.define(cls, option, value)
-    File "/home/dorionc/PLearn/python_modules/plearn/pyplearn/plargs.py", line 193, in define
-        raise KeyError(
-    KeyError: "A script should not contain two options of the same name. Clashing
-    definition of plarg 'ma_len' in 'macd' and 'PCA'"
-
-To avoid this type of error, one can L{namespace<plnamespace>} the
-arguments of his script::
-
-    # 
-    # namespaced.pyplearn
-    #
-    from plearn.pyplearn.plargs import *
-    
-    class macd(plnamespace):
-        ma_len = plopt(252,
-                    doc="The moving average length to be used in the macd model.")
-
-    class PCA(plnamespace):
-        algo      = "classical"
-        ma_len    = plopt([126, 252],
-                          doc="A list of moving average lenghts to be used "
-                          "during the preprocessing.")
-        n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-    print >>sys.stderr, macd.ma_len, PCA.algo, PCA.ma_len
-
-    try:
-        print >>sys.stderr, plargs.n_inputs
-    except AttributeError:
-        print >>sys.stderr, \
-              "n_inputs is namespaced, you must access it through 'PCA'"
-
-    #####
-    prompt %> plearn namespaced.pyplearn macd.ma_len=126 PCA.algo=weird
-    252 weird [126, 252]
-    n_inputs is namespaced, you must access it through 'PCA'
-
-Finally, B{note that} the value of C{plargs.expdir} is generated automatically and
-B{can not} be assigned a default value through binders. This behaviour
-aims to standardize the naming of experiment directories. For debugging
-purpose, however, one may provide on command-line an override to
-C{plargs.expdir} value. Otherwise, one can also provide the I{expdir_root}
-command-line argument as a (relative) path where the expdir should be
-found, e.g.::
-
-    prompt %> plearn my_script.pyplearn expdir_root=Debug
-
-would cause experiment to use the expdir::
-
-    Debug/expdir_2006_05_10_07_58_10
-    
-for instance.
-"""
-import copy, inspect, logging, new, re, sys
-from plearn.pyplearn.context import *
-from plearn.utilities.Bindings import Bindings
-
-# Helper functions
-
-def neglected_member(name, value):
-    return (name.startswith('_') 
-            or inspect.ismethod(value) or inspect.isfunction(value)
-            or inspect.isroutine(value) or inspect.isclass(value) )
-
-def list_cast(slist, elem_cast):
-    """Intelligently casts I{slist} string to a list.
-
-    The I{slist} argument can have the following forms::
-
-        - CSV::
-            slist = "1,2,3" => casted = [1, 2, 3]
-        - CSV with brackets::
-            slist = "[hello,world]" => casted = ["hello", "world"]
-        - List of strings::
-            slist = [ "100.0", "102.5" ] => casted = [ 100.0, 102.5 ]
-
-    The element cast is made using the I{elem_cast} argument.
-    """
-    # CSV (with or without brackets)
-    slist = slist.strip()
-    #print >>sys.stderr, repr(slist), type(slist)
-    if isinstance(slist,str):
-        if slist=="":
-            slist = []
-        elif slist.startswith("["):
-            assert slist.endswith("]")
-            slist = eval(slist)
-        else:
-            slist = slist.split(",")
-        return [ elem_cast(e) for e in slist ]
-
-    # List of strings
-    elif isinstance(slist, list):
-        return [ elem_cast(e) for e in slist ]
-
-    else:
-        raise ValueError, "Cannot cast '%s' into a list", str(slist)
-
-def warn(message, category=UserWarning, stacklevel=0):
-    import os
-    pytest_state = os.environ.get("PYTEST_STATE", "")
-    if pytest_state!="Active":        
-        from warnings import warn
-        warn(message, category, stacklevel=stacklevel+3)
-
-#######  Classes to Manage Command-Line Arguments  ############################
-
-class plopt(object):
-    """Typed command-line options with constraints for PLearn.
-    
-    This class provides support for default values, strong type checking,
-    conversion from a string representation, documentation, and constraints
-    on the possible values that the option can take.  The syntax to follow
-    is along the lines of::
-    
-        class MyOptions( plargs_namespace ):
-            plot_stuff  = plopt(True, doc='Whether a cute graph should be plotted')
-            K           = plopt(10, min=5, max=25, doc='Number of neighbors to consider')
-            method      = plopt('classical', choices=['new-kind','classical','crazy'])
-            lags        = plopt([1,22,252], doc='Lags to incorporate as inputs, in days')
-            weight_cols = plopt([], elem_type=int)
-
-    The supported keywords are
-
-        - B{doc}: The help string for that option.
-
-        - B{choices}: A list of values accepted for that option. Trying to
-          set this option's value to anything which is not in that list will cause
-          a ValueError to be raised.
-
-        - B{min, max}: Bounds for numeric values. You can specify only one
-          of the two. Trying to set this option to value out of these bounds will
-          cause a ValueError to be raised.
-
-        - B{free_choices}: A list of suggestions as to what the field should contain.
-          Still, this option can be set to whatever value you wish and a ValueError
-          will never be raised.
-
-        - B{type}: This keyword can and must only be used when the default
-          value is None. Otherwise, it is infered using 'type(value)'.
-
-        - B{elem_type}: For list options, the type of the elements is
-          usually infered from the first element of the list. This keyword
-          may however be used to specify the type for the elements of a
-          list which defaults as empty. If the list defaults as empty
-          I{and} 'elem_type' keyword is not provided, then the elements' type is
-          assumed to be 'str'.
-
-    A plopt I{holder} refers to either a plargs binder or a plnamespace.
-    """
-    
-    unnamed = "### UNNAMED ###"
-
-    def __init__(self, value, **kwargs):
-        self._name  = kwargs.pop("name", self.unnamed)
-        self._doc   = kwargs.pop("doc", '')
-        self._gui   = kwargs.pop("gui", True)
-
-        # type: This keyword can and must only be used when the default
-        # value is None. Otherwise, it is infered using 'type(value)'.
-        assert not ("type" in kwargs and value is not None)
-        if value is None:
-            if "type" not in kwargs:
-                warn("When a plopt value defaults to None, a valid type must be "
-                     "provided using the 'type' keyword argument. "
-                     "String is assumed for now...", FutureWarning, 2)
-                kwargs['type'] = str
-            self._type = kwargs.pop("type")
-        else:
-            self._type = type(value)
-
-        # Keep the remaining kwargs for further use.
-        self._kwargs = kwargs
-
-        # Sanity checks
-        self.checkBounds(value)
-        self.checkChoices(value)
-        self.__default_value = value # The check did not raise: 'value' is valid
-
-    def __str__(self):
-        """Short string representation of a plopt instance.
-
-        Either::
-            plopt(value = DEFAULT_VALUE)
-        or::
-            plopt(value = VALUE [default: DEFAULT_VALUE])
-
-        where the second occurs if the current value is not equal to the default value.
-        """
-        value = self.get()
-        value_str = repr(value)
-        if value != self.__default_value:
-            value_str += " [default: %s]"%repr(self.__default_value)
-        return "plopt(value = %s)"%value_str
-
-    def cast(self, value):
-        casted = None
-        
-        # Special string to bool treatment
-        if self._type is bool and isinstance(value, str):
-            if value == "True":
-                casted = True
-            elif value == "False":
-                casted = False
-            else:
-                raise ValueError("Trying to set value %s to bool-typed option %s"
-                                 %(value, self._name))
-
-        # Special treatment for list option
-        elif self._type is list :
-            elem_type = self._kwargs.pop("elem_type", None) 
-            if elem_type is None:
-                got= self.get()
-                if got!=None and len(got) > 0:
-                    elem_type = type(self.get()[0])
-                else:
-                    elem_type = str
-            casted = list_cast(value, elem_type)
-
-        # Simple type cast
-        else:
-            casted = self._type(value)
-
-        return casted
-    
-    def checkBounds(self, value):
-        """Checks that value lies between 'min' and 'max' bounds parsed from self.kwargs."""
-        minimum = self._kwargs.get("min", None)
-        if minimum is not None and value < minimum:
-            raise ValueError("Option %s (=%s) should greater than %s"
-                             %(self._name, repr(value), repr(minimum)))
-
-        maximum = self._kwargs.get("max", None)
-        if maximum is not None and value > maximum:
-            raise ValueError("Option %s (=%s) should lower than %s"
-                             %(self._name, repr(value), repr(minimum)))
-
-    def checkChoices(self, value):
-        """Checks that 'value' is one of the 'choices' parsed from self.kwargs."""
-        choices = self._kwargs.get("choices", None)
-        if choices is not None and value not in choices:
-            name = ""
-            if self._name != self.unnamed:
-                name = self._name+" " 
-            raise ValueError(
-                "Option %sshould be in choices=%s"%(name, choices))
-
-    def get(self):
-        """Returns the current context override, if any, o/w returns the default value."""
-        plopt_overrides = actualContext(plopt).plopt_overrides
-        if self in plopt_overrides:
-            return plopt_overrides[self]
-        return self.__default_value
-
-    def getBounds(self):
-        minimum = self._kwargs.get("min", None)
-        maximum = self._kwargs.get("max", None)
-        return minimum, maximum
-
-    def getChoices(self):
-        return self._kwargs.get("choices", None)
-
-    def getDefault(self):
-        return self.__default_value
-
-    def getFreeChoices(self):
-        return self._kwargs.get("free_choices", None)
-
-    def getName(self):
-        assert self._name != self.unnamed
-        return self._name
-
-    def getType(self):
-        return self._type
-
-    def getGui(self):
-        return self._gui
-
-    def reset(self):
-        """Simply deletes any override for this plopt in the current context.
-        
-        Note that the actual implementation of C{define()} leads to
-        forgetting the command-line override if C{reset()} is called on the
-        plopt instance!
-        """
-        actualContext(plopt).plopt_overrides.pop(self, None)
-        
-    def set(self, value):
-        """Sets an override for this plopt in the current context"""
-        if not isinstance(value, self._type):
-            value = self.cast(value)
-        # Sanity checks 
-        self.checkBounds(value)
-        self.checkChoices(value)
-    
-        # The previous didn't raise exeption, the 'value' is valid
-        actualContext(plopt).plopt_overrides[self] = value
-
-    def setDefault(self, default):
-        self.__default_value = default
-
-    #######  Static methods  ######################################################
-
-    def addCmdLineOverride(holder_name, optname, value):
-        context = actualContext(plopt)
-        hldr_key = (holder_name, optname)
-        # Use the 'unique' key to store command-line override's value
-        context.plopt_cmdline_overrides[hldr_key] = value
-    addCmdLineOverride = staticmethod(addCmdLineOverride)
-
-    def popCmdLineOverride(actual_holder, optname):
-        context = actualContext(plopt)
-        hldr_key = (actual_holder.__name__, optname)
-        override_value = context.plopt_cmdline_overrides.pop(hldr_key, None)
-
-        # If the option is not namespaced, it may have been provided
-        # without the binder name as prefix. Try accessing the key with
-        # None instead of the holder's name
-        if override_value is None \
-           and not issubclass(actual_holder, plnamespace):
-            hldr_key = (None, optname)
-            override_value = context.plopt_cmdline_overrides.pop(hldr_key, None)
-
-        # Will return None if no option named 'optname' were overridden on
-        # the command line
-        return override_value
-    popCmdLineOverride = staticmethod(popCmdLineOverride)
-
-    def buildClassContext(context):
-        assert not hasattr(context, 'plopt_binders')
-        context.plopt_binders = {}
-
-        assert not hasattr(context, 'plopt_namespaces')
-        context.plopt_namespaces = {}
-
-        assert not hasattr(context, 'plopt_overrides')
-        context.plopt_overrides = {}
-
-        assert not hasattr(context, 'plopt_cmdline_overrides')
-        context.plopt_cmdline_overrides = {}
-    buildClassContext = staticmethod(buildClassContext)
-
-    def closeClassContext(context):
-        exceptions = [ 'FILEBASE', 'FILEPATH', 'TIME', 'DATETIME',
-                       'FILEEXT', 'DIRPATH', 'DATE', 'HOME', 'FILENAME' ]
-
-        del context.plopt_binders
-        del context.plopt_namespaces
-        del context.plopt_overrides
-
-        unused = []
-        for (hldr_name, optname), value in context.plopt_cmdline_overrides.iteritems():
-            if optname in exceptions:
-                continue
-            elif hldr_name is None:
-                unused.append("%s=%s"%(optname, value))
-            else:
-                unused.append("%s.%s=%s"%(hldr_name, optname, value))
-                
-        if unused:
-            from plearn.pyplearn import PyPLearnError
-            raise PyPLearnError(
-                "The following command-line arguments were not expected "
-                "(Misspelled? Or namespaced?): %s" % ", ".join(unused))
-
-        # Finally, delete the list
-        del context.plopt_cmdline_overrides
-    closeClassContext = staticmethod(closeClassContext)    
-
-    def define(holder, option, value):
-        """Typical pattern to set a plopt instance member in 'holder' for the first time."""
-        context = actualContext(plopt)
-
-        # Check if an override was defined through parsing before the
-        # holder existed. Returns None in the case no override exists.
-        cmdline_override = plopt.popCmdLineOverride(holder, option)
-
-        ### Holder-type specific management
-        plopt._inner_define(holder, option, value)
-
-        # Command-line MUST override mandatory plopts
-        if isinstance(value, mandatory_plopt) and cmdline_override is None:
-            from plearn.pyplearn import PyPLearnError
-            raise PyPLearnError(
-                "Mandatory argument %s.%s was not received on command line."
-                %(holder.__name__, value.getName()) )
-        
-        # Overrides the default with the command-line override if any. Note
-        # that this way to proceed leads to forgetting the command-line
-        # override if reset() is called on the plopt instance!!!
-        if cmdline_override is not None:
-            plopt.override(holder, option, cmdline_override)            
-    define = staticmethod(define)
-
-    def _inner_define(holder, option, value):
-        """Holder-type specific management."""
-        context = actualContext(plopt)
-            
-        if issubclass(holder, plargs):
-            # A script should not contain two options of the same name
-            if option in context.plopt_binders:
-                raise KeyError(
-                    "A script should not contain two options of the same name. "
-                    "Clashing definition of plarg '%s' in '%s' and '%s'"%
-                    (option,context.plopt_binders[option].__name__,holder.__name__))
-        
-            # Keep a pointer to the binder in which the option is defined
-            context.plopt_binders[option] = holder
-
-        elif issubclass(holder, plnamespace):
-            # Keep a pointer to the namespace in which the option is defined
-            context.plopt_namespaces[option] = holder
-
-        else:
-            raise TypeError("Holder '%s' is of an unknown type: %s"
-                            % (holder.__name__, type(holder)) )
-
-        # Enforce all 'holder' members to be (named) plopt instances
-        if isinstance(value, plopt):
-            value._name = option
-        else:
-            value = plopt(value, name=option)
-
-        # Acutally sets the plopt in the holder
-        type.__setattr__(holder, option, value)        
-    _inner_define = staticmethod(_inner_define)
-
-    def getHolder(plopt_name):        
-        return actualContext(plopt).plopt_binders.get(plopt_name)
-    getHolder = staticmethod(getHolder)
-
-    def iterator(holder):
-        """Returns an iterator over the plopt instances contained in the I{holder}"""
-        keys = holder.__dict__.keys()
-        keys.sort()
-        return iter([ holder.__dict__[key] for key in keys
-                      if isinstance(holder.__dict__[key], plopt) ])
-    iterator = staticmethod(iterator)
-
-    def optdict(holder):
-        return dict([ (opt.getName(), opt.get()) for opt in plopt.iterator(holder) ])
-    optdict = staticmethod(optdict)
-
-    def override(holder, option, value):
-        """Typical pattern to override the value of an existing plopt instance."""
-        plopt_instance = type.__getattribute__(holder, option)
-        plopt_instance.set(value)
-    override = staticmethod(override)
-
-class mandatory_plopt(plopt):
-    def __init__(self, type, **kwargs):
-        self._type  = type
-        self._name  = kwargs.pop("name", self.unnamed)
-        self._doc   = kwargs.pop("doc", '')        
-
-        # Keep the remaining kwargs for further use.
-        self._kwargs = kwargs
-
-        # For __str__ use only
-        self.__default_value = None
-    
-class plargs(object):
-    """Values read from or expected for PLearn command-line variables.
-
-    The core class of this module. See modules documentation for details on
-    possible uses.
-    """
-    _extensible_ = False
-
-    #######  Static methods  ######################################################
-
-    def buildClassContext(context):
-        assert not hasattr(context, 'binders')
-        context.binders = {}
-    buildClassContext = staticmethod(buildClassContext)
-
-    def getBinders():
-        """Returns a list of all binders in the current context."""
-        context = actualContext(plargs)
-        binder_names = context.binders.keys()
-        binder_names.sort()
-
-        binders = []
-        for bname in binder_names:
-            binders.append(context.binders[bname])        
-        return binders
-    getBinders = staticmethod(getBinders)
-
-    def getContextBindings():
-        """Returns a L{Bindings} instance of plopt name-to-value pairs.
-
-        The bindings could thereafter be used to re-create the current
-        context from a command-line::
-
-            bindings = plargs.getContextBindings()
-            cmdline = [ "%s=%s"%(opt, bindings['opt']) for opt in bindings ]
-            actual_command_line = " ".join(cmdline)
-        """
-        context = actualContext(plargs)
-        bindings = Bindings( )
-
-        for binder in plargs.getBinders():
-            for opt in plopt.iterator(binder):
-                bindings[opt.getName()] = opt.get()
-
-        for namespace in plargs.getNamespaces():
-            for opt in plopt.iterator(namespace):
-                key = "%s.%s"%(namespace.__name__, opt.getName())
-                bindings[key] = opt.get()
-
-        return bindings
-    getContextBindings = staticmethod(getContextBindings)
-
-    def getNamespaces():
-        """Returns a list of all namespaces in the current context."""
-        # Note the (hackish?) use of plnamespace. Needed to ensure the
-        # existance of context.namepaces...
-        context = actualContext(plnamespace) 
-        nsp_names = context.namespaces.keys()
-        nsp_names.sort()        
-        return [ context.namespaces[nsp] for nsp in nsp_names ]
-    getNamespaces = staticmethod(getNamespaces)
-
-    def getHolder(holder_name):
-        holder = plnamespace.getHolder(holder_name)
-        if holder is None:
-            holder = actualContext(plargs).binders.get(holder_name)
-        return holder
-    getHolder = staticmethod(getHolder)
-
-    def parse(*args):
-        """Parses a list of argument strings."""
-        if len(args)==1 and isinstance(args[0], list):
-            args = args[0]
-
-        context = actualContext(plargs)
-        for statement in args:
-            assert isinstance(statement, str), statement
-
-            option, value = statement.split('=', 1)
-
-            option = option.strip()
-            value  = value.strip()
-
-            if option=="expdir":
-                context._expdir_ = value
-                continue
-
-            if option=="expdir_root":
-                context._expdir_root_ = value
-                continue            
-
-            try:
-                holder_name, option = option.split('.')
-                holder = plargs.getHolder(holder_name)
-
-            # option.split('.') did not return a pair, i.e. not dot in option
-            except ValueError:
-                holder_name, holder = None, plopt.getHolder(option)
-
-            # Was the holder for that option defined?
-            if holder is None:
-                plopt.addCmdLineOverride(holder_name, option, value)
-            else:
-                setattr(holder, option, value)
-    parse = staticmethod(parse)
-    
-    #######  Metaclass  ###########################################################
-    
-    class __metaclass__(type):
-        """Overrides the attribute management behavior."""
-        def __new__(metacls, clsname, bases, dic):
-            cls = type.__new__(metacls, clsname, bases, dic)
-            plargs = cls
-            if clsname != "plargs":
-                plargs = globals()['plargs']
-
-            context = actualContext(plargs)
-    
-            # Keep track of binder subclass
-            if clsname != "plargs":
-                context.binders[clsname] = cls
-
-            # Introspection of the subclasses
-            if cls is not plargs:
-                for option, value in dic.iteritems():                    
-                    if neglected_member(option, value):
-                        continue
-                    
-                    # Define the plopt instance
-                    plopt.define(cls, option, value)
-
-            return cls
-
-        def __setattr__(cls, option, value):
-            if option == "expdir":
-                raise AttributeError("Cannot modify the value of 'expdir'.")
-
-            plargs = cls
-            if cls.__name__ != "plargs":
-                plargs = globals()['plargs']
-
-            context = actualContext(plargs)
-            if cls is plargs:
-                raise AttributeError(
-                    "Can't set option '%s' directly on plargs. "
-                    "Proceed trough the binder or namespace." % option)
-
-            try:                    
-                plopt.override(cls, option, value)
-            except AttributeError, err:
-                if cls._extensible_:
-                    plopt.define(cls, option, value)
-                else:
-                    raise AttributeError(
-                        "Binder %s does not contain a plopt instance named %s. "
-                        "One can't set a value to an undefined option. (%s)"
-                        %(cls.__name__, option, err))
-
-        def __getattribute__(cls, key):
-            attr = None
-            try:
-                attr = type.__getattribute__(cls, key)
-                if key.startswith('_') or not isinstance(attr, plopt):
-                    return type.__getattribute__(cls, key)
-            except AttributeError:
-                pass
-
-            # The key should map to a plopt instance, we need the context...
-            plargs = cls
-            if cls.__name__ != "plargs":
-                plargs = globals()['plargs']
-
-            # Special management of expdir plarg
-            if key == "expdir":
-                return actualContext(plargs).getExpdir()
-            elif key == "expdir_root":
-                return actualContext(plargs).getExpdirRoot()
-
-            # Find to holder to which option belongs
-            holder = cls
-            if cls is plargs:
-                try:
-                    holder = actualContext(plargs).plopt_binders[key]
-                except KeyError:
-                    raise AttributeError("Unknown option '%s'. Is it namespaced?"%key)
-
-            # We now have a valid holder: dig out the plopt *value*
-            plopt_instance = type.__getattribute__(holder, key)
-            assert isinstance(plopt_instance, plopt)
-            return plopt_instance.get()
-
-# For backward compatibility
-class _plarg_defaults:
-    """Magic class to L{contextualize<context>} I{plarg_defaults}.
-
-    The I{plarg_defaults} object exists mainly for backward compatibility
-    reasons. It was (is) meant to store a default value for some command
-    argument so that::
-
-        plarg_defaults.algo = "classical"
-        print plargs.algo
-
-    would print "classical" even if no command line argument I{algo} is encountered.
-
-    For clarity sakes, we suggest that one now regroup all default values
-    he wishes to set for "plargs" in some L{binder<plargs>}, e.g.::
-
-        class Misc(plargs):
-            algo      = "classical"
-            n_inputs  = 10
-            n_outputs = 2
-
-        print plargs.algo
-
-    The behavior will be the same, but this syntax allows to I{highlight}
-    the definition of default values while being more aligned with the new
-    generation of U{plargs}.
-    """
-    def _getBinder(self):
-        binders = actualContext(plargs).binders
-        if 'plarg_defaults' not in binders:
-            binders['plarg_defaults'] = \
-                new.classobj('plarg_defaults', (plargs,), {'_extensible_' : True})
-        return binders['plarg_defaults']
-
-    def __getattribute__(self, option):
-        return getattr(self._getBinder(), option)
-
-    def __setattr__(self, option, value):
-        defaults = self._getBinder()
-
-        # Say the default was value of a given was set to 'D'. It, the
-        # *default* value, can well be set modified later to 'E'. But we
-        # want this to modify the script behaviour IFF the option's value
-        # was *not* overriden from the command line. A simple call to
-        # 'setattr' would discard the command-line override!
-        if hasattr(defaults, option):
-            attr_plopt = object.__getattribute__(defaults,option)
-            attr_plopt.setDefault(value)
-            return
-
-        setattr(defaults, option, value)
-plarg_defaults = _plarg_defaults()
-        
-class plnamespace:
-    """Avoiding name clashes for L{plargs}.
-
-    Alike binders, L{plnamespace} subclasses allow one to define the value
-    of expected L{plargs}, but plarg names are encapsulated in namespaces. This
-    allows many options to have the same name, as long as they are not in
-    the same namespace::
-
-        # 
-        # namespaced.pyplearn
-        #
-        from plearn.pyplearn.plargs import *
-        
-        class macd(plnamespace):
-            ma_len = plopt(252,
-                        doc="The moving average length to be used in the macd model.")
-
-        class PCA(plnamespace):
-            algo      = "classical"
-            ma_len    = plopt([126, 252],
-                              doc="A list of moving average lenghts to be used "
-                              "during the preprocessing.")
-            n_inputs  = plopt(10, doc="The number of inputs to be used.")
-
-        print >>sys.stderr, macd.ma_len, PCA.algo, PCA.ma_len
-
-        try:
-            print >>sys.stderr, plargs.n_inputs
-        except AttributeError:
-            print >>sys.stderr, \
-                  "n_inputs is namespaced, you must access it through 'PCA'"
-
-        #####
-        prompt %> plearn namespaced.pyplearn macd.ma_len=126 PCA.algo=weird
-        252 weird [126, 252]
-        n_inputs is namespaced, you must access it through 'PCA'
-    """    
-    def buildClassContext(context):
-        assert not hasattr(context, 'namespaces')
-        context.namespaces = {}
-    buildClassContext = staticmethod(buildClassContext)
-
-    def getHolder(holder_name):
-        return actualContext(plnamespace).namespaces.get(holder_name)
-    getHolder = staticmethod(getHolder)
-
-    def getPlopt(cls, optname):
-        return super(cls, cls).__getattribute__(cls, optname)
-    getPlopt = classmethod(getPlopt)
-
-    def inherit(namespace):
-        """A deep-copy driven inheritance-like mechanism.
-
-        In the context of plnamespace, usual (Python) inheritance is not
-        satisfactory. Indeed, the options defined in some base class will be
-        shared among subclasses. However, when one would want to subclass a
-        plnamespace, it is more likely the he want the 'subclass' to have
-        options 'of the same name' than the ones in the base-class but
-        still independent.
-
-        This method provide a concise way to enact this inheritance-like
-        relationship between some New and some Existing namespaces
-
-            class New(plnamespace):
-                __metaclass__ = plnamespace.inherit(Existing)
-
-                other_option  = plopt("Other",
-                                      doc="An option that is not in the base class.")
-
-        Note that if 'Existing.some_option=VALUE' is overriden through the command-line,
-        the value of 'New.some_option' will be VALUE unless explicitely overrode. 
-        """
-        META = namespace.__metaclass__
-        class __metaclass__(META):
-            def __new__(metacls, clsname, bases, dic):
-                # Do not use plopt.optdict: the documentation, choices and other
-                # property would be lost...
-                for opt in plopt.iterator(namespace):
-                    if not opt.getName() in dic:
-                        inh_opt = copy.deepcopy(opt)
-                        inh_opt.set( opt.get() )
-                        dic[inh_opt.getName()] = inh_opt
-                    
-                #OLD: optdict = dict([ (
-                #OLD:     opt.getName(), opt) for opt in plopt.iterator(namespace) ])
-                #OLD: dic.update( copy.deepcopy(optdict) )
-                cls = META.__new__(metacls, clsname, bases, dic)
-                return cls        
-        return __metaclass__
-    inherit = staticmethod(inherit)
-        
-    class __metaclass__(type):
-        def __new__(metacls, clsname, bases, dic):
-            cls = type.__new__(metacls, clsname, bases, dic)
-            if clsname != "plnamespace":
-                context = actualContext(globals()["plnamespace"])
-                context.namespaces[clsname] = cls
-
-                for option, value in dic.iteritems():
-                    if neglected_member(option, value):
-                        continue
-                    # Define the plopt instance
-                    plopt.define(cls, option, value)
-            return cls
-
-        def __getattribute__(cls, key):
-            if key.startswith('_'):
-                return type.__getattribute__(cls, key)
-
-            attr = type.__getattribute__(cls, key)
-            try:
-                plopt_instance = attr
-                assert isinstance(plopt_instance, plopt)
-                return plopt_instance.get()
-            except AssertionError:
-                assert inspect.ismethod(attr) or inspect.isfunction(attr)
-                return attr
-
-        def __setattr__(cls, key, value):
-            if key.startswith('_') \
-               or (hasattr(cls, key) and callable(getattr(cls,key))):
-                type.__setattr__(cls,key,value)
-            else:
-                try:
-                    plopt.override(cls, key, value)
-                except AttributeError:
-                    raise AttributeError(
-                        "Namespace %s does not contain a plopt instance named %s. "
-                        "One can't set a value to an undefined option."%(cls.__name__, key))
-
-class _TmpHolder:
-    def __init__(self, holder_name):
-        self._name = holder_name
-
-    def checkConsistency(self, holder):
-        from plearn.pyplearn import PyPLearnError
-        if self._name is None:
-            assert not issubclass(holder, plnamespace)
-        else:
-            assert holder.__name__ == self._name
-
-# class _TmpHolderMap(dict):
-#     def __getitem__(self, key):
-#         if not key in self:
-#             self.__setitem__(key, [])
-#         return super(ListMap, self).__getitem__(key)
-    
-
-#######  For backward compatibily: will be deprecated soon  ###################
-
-class plargs_binder(plargs):
-    class __metaclass__(plargs.__metaclass__):
-        def __new__(metacls, clsname, bases, dic):            
-            if clsname=="plargs_binder":
-                return type.__new__(metacls, clsname, bases, dic)
-            warn("plargs_binder is deprecated. Inherit directly from plargs instead.",
-                 DeprecationWarning)
-            return plargs.__metaclass__.__new__(metacls, clsname, bases, dic)
-
-class plargs_namespace(plnamespace):
-    class __metaclass__(plnamespace.__metaclass__):
-        def __new__(metacls, clsname, bases, dic):
-            if clsname=="plargs_namespace":
-                return type.__new__(metacls, clsname, bases, dic)
-            warn("plargs_namespace is deprecated. Inherit from plnamespace instead.",
-                 DeprecationWarning)
-            return plnamespace.__metaclass__.__new__(metacls, clsname, bases, dic)
-
-
-#######  Unit Tests: invoked from outside  #####################################
-
-def printCurrentContext(out=sys.stdout):
-    print >>out, "Expdir:", plargs.expdir
-    
-    for binder in plargs.getBinders():
-        print >>out, "Binder:", binder.__name__
-        for opt in plopt.iterator(binder):
-            print >>out, '   ',opt.getName()+':', opt
-        print >>out, ''
-    
-    for namespace in plargs.getNamespaces():
-        print >>out, "Namespace:", namespace.__name__
-        for opt in plopt.iterator(namespace):
-            print >>out, '   ',opt.getName()+':', opt
-
-def test_plargs():                
-    print "#######  Binders  #############################################################\n"
-    class binder(plargs):
-        c = "c"
-        d = "d"
-        e = "e"
-        f = "f"
-
-    def bCheck(attr):
-        plargs_attr = getattr(plargs, attr)
-        binder_attr = getattr(binder, attr)
-        assert not isinstance(binder_attr, plopt)
-
-        print "Access through plargs:", plargs_attr        
-        print "Direct access:", binder_attr
-        assert plargs_attr==binder_attr
-
-        # binder_plopt = binder.getPlopt(attr)
-        # print "Access to plopt:", "TO BE DONE" # binder_plopt
-        print 
-        
-
-    ### Untouched
-    print "+++ Untouched plarg\n"
-    bCheck('c')
-
-    ### Standard assignment
-    print "+++ Standard assignment through binder\n"
-    binder.d = "Youppi"
-    bCheck('d')
-
-    ### Subclass propagation
-    print "+++ Setting binded option using 'dotted' key\n"
-    plargs.parse("binder.e = ** E **")
-    bCheck('e')
-
-    print "#######  Namespaces  ##########################################################\n"
-    class n(plnamespace):    
-        namespaced = "within namespace n"
-
-    def nCheck():
-        assert not isinstance(n.namespaced, plopt)
-        print "Direct access:", n.namespaced
-
-        # n_plopt = n.getPlopt('namespaced')
-        # print "Access to plopt:", "TO BE DONE" # n_plopt
-        print 
-
-    ### Untouched
-    nCheck()
-    
-    ### Standard assignments
-    print "+++ Standard assignment through namespace\n"
-    n.namespaced = "WITHIN NAMESPACE n"
-    nCheck()
-    
-    ### Subclass propagation
-    plargs.parse("n.namespaced=FROM_SETATTR")
-    nCheck()    
-
-    print "#######  Contexts Management  #################################################\n"
-
-    print "+++ Context 1"
-    first_context = getCurrentContext()
-    printCurrentContext()
-    print 
-
-    print "+++ Context 2"        
-    second_context = createNewContext()
-    plarg_defaults.algo = "classical"
-
-    print "-- Before creation of the new 'n' plnamespace:"
-    print n.namespaced
-    printCurrentContext()
-    print
-    
-    class n(plnamespace):
-        namespaced = "NEW NAMESPACED ATTR"
-
-    print "-- After creation of the new 'n' plnamespace:"
-    print n.namespaced
-    printCurrentContext()
-    print
-
-
-    print "+++ Back to Context 1"
-    setCurrentContext(first_context)
-    printCurrentContext()
-    print 
-
-def test_plargs_parsing():
-    def readScript(*command_line):
-        context_handle = createNewContext()
-        
-        plargs.parse(*command_line)
-
-        class MyBinder(plargs):
-            binded1 = "binded1"
-            binded2 = "binded2"
-            binded3 = "binded3"
-
-        class MyNamespace(plnamespace):
-            namespaced1 = "namespaced1"
-            namespaced2 = "namespaced2"
-            namespaced3 = "namespaced3"
-
-        header = "Context %d"%context_handle
-        header += '\n'+("="*len(header))
-        print header
-        printCurrentContext()
-        print        
-        return context_handle, header
-    
-    contexts = [ readScript() ]
-    contexts.append(
-        # list argument
-        readScript(["binded1=BINDED1", "binded2=BINDED2",
-                    "MyNamespace.namespaced3=NAMESPACED3"]) )
-    contexts.append(
-          # string arguments
-          readScript("binded3=BINDED3", "MyNamespace.namespaced1=NAMESPACED1") )
-
-    print 
-    print "Reprint all contexts to ensure nothing was lost or corrupted."
-    print "-------------------------------------------------------------"
-    print 
-    for c, header in contexts:
-        setCurrentContext(c)
-        print header
-        printCurrentContext()
-        closeCurrentContext()
-        print 
-
-    print 
-    print "Misspelled command-line argument"
-    print "-------------------------------------------------------------"
-    print
-    from plearn.pyplearn import PyPLearnError
-    try:
-        readScript(["bindd1=BINDED1", "binded2=BINDED2",
-                    "MyNamespace.namespaced3=NAMESPACED3"])
-        closeCurrentContext()        
-    except PyPLearnError, err:
-        print 
-        print 'PyPLearnError:', err
-
-
-def test_mandatory_plargs(*command_line):
-    from plearn.pyplearn import PyPLearnError
-    plargs.parse(*command_line)
-
-    try:
-        class MyBinder(plargs):
-            mandatory = mandatory_plopt(int,
-                                        doc="Some mandatory int command line argument")
-        
-        print MyBinder.mandatory
-
-    except PyPLearnError, err:
-        print err
-
-def test_misspelled_plargs():
-    from plearn.pyplearn import PyPLearnError
-
-    plargs.parse("binded1=[]", "binded2=[ 2 ]", "namespace='misspelled'")
-
-    class Binder(plargs):
-        binded1 = [ 1 ]
-        binded2 = plopt([], elem_type=int)
-
-    class Namespace(plnamespace):
-        namespaced = "N"
-
-    print Binder.binded1, Binder.binded2, Namespace.namespaced
-
-    from plearn.pyplearn import PyPLearnError
-    try:
-        closeCurrentContext()
-    except PyPLearnError, err:
-        print err

Copied: branches/numarray/python_modules/plearn/pyplearn/plargs.py (from rev 7939, trunk/python_modules/plearn/pyplearn/plargs.py)



From yoshua at mail.berlios.de  Thu Aug  9 19:37:04 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 9 Aug 2007 19:37:04 +0200
Subject: [Plearn-commits] r7969 - trunk/plearn_learners/online
Message-ID: <200708091737.l79Hb4RO022524@sheep.berlios.de>

Author: yoshua
Date: 2007-08-09 19:37:04 +0200 (Thu, 09 Aug 2007)
New Revision: 7969

Modified:
   trunk/plearn_learners/online/VBoundDBN2.cc
Log:
fixed few things


Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-09 15:42:22 UTC (rev 7968)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-09 17:37:04 UTC (rev 7969)
@@ -66,7 +66,7 @@
     declareOption(ol, "rbm1", &VBoundDBN2::rbm1,
                   OptionBase::buildoption,
                   "First RBM, taking the DBN's input in its visible layer");
-    declareOption(ol, "rbm2", &VBoundDBN2::rbm1,
+    declareOption(ol, "rbm2", &VBoundDBN2::rbm2,
                   OptionBase::buildoption,
                   "Second RBM, producing the DBN's output and generating internal representations.");
 
@@ -94,13 +94,16 @@
             rbm2->forget();
         }
     }
-    ports.append("input"); // 0
-    ports.append("bound"); // 1
-    ports.append("nll"); // 2
-    ports.append("sampled_h"); // 3
-    ports.append("global_improvement"); // 4
-    ports.append("ph_given_v"); // 5
-    ports.append("p2ph"); // 6
+    if (ports.length()==0)
+    {
+        ports.append("input"); // 0
+        ports.append("bound"); // 1
+        ports.append("nll"); // 2
+        ports.append("sampled_h"); // 3
+        ports.append("global_improvement"); // 4
+        ports.append("ph_given_v"); // 5
+        ports.append("p2ph"); // 6
+    }
 }
 
 ///////////
@@ -134,7 +137,9 @@
     rbm2->setAllLearningRates(rbm2->cd_learning_rate);
     rbm2->hidden_layer->setExpectations(*p2ph_);
     rbm2->hidden_layer->generateSamples();
+    rbm2->sampleVisibleGivenHidden(rbm2->hidden_layer->samples);
     rbm2->computeHiddenActivations(rbm2->visible_layer->samples);
+    rbm2->hidden_layer->computeExpectations();
     rbm2->visible_layer->update(*sampled_h_,rbm2->visible_layer->samples);
     rbm2->connection->update(*sampled_h_,*p2ph_,
                              rbm2->visible_layer->samples,
@@ -213,9 +218,11 @@
 ////////////
 void VBoundDBN2::forget()
 {
-    PLASSERT(rbm1 && rbm2);
-    rbm1->forget();
-    rbm2->forget();
+    if (rbm1 && rbm2)
+    {
+        rbm1->forget();
+        rbm2->forget();
+    }
 }
 
 ///////////
@@ -259,10 +266,9 @@
         rbm1->sampleHiddenGivenVisible(*input);
         *ph_given_v << rbm1->hidden_layer->getExpectations();
         *sampled_h << rbm1->hidden_layer->samples;
-        rbm1->visible_layer->fpropNLL(*sampled_h,neglogphsample_given_v);
         rbm1->computeFreeEnergyOfVisible(*input,FE1v,false);
-        rbm1->computeFreeEnergyOfHidden(*sampled_h,FE1h);
         rbm2->computeFreeEnergyOfVisible(*sampled_h,FE2h,false);
+        p2ph->resize(mbs,rbm2->hidden_layer->size);
         *p2ph << rbm2->hidden_layer->getExpectations();
         substract(FE1h,FE2h,*global_improvement);
 
@@ -324,10 +330,18 @@
 // getPortSizes //
 //////////////////
 const TMat<int>& VBoundDBN2::getPortSizes() {
+    PLASSERT(rbm1 && rbm2);
     if (sizes.width()!=2)
     {
         sizes.resize(nPorts(),2);
         sizes.fill(-1);
+        sizes(0,1)=rbm1->visible_layer->size;
+        sizes(1,1)=1;
+        sizes(2,1)=1;
+        sizes(3,1)=rbm1->hidden_layer->size;
+        sizes(4,1)=1;
+        sizes(5,1)=rbm1->hidden_layer->size;
+        sizes(6,1)=rbm2->hidden_layer->size;
     }
     return sizes;
 }
@@ -347,7 +361,6 @@
     deepCopyField(global_improvement_state, copies);
     deepCopyField(ph_given_v_state, copies);
     deepCopyField(p2ph_state, copies);
-    deepCopyField(neglogphsample_given_v, copies);
     deepCopyField(all_h, copies);
     deepCopyField(all_h, copies);
     deepCopyField(neglogP2h, copies);



From yoshua at mail.berlios.de  Thu Aug  9 19:40:58 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Thu, 9 Aug 2007 19:40:58 +0200
Subject: [Plearn-commits] r7970 - trunk/plearn_learners/online
Message-ID: <200708091740.l79HewdE022744@sheep.berlios.de>

Author: yoshua
Date: 2007-08-09 19:40:57 +0200 (Thu, 09 Aug 2007)
New Revision: 7970

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
Changes to RBMModule to allow VBoundDBN2 to work


Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2007-08-09 17:37:04 UTC (rev 7969)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-08-09 17:40:57 UTC (rev 7970)
@@ -258,8 +258,9 @@
 ////////////////
 int ModuleLearner::outputsize() const
 {
-    PLASSERT( module && store_outputs );
-    return module->getPortWidth("output");
+    if ( module && store_outputs )
+        return module->getPortWidth("output");
+    return 0;
 }
 
 ////////////

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-09 17:37:04 UTC (rev 7969)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-09 17:40:57 UTC (rev 7970)
@@ -1197,7 +1197,7 @@
 
 }
 
-void RBMModule::computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat neg_log_pvisible_given_phidden)
+void RBMModule::computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat& neg_log_pvisible_given_phidden)
 {
     computeVisibleActivations(hidden,true);
     int n_h = hidden.length();

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-09 17:37:04 UTC (rev 7969)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-09 17:40:57 UTC (rev 7970)
@@ -325,7 +325,7 @@
 
     void computePartitionFunction();
 
-    void computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat neg_log_pvisible_given_phidden);
+    void computeNegLogPVisibleGivenPHidden(Mat visible, Mat hidden, Mat* neg_log_phidden, Mat& neg_log_pvisible_given_phidden);
 
 private:
     //#####  Private Member Functions  ########################################



From nouiz at mail.berlios.de  Thu Aug  9 20:22:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 9 Aug 2007 20:22:14 +0200
Subject: [Plearn-commits] r7971 - trunk/python_modules/plearn/parallel
Message-ID: <200708091822.l79IME9U024698@sheep.berlios.de>

Author: nouiz
Date: 2007-08-09 20:22:14 +0200 (Thu, 09 Aug 2007)
New Revision: 7971

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Speed optimisation,refactoring of redirection and new redirection attribute

-Added the attribute redirect_stdout_to_stderr
-Refactored the code to calculate the redirection in a function in DBIBase
	-This allow to have fewer log file
	-Log files with nfs ainsted of local file can slow down DBILocal by 10 times
-the generation of a unique id by hash is an option as this can slow the creation of the DBI objects by 10 times. It default to use the commands more the current time for the unique id
-changed add_command to add_commands who take a list of commant to speed up by removing function call which are costly in python


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 17:40:57 UTC (rev 7970)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 18:22:14 UTC (rev 7971)
@@ -7,14 +7,14 @@
 import time
 import traceback
 import shutil
-from subprocess import Popen,PIPE
+from subprocess import Popen,PIPE,STDOUT
 from utils import *
 from configobj import ConfigObj
 from textwrap import dedent
 import pdb
 from threading import Thread,BoundedSemaphore
 from time import sleep
-#from plearn.pymake import pymake
+import datetime
 
 STATUS_FINISHED = 0
 STATUS_RUNNING = 1
@@ -58,6 +58,7 @@
         #
         self.file_redirect_stdout = True
         self.file_redirect_stderr = True
+        self.redirect_stderr_to_stdout = True
 
         # Initialize the namespace
         self.requirements = ''
@@ -86,16 +87,26 @@
             self.post_batch = [self.post_batch]
 
     def n_avail_machines(self): raise NotImplementedError, "DBIBase.n_avail_machines()"
-    def add_command(self,command): raise NotImplementedError, "DBIBase.add_command()"
+    def add_commands(self,commands): raise NotImplementedError, "DBIBase.add_commands()"
+    def get_redirection(stdout_file,stderr_file):
+        """Calcule the needed redirection based of the objects attribute
+        return a tuple (stdout,stderr) that can be used with popen
+        """
+        output = PIPE
+        error = PIPE
+        if int(self.file_redirect_stdout):
+            output = file(stdout_file, 'w')
+        if self.redirect_stderr_to_stdout:
+            error = STDOUT
+        elif int(self.file_redirect_stderr):
+            error = file(stderr_file, 'w')
+            
     def exec_pre_batch(self):
         # Execute pre-batch
-        output = PIPE
-        error = PIPE
         pre_batch_command = ';'.join( self.pre_batch )
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.pre_batch.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.pre_batch.err', 'w')
+
+        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+            
         if not self.test:
             self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
         else:
@@ -103,13 +114,10 @@
             
     def exec_post_batch(self):
         # Execute post-batch
-        output = PIPE
-        error = PIPE
         post_batch_command = ';'.join( self.post_batch )
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.post_batch.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.post_batch.err', 'w')
+        
+        output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+        
         if not self.test:
             self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
         else:
@@ -123,11 +131,8 @@
 
 class Task:
 
-    def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, args = {}):
-        self.unique_id = get_new_sid('')
+    def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, gen_unique_id = True, args = {}):
         self.add_unique_id = 0
-        formatted_command = re.sub( '[^a-zA-Z0-9]', '_', command );
-        self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
         # The "python utils.py..." command is not exactly the same for every
         # task in a batch, so it cannot be considered a "pre-command", and
         # has to be actually part of the command.  Since the user-provided
@@ -141,6 +146,14 @@
         for key in args.keys():
             self.__dict__[key] = args[key]
 
+        formatted_command = re.sub( '[^a-zA-Z0-9]', '_', command );
+        if gen_unique_id:
+            self.unique_id = get_new_sid('')#compation intense
+            self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
+        else:
+            self.unique_id = formatted_command+str(datetime.datetime.now()).replace(' ','_')
+            self.log_file = truncate( os.path.join(log_dir, self.unique_id), 200) + ".log"
+
         if self.add_unique_id:
                 command = command + ' unique_id=' + self.unique_id
         #self.before_commands = []
@@ -233,7 +246,7 @@
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format,self.pre_tasks,
-                                   self.post_tasks,self.dolog))
+                                   self.post_tasks,self.dolog,False))
 
 
     def run_one_job(self, task):
@@ -253,12 +266,9 @@
         task.launch_time = time.time()
         set_config_value(task.log_file, 'SCHEDULED_TIME',
                 time.strftime(self.time_format, time.localtime(time.time())))
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(task.log_file + '.out','w')
-        if int(self.file_redirect_stderr):
-            error = file(task.log_file + '.err','w')
+
+        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+
         if self.test == False:
             task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
@@ -304,7 +314,7 @@
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format,
                                    [self.pre_tasks, 'cd parent;'],
-                                   self.post_tasks,self.dolog,args))
+                                   self.post_tasks,self.dolog,False,args))
 
 
     def run(self):
@@ -372,12 +382,8 @@
             exec_pre_batch()
 
         # Launch bqsubmit
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.err', 'w')
+        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+
         self.p = Popen( 'bqsubmit', shell=True, stdout=output, stderr=error)
 
         os.chdir('parent')
@@ -410,7 +416,7 @@
 
             # We use the absolute path so that we don't have corner case as with ./ 
             c = os.path.normpath(os.path.join(os.getcwd(), c))
-            command = c + c2
+            command = "".join([c,c2])
             
                 # We will execute the command on the specified architecture
                 # if it is specified. If the executable exist for both
@@ -462,7 +468,7 @@
 
             self.tasks.append(Task(newcommand, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
-                                   self.post_tasks,self.dolog,args))
+                                   self.post_tasks,self.dolog,False,args))
 
             #keeps a list of the temporary files created, so that they can be deleted at will            
 
@@ -575,29 +581,24 @@
                          '/python_modules/plearn/parallel/configobj.py',  configobj_file)
             self.temp_files.append(configobj_file)            
             os.chmod(configobj_file, 0755)
+
         # Launch condor
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.err', 'w')
-
         if self.test == False:
+            (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
             print "Executing: condor_submit " + condor_file
             self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
         else:
             print "Created condor file: " + condor_file
             
     def clean(self):
-                
-        sleep(20)
-        for file_name in self.temp_files:
-            try:
-                os.remove(file_name)
-            except os.error:
-                pass
-            pass    
+        if len(self.temp_files)>0:
+            sleep(20)
+            for file_name in self.temp_files:
+                try:
+                    os.remove(file_name)
+                except os.error:
+                    pass
+                pass    
 
 
     def run(self):
@@ -620,67 +621,75 @@
     def __init__( self, commands, **args ):
         self.nb_proc=1
         DBIBase.__init__(self, commands, **args)
+        if isinstance(self.nb_proc,basestring):
+            self.nb_proc=int(self.nb_proc)
         self.args=args
         self.threads=[]
+        self.add_commands(commands)
+            
+    def add_commands(self,commands):
+        if not isinstance(commands, list):
+            self.post_batch = [self.post_batch]
+        #We copy the variable localy as an optimisation for big list of commands
+        #save around 15% with 100 commands
+        tmp_dir=self.tmp_dir
+        log_dir=self.log_dir
+        time_format=self.time_format
+        pre_tasks=self.pre_tasks
+        post_tasks=self.post_tasks
+        dolog=self.dolog
+        args=self.args
         for command in commands:
-            self.add_command(command)
+            pos = string.find(command,' ')
+            if pos>=0:
+                c = command[0:pos]
+                c2 = command[pos:]
+            else:
+                c=command
+                c2=""
+                
+            # We use the absolute path so that we don't have corner case as with ./ 
+            c = os.path.normpath(os.path.join(os.getcwd(), c))
+            command = "".join([c,c2])
             
-    def add_command(self,command):
-        pos = string.find(command,' ')
-        if pos>=0:
-            c = command[0:pos]
-            c2 = command[pos:]
-        else:
-            c=command
-            c2=""
+            # We will execute the command on the specified architecture
+            # if it is specified. If the executable exist for both
+            # architecture we execute on both. Otherwise we execute on the
+            # same architecture as the architecture of the launch computer
             
-        # We use the absolute path so that we don't have corner case as with ./ 
-        c = os.path.normpath(os.path.join(os.getcwd(), c))
-        command = c + c2
-        
-        # We will execute the command on the specified architecture
-        # if it is specified. If the executable exist for both
-        # architecture we execute on both. Otherwise we execute on the
-        # same architecture as the architecture of the launch computer
-        
-        if not os.path.exists(c):
-            raise Exception("The command '"+c+"' do not exist!")
-        elif not os.access(c, os.X_OK):
-            raise Exception("The command '"+c+"' do not have execution permission!")
-        self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
-                               self.time_format, self.pre_tasks,
-                               self.post_tasks,self.dolog,self.args))
-        
+            if not os.access(c, os.X_OK):
+                raise Exception("The command '"+c+"' do not exist or have execution permission!")
+            self.tasks.append(Task(command, tmp_dir, log_dir,
+                                   time_format, pre_tasks,
+                                   post_tasks,dolog,False,args))
         #keeps a list of the temporary files created, so that they can be deleted at will            
 
     def run_one_job(self,task):
-            
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(self.log_file + '.out', 'w')
-        if int(self.file_redirect_stderr):
-            error = file(self.log_file + '.err', 'w')
+        c = (';'.join(task.commands))
+        print c
+        if self.test:
+            return
 
-        c = (';'.join(task.commands))
-        if self.test == False:
+        (output,error)=get_redirection(task.log_file + '.out',task.log_file + '.err')
+#        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+
+        if self.nb_proc>1:
             self.sema.acquire()
-            print c
             p = Popen(c, shell=True,stdout=output,stderr=error)
             p.wait()
             self.sema.release()
         else:
-            print c
+            p = Popen(c, shell=True,stdout=output,stderr=error)
             
     def clean(self):
-                
-        sleep(20)
-        for file_name in self.temp_files:
-            try:
-                os.remove(file_name)
-            except os.error:
-                pass
-            pass    
+        if len(self.temp_files)>0:
+            sleep(20)
+            for file_name in self.temp_files:
+                try:
+                    os.remove(file_name)
+                except os.error:
+                    pass
+                pass    
 
     def run(self):
         if self.test:
@@ -694,21 +703,26 @@
         # Execute pre-batch
         if len(self.pre_batch)>0:
             exec_pre_batch()
-        
+
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        self.sema=BoundedSemaphore(int(self.nb_proc))
-        for (task,ind) in zip(self.tasks,range(len(self.tasks))):
-            t=Thread(target=self.run_one_job,args=(task,))
-            t.start()
-            self.threads.append(t)
+        if not self.test and self.nb_proc>1:
+            self.sema=BoundedSemaphore(int(self.nb_proc))
+            for task in self.tasks:
+                t=Thread(target=self.run_one_job,args=(task,))
+                t.start()
+                self.threads.append(t)
+                for t in self.threads:
+                    t.join()
+        else:
+            for task in self.tasks:
+                self.run_one_job(task)
 
-        for t in self.threads:
-            t.join()
-
         # Execute post-batchs
         if len(self.post_batch)>0:
             exec_post_batch()
             
+        print "The Log file are under %s"%self.log_dir
+        
     def clean(self):
         pass
 
@@ -773,7 +787,7 @@
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
-                                   self.post_tasks,self.dolog))
+                                   self.post_tasks,self.dolog,False))
         self.hosts= find_all_ssh_hosts()
         
 
@@ -797,12 +811,9 @@
         task.launch_time = time.time()
         set_config_value(task.log_file, 'SCHEDULED_TIME',
                 time.strftime(self.time_format, time.localtime(time.time())))
-        output = PIPE
-        error = PIPE
-        if int(self.file_redirect_stdout):
-            output = file(task.log_file + '.out','w')
-        if int(self.file_redirect_stderr):
-            error = file(task.log_file + '.err','w')        
+        
+        (output,error)=get_redirection(task.log_file + '.out',task.log_file + '.err')
+        
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):



From nouiz at mail.berlios.de  Thu Aug  9 20:33:48 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 9 Aug 2007 20:33:48 +0200
Subject: [Plearn-commits] r7972 - trunk/python_modules/plearn/parallel
Message-ID: <200708091833.l79IXmUs026764@sheep.berlios.de>

Author: nouiz
Date: 2007-08-09 20:33:48 +0200 (Thu, 09 Aug 2007)
New Revision: 7972

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
removed duplicate redirection and made the test mode work with all system


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 18:22:14 UTC (rev 7971)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 18:33:48 UTC (rev 7972)
@@ -263,14 +263,16 @@
         
         print command
 
+        if self.test:
+            return
+
         task.launch_time = time.time()
         set_config_value(task.log_file, 'SCHEDULED_TIME',
                 time.strftime(self.time_format, time.localtime(time.time())))
 
         (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
 
-        if self.test == False:
-            task.p = Popen(command, shell=True,stdout=output,stderr=error)
+        task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):
         print "The Log file are under %s"%self.log_dir
@@ -319,16 +321,7 @@
 
     def run(self):
         pre_batch_command = ';'.join( self.pre_batch );
-        if int(self.file_redirect_stdout):
-            pre_batch_command += ' >> ' + self.log_file + '.pre_batch.out'
-        if int(self.file_redirect_stderr):
-            pre_batch_command += ' 2>> ' + self.log_file + '.pre_batch.err'
-
         post_batch_command = ';'.join( self.post_batch );
-        if int(self.file_redirect_stdout):
-            post_batch_command += ' >> ' + self.log_file + '.post_batch.out'
-        if int(self.file_redirect_stderr):
-            post_batch_command += ' 2>> ' + self.log_file + '.post_batch.err'
 
         # create one (sh) script that will launch the appropriate ~~command~~
         # in the right environment
@@ -382,10 +375,11 @@
             exec_pre_batch()
 
         # Launch bqsubmit
-        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
-
-        self.p = Popen( 'bqsubmit', shell=True, stdout=output, stderr=error)
-
+        if not self.test:
+            (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+            self.p = Popen( 'bqsubmit', shell=True, stdout=output, stderr=error)
+        else:
+            print "[DBI] in test mode, we generate all the file, but we do not execute bqsubmit"
         os.chdir('parent')
 
         # Execute post-batchs
@@ -808,6 +802,9 @@
         command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
         print command
 
+        if self.test:
+            return
+        
         task.launch_time = time.time()
         set_config_value(task.log_file, 'SCHEDULED_TIME',
                 time.strftime(self.time_format, time.localtime(time.time())))



From nouiz at mail.berlios.de  Thu Aug  9 21:12:36 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 9 Aug 2007 21:12:36 +0200
Subject: [Plearn-commits] r7973 - trunk/scripts
Message-ID: <200708091912.l79JCaiA030480@sheep.berlios.de>

Author: nouiz
Date: 2007-08-09 21:12:36 +0200 (Thu, 09 Aug 2007)
New Revision: 7973

Modified:
   trunk/scripts/cdispatch
Log:
changed the log directory of the dbi objects to use the parameter of cluster


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-08-09 18:33:48 UTC (rev 7972)
+++ trunk/scripts/cdispatch	2007-08-09 19:12:36 UTC (rev 7973)
@@ -1,5 +1,5 @@
 #!/usr/bin/env python
-import sys,os,re,time
+import sys,os,re,time,datetime
 
 ScriptName="launchdbi.py"
 ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local[=nb_process]] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>] [--32|--64|--3264]'
@@ -171,6 +171,14 @@
     dbi_param["arch"]=ARCH
 if NB_PROC:
     dbi_param["nb_proc"]=NB_PROC
+    
+tmp="_".join(sys.argv)
+tmp=tmp[tmp.find("cdispatch"):]
+re.sub( '[^a-zA-Z0-9]', '_', tmp )
+tmp+=str(datetime.datetime.now()).replace(' ','_')
+print tmp
+dbi_param["log_dir"]=os.path.join("LOGS",tmp)
+dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
 if "test" in optionargs:
     print "We generated %s command in the file"% len(commands)



From lysiane at mail.berlios.de  Thu Aug  9 21:15:44 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Thu, 9 Aug 2007 21:15:44 +0200
Subject: [Plearn-commits] r7974 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200708091915.l79JFi4s030733@sheep.berlios.de>

Author: lysiane
Date: 2007-08-09 21:15:44 +0200 (Thu, 09 Aug 2007)
New Revision: 7974

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
modifs to increase speed: inlines, compute/update of weights 


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 19:12:36 UTC (rev 7973)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 19:15:44 UTC (rev 7974)
@@ -621,11 +621,15 @@
     real scalingFactor = -1*(pl_log(pow(2*Pi*noiseVariance, inputSpaceDim/2.0)) 
                              +
                              pl_log(trainingSetLength));
+    Vec neighbor(inputSpaceDim);
+    Vec predictedTarget(inputSpaceDim);
     for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
+        seeTrainingPoint(neighborIdx,neighbor);
         for(int transformIdx=0 ; transformIdx<nbTransforms ; transformIdx++){
             weight = computeReconstructionWeight(y,
-                                                 neighborIdx,
-                                                 transformIdx);
+                                                 neighbor,
+                                                 transformIdx,
+                                                 predictedTarget);
             weight = MULT_weights(weight,
                                   transformDistribution[transformIdx]);
             totalWeight = SUM_weights(weight,totalWeight);
@@ -1568,6 +1572,23 @@
     reconstructionSet[candidateIdx].weight = w;
     return w; 
 }
+
+//! NOT A USER METHOD !
+real TransformationLearner::updateReconstructionWeight(int candidateIdx, 
+                                                       const Vec & target,
+                                                       const Vec & neighbor,
+                                                       int transformIdx,
+                                                       Vec & predictedTarget){
+    
+    real w = computeReconstructionWeight(target,
+                                         neighbor,
+                                         transformIdx,
+                                         predictedTarget);
+    reconstructionSet[candidateIdx].weight = w;
+    return w;
+}
+
+
 real TransformationLearner::computeReconstructionWeight(const ReconstructionCandidate & gc)const
 {
     return computeReconstructionWeight(gc.targetIdx,
@@ -1579,28 +1600,41 @@
                                                         int transformIdx)const
 {
 
-    Vec target;
-    target.resize(inputSpaceDim);
+    Vec target(inputSpaceDim);
     seeTrainingPoint(targetIdx,target);
     return computeReconstructionWeight(target,
                                        neighborIdx,
                                        transformIdx);
 }
-real TransformationLearner::computeReconstructionWeight(const Vec & target_,
+real TransformationLearner::computeReconstructionWeight(const Vec & target,
                                                         int neighborIdx,
                                                         int transformIdx)const
 {
-    Vec neighbor;
-    neighbor.resize(inputSpaceDim);
+    Vec neighbor(inputSpaceDim);
     seeTrainingPoint(neighborIdx, neighbor);
-    Vec predictedTarget ;
-    predictedTarget.resize(inputSpaceDim);
+    return computeReconstructionWeight(target,neighbor,transformIdx);
+}
+
+real TransformationLearner::computeReconstructionWeight(const Vec & target,
+                                                        const Vec & neighbor,
+                                                        int transformIdx )const
+{
+    Vec predictedTarget(inputSpaceDim);
+    return computeReconstructionWeight(target, neighbor, transformIdx,predictedTarget);
+}
+
+real TransformationLearner::computeReconstructionWeight(const Vec & target,
+                                                        const Vec & neighbor,
+                                                        int transformIdx,
+                                                        Vec & predictedTarget)const
+{
     applyTransformationOn(transformIdx, neighbor, predictedTarget);
     real factor = -1/(2*noiseVariance);
-    real w = factor*powdistance(target_, predictedTarget);
-    return MULT_weights(w, transformDistribution[transformIdx]); 
+    real w = factor*powdistance(target, predictedTarget);
+    return MULT_weights(w, transformDistribution[transformIdx]);      
 }
 
+
 //!applies "transformIdx"th transformation on data point "src"
 void TransformationLearner::applyTransformationOn(int transformIdx,
                                                  const Vec & src,
@@ -1842,14 +1876,20 @@
     PLASSERT(pq.empty()); 
     
     real weight;
+    Vec target(inputSpaceDim);
+    seeTrainingPoint(targetIdx, target);
+    Vec neighbor(inputSpaceDim);
+    Vec predictedTarget(inputSpaceDim);
 
     //for each potential neighbor
     for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){
+            seeTrainingPoint(neighborIdx, neighbor);
             for(int transformIdx=0; transformIdx<nbTransforms; transformIdx++){
-                weight = computeReconstructionWeight(targetIdx, 
-                                                     neighborIdx, 
-                                                     transformIdx);
+                weight = computeReconstructionWeight(target, 
+                                                     neighbor, 
+                                                     transformIdx,
+                                                     predictedTarget);
                 
                 //if the weight is among "nbEntries" biggest weight seen,
                 //keep it until to see a bigger neighbor. 
@@ -1929,14 +1969,19 @@
     PLASSERT(pq.empty()); 
     
     real weight; 
+    Vec target(inputSpaceDim);
+    seeTrainingPoint(targetIdx, target);
+    Vec neighbor(inputSpaceDim);
+    Vec predictedTarget(inputSpaceDim);
     
     //for each potential neighbor
     for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){ //(the target cannot be his own neighbor)
-          
-            weight = computeReconstructionWeight(targetIdx, 
-                                                 neighborIdx, 
-                                                 transformIdx);
+            seeTrainingPoint(neighborIdx, neighbor);
+            weight = computeReconstructionWeight(target, 
+                                                 neighbor, 
+                                                 transformIdx,
+                                                 predictedTarget);
             //if the weight of the triple is among the "nbNeighbors" biggest 
             //seen,keep it until see a bigger weight. 
             if(int(pq.size()) < nbNeighbors){
@@ -1976,11 +2021,20 @@
     int candidateIdx =0;
     int  targetIdx = reconstructionSet[candidateIdx].targetIdx;
     real totalWeight = INIT_weight(0);
+    Vec target(inputSpaceDim);
+    seeTrainingPoint(targetIdx,target);
+    Vec neighbor(inputSpaceDim);
+    Vec predictedTarget(inputSpaceDim);
     
     while(candidateIdx < nbReconstructions){
         
+        seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx, neighbor);
         totalWeight = SUM_weights(totalWeight,
-                                  updateReconstructionWeight(candidateIdx));
+                                  updateReconstructionWeight(candidateIdx,
+                                                             target,
+                                                             neighbor,
+                                                             reconstructionSet[candidateIdx].transformIdx,
+                                                             predictedTarget));
         candidateIdx ++;
     
         if(candidateIdx == nbReconstructions)
@@ -1989,6 +2043,7 @@
             normalizeTargetWeights(targetIdx, totalWeight);
             totalWeight = INIT_weight(0);
             targetIdx = reconstructionSet[candidateIdx].targetIdx;
+            seeTrainingPoint(targetIdx, target);
         }
     }    
 }

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 19:12:36 UTC (rev 7973)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 19:15:44 UTC (rev 7974)
@@ -657,7 +657,7 @@
  
 
     //! stores the "idx"th training data point into the variable 'dst'
-    void seeTrainingPoint(const int idx, Vec & dst) const ;
+    inline void seeTrainingPoint(const int idx, Vec & dst) const ;
 
 
     //!GENERATE GAMMA RANDOM VARIABLES
@@ -686,32 +686,44 @@
     //!OPERATIONS ON WEIGHTS 
     
      //!normalizes the reconstruction weights related to a given target. 
-    void normalizeTargetWeights(int targetIdx, real totalWeight);
+    inline void normalizeTargetWeights(int targetIdx, real totalWeight);
     
     //!returns a random weight 
-    real randomWeight() const;
+    inline real randomWeight() const;
     
     //!arithmetic operations on  reconstruction weights
-    real INIT_weight(real initValue) const; //!CONSTRUCTOR
-    real PROBA_weight(real weight) const; //!GET CORRESPONDING PROBABILITY 
-    real DIV_weights(real numWeight, real denomWeight) const; //!DIVISION
-    real MULT_INVERSE_weight(real weight) const ;//!MULTIPLICATIVE INVERSE
-    real MULT_weights(real weight1, real weight2) const ; //!MULTIPLICATION
-    real SUM_weights(real weight1, real weight2) const ; //!SUM 
+    inline real INIT_weight(real initValue) const; //!CONSTRUCTOR
+    inline real PROBA_weight(real weight) const; //!GET CORRESPONDING PROBABILITY 
+    inline real DIV_weights(real numWeight, real denomWeight) const; //!DIVISION
+    inline real MULT_INVERSE_weight(real weight) const ;//!MULTIPLICATIVE INVERSE
+    inline real MULT_weights(real weight1, real weight2) const ; //!MULTIPLICATION
+    inline real SUM_weights(real weight1, real weight2) const ; //!SUM 
     
     //!update/compute the weight of a reconstruction candidate with
     //!the actual transformation parameters
-    real updateReconstructionWeight(int candidateIdx);
-    real computeReconstructionWeight(const ReconstructionCandidate & gc) const;
-    real computeReconstructionWeight(int targetIdx, 
-                                     int neighborIdx, 
-                                     int transformIdx) const;
-    real computeReconstructionWeight(const Vec & target,
-                                     int neighborIdx,
-                                     int transformIdx) const;
-
+    inline real updateReconstructionWeight(int candidateIdx);
+    inline real updateReconstructionWeight(int candidateIdx,
+                                           const Vec & target,
+                                           const Vec & neighbor,
+                                           int transformIdx,
+                                           Vec & predictedTarget);
+    inline real computeReconstructionWeight(const ReconstructionCandidate & gc) const;
+    inline real computeReconstructionWeight(int targetIdx, 
+                                            int neighborIdx, 
+                                            int transformIdx) const;
+    inline real computeReconstructionWeight(const Vec & target,
+                                            int neighborIdx,
+                                            int transformIdx) const;
+    inline real computeReconstructionWeight(const Vec & target,
+                                            const Vec & neighbor,
+                                            int transformIdx)const;
+    inline real computeReconstructionWeight(const Vec & target,
+                                            const Vec & neighbor,
+                                            int transformIdx,
+                                            Vec & predictedTarget)const;
+    
     //!applies "transformIdx"th transformation on data point "src"
-    void applyTransformationOn(int transformIdx, const Vec & src , Vec & dst) const ;
+    inline void applyTransformationOn(int transformIdx, const Vec & src , Vec & dst) const ;
 
     
     //! verify if the multinomial distribution given is well-defined
@@ -830,7 +842,7 @@
     
     //!returns the distance between the reconstruction and the target
     //!for the 'candidateIdx'th reconstruction candidate
-    real reconstructionEuclideanDistance(int candidateIdx);
+    inline real reconstructionEuclideanDistance(int candidateIdx);
     
     //increment the variable 'stage' of 1
     void nextStage();



From nouiz at mail.berlios.de  Thu Aug  9 21:33:26 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 9 Aug 2007 21:33:26 +0200
Subject: [Plearn-commits] r7975 - trunk/python_modules/plearn/parallel
Message-ID: <200708091933.l79JXQv5031523@sheep.berlios.de>

Author: nouiz
Date: 2007-08-09 21:33:25 +0200 (Thu, 09 Aug 2007)
New Revision: 7975

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
-added function task.set_scheduled_time()
-modif condor log filename


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 19:15:44 UTC (rev 7974)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 19:33:25 UTC (rev 7975)
@@ -209,6 +209,12 @@
             pass
         return None
 
+    def set_scheduled_time(self):
+        if self.dolog:
+            set_config_value(self.log_file, 'STATUS',str(STATUS_WAITING))
+            set_config_value(self.log_file, 'SCHEDULED_TIME',
+                             time.strftime(self.time_format, time.localtime(time.time())))
+            
     def get_waiting_time(self):
         # get the string representation
         str_sched = get_config_value(self.log_file,'SCHEDULED_TIME')
@@ -267,8 +273,7 @@
             return
 
         task.launch_time = time.time()
-        set_config_value(task.log_file, 'SCHEDULED_TIME',
-                time.strftime(self.time_format, time.localtime(time.time())))
+        task.set_scheduled_time()
 
         (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
 
@@ -377,9 +382,12 @@
         # Launch bqsubmit
         if not self.test:
             (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+            task.set_scheduled_time()
             self.p = Popen( 'bqsubmit', shell=True, stdout=output, stderr=error)
         else:
-            print "[DBI] in test mode, we generate all the file, but we do not execute bqsubmit"
+            print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
+            if self.dolog:
+                print "[DBI] The scheduling time will not be logged when you will submit the generated file" 
         os.chdir('parent')
 
         # Execute post-batchs
@@ -509,13 +517,13 @@
                 executable     = %s/launch.sh
                 universe       = vanilla
                 requirements   = %s
-                output         = %s/condor.%s.%s.$(Process).out
-                error          = %s/condor.%s.%s.$(Process).error
-                log            = %s/condor.%s.log
+                output         = %s/condor.%s.$(Process).out
+                error          = %s/condor.%s.$(Process).error
+                log            = %s/condor.log
                 ''' % (self.tmp_dir,req,
-                       self.log_dir,self.targetcondorplatform,self.unique_id,
-                       self.log_dir,self.targetcondorplatform,self.unique_id,
-                       self.log_dir,self.targetcondorplatform)))
+                       self.log_dir,self.unique_id,
+                       self.log_dir,self.unique_id,
+                       self.log_dir)))
 
         if len(condor_datas)!=0:
             for i in condor_datas:
@@ -580,9 +588,12 @@
         if self.test == False:
             (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
             print "Executing: condor_submit " + condor_file
+            task.set_scheduled_time()
             self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
         else:
             print "Created condor file: " + condor_file
+            if self.dolog:
+                print "[DBI] The scheduling time will not be logged when you will submit the condor file" 
             
     def clean(self):
         if len(self.temp_files)>0:
@@ -663,9 +674,9 @@
         print c
         if self.test:
             return
+        task.set_scheduled_time()
 
         (output,error)=get_redirection(task.log_file + '.out',task.log_file + '.err')
-#        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
 
         if self.nb_proc>1:
             self.sema.acquire()
@@ -806,8 +817,7 @@
             return
         
         task.launch_time = time.time()
-        set_config_value(task.log_file, 'SCHEDULED_TIME',
-                time.strftime(self.time_format, time.localtime(time.time())))
+        task.set_scheduled_time()
         
         (output,error)=get_redirection(task.log_file + '.out',task.log_file + '.err')
         



From lysiane at mail.berlios.de  Thu Aug  9 21:45:15 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Thu, 9 Aug 2007 21:45:15 +0200
Subject: [Plearn-commits] r7976 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200708091945.l79JjF19032291@sheep.berlios.de>

Author: lysiane
Date: 2007-08-09 21:45:15 +0200 (Thu, 09 Aug 2007)
New Revision: 7976

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
modifs to increase speed: in MStepNoiseVarianceMAP and reconstructionEuclideanDistance


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 19:33:25 UTC (rev 7975)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-09 19:45:15 UTC (rev 7976)
@@ -1784,17 +1784,15 @@
     PLASSERT(pq.empty()); 
   
     //capture the target from his index in the training set
-    Vec target;
-    target.resize(inputSpaceDim);
+    Vec target(inputSpaceDim);
     seeTrainingPoint(targetIdx, target);
     
     //for each potential neighbor,
     real dist;    
+    Vec neighbor(inputSpaceDim);
     for(int i=0; i<trainingSetLength; i++){
         if(i != targetIdx){ //(the target cannot be his own neighbor)
             //computes the distance to the target
-            Vec neighbor;
-            neighbor.resize(inputSpaceDim);
             seeTrainingPoint(i, neighbor);
             dist = powdistance(target, neighbor); 
             //if the distance is among "nbNeighbors" smallest distances seen,
@@ -2122,7 +2120,9 @@
     B_C.clear();
     
     real lambda = 1.0*noiseVariance/transformsVariance;
-    Vec v;
+    Vec v(inputSpaceDim);
+    Vec target(inputSpaceDim);
+    Vec neighbor(inputSpaceDim);
     for(int idx=0 ; idx<nbReconstructions ; idx++){
         
         //catch a view on the next entry of our dataset, that is, a  triple:
@@ -2131,16 +2131,12 @@
         real p = PROBA_weight(reconstructionSet[idx].weight);
   
         //catch the target and neighbor points from the training set
-        Vec target;
-        target.resize(inputSpaceDim);
+        
         seeTrainingPoint(reconstructionSet[idx].targetIdx, target);
-        Vec neighbor;
-        neighbor.resize(inputSpaceDim);
         seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
         
         int t = reconstructionSet[idx].transformIdx;
         
-        v.resize(inputSpaceDim);
         v << target;
         if(transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT){
             v = v - neighbor;
@@ -2210,25 +2206,27 @@
 //!maximization step with respect to transformation bias
 //!(MAP version)
 void TransformationLearner::MStepBias(){
-    Mat newBiasSet;
-    newBiasSet.resize(nbTransforms, inputSpaceDim);
-    Vec norms;
-    norms.resize(nbTransforms);
+    Mat newBiasSet(nbTransforms, inputSpaceDim);
+    for(int i=0;i<nbTransforms;i++){
+        for(int j=0; j<inputSpaceDim; j++){
+            newBiasSet[i][j]=0;
+        }
+    }
+    Vec norms(nbTransforms);
     for(int t=0;t<nbTransforms;t++){
         norms[t] = INIT_weight(0);
     }
     int transformIdx;
-    Vec target,neighbor,reconstruction;
+    Vec target(inputSpaceDim);
+    Vec neighbor(inputSpaceDim);
+    Vec reconstruction(inputSpaceDim);
     real proba,weight;
     for(int idx=0; idx<nbReconstructions; idx++){
         transformIdx = reconstructionSet[idx].transformIdx;
         weight = reconstructionSet[idx].weight;
         proba = PROBA_weight(weight);
-        target.resize(inputSpaceDim);
         seeTrainingPoint(reconstructionSet[idx].targetIdx,target);
-        neighbor.resize(inputSpaceDim);
         seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
-        reconstruction.resize(inputSpaceDim);
         applyTransformationOn(transformIdx,neighbor, reconstruction);
         newBiasSet(transformIdx) += proba*(target - reconstruction);
         norms[transformIdx] = SUM_weights(norms[transformIdx],weight);
@@ -2254,37 +2252,61 @@
 void TransformationLearner::MStepNoiseVarianceMAP(real alpha, real beta)
 {
     
-    Vec total_k;
-    total_k.resize(nbTransforms);
+    Vec total_k(nbTransforms);
+    for(int i=0;i<nbTransforms; i++){
+        total_k[i]=0;
+    }
     int transformIdx;
     real proba;
-    for(int idx=0; idx < nbReconstructions; idx++){
-        transformIdx = reconstructionSet[idx].transformIdx;
-        proba = PROBA_weight(reconstructionSet[idx].weight);
-        total_k[transformIdx]+=(proba * reconstructionEuclideanDistance(idx));
+    Vec target(inputSpaceDim);
+    Vec neighbor(inputSpaceDim);
+    Vec reconstruction(inputSpaceDim);
+    int candidateIdx=0;
+    for(int targetIdx=0; targetIdx<trainingSetLength; targetIdx ++){
+        seeTrainingPoint(targetIdx,target);
+        for(int idx=0; idx < nbTargetReconstructions; idx++){
+            transformIdx = reconstructionSet[candidateIdx].transformIdx;
+            seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx , neighbor);
+            proba = PROBA_weight(reconstructionSet[candidateIdx].weight);
+            total_k[transformIdx]+=(proba * reconstructionEuclideanDistance(target,
+                                                                            neighbor,
+                                                                            transformIdx,
+                                                                            reconstruction));
+            candidateIdx ++;
+        }
+    noiseVariance = (2*beta + sum(total_k))/(2*alpha - 2 + trainingSetLength*inputSpaceDim);  
     }
-    noiseVariance = (2*beta + sum(total_k))/(2*alpha - 2 + trainingSetLength*inputSpaceDim);  
 }
  
 //!returns the distance between the reconstruction and the target
 //!for the 'candidateIdx'th reconstruction candidate
 real TransformationLearner::reconstructionEuclideanDistance(int candidateIdx){
-    Vec target;
-    target.resize(inputSpaceDim);
+    Vec target(inputSpaceDim);
     seeTrainingPoint(reconstructionSet[candidateIdx].targetIdx, target);
-    Vec neighbor;
-    neighbor.resize(inputSpaceDim);
+    Vec neighbor(inputSpaceDim);
     seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx,
-                neighbor);
-    Vec reconstruction;
-    reconstruction.resize(inputSpaceDim);
+                     neighbor);
+    Vec reconstruction(inputSpaceDim);
     applyTransformationOn(reconstructionSet[candidateIdx].transformIdx,
                           neighbor,
                           reconstruction);
     return powdistance(target, reconstruction);
 }
 
+real TransformationLearner::reconstructionEuclideanDistance(const Vec& target,
+                                                            const Vec& neighbor,
+                                                            int transformIdx,
+                                                            Vec& reconstruction)const
+{
+    applyTransformationOn(transformIdx,
+                          neighbor,
+                          reconstruction);
+    return powdistance(target,reconstruction);
 
+}
+
+
+
 //!increments the variable 'stage' of 1 
 void TransformationLearner::nextStage(){
     stage ++;

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 19:33:25 UTC (rev 7975)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-09 19:45:15 UTC (rev 7976)
@@ -843,7 +843,11 @@
     //!returns the distance between the reconstruction and the target
     //!for the 'candidateIdx'th reconstruction candidate
     inline real reconstructionEuclideanDistance(int candidateIdx);
-    
+    inline real reconstructionEuclideanDistance(const Vec & target,
+                                                const Vec & neighbor,
+                                                int transformIdx,
+                                                Vec & reconstruction)const;
+
     //increment the variable 'stage' of 1
     void nextStage();
 



From saintmlx at mail.berlios.de  Thu Aug  9 22:54:37 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 9 Aug 2007 22:54:37 +0200
Subject: [Plearn-commits] r7977 - in trunk: . plearn/io
Message-ID: <200708092054.l79Ksbjb004035@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-09 22:54:37 +0200 (Thu, 09 Aug 2007)
New Revision: 7977

Modified:
   trunk/plearn/io/PStream.h
   trunk/pymake.config.model
Log:
- fixed compilation for DIRO / gcc4



Modified: trunk/plearn/io/PStream.h
===================================================================
--- trunk/plearn/io/PStream.h	2007-08-09 19:45:15 UTC (rev 7976)
+++ trunk/plearn/io/PStream.h	2007-08-09 20:54:37 UTC (rev 7977)
@@ -54,6 +54,14 @@
 
 using namespace std;
 
+
+// for readBinaryNumAs
+template<bool x> struct chkUnsigned 
+{ template<class I> static bool notOk(I& y) { return false; } };
+template<> struct chkUnsigned<true> 
+{ template<class I> static bool notOk(I& y) { return y<0; } };
+
+
 /*!
  * PStream:
  *  This class defines a type of stream that should be used for all I/O within
@@ -304,10 +312,6 @@
 
     //! Writes base types to PLearn binary format
     //! TODO: forbid this mechanism for arbitrary classes?
-    template<bool x> struct chkUnsigned 
-    { template<class I> static bool notOk(I& y) { return false; } };
-    struct chkUnsigned<true> 
-    { template<class I> static bool notOk(I& y) { return y<0; } };
     template<class I>
     void writeBinaryNum(I x)
     {

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-09 19:45:15 UTC (rev 7976)
+++ trunk/pymake.config.model	2007-08-09 20:54:37 UTC (rev 7977)
@@ -349,6 +349,7 @@
 
         numpy_includedirs   = python_includedirs
 
+    python_includedirs= numpy_includedirs
 
     if python_version != pyver:
         print '*** WARNING: python version mismatch:'



From nouiz at mail.berlios.de  Thu Aug  9 23:01:54 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 9 Aug 2007 23:01:54 +0200
Subject: [Plearn-commits] r7978 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200708092101.l79L1sws004690@sheep.berlios.de>

Author: nouiz
Date: 2007-08-09 23:01:53 +0200 (Thu, 09 Aug 2007)
New Revision: 7978

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
bugfix, and changed all data printed on the stdout to begin by [DBI] so we know the source easily


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 20:54:37 UTC (rev 7977)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-09 21:01:53 UTC (rev 7978)
@@ -87,8 +87,10 @@
             self.post_batch = [self.post_batch]
 
     def n_avail_machines(self): raise NotImplementedError, "DBIBase.n_avail_machines()"
+    
     def add_commands(self,commands): raise NotImplementedError, "DBIBase.add_commands()"
-    def get_redirection(stdout_file,stderr_file):
+    
+    def get_redirection(self,stdout_file,stderr_file):
         """Calcule the needed redirection based of the objects attribute
         return a tuple (stdout,stderr) that can be used with popen
         """
@@ -100,12 +102,13 @@
             error = STDOUT
         elif int(self.file_redirect_stderr):
             error = file(stderr_file, 'w')
+        return (output,error)
             
     def exec_pre_batch(self):
         # Execute pre-batch
         pre_batch_command = ';'.join( self.pre_batch )
 
-        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+        (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
             
         if not self.test:
             self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
@@ -116,7 +119,7 @@
         # Execute post-batch
         post_batch_command = ';'.join( self.post_batch )
         
-        output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+        (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
         
         if not self.test:
             self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
@@ -246,7 +249,11 @@
 
     def __init__(self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
+        add_commands(commands)
 
+    def add_commands(self,commands):
+        if not isinstance(commands, list):
+            commands=[commands]
 
         # create the information about the tasks
         for command in commands:
@@ -267,7 +274,7 @@
             command += "--typecpu all"
         command += " '"+string.join(task.commands,';') + "'"
         
-        print command
+        print "[DBI] "+command
 
         if self.test:
             return
@@ -275,14 +282,14 @@
         task.launch_time = time.time()
         task.set_scheduled_time()
 
-        (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+        (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
 
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):
-        print "The Log file are under %s"%self.log_dir
+        print "[DBI] The Log file are under %s"%self.log_dir
         if self.test:
-            print "Test mode, we only print the command to be executed, we don't execute them"
+            print "[DBI] Test mode, we only print the command to be executed, we don't execute them"
         # Execute pre-batch
         if len(self.pre_batch)>0:
             exec_pre_batch()
@@ -295,7 +302,7 @@
         if len(self.post_batch)>0:
             exec_post_batch()
 
-        print "The Log file are under %s"%self.log_dir
+        print "[DBI]The Log file are under %s"%self.log_dir
 
     def clean(self):
         #TODO: delete all log files for the current batch
@@ -317,13 +324,19 @@
 
         # create the information about the tasks
         args['temp_dir'] = self.temp_dir
+        
+        add_commands(commands)
+
+    def add_commands(self,commands):
+        if not isinstance(commands, list):
+            commands=[commands]
+
+        # create the information about the tasks
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format,
                                    [self.pre_tasks, 'cd parent;'],
                                    self.post_tasks,self.dolog,False,args))
-
-
     def run(self):
         pre_batch_command = ';'.join( self.pre_batch );
         post_batch_command = ';'.join( self.post_batch );
@@ -381,7 +394,7 @@
 
         # Launch bqsubmit
         if not self.test:
-            (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
+            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
             task.set_scheduled_time()
             self.p = Popen( 'bqsubmit', shell=True, stdout=output, stderr=error)
         else:
@@ -404,9 +417,14 @@
         
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
-            
+
+        add_commands(commands)
+
+    def add_commands(self,commands):
+        if not isinstance(commands, list):
+            commands=[commands]
+
         # create the information about the tasks
-#        args['temp_dir'] = self.temp_dir
         for command in commands:
             pos = string.find(command,' ')
             if pos>=0:
@@ -537,13 +555,13 @@
         dbi_file=get_plearndir()+'/python_modules/plearn/parallel/dbi.py'
         overwrite_launch_file=False
         if not os.path.exists(dbi_file):
-            print 'WARNING: Can\' locate dbi.py file. Meaby the file "'+launch_file+'" is not up to date!'
+            print '[DBI] WARNING: Can\' locate dbi.py file. Meaby the file "'+launch_file+'" is not up to date!'
         else:
             if os.path.exists(launch_file):
                 mtimed=os.stat(dbi_file)[8]
                 mtimel=os.stat(launch_file)[8]
                 if mtimed>mtimel:
-                    print 'WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your need!'
+                    print '[DBI] WARNING: We overwrite the file "'+launch_file+'" with a new version. Update it to your need!'
                     overwrite_launch_file=True
         
         if not os.path.exists(launch_file) or overwrite_launch_file:
@@ -586,12 +604,12 @@
 
         # Launch condor
         if self.test == False:
-            (output,error)=get_redirection(self.log_file + '.out',self.log_file + '.err')
-            print "Executing: condor_submit " + condor_file
+            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
+            print "[DBI] Executing: condor_submit " + condor_file
             task.set_scheduled_time()
             self.p = Popen( 'condor_submit '+ condor_file, shell=True , stdout=output, stderr=error)
         else:
-            print "Created condor file: " + condor_file
+            print "[DBI] Created condor file: " + condor_file
             if self.dolog:
                 print "[DBI] The scheduling time will not be logged when you will submit the condor file" 
             
@@ -607,7 +625,7 @@
 
 
     def run(self):
-        print "The Log file are under %s"%self.log_dir
+        print "[DBI] The Log file are under %s"%self.log_dir
 
         if len(self.pre_batch)>0:
             exec_pre_batch()
@@ -634,7 +652,8 @@
             
     def add_commands(self,commands):
         if not isinstance(commands, list):
-            self.post_batch = [self.post_batch]
+            commands=[commands]
+
         #We copy the variable localy as an optimisation for big list of commands
         #save around 15% with 100 commands
         tmp_dir=self.tmp_dir
@@ -671,12 +690,12 @@
 
     def run_one_job(self,task):
         c = (';'.join(task.commands))
-        print c
+        print "[DBI] "+c
         if self.test:
             return
         task.set_scheduled_time()
 
-        (output,error)=get_redirection(task.log_file + '.out',task.log_file + '.err')
+        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
 
         if self.nb_proc>1:
             self.sema.acquire()
@@ -685,6 +704,7 @@
             self.sema.release()
         else:
             p = Popen(c, shell=True,stdout=output,stderr=error)
+            p.wait()
             
     def clean(self):
         if len(self.temp_files)>0:
@@ -703,7 +723,7 @@
             print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
         if not self.file_redirect_stderr and self.nb_proc>1:
             print "[DBI] WARNING: many process but all their stderr are redirected to the parent"
-        print "The Log file are under %s"%self.log_dir
+        print "[DBI] The Log file are under %s"%self.log_dir
 
         # Execute pre-batch
         if len(self.pre_batch)>0:
@@ -726,7 +746,7 @@
         if len(self.post_batch)>0:
             exec_post_batch()
             
-        print "The Log file are under %s"%self.log_dir
+        print "[DBI] The Log file are under %s"%self.log_dir
         
     def clean(self):
         pass
@@ -784,18 +804,23 @@
 class DBISsh(DBIBase):
 
     def __init__(self, commands, **args ):
-        print "WARNING: The SSH DBI is not fully implemented!"
-        print "Use at your own risk!"
+        print "[DBI] WARNING: The SSH DBI is not fully implemented!"
+        print "[DBI] Use at your own risk!"
         DBIBase.__init__(self, commands, **args)
 
+        add_commands(commands)
+        self.hosts= find_all_ssh_hosts()
+        
+    def add_commands(self,commands):
+        if not isinstance(commands, list):
+            commands=[commands]
+            
         # create the information about the tasks
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
                                    self.time_format, self.pre_tasks,
                                    self.post_tasks,self.dolog,False))
-        self.hosts= find_all_ssh_hosts()
-        
-
+            
     def getHost(self):
         self.hosts.sort(cmp= cmp_ssh_hosts)
         #print "hosts= "
@@ -811,7 +836,7 @@
 
         cwd= os.getcwd()
         command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
-        print command
+        print "[DBI]"+command
 
         if self.test:
             return
@@ -819,19 +844,19 @@
         task.launch_time = time.time()
         task.set_scheduled_time()
         
-        (output,error)=get_redirection(task.log_file + '.out',task.log_file + '.err')
+        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
         
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
     def run(self):
-        print "The Log file are under %s"%self.log_dir
+        print "[DBI] The Log file are under %s"%self.log_dir
 
         # Execute pre-batch
         if len(self.pre_batch)>0:
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        print "tasks= ", self.tasks
+        print "[DBI] tasks= ", self.tasks
         for task in self.tasks:
             self.run_one_job(task)
 

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-08-09 20:54:37 UTC (rev 7977)
+++ trunk/scripts/cdispatch	2007-08-09 21:01:53 UTC (rev 7978)
@@ -174,7 +174,7 @@
     
 tmp="_".join(sys.argv)
 tmp=tmp[tmp.find("cdispatch"):]
-re.sub( '[^a-zA-Z0-9]', '_', tmp )
+tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
 tmp+=str(datetime.datetime.now()).replace(' ','_')
 print tmp
 dbi_param["log_dir"]=os.path.join("LOGS",tmp)
@@ -195,7 +195,10 @@
         SCRIPT.write("   '%s',\n"%cmdstr)
     SCRIPT.write("   ],'%s'"%(launch_cmd))
     for key in dbi_param.keys():
-        SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
+        if isinstance(dbi_param[key],str):
+            SCRIPT.write(","+str(key)+"='"+str(dbi_param[key])+"'")
+        else:
+            SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
     SCRIPT.write(
 """)
 jobs.run()



From yoshua at mail.berlios.de  Fri Aug 10 05:18:02 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 10 Aug 2007 05:18:02 +0200
Subject: [Plearn-commits] r7979 - trunk/scripts
Message-ID: <200708100318.l7A3I2bC008136@sheep.berlios.de>

Author: yoshua
Date: 2007-08-10 05:18:00 +0200 (Fri, 10 Aug 2007)
New Revision: 7979

Modified:
   trunk/scripts/collectres
Log:
Trying to fix collectres to work with numpy, but still not working


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-08-09 21:01:53 UTC (rev 7978)
+++ trunk/scripts/collectres	2007-08-10 03:18:00 UTC (rev 7979)
@@ -54,7 +54,7 @@
 
 import sys,string
 from plearn.vmat.PMat import *
-from numarray import *
+from numpy import *
 
 # should probably be an option
 separator = "_"
@@ -99,7 +99,8 @@
         la = a.shape[0]
         if la<maxrow:
             b[la-minrow:] = float('NaN')
-      res=concatenate([res,b],1)
+      if b:
+        res=concatenate([res,b],1)
       if i==5:
         i=6
       else:
@@ -205,7 +206,13 @@
       arrays.append(res[0])
     # extend the shorter arrays with NaNs
     maxlen = max([a.shape[0] for a in arrays])
-    arrays = [concatenate([a,resize(array([[float("NaN")]]),(maxlen-a.shape[0],a.shape[1]))],0) for a in arrays]
+    alist=[]
+    for a in arrays:
+      if maxlen>a.shape[0]:
+        b=resize(array([[float("NaN")]]),(maxlen-a.shape[0],a.shape[1]))
+        alist.append(concatenate([a,b],0))
+      else:
+        alist.append(a)
     a = concatenate(arrays,1)
     # write the array to file, without any []
     write_array(f,a)



From yoshua at mail.berlios.de  Fri Aug 10 05:19:41 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 10 Aug 2007 05:19:41 +0200
Subject: [Plearn-commits] r7980 - trunk/commands
Message-ID: <200708100319.l7A3Jfwt008267@sheep.berlios.de>

Author: yoshua
Date: 2007-08-10 05:19:41 +0200 (Fri, 10 Aug 2007)
New Revision: 7980

Modified:
   trunk/commands/plearn_light_inc.h
   trunk/commands/plearn_noblas_inc.h
Log:
Added online stuff...


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-08-10 03:18:00 UTC (rev 7979)
+++ trunk/commands/plearn_light_inc.h	2007-08-10 03:19:41 UTC (rev 7980)
@@ -197,6 +197,7 @@
 
 // Online
 #include <plearn_learners/online/BackConvolution2DModule.h>
+#include <plearn_learners/online/BinarizeModule.h>
 #include <plearn_learners/online/CombiningCostsModule.h>
 #include <plearn_learners/online/Convolution2DModule.h>
 #include <plearn_learners/online/CostModule.h>

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-08-10 03:18:00 UTC (rev 7979)
+++ trunk/commands/plearn_noblas_inc.h	2007-08-10 03:19:41 UTC (rev 7980)
@@ -233,6 +233,7 @@
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SplitModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
+#include <plearn_learners/online/BinarizeModule.h>
 #include <plearn_learners/online/StackedAutoassociatorsNet.h>
 #include <plearn_learners/online/Subsampling2DModule.h>
 #include <plearn_learners/online/Supersampling2DModule.h>



From yoshua at mail.berlios.de  Fri Aug 10 13:23:30 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Fri, 10 Aug 2007 13:23:30 +0200
Subject: [Plearn-commits] r7981 - trunk/plearn_learners/online
Message-ID: <200708101123.l7ABNUtg001612@sheep.berlios.de>

Author: yoshua
Date: 2007-08-10 13:23:29 +0200 (Fri, 10 Aug 2007)
New Revision: 7981

Added:
   trunk/plearn_learners/online/BinarizeModule.cc
   trunk/plearn_learners/online/BinarizeModule.h
Log:
Adding new OnlineLearningModule that binarizes deterministically
or stochastically a continuous input.


Added: trunk/plearn_learners/online/BinarizeModule.cc
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.cc	2007-08-10 03:19:41 UTC (rev 7980)
+++ trunk/plearn_learners/online/BinarizeModule.cc	2007-08-10 11:23:29 UTC (rev 7981)
@@ -0,0 +1,244 @@
+// -*- C++ -*-
+
+// BinarizeModule.cc
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file BinarizeModule.cc */
+
+
+
+#include "BinarizeModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    BinarizeModule,
+    "Map probabilities in (0,1) to a bit in {0,1}, by sampling or hard threshold\n",
+    "Map input probabilities in (0,1) to a bit in {0,1}, either according to\n"
+    "a hard threshold (> 0.5), or by sampling, and ALLOW GRADIENTS\n"
+    "TO PROPAGATE BACKWARDS. The heuristic for gradient propagation is the following:\n"
+    "  If the output is incorrect (sign of gradient pushes towards the other value)\n"
+    "    then propagate gradient as is,\n"
+    "    else do not propagate any gradient.\n"
+);
+
+//////////////////
+// BinarizeModule //
+//////////////////
+BinarizeModule::BinarizeModule()
+    : stochastic(true)
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void BinarizeModule::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "stochastic", &BinarizeModule::stochastic,
+                  OptionBase::buildoption,
+                  "If true then sample the output bits stochastically, else use a hard threshold.\n");
+
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void BinarizeModule::build_()
+{
+}
+
+///////////
+// build //
+///////////
+void BinarizeModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void BinarizeModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                    const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() && ports_gradient.length() == nPorts());
+
+    Mat* output = ports_value[1];
+    Mat* input_gradient = ports_gradient[0];
+    Mat* output_gradient = ports_gradient[1];
+    
+    int mbs=output->length();
+    input_gradient->resize(mbs,output->width());
+    for (int t=0;t<mbs;t++)
+    {
+        real* yt = (*output)[t];
+        real* dyt = (*output_gradient)[t];
+        real* dxt = (*input_gradient)[t];
+        for (int i=0;i<output->width();i++)
+            if ((yt[i]-0.5)*dyt[i] > 0)
+                dxt[i] += dyt[i];
+    }
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+// the default implementation returns false
+bool BinarizeModule::bpropDoesNothing()
+{
+}
+*/
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void BinarizeModule::finalize()
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void BinarizeModule::forget()
+{
+}
+
+///////////
+// fprop //
+///////////
+void BinarizeModule::fprop(const TVec<Mat*>& ports_value)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    // check which ports are input (ports_value[i] && !ports_value[i]->isEmpty())
+    // which ports are output (ports_value[i] && ports_value[i]->isEmpty())
+    // and which ports are ignored (!ports_value[i]).
+    // If that combination of (input,output,ignored) is feasible by this class
+    // then perform the corresponding computation. Otherwise launch the error below.
+    // See the comment in the header file for more information.
+
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    Mat* input = ports_value[0];
+    Mat* output = ports_value[1];
+    int mbs=input->length();
+    output->resize(mbs,input->width());
+    for (int t=0;t<mbs;t++)
+    {
+        real* xt = (*input)[t];
+        real* yt = (*output)[t];
+        if (stochastic)
+            for (int i=0;i<input->width();i++)
+                yt[i]=random_gen->binomial_sample(xt[i]);
+        else
+            for (int i=0;i<input->width();i++)
+                yt[i]=xt[i]>=0.5?1:0;
+    }
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+/* Optional
+int BinarizeModule::getPortIndex(const string& port)
+{}
+*/
+
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& BinarizeModule::getPorts() {
+    return inherited::getPorts();
+}
+
+//////////////////
+// getPortSizes //
+//////////////////
+const TMat<int>& BinarizeModule::getPortSizes() {
+    port_sizes.resize(2,2);
+    port_sizes.fill(-1);
+    return port_sizes;
+}
+
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void BinarizeModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("BinarizeModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+// The default implementation raises a warning and does not do anything.
+void BinarizeModule::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
+
+}
+// end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/BinarizeModule.h
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.h	2007-08-10 03:19:41 UTC (rev 7980)
+++ trunk/plearn_learners/online/BinarizeModule.h	2007-08-10 11:23:29 UTC (rev 7981)
@@ -0,0 +1,300 @@
+// -*- C++ -*-
+
+// BinarizeModule.h
+//
+// Copyright (C) 2007 Yoshua Bengio
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Yoshua Bengio
+
+/*! \file BinarizeModule.h */
+
+
+#ifndef BinarizeModule_INC
+#define BinarizeModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+namespace PLearn {
+
+/**
+ * Map probabilities in (0,1) to a bit in {0,1}, either according to
+ * a hard threshold (> 0.5), or by sampling, and ALLOW GRADIENTS
+ * TO PROPAGATE BACKWARDS.
+ *
+ */
+class BinarizeModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! If true then sample the output bits stochastically, else use a hard threshold.
+    bool stochastic;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    BinarizeModule();
+
+    // Your other public member functions go here
+
+    //! Perform a fprop step.
+    //! Each Mat* pointer in the 'ports_value' vector can be one of:
+    //! - a full matrix: this is data that is provided to the module, and can
+    //!                  be used to compute other ports' values
+    //! - an empty matrix: this is what we want to compute
+    //! - a NULL pointer: this is data that is not available, but whose value
+    //!                   does not need to be returned (or even computed)
+    //! The default version will either:
+    //! - call the mini-batch versions of standard fprop if 'ports_value' has
+    //!   size 2, with the first value being provided (and the second being
+    //!   the desired output)
+    //! - crash otherwise
+    void fprop(const TVec<Mat*>& ports_value);
+
+    //! Perform a back propagation step (also updating parameters according to
+    //! the provided gradient).
+    //! The matrices in 'ports_value' must be the same as the ones given in a
+    //! previous call to 'fprop' (and thus they should in particular contain
+    //! the result of the fprop computation). However, they are not necessarily
+    //! the same as the ones given in the LAST call to 'fprop': if there is a
+    //! need to store an internal module state, this should be done using a
+    //! specific port to store this state.
+    //! Each Mat* pointer in the 'ports_gradient' vector can be one of:
+    //! - a full matrix  : this is the gradient that is provided to the module,
+    //!                    and can be used to compute other ports' gradient.
+    //! - an empty matrix: this is a gradient we want to compute and accumulate
+    //!                    into. This matrix must have length 0 and a width
+    //!                    equal to the width of the corresponding matrix in
+    //!                    the 'ports_value' vector (we can thus accumulate
+    //!                    gradients using PLearn's ability to keep intact
+    //!                    stored values when resizing a matrix' length).
+    //! - a NULL pointer : this is a gradient that is not available, but does
+    //!                    not need to be returned (or even computed).
+    //! The default version tries to use the standard mini-batch bpropUpdate
+    //! method, when possible.
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    /* Optional
+
+    //! Given the input, compute the output (possibly resize it appropriately)
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Given a batch of inputs, compute the outputs
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Mat& inputs, Mat& outputs);
+    */
+
+    /* Optional
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! Batch version
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    //! Batch version
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             const Mat& output_gradients);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    /* Optional
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
+    //! Return the list of ports in the module.
+    //! The default implementation returns a pair ("input", "output") to handle
+    //! the most common case.
+    virtual const TVec<string>& getPorts();
+
+    //! Return the size of all ports, in the form of a two-column matrix, where
+    //! each row represents a port, and the two numbers on a row are
+    //! respectively its length and its width (with -1 representing an
+    //! undefined or variable value).
+    //! The default value fills this matrix with:
+    //!     - in the first column (lengths): -1
+    //!     - in the second column (widths):
+    //!         - -1 if nPorts() < 2
+    //!         - 'input_size' for the first row and 'output_size' for the
+    //!           second row if nPorts() >= 2
+    virtual const TMat<int>& getPortSizes();
+
+    /* Optional
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    //  ### Default implementation performs a simple linear search in
+    //  ### getPorts().
+    virtual int getPortIndex(const string& port);
+    */
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(BinarizeModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(BinarizeModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From yoshua at mail.berlios.de  Sat Aug 11 00:55:59 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sat, 11 Aug 2007 00:55:59 +0200
Subject: [Plearn-commits] r7982 - trunk/plearn_learners/online
Message-ID: <200708102255.l7AMtxQh028610@sheep.berlios.de>

Author: yoshua
Date: 2007-08-11 00:55:47 +0200 (Sat, 11 Aug 2007)
New Revision: 7982

Modified:
   trunk/plearn_learners/online/BinarizeModule.cc
   trunk/plearn_learners/online/BinarizeModule.h
   trunk/plearn_learners/online/VBoundDBN2.cc
Log:
Fixed little problem in VBoundDBN2 and added options in BinarizeModule


Modified: trunk/plearn_learners/online/BinarizeModule.cc
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.cc	2007-08-10 11:23:29 UTC (rev 7981)
+++ trunk/plearn_learners/online/BinarizeModule.cc	2007-08-10 22:55:47 UTC (rev 7982)
@@ -58,7 +58,7 @@
 // BinarizeModule //
 //////////////////
 BinarizeModule::BinarizeModule()
-    : stochastic(true)
+    : stochastic(true),copy_gradients(false),saturate_gradients(false)
 {
 }
 
@@ -71,6 +71,14 @@
                   OptionBase::buildoption,
                   "If true then sample the output bits stochastically, else use a hard threshold.\n");
 
+    declareOption(ol, "copy_gradients", &BinarizeModule::copy_gradients,
+                  OptionBase::buildoption,
+                  "If true then simply copy the gradients through with no alteration.\n");
+
+    declareOption(ol, "saturate_gradients", &BinarizeModule::saturate_gradients,
+                  OptionBase::buildoption,
+                  "If true then multiply output gradients by p(1-p) for input probability p.\n");
+
     inherited::declareOptions(ol);
 }
 
@@ -98,20 +106,31 @@
 {
     PLASSERT( ports_value.length() == nPorts() && ports_gradient.length() == nPorts());
 
+    Mat* input = ports_value[0];
     Mat* output = ports_value[1];
     Mat* input_gradient = ports_gradient[0];
     Mat* output_gradient = ports_gradient[1];
     
     int mbs=output->length();
-    input_gradient->resize(mbs,output->width());
-    for (int t=0;t<mbs;t++)
+    if (input_gradient)
     {
-        real* yt = (*output)[t];
-        real* dyt = (*output_gradient)[t];
-        real* dxt = (*input_gradient)[t];
-        for (int i=0;i<output->width();i++)
-            if ((yt[i]-0.5)*dyt[i] > 0)
-                dxt[i] += dyt[i];
+        input_gradient->resize(mbs,output->width());
+        for (int t=0;t<mbs;t++)
+        {
+            real* yt = (*output)[t];
+            real* dyt = (*output_gradient)[t];
+            real* dxt = (*input_gradient)[t];
+            real* xt = (*input)[t];
+            if (copy_gradients)
+                for (int i=0;i<output->width();i++)
+                    dxt[i] += dyt[i];
+            else if (saturate_gradients)
+                for (int i=0;i<output->width();i++)
+                    dxt[i] += dyt[i]*xt[i]*(1-xt[i]);
+            else for (int i=0;i<output->width();i++)
+                if ((yt[i]-0.5)*dyt[i] > 0)
+                    dxt[i] += dyt[i];
+        }
     }
 }
 

Modified: trunk/plearn_learners/online/BinarizeModule.h
===================================================================
--- trunk/plearn_learners/online/BinarizeModule.h	2007-08-10 11:23:29 UTC (rev 7981)
+++ trunk/plearn_learners/online/BinarizeModule.h	2007-08-10 22:55:47 UTC (rev 7982)
@@ -63,6 +63,12 @@
     //! If true then sample the output bits stochastically, else use a hard threshold.
     bool stochastic;
 
+    //! If true just pass the gradient straight through with no alteration
+    bool copy_gradients;
+
+    //! If true then multiply output gradients by p(1-p) for input probability p
+    bool saturate_gradients;
+
 public:
     //#####  Public Member Functions  #########################################
 

Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-10 11:23:29 UTC (rev 7981)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-10 22:55:47 UTC (rev 7982)
@@ -267,6 +267,7 @@
         *ph_given_v << rbm1->hidden_layer->getExpectations();
         *sampled_h << rbm1->hidden_layer->samples;
         rbm1->computeFreeEnergyOfVisible(*input,FE1v,false);
+        rbm1->computeFreeEnergyOfHidden(*sampled_h,FE1h);
         rbm2->computeFreeEnergyOfVisible(*sampled_h,FE2h,false);
         p2ph->resize(mbs,rbm2->hidden_layer->size);
         *p2ph << rbm2->hidden_layer->getExpectations();



From tihocan at mail.berlios.de  Sat Aug 11 06:24:21 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 11 Aug 2007 06:24:21 +0200
Subject: [Plearn-commits] r7983 - trunk/plearn_learners/online
Message-ID: <200708110424.l7B4OLhm028025@sheep.berlios.de>

Author: tihocan
Date: 2007-08-11 06:24:20 +0200 (Sat, 11 Aug 2007)
New Revision: 7983

Modified:
   trunk/plearn_learners/online/VBoundDBN2.cc
Log:
Added missing log partition function when computing the NLL in fprop

Modified: trunk/plearn_learners/online/VBoundDBN2.cc
===================================================================
--- trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-10 22:55:47 UTC (rev 7982)
+++ trunk/plearn_learners/online/VBoundDBN2.cc	2007-08-11 04:24:20 UTC (rev 7983)
@@ -170,8 +170,8 @@
 
     // compute RBM1 weight negative gradient
     //  dlogbound/dWij sampling approx = (ph_given_v[i] + (h[i]-ph_given_v[i])*global_improvement)*v[j] - h[i]*reconstructed_v[j]
-    substract(*sampled_h_,*ph_given_v_,delta_h);
-    multiply(delta_h, delta_h,global_improvement_->toVec());
+    substract(*sampled_h_, *ph_given_v_, delta_h);
+    multiply(delta_h, delta_h, global_improvement_->toVec());
     delta_h += *ph_given_v_;
     productScaleAcc(delta_W, delta_h, true, *input, false, 1., 0.);
     productScaleAcc(delta_W, *sampled_h_, true, reconstructed_v, false, -1., 1.);
@@ -286,6 +286,7 @@
         }
         if (nll) // exact -log P(input) = - log sum_h P2(h) P1(input|h)
         {
+            PLASSERT( nll->isEmpty() );
             int n_h_configurations = 1 << rbm1->hidden_layer->size;
             if (all_h.length()!=n_h_configurations || all_h.width()!=rbm1->hidden_layer->size)
             {
@@ -303,8 +304,25 @@
             // compute -log P2(h) for each possible h configuration
             if (rbm2->partition_function_is_stale && !during_training)
                 rbm2->computePartitionFunction();
-            neglogP2h.resize(n_h_configurations,1);
-            rbm2->computeFreeEnergyOfVisible(all_h,neglogP2h,false);
+            neglogP2h.resize(n_h_configurations, 1);
+            rbm2->computeFreeEnergyOfVisible(all_h, neglogP2h, false);
+            neglogP2h += rbm2->log_partition_function;
+            /*
+            if (!during_training) {
+                // Debug code to ensure probabilities sum to 1.
+                real check = 0;
+                real check2 = 0;
+                for (int c = 0; c < n_h_configurations; c++) {
+                    check2 += exp(- neglogP2h(c, 0));
+                    if (c == 0)
+                        check = - neglogP2h(c, 0);
+                    else
+                        check = logadd(check, - neglogP2h(c, 0));
+                }
+                pout << check << endl;
+                pout << check2 << endl;
+            }
+            */
             rbm1->computeNegLogPVisibleGivenPHidden(*input,all_h,&neglogP2h,*nll);
         }
     }



From tihocan at mail.berlios.de  Sat Aug 11 08:55:23 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 11 Aug 2007 08:55:23 +0200
Subject: [Plearn-commits] r7984 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200708110655.l7B6tNSQ017402@sheep.berlios.de>

Author: tihocan
Date: 2007-08-11 08:55:23 +0200 (Sat, 11 Aug 2007)
New Revision: 7984

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
Log:
More explicit error messages, and temporarily disabled the decrease constant since it crashes when there are too many stages

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-11 04:24:20 UTC (rev 7983)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-11 06:55:23 UTC (rev 7984)
@@ -722,8 +722,11 @@
                     " value (errno = %d)", errno);
     }
     semun_v.val = stage;
+    semun_v.val = 0; // TODO Fix (pb with max semaphore value)
     int success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
-    PLCHECK( success == 0 );
+    if (success != 0)
+        PLERROR("In NatGradSMPNNet::train - Could not initialize semaphore"
+                " value to store the stage (errno = %d)", errno);
 
     // Fork one process/cpu.
     int iam = 0;
@@ -778,15 +781,26 @@
                     all_params += params_update;
                     params_update.clear();
                 }
-                sem_value = (sem_value + 1) % ncpus;
-                semun_v.val = sem_value;
-                semctl(semaphore_id, 0, SETVAL, semun_v);
                 // Update the current stage.
                 cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
                 PLASSERT( cur_stage >= 0 );
                 semun_v.val = cur_stage + nsteps * minibatch_size;
+                semun_v.val = 0; // TODO Fix: pb with max semaphore value.
                 success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
-                PLASSERT( success == 0 );
+                if (success != 0)
+                    PLERROR("In NatGradSMPNNet::train - Could not update "
+                            "stage value for semaphore (errno = %d, returned "
+                            "value = %d, set value = %d)", errno, success,
+                            semun_v.val);
+                // Give update token to next CPU.
+                sem_value = (sem_value + 1) % ncpus;
+                semun_v.val = sem_value;
+                success = semctl(semaphore_id, 0, SETVAL, semun_v);
+                if (success != 0)
+                    PLERROR("In NatGradSMPNNet::train - Could not update "
+                            "semaphore with next CPU (errno = %d, returned "
+                            "value = %d, set value = %d)", errno, success,
+                            semun_v.val);
                 nsteps = 0;
             } else {
 #if 0
@@ -914,7 +928,7 @@
     stage = nstages;
     if (stage != cur_stage)
         PLWARNING("The target stage (%d) was not reached exactly (actual "
-                "stage: %d", stage, cur_stage);
+                "stage: %d)", stage, cur_stage);
 
     Profiler::end("training");
     Profiler::pl_profile_end("Totaltraining");



From chapados at mail.berlios.de  Sun Aug 12 02:32:00 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 12 Aug 2007 02:32:00 +0200
Subject: [Plearn-commits] r7985 - trunk/plearn/python
Message-ID: <200708120032.l7C0W0Bw022492@sheep.berlios.de>

Author: chapados
Date: 2007-08-12 02:31:59 +0200 (Sun, 12 Aug 2007)
New Revision: 7985

Modified:
   trunk/plearn/python/PythonIncludes.h
Log:
Added check to ensure that PL_USE_NUMARRAY and PL_USE_NUMPY are not both defined

Modified: trunk/plearn/python/PythonIncludes.h
===================================================================
--- trunk/plearn/python/PythonIncludes.h	2007-08-11 06:55:23 UTC (rev 7984)
+++ trunk/plearn/python/PythonIncludes.h	2007-08-12 00:31:59 UTC (rev 7985)
@@ -69,6 +69,10 @@
 #endif
 */
 
+#if defined(PL_USE_NUMARRAY) && defined(PL_USE_NUMPY)
+#  error "Symbols PL_USE_NUMARRAY and PL_USE_NUMPY are mutually exclusive; they should not both be defined"
+#endif
+
 #if PL_PYTHON_VERSION >= 250
 
 #include <python2.5/Python.h>



From chapados at mail.berlios.de  Sun Aug 12 05:13:53 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 12 Aug 2007 05:13:53 +0200
Subject: [Plearn-commits] r7986 - trunk/plearn/math
Message-ID: <200708120313.l7C3DrjY030798@sheep.berlios.de>

Author: chapados
Date: 2007-08-12 05:13:52 +0200 (Sun, 12 Aug 2007)
New Revision: 7986

Modified:
   trunk/plearn/math/StatsCollector.cc
Log:
Bugfixes in agemax/agemin handling...

Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2007-08-12 00:31:59 UTC (rev 7985)
+++ trunk/plearn/math/StatsCollector.cc	2007-08-12 03:13:52 UTC (rev 7986)
@@ -389,15 +389,20 @@
         //sum_ += val * weight;
         //sumsquare_ += val*val * weight;
         last_ = val;
-        if(fast_exact_is_equal(nnonmissing_,0))    // first value encountered
+        if(fast_exact_is_equal(nnonmissing_,0)) {   // first value encountered
             min_ = max_ = first_ = last_ = val;
+            agemin_ = 0;
+            agemax_ = 0;
+        }
         else if(val<min_) {
             min_ = val;
             agemin_ = 0;
+            ++agemax_;
         }
         else if(val>max_) {
             max_ = val;
             agemax_ = 0;
+            ++agemin_;
         }
         else {
             ++agemax_;                       // works even if they are missing
@@ -505,7 +510,7 @@
             // negative values for statistics that should always be
             // positive. We don't call forget() since missing values' count
             // would be lost...
-            min_ = max_ = first_ = last_ = MISSING_VALUE;
+            min_ = max_ = agemin_ = agemax_ = first_ = last_ = MISSING_VALUE;
             sum_ = sumsquare_ = sumcube_ = sumfourth_ = sumsquarew_ = 0.0;
         }
 



From chapados at mail.berlios.de  Sun Aug 12 23:38:10 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 12 Aug 2007 23:38:10 +0200
Subject: [Plearn-commits] r7987 - trunk/plearn/base
Message-ID: <200708122138.l7CLcAY6006010@sheep.berlios.de>

Author: chapados
Date: 2007-08-12 23:38:10 +0200 (Sun, 12 Aug 2007)
New Revision: 7987

Modified:
   trunk/plearn/base/Option.h
Log:
Added a few casts-to-void to shut up annoying warnings under gcc4

Modified: trunk/plearn/base/Option.h
===================================================================
--- trunk/plearn/base/Option.h	2007-08-12 03:13:52 UTC (rev 7986)
+++ trunk/plearn/base/Option.h	2007-08-12 21:38:10 UTC (rev 7987)
@@ -100,10 +100,10 @@
     { }
 
     virtual void read(Object* o, PStream& in) const
-    { in >> dynamic_cast<ObjectType*>(o)->*ptr; }
+    { (void)(in >> dynamic_cast<ObjectType*>(o)->*ptr); }
 
     virtual void write(const Object* o, PStream& out) const
-    { out << dynamic_cast<ObjectType *>(const_cast<Object*>(o))->*ptr; }
+    { (void)(out << dynamic_cast<ObjectType *>(const_cast<Object*>(o))->*ptr); }
 
     virtual Object* getAsObject(Object* o) const
     { return toObjectPtr(dynamic_cast<ObjectType*>(o)->*ptr); }



From chapados at mail.berlios.de  Sun Aug 12 23:58:29 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 12 Aug 2007 23:58:29 +0200
Subject: [Plearn-commits] r7988 - in trunk:
	plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0
	plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0
	plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0
	plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0
	plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0
	plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0
Message-ID: <200708122158.l7CLwToO007198@sheep.berlios.de>

Author: chapados
Date: 2007-08-12 23:58:27 +0200 (Sun, 12 Aug 2007)
New Revision: 7988

Modified:
   trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
   trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
   trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave
Log:
New test results following bugfix in Statscollector::agemin/agemax

Modified: trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -21,7 +21,7 @@
 min_ = 0.0194221049593962872 ;
 max_ = 265.08702193111759 ;
 agmemin_ = 63 ;
-agemax_ = 83 ;
+agemax_ = 86 ;
 first_ = 13.1144255088962556 ;
 last_ = 34.194047475019488 ;
 counts = {0.0194221049593962872 : 1 0 0 0 35 , 0.0325698952931281691 : 1 1 0.0210580598108425186 0.000443441882997020864 25 , 0.061987379823822554 : 1 0 0 0 34 , 0.0786715812781189072 : 1 0 0 0 15 , 0.276547938266577475 : 1 2 0.312045165023246862 0.0543652000143007019 49 , 0.903268077600334784 : 1 2 1.20251619734469539 0.752392985050565066 40 , 0.957582475886613049 : 1 0 0 0 22 , 2.02714634885673828 : 1 4 6.16185831341262613 9.85467870389801881 3 , 2.05231943353131463 : 1 1 2.03043031787215211 4.12264727573440837 44 , 2.55639694673505691 : 1 4 9.56965095026421331 22.9473976535547095 46 , 3.39504868705936369 : 1 1 2.79701999842607396 7.8233208715953948 39 , 4.1429907417257068 : 1 3 11.3023925206681426 42.6903958007932758 6 , 4.67453021446005579 : 1 0 0 0 21 , 5.08242518418548705 : 1 1 4.7761127815638682 22.8112533022177502 13 , 5.25250384797228076 : 1 0 0 0 19 , 5.37207018712550166 : 1 0 0 0 45 , 6.35849467137279323 : 1 1 5.85851516894162305 34.3221999847190915 24 , 7.207466!
 45300661459 : 1 1 6.87072140435881717 47.2068126163143944 29 , 7.96578391954506237 : 1 0 0 0 20 , 8.93298576523078935 : 1 1 8.17089885561959051 66.7635881087655321 14 , 8.98920177002007748 : 1 0 0 0 36 , 12.0085320633282215 : 1 3 31.1542680463670294 326.711585935405083 28 , 12.2468237044022299 : 1 0 0 0 16 , 13.1144255088962556 : 1 0 0 0 1 , 15.2640879450440519 : 1 5 70.5059901992690072 995.965835431366486 42 , 18.0644851071384664 : 1 2 34.6909123848655199 601.832092160831735 7 , 20.2143972777140739 : 1 2 37.4966034917601547 703.430823392461662 5 , 20.7110556243879174 : 1 0 0 0 4 , 22.6057399040152376 : 1 1 21.3536138210659132 455.976823219217181 23 , 23.8020931670862268 : 1 1 23.6527978431544454 559.454845809131598 50 , 23.8229065014076049 : 1 0 0 0 17 , 26.9337117592480837 : 1 0 0 0 27 , 29.789060511597107 : 1 1 27.1438737768138765 736.789883611604068 32 , 33.5218059320941393 : 1 3 95.7311987093098651 3059.48022466103157 9 , 34.3179649564094618 : 1 1 34.194047475019488 11!
 69.23288272388663 41 , 34.5907593027237326 : 1 0 0 0 37 , 37.1!
 23299919
4826334 : 1 0 0 0 47 , 38.8322499582853808 : 1 1 37.9844470884896737 1442.81822061827165 38 , 39.8865512936735271 : 1 0 0 0 26 , 47.911092434272156 : 1 0 0 0 11 , 53.9168908401858928 : 1 1 48.3496423219557911 2337.68791266105836 10 , 54.8916107965928788 : 1 0 0 0 18 , 58.9309522653012152 : 1 2 112.094544383224672 6282.66544835667901 8 , 63.517761056607462 : 1 1 60.6704064161583645 3680.89821470183006 2 , 69.1908667492120912 : 1 0 0 0 48 , 74.3267253554510745 : 1 1 72.476685639701131 5252.86996131605974 31 , 89.8783015490838295 : 1 0 0 0 30 , 98.1643547155170637 : 1 0 0 0 33 , 120.196546132356843 : 1 0 0 0 43 , 265.08702193111759 : 1 1 124.39630041420412 15474.4395567409192 12 , 3.4028234663852886e+38 : 0 0 0 0 0 };

Modified: trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn/math/test/VecStatsCollector/.pytest/PL_constant_regressor_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 4411195564.83381653 ;
 min_ = 0 ;
 max_ = 242.7039735654227 ;
-agmemin_ = nan ;
+agmemin_ = 97 ;
 agemax_ = 86 ;
 first_ = 0 ;
 last_ = 34.194047475019488 ;

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -14,13 +14,13 @@
 nmissing_ = 0 ;
 nnonmissing_ = 50 ;
 sumsquarew_ = 50 ;
-sum_ = 48.8719180204962811 ;
-sumsquare_ = 104.363929133440806 ;
-sumcube_ = 318.950374924751884 ;
-sumfourth_ = 1177.77083991768404 ;
+sum_ = 48.8719180204965085 ;
+sumsquare_ = 104.363929133441701 ;
+sumcube_ = 318.950374924756545 ;
+sumfourth_ = 1177.77083991770928 ;
 min_ = 17.6042426640295098 ;
-max_ = 22.4027835362179744 ;
-agmemin_ = 34 ;
+max_ = 22.4027835362180134 ;
+agmemin_ = 36 ;
 agemax_ = 18 ;
 first_ = 17.6199119453043913 ;
 last_ = 17.6479526969470228 ;

Modified: trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/distributions/test/.pytest/PL_GaussianDistribution/expected_results/expdir_2_True_123456/Split0/test2_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 150 ;
 sumsquarew_ = 150 ;
-sum_ = 9.41750512464426492 ;
-sumsquare_ = 139.406100961582524 ;
-sumcube_ = 289.545736167174312 ;
-sumfourth_ = 1182.4578956590592 ;
+sum_ = 9.41750512464402334 ;
+sumsquare_ = 139.406100961583832 ;
+sumcube_ = 289.545736167176756 ;
+sumfourth_ = 1182.45789565907239 ;
 min_ = 17.600507769412193 ;
-max_ = 23.689855294846236 ;
-agmemin_ = 93 ;
+max_ = 23.6898552948462466 ;
+agmemin_ = 95 ;
 agemax_ = 6 ;
-first_ = 18.5231445696105332 ;
-last_ = 19.4342267319090674 ;
+first_ = 18.5231445696105403 ;
+last_ = 19.4342267319090887 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/online/test/DeepBeliefNet/.pytest/PL_DBN_SimpleRBM/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,6 +20,8 @@
 sumfourth_ = 0 ;
 min_ = 0.651261219307483818 ;
 max_ = 0.651261219307483818 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 0.651261219307483818 ;
 last_ = 0.651261219307483818 ;
 counts = {};
@@ -37,6 +39,8 @@
 sumfourth_ = 0 ;
 min_ = 0.5 ;
 max_ = 0.5 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 0.5 ;
 last_ = 0.5 ;
 counts = {};
@@ -54,6 +58,8 @@
 sumfourth_ = 0 ;
 min_ = 0.614096318051678525 ;
 max_ = 0.614096318051678525 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 0.614096318051678525 ;
 last_ = 0.614096318051678525 ;
 counts = {};
@@ -71,6 +77,8 @@
 sumfourth_ = 0 ;
 min_ = 0 ;
 max_ = 0 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 0 ;
 last_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 17.4914867139879391 ;
 min_ = 0.000403386661488822746 ;
 max_ = 1.79622562497000327 ;
-agmemin_ = 11 ;
+agmemin_ = 13 ;
 agemax_ = 8 ;
 first_ = 0.0133271392087507996 ;
 last_ = 0.0703378652394482801 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 22.1170742798113977 ;
 min_ = 3.99687001153608957e-05 ;
 max_ = 1.79622562497000327 ;
-agmemin_ = 230 ;
+agmemin_ = 232 ;
 agemax_ = 8 ;
 first_ = 0.00559245542604580592 ;
 last_ = 0.0703378652394482801 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 2 [ StatsCollector(
@@ -19,6 +20,8 @@
 sumfourth_ = 0 ;
 min_ = nan ;
 max_ = nan ;
+agmemin_ = nan ;
+agemax_ = nan ;
 first_ = nan ;
 last_ = nan ;
 counts = {};
@@ -34,10 +37,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.0521801135176278436 ;
-max_ = 0.0521801135176278436 ;
-first_ = 0.0521801135176278436 ;
-last_ = 0.0521801135176278436 ;
+min_ = 0.052180113517651637 ;
+max_ = 0.052180113517651637 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 0.052180113517651637 ;
+last_ = 0.052180113517651637 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 102322137.326348484 ;
 min_ = 5.4718757392551664e-06 ;
 max_ = 100.558698546447715 ;
-agmemin_ = 36 ;
+agmemin_ = 38 ;
 agemax_ = 28 ;
 first_ = 0.0159720327578714814 ;
 last_ = 0.000951494976113702773 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/test2_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 102371139.874690771 ;
 min_ = 1.04669084620911105e-16 ;
 max_ = 100.558698546447715 ;
-agmemin_ = 169 ;
+agmemin_ = 172 ;
 agemax_ = 28 ;
 first_ = 0.00401338269720593124 ;
 last_ = 0.000951494976113702773 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_regressor/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 2.50165040817058326 ;
 min_ = 1.04669084620911105e-16 ;
 max_ = 1.21796824903727607 ;
-agmemin_ = 96 ;
+agmemin_ = 97 ;
 agemax_ = 37 ;
 first_ = 0.00401338269720593124 ;
 last_ = 0.000157352772999446117 ;

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -21,7 +21,7 @@
 min_ = 4.49614059257518761 ;
 max_ = 1506.91426994253925 ;
 agmemin_ = 25 ;
-agemax_ = 39 ;
+agemax_ = 41 ;
 first_ = 564.628509411856726 ;
 last_ = 13.7380273402321649 ;
 counts = {};
@@ -40,7 +40,7 @@
 min_ = 4.49614059257518761 ;
 max_ = 1506.91426994253925 ;
 agmemin_ = 25 ;
-agemax_ = 39 ;
+agemax_ = 41 ;
 first_ = 564.628509411856726 ;
 last_ = 13.7380273402321649 ;
 counts = {};
@@ -58,8 +58,8 @@
 sumfourth_ = 0 ;
 min_ = 6.23533365254733773 ;
 max_ = 6.23533365254733773 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.23533365254733773 ;
 last_ = 6.23533365254733773 ;
 counts = {};
@@ -77,8 +77,8 @@
 sumfourth_ = 0 ;
 min_ = 6.35575906431118831 ;
 max_ = 6.35575906431118831 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.35575906431118831 ;
 last_ = 6.35575906431118831 ;
 counts = {};
@@ -96,8 +96,8 @@
 sumfourth_ = 0 ;
 min_ = 6.29554635842926302 ;
 max_ = 6.29554635842926302 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.29554635842926302 ;
 last_ = 6.29554635842926302 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/LinearRegressor/.pytest/PL_linear_regressor_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 5 [ StatsCollector(
@@ -17,10 +18,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 471.224041336228424 ;
-max_ = 471.224041336228424 ;
-first_ = 471.224041336228424 ;
-last_ = 471.224041336228424 ;
+min_ = 471.224041336225355 ;
+max_ = 471.224041336225355 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 471.224041336225355 ;
+last_ = 471.224041336225355 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -34,10 +37,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 471.224041336228424 ;
-max_ = 471.224041336228424 ;
-first_ = 471.224041336228424 ;
-last_ = 471.224041336228424 ;
+min_ = 471.224041336225355 ;
+max_ = 471.224041336225355 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 471.224041336225355 ;
+last_ = 471.224041336225355 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -51,10 +56,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.23533365254734484 ;
-max_ = 6.23533365254734484 ;
-first_ = 6.23533365254734484 ;
-last_ = 6.23533365254734484 ;
+min_ = 6.23533365254733773 ;
+max_ = 6.23533365254733773 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 6.23533365254733773 ;
+last_ = 6.23533365254733773 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -68,10 +75,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.35575906431119542 ;
-max_ = 6.35575906431119542 ;
-first_ = 6.35575906431119542 ;
-last_ = 6.35575906431119542 ;
+min_ = 6.35575906431118831 ;
+max_ = 6.35575906431118831 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 6.35575906431118831 ;
+last_ = 6.35575906431118831 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -85,10 +94,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.29554635842927013 ;
-max_ = 6.29554635842927013 ;
-first_ = 6.29554635842927013 ;
-last_ = 6.29554635842927013 ;
+min_ = 6.29554635842926302 ;
+max_ = 6.29554635842926302 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 6.29554635842926302 ;
+last_ = 6.29554635842926302 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 187859983108665.156 ;
 min_ = 1.23064138227748932 ;
 max_ = 2757.59234913165938 ;
-agmemin_ = 22 ;
+agmemin_ = 23 ;
 agemax_ = 1 ;
 first_ = 1651.65665408713994 ;
 last_ = 892.793588571159603 ;
@@ -39,7 +39,7 @@
 sumfourth_ = 187859983108665.156 ;
 min_ = 1.23064138227748932 ;
 max_ = 2757.59234913165938 ;
-agmemin_ = 22 ;
+agmemin_ = 23 ;
 agemax_ = 1 ;
 first_ = 1651.65665408713994 ;
 last_ = 892.793588571159603 ;
@@ -58,8 +58,8 @@
 sumfourth_ = 0 ;
 min_ = 6.09268869187651152 ;
 max_ = 6.09268869187651152 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.09268869187651152 ;
 last_ = 6.09268869187651152 ;
 counts = {};
@@ -77,8 +77,8 @@
 sumfourth_ = 0 ;
 min_ = 6.29339771148292826 ;
 max_ = 6.29339771148292826 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.29339771148292826 ;
 last_ = 6.29339771148292826 ;
 counts = {};
@@ -96,8 +96,8 @@
 sumfourth_ = 0 ;
 min_ = 6.19304320167971944 ;
 max_ = 6.19304320167971944 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.19304320167971944 ;
 last_ = 6.19304320167971944 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_badly_conditioned_linreg_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 5 [ StatsCollector(
@@ -17,10 +18,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 387.360332165304555 ;
-max_ = 387.360332165304555 ;
-first_ = 387.360332165304555 ;
-last_ = 387.360332165304555 ;
+min_ = 387.360335203449551 ;
+max_ = 387.360335203449551 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 387.360335203449551 ;
+last_ = 387.360335203449551 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -34,10 +37,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 387.360332165304555 ;
-max_ = 387.360332165304555 ;
-first_ = 387.360332165304555 ;
-last_ = 387.360332165304555 ;
+min_ = 387.360335203449551 ;
+max_ = 387.360335203449551 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 387.360335203449551 ;
+last_ = 387.360335203449551 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -51,10 +56,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.09268868403331076 ;
-max_ = 6.09268868403331076 ;
-first_ = 6.09268868403331076 ;
-last_ = 6.09268868403331076 ;
+min_ = 6.09268869187651152 ;
+max_ = 6.09268869187651152 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 6.09268869187651152 ;
+last_ = 6.09268869187651152 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -68,10 +75,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.2933977036397275 ;
-max_ = 6.2933977036397275 ;
-first_ = 6.2933977036397275 ;
-last_ = 6.2933977036397275 ;
+min_ = 6.29339771148292826 ;
+max_ = 6.29339771148292826 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 6.29339771148292826 ;
+last_ = 6.29339771148292826 ;
 counts = {};
 more_than_maxnvalues = 1  )
 StatsCollector(
@@ -85,10 +94,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 6.19304319383651958 ;
-max_ = 6.19304319383651958 ;
-first_ = 6.19304319383651958 ;
-last_ = 6.19304319383651958 ;
+min_ = 6.19304320167971944 ;
+max_ = 6.19304320167971944 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
+first_ = 6.19304320167971944 ;
+last_ = 6.19304320167971944 ;
 counts = {};
 more_than_maxnvalues = 1  )
 ] ;

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/test1_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -20,7 +20,7 @@
 sumfourth_ = 92944320862419.2344 ;
 min_ = 1.86363769529453394 ;
 max_ = 2256.46039029148506 ;
-agmemin_ = 22 ;
+agmemin_ = 23 ;
 agemax_ = 1 ;
 first_ = 1412.74238652622353 ;
 last_ = 782.35445173265191 ;
@@ -39,7 +39,7 @@
 sumfourth_ = 92944320862419.2344 ;
 min_ = 1.86363769529453394 ;
 max_ = 2256.46039029148506 ;
-agmemin_ = 22 ;
+agmemin_ = 23 ;
 agemax_ = 1 ;
 first_ = 1412.74238652622353 ;
 last_ = 782.35445173265191 ;
@@ -58,8 +58,8 @@
 sumfourth_ = 0 ;
 min_ = 6.05769163403522626 ;
 max_ = 6.05769163403522626 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.05769163403522626 ;
 last_ = 6.05769163403522626 ;
 counts = {};
@@ -77,8 +77,8 @@
 sumfourth_ = 0 ;
 min_ = 6.17811704579907683 ;
 max_ = 6.17811704579907683 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.17811704579907683 ;
 last_ = 6.17811704579907683 ;
 counts = {};
@@ -96,8 +96,8 @@
 sumfourth_ = 0 ;
 min_ = 6.11790433991715155 ;
 max_ = 6.11790433991715155 ;
-agmemin_ = nan ;
-agemax_ = nan ;
+agmemin_ = 49 ;
+agemax_ = 49 ;
 first_ = 6.11790433991715155 ;
 last_ = 6.11790433991715155 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:38:10 UTC (rev 7987)
+++ trunk/plearn_learners/regressors/test/StackedPCARegression/.pytest/PL_stacked_pca_regression_script/expected_results/expdir/Split0/train_stats.psave	2007-08-12 21:58:27 UTC (rev 7988)
@@ -4,6 +4,7 @@
 compute_covariance = 0 ;
 epsilon = 0 ;
 window = -1 ;
+full_update_frequency = -1 ;
 window_nan_code = 0 ;
 no_removal_warnings = 0 ;
 stats = 5 [ StatsCollector(
@@ -19,6 +20,8 @@
 sumfourth_ = 0 ;
 min_ = 394.528599820739259 ;
 max_ = 394.528599820739259 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 394.528599820739259 ;
 last_ = 394.528599820739259 ;
 counts = {};
@@ -36,6 +39,8 @@
 sumfourth_ = 0 ;
 min_ = 394.528599820739259 ;
 max_ = 394.528599820739259 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 394.528599820739259 ;
 last_ = 394.528599820739259 ;
 counts = {};
@@ -53,6 +58,8 @@
 sumfourth_ = 0 ;
 min_ = 6.05769163403522626 ;
 max_ = 6.05769163403522626 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 6.05769163403522626 ;
 last_ = 6.05769163403522626 ;
 counts = {};
@@ -70,6 +77,8 @@
 sumfourth_ = 0 ;
 min_ = 6.17811704579907683 ;
 max_ = 6.17811704579907683 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 6.17811704579907683 ;
 last_ = 6.17811704579907683 ;
 counts = {};
@@ -87,6 +96,8 @@
 sumfourth_ = 0 ;
 min_ = 6.11790433991715155 ;
 max_ = 6.11790433991715155 ;
+agmemin_ = 0 ;
+agemax_ = 0 ;
 first_ = 6.11790433991715155 ;
 last_ = 6.11790433991715155 ;
 counts = {};



From nouiz at mail.berlios.de  Tue Aug 14 19:46:27 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 14 Aug 2007 19:46:27 +0200
Subject: [Plearn-commits] r7989 - trunk/python_modules/plearn/parallel
Message-ID: <200708141746.l7EHkRMt030472@sheep.berlios.de>

Author: nouiz
Date: 2007-08-14 19:46:26 +0200 (Tue, 14 Aug 2007)
New Revision: 7989

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
bugfix


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-12 21:58:27 UTC (rev 7988)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-14 17:46:26 UTC (rev 7989)
@@ -736,8 +736,8 @@
                 t=Thread(target=self.run_one_job,args=(task,))
                 t.start()
                 self.threads.append(t)
-                for t in self.threads:
-                    t.join()
+            for t in self.threads:
+                t.join()
         else:
             for task in self.tasks:
                 self.run_one_job(task)



From nouiz at mail.berlios.de  Tue Aug 14 19:48:07 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 14 Aug 2007 19:48:07 +0200
Subject: [Plearn-commits] r7990 - trunk/python_modules/plearn/learners/autolr
Message-ID: <200708141748.l7EHm7ZN030630@sheep.berlios.de>

Author: nouiz
Date: 2007-08-14 19:48:07 +0200 (Tue, 14 Aug 2007)
New Revision: 7990

Modified:
   trunk/python_modules/plearn/learners/autolr/__init__.py
Log:
bugfix and more agressive deletion of candidate

now we also delete all candidate with a learning rate bellow the one for the best candidate


Modified: trunk/python_modules/plearn/learners/autolr/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-14 17:46:26 UTC (rev 7989)
+++ trunk/python_modules/plearn/learners/autolr/__init__.py	2007-08-14 17:48:07 UTC (rev 7990)
@@ -426,8 +426,17 @@
         start_t = max(all_start[c1],all_start[c2])
         curve1 = error_curve(c1,start_t,t)#[valid_error]
         curve2 = error_curve(c2,start_t,t)
+        
         if curve2.shape[0]-1<min_epochs_to_delete:
             return False
+
+        #If the best have a higher lr then the current one, we remove this one
+        #This one can't be better then the next one we will create
+        #as the next one we will create won't have a lower lr than this one
+        #and it start with a lower error rate
+        if all_lr[c1]>c2lr:
+            return True
+        
         for a in actives:
             if a!=c2 and all_lr[a]==c2lr and all_last_err[a]<=c2err:
                 # throw it away if worse than other actives of same lr
@@ -506,7 +515,7 @@
             print >>logfile, "Before epoch",t+1
             print >>logfile, "Actives now: ",actives
             print >>logfile, " with lr=", array(all_lr)[actives]
-        print "Before epoch %d/%d"%(t,n_train),"actives now: ",actives, " with lr=", array(all_lr)[actives]
+        print "Before epoch %d/%d"%(t+1,n_train),"actives now: ",actives, " with lr=", array(all_lr)[actives]
         print "current best actives:",best_active,"best_error:",all_last_err[best_active],"lr:",all_lr[best_active]
 
         threads = []
@@ -639,7 +648,7 @@
                         print >>logfile,"REMOVE candidate ",a
                     release_server(all_candidates[a], use_threads)
                     # hopefully this destroys the candidate
-                    all_end[a]=t
+                    all_end[a]=t+1 # so that all_end[i]-all_start[i]=nb of epoch executed by this learner
                     all_candidates[a]=None
                     del actives[j-ndeleted]
                     ndeleted+=1
@@ -656,7 +665,7 @@
                 all_candidates.append(new_candidate)
                 all_results.append(all_results[best_active].copy())
                 all_last_err.append(best_last)
-                all_start.append(t)
+                all_start.append(t+1)#start at the next epoch
                 all_end.append(-1)
                 if logfile:
                     print >>logfile,"CREATE candidate ", new_a, " from ",best_active,"at stage ",learner.stage," with lr=",all_lr[new_a]
@@ -670,14 +679,13 @@
     if logfile and best_err < all_last_err[best_active]:
         print >>logfile, "WARNING: best performing model would have stopped early at stage ",best_early_stop
     if logfile:
-        print >>logfile,"When then learner started",all_start
-        print >>logfile,"When then learner ended",all_end
+        print >>logfile,"We created %d learner"%len(all_start)
+        print >>logfile,"When the learners started:",all_start
+        all_end2=[ ifthenelse(x==-1,t+1,x) for x in all_end ]
+        print >>logfile,"When the learners ended:",all_end2
         l=[]
         for i in range(len(all_start)):
-            if all_end[i]==-1:
-                l.append(abs(all_start[i]-n_train))
-            else:
-                l.append(abs(all_start[i]-all_end[i]+1))
+            l.append(all_end2[i]-all_start[i])
         print >>logfile,"Duration of all learner in number of epoch:",l
         sum=reduce(lambda x,y:x+y, l)
         print >>logfile,"Their was a total of %d epoch executed for %d asked(%fx more)"%(sum,n_train,float(sum)/n_train)



From nouiz at mail.berlios.de  Tue Aug 14 20:17:22 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 14 Aug 2007 20:17:22 +0200
Subject: [Plearn-commits] r7991 - trunk/python_modules/plearn/parallel
Message-ID: <200708141817.l7EIHM4L002062@sheep.berlios.de>

Author: nouiz
Date: 2007-08-14 20:17:21 +0200 (Tue, 14 Aug 2007)
New Revision: 7991

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
This way we generate different logfile name when the logfile name is long


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-14 17:48:07 UTC (rev 7990)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-14 18:17:21 UTC (rev 7991)
@@ -155,7 +155,7 @@
             self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
         else:
             self.unique_id = formatted_command+str(datetime.datetime.now()).replace(' ','_')
-            self.log_file = truncate( os.path.join(log_dir, self.unique_id), 200) + ".log"
+            self.log_file = os.path.join(log_dir, self.unique_id) + ".log"
 
         if self.add_unique_id:
                 command = command + ' unique_id=' + self.unique_id



From saintmlx at mail.berlios.de  Tue Aug 14 20:22:45 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 14 Aug 2007 20:22:45 +0200
Subject: [Plearn-commits] r7992 - in trunk: . plearn/base plearn/python
Message-ID: <200708141822.l7EIMj8T002387@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-14 20:22:44 +0200 (Tue, 14 Aug 2007)
New Revision: 7992

Modified:
   trunk/plearn/base/Object.cc
   trunk/plearn/base/Object.h
   trunk/plearn/base/OptionBase.h
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/pymake.config.model
Log:
- added support for sets in PLearn/python bridge
- use maps and sets instead of lists when searching options



Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2007-08-14 18:17:21 UTC (rev 7991)
+++ trunk/plearn/base/Object.cc	2007-08-14 18:22:44 UTC (rev 7992)
@@ -156,25 +156,19 @@
 #ifdef PL_PYTHON_VERSION 
 void Object::setOptionFromPython(const string& optionname, const PythonObjectWrapper& value)
 {
-    OptionList &options= getOptionList();
-    for(OptionList::iterator it= options.begin(); 
-        it != options.end(); ++it)
-        if((*it)->optionname() == optionname)
-        {
-            (*it)->setFromPythonObject(this, value);
-            return;
-        }
+    OptionMap& om= getOptionMap();
+    OptionMap::iterator it= om.find(optionname);
+    if(it == om.end())
+        PLERROR("%s has no option %s.", classname().c_str(), optionname.c_str());
+    it->second->setFromPythonObject(this, value);
 }
 #endif //def PL_PYTHON_VERSION 
 
 bool Object::hasOption(const string &optionname) const
 { 
-    OptionList &options= getOptionList();
-    for(OptionList::iterator it= options.begin(); 
-        it != options.end(); ++it)
-        if((*it)->optionname() == optionname)
-            return true;
-    return false;
+    OptionMap& om= getOptionMap();
+    OptionMap::iterator it= om.find(optionname);
+    return it != om.end();
 }
 
 string Object::getOption(const string &optionname) const
@@ -211,14 +205,13 @@
 {
     try 
     {
-        OptionList &options = getOptionList();
-        for (OptionList::iterator it = options.begin(); it != options.end(); ++it) 
+
+        OptionMap& om= getOptionMap();
+        OptionMap::iterator it= om.find(optionname);
+        if(it != om.end())
         {
-            if ((*it)->optionname() == optionname) 
-            {
-                (*it)->read(this, in);
-                return;
-            }
+            it->second->read(this, in);
+            return;
         }
 
         // Found no options matching 'optionname'. First look for brackets. If there
@@ -234,24 +227,25 @@
             // Found no dot, or a right bracket before a dot
             if (dot_pos == string::npos || rb_pos < dot_pos) {
                 string index = optionname.substr(lb_pos + 1, rb_pos - lb_pos - 1);
-                for (OptionList::iterator it = options.begin(); it != options.end(); ++it)
-                    if ((*it)->optionname() == optname) 
-                    {
-                        // There are two cases here: either there is a dot located
-                        // immediately after the right bracket, or there is no dot.
-                        // If there is a dot, the option HAS to be an Object
-                        if (dot_pos != string::npos && dot_pos == rb_pos+1) {
-                            int i = toint(index);
-                            (*it)->getIndexedObject(this, i)->readOptionVal(
-                                in, optionname.substr(dot_pos + 1));
-                        }
-                        else if (dot_pos == string::npos)
-                            (*it)->readIntoIndex(this, in, index);
-                        else
-                            PLERROR("Object::readOptionVal() - unknown option format \"%s\"",
-                                    optionname.c_str());
-                        return;
+
+                it= om.find(optname);
+                if(it != om.end())
+                {
+                    // There are two cases here: either there is a dot located
+                    // immediately after the right bracket, or there is no dot.
+                    // If there is a dot, the option HAS to be an Object
+                    if (dot_pos != string::npos && dot_pos == rb_pos+1) {
+                        int i = toint(index);
+                        it->second->getIndexedObject(this, i)->readOptionVal(
+                            in, optionname.substr(dot_pos + 1));
                     }
+                    else if (dot_pos == string::npos)
+                        it->second->readIntoIndex(this, in, index);
+                    else
+                        PLERROR("Object::readOptionVal() - unknown option format \"%s\"",
+                                optionname.c_str());
+                    return;
+                }
             }
         }
         else if (lb_pos != string::npos)
@@ -263,12 +257,12 @@
             // Found a dot, assume it's a field with an Object * field
             string optname = optionname.substr(0, dot_pos);
             string optoptname = optionname.substr(dot_pos + 1);
-            for (OptionList::iterator it = options.begin(); it != options.end(); ++it)
-                if ((*it)->optionname() == optname) 
-                {
-                    (*it)->getAsObject(this)->readOptionVal(in, optoptname);
-                    return;
-                }
+            it= om.find(optname);
+            if(it != om.end())
+            {
+                it->second->getAsObject(this)->readOptionVal(in, optoptname);
+                return;
+            }
         }
     }
     catch(const PLearnError& e)
@@ -287,13 +281,14 @@
 
 void Object::writeOptionVal(PStream &out, const string &optionname) const
 {
-    OptionList &options = getOptionList();
-    for (OptionList::iterator it = options.begin(); it != options.end(); ++it)
-        if ((*it)->optionname() == optionname) {
-            (*it)->write(this, out);
-            return;
-        }
-
+    OptionMap& om= getOptionMap();
+    OptionMap::iterator it= om.find(optionname);
+    if(it != om.end())
+    {
+        it->second->write(this, out);
+        return;
+    }
+    
     // Found no options matching 'optionname'. First look for brackets. If there
     // are brackets, they must be located before any dot.
     size_t lb_pos = optionname.find('[');
@@ -307,23 +302,24 @@
         // Found no dot, or a right bracket before a dot
         if (dot_pos == string::npos || rb_pos < dot_pos) {
             string index = optionname.substr(lb_pos + 1, rb_pos - lb_pos - 1);
-            for (OptionList::iterator it = options.begin(); it != options.end(); ++it)
-                if ((*it)->optionname() == optname) {
-                    // There are two cases here: either there is a dot located
-                    // immediately after the right bracket, or there is no dot.  If
-                    // there is a dot, the option HAS to be an Object
-                    if (dot_pos != string::npos && dot_pos == rb_pos+1) {
-                        int i = toint(index);
-                        (*it)->getIndexedObject(this, i)->writeOptionVal(
-                            out, optionname.substr(dot_pos + 1));
-                    }
-                    else if (dot_pos == string::npos)
-                        (*it)->writeAtIndex(this, out, index);
-                    else
-                        PLERROR("Object::writeOptionVal() - unknown option format \"%s\"",
-                                optionname.c_str());
-                    return;
+            it= om.find(optname);
+            if(it != om.end())
+            {
+                // There are two cases here: either there is a dot located
+                // immediately after the right bracket, or there is no dot.  If
+                // there is a dot, the option HAS to be an Object
+                if (dot_pos != string::npos && dot_pos == rb_pos+1) {
+                    int i = toint(index);
+                    it->second->getIndexedObject(this, i)->writeOptionVal(
+                        out, optionname.substr(dot_pos + 1));
                 }
+                else if (dot_pos == string::npos)
+                    it->second->writeAtIndex(this, out, index);
+                else
+                    PLERROR("Object::writeOptionVal() - unknown option format \"%s\"",
+                            optionname.c_str());
+                return;
+            }
         }
     }
     else if (lb_pos != string::npos)
@@ -334,11 +330,12 @@
         // Found a dot, assume it's a field with an Object * field
         string optname = optionname.substr(0, dot_pos);
         string optoptname = optionname.substr(dot_pos + 1);
-        for (OptionList::iterator it = options.begin(); it != options.end(); ++it)
-            if ((*it)->optionname() == optname) {
-                (*it)->getAsObject(this)->writeOptionVal(out, optoptname);
-                return;
-            }
+        it= om.find(optname);
+        if(it != om.end())
+        {
+            it->second->getAsObject(this)->writeOptionVal(out, optoptname);
+            return;
+        }
     }
     // There are bigger problems in the world but still it isn't always funny
     PLERROR("Object::writeOptionVal() - Unknown option \"%s\"", optionname.c_str());    
@@ -520,13 +517,10 @@
             in.getline(optionname, '=');
             optionname = removeblanks(optionname);
             in.skipBlanksAndComments();
-
-            OptionList &options = getOptionList();
-            OptionList::iterator it =
-                find_if(options.begin(), options.end(),
-                        bind2nd(mem_fun(&OptionBase::isOptionNamed), optionname));
-            // if (it != options.end() && ((*it)->flags() & in.option_flags_in) == 0)
-            if (it!=options.end() && (*it)->shouldBeSkipped() ) {
+            OptionMap& om= getOptionMap();
+            OptionMap::iterator it= om.find(optionname);
+            if(it != om.end() && it->second->shouldBeSkipped())
+            {
                 // Create a dummy object that will read this option.
                 if (!dummy_obj) {
                     // Note that we do not call build on 'dummy_obj'. This is
@@ -546,15 +540,6 @@
                 // cerr << "returned from reading optiion " << optionname << endl;
             }
             in.skipBlanksAndCommentsAndSeparators();
-            /*
-              in.skipBlanksAndCommentsAndSeparators();
-              in.skipBlanksAndCommentsAndSeparators();
-              in.skipBlanksAndCommentsAndSeparators();
-              in.skipBlanksAndCommentsAndSeparators();
-              cerr << "PEEK1: " << in.peek() << endl;
-              in.peek(); in.peek(); in.peek();
-              cerr << "PEEK2: " << in.peek() << endl;
-            */
 
             if (in.peek() == ')') 
             {
@@ -661,11 +646,10 @@
                 {
                     checkNargs(args.size(), 1);
                     string optionname= args[0];
-                    OptionList &options= instance->getOptionList();
-                    for (OptionList::iterator it= options.begin(); 
-                         it != options.end(); ++it)
-                        if ((*it)->optionname() == optionname)
-                            return (*it)->getAsPythonObject(instance);
+                    OptionMap& om= instance->getOptionMap();
+                    OptionMap::iterator it= om.find(optionname);
+                    if(it != om.end())
+                        return it->second->getAsPythonObject(instance);
                     PLERROR("in ObjectTrampolineGetOption::call (python ver.) : "
                             "unknown option: '%s'", optionname.c_str());
                     return PythonObjectWrapper();//gcc: shut up!

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-08-14 18:17:21 UTC (rev 7991)
+++ trunk/plearn/base/Object.h	2007-08-14 18:22:44 UTC (rev 7992)
@@ -88,6 +88,7 @@
         virtual string classname() const;                       \
         static  OptionList& _getOptionList_();                  \
         virtual OptionList& getOptionList() const;              \
+        virtual OptionMap& getOptionMap() const;                \
         static  RemoteMethodMap& _getRemoteMethodMap_();        \
         virtual RemoteMethodMap& getRemoteMethodMap() const;    \
         static  Object* _new_instance_for_typemap_();           \
@@ -122,6 +123,18 @@
             return _getOptionList_();                                                           \
         }                                                                                       \
                                                                                                 \
+        OptionMap& CLASSTYPE::getOptionMap() const                                              \
+        {                                                                                       \
+            static OptionMap om;                                                                \
+            if(om.empty())                                                                      \
+            {                                                                                   \
+                OptionList& ol= getOptionList();                                                \
+                for(OptionList::iterator it= ol.begin(); it != ol.end(); ++it)                  \
+                    om[(*it)->optionname()]= *it;                                               \
+            }                                                                                   \
+            return om;                                                                          \
+        }                                                                                       \
+                                                                                                \
         RemoteMethodMap& CLASSTYPE::_getRemoteMethodMap_()                                      \
         {                                                                                       \
             static bool initialized = false;                                                    \
@@ -271,6 +284,7 @@
         virtual string classname() const;                                                       \
         static  OptionList& _getOptionList_();                                                  \
         virtual OptionList& getOptionList() const;                                              \
+        virtual OptionMap& getOptionMap() const;                                                \
         static  RemoteMethodMap& _getRemoteMethodMap_();                                        \
         virtual RemoteMethodMap& getRemoteMethodMap() const;                                    \
         static  Object* _new_instance_for_typemap_();                                           \
@@ -307,6 +321,18 @@
         {                                                                                       \
             return _getOptionList_();                                                           \
         }                                                                                       \
+        template < TEMPLATE_DEF_ ## CLASSTYPE >                                                 \
+        OptionMap& CLASSTYPE< TEMPLATE_ARGS_ ## CLASSTYPE >::getOptionMap() const               \
+        {                                                                                       \
+            static OptionMap om;                                                                \
+            if(om.empty())                                                                      \
+            {                                                                                   \
+                OptionList& ol= getOptionList();                                                \
+                for(OptionList::iterator it= ol.begin(); it != ol.end(); ++it)                  \
+                    om[(*it)->optionname()]= *it;                                               \
+            }                                                                                   \
+            return om;                                                                          \
+        }                                                                                       \
                                                                                                 \
         template < TEMPLATE_DEF_ ## CLASSTYPE >                                                 \
         RemoteMethodMap& CLASSTYPE< TEMPLATE_ARGS_ ## CLASSTYPE >::_getRemoteMethodMap_()       \

Modified: trunk/plearn/base/OptionBase.h
===================================================================
--- trunk/plearn/base/OptionBase.h	2007-08-14 18:17:21 UTC (rev 7991)
+++ trunk/plearn/base/OptionBase.h	2007-08-14 18:22:44 UTC (rev 7992)
@@ -326,6 +326,7 @@
 };
 
 typedef std::vector< PP<OptionBase> > OptionList;
+typedef std::map<std::string, PP<OptionBase> > OptionMap;
 
 
 

Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2007-08-14 18:17:21 UTC (rev 7991)
+++ trunk/plearn/python/PythonExtension.cc	2007-08-14 18:22:44 UTC (rev 7992)
@@ -279,10 +279,9 @@
         //set option names
         OptionList& options= tit->second.getoptionlist_method();
         unsigned int nopts= options.size();
-        TVec<string> optionnames(nopts);
+        set<string> optionnames;
         for(unsigned int i= 0; i < nopts; ++i)
-            optionnames[i]= options[i]->optionname();
-
+            optionnames.insert(options[i]->optionname());
         the_pyclass= clit->second;
         if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames", 
                                       PythonObjectWrapper(optionnames).getPyObject()))

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-08-14 18:17:21 UTC (rev 7991)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-08-14 18:22:44 UTC (rev 7992)
@@ -52,6 +52,7 @@
 #include <utility>                           // Pairs
 #include <vector>                            // vector<T>
 #include <map>                               // map<T,U>
+#include <set>                               // set<T>
 #include <limits>                            // numeric_limits<I>
 
 // From PLearn
@@ -424,6 +425,12 @@
     static std::map<T,U> convert(PyObject*, bool print_traceback);
 };
 
+template <class T>
+struct ConvertFromPyObject<std::set<T> >
+{
+    static std::set<T> convert(PyObject*, bool print_traceback);
+};
+
 template <class T, class U>
 struct ConvertFromPyObject< std::pair<T,U> >
 {
@@ -614,6 +621,10 @@
 template <class T, class U> struct ConvertToPyObject<std::map<T,U> >
 { static PyObject* newPyObject(const std::map<T,U>&); };
 
+//! C++ stlib set<>: create a Python set of those objects
+template <class T> struct ConvertToPyObject<std::set<T> >
+{ static PyObject* newPyObject(const std::set<T>&); };
+
 //! C++ stdlib pair<>: create a Python tuple with two elements
 template <class T, class U> struct ConvertToPyObject<std::pair<T,U> >
 { static PyObject* newPyObject(const std::pair<T,U>&); };
@@ -630,6 +641,10 @@
 template <class T, class U> struct ConvertToPyObject<std::map<T,U> const* >
 { static PyObject* newPyObject(const std::map<T,U>*); };
 
+//! Pointer to set<>: simply dereference pointer, or None if NULL
+template <class T> struct ConvertToPyObject<std::set<T> const* >
+{ static PyObject* newPyObject(const std::set<T>*); };
+
 //! For a general PythonObjectWrapper: we simply increment the refcount
 //! to the underlying Python object, no matter whether we own it or not.
 template<> struct ConvertToPyObject<PythonObjectWrapper>
@@ -1063,6 +1078,38 @@
     return result;
 }
 
+template <class T>
+std::set<T> ConvertFromPyObject<std::set<T> >::convert(PyObject* pyobj,
+                                                       bool print_traceback)
+{
+    PLASSERT( pyobj );
+    PyObject* env= PyDict_New();
+    if(0 != PyDict_SetItemString(env, "__builtins__", PyEval_GetBuiltins()))
+        PLERROR("in ConvertFromPyObject<std::set<T> >::convert : "
+                "cannot insert builtins in env.");
+    if(0 != PyDict_SetItemString(env, "the_set", pyobj))
+        PLERROR("in ConvertFromPyObject<std::set<T> >::convert : "
+                "cannot insert the_set in env.");
+    PyObject* res= PyRun_String("\nresult= list(the_set)\n", 
+                                Py_file_input, env, env);
+    if(!res)
+    {
+        Py_DECREF(env);
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in ConvertFromPyObject<std::set<T> >::convert : "
+                "cannot convert to a set.");
+    }
+    Py_DECREF(res);
+    TVec<PythonObjectWrapper> listobj= 
+        PythonObjectWrapper(env).as<std::map<string, PythonObjectWrapper> >()["result"];
+    Py_DECREF(env);
+    std::set<T> the_set;
+    for(TVec<PythonObjectWrapper>::iterator it= listobj.begin();
+        it != listobj.end(); ++it)
+        the_set.insert(*it);
+    return the_set;
+}
+
 template <class T, class U>
 std::pair<T,U> ConvertFromPyObject< std::pair<T,U> >::convert(PyObject* pyobj,
                                                               bool print_traceback)
@@ -1198,6 +1245,45 @@
     return newdict;
 }
 
+template <class T>
+PyObject* ConvertToPyObject<std::set<T> >::newPyObject(const std::set<T>& data)
+{
+    TVec<T> as_list(data.size());
+    int i= 0;
+    for(typename std::set<T>::iterator it= data.begin();
+        it != data.end(); ++it, ++i)
+        as_list[i]= *it;
+    PyObject* pylist= ConvertToPyObject<TVec<T> >::newPyObject(as_list);
+    PyObject* env= PyDict_New();
+    if(0 != PyDict_SetItemString(env, "__builtins__", PyEval_GetBuiltins()))
+        PLERROR("in ConvertToPyObject<std::set<T> >::newPyObject : "
+                "cannot insert builtins in env.");
+    if(0 != PyDict_SetItemString(env, "the_list", pylist))
+        PLERROR("in ConvertToPyObject<std::set<T> >::newPyObject : "
+                "cannot insert the_list in env.");
+    string code= "";
+#if PL_PYTHON_VERSION <= 230
+    code+= "\nfrom sets import Set as set\n";
+#endif // PL_PYTHON_VERSION <= 230
+    code+= "\nresult= set(the_list)\n";
+    PyObject* res= PyRun_String(const_cast<const char*>(code.c_str()),
+                                Py_file_input, env, env);
+    Py_DECREF(pylist);
+    if(!res)
+    {
+        Py_DECREF(env);
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in ConvertToPyObject<std::set<T> >::newPyObject : "
+                "cannot convert to a set.");
+    }
+    Py_DECREF(res);
+    PyObject* setobj= 
+        PythonObjectWrapper(env).as<map<string, PyObject*> >()["result"];
+    Py_INCREF(setobj);
+    Py_DECREF(env);
+    return setobj;
+}
+
 template <class T, class U>
 PyObject* ConvertToPyObject<std::pair<T,U> >::newPyObject(const std::pair<T,U>& data)
 {
@@ -1304,6 +1390,15 @@
         return PythonObjectWrapper::newPyObject();
 }
 
+template <class T>
+PyObject* ConvertToPyObject<std::set<T> const* >::newPyObject(const std::set<T>* data)
+{
+    if(data)
+        return ConvertToPyObject<std::set<T> >::newPyObject(*data);
+    else
+        return PythonObjectWrapper::newPyObject();
+}
+
 PStream& operator>>(PStream& in, PythonObjectWrapper& v);
 DECLARE_TYPE_TRAITS(PythonObjectWrapper);
 

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-14 18:17:21 UTC (rev 7991)
+++ trunk/pymake.config.model	2007-08-14 18:22:44 UTC (rev 7992)
@@ -348,7 +348,7 @@
         python_includedirs   = [ '/usr/include/python'+python_version]
 
         numpy_includedirs   = python_includedirs
-
+        
     python_includedirs= numpy_includedirs
 
     if python_version != pyver:



From nouiz at mail.berlios.de  Tue Aug 14 21:16:40 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 14 Aug 2007 21:16:40 +0200
Subject: [Plearn-commits] r7993 - trunk/python_modules/plearn/learners
Message-ID: <200708141916.l7EJGeFd006659@sheep.berlios.de>

Author: nouiz
Date: 2007-08-14 21:16:40 +0200 (Tue, 14 Aug 2007)
New Revision: 7993

Added:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
Added a new learner that generate do a boosting on 3 classes with 2 standard adaboost

Added: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-08-14 18:22:44 UTC (rev 7992)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-08-14 19:16:40 UTC (rev 7993)
@@ -0,0 +1,122 @@
+from plearn.pyext import *
+from plearn.pyplearn.plargs import *
+
+
+class AdaBoostMultiClasses:
+#class AdaBoost3PLearner(pl.PLearner):
+    def __init__(self,trainSet1,trainSet2):
+        self.trainSet1=trainSet1
+        self.trainSet2=trainSet2
+        self.learner1 = self.myAdaBoostLearner(self.weakLearner(),trainSet1)
+        self.learner1.expdir=plargs.expdirr+"/learner1"
+        self.learner1.setTrainingSet(trainSet1,True)
+        
+        self.learner2 = self.myAdaBoostLearner(self.weakLearner(),trainSet2)
+        self.learner2.expdir=plargs.expdirr+"/learner2"
+        self.learner2.setTrainingSet(trainSet2,True)
+        self.nstages = 0
+        self.stage = 0
+#        self.confusion_target=plargs.confusion_target
+        
+    def weakLearner(self):
+        """ Return a new instance of the weak learner to use"""
+        return pl.RegressionTree(
+            nstages = plargs.subnstages
+            ,loss_function_weight = 1
+            ,missing_is_valid = plargs.missing_is_valid
+            ,multiclass_outputs = plargs.multiclass_output
+            ,maximum_number_of_nodes = 500
+            ,compute_train_stats = 0
+            ,complexity_penalty_factor = 0.0
+            ,verbosity = 0
+            ,report_progress = 1
+            ,forget_when_training_set_changes = 1
+            ,leave_template = pl.RegressionTreeLeave( )
+            )
+    
+    def myAdaBoostLearner(self,sublearner,trainSet):
+        l = pl.AdaBoost()
+        l.weak_learner_template=sublearner
+        l.pseudo_loss_adaboost=True
+        l.weight_by_resampling=plargs.weight_by_resampling
+        l.setTrainingSet(trainSet,True)
+        l.setTrainStatsCollector(VecStatsCollector())
+        return l
+
+    def train(self):
+        self.learner1.nstages = self.nstages
+        self.learner1.train()
+        self.learner2.nstages = self.nstages
+        self.learner2.train()
+        self.stage=self.learner1.stage
+
+    def getTestCostNames(self):
+        costnames = ["class_error","linear_class_error","square_class_error"]
+        #    for i in range(len(conf_matrix)):
+        #        for j in range(len(conf_matrix[i])):
+        for i in range(4):
+            for j in range(3):
+                costnames.append("conf_matrix_%d_%d"%(i,j))
+        return costnames
+
+    def computeOutput(self,example):
+        """ compute the output for the example in the parameter
+        
+        return a tuple: (predicted result, output of sub learner1,output of sub learner2)
+        """
+        out1=self.learner1.computeOutput(example)[0]
+        out2=self.learner2.computeOutput(example)[0]
+        ind1=int(round(out1))
+        ind2=int(round(out2))
+        if ind1==ind2==0:
+            ind=0
+        elif ind1==1 and ind2==0:
+            ind=1
+        elif ind1==ind2==1:
+            ind=2
+        else:
+            ind=3
+        return (ind,out1,out2)
+    
+    def computeCostsFromOutput(self,input,output,target,costs=[]):
+        del costs[:]
+        class_error=int(output[0] != target)
+        linear_class_error=abs(output[0]-target)
+        square_class_error=pow(abs(output[0]-target),2)
+        costs.append(class_error)
+        costs.append(linear_class_error)
+        costs.append(square_class_error)
+        for i in range(4):
+            for j in range(3):
+                costs.append(0)
+        costs[output[0]*3+target+3]=1  
+        return costs
+        
+    def outputsize(self):
+        return len(self.getTestCostNames())
+
+    def save(self,path="",encoding="plearn_ascii"):
+        if not os.path.exists(path):
+            os.mkdir(path)
+        if path:
+            path+="/"
+        else:
+            print "WARNING: AdaBoost3PLearner - no path for saving the learner, we use the current directory"
+        self.learner1.save(path+"learner1_stage#"+str(self.stage)+".psave",encoding)
+        self.learner2.save(path+"learner2_stage#"+str(self.stage)+".psave",encoding)
+    
+    def load_old_learner(self,filepath,stage,trainSet1,trainSet2):
+        print "load_old_learner"
+        self.old_learner1=self.learner1
+        self.old_learner2=self.learner2
+        self.learner1=loadObject(filepath+"/learner1_stage#"+str(stage)+".psave")
+        self.learner2=loadObject(filepath+"/learner2_stage#"+str(stage)+".psave")
+        assert(self.learner1.stage==self.learner2.stage)
+        self.stage=self.learner1.stage
+        self.nstages=self.learner1.nstages
+#        self.learner1.expdir=plargs.expdirr+"/learner1"
+        self.learner1.setTrainingSet(trainSet1,False)
+        self.learner2.setTrainingSet(trainSet2,False)
+        print self.stage
+        print self.stage
+        



From nouiz at mail.berlios.de  Wed Aug 15 15:36:10 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Aug 2007 15:36:10 +0200
Subject: [Plearn-commits] r7994 - trunk/plearn/math
Message-ID: <200708151336.l7FDaAsN021116@sheep.berlios.de>

Author: nouiz
Date: 2007-08-15 15:36:09 +0200 (Wed, 15 Aug 2007)
New Revision: 7994

Modified:
   trunk/plearn/math/VecStatsCollector.cc
   trunk/plearn/math/VecStatsCollector.h
Log:
-forward VecStatsCollector.update(vec,real) to python
-need to find how to forward function with multiple signature


Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-08-14 19:16:40 UTC (rev 7993)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-08-15 13:36:09 UTC (rev 7994)
@@ -218,6 +218,12 @@
         (BodyDoc("Returns the number of statistics collected.\n"),
          RetDoc ("=stats.length()")));
 
+   declareMethod(
+        rmm, "update", &VecStatsCollector::remote_update,
+        (BodyDoc("Update the stats with gived data.\n"),
+         ArgDoc ("x"," the new data\n"),
+         ArgDoc ("weight"," the weight of the data")));
+
  }
 
 int VecStatsCollector::length() const
@@ -392,6 +398,11 @@
     }
 }
 
+void VecStatsCollector::remote_update(const Vec& x, real weight)
+{
+    update(x,weight);
+}
+
 bool VecStatsCollector::shouldUpdateWindow(const Vec& x)
 {
     // Avoid dealing with missings if not necessary

Modified: trunk/plearn/math/VecStatsCollector.h
===================================================================
--- trunk/plearn/math/VecStatsCollector.h	2007-08-14 19:16:40 UTC (rev 7993)
+++ trunk/plearn/math/VecStatsCollector.h	2007-08-15 13:36:09 UTC (rev 7994)
@@ -163,6 +163,10 @@
 
     //! clears all previously accumulated statistics
     virtual void forget();
+    
+    //! updates the statistics when seeing x
+    //! The weight applies to all elements of x
+    virtual void remote_update(const Vec& x, real weight = 1.0);
 
     //! updates the statistics when seeing x
     //! The weight applies to all elements of x



From nouiz at mail.berlios.de  Wed Aug 15 15:41:42 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Aug 2007 15:41:42 +0200
Subject: [Plearn-commits] r7995 - trunk/plearn_learners/meta
Message-ID: <200708151341.l7FDfgBo021758@sheep.berlios.de>

Author: nouiz
Date: 2007-08-15 15:41:42 +0200 (Wed, 15 Aug 2007)
New Revision: 7995

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
bugfix for serialization


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-08-15 13:36:09 UTC (rev 7994)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-08-15 13:41:42 UTC (rev 7995)
@@ -119,6 +119,14 @@
                   OptionBase::learntoption,
                   "Initial sum of weights on the examples. Do not temper with.\n");
 
+    declareOption(ol, "example_weights", &AdaBoost::example_weights,
+                  OptionBase::learntoption,
+                  "The current weights of the examples.\n");
+
+    declareOption(ol, "learners_error", &AdaBoost::learners_error,
+                  OptionBase::learntoption,
+                  "The error of each learners.\n");
+
     declareOption(ol, "weak_learner_template", &AdaBoost::weak_learner_template,
                   OptionBase::buildoption,
                   "Template for the regression weak learner to be"
@@ -197,6 +205,11 @@
                   OptionBase::learntoption,
                   "A sorted train set when using the class RegressionTree as a base regressor\n");
 
+    declareOption(ol, "weak_learner_output",
+                  &AdaBoost::weak_learner_output,
+                  OptionBase::nosave,
+                  "A temp vector that contain the weak learner output\n");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -252,7 +265,7 @@
     if(!train_set)
         PLERROR("In AdaBoost::train, you did not setTrainingSet");
     
-    if(!train_stats)
+    if(!train_stats and compute_training_error)
         PLERROR("In AdaBoost::train, you did not setTrainStatsCollector");
 
     if (train_set->targetsize()!=1)



From nouiz at mail.berlios.de  Wed Aug 15 22:41:35 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 15 Aug 2007 22:41:35 +0200
Subject: [Plearn-commits] r7996 - trunk/plearn_learners/generic
Message-ID: <200708152041.l7FKfZur007827@sheep.berlios.de>

Author: nouiz
Date: 2007-08-15 22:41:34 +0200 (Wed, 15 Aug 2007)
New Revision: 7996

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/generic/AddCostToLearner.h
Log:
Add a test cost: train_time.


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-15 13:41:42 UTC (rev 7995)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-15 20:41:34 UTC (rev 7996)
@@ -252,6 +252,7 @@
         } else if (c == "squared_norm_reconstruction_error") {
         } else if (c == "class_error") {
         } else if (c == "binary_class_error") {
+        } else if (c == "train_time") {
         } else if (c == "linear_class_error") {
         } else if (c == "square_class_error") {
         } else if (c == "confusion_matrix") {
@@ -268,6 +269,7 @@
     if (n > 0 && display) {
         cout << endl;
     }
+    Profiler::activate();
 }
 
 /////////////////////////////
@@ -576,6 +578,8 @@
             PLWARNING("In AddCostToLearner::computeCostsFromOutputs - 'squared_norm_reconstruction_error'"
                       " has not been tested yet, please remove this warning if it works correctly");
             costs[ind_cost] = abs(pownorm(input, 2) - pownorm(sub_learner_output, 2));
+        } else if (c == "train_time") {
+            costs[ind_cost] = train_time;
         } else {
             PLERROR("In AddCostToLearner::computeCostsFromOutputs - Unknown cost");
         }
@@ -587,7 +591,7 @@
 ///////////
 void AddCostToLearner::train()
 {
-    Profiler::pl_profile_start("AddCostToLearner::train");
+    Profiler::start("AddCostToLearner::train");
 
     int find_threshold = -1;
     if(find_class_threshold){
@@ -655,8 +659,9 @@
                 cout << "class_threshold[" << i << "] = " <<class_threshold[i] << endl;
 
     }
-    Profiler::pl_profile_end("AddCostToLearner::train");
-
+    Profiler::end("AddCostToLearner::train");
+    const Profiler::Stats& stats = Profiler::getStats("AddCostToLearner::train");
+    train_time=stats.wall_duration/Profiler::ticksPerSecond();
 }
 
 ///////////////////////////

Modified: trunk/plearn_learners/generic/AddCostToLearner.h
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.h	2007-08-15 13:41:42 UTC (rev 7995)
+++ trunk/plearn_learners/generic/AddCostToLearner.h	2007-08-15 20:41:34 UTC (rev 7996)
@@ -107,6 +107,8 @@
     //! The threshold between class
     Vec class_threshold;
 
+    //! The total time passed in training
+    real train_time;
 public:
 
     // ************************



From saintmlx at mail.berlios.de  Wed Aug 15 23:54:20 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 15 Aug 2007 23:54:20 +0200
Subject: [Plearn-commits] r7997 - in trunk/plearn: base python
Message-ID: <200708152154.l7FLsKO8012255@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-15 23:54:18 +0200 (Wed, 15 Aug 2007)
New Revision: 7997

Modified:
   trunk/plearn/base/RemoteDeclareMethod.h
   trunk/plearn/base/RemoteMethodMap.cc
   trunk/plearn/base/RemoteMethodMap.h
   trunk/plearn/base/RemoteTrampoline.cc
   trunk/plearn/base/RemoteTrampoline.h
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/PythonCodeSnippet.h
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- added flags to RemoteTrampolines
- 'nopython' flag for trampolines, to avoid injecting some functions into python
- allow construction of PythonCodeSnippets from already existing python instances (no code)



Modified: trunk/plearn/base/RemoteDeclareMethod.h
===================================================================
--- trunk/plearn/base/RemoteDeclareMethod.h	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/base/RemoteDeclareMethod.h	2007-08-15 21:54:18 UTC (rev 7997)
@@ -70,12 +70,13 @@
 template <class R>
 inline void declareFunction(const string& funcname,
                             R (*func)(),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_0<R> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 
@@ -84,12 +85,13 @@
 template <class R, class A1>
 inline void declareFunction(const string& funcname,
                             R (*func)(A1),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_1<R,A1> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 
@@ -98,12 +100,13 @@
 template <class R, class A1, class A2>
 inline void declareFunction(const string& funcname,
                             R (*func)(A1,A2),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_2<R,A1,A2> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 
@@ -112,12 +115,13 @@
 template <class R, class A1, class A2, class A3>
 inline void declareFunction(const string& funcname,
                             R (*func)(A1,A2,A3),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_3<R,A1,A2,A3> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 
@@ -126,12 +130,13 @@
 template <class R, class A1, class A2, class A3, class A4>
 inline void declareFunction(const string& funcname,
                             R (*func)(A1,A2,A3,A4),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_4<R,A1,A2,A3,A4> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 //#####  5 Arguments  #########################################################
@@ -139,12 +144,13 @@
 template <class R, class A1, class A2, class A3, class A4, class A5>
 inline void declareFunction(const string& funcname,
                             R (*func)(A1,A2,A3,A4,A5),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_5<R,A1,A2,A3,A4,A5> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 //#####  6 Arguments  #########################################################
@@ -152,12 +158,13 @@
 template <class R, class A1, class A2, class A3, class A4, class A5, class A6>
 inline void declareFunction(const string& funcname,
                             R (*func)(A1,A2,A3,A4,A5,A6),
-                            const RemoteMethodDoc& doc)
+                            const RemoteMethodDoc& doc,
+                            const RemoteTrampoline::flag_t& flgs= 0)
 {
     RemoteMethodMap& rmm = getGlobalFunctionMap();
     typedef FRemoteTrampoline_6<R,A1,A2,A3,A4,A5,A6> Trampoline;
     rmm.insert(funcname, Trampoline::expected_nargs,
-               new Trampoline(funcname, doc, func));
+               new Trampoline(funcname, doc, func, flgs));
 }
 
 
@@ -171,11 +178,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_0<T,R> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -183,11 +191,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)() const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_0<T,R> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 
@@ -198,11 +207,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_1<T,R,A1> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -210,11 +220,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1) const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_1<T,R,A1> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 
@@ -225,11 +236,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_2<T,R,A1,A2> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -237,11 +249,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2) const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_2<T,R,A1,A2> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 
@@ -252,11 +265,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_3<T,R,A1,A2,A3> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -264,11 +278,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3) const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_3<T,R,A1,A2,A3> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 
@@ -279,11 +294,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3,A4),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_4<T,R,A1,A2,A3,A4> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -291,11 +307,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3,A4) const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_4<T,R,A1,A2,A3,A4> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 
@@ -306,11 +323,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3,A4,A5),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_5<T,R,A1,A2,A3,A4,A5> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -318,11 +336,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3,A4,A5) const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_5<T,R,A1,A2,A3,A4,A5> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 
@@ -333,11 +352,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3,A4,A5,A6),
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_6<T,R,A1,A2,A3,A4,A5,A6> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 // Const method
@@ -345,11 +365,12 @@
 inline void declareMethod(RemoteMethodMap& rmm,
                           const string& methodname,
                           R (T::*method)(A1,A2,A3,A4,A5,A6) const,
-                          const RemoteMethodDoc& doc)
+                          const RemoteMethodDoc& doc,
+                          const RemoteTrampoline::flag_t& flgs= 0)
 {
     typedef RemoteTrampoline_6<T,R,A1,A2,A3,A4,A5,A6> Trampoline;
     rmm.insert(methodname, Trampoline::expected_nargs,
-               new Trampoline(methodname, doc, METHOD_UNCONST(method)));
+               new Trampoline(methodname, doc, METHOD_UNCONST(method), flgs));
 }
 
 

Modified: trunk/plearn/base/RemoteMethodMap.cc
===================================================================
--- trunk/plearn/base/RemoteMethodMap.cc	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/base/RemoteMethodMap.cc	2007-08-15 21:54:18 UTC (rev 7997)
@@ -41,7 +41,6 @@
 namespace PLearn {
 using namespace std;
 
-
 //#####  RemoteMethodMap Implementation  ######################################
 
 RemoteMethodMap::~RemoteMethodMap()
@@ -117,7 +116,6 @@
     return txt;
 }
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn/base/RemoteMethodMap.h
===================================================================
--- trunk/plearn/base/RemoteMethodMap.h	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/base/RemoteMethodMap.h	2007-08-15 21:54:18 UTC (rev 7997)
@@ -155,7 +155,7 @@
     //! registered methods with that name (whatever their arity),
     //! or return the string "** No method named ... **"
     string getMethodHelpText(const string& methodname, int arity=-1) const;
-    
+
     //! Get the method map itself
     const MethodMap& getMap() const
     { return m_methods; }

Modified: trunk/plearn/base/RemoteTrampoline.cc
===================================================================
--- trunk/plearn/base/RemoteTrampoline.cc	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/base/RemoteTrampoline.cc	2007-08-15 21:54:18 UTC (rev 7997)
@@ -44,6 +44,8 @@
 
 //#####  RemoteTrampoline  ####################################################
 
+const RemoteTrampoline::flag_t RemoteTrampoline::nopython= 1;
+
 void RemoteTrampoline::checkNargs(int nargs, int expected_nargs) const
 {
     if (nargs != expected_nargs)

Modified: trunk/plearn/base/RemoteTrampoline.h
===================================================================
--- trunk/plearn/base/RemoteTrampoline.h	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/base/RemoteTrampoline.h	2007-08-15 21:54:18 UTC (rev 7997)
@@ -105,10 +105,17 @@
  */
 struct RemoteTrampoline : public PPointable
 {
+    //! type for remote method flags
+    typedef unsigned int flag_t;
+    //! nopython flag: do not inject this method in python
+    static const flag_t nopython;
+
     //! Constructor takes the methodname and some documentation.
-    RemoteTrampoline(const string& methodname, const RemoteMethodDoc& doc)
+    RemoteTrampoline(const string& methodname, const RemoteMethodDoc& doc,
+                     const flag_t& flgs= 0)
         : m_methodname(methodname),
-          m_documentation(doc)
+          m_documentation(doc),
+          m_flags(flgs)
     {
         m_documentation.setName(methodname);
         m_documentation.checkConsistency();
@@ -119,6 +126,9 @@
     {
         return m_documentation;
     }
+
+    //! Flags accessor
+    const flag_t& flags() const { return m_flags; }
     
     /**
      *  Perform the act of binding arguments on a stream with an object
@@ -225,6 +235,9 @@
 
     //! Documentation associated with the method
     RemoteMethodDoc m_documentation;
+
+    //! Flags associated with this method
+    flag_t m_flags;
 };
 
 //#####  Zero Arguments  ######################################################
@@ -240,9 +253,10 @@
     typedef R (T::*MethodType)();
     
     RemoteTrampoline_0(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
-                                 RTYPE_DOC(R))),
+                                 RTYPE_DOC(R)),
+                    flgs),
           m_method(m)
     { }
 
@@ -277,9 +291,10 @@
     typedef void (T::*MethodType)();
     
     RemoteTrampoline_0(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
-                                 RTYPE_DOC(void))),
+                                 RTYPE_DOC(void)),
+                    flgs),
           m_method(m)
     { }
 
@@ -317,10 +332,11 @@
     typedef R (T::*MethodType)(A1);
     
     RemoteTrampoline_1(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(R),
-                                 ATYPE_DOC(A1))),
+                                 ATYPE_DOC(A1)),
+                    flgs),
           m_method(m)
     { }
 
@@ -357,10 +373,11 @@
     typedef void (T::*MethodType)(A1);
     
     RemoteTrampoline_1(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(void),
-                                 ATYPE_DOC(A1))),
+                                 ATYPE_DOC(A1)),
+                    flgs),
           m_method(m)
     { }
 
@@ -400,10 +417,11 @@
     typedef R (T::*MethodType)(A1,A2);
     
     RemoteTrampoline_2(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(R),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2)),
+                    flgs),
           m_method(m)
     { }
 
@@ -442,10 +460,11 @@
     typedef void (T::*MethodType)(A1,A2);
     
     RemoteTrampoline_2(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(void),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2)),
+                    flgs),
           m_method(m)
     { }
 
@@ -487,10 +506,11 @@
     typedef R (T::*MethodType)(A1,A2,A3);
     
     RemoteTrampoline_3(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(R),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3)),
+                    flgs),
           m_method(m)
     { }
 
@@ -531,10 +551,11 @@
     typedef void (T::*MethodType)(A1,A2,A3);
     
     RemoteTrampoline_3(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(void),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3)),
+                    flgs),
           m_method(m)
     { }
 
@@ -578,11 +599,12 @@
     typedef R (T::*MethodType)(A1,A2,A3,A4);
     
     RemoteTrampoline_4(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(R),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4))),
+                                 ATYPE_DOC(A4)),
+                    flgs),
           m_method(m)
     { }
 
@@ -625,11 +647,12 @@
     typedef void (T::*MethodType)(A1,A2,A3,A4);
     
     RemoteTrampoline_4(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(void),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4))),
+                                 ATYPE_DOC(A4)),
+                    flgs),
           m_method(m)
     { }
 
@@ -675,11 +698,12 @@
     typedef R (T::*MethodType)(A1,A2,A3,A4,A5);
     
     RemoteTrampoline_5(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(R),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5)),
+                    flgs),
           m_method(m)
     { }
 
@@ -724,11 +748,12 @@
     typedef void (T::*MethodType)(A1,A2,A3,A4,A5);
     
     RemoteTrampoline_5(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(void),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5)),
+                    flgs),
           m_method(m)
     { }
 
@@ -776,11 +801,12 @@
     typedef R (T::*MethodType)(A1,A2,A3,A4,A5,A6);
     
     RemoteTrampoline_6(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(R),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6)),
+                    flgs),
           m_method(m)
     { }
 
@@ -827,11 +853,12 @@
     typedef void (T::*MethodType)(A1,A2,A3,A4,A5,A6);
     
     RemoteTrampoline_6(const string& methodname, const RemoteMethodDoc& doc,
-                       MethodType m)
+                       MethodType m, const flag_t& flgs= 0)
         : inherited(methodname, (doc,
                                  RTYPE_DOC(void),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6)),
+                    flgs),
           m_method(m)
     { }
 
@@ -888,9 +915,10 @@
     typedef R (*FunctionType)();
     
     FRemoteTrampoline_0(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
-                                 RTYPE_DOC(R))),
+                                 RTYPE_DOC(R)),
+                    flgs),
           m_function(m)
     { }
 
@@ -926,8 +954,8 @@
     typedef void (*FunctionType)();
     
     FRemoteTrampoline_0(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
-        : inherited(functionname, (doc, RetTypeDoc("void"))),
+                       FunctionType m, const flag_t& flgs= 0)
+        : inherited(functionname, (doc, RetTypeDoc("void")), flgs),
           m_function(m)
     { }
 
@@ -966,10 +994,11 @@
     typedef R (*FunctionType)(A1);
     
     FRemoteTrampoline_1(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(R),
-                                 ATYPE_DOC(A1))),
+                                 ATYPE_DOC(A1)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1007,10 +1036,11 @@
     typedef void (*FunctionType)(A1);
     
     FRemoteTrampoline_1(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(void),
-                                 ATYPE_DOC(A1))),
+                                 ATYPE_DOC(A1)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1051,10 +1081,11 @@
     typedef R (*FunctionType)(A1,A2);
     
     FRemoteTrampoline_2(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(R),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1094,10 +1125,11 @@
     typedef void (*FunctionType)(A1,A2);
     
     FRemoteTrampoline_2(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(void),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1140,10 +1172,11 @@
     typedef R (*FunctionType)(A1,A2,A3);
     
     FRemoteTrampoline_3(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(R),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1185,10 +1218,11 @@
     typedef void (*FunctionType)(A1,A2,A3);
     
     FRemoteTrampoline_3(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                        FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(void),
-                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3))),
+                                 ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1233,11 +1267,12 @@
     typedef R (*FunctionType)(A1,A2,A3,A4);
     
     FRemoteTrampoline_4(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(R),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4))),
+                                 ATYPE_DOC(A4)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1281,11 +1316,12 @@
     typedef void (*FunctionType)(A1,A2,A3,A4);
     
     FRemoteTrampoline_4(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                       FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(void),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4))),
+                                 ATYPE_DOC(A4)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1332,11 +1368,12 @@
     typedef R (*FunctionType)(A1,A2,A3,A4,A5);
     
     FRemoteTrampoline_5(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                        FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(R),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1382,11 +1419,12 @@
     typedef void (*FunctionType)(A1,A2,A3,A4,A5);
     
     FRemoteTrampoline_5(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                        FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(void),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1435,11 +1473,12 @@
     typedef R (*FunctionType)(A1,A2,A3,A4,A5,A6);
     
     FRemoteTrampoline_6(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                        FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(R),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6)),
+                    flgs),
           m_function(m)
     { }
 
@@ -1487,11 +1526,12 @@
     typedef void (*FunctionType)(A1,A2,A3,A4,A5,A6);
     
     FRemoteTrampoline_6(const string& functionname, const RemoteMethodDoc& doc,
-                       FunctionType m)
+                        FunctionType m, const flag_t& flgs= 0)
         : inherited(functionname, (doc,
                                  RTYPE_DOC(void),
                                  ATYPE_DOC(A1), ATYPE_DOC(A2), ATYPE_DOC(A3),
-                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6))),
+                                 ATYPE_DOC(A4), ATYPE_DOC(A5), ATYPE_DOC(A6)),
+                    flgs),
           m_function(m)
     { }
 

Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2007-08-15 21:54:18 UTC (rev 7997)
@@ -123,6 +123,23 @@
 }
 
 
+PythonCodeSnippet::PythonCodeSnippet(const PythonObjectWrapper& instance,
+                                     bool remap_python_exceptions)
+    : inherited(),
+      m_code(""),
+      m_remap_python_exceptions(remap_python_exceptions),
+      m_instance_params(),
+      m_instance(instance),
+      m_handle(this),
+      m_compiled_code(),
+      m_injected_functions(4),
+      m_python_methods(4)
+{
+    PyObject* compiled_code= 
+        PyObject_GetAttrString(m_instance.getPyObject(), "__dict__");
+    m_compiled_code= PythonObjectWrapper(compiled_code, PythonObjectWrapper::transfer_ownership);
+    // NOTE: build() not called
+}
 
 void PythonCodeSnippet::declareOptions(OptionList& ol)
 {
@@ -179,7 +196,8 @@
         sprintf(set_current_snippet, SetCurrentSnippetVar, m_handle);
         m_compiled_code = compileGlobalCode(InjectSetupSnippet+
                                             string(set_current_snippet)+m_code);
-        resetCurrentSnippet();
+        if(m_instance.isNull())
+            resetCurrentSnippet();
     }
 
     // Forget about injected functions
@@ -214,8 +232,13 @@
 PythonCodeSnippet::getGlobalObject(const string& object_name) const
 {
     PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pyobj = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           object_name.c_str());
+    PyObject* pyobj;
+    if(!m_instance.isNull())
+        pyobj= PyObject_GetAttrString(m_instance.getPyObject(),
+                                      const_cast<char*>(object_name.c_str()));
+    else
+        pyobj= PyDict_GetItemString(m_compiled_code.getPyObject(),
+                                    object_name.c_str());
     if (pyobj) {
         // pyobj == borrowed reference
         // Increment refcount to keep long-lived reference
@@ -232,7 +255,11 @@
 
     // Note that PyDict_SetItemString increments the reference count for us
     int non_success = 0;
-    if (! pow.isNull())
+    if(!m_instance.isNull())
+        non_success= PyObject_SetAttrString(m_instance.getPyObject(),
+                                            const_cast<char*>(object_name.c_str()),
+                                            pow.getPyObject());
+    else if (! pow.isNull())
         non_success = PyDict_SetItemString(m_compiled_code.getPyObject(),
                                            object_name.c_str(),
                                            pow.getPyObject());
@@ -301,7 +328,8 @@
 
     PyObject* return_value = 0;
     if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
+        if(!instance_method)
+            setCurrentSnippet(m_handle);
 
         return_value = PyObject_CallObject(pFunc, NULL);
         if (! return_value)
@@ -313,7 +341,8 @@
                                + "' with no params.");
         }
 
-        resetCurrentSnippet();
+        if(!instance_method)
+            resetCurrentSnippet();
     }
     else
     {
@@ -357,7 +386,8 @@
 
     PyObject* return_value = 0;
     if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
+        if(!instance_method)
+            setCurrentSnippet(m_handle);
 
         // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
         PyObject* pArgs = PyTuple_New(args.size());
@@ -380,7 +410,8 @@
                                + tostring(args.length())
                                + " params.");
         }
-        resetCurrentSnippet();
+        if(!instance_method)
+            resetCurrentSnippet();
     }
     else
     {
@@ -678,7 +709,8 @@
         Py_XDECREF(m_compiled_code.getPyObject());
         PyErr_Print();
         PLERROR("PythonCodeSnippet::setCurrentSnippet: error compiling "
-                "Python code contained in the 'SetCurrentSnippetVar'.");
+                "Python code contained in the 'SetCurrentSnippetVar'."
+                "\n\t'%s'", set_current_snippet);
     }
 }
 

Modified: trunk/plearn/python/PythonCodeSnippet.h
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.h	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/python/PythonCodeSnippet.h	2007-08-15 21:54:18 UTC (rev 7997)
@@ -160,6 +160,9 @@
     PythonCodeSnippet(const string& code = "",
                       bool remap_python_exceptions = false);
 
+    PythonCodeSnippet(const PythonObjectWrapper& instance,
+                      bool remap_python_exceptions = false);
+
     //! Default copy ctor, assignment op, dtor
 
 
@@ -373,44 +376,6 @@
     TVec<PythonObjectWrapper> args(1);
     args[0]= PythonObjectWrapper(arg1);
     return invoke(function_name, args);
-/*
-
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(1);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-
-        if (! (py_arg1)) {
-            Py_XDECREF(py_arg1);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else {
-        PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
-                function_name);
-    }
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 
@@ -424,45 +389,6 @@
     args[0]= PythonObjectWrapper(arg1);
     args[1]= PythonObjectWrapper(arg2);
     return invoke(function_name, args);
-/*
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(2);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-        PyObject* py_arg2 = PythonObjectWrapper::newPyObject(arg2);
-
-        if (! (py_arg1 && py_arg2)) {
-            Py_XDECREF(py_arg1);
-            Py_XDECREF(py_arg2);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-        PyTuple_SetItem(pArgs, 1, py_arg2);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else
-        PLERROR("PythonCodeSnippet::invoke: cannot invoke function '%s'",
-                function_name);
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 
@@ -478,49 +404,6 @@
     args[1]= PythonObjectWrapper(arg2);
     args[2]= PythonObjectWrapper(arg3);
     return invoke(function_name, args);
-
-/*
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(3);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-        PyObject* py_arg2 = PythonObjectWrapper::newPyObject(arg2);
-        PyObject* py_arg3 = PythonObjectWrapper::newPyObject(arg3);
-
-        if (! (py_arg1 && py_arg2 && py_arg3)) {
-            Py_XDECREF(py_arg1);
-            Py_XDECREF(py_arg2);
-            Py_XDECREF(py_arg3);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-        PyTuple_SetItem(pArgs, 1, py_arg2);
-        PyTuple_SetItem(pArgs, 2, py_arg3);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else
-        PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
-                function_name);
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 
@@ -538,51 +421,6 @@
     args[2]= PythonObjectWrapper(arg3);
     args[3]= PythonObjectWrapper(arg4);
     return invoke(function_name, args);
-/*
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(4);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-        PyObject* py_arg2 = PythonObjectWrapper::newPyObject(arg2);
-        PyObject* py_arg3 = PythonObjectWrapper::newPyObject(arg3);
-        PyObject* py_arg4 = PythonObjectWrapper::newPyObject(arg4);
-
-        if (! (py_arg1 && py_arg2 && py_arg3 && py_arg4)) {
-            Py_XDECREF(py_arg1);
-            Py_XDECREF(py_arg2);
-            Py_XDECREF(py_arg3);
-            Py_XDECREF(py_arg4);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-        PyTuple_SetItem(pArgs, 1, py_arg2);
-        PyTuple_SetItem(pArgs, 2, py_arg3);
-        PyTuple_SetItem(pArgs, 3, py_arg4);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else
-        PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
-                function_name);
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 template <class T, class U, class V, class W, class X>
@@ -601,54 +439,6 @@
     args[3]= PythonObjectWrapper(arg4);
     args[4]= PythonObjectWrapper(arg5);
     return invoke(function_name, args);
-/*
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(5);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-        PyObject* py_arg2 = PythonObjectWrapper::newPyObject(arg2);
-        PyObject* py_arg3 = PythonObjectWrapper::newPyObject(arg3);
-        PyObject* py_arg4 = PythonObjectWrapper::newPyObject(arg4);
-        PyObject* py_arg5 = PythonObjectWrapper::newPyObject(arg5);
-
-        if (! (py_arg1 && py_arg2 && py_arg3 && py_arg4 && py_arg5)) {
-            Py_XDECREF(py_arg1);
-            Py_XDECREF(py_arg2);
-            Py_XDECREF(py_arg3);
-            Py_XDECREF(py_arg4);
-            Py_XDECREF(py_arg5);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-        PyTuple_SetItem(pArgs, 1, py_arg2);
-        PyTuple_SetItem(pArgs, 2, py_arg3);
-        PyTuple_SetItem(pArgs, 3, py_arg4);
-        PyTuple_SetItem(pArgs, 4, py_arg5);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else
-        PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
-                function_name);
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 template <class T, class U, class V, class W, class X, class Y>
@@ -669,58 +459,6 @@
     args[4]= PythonObjectWrapper(arg5);
     args[5]= PythonObjectWrapper(arg6);
     return invoke(function_name, args);
-/*
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(6);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-        PyObject* py_arg2 = PythonObjectWrapper::newPyObject(arg2);
-        PyObject* py_arg3 = PythonObjectWrapper::newPyObject(arg3);
-        PyObject* py_arg4 = PythonObjectWrapper::newPyObject(arg4);
-        PyObject* py_arg5 = PythonObjectWrapper::newPyObject(arg5);
-        PyObject* py_arg6 = PythonObjectWrapper::newPyObject(arg6);
-
-        if (! (py_arg1 && py_arg2 && py_arg3 && py_arg4 && py_arg5 &&
-               py_arg6 )) {
-            Py_XDECREF(py_arg1);
-            Py_XDECREF(py_arg2);
-            Py_XDECREF(py_arg3);
-            Py_XDECREF(py_arg4);
-            Py_XDECREF(py_arg5);
-            Py_XDECREF(py_arg6);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-        PyTuple_SetItem(pArgs, 1, py_arg2);
-        PyTuple_SetItem(pArgs, 2, py_arg3);
-        PyTuple_SetItem(pArgs, 3, py_arg4);
-        PyTuple_SetItem(pArgs, 4, py_arg5);
-        PyTuple_SetItem(pArgs, 5, py_arg6);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else
-        PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
-                function_name);
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 
@@ -744,61 +482,6 @@
     args[5]= PythonObjectWrapper(arg6);
     args[6]= PythonObjectWrapper(arg7);
     return invoke(function_name, args);
-/*
-    PythonGlobalInterpreterLock gil;         // For thread-safety
-    PyObject* pFunc = PyDict_GetItemString(m_compiled_code.getPyObject(),
-                                           function_name);
-    // pFunc: Borrowed reference
-
-    PyObject* return_value = 0;
-    if (pFunc && PyCallable_Check(pFunc)) {
-        setCurrentSnippet(m_handle);
-
-        // Create argument tuple.  Warning: PyTuple_SetItem STEALS references.
-        PyObject* pArgs = PyTuple_New(7);
-        PyObject* py_arg1 = PythonObjectWrapper::newPyObject(arg1);
-        PyObject* py_arg2 = PythonObjectWrapper::newPyObject(arg2);
-        PyObject* py_arg3 = PythonObjectWrapper::newPyObject(arg3);
-        PyObject* py_arg4 = PythonObjectWrapper::newPyObject(arg4);
-        PyObject* py_arg5 = PythonObjectWrapper::newPyObject(arg5);
-        PyObject* py_arg6 = PythonObjectWrapper::newPyObject(arg6);
-        PyObject* py_arg7 = PythonObjectWrapper::newPyObject(arg7);
-
-        if (! (py_arg1 && py_arg2 && py_arg3 && py_arg4 && py_arg5 &&
-               py_arg6 && py_arg7 )) {
-            Py_XDECREF(py_arg1);
-            Py_XDECREF(py_arg2);
-            Py_XDECREF(py_arg3);
-            Py_XDECREF(py_arg4);
-            Py_XDECREF(py_arg5);
-            Py_XDECREF(py_arg6);
-            Py_XDECREF(py_arg7);
-            PLERROR("PythonCodeSnippet::invoke: error during argument conversion "
-                    "from C++ to Python for function '%s'", function_name);
-        }
-
-        PyTuple_SetItem(pArgs, 0, py_arg1);
-        PyTuple_SetItem(pArgs, 1, py_arg2);
-        PyTuple_SetItem(pArgs, 2, py_arg3);
-        PyTuple_SetItem(pArgs, 3, py_arg4);
-        PyTuple_SetItem(pArgs, 4, py_arg5);
-        PyTuple_SetItem(pArgs, 5, py_arg6);
-        PyTuple_SetItem(pArgs, 6, py_arg7);
-
-        return_value = PyObject_CallObject(pFunc, pArgs);
-
-        Py_XDECREF(pArgs);
-        if (! return_value)
-            handlePythonErrors();
-
-        resetCurrentSnippet();
-    }
-    else
-        PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
-                function_name);
-
-    return PythonObjectWrapper(return_value);
-*/
 }
 
 

Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/python/PythonExtension.cc	2007-08-15 21:54:18 UTC (rev 7997)
@@ -304,11 +304,17 @@
         TVec<string>& methods_help= 
             PythonObjectWrapper::m_pypl_classes.find(classname)->second.methods_help;
 
+        set<pair<string, int> > methods_done;
         while(methods)
         {
             for(RemoteMethodMap::MethodMap::const_iterator it= methods->begin();
                 it != methods->end(); ++it)
             {
+                //skip methods already injected, or not to inject
+                if(methods_done.find(it->first) != methods_done.end()) continue;
+                methods_done.insert(it->first);
+                if(it->second->flags() & RemoteTrampoline::nopython) continue;
+
                 //get the RemoteTrampoline
                 PyObject* tramp= PyCObject_FromVoidPtr(it->second, NULL);
             

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-08-15 21:54:18 UTC (rev 7997)
@@ -683,10 +683,87 @@
 
 PStream& operator>>(PStream& in, PythonObjectWrapper& v)
 {
-    PLERROR("Attempting to read a PythonObjectWrapper from a stream : not supported");
+    PLERROR("operator>>(PStream&, PythonObjectWrapper&) : "
+            "not supported (yet).");
+/*
+    string s;
+    in >> s;
+    string sub= "PythonObjectWrapper(ownership=";
+    if(s.substr(0,sub.length()) != sub)
+        PLERROR("in operator>>(PStream& in, PythonObjectWrapper& v) : "
+                "expected '%s' but got '%s'.",
+                sub.c_str(), s.c_str());
+    s= s.substr(sub.length());
+    v.m_ownership= static_cast<PythonObjectWrapper::OwnershipMode>(s[0]-'0');
+    s= s.substr(1);
+    sub= ", object=";
+    if(s.substr(0,sub.length()) != sub)
+        PLERROR("in operator>>(PStream& in, PythonObjectWrapper& v) : "
+                "expected '%s' but got '%s'.",
+                sub.c_str(), s.c_str());
+    s= s.substr(sub.length());
+    PStream sin= openString(s, PStream::plearn_ascii, "r");
+    string pickle;
+    sin >> pickle;
+
+    PyObject* pypickle= PyString_FromString(pickle.c_str());
+    PyObject* env= PyDict_New();
+    if(0 != PyDict_SetItemString(env, "__builtins__", PyEval_GetBuiltins()))
+        PLERROR("in operator>>(PStream&, PythonObjectWrapper& v) : "
+                "cannot insert builtins in env.");
+    if(0 != PyDict_SetItemString(env, "the_pickle", pypickle))
+        PLERROR("in operator>>(PStream&, PythonObjectWrapper& v) : "
+                "cannot insert the_pickle in env.");
+    Py_DECREF(pypickle);
+    PyObject* res= PyRun_String("\nfrom cPickle import *\nresult= loads(the_pickle)\n", 
+                                Py_file_input, env, env);
+    if(!res)
+    {
+        Py_DECREF(env);
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in operator<<(PStream&, const PythonObjectWrapper& v) : "
+                "cannot unpickle python object '%s'.",pickle.c_str());
+    }
+    Py_DECREF(res);
+    v.m_object= 
+        PythonObjectWrapper(env).as<std::map<string, PyObject*> >()["result"];
+    Py_INCREF(v.m_object);
+    Py_DECREF(env);
+*/
     return in;
 }
 
+PStream& operator<<(PStream& out, const PythonObjectWrapper& v)
+{
+    PLERROR("operator<<(PStream&, const PythonObjectWrapper&) : "
+            "not supported (yet).");
+/*
+    PyObject* env= PyDict_New();
+    if(0 != PyDict_SetItemString(env, "__builtins__", PyEval_GetBuiltins()))
+        PLERROR("in operator<<(PStream&, const PythonObjectWrapper& v) : "
+                "cannot insert builtins in env.");
+    if(0 != PyDict_SetItemString(env, "the_obj", v.m_object))
+        PLERROR("in operator<<(PStream&, const PythonObjectWrapper& v) : "
+                "cannot insert the_obj in env.");
+    PyObject* res= PyRun_String("\nfrom cPickle import *\nresult= dumps(the_obj)\n", 
+                                Py_file_input, env, env);
+    if(!res)
+    {
+        Py_DECREF(env);
+        if(PyErr_Occurred()) PyErr_Print();
+        PLERROR("in operator<<(PStream&, const PythonObjectWrapper& v) : "
+                "cannot pickle python object.");
+    }
+    Py_DECREF(res);
+    string pickle= 
+        PythonObjectWrapper(env).as<std::map<string, PythonObjectWrapper> >()["result"];
+    Py_DECREF(env);
+    string toout= string("PythonObjectWrapper(ownership=") + tostring(v.m_ownership) + ", object=\"" + pickle + "\")";
+    out << toout;
+*/
+    return out; // shut up compiler
+}
+
 //! debug
 void printWrappedObjects()
 {

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-08-15 20:41:34 UTC (rev 7996)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-08-15 21:54:18 UTC (rev 7997)
@@ -876,6 +876,8 @@
     PyObject* m_object;
 
     template<class T> friend class ConvertToPyObject;
+    friend PStream& operator>>(PStream& in, PythonObjectWrapper& v);
+    friend PStream& operator<<(PStream& out, const PythonObjectWrapper& v);
 };
 
 // Specialization for General T*.  Attempt to cast into Object*.  If that works
@@ -931,17 +933,8 @@
 {
     return StaticConvertEnumFromPyObject<T, boost::is_enum<T>::value>
         ::convert(x, print_traceback);
-    /*
-    if(boost::is_enum<T>::value)
-        return ConvertFromPyObject<int>::convert(x, print_traceback);
-
-    PLERROR("Cannot convert this object by value from python (type=%s).",
-            TypeTraits<T>::name().c_str());
-    return T();//to silence compiler
-    */
 }
 
-
 template <class T>
 PP<T> ConvertFromPyObject<PP<T> >::convert(PyObject* pyobj,
                                            bool print_traceback)
@@ -1399,7 +1392,9 @@
         return PythonObjectWrapper::newPyObject();
 }
 
+
 PStream& operator>>(PStream& in, PythonObjectWrapper& v);
+PStream& operator<<(PStream& out, const PythonObjectWrapper& v);
 DECLARE_TYPE_TRAITS(PythonObjectWrapper);
 
 //! for debug purposes



From larocheh at mail.berlios.de  Thu Aug 16 21:41:08 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Thu, 16 Aug 2007 21:41:08 +0200
Subject: [Plearn-commits] r7998 - trunk/plearn_learners_experimental
Message-ID: <200708161941.l7GJf8cd006456@sheep.berlios.de>

Author: larocheh
Date: 2007-08-16 21:41:07 +0200 (Thu, 16 Aug 2007)
New Revision: 7998

Modified:
   trunk/plearn_learners_experimental/StackedSVDNet.cc
   trunk/plearn_learners_experimental/StackedSVDNet.h
Log:
Added option to fill diagonal.


Modified: trunk/plearn_learners_experimental/StackedSVDNet.cc
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-08-15 21:54:18 UTC (rev 7997)
+++ trunk/plearn_learners_experimental/StackedSVDNet.cc	2007-08-16 19:41:07 UTC (rev 7998)
@@ -59,6 +59,7 @@
     fine_tuning_decrease_ct( 0. ),
     batch_size(50),
     global_output_layer(false),
+    fill_in_null_diagonal(true),
     relative_min_improvement(1e-3),
     n_layers( 0 )
 {
@@ -107,6 +108,13 @@
                   "should have as input all units of the network (including the"
                   "input units).\n");
 
+    declareOption(ol, "fill_in_null_diagonal", 
+                  &StackedSVDNet::fill_in_null_diagonal,
+                  OptionBase::buildoption,
+                  "Indication that the zero diagonal of the weight matrix after\n"
+                  "logistic auto-regression should be filled with the\n"
+                  "maximum absolute value of each corresponding row.\n");
+
     declareOption(ol, "relative_min_improvement", 
                   &StackedSVDNet::relative_min_improvement,
                   OptionBase::buildoption,
@@ -412,12 +420,15 @@
                         cost << " or " << cost/layers[i]->size << " (rel)" << endl;
             }
 
-            // Fill in the empty diagonal
-            for(int j=0; j<layers[i]->size; j++)
+            if(fill_in_null_diagonal)
             {
-                connections[i]->weights(j,j) = maxabs(connections[i]->weights(j));
+                // Fill in the empty diagonal
+                for(int j=0; j<layers[i]->size; j++)
+                {
+                    connections[i]->weights(j,j) = maxabs(connections[i]->weights(j));
+                }
             }
-
+            
             if(layers[i]->size != layers[i+1]->size)
             {
                 Mat A,U,Vt;

Modified: trunk/plearn_learners_experimental/StackedSVDNet.h
===================================================================
--- trunk/plearn_learners_experimental/StackedSVDNet.h	2007-08-15 21:54:18 UTC (rev 7997)
+++ trunk/plearn_learners_experimental/StackedSVDNet.h	2007-08-16 19:41:07 UTC (rev 7998)
@@ -81,6 +81,11 @@
     //! Indication that the output layer (given by the final module)
     //! should have as input all units of the network (including the input units)
     bool global_output_layer;
+
+    //! Indication that the zero diagonal of the weight matrix after
+    //! logistic auto-regression should be filled with the
+    //! maximum absolute value of each corresponding row.
+    bool fill_in_null_diagonal;
     
     //! Minimum relative improvement convergence criteria
     //! for the logistic auto-regression.



From dorionc at mail.berlios.de  Fri Aug 17 17:58:18 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 17 Aug 2007 17:58:18 +0200
Subject: [Plearn-commits] r7999 - in trunk: plearn/math
	python_modules/plearn/math python_modules/plearn/report
Message-ID: <200708171558.l7HFwIA8008304@sheep.berlios.de>

Author: dorionc
Date: 2007-08-17 17:58:17 +0200 (Fri, 17 Aug 2007)
New Revision: 7999

Modified:
   trunk/plearn/math/ObservationWindow.cc
   trunk/plearn/math/ObservationWindow.h
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/VecStatsCollector.cc
   trunk/python_modules/plearn/math/arrays.py
   trunk/python_modules/plearn/report/__init__.py
   trunk/python_modules/plearn/report/graphical_tools.py
Log:
Minor fixes

Modified: trunk/plearn/math/ObservationWindow.cc
===================================================================
--- trunk/plearn/math/ObservationWindow.cc	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/plearn/math/ObservationWindow.cc	2007-08-17 15:58:17 UTC (rev 7999)
@@ -225,6 +225,37 @@
     return m_obs_weights[last_obs];
 }
 
+// The TMatElementIterator<double> are not as easy to handle as real*...
+//
+//TBA:// Get the min/max value in a column
+//TBA:void ObservationWindow::columnMin(int col, real& min, real& agemin) const
+//TBA:{
+//TBA:    const Mat& column = m_observations.column(col);
+//TBA:    real* beg = column.begin();
+//TBA:    real* ptr = std::min_element(beg, column.end());
+//TBA:    min = *ptr;
+//TBA:
+//TBA:    // The cursor is now at the position *after* the position of the last added
+//TBA:    // value... 
+//TBA:    agemin = abs((ptr - beg) - m_cursor);
+//TBA:    if ( agemin == 0 )
+//TBA:        agemin = m_window;
+//TBA:}
+//TBA:
+//TBA:void ObservationWindow::columnMax(int col, real& max, real& agemax) const
+//TBA:{
+//TBA:    const Mat& column = m_observations.column(col);
+//TBA:    real* beg = column.begin();
+//TBA:    real* ptr = std::max_element(beg, column.end());
+//TBA:    max = *ptr;
+//TBA:
+//TBA:    // The cursor is now at the position *after* the position of the last added
+//TBA:    // value... 
+//TBA:    agemax = abs((ptr - beg) - m_cursor);
+//TBA:    if ( agemax == 0 )
+//TBA:        agemax = m_window;
+//TBA:}
+
 /*
 //! Deep copying
 ObservationWindow* ObservationWindow::deepCopy(CopiesMap& copies) const

Modified: trunk/plearn/math/ObservationWindow.h
===================================================================
--- trunk/plearn/math/ObservationWindow.h	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/plearn/math/ObservationWindow.h	2007-08-17 15:58:17 UTC (rev 7999)
@@ -87,8 +87,11 @@
     
     const Vec lastObs() const;
     real lastWeight() const;
-    
 
+    //TBA: // Get the min/max value in a column
+    //TBA: void columnMin(int col, real& min, real& agemin) const;
+    //TBA: void columnMax(int col, real& max, real& agemax) const;
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.

Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/plearn/math/StatsCollector.cc	2007-08-17 15:58:17 UTC (rev 7999)
@@ -529,8 +529,17 @@
         }
         
         if(storeCounts())
-            PLERROR("The remove observation mechanism is incompatible with "
-                    "maxnvalues.");
+        {
+            if ( maxnvalues > 0 )
+                PLERROR("The remove observation mechanism is incompatible with "
+                        "maxnvalues > 0.");
+
+            // Find the associated count and decrement. Note that I do not
+            // verify whether the count reaches 0.0. A null count does not have
+            // any impact on pseudo_quantile() while removing the element from
+            // the map could mess up with ids...
+            counts[val].n -= weight;
+        }
     }
 }                           
 

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-08-17 15:58:17 UTC (rev 7999)
@@ -437,8 +437,15 @@
                  n, stats.size() );
 
     for(int k=0; k<n; k++)
-        stats[k].remove_observation(x[k], weight);
-  
+    {
+        real obs = x[k];
+        stats[k].remove_observation(obs, weight);
+        //TBA: if ( is_equal(obs, stats[k].min_) )
+        //TBA:     m_observation_window->columnMin(k, stats[k].min_, stats[k].agemin_);
+        //TBA: if ( is_equal(obs, stats[k].max_) )
+        //TBA:     m_observation_window->columnMax(k, stats[k].max_, stats[k].agemax_);
+    }
+        
     // This removes the observation x contribution to the covariance matrix.
     if( compute_covariance ) {
         if (fast_exact_is_equal(stats[0].nnonmissing(), 0)) {

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/python_modules/plearn/math/arrays.py	2007-08-17 15:58:17 UTC (rev 7999)
@@ -203,11 +203,7 @@
     f = choose(isnan(f), (0, 1))
     return sum(f)
     
-def isNaN(f):
-    """Return 1 where f contains NaN values, 0 elsewhere."""
-    import numpy as ieeespecial
-    return ieeespecial.mask(f, ieeespecial.NAN)
-
+isNaN = isnan
 def isNotNaN(f):
     """Return 0 where f contains NaN values, 1 elsewhere."""
     return ufunc.equal(isnan(f), 0.0)

Modified: trunk/python_modules/plearn/report/__init__.py
===================================================================
--- trunk/python_modules/plearn/report/__init__.py	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/python_modules/plearn/report/__init__.py	2007-08-17 15:58:17 UTC (rev 7999)
@@ -54,18 +54,21 @@
     FONTPROP   = FontProperties(family='sans-serif', weight='normal', size=FONTSIZE)
 
     global LEGEND_FONTSIZE, LEGEND_FONTPROP
-    LEGEND_FONTSIZE = 10
+    LEGEND_FONTSIZE = 24
     LEGEND_FONTPROP = FontProperties(family='sans-serif',
                                      weight='normal', size=LEGEND_FONTSIZE)    
 
-    global TICK_LABEL_FONTSIZE, TICK_LABEL_FONTPROP
-    TICK_LABEL_FONTSIZE = 10
-    TICK_LABEL_FONTPROP = FontProperties(family='sans-serif',
-                                         weight='normal', size=TICK_LABEL_FONTSIZE)
-    matplotlib.rc('text',           usetex=True)
-    matplotlib.rc('tick',           color='k')
+    #global TICK_LABEL_FONTSIZE, TICK_LABEL_FONTPROP
+    #TICK_LABEL_FONTSIZE = 10
+    #TICK_LABEL_FONTPROP = FontProperties(family='sans-serif',
+    #                                     weight='normal', size=TICK_LABEL_FONTSIZE)
+    matplotlib.rc('text',   usetex=True)
+    matplotlib.rc('legend', fontsize=14)
+    matplotlib.rc('xtick',  color='k', labelsize=14)
+    matplotlib.rc('ytick',  color='k', labelsize=14)
+    
     matplotlib.rc('figure',         facecolor=FIGURE_BG)
-    matplotlib.rc('figure.subplot', bottom=0.15)
+    matplotlib.rc('figure.subplot', bottom=0.10)
     matplotlib.rc('lines',          linewidth=1.5, linestyle='-')
     matplotlib.rc('grid',           color=GRID_COL, linewidth=0.5, linestyle='-')
 

Modified: trunk/python_modules/plearn/report/graphical_tools.py
===================================================================
--- trunk/python_modules/plearn/report/graphical_tools.py	2007-08-16 19:41:07 UTC (rev 7998)
+++ trunk/python_modules/plearn/report/graphical_tools.py	2007-08-17 15:58:17 UTC (rev 7999)
@@ -91,8 +91,9 @@
     if not sorted_keys:
         sorted_keys = legend_map.keys(); sorted_keys.sort()
     values = [ legend_map[k] for k in sorted_keys ]
-    legend = axes.legend(values, sorted_keys,
-                         loc=loc, shadow=False, prop = FontProperties(size=13))
+    legend = axes.legend(values, sorted_keys, loc=loc, shadow=False)
+    #legend = axes.legend(values, sorted_keys,
+    #                     loc=loc, shadow=False, prop = FontProperties(size=13))
     legend.set_zorder(100)
 
 class Struct(dict):



From saintmlx at mail.berlios.de  Fri Aug 17 21:49:40 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 17 Aug 2007 21:49:40 +0200
Subject: [Plearn-commits] r8000 - trunk/plearn/base
Message-ID: <200708171949.l7HJneDo008214@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-17 21:49:40 +0200 (Fri, 17 Aug 2007)
New Revision: 8000

Modified:
   trunk/plearn/base/Object.h
Log:
- fix options map so that derived class's options have priority over base class's options w/ the same names



Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2007-08-17 15:58:17 UTC (rev 7999)
+++ trunk/plearn/base/Object.h	2007-08-17 19:49:40 UTC (rev 8000)
@@ -130,7 +130,9 @@
             {                                                                                   \
                 OptionList& ol= getOptionList();                                                \
                 for(OptionList::iterator it= ol.begin(); it != ol.end(); ++it)                  \
-                    om[(*it)->optionname()]= *it;                                               \
+                    /*N.B. option map will contain derived class's option*/                     \
+                    /*  when it also exists in a base class.             */                     \
+                    om.insert(make_pair((*it)->optionname(), *it));                             \
             }                                                                                   \
             return om;                                                                          \
         }                                                                                       \
@@ -329,7 +331,9 @@
             {                                                                                   \
                 OptionList& ol= getOptionList();                                                \
                 for(OptionList::iterator it= ol.begin(); it != ol.end(); ++it)                  \
-                    om[(*it)->optionname()]= *it;                                               \
+                    /*N.B. option map will contain derived class's option*/                     \
+                    /*  when it also exists in a base class.             */                     \
+                    om.insert(make_pair((*it)->optionname(), *it));                             \
             }                                                                                   \
             return om;                                                                          \
         }                                                                                       \



From dorionc at mail.berlios.de  Fri Aug 17 22:28:04 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Fri, 17 Aug 2007 22:28:04 +0200
Subject: [Plearn-commits] r8001 - trunk/python_modules/plearn/pytest
Message-ID: <200708172028.l7HKS4wa010511@sheep.berlios.de>

Author: dorionc
Date: 2007-08-17 22:28:04 +0200 (Fri, 17 Aug 2007)
New Revision: 8001

Modified:
   trunk/python_modules/plearn/pytest/programs.py
   trunk/python_modules/plearn/pytest/tests.py
Log:
Fixed the dependencies mechanism.

Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2007-08-17 19:49:40 UTC (rev 8000)
+++ trunk/python_modules/plearn/pytest/programs.py	2007-08-17 20:28:04 UTC (rev 8001)
@@ -185,40 +185,50 @@
 
         # Account for dependencies
         success = no_need_to_compile or (self.__attempted_to_compile and exec_exists)
-        for dep in self.dependencies:
+        #print "+++ compilationSucceeded():", self.name, success, no_need_to_compile, self.__attempted_to_compile, exec_exists
+        for dep in self.dependencies:            
             success = (success and dep.compilationSucceeded())
+            #print "+++ DEP compilationSucceeded():", success
         return success
 
     def compile(self, publish_dirpath=""):
-        # Remove old compile log if any
-        publish_target = os.path.join(publish_dirpath, os.path.basename(self.__log_file_path))
-        if os.path.islink(publish_target) or os.path.isfile(publish_target):
-            os.remove(publish_target)
-        assert not os.path.exists(publish_target)
+        succeeded = True
+        if self.isCompilable():
+            # Remove old compile log if any        
+            publish_target = os.path.join(publish_dirpath,
+                                          os.path.basename(self.__log_file_path))
+            if os.path.islink(publish_target) or os.path.isfile(publish_target):
+                os.remove(publish_target)
+            assert not os.path.exists(publish_target)
+            
+            # Ensure compilation is needed
+            if self.compilationSucceeded():
+                logging.debug("Already successfully compiled %s"%self.getInternalExecPath())
+                succeeded = True
+            
+            elif self.__attempted_to_compile:
+                logging.debug("Already attempted to compile %s"%self.getInternalExecPath())
+                succeeded = False
+            
+            # First compilation attempt
+            else:
+                #print "+++ SHORTCUT!!!", self.name
+                #succeeded = True 
+                succeeded = self.__first_compilation_attempt()               
+                #print "+++ FIRST ATTEMPT", self.name, succeeded
+            
+            # Publish the compile log
+            if succeeded and publish_dirpath:
+                logging.debug("Publishing the compile log %s"%self.__log_file_path)
+                toolkit.symlink(self.__log_file_path,
+                                moresh.relative_path(publish_target))
 
-        # Ensure compilation is needed
-        if self.compilationSucceeded():
-            logging.debug("Already successfully compiled %s"%self.getInternalExecPath())
-            succeeded = True
-
-        elif self.__attempted_to_compile:
-            logging.debug("Already attempted to compile %s"%self.getInternalExecPath())
-            succeeded = False
-
-        # First compilation attempt
-        else:
-            succeeded = self.__first_compilation_attempt()               
-
-        # Publish the compile log
-        if succeeded and publish_dirpath:
-            logging.debug("Publishing the compile log %s"%self.__log_file_path)
-            toolkit.symlink(self.__log_file_path,
-                            moresh.relative_path(publish_target))
-
         # Account for dependencies
+        #print "+++ Success", self.name, succeeded        
         for dep in self.dependencies:
-            succeeded = (succeeded and dep.compile(publish_dirpath))
-        
+            succeeded = (succeeded and dep.compile(publish_dirpath))        
+            #print "+++ DEP", succeeded
+            
         return succeeded
         
     def __first_compilation_attempt(self):
@@ -365,14 +375,18 @@
             # Otherwise assumed to be non-compilable
             else:
                 self.__is_compilable = False
-                for dep in self.dependencies:
-                    if dep.isCompilable():
-                        self.__is_compilable = True
-                        break
 
             # It is now cached... 
             return self.__is_compilable
-        
+
+    def areDependenciesCompilable(self):
+        compilable = False
+        for dep in self.dependencies:
+            if dep.isCompilable():
+                compilable = True
+                break        
+        return compilable
+    
     def isGlobal(self):
         return self.__is_global
 

Modified: trunk/python_modules/plearn/pytest/tests.py
===================================================================
--- trunk/python_modules/plearn/pytest/tests.py	2007-08-17 19:49:40 UTC (rev 8000)
+++ trunk/python_modules/plearn/pytest/tests.py	2007-08-17 20:28:04 UTC (rev 8001)
@@ -692,7 +692,8 @@
         os.chdir( self.test.directory() )
         
     def compileProgram(self):
-        if not self.test.program.isCompilable():
+        if not self.test.program.isCompilable() \
+           and not self.test.program.areDependenciesCompilable():
             return True
 
         logging.debug("\nCompilation:")



From yoshua at mail.berlios.de  Sun Aug 19 05:04:42 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 19 Aug 2007 05:04:42 +0200
Subject: [Plearn-commits] r8002 - trunk/plearn_learners/online
Message-ID: <200708190304.l7J34g0S020467@sheep.berlios.de>

Author: yoshua
Date: 2007-08-19 05:04:41 +0200 (Sun, 19 Aug 2007)
New Revision: 8002

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Added the bprop part of the downward fprop (hidden-->visible). Untested yet.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-17 20:28:04 UTC (rev 8001)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-19 03:04:41 UTC (rev 8002)
@@ -297,7 +297,7 @@
     addPortName("hidden_activations.state");
     addPortName("visible_sample");
     addPortName("visible_expectation");
-    addPortName("visible_activation");
+    addPortName("visible_activation.state");
     addPortName("hidden_sample");
     addPortName("energy");
     addPortName("hidden_bias");
@@ -330,7 +330,7 @@
         port_sizes(getPortIndex("visible"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_sample"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_expectation"), 1) = visible_layer->size;
-        port_sizes(getPortIndex("visible_activation"), 1) = visible_layer->size;
+        port_sizes(getPortIndex("visible_activation.state"), 1) = visible_layer->size;
     }
     if (hidden_layer) {
         port_sizes(getPortIndex("hidden.state"), 1) = hidden_layer->size;
@@ -692,7 +692,7 @@
     bool visible_sample_is_output = visible_sample && visible_sample->isEmpty();
     Mat* visible_expectation = ports_value[getPortIndex("visible_expectation")];
     bool visible_expectation_is_output = visible_expectation && visible_expectation->isEmpty();
-    Mat* visible_activation = ports_value[getPortIndex("visible_activation")];
+    Mat* visible_activation = ports_value[getPortIndex("visible_activation.state")];
     bool visible_activation_is_output = visible_activation && visible_activation->isEmpty();
     Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
     bool hidden_sample_is_output = hidden_sample && hidden_sample->isEmpty();
@@ -1153,6 +1153,8 @@
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)
     if (hidden_act && hidden_act->isEmpty())
         hidden_act->resize(1,1);
+    if (visible_activation && visible_activation->isEmpty())
+        visible_activation->resize(1,1);
     if (hidden && hidden->isEmpty())
         hidden->resize(1,1);
     if (visible_reconstruction && visible_reconstruction->isEmpty())
@@ -1232,11 +1234,12 @@
 {
     PLASSERT( ports_value.length() == nPorts() );
     PLASSERT( ports_gradient.length() == nPorts() );
+    Mat* visible = ports_value[getPortIndex("visible")];
     Mat* visible_grad = ports_gradient[getPortIndex("visible")];
     Mat* hidden_grad = ports_gradient[getPortIndex("hidden.state")];
-    Mat* visible = ports_value[getPortIndex("visible")];
     Mat* hidden = ports_value[getPortIndex("hidden.state")];
     hidden_act = ports_value[getPortIndex("hidden_activations.state")];
+    Mat* visible_activations = ports_value[getPortIndex("visible_activations.state")];
     Mat* reconstruction_error_grad = 0;
     Mat* hidden_bias_grad = ports_gradient[getPortIndex("hidden_bias")];
     weights = ports_value[getPortIndex("weights")];
@@ -1271,10 +1274,12 @@
                   " the corresponding matrix should have 0 length" );
 
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
+    bool compute_hidden_grad = hidden_grad && hidden_grad->isEmpty();
     bool compute_weights_grad = weights_grad && weights_grad->isEmpty();
 
     int mbs = (visible && !visible->isEmpty()) ? visible->length() : -1;
 
+    // BPROP of UPWARD FPROP
     if (hidden_grad && !hidden_grad->isEmpty())
     {
         // Note: the assert below is for behavior compatibility with previous
@@ -1351,6 +1356,29 @@
         partition_function_is_stale = true;
     }
 
+    // BPROP of DOWNWARD FPROP
+    if (compute_hidden_grad && visible_grad && !compute_visible_grad)
+    {
+        PLASSERT(visible && !visible->isEmpty());
+        PLASSERT(visible_activations && !visible_activations->isEmpty());
+        PLASSERT(hidden && !hidden->isEmpty());
+        setAllLearningRates(grad_learning_rate);
+        visible_layer->bpropUpdate(*visible_activations,
+                                   *visible, visible_act_grad, *visible_grad, 
+                                   false);
+        
+        PLASSERT_MSG(!visible_bias_grad,"back-prop into visible bias  not implemented for downward fprop");
+        PLASSERT_MSG(!weights_grad,"back-prop into weights  not implemented for downward fprop");
+        hidden_grad->resize(mbs,hidden_layer->size);
+        TVec<Mat*> ports_value(2);
+        TVec<Mat*> ports_gradient(2);
+        ports_value[0] = visible_activations;
+        ports_value[1] = hidden;
+        ports_gradient[0] = &visible_act_grad;
+        ports_gradient[1] = hidden_grad;
+        connection->bpropAccUpdate(ports_value,ports_gradient);
+    }
+
     if (cd_learning_rate > 0 && minimize_log_likelihood) {
         PLASSERT( visible && !visible->isEmpty() );
         PLASSERT( hidden && !hidden->isEmpty() );



From yoshua at mail.berlios.de  Sun Aug 19 14:46:26 2007
From: yoshua at mail.berlios.de (yoshua at BerliOS)
Date: Sun, 19 Aug 2007 14:46:26 +0200
Subject: [Plearn-commits] r8003 - trunk/plearn_learners/online
Message-ID: <200708191246.l7JCkQB9017115@sheep.berlios.de>

Author: yoshua
Date: 2007-08-19 14:46:26 +0200 (Sun, 19 Aug 2007)
New Revision: 8003

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed problem which caused tests to fail, due to typo in port name


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-19 03:04:41 UTC (rev 8002)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-19 12:46:26 UTC (rev 8003)
@@ -297,7 +297,7 @@
     addPortName("hidden_activations.state");
     addPortName("visible_sample");
     addPortName("visible_expectation");
-    addPortName("visible_activation.state");
+    addPortName("visible_activations.state");
     addPortName("hidden_sample");
     addPortName("energy");
     addPortName("hidden_bias");
@@ -330,7 +330,7 @@
         port_sizes(getPortIndex("visible"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_sample"), 1) = visible_layer->size;
         port_sizes(getPortIndex("visible_expectation"), 1) = visible_layer->size;
-        port_sizes(getPortIndex("visible_activation.state"), 1) = visible_layer->size;
+        port_sizes(getPortIndex("visible_activations.state"), 1) = visible_layer->size;
     }
     if (hidden_layer) {
         port_sizes(getPortIndex("hidden.state"), 1) = hidden_layer->size;
@@ -692,7 +692,7 @@
     bool visible_sample_is_output = visible_sample && visible_sample->isEmpty();
     Mat* visible_expectation = ports_value[getPortIndex("visible_expectation")];
     bool visible_expectation_is_output = visible_expectation && visible_expectation->isEmpty();
-    Mat* visible_activation = ports_value[getPortIndex("visible_activation.state")];
+    Mat* visible_activation = ports_value[getPortIndex("visible_activations.state")];
     bool visible_activation_is_output = visible_activation && visible_activation->isEmpty();
     Mat* hidden_sample = ports_value[getPortIndex("hidden_sample")];
     bool hidden_sample_is_output = hidden_sample && hidden_sample->isEmpty();



From dpopovici at mail.berlios.de  Tue Aug 21 16:52:46 2007
From: dpopovici at mail.berlios.de (dpopovici at BerliOS)
Date: Tue, 21 Aug 2007 16:52:46 +0200
Subject: [Plearn-commits] r8004 - trunk/plearn_learners/online
Message-ID: <200708211452.l7LEqkbx030445@sheep.berlios.de>

Author: dpopovici
Date: 2007-08-21 16:52:45 +0200 (Tue, 21 Aug 2007)
New Revision: 8004

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
fixed downwards bprop

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-19 12:46:26 UTC (rev 8003)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-21 14:52:45 UTC (rev 8004)
@@ -842,9 +842,7 @@
 
     // DOWNWARD FPROP
     // we are given hidden  and we want to compute the visible or visible_activation
-    if ( hidden && !hidden_is_output && 
-         ((visible && visible_is_output) ||
-          (visible_activation && visible_activation_is_output)) )
+    if ( hidden && !hidden_is_output && visible && visible_is_output)
     {
         computeVisibleActivations(*hidden,true);
         if (visible_activation)
@@ -1270,8 +1268,8 @@
 
     // Ensure the visible gradient is not provided as input. This is because we
     // accumulate more than once in 'visible_grad'.
-    PLASSERT_MSG( !visible_grad || visible_grad->isEmpty(), "If visible gradient is desired "
-                  " the corresponding matrix should have 0 length" );
+//    PLASSERT_MSG( !visible_grad || visible_grad->isEmpty(), "If visible gradient is desired "
+//                  " the corresponding matrix should have 0 length" );
 
     bool compute_visible_grad = visible_grad && visible_grad->isEmpty();
     bool compute_hidden_grad = hidden_grad && hidden_grad->isEmpty();
@@ -1367,9 +1365,9 @@
                                    *visible, visible_act_grad, *visible_grad, 
                                    false);
         
-        PLASSERT_MSG(!visible_bias_grad,"back-prop into visible bias  not implemented for downward fprop");
-        PLASSERT_MSG(!weights_grad,"back-prop into weights  not implemented for downward fprop");
-        hidden_grad->resize(mbs,hidden_layer->size);
+//        PLASSERT_MSG(!visible_bias_grad,"back-prop into visible bias  not implemented for downward fprop");
+//        PLASSERT_MSG(!weights_grad,"back-prop into weights  not implemented for downward fprop");
+//        hidden_grad->resize(mbs,hidden_layer->size);
         TVec<Mat*> ports_value(2);
         TVec<Mat*> ports_gradient(2);
         ports_value[0] = visible_activations;



From nouiz at mail.berlios.de  Tue Aug 21 17:01:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 21 Aug 2007 17:01:23 +0200
Subject: [Plearn-commits] r8005 - trunk/plearn_learners/regressors
Message-ID: <200708211501.l7LF1Noa031192@sheep.berlios.de>

Author: nouiz
Date: 2007-08-21 17:01:23 +0200 (Tue, 21 Aug 2007)
New Revision: 8005

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
We do not save some data as they take too much space


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-08-21 14:52:45 UTC (rev 8004)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2007-08-21 15:01:23 UTC (rev 8005)
@@ -83,14 +83,18 @@
                   "The target size of the train set\n");
     declareOption(ol, "weightsize", &RegressionTreeRegisters::weightsize_, OptionBase::learntoption,
                   "The weight of each sample in the train set\n");
-    declareOption(ol, "sorted_row", &RegressionTreeRegisters::sorted_row, OptionBase::learntoption,
-                  "The matrix holding the sequence of samples in ascending value order for each dimension\n");
-    declareOption(ol, "inverted_sorted_row", &RegressionTreeRegisters::inverted_sorted_row, OptionBase::learntoption,
-                  "The matrix holding the position of a given entry in the sorted row matrix\n");
     declareOption(ol, "leave_register", &RegressionTreeRegisters::leave_register, OptionBase::learntoption,
                   "The vector identifying the leave to which, each row belongs\n");
     declareOption(ol, "leave_candidate", &RegressionTreeRegisters::leave_candidate, OptionBase::learntoption,
                   "The vector identifying the candidate leave to which, each row could belong after split\n");
+
+    //too big to save
+    declareOption(ol, "sorted_row", &RegressionTreeRegisters::sorted_row, OptionBase::nosave,
+                  "The matrix holding the sequence of samples in ascending value order for each dimension\n");
+    //too big to save
+    declareOption(ol, "inverted_sorted_row", &RegressionTreeRegisters::inverted_sorted_row, OptionBase::nosave,
+                  "The matrix holding the position of a given entry in the sorted row matrix\n");
+
     inherited::declareOptions(ol);
 }
 
@@ -129,6 +133,9 @@
 void RegressionTreeRegisters::reinitRegisters()
 {
     next_id = 0;
+
+    //in case we don't save the sorted data
+    sortRows();
 }
 
 void RegressionTreeRegisters::applyForRow(int leave_id, int row)



From saintmlx at mail.berlios.de  Tue Aug 21 21:12:58 2007
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 21 Aug 2007 21:12:58 +0200
Subject: [Plearn-commits] r8006 - in trunk: plearn/python
	python_modules/plearn/pymake
Message-ID: <200708211912.l7LJCwik032116@sheep.berlios.de>

Author: saintmlx
Date: 2007-08-21 21:12:57 +0200 (Tue, 21 Aug 2007)
New Revision: 8006

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
   trunk/python_modules/plearn/pymake/pymake.py
Log:
- pybridge: keep python instances alive as long as corresponding plearn object is alive (w/ rudimentary gc)
- new -pyso pymake option to create shared object libraries for python modules



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2007-08-21 15:01:23 UTC (rev 8005)
+++ trunk/plearn/python/PythonExtension.cc	2007-08-21 19:12:57 UTC (rev 8006)
@@ -50,6 +50,9 @@
     TVec<PythonObjectWrapper> as;
     for(int i= 0; i < nas; ++i)
         as.push_back(PythonObjectWrapper(PyTuple_GET_ITEM(args, i)));
+
+    PythonObjectWrapper::gc_collect1();
+
     try
     {
         PythonObjectWrapper returned_value= t->call(0, as);

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-08-21 15:01:23 UTC (rev 8005)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-08-21 19:12:57 UTC (rev 8006)
@@ -395,6 +395,8 @@
     Object* obj= args_tvec[0];
     args_tvec.subVecSelf(1, args_tvec.size()-1);
 
+    gc_collect1();
+
     //call, catch and send any errors to python
     try
     {
@@ -441,6 +443,10 @@
     obj->unref();//python no longer references this obj.
     //perr << "aft.del o->usage()= " << obj->usage() << endl;
 
+    //GC
+    if(m_gc_next_object->first == obj)
+        m_gc_next_object= m_wrapped_objects.end();
+
     m_wrapped_objects.erase(obj);
 
     //printWrappedObjects();
@@ -453,7 +459,9 @@
     TVec<PyObject*> args_tvec= 
         PythonObjectWrapper(args).as<TVec<PyObject*> >();
     Object* o= newObjectFromClassname(PyString_AsString(args_tvec[1]));
+
     //perr << "new o->usage()= " << o->usage() << endl;
+
     return PyCObject_FromVoidPtr(o, 0);
 }
 
@@ -465,15 +473,34 @@
     Object* o= PythonObjectWrapper(pyo);
     if(args_tvec.length() < 3 || args_tvec[2]==Py_True)
         o->ref();
+
     //perr << "ref o->usage()= " << o->usage() << endl;
     PythonObjectWrapper::m_wrapped_objects[o]= pyo;
     //perr << "refCPPObj: " << (void*)o << " : " << (void*)pyo << endl;
 
+    Py_INCREF(pyo);
     //printWrappedObjects();
 
     return newPyObject();//None
 }
 
+void PythonObjectWrapper::gc_collect1()
+{
+    if(m_gc_next_object == m_wrapped_objects.end())
+        m_gc_next_object= m_wrapped_objects.begin();
+    if(m_gc_next_object != m_wrapped_objects.end())
+    {
+        wrapped_objects_t::iterator it= m_gc_next_object;
+        ++m_gc_next_object;
+        if(it->first->usage() == 1 && it->second->ob_refcnt == 1)
+        {
+            Py_DECREF(it->second);
+            gc_collect1();
+        }
+    }
+}
+
+
 //#####  newPyObject  #########################################################
 
 //! Return None (increments refcount)
@@ -493,11 +520,14 @@
 PyMethodDef PythonObjectWrapper::m_unref_method_def;
 PyMethodDef PythonObjectWrapper::m_newCPPObj_method_def;
 PyMethodDef PythonObjectWrapper::m_refCPPObj_method_def;
+PythonObjectWrapper::wrapped_objects_t::iterator 
+  PythonObjectWrapper::m_gc_next_object= 
+  PythonObjectWrapper::m_wrapped_objects.end();
 
 PyObject* ConvertToPyObject<Object*>::newPyObject(const Object* x)
 {
-//     perr << "in ConvertToPyObject<Object*>::newPyObject : "
-//          << x->classname() << ' ' << (void*)x << endl;
+    //perr << "in ConvertToPyObject<Object*>::newPyObject : "
+    //     << x->classname() << ' ' << (void*)x << endl;
 
     // void ptr becomes None
     if(!x) return PythonObjectWrapper::newPyObject();
@@ -555,7 +585,8 @@
 
 //    perr << "newPyObject: " << (void*)x << " : " << (void*)the_obj << endl;
 
-//    printWrappedObjects();
+    Py_INCREF(the_obj);
+    //printWrappedObjects();
 
     return the_obj;
 }
@@ -776,9 +807,29 @@
              << (void*)it->second << ' ' << it->second->ob_refcnt << endl;
 }
 
+void ramassePoubelles()
+{
+    size_t sz= 0;
+    while(sz != PythonObjectWrapper::m_wrapped_objects.size())
+    {
+        sz= PythonObjectWrapper::m_wrapped_objects.size();
+        PythonObjectWrapper::wrapped_objects_t::iterator it= 
+            PythonObjectWrapper::m_wrapped_objects.begin();
+        while(it != PythonObjectWrapper::m_wrapped_objects.end())
+        {
+            PythonObjectWrapper::wrapped_objects_t::iterator jt= it;
+            ++it;
+            if(jt->second->ob_refcnt == 1 && jt->first->usage() == 1)
+                Py_DECREF(jt->second);
+        }
+    }
+}
+
 BEGIN_DECLARE_REMOTE_FUNCTIONS
     declareFunction("printWrappedObjects", &printWrappedObjects,
                     (BodyDoc("Prints PLearn objects wrapped into python.\n")));
+    declareFunction("ramassePoubelles", &ramassePoubelles,
+                    (BodyDoc("GC for wrapped objects.\n")));
 END_DECLARE_REMOTE_FUNCTIONS
 
 

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-08-21 15:01:23 UTC (rev 8005)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-08-21 19:12:57 UTC (rev 8006)
@@ -846,6 +846,8 @@
 
     static PyObject* refCPPObj(PyObject* self, PyObject* args);
 
+    static void gc_collect1();
+
     //#####  Low-Level PyObject Creation  #####################################
 
     /**
@@ -870,6 +872,7 @@
     static pypl_classes_t m_pypl_classes;
     typedef map<const Object*, PyObject*> wrapped_objects_t;
     static wrapped_objects_t m_wrapped_objects; //!< for wrapped PLearn Objects
+    static wrapped_objects_t::iterator m_gc_next_object;
 
 protected:
     OwnershipMode m_ownership;               //!< Whether we own the PyObject or not
@@ -1399,6 +1402,7 @@
 
 //! for debug purposes
 void printWrappedObjects();
+void ramassePoubelles();
 
 } // end of namespace PLearn
 

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-08-21 15:01:23 UTC (rev 8005)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-08-21 19:12:57 UTC (rev 8006)
@@ -911,8 +911,8 @@
             print "Warning: bad target", target
         else:
             info = file_info(cctarget)
-            if info.hasmain or create_dll or create_so:
-                if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll and not create_so:
+            if info.hasmain or create_dll or create_so or create_pyso:
+                if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll and not create_so and not create_pyso:
                     info.make_symbolic_link(linkname, None, info.corresponding_output) # remake the correct symbolic link
                     print 'Target',info.filebase,'is up to date.'
                 else:
@@ -1051,7 +1051,7 @@
                 print '[ LINKING',ccfile.filebase,']'
             link_exit_code = ccfile.launch_linking()
             if platform!='win32':
-                if create_so:
+                if create_so or create_pyso:
                     so_filename = os.path.basename(ccfile.corresponding_output)
                     ccfile.make_symbolic_link(so_filename, so_filename)
                 else:
@@ -1096,7 +1096,7 @@
     so_options = ""
     if force_32bits:
         linkeroptions = linkeroptions + ' -m32'
-    if create_so:
+    if create_so or create_pyso:
         so_options = " -shared "
     for opt in options:
         optdef = pymake_options_defs[opt]
@@ -1639,6 +1639,9 @@
 
             elif create_so:
                 self.corresponding_output = join(self.filedir, objsdir, 'lib'+self.filebase+'.so')
+                
+            elif create_pyso:
+                self.corresponding_output = join(self.filedir, objsdir, self.filebase+'.so')
 
             elif self.hasmain:
                 self.corresponding_output = join(self.filedir, objsdir, self.filebase)
@@ -1683,7 +1686,7 @@
         """returns the list of FileInfos of all .cc files that need to be linked together to produce the corresponding_output"""
         if not hasattr(self,"ccfiles_to_link"):
             #if not self.hasmain or not self.is_ccfile:
-            if (not self.hasmain and not create_dll and not create_so) or not self.is_ccfile:
+            if (not self.hasmain and not create_dll and not create_so and not create_pyso) or not self.is_ccfile:
                 raise "called get_ccfiles_to_link on a file that is not a .cc file or that does not contain a main()"
             self.ccfiles_to_link = []
             visited_hfiles = []
@@ -2216,7 +2219,7 @@
         so_options = ""
         if force_32bits:
             linkeroptions = linkeroptions + ' -m32'
-        if create_so:
+        if create_so or create_pyso:
             so_options = " -shared "
         elif static_linking:
             so_options = " -static "
@@ -2401,6 +2404,7 @@
     # Variables that can be useful to have read access to in the config file
     global optionargs, otherargs, linkname, link_target_override, \
             create_dll, relocatable_dll, no_cygwin, force_32bits, create_so, \
+            create_pyso, \
             static_linking, force_recompilation, force_link, \
             local_compilation, symlinkobjs, temp_objs, distribute, vcproj, \
             local_ofiles, local_ofiles_base_path
@@ -2546,6 +2550,12 @@
     else:
         create_so = 0
 
+    if 'pyso' in optionargs:
+        create_pyso = 1
+        optionargs.remove('pyso')
+    else:
+        create_pyso = 0
+
     # do we want to create a statically linked executable
     if 'static' in optionargs:
         static_linking = 1
@@ -2554,12 +2564,13 @@
         static_linking = 0
 
     # Check for incompatibilities
-    if create_so and create_dll:
-        print 'Error: cannot create a DLL and a Shared Object at the same time.  Remove "-dll" or "-so" option.'
+    if create_so + create_dll + create_pyso > 1:
+        print ('Error: cannot create a DLL, Shared Object and/or Python Shared Object '
+               +'at the same time.  Remove "-dll", "-so" or "-pyso" option.')
         sys.exit(100)
 
-    if static_linking and (create_so or create_dll):
-        print 'Incompatible command line options specified: you may specify only one of -static, -so, -dll'
+    if static_linking and (create_so or create_dll or create_pyso):
+        print 'Incompatible command line options specified: you may specify only one of -static, -so, -dll, -pyso'
         sys.exit(100)
 
     ##  Options that will not affect the final compiled file



From louradou at mail.berlios.de  Tue Aug 21 23:08:06 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Tue, 21 Aug 2007 23:08:06 +0200
Subject: [Plearn-commits] r8007 - trunk/plearn_learners/online
Message-ID: <200708212108.l7LL86nH005984@sheep.berlios.de>

Author: louradou
Date: 2007-08-21 23:08:05 +0200 (Tue, 21 Aug 2007)
New Revision: 8007

Modified:
   trunk/plearn_learners/online/RBMModule.cc
   trunk/plearn_learners/online/RBMModule.h
Log:
RBMModule: added an option "deterministic_reconstruction_in_cd"
(a variant of contrastive divergence learning,
where the visible are reconstructed by mean field and not sampling,
as suggested by Hinton)



Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-08-21 19:12:57 UTC (rev 8006)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-08-21 21:08:05 UTC (rev 8007)
@@ -115,6 +115,7 @@
     Gibbs_step(0),
     log_partition_function(0),
     partition_function_is_stale(true),
+    deterministic_reconstruction_in_cd(false),
     standard_cd_grad(true),
     standard_cd_bias_grad(true),
     standard_cd_weights_grad(true),
@@ -165,6 +166,16 @@
                   OptionBase::buildoption,
         "Compute the constrastive divergence in an output port.");
 
+    declareOption(ol, "deterministic_reconstruction_in_cd",
+                  &RBMModule::deterministic_reconstruction_in_cd,
+                  OptionBase::buildoption,
+        "Whether to use the expectation of the visible (given a hidden sample)\n"
+	"or a sample of the visible in the contrastive divergence learning.\n"
+        "In other words, instead of the classical Gibbs sampling\n"
+        "   v_0 --> h_0 ~ p(h|v_0) --> v_1 ~ p(v|h_0) -->  p(h|v_1)\n"
+        "we will have by setting 'deterministic_reconstruction_in_cd=1'\n"
+        "   v_0 --> h_0 ~ p(h|v_0) --> v_1 = E(v|h_0) -->  p(h|E(v|h_0)).");
+ 
     declareOption(ol, "standard_cd_grad",
                   &RBMModule::standard_cd_grad,
                   OptionBase::buildoption,
@@ -1106,10 +1117,21 @@
             for( int i=0; i<n_Gibbs_steps_CD; i++)
             {
                 hidden_layer->generateSamples();
-                // (Negative phase) Generate visible samples.
-                sampleVisibleGivenHidden(hidden_layer->samples);
-                // compute corresponding hidden expectations.
-                computeHiddenActivations(visible_layer->samples);
+                if (deterministic_reconstruction_in_cd)
+                {
+                   // (Negative phase) compute visible expectations
+                   computeVisibleActivations(hidden_layer->samples);
+                   visible_layer->computeExpectations();
+                   // compute corresponding hidden expectations.
+                   computeHiddenActivations(visible_layer->getExpectations());
+                }
+                else
+                {
+                   // (Negative phase) Generate visible samples.
+                   sampleVisibleGivenHidden(hidden_layer->samples);
+                   // compute corresponding hidden expectations.
+                   computeHiddenActivations(visible_layer->samples);
+                }
                 hidden_activations_are_computed = false;
                 hidden_layer->computeExpectations();
             }
@@ -1119,7 +1141,10 @@
             PLASSERT(negative_phase_hidden_activations &&
                      negative_phase_hidden_activations_is_output);
             negative_phase_visible_samples->resize(mbs,visible_layer->size);
-            *negative_phase_visible_samples << visible_layer->samples;
+            if (deterministic_reconstruction_in_cd)
+               *negative_phase_visible_samples << visible_layer->getExpectations();
+            else
+               *negative_phase_visible_samples << visible_layer->samples;
             negative_phase_hidden_expectations->resize(hidden_expectations.length(),
                                                        hidden_expectations.width());
             *negative_phase_hidden_expectations << hidden_expectations;
@@ -1177,19 +1202,6 @@
 
     if (!found_a_valid_configuration)
     {
-        /*
-        if (visible)
-            cout << "visible_empty : "<< (bool) visible->isEmpty() << endl;
-        if (hidden)
-            cout << "hidden_empty : "<< (bool) hidden->isEmpty() << endl;
-        if (visible_sample)
-            cout << "visible_sample_empty : "<< (bool) visible_sample->isEmpty() << endl;
-        if (hidden_sample)
-            cout << "hidden_sample_empty : "<< (bool) hidden_sample->isEmpty() << endl;
-        if (visible_expectation)
-            cout << "visible_expectation_empty : "<< (bool) visible_expectation->isEmpty() << endl;
-
-        */
         PLERROR("In RBMModule::fprop - Unknown port configuration for module %s", name.c_str());
     }
 
@@ -1474,6 +1486,8 @@
         PLASSERT( visible && hidden );
         PLASSERT( !negative_phase_visible_samples ||
                   !negative_phase_visible_samples->isEmpty() );
+
+        Mat vis_expect_ptr;
         if (!negative_phase_visible_samples)
         {
             // Generate hidden samples.
@@ -1481,18 +1495,34 @@
             for( int i=0; i<n_Gibbs_steps_CD; i++)
             {
                 hidden_layer->generateSamples();
-                // (Negative phase) Generate visible samples.
-                sampleVisibleGivenHidden(hidden_layer->samples);
-                // compute corresponding hidden expectations.
-                computeHiddenActivations(visible_layer->samples);
+                if (deterministic_reconstruction_in_cd)
+                {
+                   // (Negative phase) compute visible expectations
+                   computeVisibleActivations(hidden_layer->samples);
+                   visible_layer->computeExpectations();
+                   // compute corresponding hidden expectations.
+                   computeHiddenActivations(visible_layer->getExpectations());
+                }
+                else // classical CD learning
+                {
+                   // (Negative phase) Generate visible samples.
+                   sampleVisibleGivenHidden(hidden_layer->samples);
+                   // compute corresponding hidden expectations.
+                   computeHiddenActivations(visible_layer->samples);
+                }
                 hidden_layer->computeExpectations();
             }
             PLASSERT( !computed_contrastive_divergence );
             PLASSERT( !negative_phase_hidden_expectations );
             PLASSERT( !negative_phase_hidden_activations );
+            if (deterministic_reconstruction_in_cd) {
+                vis_expect_ptr = visible_layer->getExpectations();
+                negative_phase_visible_samples = &vis_expect_ptr;
+            }
+	    else // classical CD learning
+               negative_phase_visible_samples = &(visible_layer->samples);
+            negative_phase_hidden_activations = &(hidden_layer->activations);
             negative_phase_hidden_expectations = &(hidden_layer->getExpectations());
-            negative_phase_visible_samples = &(visible_layer->samples);
-            negative_phase_hidden_activations = &(hidden_layer->activations);
         }
         PLASSERT( negative_phase_hidden_expectations &&
                   !negative_phase_hidden_expectations->isEmpty() );
@@ -1589,7 +1619,6 @@
             }
         }
         if (!connection_update_is_done)
-            // Perform standard update of the connection.
             connection->update(*visible, *hidden,
                     *negative_phase_visible_samples,
                     *negative_phase_hidden_expectations);
@@ -1640,7 +1669,6 @@
                 bias -= lr * (*hidden_bias_g)(i);
         }
 
-
         partition_function_is_stale = true;
     } else {
         PLCHECK_MSG( !contrastive_divergence_grad ||

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-21 19:12:57 UTC (rev 8006)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-21 21:08:05 UTC (rev 8007)
@@ -92,6 +92,9 @@
     real log_partition_function;
     bool partition_function_is_stale;
 
+    bool deterministic_reconstruction_in_cd;
+    bool stochastic_posterior_in_cd;
+
     bool standard_cd_grad;
     bool standard_cd_bias_grad;
     bool standard_cd_weights_grad;



From tihocan at mail.berlios.de  Wed Aug 22 18:21:00 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 22 Aug 2007 18:21:00 +0200
Subject: [Plearn-commits] r8008 - in trunk/plearn/python/test: .
	.pytest/EmbeddedPython_InterfunctionXchg/expected_results
Message-ID: <200708221621.l7MGL0tM018969@sheep.berlios.de>

Author: tihocan
Date: 2007-08-22 18:20:59 +0200 (Wed, 22 Aug 2007)
New Revision: 8008

Modified:
   trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
   trunk/plearn/python/test/InterfunctionXchgTest.cc
Log:
Fix for test EmbeddedPython_InterfunctionXchg under Python 2.5: some system information has changed and there is less debug output

Modified: trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log
===================================================================
--- trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-08-21 21:08:05 UTC (rev 8007)
+++ trunk/plearn/python/test/.pytest/EmbeddedPython_InterfunctionXchg/expected_results/RUN.log	2007-08-22 16:20:59 UTC (rev 8008)
@@ -26,12 +26,6 @@
  <string> in get_value()
 
 NameError: global name 'buf' is not defined
-    __doc__ = 'Name not found globally.'
-    __getitem__ = <bound method NameError.__getitem__ of <exceptions.NameError instance>>
-    __init__ = <bound method NameError.__init__ of <exceptions.NameError instance>>
-    __module__ = 'exceptions'
-    __str__ = <bound method NameError.__str__ of <exceptions.NameError instance>>
-    args = ("global name 'buf' is not defined",)
 
 The above is a description of an error in a Python program.  Here is
 the original traceback:

Modified: trunk/plearn/python/test/InterfunctionXchgTest.cc
===================================================================
--- trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-08-21 21:08:05 UTC (rev 8007)
+++ trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-08-22 16:20:59 UTC (rev 8008)
@@ -174,18 +174,30 @@
     }
     catch (const PythonException& e) {
         string exception_msg = e.message();
-        // Remove memory addresses from the exception message, as they may
-        // differ from one computer to another, causing the test to fail.
+        // Remove system dependent strings from the error message.
         boost::regex memory_adr("0x[abcdef0123456789]{6,}",
                                 boost::regex::perl|boost::regex::icase);
         string msg_without_sys_dependent_data =
             regex_replace(exception_msg, memory_adr,
                     "0x[memory_address]");
-        boost::regex python_ver("Python 2\\.[0-9]\\.[0-9]",
+
+        boost::regex python_ver("Python 2\\.[0-9](\\.[0-9])?",
                                 boost::regex::perl|boost::regex::icase);
         msg_without_sys_dependent_data =
             regex_replace(msg_without_sys_dependent_data, python_ver,
                     "Python 2.X.Y");
+
+        boost::regex except_display("<type 'exceptions.([a-z]+)'>",
+                                    boost::regex::perl|boost::regex::icase);
+        msg_without_sys_dependent_data =
+            regex_replace(msg_without_sys_dependent_data, except_display,
+                    "\\1");
+
+        boost::regex extra_info("__doc__(.)*args = \\([^\\)]*\\)$\\n",
+                                boost::regex::perl|boost::regex::icase);
+        msg_without_sys_dependent_data =
+            regex_replace(msg_without_sys_dependent_data, extra_info, "");
+
         cout << "Caught Python Exception: '" << msg_without_sys_dependent_data
              << "'" << endl;
     }



From tihocan at mail.berlios.de  Wed Aug 22 19:30:14 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 22 Aug 2007 19:30:14 +0200
Subject: [Plearn-commits] r8009 - trunk/plearn/python/test
Message-ID: <200708221730.l7MHUEML008803@sheep.berlios.de>

Author: tihocan
Date: 2007-08-22 19:30:14 +0200 (Wed, 22 Aug 2007)
New Revision: 8009

Modified:
   trunk/plearn/python/test/InterfunctionXchgTest.cc
Log:
Fix to test EmbeddedPython_InterfunctionXchg, that failed due to a line containing blank characters

Modified: trunk/plearn/python/test/InterfunctionXchgTest.cc
===================================================================
--- trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-08-22 16:20:59 UTC (rev 8008)
+++ trunk/plearn/python/test/InterfunctionXchgTest.cc	2007-08-22 17:30:14 UTC (rev 8009)
@@ -198,6 +198,10 @@
         msg_without_sys_dependent_data =
             regex_replace(msg_without_sys_dependent_data, extra_info, "");
 
+        boost::regex blank_line("^( )+$", boost::regex::perl);
+        msg_without_sys_dependent_data =
+            regex_replace(msg_without_sys_dependent_data, blank_line, "");
+
         cout << "Caught Python Exception: '" << msg_without_sys_dependent_data
              << "'" << endl;
     }



From tihocan at mail.berlios.de  Thu Aug 23 16:56:44 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 16:56:44 +0200
Subject: [Plearn-commits] r8010 - trunk/plearn/math
Message-ID: <200708231456.l7NEui3v025139@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 16:56:43 +0200 (Thu, 23 Aug 2007)
New Revision: 8010

Modified:
   trunk/plearn/math/pl_math.cc
Log:
Finished implementation of logadd in double precision

Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2007-08-22 17:30:14 UTC (rev 8009)
+++ trunk/plearn/math/pl_math.cc	2007-08-23 14:56:43 UTC (rev 8010)
@@ -159,13 +159,13 @@
 {
     if (log_a < log_b)
     { // swap them
-        real tmp = log_a;
+        double tmp = log_a;
         log_a = log_b;
         log_b = tmp;
     }
     double negative_absolute_difference = log_b - log_a;
     if (negative_absolute_difference < MINUS_LOG_THRESHOLD)
-        return log_a;
+        return real(log_a);
     return (real)(log_a + log1p(exp(negative_absolute_difference)));
 }
 



From tihocan at mail.berlios.de  Thu Aug 23 17:39:25 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 17:39:25 +0200
Subject: [Plearn-commits] r8011 - trunk
Message-ID: <200708231539.l7NFdPOc029092@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 17:39:24 +0200 (Thu, 23 Aug 2007)
New Revision: 8011

Modified:
   trunk/pymake.config.model
Log:
Added check to ensure the python executable being called (i) exists and (ii) is really the correct version

Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-23 14:56:43 UTC (rev 8010)
+++ trunk/pymake.config.model	2007-08-23 15:39:24 UTC (rev 8011)
@@ -357,6 +357,26 @@
 
     if 'numpy' in optionargs:
         try:
+            # Ensure Python and Numpy are correctly installed.
+            # First check for the correct Python version.
+            ok_version = False
+            getpyver = toolkit.command_output('python%s -V' % python_version)
+            if len(getpyver) == 1:
+                tokens = getpyver[0].split()
+                if len(tokens) == 2:
+                    foundver = tokens[1]
+                    tokens = foundver.split('.')
+                    if len(tokens) >= 2:
+                        foundver = '.'.join(tokens[0:2])
+                        if foundver == python_version:
+                            ok_version = True
+            if not ok_version:
+                print "Could not find correct Python version: " + \
+                        "'python%s -V' should return 'Python %s.X', but returned '%s'" % \
+                        (python_version, python_version, ''.join(getpyver).strip())
+                sys.exit(100)
+            
+            # Then verify Numpy works with this version.
             numpy_numarray_include, numpy_core_include= \
               os.popen('python' + python_version + ' ' 
                        + os.path.join( plearndir,'scripts','get_numpy_includes.py'),



From simonl at mail.berlios.de  Thu Aug 23 19:09:43 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Thu, 23 Aug 2007 19:09:43 +0200
Subject: [Plearn-commits] r8012 - trunk/scripts/EXPERIMENTAL
Message-ID: <200708231709.l7NH9htm022609@sheep.berlios.de>

Author: simonl
Date: 2007-08-23 19:09:41 +0200 (Thu, 23 Aug 2007)
New Revision: 8012

Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
cleaned...


Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-23 15:39:24 UTC (rev 8011)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-08-23 17:09:41 UTC (rev 8012)
@@ -31,7 +31,7 @@
     sys.exit()
 
 def print_usage_repAndRec():
-    print 'putting you mouse OVER a hidden layer an hitting these keyes does... things :'
+    print 'putting you mouse OVER a hidden layer and hitting these keyes does... things :'
     print
     print 'f : fproping from the last layer'
     print 'F : fproping form the last layer and for the next ones'
@@ -52,6 +52,7 @@
     print 'C : same as w but for the hidden unit that has the highest value in each group'
     print 'o : set the current hidden layer to its original state'
     print 'O : same as o but for every layer'
+    print 't : now we have the same scale for  W, C'
     print 'right arrow : prints the next character in the dataset and its corresponding hidden layers and reconstructions'
     print 'left arrow :same but for previous character'
     print '0,1,2,3...9 : after having pressed a digit, right and left arrows will only find this digit'
@@ -147,7 +148,9 @@
         self.plotNext()#starting with a plot...
         self.__linkEvents()
 
+        self.same_scale = True
 
+
     def size(self):
         return len(self.originals_hl)
         
@@ -325,7 +328,7 @@
             elif char == 'm':
                 print 'maximum...'
 
-                for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+                for n in arange(hl.hidden_layer.size/hl.groupsize):
 
                     row = hl.getRow(n)
 
@@ -432,10 +435,10 @@
                     #HACK !!!
                     print 'just hacked...'
                     if i==1 and len(row) == 28*28*2:
-                        row = array(toMinusRow(list(row)))
+                        matricesToPlot.append(doubleSizedWeightVectorToImageMatrix(row))
                     #END OF HACK
-
-                    matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                    else:
+                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
                     namesToPlot.append(nameW)
 
                     if nameWr in listNames:
@@ -445,10 +448,10 @@
                         #HACK !!!
                         print 'just hacked...'
                         if i==1 and len(row) == 28*28*2:
-                            row = array(toMinusRow(list(row)))
+                            matricesToPlot.append(doubleSizedWeightVectorToImageMatrix(row))
                         #END OF HACK 
-                        
-                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
+                        else:
+                            matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
                         namesToPlot.append(nameWr)
 
                     figure(3)
@@ -468,19 +471,21 @@
                     #    row = array(toMinusRow(row))
                     #END OF HACK
                                       
-                    matW1 = reshape(toMinusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
-                    matM1 = reshape(toMinusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
-                    matW2 = reshape(toPlusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
-                    matM2 = reshape(toPlusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
+                    #matW1 = reshape(toMinusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
+                    #matM1 = reshape(toMinusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
+                    #matW2 = reshape(toPlusRow(rowW), (-1,self.hidden_layers[i-1].groupsize))
+                    #matM2 = reshape(toPlusRow(rowM),(-1,self.hidden_layers[i-1].groupsize))
+                    
+                    matW =doubleSizedWeightVectorToImageMatrix(rowW)
+                    matM = doubleSizedWeightVectorToImageMatrix(rowM)
+                    produit = matW*matM
                    
-                    produit = matW1*matM1
-                    produit2 = matW2*matM2
 
                     
                     figure(3)
                     ioff()
                     clf()                  
-                    plotMatrices([matW1,matM1,matW2,matM2,produit,produit2], [nameW + "-",nameM+"-", nameW + "+",nameM+"+",'term-to-term product (-)', 'term-to-term product (+)'])
+                    plotMatrices([matW,matM,produit], [nameW, nameM,'term-to-term product'])
                     ion()
                     draw()
 
@@ -508,37 +513,8 @@
 #                         draw()
 
                 #BIAS
-
-                if nameB in listNames:
-
-                    row = learner.getParameterValue(nameB)
-                    print nameB,row.shape
-
-                    print 'i',i
-                    matricesToPlot = [reshape(row, (-1, self.hidden_layers[i].groupsize))]
-                                           
-                    namesToPlot = [nameB]
-
-                    if nameBr in listNames:
-
-                        row = learner.getParameterValue(nameBr)
-                        print nameBr,row.shape
-
-                        #HACK !!!
-                        print i, row.shape[1]
-                        if i==1 and row.shape[1] == 28*28*2:
-                            row = array(toMinusRow(list(row[0])))
-                            print 'just hacked...'
-                        #END OF HACK
-
-                        matricesToPlot.append(reshape(row, (-1, self.hidden_layers[i-1].groupsize)))
-                        
-                        namesToPlot.append(nameBr)
-
-                    figure(4)
-                    clf()
-                    plotMatrices(matricesToPlot, namesToPlot)
-                    draw()
+                #TODO
+                
             
             #like 'w' but for an entire row -- 'W'
             elif char == 'W':
@@ -557,8 +533,14 @@
                 namesToPlot = []                
 
                 print 'x,y=', event.xdata, event.ydata
-                n = hl.matrixToLayer(event.xdata, event.ydata)
+                x,y = hl.correctXY(event.xdata, event.ydata)
+                n = hl.matrixToLayer(x,y)
                 print 'n =',n
+
+
+                names = []
+                for i in arange(n, n+hl.groupsize):
+                    names.append( formatFloat(hl.hidden_layer[i]))
             
                 if nameW in listNames and nameM not in listNames:                
 
@@ -569,13 +551,27 @@
                     figure(3)
                     ioff()                    
                     clf()
-                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.01, toMinusRow)
+                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.05, softmaxGroup2, [], names, self.same_scale)
                     ion()
                     draw()
+
+                if nameW in listNames and nameM in listNames:
+
                     
+                    w = learner.getParameterValue(nameW)
+                    m = learner.getParameterValue(nameM)
+                    M = zeros((hl.groupsize,w.shape[1]))
+                    
+                    for a in arange(hl.groupsize):
+                        M[a] = m[y]*w[a]                    
 
+                    figure(3)
+                    ioff()
+                    clf()
+                    plotLayer1(M, 28, .056,0,M.shape[0],.05, softmaxGroup2, [], names, self.same_scale)
+                    ion()
+                    draw()
 
-
             # like 'w' on the max of each row -- 'C'
 
             elif char == 'C':
@@ -605,42 +601,49 @@
                             index = k
 
                     indexes.append(j*hl.groupsize + index)
-                    names.append(str(row[index])[0:8])
-                
-                
-                if nameW in listNames:# and nameM not in listNames:                
+                    names.append(formatFloat(row[index]))
+
+                if nameW in listNames and nameM not in listNames:
                    
                     figure(3)
                     ioff()                    
                     clf()
-                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.01, toMinusRow, indexes,names)
+                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.05, softmaxGroup2, indexes,names, self.same_scale)
                     ion()
                     draw()
                     
-                if nameM in listNames:
+                if nameW in listNames and nameM in listNames:
                     
-                    figure(4)
+                    figure(3)
                     ioff()
                     clf()
-                    plotLayer1(learner.getParameterValue(nameM), 28, .056,0,0,.01,toMinusRow,indexes,names)
+
+                    w = learner.getParameterValue(nameW)
+                    m = learner.getParameterValue(nameM)
+                    M = zeros((len(indexes),w.shape[1]))
+                    
+                    for y,x in enumerate(indexes):
+                        print x%hl.groupsize,y
+                        M[y] =  m[y]*w[x%hl.groupsize]
+                    print M
+                    
+                    plotLayer1(M, 28, .056,0,M.shape[0],.05,softmaxGroup2,[],names, self.same_scale)
                     ion()
                     draw()
 
-                if nameWr in listNames:
-
-                    figure(5)
+                    figure(4)
                     ioff()
-                    clf()
-                    plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.01,toMinusRow,indexes,names)
+                    clf()                    
+                    plotLayer1(M, 28, .056,0,M.shape[0],.05,softmaxGroup2,[],names, self.same_scale)
                     ion()
                     draw()
 
-                if nameMr in listNames:
+                if nameWr in listNames and nameMr not in listNames:
 
-                    figure(6)
+                    figure(5)
                     ioff()
                     clf()
-                    plotLayer1(learner.getParameterValue(nameMr), 28, .056,0,0,.01,toMinusRow,indexes,names)
+                    plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.05,softmaxGroup2,indexes,names, self.same_scale)
                     ion()
                     draw()
                     
@@ -657,10 +660,20 @@
 
             else :
                 print_usage_repAndRec()
+
+
+        # toggle "same scale" or "individuals scales" for  'W' and 'C' -- t
+        
+        if char == 't':
+            self.same_scale = 1-self.same_scale
+            if self.same_scale:
+                print 'now we have the same scale for  W, C'
+            else:
+                print 'now we have individuals scales for  W, C'
                 
-       
-
+            
         # big-back to Original -- O
+        
         if char == 'O':
             print 'getting all original layers...'
             for k in arange(0,self.size()):
@@ -844,7 +857,7 @@
 
     def __sampleLayer(self, which_layer):
         hl = self.hidden_layers[which_layer]
-        for n in arange(hl.hidden_layer.nelements()/hl.groupsize):
+        for n in arange(hl.hidden_layer.size/hl.groupsize):
             multi = numpy.random.multinomial(1,hl.getRow(n))                    
             hl.setRow(n,multi)
 



From simonl at mail.berlios.de  Thu Aug 23 19:10:21 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Thu, 23 Aug 2007 19:10:21 +0200
Subject: [Plearn-commits] r8013 - trunk/python_modules/plearn/plotting
Message-ID: <200708231710.l7NHALBc023167@sheep.berlios.de>

Author: simonl
Date: 2007-08-23 19:10:17 +0200 (Thu, 23 Aug 2007)
New Revision: 8013

Modified:
   trunk/python_modules/plearn/plotting/netplot.py
Log:
cleaned...


Modified: trunk/python_modules/plearn/plotting/netplot.py
===================================================================
--- trunk/python_modules/plearn/plotting/netplot.py	2007-08-23 17:09:41 UTC (rev 8012)
+++ trunk/python_modules/plearn/plotting/netplot.py	2007-08-23 17:10:17 UTC (rev 8013)
@@ -39,7 +39,7 @@
     axes((x, y, width,height))
     if(min == max):
         max=min + 1e-6
-    cbarh = arange(min, max,  (max-min)/50.)
+    cbarh = arange(min, max,  (max-min)/nb_ticks)
     cbar = vecToVerticalMatrix(cbarh)
     cbarh_str = []
     for el in cbarh:
@@ -48,54 +48,13 @@
     xticks([],[])
     imshow(cbar, cmap = color_map, vmin=min, vmax=max)
 
+def formatFloat(float):
+    return "%.2e" % float
 
 
+def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.02, apply_to_rows = None, index_to_plot = [], names = [], same_scale = False):
 
-def plotLayer1Old(W, M, width):
-    '''plots each row of W and M as if they where matrix (width is the width of one of these matrix)'''
-
-    #some calculations for plotting
-
-    nbPlotStyles = 3 # (normal and two times 1 sur 2)
-    subPlotHeight = nbPlotStyles*2
-    subPlotWidth = max(len(W), len(M))
-    axesHeight = 1./float(subPlotHeight)
-    axesWidth = 1./float(subPlotWidth)
-
-    colorMap = defaultColorMap
     
-    matrices = [W,M]
-    names = ["W", "M"]
-
-    #THE plotting
-
-    for i, matrix in enumerate(matrices):
-        for j, row in enumerate(matrix):
-            
-            #normal
-        
-            #subplot(subPlotHeight, subPlotWidth,(i%2)*subPlotWidth + j + 1)
-            axes((j*axesWidth, i*axesHeight, axesWidth, axesHeight))
-            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap)
-            setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
-            
-            #1 sur 2 -
-        
-            #subplot(subPlotHeight, subPlotWidth, (2+i%2)*subPlotWidth + j + 1)
-            axes((j*axesWidth, (i+2)*axesHeight, axesWidth, axesHeight))
-            imshow(rowToMatrix(toMinusRow(row),width), interpolation="nearest", cmap = colorMap)
-            setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
-            
-            #1 sur 2 +
-
-            #subplot(subPlotHeight, subPlotWidth, (4+i%2)*subPlotWidth + j + 1)
-            axes((j*axesWidth, (i+4)*axesHeight, axesWidth, axesHeight))
-            imshow(rowToMatrix(toPlusRow(row),width), interpolation="nearest", cmap = colorMap)
-            setPlotParams(names[i%2] + "_" + str(i) + "_" + str(j), False, True)
-
-def plotLayer1(M, width, plotWidth=.1, start=0, length=-1, space_between_images=.01, apply_to_rows = None, index_to_plot = [], names = []):
-
-    
     #some calculations for plotting
 
     #hack
@@ -111,12 +70,13 @@
 
     colorMap = defaultColorMap
 
-    mi,ma = findMinMax(M)
-    print 'min', mi
-    print 'max', ma
+    if same_scale:
+        mi,ma = findMinMax(M)
+        print 'min', mi
+        print 'max', ma
     
-    ma = max(abs(mi),abs(ma))
-    mi = -ma
+        ma = max(abs(mi),abs(ma))
+        mi = -ma
             
 
     #THE plotting
@@ -126,12 +86,11 @@
 
     if index_to_plot == []:
         index_to_plot = arange(start,start+length)
-    print index_to_plot
 
     j=0
     for i in index_to_plot:
            
-        #normal
+        #normal        
         row = M[i]
         if apply_to_rows != None:
             row = apply_to_rows(row)
@@ -140,11 +99,15 @@
        # print x,y,plotWidth, plotHeight
         
         axes((x, y, plotWidth, plotHeight))
-        imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
+        if same_scale:
+            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap, vmin = mi, vmax = ma)
+        else:
+            imshow(rowToMatrix(row, width), interpolation="nearest", cmap = colorMap)#, vmin = mi, vmax = ma)
+            
         if names == []:
-            setPlotParams('row_' + str(i), False, True)
+            setPlotParams('row_' + str(i), 1-same_scale, True)
         else:
-            setPlotParams(names[j],False,True)
+            setPlotParams(names[j], 1-same_scale, True)
         j+=1
         
 
@@ -158,7 +121,7 @@
             break
 
     #custom color bar
-    customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))
+   # customColorBar(mi,ma,(1.-cbw-sbi, sbi, sbi, 1.-2.*cbw))
     return toReturn
 
 
@@ -313,6 +276,23 @@
         row2.append(row[i])
     return row2
 
+def evenMinusOdd(row):
+    return row[0::2]-row[1::2]
+
+def softmaxGroup2(row):
+    er = exp(row)
+    return er[0::2]/(er[0::2]+er[1::2])
+    # return er[1::2]/(er[0::2]+er[1::2])
+
+def doubleSizedWeightVectorToImageMatrix(row):
+    # v = evenMinusOdd(row)
+    v = softmaxGroup2(row)
+    d = sqrt(len(v))
+    print "Built %d x %d matrix" % (d,d)
+    m = reshape(v, (-1,d))
+    return m
+
+
 def rowToMatrixOld(row, width, validate_size = True, fill_value = 0.):
     '''change a row [1,2,3,4,5,6] into a matrix [[1,2],[3,4],[5,6]] if width = 2
        or [1,2,3,4,5,6] -> [[1,2,3,4],[5,6,fill_value,fill_value]] if width = 4 and validate_size = False
@@ -355,46 +335,7 @@
     '''
     return reshape(array(vec), (len(vec),-1))
 
-def truncateMatrixOld(mat, maxHeight=10, maxWidth=10):
-    
-    width = len(mat[0])
-    height = len(mat)
 
-    #si notre matrice est trop haute (seulement), on va la couper en morceaux
-    if width > maxWidth and height <= maxHeight:        
-        truncMat = []
-        a = -1
-        for indCol in arange(len(mat[0])):
-            if indCol % maxWidth == 0:               
-                truncMat.append([])
-                a=a+1
-                for l in arange(height):#on ajoute des lignes
-                    truncMat[a].append([])                
-            for l in arange(height):#on remplie les lignes
-                truncMat[a][l].append(mat[l][indCol])            
-            
-        return truncMat
-    #si notre matrice est trop large   (seulement)    
-    elif height > maxHeight and width <= maxWidth:
-        truncMat = [[]]
-        a,l=0,0
-        for ligne in mat:
-            if(l >= maxHeight):
-                truncMat.append([])
-                a=a+1
-                l=0
-            truncMat[a].append(ligne)
-            l=l+1
-            
-        return truncMat
-    elif height > maxHeight and width > maxWidth:
-        print 'matrice trop grosse... rien a faire..'
-        truncMat = [mat]
-    else:
-        truncMat = [mat]
-
-    return truncMat
-
 def truncateMatrix(mat, n=10.):
         
     width = len(mat[0])



From tihocan at mail.berlios.de  Thu Aug 23 19:22:38 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 19:22:38 +0200
Subject: [Plearn-commits] r8014 - trunk/plearn_learners/online
Message-ID: <200708231722.l7NHMcH5020330@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 19:22:37 +0200 (Thu, 23 Aug 2007)
New Revision: 8014

Modified:
   trunk/plearn_learners/online/ModuleLearner.cc
Log:
Return -1 instead of 0 when outputsize is not defined yet

Modified: trunk/plearn_learners/online/ModuleLearner.cc
===================================================================
--- trunk/plearn_learners/online/ModuleLearner.cc	2007-08-23 17:10:17 UTC (rev 8013)
+++ trunk/plearn_learners/online/ModuleLearner.cc	2007-08-23 17:22:37 UTC (rev 8014)
@@ -260,7 +260,8 @@
 {
     if ( module && store_outputs )
         return module->getPortWidth("output");
-    return 0;
+    else
+        return -1; // Undefined.
 }
 
 ////////////



From tihocan at mail.berlios.de  Thu Aug 23 19:28:41 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 19:28:41 +0200
Subject: [Plearn-commits] r8015 - trunk/plearn/var
Message-ID: <200708231728.l7NHSfHN021055@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 19:28:41 +0200 (Thu, 23 Aug 2007)
New Revision: 8015

Modified:
   trunk/plearn/var/GaussianProcessNLLVariable.cc
Log:
Use existing function to symmetrize Gram matrix (also perform additional checks to ensure the matrix is symmetric in the first place)

Modified: trunk/plearn/var/GaussianProcessNLLVariable.cc
===================================================================
--- trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-08-23 17:22:37 UTC (rev 8014)
+++ trunk/plearn/var/GaussianProcessNLLVariable.cc	2007-08-23 17:28:41 UTC (rev 8015)
@@ -296,11 +296,11 @@
     addToDiagonal(gram, noise);
 
     // The PLearn code relies on the matrix actually being symmetric in memory
-    // (assumption which LAPACK does not make).  Symmetrize the matrix
-    for (int i=1, n=gram.width() ; i<n ; ++i)
-        for (int j=0 ; j<i ; ++j)
-            gram(j,i) = gram(i,j);
-    
+    // (assumption which LAPACK does not make). Symmetrize the matrix.
+    PLCHECK(kernel->is_symmetric);
+    PLASSERT_MSG(gram.isSymmetric(false), "Gram matrix is not symmetric");
+    fillItSymmetric(gram);
+
     // Save the Gram matrix if requested
     if (save_gram_matrix) {
         static int counter = 1;



From tihocan at mail.berlios.de  Thu Aug 23 19:40:23 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 19:40:23 +0200
Subject: [Plearn-commits] r8016 - trunk/plearn_learners/online
Message-ID: <200708231740.l7NHeNh4022773@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 19:40:22 +0200 (Thu, 23 Aug 2007)
New Revision: 8016

Modified:
   trunk/plearn_learners/online/RBMModule.h
Log:
Removed boolean class member that is not used anywhere

Modified: trunk/plearn_learners/online/RBMModule.h
===================================================================
--- trunk/plearn_learners/online/RBMModule.h	2007-08-23 17:28:41 UTC (rev 8015)
+++ trunk/plearn_learners/online/RBMModule.h	2007-08-23 17:40:22 UTC (rev 8016)
@@ -93,7 +93,6 @@
     bool partition_function_is_stale;
 
     bool deterministic_reconstruction_in_cd;
-    bool stochastic_posterior_in_cd;
 
     bool standard_cd_grad;
     bool standard_cd_bias_grad;



From tihocan at mail.berlios.de  Thu Aug 23 19:47:12 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 19:47:12 +0200
Subject: [Plearn-commits] r8017 - trunk/plearn_learners/meta
Message-ID: <200708231747.l7NHlCPx023329@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 19:47:12 +0200 (Thu, 23 Aug 2007)
New Revision: 8017

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
So we can use 'and' in C++? :o

Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-08-23 17:40:22 UTC (rev 8016)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-08-23 17:47:12 UTC (rev 8017)
@@ -265,7 +265,7 @@
     if(!train_set)
         PLERROR("In AdaBoost::train, you did not setTrainingSet");
     
-    if(!train_stats and compute_training_error)
+    if(!train_stats && compute_training_error)
         PLERROR("In AdaBoost::train, you did not setTrainStatsCollector");
 
     if (train_set->targetsize()!=1)



From tihocan at mail.berlios.de  Thu Aug 23 21:08:13 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 23 Aug 2007 21:08:13 +0200
Subject: [Plearn-commits] r8018 - trunk/plearn/python
Message-ID: <200708231908.l7NJ8DBW028085@sheep.berlios.de>

Author: tihocan
Date: 2007-08-23 21:08:13 +0200 (Thu, 23 Aug 2007)
New Revision: 8018

Modified:
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Fixed compilation crash with Intel Compiler

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-08-23 17:47:12 UTC (rev 8017)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-08-23 19:08:13 UTC (rev 8018)
@@ -1246,7 +1246,7 @@
 {
     TVec<T> as_list(data.size());
     int i= 0;
-    for(typename std::set<T>::iterator it= data.begin();
+    for(typename std::set<T>::const_iterator it= data.begin();
         it != data.end(); ++it, ++i)
         as_list[i]= *it;
     PyObject* pylist= ConvertToPyObject<TVec<T> >::newPyObject(as_list);



From chapados at mail.berlios.de  Fri Aug 24 02:22:26 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 24 Aug 2007 02:22:26 +0200
Subject: [Plearn-commits] r8019 - trunk/plearn/math
Message-ID: <200708240022.l7O0MQAP001382@sheep.berlios.de>

Author: chapados
Date: 2007-08-24 02:22:17 +0200 (Fri, 24 Aug 2007)
New Revision: 8019

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Minor adjustments related to windows and forgetting

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-08-23 19:08:13 UTC (rev 8018)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-08-24 00:22:17 UTC (rev 8019)
@@ -539,6 +539,7 @@
     sum_non_missing_square_weights = 0;
 
     // Window mechanism
+    m_num_incremental = 0;
     if (m_window > 0 || m_window == -2)
         m_observation_window->forget();
 }



From lysiane at mail.berlios.de  Fri Aug 24 19:25:30 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Fri, 24 Aug 2007 19:25:30 +0200
Subject: [Plearn-commits] r8020 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200708241725.l7OHPUA1005852@sheep.berlios.de>

Author: lysiane
Date: 2007-08-24 19:25:21 +0200 (Fri, 24 Aug 2007)
New Revision: 8020

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
before do optimizations for more diversity


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-24 00:22:17 UTC (rev 8019)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-24 17:25:21 UTC (rev 8020)
@@ -621,15 +621,13 @@
     real scalingFactor = -1*(pl_log(pow(2*Pi*noiseVariance, inputSpaceDim/2.0)) 
                              +
                              pl_log(trainingSetLength));
-    Vec neighbor(inputSpaceDim);
-    Vec predictedTarget(inputSpaceDim);
     for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
-        seeTrainingPoint(neighborIdx,neighbor);
+        seeTrainingPoint(neighborIdx,ses_neighbor);
         for(int transformIdx=0 ; transformIdx<nbTransforms ; transformIdx++){
             weight = computeReconstructionWeight(y,
-                                                 neighbor,
+                                                 ses_neighbor,
                                                  transformIdx,
-                                                 predictedTarget);
+                                                 ses_predictedTarget);
             weight = MULT_weights(weight,
                                   transformDistribution[transformIdx]);
             totalWeight = SUM_weights(weight,totalWeight);
@@ -790,11 +788,46 @@
 //! initialization operations that have to be done before the training
 //!WARNING: the trainset ("train_set") must be given
 void TransformationLearner::mainLearnerBuild(){
+
+    //dimension of the input space
+    inputSpaceDim = train_set->inputsize();
+
+    newDistribution.resize(nbTransforms) ;
+    ses_target.resize(inputSpaceDim);
+    ses_neighbor.resize(inputSpaceDim);
+    ses_predictedTarget.resize(inputSpaceDim);
+    lg_neighbor.resize(inputSpaceDim);
+    lg_predictedTarget.resize(inputSpaceDim);
+    fnn_target.resize(inputSpaceDim);
+    fnn_neighbor.resize(inputSpaceDim);
+    fbtrc_neighbor.resize(inputSpaceDim);
+    fbtrc_target.resize(inputSpaceDim);
+    fbtrc_predictedTarget.resize(inputSpaceDim);
+    fbwn_target.resize(inputSpaceDim);
+    fbwn_neighbor.resize(inputSpaceDim);
+    fbwn_predictedTarget.resize(inputSpaceDim);
+    mst_v.resize(inputSpaceDim);
+    mst_target.resize(inputSpaceDim);
+    mst_neighbor.resize(inputSpaceDim);
+    mst_pivots.resize(inputSpaceDim);
+    msb_newBiasSet.resize(nbTransforms,inputSpaceDim);
+    msb_norms.resize(nbTransforms);
+    msb_target.resize(inputSpaceDim);
+    msb_neighbor.resize(inputSpaceDim);
+    msb_reconstruction.resize(inputSpaceDim);
+    msnvMAP_total_k.resize(inputSpaceDim);
+    msnvMAP_target.resize(inputSpaceDim);
+    msnvMAP_neighbor.resize(inputSpaceDim);
+    msnvMAP_reconstruction.resize(inputSpaceDim);
+    
+
+
+
     int defaultPeriod = 1;
-    int defaultTransformsOffset;
-    int defaultBiasOffset;
-    int defaultNoiseVarianceOffset;
-    int defaultTransformDistributionOffset;
+    int defaultTransformsOffset=0;
+    int defaultBiasOffset=0;
+    int defaultNoiseVarianceOffset=0;
+    int defaultTransformDistributionOffset=0;
 
     defaultTransformsOffset = 0;
     
@@ -815,10 +848,7 @@
     transformsSD = sqrt(transformsVariance);
     
     //DIMENSION VARIABLES
-    
-    //dimension of the input space
-    inputSpaceDim = train_set->inputsize();
-      
+          
     //number of samples given in the training set
     trainingSetLength = train_set->length();
     
@@ -1397,9 +1427,7 @@
 // stores the 'idx'th training data point into 'dst'
 void TransformationLearner::seeTrainingPoint(const int idx, Vec & dst)const
 {
-    Vec v;
-    real w;
-    train_set->getExample(idx, dst,v,w);
+    train_set->getExample(idx, dst,stp_v,stp_w);
 }
 
 
@@ -1784,17 +1812,15 @@
     PLASSERT(pq.empty()); 
   
     //capture the target from his index in the training set
-    Vec target(inputSpaceDim);
-    seeTrainingPoint(targetIdx, target);
+    seeTrainingPoint(targetIdx, fnn_target);
     
     //for each potential neighbor,
     real dist;    
-    Vec neighbor(inputSpaceDim);
     for(int i=0; i<trainingSetLength; i++){
         if(i != targetIdx){ //(the target cannot be his own neighbor)
             //computes the distance to the target
-            seeTrainingPoint(i, neighbor);
-            dist = powdistance(target, neighbor); 
+            seeTrainingPoint(i, fnn_neighbor);
+            dist = powdistance(fnn_target, fnn_neighbor); 
             //if the distance is among "nbNeighbors" smallest distances seen,
             //keep it until to see a closer neighbor. 
             if(int(pq.size()) < nbNeighbors){
@@ -1874,20 +1900,16 @@
     PLASSERT(pq.empty()); 
     
     real weight;
-    Vec target(inputSpaceDim);
-    seeTrainingPoint(targetIdx, target);
-    Vec neighbor(inputSpaceDim);
-    Vec predictedTarget(inputSpaceDim);
-
+    seeTrainingPoint(targetIdx, fbtrc_target);
     //for each potential neighbor
     for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){
-            seeTrainingPoint(neighborIdx, neighbor);
+            seeTrainingPoint(neighborIdx, fbtrc_neighbor);
             for(int transformIdx=0; transformIdx<nbTransforms; transformIdx++){
-                weight = computeReconstructionWeight(target, 
-                                                     neighbor, 
+                weight = computeReconstructionWeight(fbtrc_target, 
+                                                     fbtrc_neighbor, 
                                                      transformIdx,
-                                                     predictedTarget);
+                                                     fbtrc_predictedTarget);
                 
                 //if the weight is among "nbEntries" biggest weight seen,
                 //keep it until to see a bigger neighbor. 
@@ -1967,19 +1989,15 @@
     PLASSERT(pq.empty()); 
     
     real weight; 
-    Vec target(inputSpaceDim);
-    seeTrainingPoint(targetIdx, target);
-    Vec neighbor(inputSpaceDim);
-    Vec predictedTarget(inputSpaceDim);
-    
+    seeTrainingPoint(targetIdx, fbwn_target);
     //for each potential neighbor
     for(int neighborIdx=0; neighborIdx<trainingSetLength; neighborIdx++){
         if(neighborIdx != targetIdx){ //(the target cannot be his own neighbor)
-            seeTrainingPoint(neighborIdx, neighbor);
-            weight = computeReconstructionWeight(target, 
-                                                 neighbor, 
+            seeTrainingPoint(neighborIdx, fbwn_neighbor);
+            weight = computeReconstructionWeight(fbwn_target, 
+                                                 fbwn_neighbor, 
                                                  transformIdx,
-                                                 predictedTarget);
+                                                 fbwn_predictedTarget);
             //if the weight of the triple is among the "nbNeighbors" biggest 
             //seen,keep it until see a bigger weight. 
             if(int(pq.size()) < nbNeighbors){
@@ -2019,20 +2037,17 @@
     int candidateIdx =0;
     int  targetIdx = reconstructionSet[candidateIdx].targetIdx;
     real totalWeight = INIT_weight(0);
-    Vec target(inputSpaceDim);
-    seeTrainingPoint(targetIdx,target);
-    Vec neighbor(inputSpaceDim);
-    Vec predictedTarget(inputSpaceDim);
+    seeTrainingPoint(targetIdx,ses_target);
     
     while(candidateIdx < nbReconstructions){
         
-        seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx, neighbor);
+        seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx, ses_neighbor);
         totalWeight = SUM_weights(totalWeight,
                                   updateReconstructionWeight(candidateIdx,
-                                                             target,
-                                                             neighbor,
+                                                             ses_target,
+                                                             ses_neighbor,
                                                              reconstructionSet[candidateIdx].transformIdx,
-                                                             predictedTarget));
+                                                             ses_predictedTarget));
         candidateIdx ++;
     
         if(candidateIdx == nbReconstructions)
@@ -2041,7 +2056,7 @@
             normalizeTargetWeights(targetIdx, totalWeight);
             totalWeight = INIT_weight(0);
             targetIdx = reconstructionSet[candidateIdx].targetIdx;
-            seeTrainingPoint(targetIdx, target);
+            seeTrainingPoint(targetIdx, ses_target);
         }
     }    
 }
@@ -2077,14 +2092,8 @@
 //!NOTE :  alpha =1 ->  no regularization
 void TransformationLearner::MStepTransformDistributionMAP(real alpha)
 {
-   
-    Vec newDistribution ;
-    newDistribution.resize(nbTransforms);
-    
-    for(int k=0; k<nbTransforms ; k++){
-        newDistribution[k] = INIT_weight(0);
-    }
-    
+    newDistribution.fill(INIT_weight(0));
+        
     int transformIdx;
     real weight;
     for(int idx =0 ;idx < nbReconstructions ; idx ++){
@@ -2120,9 +2129,6 @@
     B_C.clear();
     
     real lambda = 1.0*noiseVariance/transformsVariance;
-    Vec v(inputSpaceDim);
-    Vec target(inputSpaceDim);
-    Vec neighbor(inputSpaceDim);
     for(int idx=0 ; idx<nbReconstructions ; idx++){
         
         //catch a view on the next entry of our dataset, that is, a  triple:
@@ -2132,33 +2138,33 @@
   
         //catch the target and neighbor points from the training set
         
-        seeTrainingPoint(reconstructionSet[idx].targetIdx, target);
-        seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
+        seeTrainingPoint(reconstructionSet[idx].targetIdx, mst_target);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, mst_neighbor);
         
         int t = reconstructionSet[idx].transformIdx;
         
-        v << target;
+        mst_v << mst_target;
         if(transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT){
-            v = v - neighbor;
+            mst_v = mst_v - mst_neighbor;
         }
         if(withBias){
-            v = v - biasSet(t);
+            mst_v = mst_v - biasSet(t);
         }
         //at the end, we want that matrix C[t] represents
         //the matrix ( (NeighborPart(t)_T)W(NeighborPart(t)) + lambdaI ) transposed. 
-        externalProductScaleAcc(C[t], neighbor, neighbor, p);
+        externalProductScaleAcc(C[t], mst_neighbor, mst_neighbor, p);
         
         //at the end, that matrix B[t] represents
         //the matrix (NeighborPart(t)_T)W(TargetPart(t)) transposed.
         //externalProductScaleAcc(B[t], neighbor, v,p);
-        externalProductScaleAcc(B[t],v,neighbor,p);
+        externalProductScaleAcc(B[t],mst_v,mst_neighbor,p);
     }
     
-    TVec<int> pivots(inputSpaceDim);
+ 
     for(int t=0; t<nbTransforms; t++){
         addToDiagonal(C[t],lambda);
         //transforms[t] << solveLinearSystem(C[t], B[t]);  
-        lapackSolveLinearSystem(C[t],B[t],pivots);
+        lapackSolveLinearSystem(C[t],B[t],mst_pivots);
         transforms[t] << B[t];
         
     }  
@@ -2206,37 +2212,26 @@
 //!maximization step with respect to transformation bias
 //!(MAP version)
 void TransformationLearner::MStepBias(){
-    Mat newBiasSet(nbTransforms, inputSpaceDim);
-    for(int i=0;i<nbTransforms;i++){
-        for(int j=0; j<inputSpaceDim; j++){
-            newBiasSet[i][j]=0;
-        }
-    }
-    Vec norms(nbTransforms);
-    for(int t=0;t<nbTransforms;t++){
-        norms[t] = INIT_weight(0);
-    }
+    msb_newBiasSet.fill(0);
+    msb_norms.fill(INIT_weight(0));
     int transformIdx;
-    Vec target(inputSpaceDim);
-    Vec neighbor(inputSpaceDim);
-    Vec reconstruction(inputSpaceDim);
     real proba,weight;
     for(int idx=0; idx<nbReconstructions; idx++){
         transformIdx = reconstructionSet[idx].transformIdx;
         weight = reconstructionSet[idx].weight;
         proba = PROBA_weight(weight);
-        seeTrainingPoint(reconstructionSet[idx].targetIdx,target);
-        seeTrainingPoint(reconstructionSet[idx].neighborIdx, neighbor);
-        applyTransformationOn(transformIdx,neighbor, reconstruction);
-        newBiasSet(transformIdx) += proba*(target - reconstruction);
-        norms[transformIdx] = SUM_weights(norms[transformIdx],weight);
+        seeTrainingPoint(reconstructionSet[idx].targetIdx,msb_target);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, msb_neighbor);
+        applyTransformationOn(transformIdx,msb_neighbor, msb_reconstruction);
+        msb_newBiasSet(transformIdx) += proba*(msb_target - msb_reconstruction);
+        msb_norms[transformIdx] = SUM_weights(msb_norms[transformIdx],weight);
     }
     for(int t=0; t<nbTransforms ; t++){
-        newBiasSet(t) /= ((noiseVariance/transformsVariance) 
-                       +
-                       PROBA_weight(norms[t]));
+        msb_newBiasSet(t) /= ((noiseVariance/transformsVariance) 
+                              +
+                              PROBA_weight(msb_norms[t]));
     }
-    biasSet << newBiasSet;   
+    biasSet << msb_newBiasSet;   
 }
 
 
@@ -2252,30 +2247,25 @@
 void TransformationLearner::MStepNoiseVarianceMAP(real alpha, real beta)
 {
     
-    Vec total_k(nbTransforms);
-    for(int i=0;i<nbTransforms; i++){
-        total_k[i]=0;
-    }
+    msnvMAP_total_k.fill(0);
     int transformIdx;
     real proba;
-    Vec target(inputSpaceDim);
-    Vec neighbor(inputSpaceDim);
-    Vec reconstruction(inputSpaceDim);
     int candidateIdx=0;
     for(int targetIdx=0; targetIdx<trainingSetLength; targetIdx ++){
-        seeTrainingPoint(targetIdx,target);
+        seeTrainingPoint(targetIdx,msnvMAP_target);
         for(int idx=0; idx < nbTargetReconstructions; idx++){
             transformIdx = reconstructionSet[candidateIdx].transformIdx;
-            seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx , neighbor);
+            seeTrainingPoint(reconstructionSet[candidateIdx].neighborIdx , msnvMAP_neighbor);
             proba = PROBA_weight(reconstructionSet[candidateIdx].weight);
-            total_k[transformIdx]+=(proba * reconstructionEuclideanDistance(target,
-                                                                            neighbor,
-                                                                            transformIdx,
-                                                                            reconstruction));
+            msnvMAP_total_k[transformIdx]+=(proba * reconstructionEuclideanDistance(msnvMAP_target,
+                                                                                    msnvMAP_neighbor,
+                                                                                    transformIdx,
+                                                                                    msnvMAP_reconstruction));
             candidateIdx ++;
         }
-    noiseVariance = (2*beta + sum(total_k))/(2*alpha - 2 + trainingSetLength*inputSpaceDim);  
     }
+    noiseVariance = (2*beta + sum(msnvMAP_total_k))/(2*alpha - 2 + trainingSetLength*inputSpaceDim);  
+        
 }
  
 //!returns the distance between the reconstruction and the target

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-24 00:22:17 UTC (rev 8019)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-24 17:25:21 UTC (rev 8020)
@@ -627,9 +627,40 @@
     //!Vectors of matrices that will be used in transformations parameters updating process.
     //!Each matrix is a view on a sub-matrix in th bigger matrix "B_C" described above.
     TVec<Mat> B,C;
+
+    // Temporary storage for internal methods
+    mutable Vec newDistribution;
+    mutable Vec ses_target;
+    mutable Vec ses_neighbor;
+    mutable Vec ses_predictedTarget;
+    mutable Vec lg_neighbor;
+    mutable Vec lg_predictedTarget;
+    mutable Vec stp_v;
+    mutable real stp_w;
+    mutable Vec fnn_target;
+    mutable Vec fnn_neighbor;
+    mutable Vec fbtrc_target;
+    mutable Vec fbtrc_neighbor;
+    mutable Vec fbtrc_predictedTarget;
+    mutable Vec fbwn_target;
+    mutable Vec fbwn_neighbor;
+    mutable Vec fbwn_predictedTarget;
+    mutable Vec mst_v;
+    mutable Vec mst_target;
+    mutable Vec mst_neighbor;
+    mutable TVec<int> mst_pivots;
+    mutable Mat msb_newBiasSet;
+    mutable Vec msb_norms;
+    mutable Vec msb_target;
+    mutable Vec msb_neighbor;
+    mutable Vec msb_reconstruction;
+    mutable Vec msnvMAP_total_k;
+    mutable Vec msnvMAP_target;
+    mutable Vec msnvMAP_neighbor;
+    mutable Vec msnvMAP_reconstruction;
     
-   
-    
+
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From lysiane at mail.berlios.de  Fri Aug 24 21:06:43 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Fri, 24 Aug 2007 21:06:43 +0200
Subject: [Plearn-commits] r8021 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200708241906.l7OJ6hHP004448@sheep.berlios.de>

Author: lysiane
Date: 2007-08-24 21:06:43 +0200 (Fri, 24 Aug 2007)
New Revision: 8021

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
modifications for more diversity : MStepTransformationDiv written


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-24 17:25:21 UTC (rev 8020)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-08-24 19:06:43 UTC (rev 8021)
@@ -61,6 +61,8 @@
     regOnNoiseVariance(false),
     learnTransformDistribution(false),
     regOnTransformDistribution(false),
+    emphasisOnDiversity(false),
+    diversityFactor(0),
     initializationMode(INIT_MODE_DEFAULT),
     largeEStepAPeriod(UNDEFINED),
     largeEStepAOffset(UNDEFINED),
@@ -150,6 +152,21 @@
                   "yes/no: prior assumptions on the transformation distribution ?");
     
     declareOption(ol,
+                  "emphasisOnDiversity",
+                  &TransformationLearner::emphasisOnDiversity,
+                  OptionBase::buildoption,
+                  "increase probability of a set of transformations if they are more diversified \n"
+                  "note: -the learning process is changed :\n"
+                  "       the transformation functions can no more be updated at the same time \n"
+                  "      -we assume there are no bias added to the transformation functions \n");
+
+    declareOption(ol,
+                  "diversityFactor",
+                  &TransformationLearner::diversityFactor,
+                  OptionBase::buildoption,
+                  "positive real number: high value  gives  high importance to diversity among transformations \n"
+                  "(has an effect only if the boolean 'emphasisOnDiversity' is set to True)\n");
+    declareOption(ol,
                   "initializationMode",
                   &TransformationLearner::initializationMode,
                   OptionBase::buildoption,
@@ -490,6 +507,11 @@
                   &TransformationLearner::MStepTransformations,
                   (BodyDoc("maximization step with respect to transformation matrices (MAP version)")));
     declareMethod(rmm,
+                  "MStepTransformationDiv",
+                  &TransformationLearner::MStepTransformationDiv,
+                  (BodyDoc("maximization step with respect to a specific transformation matrix (MAP version + emphasis on diversity)"),
+                   ArgDoc("int transformIdx","index of the transformation matrix to optimize")));
+    declareMethod(rmm,
                   "MStepBias",
                   &TransformationLearner::MStepBias,
                   (BodyDoc("maximization step with respect to transformation bias (MAP version)")));
@@ -792,6 +814,7 @@
     //dimension of the input space
     inputSpaceDim = train_set->inputsize();
 
+    //some storage variables that we will re-use to save time
     newDistribution.resize(nbTransforms) ;
     ses_target.resize(inputSpaceDim);
     ses_neighbor.resize(inputSpaceDim);
@@ -819,8 +842,23 @@
     msnvMAP_target.resize(inputSpaceDim);
     msnvMAP_neighbor.resize(inputSpaceDim);
     msnvMAP_reconstruction.resize(inputSpaceDim);
-    
+    mstd_B.resize(inputSpaceDim,inputSpaceDim);
+    mstd_C.resize(inputSpaceDim,inputSpaceDim);
+    mstd_D.resize(inputSpaceDim,inputSpaceDim);
+    mstd_v.resize(inputSpaceDim);
+    mstd_target.resize(inputSpaceDim);
+    mstd_neighbor.resize(inputSpaceDim);
 
+    //put more emphasis on diversity among transformation?
+    if(emphasisOnDiversity){
+        PLASSERT(!withBias);
+        if(diversityFactor<=0){
+            diversityFactor = (nbTransforms - 1)*1.0/transformsVariance;  
+        }
+    }
+    else{
+        diversityFactor = 0;
+    }
 
 
     int defaultPeriod = 1;
@@ -2075,8 +2113,15 @@
         MStepTransformDistribution();
     if(biasPeriod > 0 && stage % biasPeriod == biasOffset)
         MStepBias();
-    if(stage % transformsPeriod == transformsOffset)
-        MStepTransformations();    
+    if(stage % transformsPeriod == transformsOffset){
+        if(emphasisOnDiversity){
+            int t  = ((stage - transformsOffset)/transformsPeriod) % nbTransforms;
+            MStepTransformationDiv(t);
+        }
+        else{
+            MStepTransformations();
+        }    
+    }
 }
 
 //!maximization step  with respect to  transformation distribution
@@ -2205,10 +2250,67 @@
 
  
 
+//!TODO
+ //!maximization step with respect to a specific transformation matrix
+//! - it is also a MAP version, but this time the prior probability of the matrix is different:
+//!   we put more probability on a matrix that diverges from the other transformations matrices
+//! - for the moment, we assume that they are no bias associated to the function
+void TransformationLearner::MStepTransformationDiv(int transformIdx){
+    //set the m dXd matrices Ck and Bk , k in{1, ...,m} to 0.
+    mstd_B.clear();
+    mstd_C.clear();
+    mstd_D.clear();
+    
+    for(int t=0; t<nbTransforms ; t++){
+        if(t != transformIdx){
+            mstd_D += transforms[t];
+        }
+    }
+    mstd_D *= -2*diversityFactor;
+   
 
+    real lambda = 1.0*noiseVariance*(1.0/transformsVariance -2*(nbTransforms - 1)*diversityFactor);
+    for(int idx=0 ; idx<nbReconstructions ; idx++){
+        
+        //catch a view on the next entry of our dataset, that is, a  triple:
+        //(target_idx, neighbor_idx, transformation_idx)
+        
+        real p = PROBA_weight(reconstructionSet[idx].weight);
+  
+        //catch the target and neighbor points from the training set
+        
+        seeTrainingPoint(reconstructionSet[idx].targetIdx, mstd_target);
+        seeTrainingPoint(reconstructionSet[idx].neighborIdx, mstd_neighbor);
+        
+        if( reconstructionSet[idx].transformIdx == transformIdx){
+            mstd_v << mstd_target;
+            if(transformFamily == TRANSFORM_FAMILY_LINEAR_INCREMENT){
+                mstd_v = mstd_v - mstd_neighbor;
+            }
+     
+            //at the end, we want that matrix C[t] represents
+            //the matrix ( (NeighborPart(t)_T)W(NeighborPart(t)) + lambdaI ) transposed. 
+            externalProductScaleAcc(mstd_C, mstd_neighbor, mstd_neighbor, p);
+            
+            //at the end, that matrix B[t] represents
+            //the matrix (NeighborPart(t)_T)W(TargetPart(t)) transposed.
+            //externalProductScaleAcc(B[t], neighbor, v,p);
+            externalProductScaleAcc(mstd_B,mstd_v,mstd_neighbor,p);
+        }
+        
+    }
 
+    addToDiagonal(mstd_C,lambda);
+    //transforms[t] << solveLinearSystem(C[t], B[t]); 
+    mstd_B += mstd_D;
+    lapackSolveLinearSystem(mstd_C,mstd_B, mst_pivots);
+    transforms[transformIdx] << mstd_B;
+    
+}
 
-//TODO
+
+
+
 //!maximization step with respect to transformation bias
 //!(MAP version)
 void TransformationLearner::MStepBias(){

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-24 17:25:21 UTC (rev 8020)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-08-24 19:06:43 UTC (rev 8021)
@@ -212,6 +212,15 @@
     //!if we learn the transformation distribution, do we use the MAP estimator ?
     bool regOnTransformDistribution;
 
+    //!set to True, it modifies the way the transformation parameters are learned
+    //!A term which represents diversity among transformations is added to the function
+    //!to optimize : div_factor*sum(||theta_i - theta_j ||^2)
+    //!The transformations can no more be updated all the same time
+    //!We will need to define periods and offsets to know when to update them.  
+    bool emphasisOnDiversity;
+    real diversityFactor;
+   
+
     //!how the initial values of the parameters to learn are choosen?
     int initializationMode;
 
@@ -264,6 +273,9 @@
     int biasPeriod;
     int biasOffset;
 
+    
+
+
     //PARAMETERS OF THE DISTRIBUTION
 
     //! variance of the NOISE random variable. 
@@ -658,9 +670,13 @@
     mutable Vec msnvMAP_target;
     mutable Vec msnvMAP_neighbor;
     mutable Vec msnvMAP_reconstruction;
-    
+    mutable Mat mstd_B;
+    mutable Mat mstd_C;
+    mutable Mat mstd_D;
+    mutable Vec mstd_v;
+    mutable Vec mstd_target;
+    mutable Vec mstd_neighbor;
 
-
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -849,6 +865,7 @@
     //!parameters
     void MStepTransformDistribution();
     
+
     //!maximization step  with respect to transformation distribution
     //!parameters
     //!(MAP version, alpha = dirichlet prior distribution parameter)
@@ -859,6 +876,12 @@
     //!(MAP version)
     void MStepTransformations();
   
+    //!maximization step with respect to a specific transformation matrix
+    //! - it is a MAP version, prior probability of the matrix put 
+    //!   more emphasis on a matrix that diverges from the other transformations matrices
+    void MStepTransformationDiv(int transformIdx);
+
+
     //!maximization step with respect to transformation bias
     //!(MAP version)
     void MStepBias();



From nouiz at mail.berlios.de  Fri Aug 24 22:44:13 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 24 Aug 2007 22:44:13 +0200
Subject: [Plearn-commits] r8022 - trunk/scripts
Message-ID: <200708242044.l7OKiDY7013983@sheep.berlios.de>

Author: nouiz
Date: 2007-08-24 22:44:12 +0200 (Fri, 24 Aug 2007)
New Revision: 8022

Modified:
   trunk/scripts/collectres
Log:
Now accept cols name instead of column number. Catch more error.



Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-08-24 19:06:43 UTC (rev 8021)
+++ trunk/scripts/collectres	2007-08-24 20:44:12 UTC (rev 8022)
@@ -65,11 +65,17 @@
   if loc_mode=="pos":
     row=int(loc_specs[1])
     if row<0:
-      row = max(0,a.shape[0]+row)
+      row = max(0,a.length+row)
     i=2
-    if row<a.shape[0]:
+    if row<a.length:
       while len(loc_specs[i:])>0:
-        res.append(a[row,int(loc_specs[i])])
+        if loc_specs[i]=="all":
+          res.extend(a[row])
+        elif loc_specs[i] in a.fieldnames:
+          index=a.fieldnames.index(loc_specs[i])
+          res.append(a[row,index])
+        else:
+          res.append(a[row,int(loc_specs[i])])
         i+=1
   elif loc_mode=="mincol":
     mcol = int(loc_specs[1])
@@ -77,13 +83,19 @@
     print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
     i=1
     while len(loc_specs[i:])>0:
-      res.append(a[mrow,int(loc_specs[i])])
+      if loc_specs[i]=="all":
+        res.extend(a[mrow])
+      elif loc_specs[i] in a.fieldnames:
+        index=a.fieldnames.index(loc_specs[i])
+        res.append(a[mrow,index])
+      else:
+        res.append(a[mrow,int(loc_specs[i])])
       i+=1
   elif loc_mode=="cols":
     minrow = int(loc_specs[3])
     maxrow = int(loc_specs[4])
     if maxrow<0:
-      maxrow = a.shape[0]+maxrow
+      maxrow = a.length+maxrow
     maxrow += 1
     i=5
     b=[]
@@ -96,7 +108,7 @@
       else:
         b=a[minrow:maxrow,col].copy()
         b = b.resize([maxrow-minrow,1])
-        la = a.shape[0]
+        la = a.length
         if la<maxrow:
             b[la-minrow:] = float('NaN')
       if b:
@@ -114,12 +126,20 @@
   for filename in filenames:
     try:
       file_res = selectres(loc_specs,
-                           load_pmat_as_array(filename))
+                           PMat(filename))
       all_results.append([file_res,filename])
     except ValueError,v:
       print >>sys.stderr, "caught ValueError exception in", filename
       print >>sys.stderr, v
       print >>sys.stderr, ""
+    except IndexError,v:
+      print >>sys.stderr, "caught IndexError exception in", filename
+      print >>sys.stderr, v
+      print >>sys.stderr, ""
+    except IOError,v:
+      print >>sys.stderr, "caught IOError exception in", filename
+      print >>sys.stderr, v
+      print >>sys.stderr, ""
   return all_results
 
 def compare_res(x,y):



From nouiz at mail.berlios.de  Sat Aug 25 22:22:18 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sat, 25 Aug 2007 22:22:18 +0200
Subject: [Plearn-commits] r8023 - trunk/python_modules/plearn/parallel
Message-ID: <200708252022.l7PKMIYJ031792@sheep.berlios.de>

Author: nouiz
Date: 2007-08-25 22:22:17 +0200 (Sat, 25 Aug 2007)
New Revision: 8023

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Many small improuvement and bugfix
-print more info
-bugfix of the handling of cluster parameter
..


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-24 20:44:12 UTC (rev 8022)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-25 20:22:17 UTC (rev 8023)
@@ -58,7 +58,7 @@
         #
         self.file_redirect_stdout = True
         self.file_redirect_stderr = True
-        self.redirect_stderr_to_stdout = True
+        self.redirect_stderr_to_stdout = False
 
         # Initialize the namespace
         self.requirements = ''
@@ -70,9 +70,9 @@
             self.__dict__[key] = args[key]
 
         # check if log directory exists, if not create it
-#        if (not os.path.exists(self.log_dir)):
+        if (not os.path.exists(self.log_dir)):
 #            if self.dolog or self.file_redirect_stdout or self.file_redirect_stderr:
-        os.mkdir(self.log_dir)
+            os.mkdir(self.log_dir)
 
         # If some arguments aren't lists, put them in a list
         if not isinstance(commands, list):
@@ -148,7 +148,8 @@
 
         for key in args.keys():
             self.__dict__[key] = args[key]
-
+        self.dolog = dolog
+        
         formatted_command = re.sub( '[^a-zA-Z0-9]', '_', command );
         if gen_unique_id:
             self.unique_id = get_new_sid('')#compation intense
@@ -168,7 +169,7 @@
         if len(pre_tasks) > 0:
             self.commands.extend( pre_tasks )
 
-        if dolog == True:
+        if self.dolog == True:
             self.commands.append(utils_file + ' set_config_value '+
                 string.join([self.log_file,'STATUS',str(STATUS_RUNNING)],' '))
             # set the current date in the field LAUNCH_TIME
@@ -178,7 +179,7 @@
 
         self.commands.append( command )
         self.commands.extend( post_tasks )
-        if dolog == True:
+        if self.dolog == True:
             self.commands.append(utils_file + ' set_config_value '+
                 string.join([self.log_file,'STATUS',str(STATUS_FINISHED)],' '))
             # set the current date in the field FINISHED_TIME
@@ -249,7 +250,7 @@
 
     def __init__(self, commands, **args ):
         DBIBase.__init__(self, commands, **args)
-        add_commands(commands)
+        self.add_commands(commands)
 
     def add_commands(self,commands):
         if not isinstance(commands, list):
@@ -265,14 +266,16 @@
     def run_one_job(self, task):
         DBIBase.run(self)
         
-        command = "cluster --execute" 
-        if self.arch == 32:
-            command += "--typecpu 32bits"
-        elif self.arch == 64:
-            command += "--typecpu 64bits"
-        if self.arch==3264:
-            command += "--typecpu all"
-        command += " '"+string.join(task.commands,';') + "'"
+        command = "cluster" 
+        if self.arch == "32":
+            command += " --typecpu 32bits"
+        elif self.arch == "64":
+            command += " --typecpu 64bits"
+        elif self.arch == "3264":
+            command += " --typecpu all"
+        if self.duree:
+            command += " --duree "+self.duree
+        command += " --execute '"+string.join(task.commands,';') + "'"
         
         print "[DBI] "+command
 
@@ -282,7 +285,7 @@
         task.launch_time = time.time()
         task.set_scheduled_time()
 
-        (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
+        (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
 
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
 
@@ -302,7 +305,7 @@
         if len(self.post_batch)>0:
             exec_post_batch()
 
-        print "[DBI]The Log file are under %s"%self.log_dir
+        print "[DBI] The Log file are under %s"%self.log_dir
 
     def clean(self):
         #TODO: delete all log files for the current batch
@@ -325,7 +328,7 @@
         # create the information about the tasks
         args['temp_dir'] = self.temp_dir
         
-        add_commands(commands)
+        self.add_commands(commands)
 
     def add_commands(self,commands):
         if not isinstance(commands, list):
@@ -418,7 +421,7 @@
         if not os.path.exists(self.tmp_dir):
             os.mkdir(self.tmp_dir)
 
-        add_commands(commands)
+        self.add_commands(commands)
 
     def add_commands(self,commands):
         if not isinstance(commands, list):
@@ -690,19 +693,22 @@
 
     def run_one_job(self,task):
         c = (';'.join(task.commands))
-        print "[DBI] "+c
+        task.set_scheduled_time()
+
         if self.test:
+            print "[DBI] "+c
             return
-        task.set_scheduled_time()
 
         (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
 
         if self.nb_proc>1:
             self.sema.acquire()
+            print "[DBI] ",time.ctime()+"::"+c
             p = Popen(c, shell=True,stdout=output,stderr=error)
             p.wait()
             self.sema.release()
         else:
+            print "[DBI] ",time.ctime()+"::"+c
             p = Popen(c, shell=True,stdout=output,stderr=error)
             p.wait()
             
@@ -808,7 +814,7 @@
         print "[DBI] Use at your own risk!"
         DBIBase.__init__(self, commands, **args)
 
-        add_commands(commands)
+        self.add_commands(commands)
         self.hosts= find_all_ssh_hosts()
         
     def add_commands(self,commands):
@@ -836,7 +842,7 @@
 
         cwd= os.getcwd()
         command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
-        print "[DBI]"+command
+        print "[DBI] "+command
 
         if self.test:
             return



From nouiz at mail.berlios.de  Sun Aug 26 02:08:42 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 26 Aug 2007 02:08:42 +0200
Subject: [Plearn-commits] r8024 - trunk/scripts
Message-ID: <200708260008.l7Q08gK3028273@sheep.berlios.de>

Author: nouiz
Date: 2007-08-26 02:08:41 +0200 (Sun, 26 Aug 2007)
New Revision: 8024

Modified:
   trunk/scripts/cdispatch
Log:
-added option for cluster --duree=X and --wait
-better log dir
-small refactoring


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-08-25 20:22:17 UTC (rev 8023)
+++ trunk/scripts/cdispatch	2007-08-26 00:08:41 UTC (rev 8024)
@@ -2,12 +2,15 @@
 import sys,os,re,time,datetime
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--log|--nolog] [--cluster|--local[=nb_process]] [--test] [--req="CONDOR_REQUIREMENT"] [--file=FILEPATH | <command-template>] [--32|--64|--3264]'
+ShortHelp='Usage: cdispatch [--help|-h] [--log|*--nolog] [--cluster|--local[=nb_process]|*--condor] [--test] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
 LongHelp="""
 Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
 
 %s
 
+parameter only for cluster:--duree,--wait,--3264,--32,--64
+parameter only for condor:--req=X
+
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain segments of the form {{a,b,c,d}}, which trigger
@@ -58,35 +61,41 @@
 optionargs = []
 otherargs = []
 FILE = ""
-REQ = ""
-ARCH = ""
-NB_PROC=None
+dbi_param={}
+
 for argv in sys.argv[1:]:
 
     if argv == "--help" or argv == "-h":
         print LongHelp
         sys.exit(1)
     elif argv == "--nolog":
-        optionargs.append(argv[2:])
+        dbi_param["dolog"]=False
     elif argv == "--log":
-        optionargs.append(argv[2:])
+        dbi_param["dolog"]=True
     elif argv == "--cluster":
         optionargs.append(argv[2:])
-    elif argv[0:7] == "--local":
+    elif argv == "--condor":
+        #it is the default
+        optionargs.append(argv[2:])
+    elif argv.startswith("--duree="):
+        dbi_param["duree"]=argv[8:]
+    elif argv.startswith("--local"):
         optionargs.append(argv[2:7])
         if len(argv)>7:
             assert(argv[7]=="=")
             NB_PROC=argv[8:]
+            dbi_param["nb_proc"]=argv[8:]
     elif argv == "--test":
-        optionargs.append(argv[2:])
-    elif argv[0:7] == "--file=":
+        dbi_param["test"]=True
+    elif argv.startswith("--file="):
         FILE = argv[7:]
         optionargs.append(argv[2:])
     elif argv == "--32"  or argv == "--64" or argv == "--3264":
-        ARCH=argv[2:]
-        optionargs.append(argv[2:])
+        dbi_param["arch"]=argv[2:]
+    elif argv == "--wait":
+        dbi_param["wait"]=True
     elif argv[0:6] == "--req=":
-        REQ = argv[6:]
+        dbi_param["requirements"]="\"%s\""%argv[6:]
     elif argv[0:1] == '-':
 	print "Unknow parameter (%s)",argv
 	print ShortHelp
@@ -158,29 +167,17 @@
 else:
     launch_cmd='Condor'
 
-dbi_param={}
-if REQ != "":
-    dbi_param["requirements"]="\"%s\""%REQ
-if "test" in optionargs:
-    dbi_param["test"]=True
-if "log" in optionargs:
-    dbi_param["dolog"]=True
-else:
-    dbi_param["dolog"]=False
-if ARCH!="":
-    dbi_param["arch"]=ARCH
-if NB_PROC:
-    dbi_param["nb_proc"]=NB_PROC
     
-tmp="_".join(sys.argv)
-tmp=tmp[tmp.find("cdispatch"):]
+t = [x for x in sys.argv[1:] if not x[:2]=="--"]
+t[0]=os.path.split(t[0])[1]
+tmp="_".join(t)
 tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
 tmp+=str(datetime.datetime.now()).replace(' ','_')
-print tmp
+print "tmp:",tmp
 dbi_param["log_dir"]=os.path.join("LOGS",tmp)
 dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
-if "test" in optionargs:
+if "test" in dbi_param:
     print "We generated %s command in the file"% len(commands)
     print "The script %s was not launched"% ScriptName
     SCRIPT=open(ScriptName,'w');
@@ -210,6 +207,7 @@
 else:
     print "We generate the DBI object with %s command"%(len(commands))
     from plearn.parallel.dbi import *
+    print time.ctime()
     t1=time.time()
     jobs = DBI(commands,launch_cmd,**dbi_param)
     t2=time.time()



From nouiz at mail.berlios.de  Sun Aug 26 02:14:51 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 26 Aug 2007 02:14:51 +0200
Subject: [Plearn-commits] r8025 - trunk/plearn_learners/generic
Message-ID: <200708260014.l7Q0Ep1M031003@sheep.berlios.de>

Author: nouiz
Date: 2007-08-26 02:14:49 +0200 (Sun, 26 Aug 2007)
New Revision: 8025

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
Changed how the confusion_matrix is done so that it work with learner that give as output: probability, target or target and more.
The case target and more is seen with the class RegressionTree who give the confidance of the output.


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-26 00:08:41 UTC (rev 8024)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2007-08-26 00:14:49 UTC (rev 8025)
@@ -540,36 +540,46 @@
         } else if (c == "confusion_matrix") {
 
 #ifdef BOUNDCHECK
-            if (confusion_matrix_target >= target_length)
+            if (confusion_matrix_target >= target_length || confusion_matrix_target<-1)
                 PLERROR("In AddCostToLearner::computeCostsFromOutputs - confusion_matrix_target(%d) "
-                        "not in the range of target_length(%d)", confusion_matrix_target, target_length);            
-            if (sub_learner_output[confusion_matrix_target] >= n_classes
-                || is_missing(sub_learner_output[confusion_matrix_target]))
-                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of sub_learner: sub_learner_output[confusion_matrix_target]=%f,  "
+                        "not in the range of target_length(%d)", confusion_matrix_target, target_length);
+#endif
+            int sub_learner_out;
+            real the_target;
+            if (confusion_matrix_target==-1) {
+                //output are probability
+                sub_learner_out = argmax(sub_learner_output);
+                the_target = desired_target[0];
+            }else{
+                sub_learner_out = int(round(sub_learner_output[confusion_matrix_target]));
+                the_target = desired_target[confusion_matrix_target];
+            }
+	    if(sub_learner_out<0){
+	      PLWARNING("In AddCostToLearner::computeCostsFromOutputs - bad value for sub_learner_out %d, we use 0 instead", sub_learner_out);
+	      sub_learner_out = 0;
+	    }
+	    if(sub_learner_out>=n_classes){
+	      PLWARNING("In AddCostToLearner::computeCostsFromOutputs - bad value for sub_learner_out %d, we use %d instead", sub_learner_out,n_classes -1);
+	      sub_learner_out = n_classes - 1;
+	    }
+            PLCHECK(sub_learner_out<n_classes && sub_learner_out>=0);
+//if outside allowd range, will access the wrong element in the cost vector
+#ifdef BOUNDCHECK
+            if (sub_learner_out >= n_classes
+                || is_missing(sub_learner_out))
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of sub_learner: sub_learner_out=%f,  "
                         " missing or higher or egual to n_classes (%d)",
-                        sub_learner_output[confusion_matrix_target],n_classes);
-            if (desired_target[confusion_matrix_target] >= n_classes
-                ||is_missing(desired_target[confusion_matrix_target]))
-                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of desired_target[i]=%f, missing or higher or egual to n_classes (%d)",
+                        sub_learner_out,n_classes);
+            if (the_target >= n_classes
+                ||is_missing(the_target))
+                PLERROR("In AddCostToLearner::computeCostsFromOutputs - bad output value of the_target=%f, missing or higher or egual to n_classes (%d)",
                         desired_target[confusion_matrix_target], n_classes);
 #endif
             for(int local_ind = ind_cost ; local_ind < (n_classes*n_classes+ind_cost); local_ind++){
                 costs[local_ind] = 0;
             }
-            int output_length = sub_learner_output.length();
-            int local_ind = 0;
-            if (output_length == target_length) {
-                int sub_learner_out = int(round(sub_learner_output[confusion_matrix_target]));
-//if outside allowd range, will access the wrong element in the cost vector
-                PLASSERT(sub_learner_out>=n_classes || sub_learner_out<0);
-                local_ind = ind_cost + sub_learner_out
-                    + int(round(desired_target[confusion_matrix_target]))*n_classes;
-            } else if (target_length == 1){
-                local_ind = ind_cost + argmax(sub_learner_output) + int(round(desired_target[confusion_matrix_target]))*n_classes;
-            } else {
-                PLERROR("In AddCostToLearner::computeCostsFromOutputs - Wrong "
-                        "output and/or target for the 'confusion_matrix' cost");
-            }
+            int local_ind = ind_cost + sub_learner_out + int(round(the_target))*n_classes;
+
             costs[local_ind] = 1;
             ind_cost += n_classes*n_classes - 1;//less one as the loop add one
         } else if (c == "mse") {



From nouiz at mail.berlios.de  Sun Aug 26 02:20:42 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 26 Aug 2007 02:20:42 +0200
Subject: [Plearn-commits] r8026 - trunk/plearn/sys
Message-ID: <200708260020.l7Q0Kg8N001677@sheep.berlios.de>

Author: nouiz
Date: 2007-08-26 02:20:42 +0200 (Sun, 26 Aug 2007)
New Revision: 8026

Modified:
   trunk/plearn/sys/Profiler.cc
Log:
print more information for debugging


Modified: trunk/plearn/sys/Profiler.cc
===================================================================
--- trunk/plearn/sys/Profiler.cc	2007-08-26 00:14:49 UTC (rev 8025)
+++ trunk/plearn/sys/Profiler.cc	2007-08-26 00:20:42 UTC (rev 8026)
@@ -143,8 +143,8 @@
 {
     map<string,Stats>::iterator it = codes_statistics.find(name_of_piece_of_code);
     if (it == codes_statistics.end())
-        PLERROR("Profiler::getStats: cannot find statistics for '%s'",
-                name_of_piece_of_code.c_str());
+        PLERROR("Profiler::getStats: cannot find statistics for '%s'. the active variable is at: %d",
+                name_of_piece_of_code.c_str(),active);
     return it->second;
 }
 



From nouiz at mail.berlios.de  Mon Aug 27 23:01:57 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 27 Aug 2007 23:01:57 +0200
Subject: [Plearn-commits] r8027 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200708272101.l7RL1vJ4028994@sheep.berlios.de>

Author: nouiz
Date: 2007-08-27 23:01:56 +0200 (Mon, 27 Aug 2007)
New Revision: 8027

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
-Better help message
-Added parameter for --cluster=nb_proc and --nb_proc=X
-Added MultiTheard that created a fixed number of thread to execute a function with a list of different arguments
-Added/corrected option of cluster --duree=X --typecpu --wait
-Added detection of launch failure for cluster


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-26 00:20:42 UTC (rev 8026)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-27 21:01:56 UTC (rev 8027)
@@ -12,7 +12,7 @@
 from configobj import ConfigObj
 from textwrap import dedent
 import pdb
-from threading import Thread,BoundedSemaphore
+from threading import Thread,Lock
 from time import sleep
 import datetime
 
@@ -21,7 +21,50 @@
 STATUS_WAITING = 2
 STATUS_ERROR = 3
 
-
+#original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
+class LockedIterator:
+    def __init__( self, iterator ):
+        self._lock     = Lock()
+        self._iterator = iterator
+        
+    def __iter__( self ):
+        return self
+    
+    def next( self ):
+        try:
+            self._lock.acquire()
+            return self._iterator.next()
+        finally:
+            self._lock.release()
+            
+#original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
+class MultiThread:
+    def __init__( self, function, argsVector, maxThreads=5, print_when_finished=None):
+        self._function     = function
+        self._argsIterator = LockedIterator( iter( argsVector ) )
+        self._threadPool   = []
+        self.print_when_finish = print_when_finished
+        self.running = 0
+        for i in range( maxThreads ):
+            self._threadPool.append( Thread( target=self._tailRecurse ) )
+            
+    def _tailRecurse( self ):
+        for args in self._argsIterator:
+            self._function( args )
+        self.running-=1
+        if self.print_when_finish:
+            print self.print_when_finish,"left running:",self.running
+            
+    def start( self  ):
+        for thread in self._threadPool:
+            time.sleep( 0 ) # necessary to give other threads a chance to run
+            self.running+=1
+            thread.start()
+            
+    def join( self, timeout=None ):
+        for thread in self._threadPool:
+            thread.join( timeout )
+                    
 class DBIBase:
 
     def __init__(self, commands, **args ):
@@ -249,8 +292,15 @@
 class DBICluster(DBIBase):
 
     def __init__(self, commands, **args ):
+        self.duree=None
+        self.arch=None
+        self.wait=None
+        self.threads=[]
+        self.started=0
+        self.nb_proc=50
         DBIBase.__init__(self, commands, **args)
         self.add_commands(commands)
+        self.nb_proc=int(self.nb_proc)
 
     def add_commands(self,commands):
         if not isinstance(commands, list):
@@ -275,10 +325,12 @@
             command += " --typecpu all"
         if self.duree:
             command += " --duree "+self.duree
+        if self.wait:
+            command += " --wait"
         command += " --execute '"+string.join(task.commands,';') + "'"
-        
-        print "[DBI] "+command
-
+        self.started+=1
+        started=self.started# not thread safe!!!
+        print "[DBI,%d/%d,%s] %s"%(started,len(self.tasks),time.ctime(),command)
         if self.test:
             return
 
@@ -286,9 +338,11 @@
         task.set_scheduled_time()
 
         (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
-
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
-
+        ret=task.p.wait()
+        if task.p.returncode!=0:
+            print "[DBI,%d/%d,%s] Failed to launch: '%s' returned %d,%d"%(started,len(self.tasks),time.ctime(),command,task.p.returncode,ret)
+            
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
         if self.test:
@@ -298,8 +352,9 @@
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        for task in self.tasks:
-            self.run_one_job(task)
+        mt= MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
+        mt.start()
+        mt.join()
 
         # Execute post-batchs
         if len(self.post_batch)>0:
@@ -651,6 +706,8 @@
             self.nb_proc=int(self.nb_proc)
         self.args=args
         self.threads=[]
+        self.started=0
+        self.nb_proc=int(self.nb_proc)
         self.add_commands(commands)
             
     def add_commands(self,commands):
@@ -701,16 +758,10 @@
 
         (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
 
-        if self.nb_proc>1:
-            self.sema.acquire()
-            print "[DBI] ",time.ctime()+"::"+c
-            p = Popen(c, shell=True,stdout=output,stderr=error)
-            p.wait()
-            self.sema.release()
-        else:
-            print "[DBI] ",time.ctime()+"::"+c
-            p = Popen(c, shell=True,stdout=output,stderr=error)
-            p.wait()
+        self.started+=1
+        print "[DBI,%d/%d,%s] %s"%(self.started,len(self.tasks),time.ctime(),c)
+        p = Popen(c, shell=True,stdout=output,stderr=error)
+        p.wait()
             
     def clean(self):
         if len(self.temp_files)>0:
@@ -736,18 +787,10 @@
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        if not self.test and self.nb_proc>1:
-            self.sema=BoundedSemaphore(int(self.nb_proc))
-            for task in self.tasks:
-                t=Thread(target=self.run_one_job,args=(task,))
-                t.start()
-                self.threads.append(t)
-            for t in self.threads:
-                t.join()
-        else:
-            for task in self.tasks:
-                self.run_one_job(task)
-
+        mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
+        mt.start()
+        mt.join()
+        
         # Execute post-batchs
         if len(self.post_batch)>0:
             exec_post_batch()

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-08-26 00:20:42 UTC (rev 8026)
+++ trunk/scripts/cdispatch	2007-08-27 21:01:56 UTC (rev 8027)
@@ -2,15 +2,18 @@
 import sys,os,re,time,datetime
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--log|*--nolog] [--cluster|--local[=nb_process]|*--condor] [--test] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
+ShortHelp='Usage: cdispatch [--help|-h] [--log|*--nolog] [--cluster[=nb_process]|--local[=nb_process]|*--condor] [--nb_proc=nb_process] [--test] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
 LongHelp="""
 Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
 
 %s
-
-parameter only for cluster:--duree,--wait,--3264,--32,--64
+parameter for local and cluster:--nb_proc=INT, give the maximum number of concurent jobs running
+parameter only for cluster:--duree,--wait,--3264,--32,--64,
 parameter only for condor:--req=X
 
+parameter --local=X is the same as --local --nb_proc=X
+parameter --cluster=X is the same as --cluster --nb_proc=X
+
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain segments of the form {{a,b,c,d}}, which trigger
@@ -72,8 +75,11 @@
         dbi_param["dolog"]=False
     elif argv == "--log":
         dbi_param["dolog"]=True
-    elif argv == "--cluster":
-        optionargs.append(argv[2:])
+    elif argv.startswith("--cluster"):
+        optionargs.append(argv[2:9])
+        if len(argv)>9:
+            assert(argv[9]=="=")
+            dbi_param["nb_proc"]=argv[10:]
     elif argv == "--condor":
         #it is the default
         optionargs.append(argv[2:])
@@ -83,8 +89,9 @@
         optionargs.append(argv[2:7])
         if len(argv)>7:
             assert(argv[7]=="=")
-            NB_PROC=argv[8:]
             dbi_param["nb_proc"]=argv[8:]
+    elif argv.startswith("--nb_proc="):
+        dbi_param["nb_proc"]=argv[10:]
     elif argv == "--test":
         dbi_param["test"]=True
     elif argv.startswith("--file="):



From nouiz at mail.berlios.de  Mon Aug 27 23:09:53 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 27 Aug 2007 23:09:53 +0200
Subject: [Plearn-commits] r8028 - trunk/python_modules/plearn/learners
Message-ID: <200708272109.l7RL9rtA029608@sheep.berlios.de>

Author: nouiz
Date: 2007-08-27 23:09:53 +0200 (Mon, 27 Aug 2007)
New Revision: 8028

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
-Added cost train_time and conflict
-Added option conf_rated_adaboost
-Corrected and ameliorated load_old_learner()


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-08-27 21:01:56 UTC (rev 8027)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-08-27 21:09:53 UTC (rev 8028)
@@ -1,7 +1,7 @@
 from plearn.pyext import *
 from plearn.pyplearn.plargs import *
+import time
 
-
 class AdaBoostMultiClasses:
 #class AdaBoost3PLearner(pl.PLearner):
     def __init__(self,trainSet1,trainSet2):
@@ -16,6 +16,7 @@
         self.learner2.setTrainingSet(trainSet2,True)
         self.nstages = 0
         self.stage = 0
+        self.train_time = 0
 #        self.confusion_target=plargs.confusion_target
         
     def weakLearner(self):
@@ -25,12 +26,13 @@
             ,loss_function_weight = 1
             ,missing_is_valid = plargs.missing_is_valid
             ,multiclass_outputs = plargs.multiclass_output
-            ,maximum_number_of_nodes = 500
+            ,maximum_number_of_nodes = 250
             ,compute_train_stats = 0
             ,complexity_penalty_factor = 0.0
             ,verbosity = 0
             ,report_progress = 1
             ,forget_when_training_set_changes = 1
+            ,conf_rated_adaboost = plargs.conf_rated_adaboost
             ,leave_template = pl.RegressionTreeLeave( )
             )
     
@@ -44,12 +46,15 @@
         return l
 
     def train(self):
+        t1=time.time()
         self.learner1.nstages = self.nstages
         self.learner1.train()
         self.learner2.nstages = self.nstages
         self.learner2.train()
         self.stage=self.learner1.stage
-
+        t2=time.time()
+        self.train_time+=t2-t1
+        
     def getTestCostNames(self):
         costnames = ["class_error","linear_class_error","square_class_error"]
         #    for i in range(len(conf_matrix)):
@@ -57,8 +62,10 @@
         for i in range(4):
             for j in range(3):
                 costnames.append("conf_matrix_%d_%d"%(i,j))
+        costnames.append("train_time")
+        costnames.append("conflict")
         return costnames
-
+    
     def computeOutput(self,example):
         """ compute the output for the example in the parameter
         
@@ -89,7 +96,13 @@
         for i in range(4):
             for j in range(3):
                 costs.append(0)
-        costs[output[0]*3+target+3]=1  
+        costs[output[0]*3+target+3]=1
+        costs.append(self.train_time)
+        if output[0]==3:
+            costs.append(1)
+        else:
+            costs.append(0)
+        
         return costs
         
     def outputsize(self):
@@ -105,18 +118,53 @@
         self.learner1.save(path+"learner1_stage#"+str(self.stage)+".psave",encoding)
         self.learner2.save(path+"learner2_stage#"+str(self.stage)+".psave",encoding)
     
-    def load_old_learner(self,filepath,stage,trainSet1,trainSet2):
-        print "load_old_learner"
-        self.old_learner1=self.learner1
-        self.old_learner2=self.learner2
-        self.learner1=loadObject(filepath+"/learner1_stage#"+str(stage)+".psave")
-        self.learner2=loadObject(filepath+"/learner2_stage#"+str(stage)+".psave")
-        assert(self.learner1.stage==self.learner2.stage)
+    def load_old_learner(self,filepath=None,trainSet1=None,trainSet2=None,stage1=-1,stage2=-1):
+        assert(trainSet1 and trainSet2)
+        if not filepath:
+            assert(not self.learner1.expdir.endswith("/learner1") or not self.learner2.expdir.endswith("/learner2"))
+            path=self.learner1.expdir[:-9]
+            assert(path==self.learner2.expdir[:-9])
+            i=path.rfind("-2007-")
+            (subdir,subfile)=os.path.split(path[:i])
+            tmp=[x for x in os.listdir(subdir) if x.startswith(subpath)]
+            assert(len(tmp)>0)
+            filepath=max(tmp)
+        #if stage=-1 we find the last one
+        if stage1 == -1:
+            s="learner1_stage#"
+            lens=len(s)
+            e=".psave"
+            lene=len(e)
+            tmp=[ x for x in os.listdir(filepath) if x.startswith(s) and x.endswith(".psave") ]
+            for x in tmp:
+                t=int(x[lens:-lene])
+                if t>stage1: stage1=t
+        #We must split stage1 and stage2 as one learner can early stop.
+        if stage2 == -1:
+            s="learner2_stage#"
+            lens=len(s)
+            e=".psave"
+            lene=len(e)
+            tmp=[ x for x in os.listdir(filepath) if x.startswith(s) and x.endswith(e) ]
+            for x in tmp:
+                t=int(x[lens:-lene])
+                if t>stage2: stage2=t
+                
+        file1=filepath+"/learner1_stage#"+str(stage1)+".psave"
+        file2=filepath+"/learner2_stage#"+str(stage2)+".psave"
+        if (not os.path.exists(file1)) or (not os.path.exists(file2)):
+            print "ERROR: no file to load in the gived directory"
+            sys.exit(1)
+        self.learner1=loadObject(file1)
+        self.learner2=loadObject(file2)
+        if not self.learner1.found_zero_error_weak_learner and not self.learner2.found_zero_error_weak_learner:
+            assert(self.learner1.stage==self.learner2.stage)
         self.stage=self.learner1.stage
         self.nstages=self.learner1.nstages
-#        self.learner1.expdir=plargs.expdirr+"/learner1"
-        self.learner1.setTrainingSet(trainSet1,False)
-        self.learner2.setTrainingSet(trainSet2,False)
-        print self.stage
-        print self.stage
-        
+        if trainSet1:
+            self.learner1.setTrainingSet(trainSet1,False)
+        if trainSet2:
+            self.learner2.setTrainingSet(trainSet2,False)
+        self.learner1.setTrainStatsCollector(VecStatsCollector())
+        self.learner2.setTrainStatsCollector(VecStatsCollector())
+



From nouiz at mail.berlios.de  Mon Aug 27 23:12:10 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 27 Aug 2007 23:12:10 +0200
Subject: [Plearn-commits] r8029 - trunk
Message-ID: <200708272112.l7RLCATD029754@sheep.berlios.de>

Author: nouiz
Date: 2007-08-27 23:12:10 +0200 (Mon, 27 Aug 2007)
New Revision: 8029

Modified:
   trunk/pymake.config.model
Log:
bug fix


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-27 21:09:53 UTC (rev 8028)
+++ trunk/pymake.config.model	2007-08-27 21:12:10 UTC (rev 8029)
@@ -294,6 +294,7 @@
 
 if not 'nopython' in optionargs:
     # First find which version of python is installed.
+    python_includedirs=[]
     if domain_name.endswith('iro.umontreal.ca'):
         optionargs += [ pyoption ]
         python_version = pyver



From nouiz at mail.berlios.de  Tue Aug 28 20:39:28 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Aug 2007 20:39:28 +0200
Subject: [Plearn-commits] r8030 - trunk/scripts
Message-ID: <200708281839.l7SIdSTk032459@sheep.berlios.de>

Author: nouiz
Date: 2007-08-28 20:39:28 +0200 (Tue, 28 Aug 2007)
New Revision: 8030

Modified:
   trunk/scripts/collectres
Log:
bugfix: resize returning None. We remove it as it is not needed anymore.


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-08-27 21:12:10 UTC (rev 8029)
+++ trunk/scripts/collectres	2007-08-28 18:39:28 UTC (rev 8030)
@@ -104,14 +104,12 @@
       col = int(loc_specs[i])
       if col<0:
         b = array(range(minrow,maxrow))
-        b = b.resize([maxrow-minrow,1])
       else:
         b=a[minrow:maxrow,col].copy()
-        b = b.resize([maxrow-minrow,1])
         la = a.length
         if la<maxrow:
             b[la-minrow:] = float('NaN')
-      if b:
+      if b.any():
         res=concatenate([res,b],1)
       if i==5:
         i=6



From nouiz at mail.berlios.de  Tue Aug 28 20:42:26 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Aug 2007 20:42:26 +0200
Subject: [Plearn-commits] r8031 - trunk/scripts
Message-ID: <200708281842.l7SIgQWH032649@sheep.berlios.de>

Author: nouiz
Date: 2007-08-28 20:42:25 +0200 (Tue, 28 Aug 2007)
New Revision: 8031

Modified:
   trunk/scripts/collectres
Log:
Now all for all columns number we should be able to use the name instead


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-08-28 18:39:28 UTC (rev 8030)
+++ trunk/scripts/collectres	2007-08-28 18:42:25 UTC (rev 8031)
@@ -58,12 +58,18 @@
 
 # should probably be an option
 separator = "_"
+def get_col_index(a,colspec):
+  if colspec in a.fieldnames:
+    index=a.fieldnames.index(colspec)
+  else:
+    index=int(colspec)
+  return index
 
 def selectres(loc_specs,a):
   res = []
   loc_mode = loc_specs[0]
   if loc_mode=="pos":
-    row=int(loc_specs[1])
+    row=get_col_index(a,loc_specs[1])
     if row<0:
       row = max(0,a.length+row)
     i=2
@@ -71,25 +77,21 @@
       while len(loc_specs[i:])>0:
         if loc_specs[i]=="all":
           res.extend(a[row])
-        elif loc_specs[i] in a.fieldnames:
-          index=a.fieldnames.index(loc_specs[i])
+        else:
+          index=get_col_index(a,loc_specs[i])
           res.append(a[row,index])
-        else:
-          res.append(a[row,int(loc_specs[i])])
         i+=1
   elif loc_mode=="mincol":
-    mcol = int(loc_specs[1])
+    mcol = get_col_index(a,loc_specs[1])
     mrow = argmin(a[:,mcol])
     print "found min row = ",mrow," for col ",mcol,", with value=",a[mrow,mcol]
     i=1
     while len(loc_specs[i:])>0:
       if loc_specs[i]=="all":
         res.extend(a[mrow])
-      elif loc_specs[i] in a.fieldnames:
-        index=a.fieldnames.index(loc_specs[i])
+      else:
+        index=get_col_index(a,loc_specs[i])
         res.append(a[mrow,index])
-      else:
-        res.append(a[mrow,int(loc_specs[i])])
       i+=1
   elif loc_mode=="cols":
     minrow = int(loc_specs[3])
@@ -101,7 +103,7 @@
     b=[]
     res = zeros([maxrow-minrow,0])
     while len(loc_specs[i:])>0:
-      col = int(loc_specs[i])
+      col = get_col_index(a,loc_specs[i])
       if col<0:
         b = array(range(minrow,maxrow))
       else:



From nouiz at mail.berlios.de  Tue Aug 28 20:49:00 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 28 Aug 2007 20:49:00 +0200
Subject: [Plearn-commits] r8032 - trunk/python_modules/plearn/parallel
Message-ID: <200708281849.l7SIn0oi000247@sheep.berlios.de>

Author: nouiz
Date: 2007-08-28 20:48:59 +0200 (Tue, 28 Aug 2007)
New Revision: 8032

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
Do not generate more thread then the number of job to run


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-08-28 18:42:25 UTC (rev 8031)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-08-28 18:48:59 UTC (rev 8032)
@@ -45,7 +45,10 @@
         self._threadPool   = []
         self.print_when_finish = print_when_finished
         self.running = 0
-        for i in range( maxThreads ):
+        nb_thread=maxThreads
+        if nb_thread>len(argsVector):
+            nb_thread=len(argsVector)
+        for i in range( nb_thread ):
             self._threadPool.append( Thread( target=self._tailRecurse ) )
             
     def _tailRecurse( self ):



From sakenasv at mail.berlios.de  Tue Aug 28 23:34:23 2007
From: sakenasv at mail.berlios.de (sakenasv at mail.berlios.de)
Date: Tue, 28 Aug 2007 23:34:23 +0200
Subject: [Plearn-commits] r8033 - trunk/plearn_learners/online
Message-ID: <200708282134.l7SLYNpJ012787@sheep.berlios.de>

Author: sakenasv
Date: 2007-08-28 23:34:20 +0200 (Tue, 28 Aug 2007)
New Revision: 8033

Modified:
   trunk/plearn_learners/online/RBMMixedLayer.cc
Log:
Fixed a serialization bug when bias of underlying layers is not loaded properly.

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-08-28 18:48:59 UTC (rev 8032)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-08-28 21:34:20 UTC (rev 8033)
@@ -592,7 +592,8 @@
         layer->setExpectationsByRef(expectations.subMatColumns(init_pos,
                                                               layer_size));
 
-        layer->bias = bias.subVec(init_pos, layer_size);
+        bias.subVec(init_pos, layer_size) << layer->bias;
+        layer->bias = bias.subVec(init_pos, layer_size);        
 
         // We changed fields of layer, so we need to rebuild it (especially
         // if it is another RBMMixedLayer)



From nouiz at mail.berlios.de  Wed Aug 29 17:35:18 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 29 Aug 2007 17:35:18 +0200
Subject: [Plearn-commits] r8034 -
	branches/cgi-desjardin/plearn_learners/second_iteration
Message-ID: <200708291535.l7TFZIb9014363@sheep.berlios.de>

Author: nouiz
Date: 2007-08-29 17:35:18 +0200 (Wed, 29 Aug 2007)
New Revision: 8034

Modified:
   branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
Log:
Use existing function


Modified: branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc
===================================================================
--- branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-08-28 21:34:20 UTC (rev 8033)
+++ branches/cgi-desjardin/plearn_learners/second_iteration/TestImputations.cc	2007-08-29 15:35:18 UTC (rev 8034)
@@ -529,7 +529,7 @@
         pb->update( test_row );
     }
     delete pb;
-    for (int knnf_row = 0; knnf_row < knnf_mean_err.size(); knnf_row++) knnf_mean_err[knnf_row] = knnf_mean_err[knnf_row] /  (real) test_length;
+    knnf_mean_err/=(real) test_length;
 }
 
 void TestImputations::createHeaderFile()



From sakenasv at mail.berlios.de  Wed Aug 29 20:23:10 2007
From: sakenasv at mail.berlios.de (sakenasv at mail.berlios.de)
Date: Wed, 29 Aug 2007 20:23:10 +0200
Subject: [Plearn-commits] r8035 - trunk/plearn/vmat
Message-ID: <200708291823.l7TINAl4006996@sheep.berlios.de>

Author: sakenasv
Date: 2007-08-29 20:23:02 +0200 (Wed, 29 Aug 2007)
New Revision: 8035

Added:
   trunk/plearn/vmat/TextStreamVMatrix.cc
   trunk/plearn/vmat/TextStreamVMatrix.h
Log:
New VMatrix for dealing with text data on char level.

Added: trunk/plearn/vmat/TextStreamVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextStreamVMatrix.cc	2007-08-29 15:35:18 UTC (rev 8034)
+++ trunk/plearn/vmat/TextStreamVMatrix.cc	2007-08-29 18:23:02 UTC (rev 8035)
@@ -0,0 +1,168 @@
+// -*- C++ -*-
+
+// TextStreamVMatrix.cc
+//
+// Copyright (C) 2007 Vytenis Sakenas
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Vytenis Sakenas
+
+/*! \file TextStreamVMatrix.cc */
+
+
+#include "TextStreamVMatrix.h"
+#include <fstream>
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    TextStreamVMatrix,
+    "Character based sliding window data matrix from given text file.",
+    "Given the text file, and symbols that must be parsed this creates a data"
+	"matrix that every row contains concecutive number_of_symbols chars "
+	"expressed in one-hot vector. For example:\n"
+	"text file contents: abaab\n"
+	"symbols:	ab\n"
+	"number_of_symbols: 3\n"
+	"result matrix will be:\n"
+	"1 0 0 1 1 0 <-- aba\n"
+	"0 1 1 0 1 0 <-- baa\n"
+	"1 0 1 0 0 1 <-- aab\n"
+	"Position of 1 in one-hot vector is defined by chars position in symbols string."
+    );
+
+TextStreamVMatrix::TextStreamVMatrix()
+/* ### Initialize all fields to their default value */
+{
+}
+
+void TextStreamVMatrix::getNewRow(int i, const Vec& v) const
+{
+    v.subVec(0, symbol_width*number_of_symbols) << data.subVec( i*symbol_width, symbol_width*number_of_symbols);
+    v[symbol_width*number_of_symbols] = target[i];
+}
+
+void TextStreamVMatrix::declareOptions(OptionList& ol)
+{
+     declareOption(ol, "data_file", &TextStreamVMatrix::data_file,
+                   OptionBase::buildoption,
+                   "File that contains data.");
+     declareOption(ol, "number_of_symbols", &TextStreamVMatrix::number_of_symbols,
+                   OptionBase::buildoption,
+                   "Number of symbols to represent in one row (window size).");
+     declareOption(ol, "symbols", &TextStreamVMatrix::symbols,
+                   OptionBase::buildoption,
+                   "Symbols that are represented. All symbols that are not in this string will be ignored. "
+			"Also the index of a symbol will show which bit will be set to one in the one-hot vector.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void TextStreamVMatrix::build_()
+{
+	int ids[256];
+
+	symbol_width = symbols.length();
+
+	// Calculate id table
+	memset(ids, -1, sizeof(ids));
+	for (int i = 0; i < symbol_width; ++i)
+		ids[(unsigned int)symbols[i]] = i;
+
+	int text_length = 0;
+	string text;
+
+	// Read text from file
+	// TODO: optimize
+	ifstream fin(data_file.c_str(), ios::in | ios::binary);
+	unsigned char chr;
+	
+	fin.unsetf(ios::skipws);
+
+	while (fin >> chr) {
+		if (ids[(unsigned int)chr] == -1) continue;
+		text += chr;
+		++text_length;
+	}
+
+	fin.close();
+
+	data.resize(symbol_width*text_length);
+	data.fill(0);
+
+	target.resize(text_length - number_of_symbols);
+
+	//cout << text_length << endl;
+	for (int i = 0; i < text_length; ++i) {
+		//cout << symbol_width*i << endl;
+ 		//cout << "id: " << ids[text[i]] << " " << i << endl;
+		data[ symbol_width*i + ids[(unsigned int)text[i]] ] = 1;
+	}
+
+	for (int i = 0; i < text_length-number_of_symbols; ++i)
+		target[i] = ids[(unsigned int)text[i + number_of_symbols]];
+
+	length_ = text_length - number_of_symbols;
+	width_ = symbol_width*number_of_symbols + 1;
+	inputsize_ = symbol_width*number_of_symbols;
+	targetsize_ = 1;
+	weightsize_ = 0;
+	extrasize_ = 0;
+}
+
+// ### Nothing to add here, simply calls build_
+void TextStreamVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+void TextStreamVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/TextStreamVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextStreamVMatrix.h	2007-08-29 15:35:18 UTC (rev 8034)
+++ trunk/plearn/vmat/TextStreamVMatrix.h	2007-08-29 18:23:02 UTC (rev 8035)
@@ -0,0 +1,137 @@
+// -*- C++ -*-
+
+// TextStreamVMatrix.h
+//
+// Copyright (C) 2007 Vytenis Sakenas
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Vytenis Sakenas
+
+/*! \file TextStreamVMatrix.h */
+
+
+#ifndef TextStreamVMatrix_INC
+#define TextStreamVMatrix_INC
+
+#include <plearn/vmat/RowBufferedVMatrix.h>
+
+namespace PLearn {
+
+class TextStreamVMatrix : public RowBufferedVMatrix
+{
+    typedef RowBufferedVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+
+string data_file;
+
+
+int number_of_symbols;
+
+
+string symbols;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    TextStreamVMatrix();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(TextStreamVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! 'v' is assumed to be the right size.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+int symbol_width;
+TVec <int> data;
+TVec <int> target;
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(TextStreamVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From sakenasv at mail.berlios.de  Wed Aug 29 20:53:17 2007
From: sakenasv at mail.berlios.de (sakenasv at mail.berlios.de)
Date: Wed, 29 Aug 2007 20:53:17 +0200
Subject: [Plearn-commits] r8036 - trunk/plearn_learners/online/EXPERIMENTAL
Message-ID: <200708291853.l7TIrHR4008861@sheep.berlios.de>

Author: sakenasv
Date: 2007-08-29 20:53:09 +0200 (Wed, 29 Aug 2007)
New Revision: 8036

Added:
   trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.cc
   trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.h
Log:
Hierarchical deep network.

Added: trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.cc
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.cc	2007-08-29 18:23:02 UTC (rev 8035)
+++ trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.cc	2007-08-29 18:53:09 UTC (rev 8036)
@@ -0,0 +1,1301 @@
+// TreeDBNModule.cc
+//
+// Copyright (C) 2007 Vytenis Sakenas
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Vytenis Sakenas
+
+/*! \file TreeDBNModule.cc */
+
+
+
+#include "TreeDBNModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+   TreeDBNModule,
+  "Hierarchical deep network.",
+   "Hierarchical deep network. In every level, a RBM takes input from n_parents_per_node lower"
+       " layer RBMs. All RBMs in a layer share weights. So, for example, a network with 3 layers and"
+       " n_parents_per_node=2 will have 1, 2 and 4 RBMs in top, middle and bottom layers respectively."
+       " Typical usage is providing RBM modules for every layer through modules option, possibly adding "
+       "additional ports we want to compute and setting flags like propagate_gradient, propagate_energy_gradient"
+       " and propagate_full_gradient to a desired state."
+       "Ports:\n"
+       "\tinput, output_1 ... output_n"
+       "where n is number of layers"
+);
+
+//////////////////
+// TreeDBNModule //
+//////////////////
+TreeDBNModule::TreeDBNModule() : n_parents_per_node(2), n_shared_parents(0), gradient_multiplier(1.0),
+                               propagate_gradient(false), propagate_energy_gradient(false), propagate_full_gradient(false)
+/* ### Initialize all fields to their default value here */
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void TreeDBNModule::declareOptions(OptionList& ol)
+{
+   // Now call the parent class' declareOptions
+   inherited::declareOptions(ol);
+
+       declareOption(ol, "modules", &TreeDBNModule::modules,
+                  OptionBase::buildoption,
+                  "RBMModule list that is used to build DBN.");
+
+       declareOption(ol, "n_parents_per_node", &TreeDBNModule::n_parents_per_node,
+                                 OptionBase::buildoption,
+                                 "How many parents each node has.");
+
+       // Not implemented.
+       //declareOption(ol, "n_shared_parents", &TreeDBNModule::n_shared_parents,
+       //                        OptionBase::buildoption,
+       //                        "Number of parents that two adjacent nodes share.");
+
+       declareOption(ol, "propagate_gradient", &TreeDBNModule::propagate_gradient,
+                                 OptionBase::buildoption,
+                                 "Whether we propagate gradient through hierarchy.");
+
+       declareOption(ol, "propagate_full_gradient", &TreeDBNModule::propagate_full_gradient,
+                                 OptionBase::buildoption,
+                                 "If propagate_gradient==true then this flag determines that gradient should be propagated"
+                                 " through full hierarchy. Else propagation is only done through the rightmost branch.");
+
+       declareOption(ol, "propagate_energy_gradient", &TreeDBNModule::propagate_energy_gradient,
+                                 OptionBase::buildoption,
+                                 "Whether we compute and propagate free energy gradient from top layer.");
+
+	// Probabaly not useful.
+       declareOption(ol, "gradient_multiplier", &TreeDBNModule::gradient_multiplier,
+                                 OptionBase::buildoption,
+                                 "Value that propagated gradient is multiplied before propagating from top layer.");
+
+       declareOption(ol, "ports", &TreeDBNModule::ports,
+                                 OptionBase::buildoption,
+                                 "A sequence of pairs of strings, where each pair is of the form\n"
+                                                 "\"P\":\"M.N\" with 'M' the name of an underlying module, 'N' one of\n"
+                                                 "its ports, and 'P' the name under which the TreeDBNModule sees this\n"
+                                                 "port. See the class help for an example. If 'P' is an empty string,\n"
+                                                 "then the port name will be 'M.N'.");
+
+}
+
+
+
+////////////////////
+// declareMethods //
+////////////////////
+void TreeDBNModule::declareMethods(RemoteMethodMap& rmm)
+{
+   // Insert a backpointer to remote methods; note that this
+   // different than for declareOptions()
+       rmm.inherited(inherited::_getRemoteMethodMap_());
+
+       declareMethod(
+                       rmm, "initSampling", &TreeDBNModule::initSampling,
+       (BodyDoc("Initializes network for sampling. This function must be called before any calls to sample().\n"),
+        ArgDoc ("gibbsTop", "Number of gibbs steps to do in top rbm.")));
+
+       declareMethod(
+                       rmm, "clearCache", &TreeDBNModule::clearCache,
+       (BodyDoc("Clears all caches. Call this after changing any of the module parameters.\n")));
+
+       declareMethod(
+                       rmm, "sample", &TreeDBNModule::sample,
+       (BodyDoc("Samples the network. Returns a sample on the visible layer.\n"),
+        ArgDoc("gibbsTop", "Number of gibbs steps in the top layer for each sample."),
+        RetDoc ("Sample.")));
+}
+
+//! Add a port to the module with given name, which is filled from a rbm modules[rbm_index]
+//! an port port_name and provided port width. If a port you add is not directly filled from
+//! a rbm then provide rbm_index=-1. If port_width is not provided then it is determined from
+//! the rbm it is filled from.
+void TreeDBNModule::appendPort(string name, int rbm_index, string port_name, int port_width = -1)
+{
+       port_names.append(name);
+       port_rbms.append(rbm_index);
+
+       if (rbm_index >= 0) {
+               int index = modules[rbm_index]->getPortIndex(port_name);
+               PLASSERT(index >= 0);
+               port_index.append( index );
+       }
+       else
+               port_index.append( -1 );
+
+       if (port_width == -1) {
+               // We need to extract actual port size
+               port_width = modules[rbm_index]->getPortWidth(port_name);
+       }
+
+       TVec <int> sz(2, -1);
+       sz[1] = port_width;
+       port_sizes.appendRow(sz);
+}
+
+////////////
+// build_ //
+////////////
+void TreeDBNModule::build_()
+{
+       n_layers = modules.length();
+       time = 0;
+
+       // Fill ports
+       port_names.clear();
+       port_rbms.clear();
+       port_index.clear();
+       port_sizes.clear();
+       appendPort("input", -1, "", modules[0]->visible_layer->size);
+
+       layer_sizes.resize(n_layers);
+
+       // Add output ports for every layer rbm
+       for (int i = 1; i <= n_layers; ++i) {
+               appendPort("output_" + tostring(i), i-1, "hidden.state");
+               layer_sizes[i-1] = 1<<(n_layers-i);
+       }
+
+       // Add ports that are forwarded from internal modules
+       for (int i = 0; i < ports.size(); ++i) {
+               string s = ports[i].second;
+
+               size_t dot = s.find('.');
+               PLASSERT( dot != string::npos );
+               string module_name = s.substr(0, dot);
+               string port_name = s.substr(dot + 1);
+
+               bool valid_redirect = false;
+               for (int j = 0; j < n_layers; ++j) {
+                       if (modules[j]->name == module_name) {
+                               appendPort(ports[i].first, j, port_name);
+                               valid_redirect = true;
+                       }
+               }
+
+               PLASSERT(valid_redirect);
+       }
+
+       // Make sure storage matrix vectors will not be resized and we will not loose pointers.
+       mats.resize(1000);
+       mats.resize(0);
+       cache_mats.resize(1000);
+       cache_mats.resize(0);
+
+       step_size.resize(n_layers);
+       step_size[0] = 2;
+       for (int i = 1; i < n_layers; ++i) {
+               step_size[i] = n_parents_per_node * step_size[i-1];
+       }
+
+       // Prepare arrays for holding fprop and bprop data
+       bprop_data.resize(n_layers);
+       fprop_data.resize(n_layers);
+       bprop_data_cache.resize(n_layers);                                      // do not cache (?)
+       fprop_data_cache.resize(n_layers);
+
+       for (int i = 0; i < n_layers; ++i) {
+               int np = modules[i]->nPorts();
+               bprop_data[i].resize(np);
+               fprop_data[i].resize(np);
+               bprop_data_cache[i].resize(np);
+               fprop_data_cache[i].resize(np);
+               bprop_data[i].fill((Mat*)NULL);
+               fprop_data[i].fill((Mat*)NULL);
+               bprop_data_cache[i].fill((Mat*)NULL);
+               fprop_data_cache[i].fill((Mat*)NULL);
+       }
+
+       // Here we will hold last full input to lower layer
+       // It is done to be able to check if input is a shifted
+       // version of previous input.
+       last_full_input.resize(0);
+
+       // Safety check
+       for (int i = 0; i < n_layers-1; ++i)
+               PLASSERT(modules[i]->hidden_layer->size * n_parents_per_node == modules[i+1]->visible_layer->size);
+
+       // Forward random number generator to all underlying modules.
+       if (random_gen) {
+               cout << "Forget in build" << endl;
+               for (int i = 0; i < modules.length(); i++) {
+                       if (!modules[i]->random_gen) {
+				cout << "pass forget" << endl;
+                               modules[i]->random_gen = random_gen;
+                               modules[i]->build();
+                               modules[i]->forget();
+                       }
+               }
+       }
+}
+
+///////////
+// build //
+///////////
+void TreeDBNModule::build()
+{
+   inherited::build();
+   build_();
+   Profiler::activate();
+}
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+void TreeDBNModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                                                  const TVec<Mat*>& ports_gradient)
+{
+       PLASSERT( ports_value.length() == nPorts() && ports_gradient.length() == nPorts());
+
+       Profiler::start("full bprop");
+       if (!propagate_gradient) {			// Only unsupervised learning in a module
+               for (int layer = n_layers-1; layer >= 0; layer--) {
+                       int n_mod_ports = modules[layer]->nPorts();
+
+                       bprop_data[layer].resize(n_mod_ports);
+                       bprop_data[layer].fill((Mat*)NULL);
+                       int mod_batch_size = fprop_data[layer][modules[layer]->getPortIndex("hidden.state")]->length();
+
+                       if (modules[layer]->reconstruction_connection != NULL) {
+                               bprop_data[layer][modules[layer]->getPortIndex("reconstruction_error.state")] = createMatrix(mod_batch_size, 1, mats);
+                               bprop_data[layer][modules[layer]->getPortIndex("reconstruction_error.state")]->fill(1);
+                       }
+
+                       Profiler::start("bprop");
+                       modules[layer]->bpropAccUpdate(fprop_data[layer], bprop_data[layer]);
+                       Profiler::end("bprop");
+               }
+       } else
+       {
+               if (!propagate_full_gradient)           // Propagate only rightmost branch
+               {
+                       // For top RBM we provide energy gradient only and get gradient on visible
+                       bprop_data[n_layers - 1].resize( modules[n_layers-1]->nPorts() );
+                       bprop_data[n_layers - 1].fill((Mat*)NULL);
+
+                       int mod_batch_size = fprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")]->length();
+
+                       if (propagate_energy_gradient) {
+                               bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("energy")] = createMatrix(mod_batch_size, 1, mats);
+                               bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("energy")]->fill(1);
+                       }
+
+                       if (modules[n_layers-1]->reconstruction_connection != NULL) {
+                               bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("reconstruction_error.state")] = 
+												createMatrix(mod_batch_size, 1, mats);
+                               bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("reconstruction_error.state")]->fill(1);
+                       }
+
+                       // Take external gradient on output
+                       int out_grad = getPortIndex("output_"+tostring(n_layers));
+
+                       if ( ports_gradient[out_grad] == NULL || ports_gradient[out_grad]->isEmpty() ) {
+                               // Make gradient zero
+                               ports_gradient[out_grad] = createMatrix(mod_batch_size, modules[n_layers-1]->hidden_layer->size, mats);
+                               ports_gradient[out_grad]->fill(0);
+                               PLWARNING("Top RBM output port has no gradient information. Using 0 gradient.");
+                       }
+                       //PLASSERT(ports_gradient[out_grad] != NULL);
+
+
+                       bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("hidden.state")] = 
+							createMatrix(mod_batch_size, ports_gradient[out_grad]->width(), mats);
+                       *bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("hidden.state")] << *ports_gradient[out_grad];
+
+                       // Ask for visible gradient
+                       bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")] = 
+							createMatrix(0, modules[n_layers-1]->visible_layer->size, mats);
+
+                       Profiler::start("bprop");
+                       modules[n_layers-1]->bpropAccUpdate(fprop_data[n_layers-1], bprop_data[n_layers-1]);
+                       Profiler::end("bprop");
+
+
+                       Mat *mat = bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")];
+                       for (int i = 0; i < mat->length(); ++i)
+                               for (int j = 0; j < mat->width(); ++j)
+                                       (*mat)[i][j] *= gradient_multiplier;
+
+
+                       // Now for every layer take upper layers visible gradient
+                       // and pass it to current layers hidden.state port.
+                       for (int layer = n_layers-1; layer > 0; layer--) {
+                               int n_mod_ports = modules[layer-1]->nPorts();
+
+                               bprop_data[layer-1].resize(n_mod_ports);
+                               bprop_data[layer-1].fill((Mat*)NULL);
+
+                               int mod_batch_size = fprop_data[layer-1][modules[layer-1]->getPortIndex("visible")]->length();
+                               int width = modules[layer-1]->hidden_layer->size;
+
+
+                               Mat *hidden_state = createMatrix(mod_batch_size, width, mats);
+                               Mat *rbm_visible = bprop_data[layer][modules[layer]->getPortIndex("visible")];
+
+                               int parent_width = modules[layer-1]->hidden_layer->size;
+                               int minibatch_size = ports_value[getPortIndex("input")]->length();
+
+                               TVec <int> used(mod_batch_size, 0);	// Ensure that we right gradient only once (the one we need is first one)
+
+                               // do the same thing like in fprop
+                               for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+                               {
+                                       if (mbi_time[mbi] < step_size[layer]) {
+                                               // Computed all rbms in upper layer
+                                               for (int i = 0; i < layer_sizes[layer]; ++i)
+                                               {
+                                                       // Here parents are this layer rbm (where we want to write gradient)
+                                                       for (int parent = 0; parent < n_parents_per_node; ++parent) {
+                                                               int row_id = mod_batch_length[layer-1][mbi] - 
+											hash(mbi_time[mbi], layer-1, 2*i + parent);
+                                                               if (row_id < 0) {
+                                                                       // It must be in cache - do nothing
+                                                               } else {
+                                                                       if (!used[row_id])
+                                                                               (*hidden_state)(row_id) <<
+										(*rbm_visible)(index).subVec(parent*parent_width, parent_width);
+                                                                       used[row_id]++;
+                                                               }
+                                                       }
+                                                       ++index;
+                                               }
+                                       } else {
+                                               // Compute only last rbm
+                                               for (int parent = 0; parent < n_parents_per_node; ++parent) {
+                                                       int row_id = mod_batch_length[layer-1][mbi] - 
+										hash(mbi_time[mbi], layer-1, 2*(layer_sizes[layer]-1) + parent);
+                                                       if (row_id < 0) {
+                                                               // It must be in cache - do nothing
+                                                       } else {
+                                                               if (!used[row_id])
+                                                                       (*hidden_state)(row_id) << 
+										(*rbm_visible)(index).subVec(parent*parent_width, parent_width);
+                                                               used[row_id]++;
+                                                       }
+                                               }
+                                               ++index;
+                                       }
+                               }
+
+                               // Provide hidden gradient..
+                               bprop_data[layer-1][modules[layer-1]->getPortIndex("hidden.state")] = hidden_state;
+
+                               // add a gradient that is provided externally on output_i port
+                               Mat *xgrad = ports_gradient[getPortIndex("output_"+tostring(layer))];
+                               if (xgrad != NULL && !xgrad->isEmpty()) {
+                                       //cout << "grad_flow: " << layer << " " << (*xgrad)(0)[0] << endl;
+                                       // Length of xgrad is <= hidden_state so we need to sum row by row
+                                       for (int mbi = 0; mbi < minibatch_size; ++mbi) {
+                                               (*hidden_state)(mod_batch_length[layer-1][mbi]-1) += (*xgrad)(mbi);
+                                       }
+                               }
+
+                               // and ask for visible gradient
+                               bprop_data[layer-1][modules[layer-1]->getPortIndex("visible")] = 
+							createMatrix(0, modules[layer-1]->visible_layer->size, mats);
+
+                               if (modules[layer-1]->reconstruction_connection != NULL) {
+                                       bprop_data[layer-1][modules[layer-1]->getPortIndex("reconstruction_error.state")] =
+												createMatrix(mod_batch_size, 1, mats);
+                                       bprop_data[layer-1][modules[layer-1]->getPortIndex("reconstruction_error.state")]->fill(1);
+                               }
+
+                               Profiler::start("bprop");
+                               modules[layer-1]->bpropAccUpdate(fprop_data[layer-1], bprop_data[layer-1]);
+                               Profiler::end("bprop");
+                       }  // for every layer
+               } else                          // Propagate through all hierarchy
+               {
+			bprop_data[n_layers - 1].resize( modules[n_layers-1]->nPorts() );
+			bprop_data[n_layers - 1].fill((Mat*)NULL);
+		
+			int mod_batch_size = fprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")]->length();
+		
+			if (propagate_energy_gradient) {
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("energy")] = createMatrix(mod_batch_size, 1, mats);
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("energy")]->fill(1);
+			}
+		
+			if (modules[n_layers-1]->reconstruction_connection != NULL) {
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("reconstruction_error.state")] =
+												createMatrix(mod_batch_size, 1, mats);
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("reconstruction_error.state")]->fill(1);
+			}
+		
+			// Take external gradient on output
+			int out_grad = getPortIndex("output_"+tostring(n_layers));
+		
+			if ( ports_gradient[out_grad] == NULL || ports_gradient[out_grad]->isEmpty() ) {
+				// Make gradient zero
+				ports_gradient[out_grad] = createMatrix(mod_batch_size, modules[n_layers-1]->hidden_layer->size, mats);
+				ports_gradient[out_grad]->fill(0);
+				PLWARNING("Top RBM output port has no gradient information. Using 0 gradient.");
+			}
+			//PLASSERT(ports_gradient[out_grad] != NULL);
+		
+		
+			bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("hidden.state")] = 
+								createMatrix(mod_batch_size, ports_gradient[out_grad]->width(), mats);
+			*bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("hidden.state")] << *ports_gradient[out_grad];
+		
+			// Ask for visible gradient
+			bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")] = 
+								createMatrix(0, modules[n_layers-1]->visible_layer->size, mats);
+		
+			Profiler::start("bprop");
+			modules[n_layers-1]->bpropAccUpdate(fprop_data[n_layers-1], bprop_data[n_layers-1]);
+			Profiler::end("bprop");
+		
+		
+			Mat *mat = bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")];
+			for (int i = 0; i < mat->length(); ++i)
+				for (int j = 0; j < mat->width(); ++j)
+					(*mat)[i][j] *= gradient_multiplier;
+		
+			int minibatch_size = ports_value[getPortIndex("input")]->length();
+		
+			// Now for every layer take upper layers visible gradient
+			// and pass it to current layers hidden.state port.
+			for (int layer = n_layers-1; layer > 0; layer--) {
+				int n_mod_ports = modules[layer-1]->nPorts();
+		
+				bprop_data[layer-1].resize(n_mod_ports);
+				bprop_data[layer-1].fill((Mat*)NULL);
+		
+				int mod_batch_size = minibatch_size*layer_sizes[layer-1];
+				int width = modules[layer-1]->hidden_layer->size;
+		
+				Mat *hidden_state = createMatrix(mod_batch_size, width, mats);
+				Mat *rbm_visible = bprop_data[layer][modules[layer]->getPortIndex("visible")];
+		
+				int parent_width = modules[layer-1]->hidden_layer->size;
+		
+				for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+				{
+					for (int i = 0; i < layer_sizes[layer-1]; ++i)
+					{
+						// Write gradient from parent
+						int parent_ix = mbi*layer_sizes[layer] + i/n_parents_per_node;
+						int child_ix = i%n_parents_per_node;
+						(*hidden_state)(index++) << (*rbm_visible)(parent_ix).subVec(child_ix*parent_width, parent_width);
+					}
+				}
+		
+				// Provide hidden gradient..
+				bprop_data[layer-1][modules[layer-1]->getPortIndex("hidden.state")] = hidden_state;
+		
+				// add a gradient that is provided externally on output_i port
+				Mat *xgrad = ports_gradient[getPortIndex("output_"+tostring(layer))];
+				if (xgrad != NULL && !xgrad->isEmpty()) {
+					//cout << "grad_flow: " << layer << " " << (*xgrad)(0)[0] << endl;
+					// Length of xgrad is <= hidden_state so we need to sum row by row
+					for (int mbi = 0; mbi < minibatch_size; ++mbi) {
+						(*hidden_state)(mbi*layer_sizes[layer-1]+layer_sizes[layer-1]-1) += (*xgrad)(mbi);
+					}
+				}
+		
+				// and ask for visible gradient
+				bprop_data[layer-1][modules[layer-1]->getPortIndex("visible")] = 
+									createMatrix(0, modules[layer-1]->visible_layer->size, mats);
+		
+				if (modules[layer-1]->reconstruction_connection != NULL) {
+					bprop_data[layer-1][modules[layer-1]->getPortIndex("reconstruction_error.state")] = 
+												createMatrix(mod_batch_size, 1, mats);
+					bprop_data[layer-1][modules[layer-1]->getPortIndex("reconstruction_error.state")]->fill(1);
+				}
+
+				/*for (int i = 0; i < n_mod_ports; ++i) {
+					cout << i << " " << modules[layer-1]->getPorts()[i] << " ";
+					if (full_fprop_data[i])
+						cout << full_fprop_data[i]->length() << endl;
+					else
+						cout << "NULL" << endl;
+				}*/
+		
+				Profiler::start("bprop");
+				modules[layer-1]->bpropAccUpdate(fprop_data[layer-1], bprop_data[layer-1]);
+				Profiler::end("bprop");
+			}	// for every layer
+			//updateCache();		// no cache update as we dont have any
+		}
+
+
+		// Following code would work without need of doing full_fprop. However because RBMMixedLayer caches nll
+		// during fprop and then reuses it in bprop it is not possible.
+		/*{
+			// For top RBM we provide energy gradient only and get gradient on visible
+			bprop_data[n_layers - 1].resize( modules[n_layers-1]->nPorts() );
+			bprop_data[n_layers - 1].fill((Mat*)NULL);
+		
+			int mod_batch_size = fprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")]->length();
+		
+			if (propagate_energy_gradient) {
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("energy")] = createMatrix(mod_batch_size, 1, mats);
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("energy")]->fill(1);
+			}
+		
+			if (modules[n_layers-1]->reconstruction_connection != NULL) {
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("reconstruction_error.state")] = createMatrix(mod_batch_size, 1, mats);
+				bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("reconstruction_error.state")]->fill(1);
+			}
+		
+			// Take external gradient on output
+			int out_grad = getPortIndex("output_"+tostring(n_layers));
+		
+			if ( ports_gradient[out_grad] == NULL || ports_gradient[out_grad]->isEmpty() ) {
+				// Make gradient zero
+				ports_gradient[out_grad] = createMatrix(mod_batch_size, modules[n_layers-1]->hidden_layer->size, mats);
+				ports_gradient[out_grad]->fill(0);
+				PLWARNING("Top RBM output port has no gradient information. Using 0 gradient.");
+			}
+			//PLASSERT(ports_gradient[out_grad] != NULL);
+		
+		
+			bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("hidden.state")] = createMatrix(mod_batch_size, ports_gradient[out_grad]->width(), mats);
+			*bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("hidden.state")] << *ports_gradient[out_grad];
+		
+			// Ask for visible gradient
+			bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")] = createMatrix(0, modules[n_layers-1]->visible_layer->size, mats);
+		
+			Profiler::start("bprop");
+			modules[n_layers-1]->bpropAccUpdate(fprop_data[n_layers-1], bprop_data[n_layers-1]);
+			Profiler::end("bprop");
+		
+		
+			Mat *mat = bprop_data[n_layers-1][modules[n_layers-1]->getPortIndex("visible")];
+			for (int i = 0; i < mat->length(); ++i)
+				for (int j = 0; j < mat->width(); ++j)
+					(*mat)[i][j] *= gradient_multiplier;
+		
+			int minibatch_size = ports_value[getPortIndex("input")]->length();
+		
+			// Now for every layer take upper layers visible gradient
+			// and pass it to current layers hidden.state port.
+			for (int layer = n_layers-1; layer > 0; layer--) {
+				int n_mod_ports = modules[layer-1]->nPorts();
+		
+				bprop_data[layer-1].resize(n_mod_ports);
+				bprop_data[layer-1].fill((Mat*)NULL);
+		
+				int mod_batch_size = minibatch_size*layer_sizes[layer-1];
+				int width = modules[layer-1]->hidden_layer->size;
+		
+				// We need to make new fprop_data vector with full(expanded) data.
+				TVec <Mat*> full_fprop_data(n_mod_ports, (Mat*)NULL);
+				for (int i = 0; i < n_mod_ports; ++i) {
+					if (fprop_data[layer-1][i] != NULL && !fprop_data[layer-1][i]->isEmpty()
+						// HACK to make it work with a hack in RBMModule when visible_activations.state is not computed
+						&& (fprop_data[layer-1][i]->length() > 1 || fprop_data[layer-1][i]->width() > 1) ) {
+						full_fprop_data[i] = createMatrix(mod_batch_size, fprop_data[layer-1][i]->width(), mats);
+					}
+				}
+		
+				Mat *hidden_state = createMatrix(mod_batch_size, width, mats);
+				Mat *rbm_visible = bprop_data[layer][modules[layer]->getPortIndex("visible")];
+		
+				int parent_width = modules[layer-1]->hidden_layer->size;
+		
+				for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+				{
+					for (int i = 0; i < layer_sizes[layer-1]; ++i)
+					{
+						// Fill full_fprop_data properly
+						int row_id = mod_batch_length[layer-1][mbi] - hash(mbi_time[mbi], layer-1, i);
+						for (int j = 0; j < n_mod_ports; ++j) {
+							if (full_fprop_data[j] != NULL) {
+								if (row_id < 0) {
+									// Fill from cache
+									PLASSERT_MSG(fprop_data_cache[layer-1][j], "Cache is NULL");
+									int row_in_cache = fprop_data_cache[layer-1][j]->length()+row_id;
+									PLASSERT_MSG(row_in_cache >= 0, "Cache is provided but is too small");
+									(*full_fprop_data[j])(index) << (*fprop_data_cache[layer-1][j])(row_in_cache);
+								} else {
+									(*full_fprop_data[j])(index) << (*fprop_data[layer-1][j])(row_id);
+								}
+							}
+						}
+		
+						// Write gradient from parent
+						int parent_ix = mbi*layer_sizes[layer] + i/n_parents_per_node;
+						int child_ix = i%n_parents_per_node;
+						(*hidden_state)(index++) << (*rbm_visible)(parent_ix).subVec(child_ix*parent_width, parent_width);
+					}
+				}
+		
+		
+				// Provide hidden gradient..
+				bprop_data[layer-1][modules[layer-1]->getPortIndex("hidden.state")] = hidden_state;
+		
+				// add a gradient that is provided externally on output_i port
+				Mat *xgrad = ports_gradient[getPortIndex("output_"+tostring(layer))];
+				if (xgrad != NULL && !xgrad->isEmpty()) {
+					//cout << "grad_flow: " << layer << " " << (*xgrad)(0)[0] << endl;
+					// Length of xgrad is <= hidden_state so we need to sum row by row
+					for (int mbi = 0; mbi < minibatch_size; ++mbi) {
+						(*hidden_state)(mbi*layer_sizes[layer-1]+layer_sizes[layer-1]-1) += (*xgrad)(mbi);
+					}
+				}
+		
+				// and ask for visible gradient
+				bprop_data[layer-1][modules[layer-1]->getPortIndex("visible")] = createMatrix(0, modules[layer-1]->visible_layer->size, mats);
+		
+				if (modules[layer-1]->reconstruction_connection != NULL) {
+					bprop_data[layer-1][modules[layer-1]->getPortIndex("reconstruction_error.state")] = createMatrix(mod_batch_size, 1, mats);
+					bprop_data[layer-1][modules[layer-1]->getPortIndex("reconstruction_error.state")]->fill(1);
+				}
+
+				for (int i = 0; i < n_mod_ports; ++i) {
+					cout << i << " " << modules[layer-1]->getPorts()[i] << " ";
+					if (full_fprop_data[i])
+						cout << full_fprop_data[i]->length() << endl;
+					else
+						cout << "NULL" << endl;
+				}
+		
+				Profiler::start("bprop");
+				modules[layer-1]->bpropAccUpdate(full_fprop_data, bprop_data[layer-1]);
+				Profiler::end("bprop");
+			}	// for every layer
+			updateCache();
+		}*/
+
+
+       }
+
+       //cout << "end back" << endl;
+   // Ensure all required gradients have been computed.
+       checkProp(ports_gradient);
+
+       Profiler::end("full bprop");
+}
+
+
+
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+// the default implementation returns false
+bool TreeDBNModule::bpropDoesNothing()
+{
+}
+*/
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void TreeDBNModule::finalize()
+{
+}
+*/
+
+////////////
+// forget //
+////////////
+void TreeDBNModule::forget()
+{
+       cout << "Forget" << endl;
+       for (int i  = 0; i < n_layers; ++i)
+               modules[i]->forget();
+}
+
+//! Check if b equals a shifted left by k.
+bool TreeDBNModule::check_shift(Vec &a, Vec& b, int k)
+{
+       PLASSERT(a.length() == b.length());
+
+       for (int i = k; i < a.length(); ++i) {
+               if ( !fast_is_equal(a[i], b[i-k]) )
+                       return false;
+       }
+
+       return true;
+}
+
+
+//! Provided pseudotime, rbm layer and rbm index (both zero based) in the layer returns 
+//! distance from the end of computed fprop_data where rbm with same 
+//! parameters was computed. For example, if provided parameters 6, 1, 1 
+//! it returns -3, then it means that second rbm in the second layer
+//! was computed and is stored in fprop_data[fprop_data.length-3]
+// OK
+int TreeDBNModule::hash(int t, int k, int i)
+{
+       if (t < step_size[k]) return layer_sizes[k] - i;            // all rbms were computed
+       if (i == layer_sizes[k] - 1) return 1;                                          // last rbm in layer asked, and was computed
+
+  // check if there was a moment when this input was fed to the last rbm in the layer
+       if ( (layer_sizes[k] - 1 - i)*step_size[k] <= t) {
+               int t_diff = (layer_sizes[k] - 1 - i)*step_size[k];
+      // In first step_size[k] time steps we added layer_size[k] entries.
+               return t_diff + max(0, step_size[k] - (t - t_diff) - 1)*(layer_sizes[k]-1) + 1;
+       }
+
+  // the only option is that this input was fed to some intermediate rbm
+       int ix = i + t/step_size[k];                    // Index of that rbm
+       int t_diff = (ix - i)*step_size[k];             //
+       return t_diff + max(0, step_size[k] - (t - t_diff) - 1)*(layer_sizes[k]-1) + layer_sizes[k] - 1 - ix + 1;
+}
+
+// helper function that creates matrix of given size in
+// mats vector and returns pointer to it.
+Mat* TreeDBNModule::createMatrix(int length, int width, TVec <Mat> &mats)
+{
+       mats.append(Mat(length, width));
+       return &mats.lastElement();
+}
+
+
+//! Unoptimized version of fprop
+void TreeDBNModule::full_fprop(const TVec<Mat*>& ports_value)
+{
+       Profiler::start("full fprop");
+       mats.resize(0);
+
+       vector <string> prts = modules[0]->getPorts();
+
+	Mat* input = ports_value[getPortIndex("input")];
+	int minibatch_size = input->length();
+
+	mbi_time.resize(minibatch_size);
+	mod_batch_length.resize(n_layers, minibatch_size);
+
+	// Process layerwise
+	for (int layer = 0; layer < n_layers; ++layer)
+	{
+		fprop_data[layer].resize(modules[layer]->nPorts());
+		fprop_data[layer].fill((Mat*)NULL);
+
+		// Count number of rows
+		int nRows = layer_sizes[layer]*minibatch_size;
+
+		// Prepare matrices
+		Mat* rbm_visible = createMatrix(nRows, modules[layer]->visible_layer->size, mats);
+		fprop_data[layer][modules[layer]->getPortIndex("visible")] = rbm_visible;
+
+		//Create all .state matrices
+		for (int i = 0; i < modules[layer]->nPorts(); ++i) {
+			string pname = modules[layer]->getPorts()[i];
+			if ( pname.length() > 6 && ".state" == pname.substr(pname.length()-6) ) {
+				if (fprop_data[layer][i] == NULL)
+					fprop_data[layer][i] = createMatrix(0, 0, mats);
+			}
+		}
+
+		if (modules[layer]->reconstruction_connection == NULL) {
+			fprop_data[layer][modules[layer]->getPortIndex("reconstruction_error.state")] = NULL;
+			fprop_data[layer][modules[layer]->getPortIndex("visible_reconstruction.state")] = NULL;
+			fprop_data[layer][modules[layer]->getPortIndex("visible_reconstruction_activations.state")] = NULL;
+		}
+
+		// Create empty matrices for forwarded ports
+		for (int i = 0; i < nPorts(); ++i) {
+			if (port_rbms[i] >= 0) {
+				if (ports_value[i] != NULL && fprop_data[port_rbms[i]][port_index[i]] == NULL)
+					fprop_data[port_rbms[i]][port_index[i]] = createMatrix(0, 0, mats);
+			}
+		}
+
+		// Go through all minibatch and fill visible expectations
+		if (layer == 0)
+		{       // Handle input layer in different manner
+			int visible_size = modules[layer]->visible_layer->size;
+
+			for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+			{
+				for (int i = 0; i < layer_sizes[layer]; ++i)
+				{
+					(*rbm_visible)(index++) << (*input)(mbi).subVec(i*visible_size, visible_size);
+				}
+			}
+		}
+		else
+		{
+			// Take parent layer expectations
+			Mat *expectations = fprop_data[layer-1][modules[layer-1]->getPortIndex("hidden.state")];
+
+			int parent_width = modules[layer-1]->hidden_layer->size;
+			for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+			{
+				// Compute all rbms
+				for (int i = 0; i < layer_sizes[layer]; ++i)
+				{
+					for (int parent = 0; parent < n_parents_per_node; ++parent) {
+						int row_id = mbi*layer_sizes[layer-1] + i*n_parents_per_node + parent;
+						(*rbm_visible)(index).subVec(parent*parent_width, parent_width) <<
+									(*expectations)(row_id);
+					}
+					++index;
+				}
+			}
+		}
+
+		Profiler::start("fprop");
+		//cout << "fprop: " << endl;
+		//cout << (*fprop_data[layer][0]) << endl;
+		//cout << "************" << endl;
+		modules[layer]->fprop(fprop_data[layer]);
+		Profiler::end("fprop");
+	}
+
+	time = 0;
+	last_full_input.resize(input->width());
+	last_full_input << (*input)(minibatch_size-1);
+
+	// and write all required output to the provided ports ( output_i + requested )
+	//cout << "write" << endl;
+	for (int i = 0; i < nPorts(); ++i) {
+		Mat *mat = ports_value[i];
+
+		if ( mat != NULL && mat->isEmpty() ) {
+			// We check of which layer output should be writen to the port
+			int pl = port_rbms[i];
+			if (pl >= 0) {
+				mat->resize(minibatch_size, fprop_data[pl][port_index[i]]->width());
+				//cout << modules[pl]->getPorts()[i] << endl;
+				for (int j = 0; j < minibatch_size; ++j)
+					(*mat)(j) << (*fprop_data[pl][port_index[i]])(layer_sizes[pl]*j + layer_sizes[pl]-1);
+			} else
+				PLERROR("Data was requested for a port, but not computed!");
+		}
+	}
+
+       //cout << "redirected " << *ports_value[port_redirects[0][0].first] << endl;
+	//cout << "ffprop end" << endl;
+       Profiler::end("full fprop");
+
+       //Profiler::report(cout);
+}
+
+
+//! Optimized fprop
+void TreeDBNModule::fprop(const TVec<Mat*>& ports_value)
+{
+	if (propagate_gradient && propagate_full_gradient) {
+		full_fprop(ports_value);
+		return;
+	}
+
+       Profiler::start("full fprop");
+       mats.resize(0);
+
+       vector <string> prts = modules[0]->getPorts();
+       //cout << "*********************" << endl;
+       //for (int i = 0; i < prts.size(); ++i)
+       //      cout << prts[i] << endl;
+       //cout << "*********************" << endl;
+
+	Mat* input = ports_value[getPortIndex("input")];
+	int minibatch_size = input->length();
+	int symbol_size = modules[0]->visible_layer->size/n_parents_per_node;
+
+	mbi_time.resize(minibatch_size);
+	mod_batch_length.resize(n_layers, minibatch_size);
+
+	// Compute pseudo-time
+	Vec v = (*input)(0), v2;
+	if ( last_full_input != NULL && !last_full_input.isEmpty() && check_shift( last_full_input, v, symbol_size ) )
+		mbi_time[0] = time + 1;
+	else
+		mbi_time[0] = 0;
+
+	for (int mbi = 1; mbi < minibatch_size; ++mbi)
+	{
+		// Two cases: either it is a shifted version of the previous
+		// or it is a new word
+		v = (*input)(mbi-1);    v2 = (*input)(mbi);
+		if ( check_shift( v, v2, symbol_size ) )
+			mbi_time[mbi] = mbi_time[mbi-1] + 1;
+		else
+			mbi_time[mbi] = 0;
+	}
+
+	// Process layerwise
+	for (int layer = 0; layer < n_layers; ++layer)
+	{
+		fprop_data[layer].resize(modules[layer]->nPorts());
+		fprop_data[layer].fill((Mat*)NULL);
+
+		// Count number of rows
+		int nRows = 0;
+		for (int mbi = 0; mbi < minibatch_size; ++mbi)
+		{
+			// We might need to compute either all or only last rbm
+			if (mbi_time[mbi] < step_size[layer]) nRows += layer_sizes[layer];
+			else ++nRows;
+		}
+
+		// Prepare matrices
+		Mat* rbm_visible = createMatrix(nRows, modules[layer]->visible_layer->size, mats);
+		fprop_data[layer][modules[layer]->getPortIndex("visible")] = rbm_visible;
+
+		//Create all .state matrices
+		for (int i = 0; i < modules[layer]->nPorts(); ++i) {
+			string pname = modules[layer]->getPorts()[i];
+			if ( pname.length() > 6 && ".state" == pname.substr(pname.length()-6) ) {
+				if (fprop_data[layer][i] == NULL)
+					fprop_data[layer][i] = createMatrix(0, 0, mats);
+			}
+		}
+
+		//fprop_data[layer][modules[layer]->getPortIndex("hidden.state")] = createMatrix(0, 0, mats);
+		//fprop_data[layer][modules[layer]->getPortIndex("hidden_activations.state")] = createMatrix(0, 0, mats);
+
+		if (modules[layer]->reconstruction_connection == NULL) {
+			fprop_data[layer][modules[layer]->getPortIndex("reconstruction_error.state")] = NULL;
+			fprop_data[layer][modules[layer]->getPortIndex("visible_reconstruction.state")] = NULL;
+			fprop_data[layer][modules[layer]->getPortIndex("visible_reconstruction_activations.state")] = NULL;
+		}
+
+		// Create empty matrices for forwarded ports
+		for (int i = 0; i < nPorts(); ++i) {
+			if (port_rbms[i] >= 0) {
+				if (ports_value[i] != NULL && fprop_data[port_rbms[i]][port_index[i]] == NULL)
+					fprop_data[port_rbms[i]][port_index[i]] = createMatrix(0, 0, mats);
+			}
+		}
+
+		// Go through all minibatch and fill visible expectations
+		if (layer == 0)
+		{                               // Handle input layer in different manner
+			int visible_size = modules[layer]->visible_layer->size;
+
+			for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+			{
+				// We might need to compute either all or only last rbm
+				if (mbi_time[mbi] < step_size[layer]) {
+					// Compute all rbms
+					for (int i = 0; i < layer_sizes[layer]; ++i)
+					{
+						(*rbm_visible)(index++) << (*input)(mbi).subVec(i*visible_size, visible_size);
+					}
+				} else {
+					// Compute only last rbm
+					(*rbm_visible)(index++) << (*input)(mbi).subVec((layer_sizes[layer]-1)*visible_size, visible_size);
+				}
+				mod_batch_length[0][mbi] = index;
+			}
+		}
+		else
+		{
+			// Take parent layer expectations
+			Mat *expectations = fprop_data[layer-1][modules[layer-1]->getPortIndex("hidden.state")];
+			Mat *expectations_cache = fprop_data_cache[layer-1][modules[layer-1]->getPortIndex("hidden.state")];
+
+			int parent_width = modules[layer-1]->hidden_layer->size;
+			for (int mbi = 0, index = 0; mbi < minibatch_size; ++mbi)
+			{
+				// We might need to compute either all or only last rbm
+				if (mbi_time[mbi] < step_size[layer]) {
+					// Compute all rbms
+					for (int i = 0; i < layer_sizes[layer]; ++i)
+					{
+						for (int parent = 0; parent < n_parents_per_node; ++parent) {
+							int row_id = mod_batch_length[layer-1][mbi] - hash(mbi_time[mbi], layer-1, n_parents_per_node*i + parent);
+							//cout << "RID*: " << row_id << endl;
+							if (row_id < 0) {
+								// It must be in cache
+								PLASSERT_MSG(expectations_cache, "Cache is NULL");
+								int row_in_cache = expectations_cache->length()+row_id;
+								PLASSERT_MSG(row_in_cache >= 0, "Cache is provided but is too small");
+								(*rbm_visible)(index).subVec(parent*parent_width, parent_width) <<
+										(*expectations_cache)(row_in_cache);
+							} else {
+								(*rbm_visible)(index).subVec(parent*parent_width, parent_width) <<
+										(*expectations)(row_id);
+							}
+						}
+						++index;
+					}
+				} else {
+					// Compute only last rbm
+					for (int parent = 0; parent < n_parents_per_node; ++parent) {
+						int row_id = mod_batch_length[layer-1][mbi] - hash(mbi_time[mbi], layer-1, n_parents_per_node*(layer_sizes[layer]-1) + parent);
+						//cout << "RID: " << row_id << endl;
+						//cout << mbi_time[mbi] << " " << mod_batch_length[mbi] << " " << hash(mbi_time[mbi], layer-1, 2*(layer_sizes[layer]-1) + parent) << " "<< row_id << endl;
+						if (row_id < 0) {
+							// It must be in cache
+							PLASSERT_MSG(expectations_cache, "Cache is NULL");
+							int row_in_cache = expectations_cache->length()+row_id;
+							PLASSERT_MSG(row_in_cache >= 0, "Cache is provided but is too small");
+							(*rbm_visible)(index).subVec(parent*parent_width, parent_width) <<
+									(*expectations_cache)(row_in_cache);
+						} else {
+							(*rbm_visible)(index).subVec(parent*parent_width, parent_width) <<
+									(*expectations)(row_id);
+						}
+					}
+					++index;
+				}
+				mod_batch_length[layer][mbi] = index;
+			}
+		}
+
+		Profiler::start("fprop");
+		//cout << "fprop: " << endl;
+		//cout << (*fprop_data[layer][0]) << endl;
+		//cout << "************" << endl;
+		modules[layer]->fprop(fprop_data[layer]);
+		Profiler::end("fprop");
+	}
+
+	time = mbi_time[minibatch_size-1];
+	last_full_input.resize(input->width());
+	last_full_input << (*input)(minibatch_size-1);
+
+	// Final things: fill the cache...
+	if (!propagate_gradient || !propagate_full_gradient)
+		updateCache();
+
+	// and write all required output to the provided ports ( output_i + requested )
+	for (int i = 0; i < nPorts(); ++i) {
+		Mat *mat = ports_value[i];
+
+		if ( mat != NULL && mat->isEmpty() ) {
+			// We check of which layer output should be writen to the port
+			int pl = port_rbms[i];
+			if (pl >= 0) {
+				mat->resize(minibatch_size, fprop_data[pl][port_index[i]]->width());
+				for (int j = 0; j < minibatch_size; ++j)
+					(*mat)(j) << (*fprop_data[pl][port_index[i]])(mod_batch_length[pl][j] - 1);
+			} else
+				PLERROR("Data was requested for a port, but not computed!");
+		}
+	}
+
+       //cout << "redirected " << *ports_value[port_redirects[0][0].first] << endl;
+
+       Profiler::end("full fprop");
+
+       //Profiler::report(cout);
+}
+
+//! Updates a cache with new fprop_data
+void TreeDBNModule::updateCache()
+{
+       //cache_mats.resize(0);
+       for (int i = 0; i < n_layers; ++i) {
+               int n_ports = modules[i]->nPorts();
+               for (int j = 0; j < n_ports; ++j) {
+
+                       if (fprop_data[i][j] != NULL && !fprop_data[i][j]->isEmpty()) {
+                               // Take last rows
+                               int max_rows = layer_sizes[0]*n_parents_per_node;               // max we could need
+                               if (fprop_data[i][j]->length() > max_rows) {
+                                       //cout << "full cache" << endl;
+                                       // copy submatrix
+                                       if (fprop_data_cache[i][j] == NULL)
+                                               fprop_data_cache[i][j] = createMatrix(max_rows, fprop_data[i][j]->width(), cache_mats);
+                                       else
+                                               fprop_data_cache[i][j]->resize(max_rows, fprop_data[i][j]->width());
+                                       *fprop_data_cache[i][j] << fprop_data[i][j]->subMatRows(fprop_data[i][j]->length()-max_rows, max_rows);
+                               } else {
+                                       if (fprop_data_cache[i][j] == NULL) {           // have no cache, copy all
+                                               //cout << "first cache " << i << " " << j << endl;
+                                               fprop_data_cache[i][j] = createMatrix(fprop_data[i][j]->length(), fprop_data[i][j]->width(), cache_mats);
+                                               *fprop_data_cache[i][j] << *fprop_data[i][j];
+                                       } else {
+                                               //cout << "part cache" << endl;
+                                               // had something.., check how many rows we have to leave
+                                               int rows_reuse = min(max_rows - fprop_data[i][j]->length(), fprop_data_cache[i][j]->length());
+                                               Mat tmp(rows_reuse, fprop_data[i][j]->width());
+                                               tmp << fprop_data_cache[i][j]->subMatRows(fprop_data_cache[i][j]->length() - rows_reuse, rows_reuse);
+                                               fprop_data_cache[i][j]->resize(rows_reuse + fprop_data[i][j]->length(), fprop_data[i][j]->width());
+                                               fprop_data_cache[i][j]->subMatRows(0, rows_reuse) << tmp;
+                                               fprop_data_cache[i][j]->subMatRows(rows_reuse, fprop_data[i][j]->length()) << *fprop_data[i][j];
+                                       }
+                               }
+                       }
+
+                       // TODO if we stop calculate fprop_data for some port the cache should be deleted (?)
+               }
+       }
+}
+
+//! Clears the cache. Do this if parameters changed.
+void TreeDBNModule::clearCache()
+{
+       time = 0;
+       cache_mats.resize(0);
+       for (int i = 0; i < n_layers; ++i) {
+               int n_ports = modules[i]->nPorts();
+               for (int j = 0; j < n_ports; ++j) {
+                       fprop_data_cache[i][j] = NULL;
+                       bprop_data_cache[i][j] = NULL;
+               }
+       }
+}
+
+//! Initializes sampling. Basically, it writes a random value to the
+//! top rbm and does gibbsTop gibbs steps. Call this before calling sample().
+void TreeDBNModule::initSampling(int gibbsTop)
+{
+       modules[n_layers-1]->min_n_Gibbs_steps = gibbsTop;
+
+       Mat hidden(1, modules[n_layers-1]->hidden_layer->size);
+
+       for (int i = 0; i < modules[n_layers-1]->hidden_layer->size; ++i)
+       {
+               hidden[0][i] = rand() & 1;
+       }
+
+       Mat exp;
+       TVec <Mat*> fprop_data(modules[n_layers-1]->nPorts(), (Mat*)NULL);
+
+       fprop_data[modules[n_layers-1]->getPortIndex("hidden_sample")] = &hidden;
+       fprop_data[modules[n_layers-1]->getPortIndex("visible_sample")] = &exp;
+
+       // Initialize with random sample
+       modules[n_layers-1]->fprop(fprop_data);
+
+       // Run chain for min_n_Gibbs_steps
+       fprop_data.fill((Mat*)NULL);
+       exp.resize(0,0);
+       fprop_data[modules[n_layers-1]->getPortIndex("visible_sample")] = &exp;
+       modules[n_layers-1]->fprop(fprop_data);
+}
+
+
+//! Returns a sample from the visible layer.
+Vec TreeDBNModule::sample(int gibbsTop)
+{
+	modules[n_layers-1]->n_Gibbs_steps_per_generated_sample = gibbsTop;
+
+       // Sample visible expectations from top layer rbm
+       TVec <Mat> samples(n_layers);
+
+       TVec <Mat*> fprop_data(modules[n_layers-1]->nPorts(), (Mat*)NULL);
+
+       fprop_data[modules[n_layers-1]->getPortIndex("visible_sample")] = &samples[n_layers-1];
+
+       modules[n_layers-1]->fprop(fprop_data);
+
+       // Propagate expectations down the network
+       for (int layer = n_layers-2; layer >= 0; --layer)
+       {
+               // Fill hidden sample for layer rbms
+               int width = modules[layer]->hidden_layer->size;
+               Mat hidden_sample(layer_sizes[layer], width);
+               for (int i = 0; i < layer_sizes[layer]; ++i)
+               {
+                       hidden_sample(i) << samples[layer+1](i/n_parents_per_node).subVec((i%n_parents_per_node)*width, width);
+               }
+
+               TVec <Mat*> fp_data(modules[layer]->nPorts(), (Mat*)NULL);
+               //fp_data[modules[layer]->getPortIndex("visible_reconstruction.state")] = &samples[layer];
+               //fp_data[modules[layer]->getPortIndex("hidden.state")] = &hidden_sample;
+               fp_data[modules[layer]->getPortIndex("visible_sample")] = &samples[layer];
+               fp_data[modules[layer]->getPortIndex("hidden_sample")] = &hidden_sample;
+
+               modules[layer]->fprop(fp_data);
+       }
+
+       Vec sample(samples[0].size());
+       for (int i = 0; i < samples[0].length(); ++i)
+               sample.subVec(i*samples[0].width(), samples[0].width()) << samples[0](i);
+
+       return sample;
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+/* Optional
+int TreeDBNModule::getPortIndex(const string& port)
+{}
+*/
+
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& TreeDBNModule::getPorts() {
+       return port_names;
+}
+
+//////////////////
+// getPortSizes //
+//////////////////
+/* Optional
+const TMat<int>& TreeDBNModule::getPortSizes() {
+}
+*/
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void TreeDBNModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+   inherited::makeDeepCopyFromShallowCopy(copies);
+
+   // ### Call deepCopyField on all "pointer-like" fields
+   // ### that you wish to be deepCopied rather than
+   // ### shallow-copied.
+   // ### ex:
+   deepCopyField(modules, copies);
+
+   // ### Remove this line when you have fully implemented this method.
+   //PLERROR("TreeDBNModule::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+// The default implementation raises a warning and does not do anything.
+void TreeDBNModule::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
+
+}
+// end of namespace PLearn
+
+
+/*
+ Local Variables:
+ mode:c++
+ c-basic-offset:4
+ c-file-style:"stroustrup"
+ c-file-offsets:((innamespace . 0)(inline-open . 0))
+ indent-tabs-mode:nil
+ fill-column:79
+ End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.h
===================================================================
--- trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.h	2007-08-29 18:23:02 UTC (rev 8035)
+++ trunk/plearn_learners/online/EXPERIMENTAL/TreeDBNModule.h	2007-08-29 18:53:09 UTC (rev 8036)
@@ -0,0 +1,389 @@
+// -*- C++ -*-
+
+// TreeDBNModule.h
+//
+// Copyright (C) 2007 Vytenis Sakenas
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Vytenis Sakenas
+
+/*! \file TreeDBNModule.h */
+
+
+#ifndef TreeDBNModule_INC
+#define TreeDBNModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/RBMModule.h>
+#include <plearn/sys/Profiler.h>
+
+namespace PLearn {
+
+
+class TreeDBNModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+	//! Array of RBMModule's used for every layer	
+	TVec <PP<RBMModule> > modules;
+	
+	//! From how many parents the upper layer RBM takes its input.
+	int n_parents_per_node;
+	
+	//! NOT IMPLEMENTED. Defines how many parent modules adjacent
+	//! upper modules share
+	int n_shared_parents;
+	
+	//! Value that multiplies the gradient of energy if it is computed.
+	real gradient_multiplier;
+	
+	//! Whether to propagate gradient throug all hierarchical structure.
+	bool propagate_gradient;
+
+	//! Whether to compute and propagate energy gradient.
+	bool propagate_energy_gradient;
+	
+	//! If true, gradient is propagated through all hierarchy, not only rightmost branch
+	bool propagate_full_gradient;
+	
+	//! Ports that should be provided by module. It is mapping
+	//! ("external_name", "rbm_name.rbm_port")
+	TVec< pair<string, string > > ports;
+	
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    TreeDBNModule();
+
+    // Your other public member functions go here
+
+    //! Perform a fprop step.
+    //! Each Mat* pointer in the 'ports_value' vector can be one of:
+    //! - a full matrix: this is data that is provided to the module, and can
+    //!                  be used to compute other ports' values
+    //! - an empty matrix: this is what we want to compute
+    //! - a NULL pointer: this is data that is not available, but whose value
+    //!                   does not need to be returned (or even computed)
+    //! The default version will either:
+    //! - call the mini-batch versions of standard fprop if 'ports_value' has
+    //!   size 2, with the first value being provided (and the second being
+    //!   the desired output)
+    //! - crash otherwise
+    void fprop(const TVec<Mat*>& ports_value);
+
+    //! Perform a back propagation step (also updating parameters according to
+    //! the provided gradient).
+    //! The matrices in 'ports_value' must be the same as the ones given in a
+    //! previous call to 'fprop' (and thus they should in particular contain
+    //! the result of the fprop computation). However, they are not necessarily
+    //! the same as the ones given in the LAST call to 'fprop': if there is a
+    //! need to store an internal module state, this should be done using a
+    //! specific port to store this state.
+    //! Each Mat* pointer in the 'ports_gradient' vector can be one of:
+    //! - a full matrix  : this is the gradient that is provided to the module,
+    //!                    and can be used to compute other ports' gradient.
+    //! - an empty matrix: this is a gradient we want to compute and accumulate
+    //!                    into. This matrix must have length 0 and a width
+    //!                    equal to the width of the corresponding matrix in
+    //!                    the 'ports_value' vector (we can thus accumulate
+    //!                    gradients using PLearn's ability to keep intact
+    //!                    stored values when resizing a matrix' length).
+    //! - a NULL pointer : this is a gradient that is not available, but does
+    //!                    not need t(const TVec<Mat*>& ports_value);o be returned (or even computed).
+    //! The default version tries to use the standard mini-batch bpropUpdate
+    //! method, when possible.
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    /* Optional
+
+    //! Given the input, compute the output (possibly resize it appropriately)
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Given a batch of inputs, compute the outputs
+    //! SOON TO BE DEPRECATED, USE fprop(const TVec<Mat*>& ports_value)
+    virtual void fprop(const Mat& inputs, Mat& outputs);
+    */
+
+    /* Optional
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! The flag indicates wether the input_gradient is accumulated or set.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! Batch version
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate=false);
+    */
+
+    /* Optional
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    //! Batch version
+    //! SOON TO BE DEPRECATED, USE bpropAccUpdate(const TVec<Mat*>& ports_value,
+    //!                                           const TVec<Mat*>& ports_gradient)
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             const Mat& output_gradients);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian,
+                              bool accumulate=false);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+    */
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    /* Optional
+       Default implementation prints a warning and does nothing
+    //! If this class has a learning rate (or something close to it), set it.
+    //! If not, you can redefine this method to get rid of the warning.
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
+    //! Return the list of ports in the module.
+    //! The default implementation returns a pair ("input", "output") to handle
+    //! the most common case.
+    virtual const TVec<string>& getPorts();
+	
+	//! Initialize tree to perform sampling.
+	//! gibbsTop - number of Gibbs steps to do on the top layer.
+	void initSampling(int gibbsTop);
+	
+	//! Returns a sample of the visible layer.
+	//! nGibbs - number of Gibbs steps to do before sampling
+	Vec sample(int nGibbs);
+
+	//! Clears all caches.
+	void clearCache();
+
+
+    /* Optional
+    //! Return the size of all ports, in the form of a two-column matrix, where
+    //! each row represents a port, and the two numbers on a row are
+    //! respectively its length and its width (with -1 representing an
+    //! undefined or variable value).
+    //! The default value fills this matrix with:
+    //!     - in the first column (lengths): -1
+    //!     - in the second column (widths):
+    //!         - -1 if nPorts() < 2
+    //!         - 'input_size' for the first row and 'output_size' for the
+    //!           second row if nPorts() >= 2
+    virtual const TMat<int>& getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    //  ### Default implementation performs a simple linear search in
+    //  ### getPorts().
+    virtual int getPortIndex(const string& port);
+    */
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(TreeDBNModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+	
+	static void declareMethods(RemoteMethodMap& rmm);
+	
+	// Number of layers.
+	int n_layers;
+	
+	TVec<string> port_names;
+	TVec<int> port_rbms;		// Index of rbm that writes to this port, or -1 if none does.
+	TVec<int> port_index;		// Index of rbms port that writes to this port.
+	
+	// Number of rbms in layer.
+	TVec<int> layer_sizes;
+	
+	// Steps after which current rbm output will be reused.
+	// Same as count of lowest layer inputs that this rbm depends on.
+	TVec <int> step_size;
+	
+	// Last input provided to the module. Is keeped for determining if input is shifted
+	// during subsequent fprop calls.
+	Vec last_full_input;
+	
+	// Fprop and bprop data that is used when calling individual
+	// rbm methods
+	TVec < TVec<Mat*> > fprop_data;
+	
+	TVec < TVec<Mat*> > bprop_data;
+	
+	// Same as above but saved for reuse between subsequent
+	// calls to fprop
+	TVec < TVec<Mat*> > fprop_data_cache;
+	
+	TVec < TVec<Mat*> > bprop_data_cache;
+	
+	int time;		// Current pseudo-time of module
+
+	TVec <int> mbi_time;	// Pseudo-time for each sample in the minibatch.
+	
+	// In ..[i][j] Keeps length of fprop_data for layer i rbm after processing j samples in the minibatch
+	TMat <int> mod_batch_length;	
+	
+	// Storage for actual matrices
+	TVec <Mat> mats;
+	TVec <Mat> cache_mats;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+	
+	//! Checks whether b is a shifted to the left by k 
+	bool check_shift(Vec &a, Vec &b, int k);
+	
+	//! Used to compute row number in fprop matrix for a sample
+	int hash(int t, int k, int i);
+
+	void appendPort(string name, int rbm_index, string port_name, int port_width);
+	
+	//! Helper function to create a matrix
+	Mat* createMatrix(int length, int width, TVec <Mat> &mats);
+
+	//! Updates cache after fprop
+	void updateCache();
+
+	//! Fprop that does not use any optimization
+	void full_fprop(const TVec<Mat*>& ports_value);
+	
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(TreeDBNModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From tihocan at mail.berlios.de  Thu Aug 30 20:20:09 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 30 Aug 2007 20:20:09 +0200
Subject: [Plearn-commits] r8037 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200708301820.l7UIK9Ao011209@sheep.berlios.de>

Author: tihocan
Date: 2007-08-30 20:20:09 +0200 (Thu, 30 Aug 2007)
New Revision: 8037

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
Put back the decrease constant mechanism. Now it does not use the semaphore anymore, but instead uses shared memory: this overcomes the issue of a max value for semaphores.

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-29 18:53:09 UTC (rev 8036)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-30 18:20:09 UTC (rev 8037)
@@ -99,12 +99,17 @@
       cumulative_training_time(0),
       params_ptr(NULL),
       params_id(-1),
+      params_int_ptr(NULL),
+      params_int_id(-1),
       nsteps(0),
       semaphore_id(-1)
 {
     random_gen = new PRandom();
 }
 
+////////////////////
+// declareOptions //
+////////////////////
 void NatGradSMPNNet::declareOptions(OptionList& ol)
 {
     declareOption(ol, "delayed_update", &NatGradSMPNNet::delayed_update,
@@ -349,6 +354,9 @@
     inherited::declareOptions(ol);
 }
 
+////////////
+// build_ //
+////////////
 void NatGradSMPNNet::build_()
 {
     if (!train_set)
@@ -410,16 +418,17 @@
     }
 
     // Allocate shared memory for parameters.
-    // First deallocate memory if needed.
-    if (params_ptr) {
-        shmctl(params_id, IPC_RMID, 0);
-        params_ptr = NULL;
-    }
+    freeSharedMemory(); // First deallocate memory if needed.
     long total_memory_needed = long(n_params) * sizeof(real);
     params_id = shmget(IPC_PRIVATE, total_memory_needed, 0666 | IPC_CREAT);
     PLCHECK( params_id != -1 );
     params_ptr = (real*) shmat(params_id, 0, 0);
-    assert( params_ptr );
+    PLCHECK( params_ptr );
+    long total_int_memory_needed = 1 * sizeof(int);
+    params_int_id = shmget(IPC_PRIVATE, total_int_memory_needed, 0666 | IPC_CREAT);
+    PLCHECK( params_int_id != -1 );
+    params_int_ptr = (int*) shmat(params_int_id, 0, 0);
+    PLCHECK( params_int_ptr );
     // We should have copied data from 'all_params' first if there were some!
     PLCHECK_MSG( all_params.isEmpty(), "Multiple builds not implemented yet" );
     all_params = Vec(n_params, params_ptr);
@@ -552,14 +561,36 @@
 
 }
 
-// ### Nothing to add here, simply calls build_
+///////////
+// build //
+///////////
 void NatGradSMPNNet::build()
 {
     inherited::build();
     build_();
 }
 
+//////////////////////
+// freeSharedMemory //
+//////////////////////
+void NatGradSMPNNet::freeSharedMemory()
+{
+    if (params_ptr) {
+        shmctl(params_id, IPC_RMID, 0);
+        params_ptr = NULL;
+        params_id = -1;
+    }
+    if (params_int_ptr) {
+        shmctl(params_int_id, IPC_RMID, 0);
+        params_int_ptr = NULL;
+        params_int_id = -1;
+    }
+}
 
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void NatGradSMPNNet::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
@@ -604,8 +635,12 @@
     if (params_ptr)
         PLERROR("In NatGradSMPNNet::makeDeepCopyFromShallowCopy - Deep copy of"
                 " 'params_ptr' not implemented");
+    if (params_int_ptr)
+        PLERROR("In NatGradSMPNNet::makeDeepCopyFromShallowCopy - Deep copy of"
+                " 'params_int_ptr' not implemented");
 
 
+
 /*
     deepCopyField(, copies);
 */
@@ -708,7 +743,7 @@
     // period.
     // Finally, the last value is the current stage, i.e. the number of samples
     // with which the network has been updated so far.
-    semaphore_id = semget(IPC_PRIVATE, ncpus + 2, 0666 | IPC_CREAT);
+    semaphore_id = semget(IPC_PRIVATE, ncpus + 1, 0666 | IPC_CREAT);
     if (semaphore_id == -1)
         PLERROR("In NatGradSMPNNet::train - Could not create semaphore "
                 "(errno = %d)", errno);
@@ -721,13 +756,11 @@
             PLERROR("In NatGradSMPNNet::train - Could not initialize semaphore"
                     " value (errno = %d)", errno);
     }
-    semun_v.val = stage;
-    semun_v.val = 0; // TODO Fix (pb with max semaphore value)
-    int success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
-    if (success != 0)
-        PLERROR("In NatGradSMPNNet::train - Could not initialize semaphore"
-                " value to store the stage (errno = %d)", errno);
 
+    // Initialize current stage, stored in integer shared memory.
+    int stage_idx = 0;
+    params_int_ptr[stage_idx] = stage;
+
     // Fork one process/cpu.
     int iam = 0;
     for (int cpu = 1; cpu < ncpus ; cpu++)
@@ -762,7 +795,7 @@
         {
             // Read the current stage value (will be used to compute the
             // current learning rate).
-            int cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
+            int cur_stage = params_int_ptr[stage_idx];
             PLASSERT( cur_stage >= 0 );
             onlineStep(cur_stage, targets, train_costs, example_weights );
             nsteps++;
@@ -782,20 +815,14 @@
                     params_update.clear();
                 }
                 // Update the current stage.
-                cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
+                cur_stage = params_int_ptr[stage_idx];
                 PLASSERT( cur_stage >= 0 );
-                semun_v.val = cur_stage + nsteps * minibatch_size;
-                semun_v.val = 0; // TODO Fix: pb with max semaphore value.
-                success = semctl(semaphore_id, ncpus + 1, SETVAL, semun_v);
-                if (success != 0)
-                    PLERROR("In NatGradSMPNNet::train - Could not update "
-                            "stage value for semaphore (errno = %d, returned "
-                            "value = %d, set value = %d)", errno, success,
-                            semun_v.val);
+                int new_stage = cur_stage + nsteps * minibatch_size;
+                params_int_ptr[stage_idx] = new_stage;
                 // Give update token to next CPU.
                 sem_value = (sem_value + 1) % ncpus;
                 semun_v.val = sem_value;
-                success = semctl(semaphore_id, 0, SETVAL, semun_v);
+                int success = semctl(semaphore_id, 0, SETVAL, semun_v);
                 if (success != 0)
                     PLERROR("In NatGradSMPNNet::train - Could not update "
                             "semaphore with next CPU (errno = %d, returned "
@@ -912,7 +939,7 @@
     */
 
     // Get current stage (for debug purpose).
-    int cur_stage = semctl(semaphore_id, ncpus + 1, GETVAL);
+    int cur_stage = params_int_ptr[stage_idx];
     PLASSERT( cur_stage >= 0 );
 
     // Free semaphore's ressources.
@@ -1402,10 +1429,7 @@
 
 NatGradSMPNNet::~NatGradSMPNNet()
 {
-    if (params_ptr) {
-        shmctl(params_id, IPC_RMID, 0);
-        params_ptr = NULL;
-    }
+    freeSharedMemory();
     if (semaphore_id >= 0) {
         int success = semctl(semaphore_id, 0, IPC_RMID);
         if (success < 0)

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-08-29 18:53:09 UTC (rev 8036)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-08-30 18:20:09 UTC (rev 8037)
@@ -329,8 +329,10 @@
     Vec example_weights; // one element per example in a minibatch
     Mat train_costs; // one row per example in a minibatch
 
-    real* params_ptr; // Raw pointer to the (shared) parameters.
-    int params_id; // Shared memory id for parameters.
+    real* params_ptr;       // Raw pointer to the (shared) parameters.
+    int params_id;          // Shared memory id for parameters.
+    int* params_int_ptr;    // Raw pointer to the (shared) integer parameters.
+    int params_int_id;      // Shared memory id for integer parameters.
 
     //! Number of online steps performed since the last global parameter update.
     int nsteps;
@@ -347,6 +349,9 @@
     TVec<Mat> layer_params_update;
 
     //PP<CorrelationProfiler> g_corrprof, ng_corrprof;    // for optional gradient correlation profiling
+
+    //! Free shared memory that may still be allocated.
+    void freeSharedMemory();
 };
 
 // Declares a few other classes and functions related to this class



From tihocan at mail.berlios.de  Thu Aug 30 21:48:34 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 30 Aug 2007 21:48:34 +0200
Subject: [Plearn-commits] r8038 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200708301948.l7UJmYBn019530@sheep.berlios.de>

Author: tihocan
Date: 2007-08-30 21:48:34 +0200 (Thu, 30 Aug 2007)
New Revision: 8038

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
- Improved splitting of samples between each CPU: now the difference between the number of samples seen by two CPUs is at most 1
- Fixed problem where the last few samples would not always be seen
- Fixed the computation of the number of stages being actually performed


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-30 18:20:09 UTC (rev 8037)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-08-30 19:48:34 UTC (rev 8038)
@@ -686,6 +686,9 @@
     params_update.fill(0);
 }
 
+///////////
+// train //
+///////////
 void NatGradSMPNNet::train()
 {
 
@@ -770,12 +773,29 @@
         }
 
     // Each processor computes gradient over its own subset of samples (between
-    // indices 'start' and 'end' in the training set).
-    int start = (nsamples / ncpus) * iam;
-    int end = iam == ncpus - 1 ? nsamples
-                               : (nsamples / ncpus) * (iam + 1);
-    int my_n_samples = end - start;
+    // indices 'start' and 'start + my_n_samples' in the training set).
+    int n_left = nsamples % ncpus;
+    int n_per_cpu = nsamples / ncpus;
+    int start, my_n_samples;
+    if (iam < n_left) {
+        // This CPU is given one extra training sample to compensate for the
+        // fact that the number of samples is not an exact multiple of the
+        // number of CPUs.
+        start = (n_per_cpu + 1) * iam;
+        my_n_samples = n_per_cpu + 1;
+    } else {
+        start = (n_per_cpu + 1) * n_left + n_per_cpu * (iam - n_left);
+        my_n_samples = n_per_cpu;
+    }
+    if (iam == 0)
+        PLASSERT_MSG( start == 0, "First CPU must start at first sample" );
+    if (iam == ncpus - 1)
+        PLASSERT_MSG( start + my_n_samples == nsamples,
+                      "Last CPU must start at last sample" );
 
+    // The total number of examples that must be seen is given by 'stage_incr',
+    // computed as 'nstages - stage'. Each CPU is responsible for going through
+    // a fraction of 'stage_incr', denoted by 'my_stage_incr'.
     int stage_incr = nstages - stage;
     int stage_incr_per_cpu = stage_incr / ncpus;
     int stage_incr_left = stage_incr % ncpus;
@@ -791,14 +811,17 @@
         //Profiler::pl_profile_start("getting_data");
         train_set->getExample(sample, input, target, example_weights[b]);
         //Profiler::pl_profile_end("getting_data");
-        if (b+1==minibatch_size) // do also special end-case || stage+1==nstages)
+        if (b == minibatch_size - 1 || i == my_stage_incr - 1 )
         {
             // Read the current stage value (will be used to compute the
             // current learning rate).
             int cur_stage = params_int_ptr[stage_idx];
             PLASSERT( cur_stage >= 0 );
+            // Note that we should actually call onlineStep only on the subset
+            // of samples that are new (compared to the previous mini-batch).
+            // This is left as a TODO since it is not a priority.
             onlineStep(cur_stage, targets, train_costs, example_weights );
-            nsteps++;
+            nsteps += b + 1;
             /*
             for (int i=0;i<minibatch_size;i++)
             {
@@ -817,7 +840,7 @@
                 // Update the current stage.
                 cur_stage = params_int_ptr[stage_idx];
                 PLASSERT( cur_stage >= 0 );
-                int new_stage = cur_stage + nsteps * minibatch_size;
+                int new_stage = cur_stage + nsteps;
                 params_int_ptr[stage_idx] = new_stage;
                 // Give update token to next CPU.
                 sem_value = (sem_value + 1) % ncpus;
@@ -859,6 +882,9 @@
                 all_params += params_update;
                 params_update.clear();
             }
+            // Note that the line below is not safe: if two CPUs are running it
+            // at the same time, the number of stages may not be correct.
+            params_int_ptr[stage_idx] += nsteps;
             nsteps = 0;
         }
         // Indicate this CPU is done.
@@ -885,6 +911,7 @@
                         all_params += params_update;
                         params_update.clear();
                     }
+                    params_int_ptr[stage_idx] += nsteps;
                     nsteps = 0;
                 }
                 // Indicate this CPU is done.

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-08-30 18:20:09 UTC (rev 8037)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-08-30 19:48:34 UTC (rev 8038)
@@ -334,7 +334,10 @@
     int* params_int_ptr;    // Raw pointer to the (shared) integer parameters.
     int params_int_id;      // Shared memory id for integer parameters.
 
-    //! Number of online steps performed since the last global parameter update.
+    //! Number of samples seen since the last global parameter update. Note
+    //! that usually this is a multiple of 'minibatch_size', but this may
+    //! not be the case at the end of training, when the total number of
+    //! samples that must be seen is not a multiple of 'minibatch_size'.
     int nsteps;
 
     //! Semaphore used to control which CPU must perform an update.



From tihocan at mail.berlios.de  Thu Aug 30 22:07:39 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 30 Aug 2007 22:07:39 +0200
Subject: [Plearn-commits] r8039 - trunk
Message-ID: <200708302007.l7UK7dVq021102@sheep.berlios.de>

Author: tihocan
Date: 2007-08-30 22:07:39 +0200 (Thu, 30 Aug 2007)
New Revision: 8039

Modified:
   trunk/pymake.config.model
Log:
- Removed -fPIC from the compilation line under Windows to remove compiler warning
- The correct curses library (either ncurses or pdcurses) now depends on the platform, instead of requiring the user to modify the config file by hand
- Fixed Python stuff to work under Windows after changes to the PLearn/Numpy dependences


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2007-08-30 19:48:34 UTC (rev 8038)
+++ trunk/pymake.config.model	2007-08-30 20:07:39 UTC (rev 8039)
@@ -84,7 +84,9 @@
 
 # We add -fPIC, as it is requested to make a shared '.so' file, it does not
 # have any speed impact, and is ignored by the compiler if it cannot use it.
-compileflags += ' -fPIC'
+# We do not add it under Windows since it generates a useless compiler warning.
+if platform != 'win32':
+    compileflags += ' -fPIC'
 
 # The platform variable contains the type of platform from which pymake has been invoked. 
 # This is something like 'linux-i386', 'linux-alpha', 'sunos5', 'irix6', 'irix6-n32' etc...
@@ -210,14 +212,16 @@
 #                 linkeroptions = '/u/lisa/local/'+target_platform+'/Install/wxWindows/wxGTK-2.2.9/lib/libwx_gtk-2.2.so.6.2.6 -L/usr/lib -L/usr/X11R6/lib -lgtk -lgdk -rdynamic -lgmodule -lgthread -lglib -lpthread -ldl -lXi -lXext -lX11 -lm  -lpng -ljpeg -ltiff  -ldl -lpthread -lz -lm' )
 
 
-# Note: under Unix-like systems, one will probably use 'ncurses'.
-# However, 'ncurses' is not implemented under Windows, and one can instead use
-# 'pdcurses'. Currently, you need to edit this file by hand to replace 'ncurses'
-# with 'pdcurses', as it will not be detected automatically.
-optionalLibrary( name = 'ncurses',
-                 triggers = 'curses.h',
-                 linkeroptions = '-lncurses'
-                 )
+if platform == 'win32':
+    optionalLibrary( name = 'pdcurses',
+                     triggers = 'curses.h',
+                     linkeroptions = '-lpdcurses'
+                   )
+else:
+    optionalLibrary( name = 'ncurses',
+                     triggers = 'curses.h',
+                     linkeroptions = '-lncurses'
+                   )
 
 if target_platform=='linux-x86_64':
     wn_includedirs = '/u/lisa/WordNet/LINUX_64/v2.0/include'
@@ -303,11 +307,9 @@
             numpy_includedirs = [ ]
             numpy_site_packages = '$HOME/local-cygwin/lib/python%s/site-packages/numarray' % pyver
         elif platform == 'win32':
-            python_version = '2.4'
-            python_lib_root = ''
-            linkeroptions_tail += '-L/cygdrive/r/python2.4_mingw'
-            numpy_site_packages = join(homedir, \
-                  'local/lib/python2.4/site-packages/numarray')
+            python_version = '2.5'
+            python_lib_root = 'C:\\Python25\\libs'
+            numpy_site_packages = []
             numpy_includedirs = []
         else:
             numpy_includedirs = [ '/u/lisa/local/' + target_platform + '/include/', '/usr/include/python'+python_version ]
@@ -395,22 +397,29 @@
     if 'numpy' in optionargs:
         numpy_lib= '/_capi.so -lutil '
 
-    if platform!='darwin':
+    if platform == 'darwin':
         optionalLibrary( name = 'python',
                      triggers = '[Pp]ython*',
                      includedirs = numpy_includedirs,
-                     linkeroptions = ( '%s%s ' % (numpy_site_packages, numpy_lib) + 
+                     linkeroptions = ( '-L%s -lnumarray ' % numpy_site_packages + 
                                        '-L%s/python%s/config -lpython%s ' % (python_lib_root, python_version, python_version) +
-                                       '-Xlinker -export-dynamic ' + 
-                                       '-Xlinker -rpath -Xlinker %s' % numpy_site_packages )
+                                       '' )
                      )
+    elif platform == 'win32':
+        optionalLibrary( name = 'python',
+                     triggers = '[Pp]ython*',
+                     includedirs = numpy_includedirs,
+                     linkeroptions = ( '-L%s -lpython%s ' % \
+                             (python_lib_root,
+                              python_version.replace('.', ''))))
     else:
         optionalLibrary( name = 'python',
                      triggers = '[Pp]ython*',
                      includedirs = numpy_includedirs,
-                     linkeroptions = ( '-L%s -lnumarray ' % numpy_site_packages + 
+                     linkeroptions = ( '%s%s ' % (numpy_site_packages, numpy_lib) + 
                                        '-L%s/python%s/config -lpython%s ' % (python_lib_root, python_version, python_version) +
-                                       '' )
+                                       '-Xlinker -export-dynamic ' + 
+                                       '-Xlinker -rpath -Xlinker %s' % numpy_site_packages )
                      )
 else:
     python_version = '' # should not be used anyway



From tihocan at mail.berlios.de  Thu Aug 30 22:08:39 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 30 Aug 2007 22:08:39 +0200
Subject: [Plearn-commits] r8040 - trunk/python_modules/plearn/pymake
Message-ID: <200708302008.l7UK8dMI021202@sheep.berlios.de>

Author: tihocan
Date: 2007-08-30 22:08:38 +0200 (Thu, 30 Aug 2007)
New Revision: 8040

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
- Fixed link bug under Windows, due to the case-insensitive filesystem
- Added (commented) debug code
- Fixed a few lines that had more than 80 characters


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-08-30 20:07:39 UTC (rev 8039)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-08-30 20:08:38 UTC (rev 8040)
@@ -861,6 +861,9 @@
     "returns a FileInfo object for this file"
     "parsing the file if necessary, and remembering the result for later buffered reuse"
     absfilepath = abspath(filepath)
+    if platform == 'win32':
+        # Under Windows, file paths are case-insensitive.
+        absfilepath = absfilepath.lower()
     if file_info_map.has_key(absfilepath):
         return file_info_map[absfilepath]
     else:
@@ -1666,21 +1669,27 @@
         if self.is_ccfile:
             if self not in ccfiles_to_link:
                 ccfiles_to_link.append(self)
-#                 print self.filebase,self.fileext
+                #print 'APPENDING CC: %s%s' % (self.filebase, self.fileext)
                 for include in self.includes_from_sourcedirs:
-                    #print self.filebase
-                    include.collect_ccfiles_to_link(ccfiles_to_link, visited_hfiles)
+                    include.collect_ccfiles_to_link(ccfiles_to_link,
+                                                    visited_hfiles)
         else: # it's a .h file
             if self not in visited_hfiles:
                 visited_hfiles.append(self)
+                #print 'APPENDING H: %s%s' % (self.filebase, self.fileext)
                 if self.corresponding_ccfile:
-                    self.corresponding_ccfile.collect_ccfiles_to_link(ccfiles_to_link, visited_hfiles)
+                    #print 'HAS_CC: %s%s' % (self.corresponding_ccfile.filebase,
+                    #                        self.corresponding_ccfile.fileext)
+                    self.corresponding_ccfile.collect_ccfiles_to_link(
+                            ccfiles_to_link, visited_hfiles)
 
                 if self.hasqobject:
-                    self.corresponding_moc_cc.collect_ccfiles_to_link(ccfiles_to_link, visited_hfiles)
+                    self.corresponding_moc_cc.collect_ccfiles_to_link(
+                            ccfiles_to_link, visited_hfiles)
 
                 for include in self.includes_from_sourcedirs:
-                    include.collect_ccfiles_to_link(ccfiles_to_link, visited_hfiles)
+                    include.collect_ccfiles_to_link(ccfiles_to_link,
+                                                    visited_hfiles)
 
     def get_ccfiles_to_link(self):
         """returns the list of FileInfos of all .cc files that need to be linked together to produce the corresponding_output"""
@@ -2202,6 +2211,7 @@
                 # actually does a copy).
                 symlink_command = 'ln -s %s %s' % (ccfile.corresponding_ofile,
                                                    dummy_obj_file)
+                #print 'LINK: ' + symlink_command
                 os.system(symlink_command)
                 objs_count += 1
                 result.append(dummy_obj_file)



