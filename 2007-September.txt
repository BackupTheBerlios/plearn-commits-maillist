From louradou at mail.berlios.de  Sat Sep  1 01:30:02 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 1 Sep 2007 01:30:02 +0200
Subject: [Plearn-commits] r8041 - trunk/python_modules/plearn/learners
Message-ID: <200708312330.l7VNU2is012925@sheep.berlios.de>

Author: louradou
Date: 2007-09-01 01:30:02 +0200 (Sat, 01 Sep 2007)
New Revision: 8041

Modified:
   trunk/python_modules/plearn/learners/SVM.py
Log:


Modified: trunk/python_modules/plearn/learners/SVM.py
===================================================================
--- trunk/python_modules/plearn/learners/SVM.py	2007-08-30 20:08:38 UTC (rev 8040)
+++ trunk/python_modules/plearn/learners/SVM.py	2007-08-31 23:30:02 UTC (rev 8041)
@@ -1,5 +1,5 @@
 import sys, os, time
-#from numarray import *
+from numarray import *
 from math import *
 from libsvm import *
 
@@ -9,7 +9,7 @@
                         'parameters_names',
                         'tried_parameters',
                         'best_parameters',
-                        'error_rate'
+                        'error_rate',
                         ]
                         
       def __init__( self ):
@@ -23,6 +23,11 @@
           #if self.best_parameters != None:
           #   self.add_parameter_to_tried_list(self.getBestValue('C'), self.best_parameters[1:])
           self.error_rate       = 1.
+	  self.should_be_tuned_again = 1
+
+
+      def should_be_tuned_again():
+          raise StandardError, "should_be_tuned_again is nor implemented for this class"
           
       def get_svm_parameter( self, parameters ):
           s= ', '.join([ self.parameters_names[i]+' = '+str(parameters[i]) for i in range(len(self.parameters_names)) ])
@@ -68,12 +73,12 @@
           self.parameters_names += ['gamma']
 
       def init_gamma(self, gamma):
-          return [gamma, gamma/9., gamma*9.]
+          return [gamma/9., gamma, gamma*9.]
           
       def init_parameters( self, samples ):
           dim = len(samples[0])
           std = mean_std(samples)
-          rho=sqrt(dim)*std
+          rho = sqrt(dim)*std
           gamma0 = 1/(2*rho**2)
           gamma_base = self.init_gamma(gamma0)
           return SVM_expert.init_parameters( self, gamma_base )
@@ -90,6 +95,26 @@
              proposed_gammas = self.init_gamma(best_gamma)
           return SVM_expert.init_parameters(self, proposed_gammas)
 
+      def should_be_tuned_again( self ):
+          best_gamma = self.getBestValue('gamma') 
+          tried_gamma = self.tried_parameters
+	  if tried_gamma.has_key(best_gamma):
+	     is_lower = False
+	     is_higher = False
+             for gamma in tried_gamma:
+	         if gamma<tried_gamma:
+		    is_lower = True
+	         elif gamma<tried_gamma:
+		    is_higher = True
+	     if is_lower and is_higher:
+	        best_C_for_this_gamma  = self.getBestValue('C') 
+		tried_C_for_this_gamma = self.tried_parameters[best_gamma]
+		return not ( best_C_for_this_gamma <> min(tried_C_for_this_gamma) and best_C_for_this_gamma <> max(tried_C_for_this_gamma) )
+	     else:
+	        return True
+	  else:
+	     return True
+      
 class POLY_expert(SVM_expert):
       def __init__( self ):
           SVM_expert.__init__( self )
@@ -113,7 +138,17 @@
                 return SVM_expert.choose_new_parameters(self, (self.best_parameters[1], self.best_parameters[2])  )
           else:
              return SVM_expert.init_parameters(self, self.init_degree(best_degree) )
-             
+	     
+      def should_be_tuned_again( self ):
+          best_degree = self.getBestValue('degree')
+          tried_degrees = [prms[0] for prms in self.tried_parameters]
+          if self.tried_parameters.has_key(best_degree):
+             if best_degree <> max(tried_degrees):
+	        best_C_for_this_degree  = self.getBestValue('C') 
+		tried_C_for_this_degree = self.tried_parameters[(best_degree,self.best_parameters[2])]
+		return not ( best_C_for_this_degree <> min(tried_C_for_this_degree) and best_C_for_this_degree <> max(tried_C_for_this_degree) )
+	  return True
+      
 class LINEAR_expert(SVM_expert):
       def __init__( self ):
           SVM_expert.__init__( self )
@@ -125,6 +160,12 @@
       def choose_new_parameters( self ):
           return SVM_expert.choose_new_parameters(self, None)
 
+      def should_be_tuned_again( self ):
+          if len(self.tried_parameters[None]) <= 3:
+	     return True
+          best_C = self.getBestValue('C') 
+          tried_C = self.tried_parameters[None]
+	  return not (best_C <> min(tried_C) and best_C <> max(tried_C) )
 
 class SVM(object):
 
@@ -136,6 +177,7 @@
 			'best_model',
 			'nr_fold'
                         'result_list',
+			'automatically_decide_when_to_stop_tuning'
                         ]
        
       def __init__( self ):
@@ -157,6 +199,8 @@
           # For cross-validation
           self.nr_fold        = 5
 
+	  self.automatically_decide_when_to_stop_tuning = False
+
       def reset( self ):
           self.LINEAR_expert.reset()
           self.RBF_expert.reset()
@@ -195,12 +239,18 @@
 
       def test(self, samples_target_list):
           check_samples_target_list([samples_target_list])
-          return test_model(self.best_model, samples_target_list[0], samples_target_list[1])
+          return test_model(self.best_model, [[x_i for x_i in x] for x in samples_target_list[0]], [float(l) for l in samples_target_list[1]])['error_rate']
 
 
       def train_and_tune(self, kernel_type, samples_target_list):
           check_samples_target_list(samples_target_list)
-          
+          if len(samples_target_list) == 1:
+             print "\nCross-validation...\n"
+          elif len(samples_target_list) == 2:
+             print "\nSimple validation...\n"
+          elif len(samples_target_list) == 3:
+             print "\nValidation + test...\n"
+           
           expert = eval( 'self.'+kernel_type+'_expert' )
           
           if len(expert.tried_parameters) == 0:
@@ -215,6 +265,7 @@
           
           best_parameters   = expert.best_parameters
           best_error_rate   = expert.error_rate
+          best_model        = None
 
           for parameters in parameters_to_try:
               if parameters != expert.best_parameters or recompute_best:
@@ -223,7 +274,7 @@
                   param = expert.get_svm_parameter( parameters )
                   
                   if len(samples_target_list) == 1: # cross-validation
-                     error_rate = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)
+                     error_rate = do_cross_validation(samples_target_list[0][0], samples_target_list[0][1], param, self.nr_fold)['error_rate']
                   else:
                      train_problem = svm_problem( samples_target_list[0][1] , samples_target_list[0][0] )
                      model = svm_model(train_problem, param)
@@ -242,7 +293,8 @@
                   if error_rate < best_error_rate:
                      best_parameters = parameters
                      best_error_rate = error_rate
-                     self.best_model = model
+		     if len(samples_target_list) <> 1: # in case of cross-validation, we will compute the model later
+                        best_model   = model
 
           if best_error_rate < expert.error_rate:
              expert.best_parameters = best_parameters
@@ -251,12 +303,21 @@
              if best_error_rate < self.valid_error_rate:
                 self.best_parameters = [kernel_type, best_parameters]
                 self.valid_error_rate = best_error_rate
+                if len(samples_target_list) == 1: # compute the best model in the case of cross-validation
+                   train_problem = svm_problem( [float(l) for l in samples_target_list[0][1]] , [[float(x_i) for x_i in x] for x in samples_target_list[0][0]] )
+                   param = expert.get_svm_parameter( best_parameters )
+                   best_model = svm_model(train_problem, param)
+                self.best_model = best_model
                 if len(samples_target_list) == 3: # train-valid-test
                    self.error_rate = test_model(self.best_model, samples_target_list[2][0], samples_target_list[2][1])['error_rate']
                 else:
                    self.error_rate = self.valid_error_rate
           
-          return self.error_rate
+	  if self.automatically_decide_when_to_stop_tuning:
+	     if expert.should_be_tuned_again():
+	        self.train_and_tune(kernel_type, samples_target_list)
+	  
+	  return self.error_rate
           
 
 ##
@@ -281,30 +342,50 @@
 ## - Then the error rate is simply the average error rate...
 ##
 def do_cross_validation(samples, targets, param, nr_fold):
+    arrayType=False
+    if 'array' in str(type(samples)):
+       arrayType=True
+    targets_subsets=[]
     samples_subsets=[]
-    targets_subsets=[]
     N=len(samples)
     for i in range(nr_fold):
-        samples_subsets.append(samples[i:N:nr_fold])
+        samples_subsets.append(samples[i:N:nr_fold,:])
         targets_subsets.append(targets[i:N:nr_fold])
     cum_error_rate=0.
     for i in range(nr_fold):
         test_samples = samples_subsets[i]
         test_targets = targets_subsets[i]
-        train_samples=[]
         train_targets=[]
+	if arrayType:
+           train_samples=[]
+	else:
+           train_samples=[]
         for j in range(0,i)+range(i+1,nr_fold):
-            train_samples += samples_subsets[j]
             train_targets += targets_subsets[j]
+	    if arrayType:
+	       L=len(train_samples)
+               train_samples=resize(samples,[L+len(samples_subsets[j]),len(samples_subsets[j][0])])
+	       train_samples[L:,:]=samples_subsets[j]
+	    else:
+               train_samples += samples_subsets[j]	       
         cum_error_rate += do_simple_validation(train_samples, train_targets, test_samples, test_targets, param)['error_rate']
-    ret = cum_error_rate / nr_fold
-    return {'error_rate':err}
+#        cum_error_rate += do_simple_validation(samples, targets, test_samples, test_targets, param)['error_rate']
+    av_error_rate = cum_error_rate*1.0 / nr_fold
+    print av_error_rate
+    return {'error_rate':av_error_rate}
         
+def do_simple_validation(train_samples, train_targets, test_samples, test_targets, param):    
+    train_samples = [[float(x_i) for x_i in x] for x in train_samples]
+    test_samples = [[float(x_i) for x_i in x] for x in test_samples]
+    train_problem = svm_problem( [float(l) for l in train_targets], train_samples )
+    model = svm_model(train_problem, param)
+    return test_model(model,test_samples,[float(l) for l in test_targets])
+
 def test_model(model, samples, targets):
     N = len(samples)
     diffs = {}
     for i in range(N):
-          diff = abs(model.predict(samples[i]) - targets[i])
+          diff = abs(model.predict([float(x_i) for x_i in samples[i]]) - float(targets[i]))
           if diffs.has_key(diff):
                 diffs[diff] += 1
           else:
@@ -320,19 +401,8 @@
     error_rate = float(error_rate) / N
     linear_class_error = float(linear_class_error) / N
     square_class_error = float(square_class_error) / N
-    
     return {'error_rate':error_rate, 'linear_class_error':linear_class_error, 'square_class_error':square_class_error }
 
-def do_simple_validation(train_samples, train_targets, test_samples, test_targets, param):
-    train_problem = svm_problem( train_targets, train_samples )
-    model = svm_model(train_problem, param)
-    return test_model(model,test_samples,test_targets)
-
-
-
-
-
-
 #
 # Some useful functions
 #
@@ -419,13 +489,7 @@
               raise ValueError, "ERROR: samples_target_list has an element that has an element with an empty length"
            if len(samples_target[0]) != len(samples_target[1]):
               raise ValueError, "ERROR: samples_target_list has an element that has an elements with different len. Len are: " + len(samples_target[0])+" and " + len(samples_target[1])
-    if len(samples_target_list) == 1:
-       print "\nCross-validation...\n"
-    elif len(samples_target_list) == 2:
-       print "\nSimple validation...\n"
-    elif len(samples_target_list) == 3:
-       print "\nValidation + test...\n"
-    else:
+    if len(samples_target_list) not in [1,2,3]:
        raise TypeError, "ERROR: samples_target_list have length "+str(len(samples_target_list))+" (not in [1,2,3])\n"+"samples_target_list has to be a list of [sample, target] arrays\n"+"for example :\n\t[[TrainSet, TrainLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels]]\n"+"\tor [[TrainSamples, TrainLabels], [ValidSamples, ValidLabels], [TestSamples, TestLabels]]\n"
 
 def parameters2list(C, kernel_parameters):



From louradou at mail.berlios.de  Sat Sep  1 01:31:00 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 1 Sep 2007 01:31:00 +0200
Subject: [Plearn-commits] r8042 - trunk/plearn_learners/online
Message-ID: <200708312331.l7VNV0R6012981@sheep.berlios.de>

Author: louradou
Date: 2007-09-01 01:31:00 +0200 (Sat, 01 Sep 2007)
New Revision: 8042

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
added an option i_output_layer (in DeepBeliefNet)
to have access to any layer by calling computeOutput(s)



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-08-31 23:30:02 UTC (rev 8041)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-08-31 23:31:00 UTC (rev 8042)
@@ -66,6 +66,7 @@
     use_classification_cost( true ),
     reconstruct_layerwise( false ),
     n_layers( 0 ),
+    i_output_layer( -1 ),
     online ( false ),
     background_gibbs_update_ratio(0),
     gibbs_chain_reinit_freq( INT_MAX ),
@@ -158,6 +159,13 @@
                   OptionBase::buildoption,
                   "The layers of units in the network (including the input layer).");
 
+    declareOption(ol, "i_output_layer", &DeepBeliefNet::i_output_layer,
+                  OptionBase::buildoption,
+                  "The index of the layers from which you want to compute output"
+                  "when there is NO final_module NEITHER final_cost."
+                  "If -1, then the outputs (with this setting) will be"
+                  "the expectations of the last layer.");
+
     declareOption(ol, "connections", &DeepBeliefNet::connections,
                   OptionBase::buildoption,
                   "The weights of the connections between the layers");
@@ -294,6 +302,9 @@
     else
         n_layers = layers.length();
 
+    if( i_output_layer < 0)
+        i_output_layer = n_layers - 1;
+
     if( !online )
     {
         if( training_schedule.length() != n_layers )
@@ -2080,7 +2091,18 @@
             layers[ n_layers-2 ]->expectation );
         layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
         layers[ n_layers-1 ]->computeExpectation();
-        output << layers[ n_layers-1 ]->expectation;
+        output << layers[ i_output_layer ]->expectation;
+
+        //! Copy of the part above: hope it makes sense
+        if (reconstruct_layerwise)
+        {
+            layer_input.resize(layers[n_layers-2]->size);
+            layer_input << layers[n_layers-2]->expectation;
+            connections[n_layers-2]->setAsUpInput(layers[n_layers-1]->expectation);
+            layers[n_layers-2]->getAllActivations(connections[n_layers-2]);
+            real rc = reconstruction_costs[n_layers-1] = layers[n_layers-2]->fpropNLL( layer_input );
+            reconstruction_costs[0] += rc;
+        }
     }
 }
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-08-31 23:30:02 UTC (rev 8041)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-08-31 23:31:00 UTC (rev 8042)
@@ -114,6 +114,10 @@
     //! The layers of units in the network
     TVec< PP<RBMLayer> > layers;
 
+    //! The index of the output layer
+    //! (when the final module is external)
+    int i_output_layer;
+
     //! The weights of the connections between the layers
     TVec< PP<RBMConnection> > connections;
 



From louradou at mail.berlios.de  Sat Sep  1 02:12:32 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 1 Sep 2007 02:12:32 +0200
Subject: [Plearn-commits] r8043 - trunk/plearn_learners/online
Message-ID: <200709010012.l810CWU7015202@sheep.berlios.de>

Author: louradou
Date: 2007-09-01 02:12:30 +0200 (Sat, 01 Sep 2007)
New Revision: 8043

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-08-31 23:31:00 UTC (rev 8042)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-01 00:12:30 UTC (rev 8043)
@@ -2097,7 +2097,7 @@
         if (reconstruct_layerwise)
         {
             layer_input.resize(layers[n_layers-2]->size);
-            layer_input << layers[n_layers-2]->expectation;
+            layer_input << layers[n_layers-2]->expectation.subVec(0,layers[n_layers-2]->expectation.length()-2);
             connections[n_layers-2]->setAsUpInput(layers[n_layers-1]->expectation);
             layers[n_layers-2]->getAllActivations(connections[n_layers-2]);
             real rc = reconstruction_costs[n_layers-1] = layers[n_layers-2]->fpropNLL( layer_input );



From louradou at mail.berlios.de  Sat Sep  1 03:35:56 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 1 Sep 2007 03:35:56 +0200
Subject: [Plearn-commits] r8044 - trunk/plearn_learners/online
Message-ID: <200709010135.l811ZuOI020342@sheep.berlios.de>

Author: louradou
Date: 2007-09-01 03:35:55 +0200 (Sat, 01 Sep 2007)
New Revision: 8044

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-01 00:12:30 UTC (rev 8043)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-01 01:35:55 UTC (rev 8044)
@@ -697,7 +697,7 @@
         out_size += final_module->output_size;
     
     if( !use_classification_cost && !final_module )
-        out_size += layers[n_layers-1]->size;
+        out_size += layers[i_output_layer]->size;
 
     return out_size;
 }
@@ -2087,7 +2087,9 @@
 
     if( !use_classification_cost && !final_module)
     {
-        connections[ n_layers-2 ]->setAsDownInput(
+        output.resize(outputsize());
+	
+	connections[ n_layers-2 ]->setAsDownInput(
             layers[ n_layers-2 ]->expectation );
         layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
         layers[ n_layers-1 ]->computeExpectation();



From nouiz at mail.berlios.de  Tue Sep  4 16:49:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Sep 2007 16:49:34 +0200
Subject: [Plearn-commits] r8045 - trunk/python_modules/plearn/learners
Message-ID: <200709041449.l84EnYE1029569@sheep.berlios.de>

Author: nouiz
Date: 2007-09-04 16:49:34 +0200 (Tue, 04 Sep 2007)
New Revision: 8045

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
better variable name


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-01 01:35:55 UTC (rev 8044)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-04 14:49:34 UTC (rev 8045)
@@ -136,8 +136,8 @@
             e=".psave"
             lene=len(e)
             tmp=[ x for x in os.listdir(filepath) if x.startswith(s) and x.endswith(".psave") ]
-            for x in tmp:
-                t=int(x[lens:-lene])
+            for file in tmp:
+                t=int(file[lens:-lene])
                 if t>stage1: stage1=t
         #We must split stage1 and stage2 as one learner can early stop.
         if stage2 == -1:



From nouiz at mail.berlios.de  Tue Sep  4 17:20:57 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Sep 2007 17:20:57 +0200
Subject: [Plearn-commits] r8046 - trunk/doc
Message-ID: <200709041520.l84FKvMV031967@sheep.berlios.de>

Author: nouiz
Date: 2007-09-04 17:20:57 +0200 (Tue, 04 Sep 2007)
New Revision: 8046

Modified:
   trunk/doc/installation_guide.tex
Log:
old amelioration


Modified: trunk/doc/installation_guide.tex
===================================================================
--- trunk/doc/installation_guide.tex	2007-09-04 14:49:34 UTC (rev 8045)
+++ trunk/doc/installation_guide.tex	2007-09-04 15:20:57 UTC (rev 8046)
@@ -61,7 +61,9 @@
 
 \parskip=2mm
 \parindent=0mm
-
+\setlength{\oddsidemargin}{0cm}
+\setlength{\evensidemargin}{0cm}
+\setlength{\textwidth}{15cm}
 \title{{\Huge PLearn Installation Guide\\ \Large How to install the PLearn Machine-Learning library and tools}}
 
 \begin{document}
@@ -348,9 +350,62 @@
 
 \chapter{Installing PLearn}
 
-The installation of PLearn consist of three phases: installing the dependensys of PLearn, configuring the environnement and we finish with the compilation of PLearn. The compilation is done with pymake witch use python. To have more information on pymake do: \verb!pymake help!.
+The installation of PLearn consist of three phases: installing the dependensys of PLearn, configuring the environnement and we finish with the compilation of PLearn. The compilation is done with pymake witch use python. To have more information on pymake do: \verb!pymake help!. 
 
-The first section discuss the installation on linux, the second on Mac OS X, the third on windows with cygwin.
+
+The next subsection discuss some other information about pymake that is cross-platform. After that their is sections that discuss the installation on linux, on Mac OS X and on windows with cygwin.
+
+\subsection{Cross-platform information about pymake}
+To have more information on pymake do: \verb!pymake help!
+
+To clean all the file generated during the compilation do \verb!pymake -clean [dir]!. The dir parameter is optinal and if it is not there, the current directory will be used.
+
+(Not sure this is cross-platform, tested on linux)
+If PLearn is on NFS(or others non local directory), you can speed up the
+recompilation when you modified somes files, with the \verb!-tmp! or the
+\verb!-local_ofiles! options. Both will put the objects files in the
+local directory \verb!/tmp/.pymake! to speed up the linking instead of in the
+PLearn directory. 
+The \verb!-tmp! options will compile all the objects files only with the local host.
+When it is executed for the first
+time on a computer, it will compile all files. The next time, it will reuse the one
+previously compiled if it didn't change. This is usefull is you have
+limited space on the PLearn directory on the NFS, as their won't be
+any objects
+file in it, but if you change of local computer, you must recompile
+everything.
+
+The \verb!-local_ofiles! option will distribute the compilation on many computer(see
+next paragraphe). When executed the first time, it will copy the
+objects files from the PLearn directory to the \verb!/tmp/.pymake!
+directory. Then it will recompiles modified files and then link
+them in this directory. Finaly it will
+copy them in the PLearn directory. This way, if you change of
+computer, you won't need to recompile everything, but it will need
+more space in the PLearn directory.
+So the only advantage of \verb!-tmp! over \verb!-local_ofiles! is that it take less
+space in the directory of PLearn, but both will link at the same speed.
+
+\verb!pymake! support the compilation on multiple computer for faster 
+compilation. The list of host is in a file in the directory ~/.pymake/. To know the name of the file run the compile command wanted. It will give you a line that look like this: \verb!(create a linux-i386.hosts file in your .pymake directory to list hosts for parallel compilation.)! In this exemple, the file is \verb!linux-i386.hosts!.
+On this file, you must put one host by line and those hosts most be of the same architecture that the one who start the compilation. If you want the computer that start the compilation to participate in the compiling, it must be included in the file.
+
+The default compilation mode is in debug mode (-dbg). To use other mode, add it as a paramater to the compilation line like this: \verb!pymake -opt plearn_curses.cc!.
+Here is the list of compilation mode:
+\begin{itemize}
+\item -dbg: debug mode (default)
+\item -opt: optimized
+\item -pintel: parallelized for intel compiler
+\item -safeopt: safe optimized mode (includes bound checking)
+\item -safeoptdbg: safe optimized mode (includes bound checking), w/ debug info
+\item -checkopt: some variation on optimized mode
+\item -gprof: optimized mode with profiler support (-pg)
+\item -optdbggprof: optimized mode with profiler support WITH DEBUGGING (-pg)
+\item -safegprof: safe optimized mode with profiler support (-pg)',
+\item -genericvc++: 'Generic compilation options for Visual C++: the debug/opt options are actually set directly in the .vcproj project file',
+\end{itemize}
+
+
 \section{Installation on Linux}
 
 \subsection{PLearn setup and compilation}
@@ -412,29 +467,6 @@
 
 If it does work, you can try with more dependencies to have more fonctionality with the commands \verb!pymake plearn_lapack.cc!, \verb!pymake plearn\_curses.cc! or even \verb!pymake plearn_python.cc!
 
-\subsection{Parallel compilation}
-
-\verb!pymake! support the compilation on multiple computer for faster 
-compilation. The list of host is in a file in the directory ~/.pymake/. To know the name of the file run the compile command wanted. It will give you a line that look like this: \verb!(create a linux-i386.hosts file in your .pymake directory to list hosts for parallel compilation.)! In this exemple, the file is \verb!linux-i386.hosts!.
-On this file, you must put one host by line and those hosts most be of the same architecture that the one who start the compilation. If you want the computer that start the compilation to participate in the compiling, it must be included in the file.
-
-\subsection{Compilation Mode}
-
-The default compilation mode is in debug mode. To use other mode, add it as a paramater to the compilation line like this: \verb!pymake -opt plearn_curses.cc!.
-Here is the list of compilation mode:
-\begin{itemize}
-\item dbg: debug mode (default)
-\item -opt: optimized
-\item -pintel: parallelized for intel compiler
-\item -safeopt: safe optimized mode (includes bound checking)
-\item safeoptdbg: safe optimized mode (includes bound checking), w/ debug info
-\item -checkopt: some variation on optimized mode
-\item -gprof: optimized mode with profiler support (-pg)
-\item -optdbggprof: optimized mode with profiler support WITH DEBUGGING (-pg)
-\item -safegprof: safe optimized mode with profiler support (-pg)',
-\item -genericvc++: 'Generic compilation options for Visual C++: the debug/opt options are actually set directly in the .vcproj project file',
-\end{itemize}
-
 \section{Installation on Mac OS X}
 
 \subsection{External dependencies}
@@ -516,9 +548,7 @@
 with the commands \verb!pymake plearn_lapack.cc!, \verb!pymake plearn_curses.cc!\
  or even \verb!pymake plearn_python.cc!
 
-To clean all the file generated during the compilation do \verb!pymake -clean!.
 
-If you modify one file and want to recompile, to speed up the linking, you can use the \verb!-tmp! option. This options will use only the localhost for compilation and move file in the /tmp folder. This will speed up compilation when the file are on the network.
 \section{Installation on Windows with cygwin}
 \label{sec:windows}
 
@@ -758,6 +788,7 @@
  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
 \end{verbatim}
 
 \end{document}



From nouiz at mail.berlios.de  Tue Sep  4 17:22:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 4 Sep 2007 17:22:23 +0200
Subject: [Plearn-commits] r8047 - trunk/scripts
Message-ID: <200709041522.l84FMNuC032099@sheep.berlios.de>

Author: nouiz
Date: 2007-09-04 17:22:23 +0200 (Tue, 04 Sep 2007)
New Revision: 8047

Modified:
   trunk/scripts/cdispatch
Log:
bugfix if the command are in a file


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-04 15:20:57 UTC (rev 8046)
+++ trunk/scripts/cdispatch	2007-09-04 15:22:23 UTC (rev 8047)
@@ -159,10 +159,11 @@
 #generate the command
 if FILE != "":
     FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
+    commands=[]
     for line in FD.readlines():
         line = line.rstrip()
 	sp = line.split(" ")
-        commands=generate_commands(sp)
+        commands+=generate_commands(sp)
     FD.close
 else:
     commands=generate_commands(otherargs)
@@ -174,15 +175,18 @@
 else:
     launch_cmd='Condor'
 
-    
-t = [x for x in sys.argv[1:] if not x[:2]=="--"]
-t[0]=os.path.split(t[0])[1]
-tmp="_".join(t)
-tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
-tmp+=str(datetime.datetime.now()).replace(' ','_')
-print "tmp:",tmp
-dbi_param["log_dir"]=os.path.join("LOGS",tmp)
-dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
+if FILE == "":    
+    t = [x for x in sys.argv[1:] if not x[:2]=="--"]
+    t[0]=os.path.split(t[0])[1]
+    tmp="_".join(t)
+    tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
+    tmp+=str(datetime.datetime.now()).replace(' ','_')
+    print "tmp:",tmp
+    dbi_param["log_dir"]=os.path.join("LOGS",tmp)
+    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
+else:
+    dbi_param["log_dir"]=os.path.join("LOGS",FILE)
+    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
 if "test" in dbi_param:
     print "We generated %s command in the file"% len(commands)



From simonl at mail.berlios.de  Wed Sep  5 22:33:03 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Wed, 5 Sep 2007 22:33:03 +0200
Subject: [Plearn-commits] r8048 - trunk/scripts/EXPERIMENTAL
Message-ID: <200709052033.l85KX3Q1015074@sheep.berlios.de>

Author: simonl
Date: 2007-09-05 22:33:03 +0200 (Wed, 05 Sep 2007)
New Revision: 8048

Modified:
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
some improvements ?


Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-09-04 15:22:23 UTC (rev 8047)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-09-05 20:33:03 UTC (rev 8048)
@@ -53,6 +53,7 @@
     print 'o : set the current hidden layer to its original state'
     print 'O : same as o but for every layer'
     print 't : now we have the same scale for  W, C'
+    print 'h : change the function that converts layers with 2 times the number of units of the input to layers with the same number of units of the input'
     print 'right arrow : prints the next character in the dataset and its corresponding hidden layers and reconstructions'
     print 'left arrow :same but for previous character'
     print '0,1,2,3...9 : after having pressed a digit, right and left arrows will only find this digit'
@@ -61,6 +62,7 @@
 
 
 def appendMatrixToFile(file, matrix, matrix_name=""):
+    '''output a matrix into a file'''
     file.write("\n\n" + matrix_name + ' ('+ str(len(matrix)) + 'x' + str(len(matrix[0])) + ')\n\n')
     for i, row in enumerate(matrix):
         file.write('[')
@@ -93,7 +95,7 @@
     def getMatrix(self):        
         return reshape(self.hidden_layer, (-1,self.groupsize*self.nbgroups))
     
-    def matrixToLayer(self, x, y):
+    def matrixToLayer(self, x, y):        
         gs = self.groupsize
         x,y = self.correctXY(x,y)
         return x + gs*self.nbgroups*y        
@@ -149,6 +151,8 @@
         self.__linkEvents()
 
         self.same_scale = True
+        self.from1568to784function = 0
+        self.from1568to784functions = [softmaxGroup2, toMinusRow, toPlusRow, evenMinusOdd]
 
 
     def size(self):
@@ -551,7 +555,7 @@
                     figure(3)
                     ioff()                    
                     clf()
-                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.05, softmaxGroup2, [], names, self.same_scale)
+                    plotLayer1(learner.getParameterValue(nameW), 28, .1, n, hl.groupsize,.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
                     ion()
                     draw()
 
@@ -568,7 +572,7 @@
                     figure(3)
                     ioff()
                     clf()
-                    plotLayer1(M, 28, .056,0,M.shape[0],.05, softmaxGroup2, [], names, self.same_scale)
+                    plotLayer1(M, 28, .056,0,M.shape[0],.05, self.from1568to784functions[self.from1568to784function], [], names, self.same_scale)
                     ion()
                     draw()
 
@@ -608,7 +612,7 @@
                     figure(3)
                     ioff()                    
                     clf()
-                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.05, softmaxGroup2, indexes,names, self.same_scale)
+                    plotLayer1(learner.getParameterValue(nameW), 28, .056, 0,0,.05, self.from1568to784functions[self.from1568to784function], indexes,names, self.same_scale)
                     ion()
                     draw()
                     
@@ -627,14 +631,14 @@
                         M[y] =  m[y]*w[x%hl.groupsize]
                     print M
                     
-                    plotLayer1(M, 28, .056,0,M.shape[0],.05,softmaxGroup2,[],names, self.same_scale)
+                    plotLayer1(M, 28, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
                     ion()
                     draw()
 
                     figure(4)
                     ioff()
                     clf()                    
-                    plotLayer1(M, 28, .056,0,M.shape[0],.05,softmaxGroup2,[],names, self.same_scale)
+                    plotLayer1(M, 28, .056,0,M.shape[0],.05,self.from1568to784functions[self.from1568to784function],[],names, self.same_scale)
                     ion()
                     draw()
 
@@ -643,7 +647,7 @@
                     figure(5)
                     ioff()
                     clf()
-                    plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.05,softmaxGroup2,indexes,names, self.same_scale)
+                    plotLayer1(learner.getParameterValue(nameWr), 28, .056,0,0,.05,self.from1568to784functions[self.from1568to784function],indexes,names, self.same_scale)
                     ion()
                     draw()
                     
@@ -658,10 +662,19 @@
                 draw()
                 print '...done'
 
-            else :
+            elif char != 'shift' and char != 'control':
+                print char
                 print_usage_repAndRec()
 
+        # change the "hack" function -- h
 
+        if char == 'h':
+            self.from1568to784function = (self.from1568to784function + 1 ) % len(self.from1568to784functions)
+            print '1568 to 784 function is now', self.from1568to784functions[self.from1568to784function]
+            
+            
+
+
         # toggle "same scale" or "individuals scales" for  'W' and 'C' -- t
         
         if char == 't':



From simonl at mail.berlios.de  Wed Sep  5 22:36:40 2007
From: simonl at mail.berlios.de (simonl at BerliOS)
Date: Wed, 5 Sep 2007 22:36:40 +0200
Subject: [Plearn-commits] r8049 - trunk/plearn/var
Message-ID: <200709052036.l85Kaert015221@sheep.berlios.de>

Author: simonl
Date: 2007-09-05 22:36:39 +0200 (Wed, 05 Sep 2007)
New Revision: 8049

Modified:
   trunk/plearn/var/Variable.cc
Log:
Added max_value and min_value as build options for variable.


Modified: trunk/plearn/var/Variable.cc
===================================================================
--- trunk/plearn/var/Variable.cc	2007-09-05 20:33:03 UTC (rev 8048)
+++ trunk/plearn/var/Variable.cc	2007-09-05 20:36:39 UTC (rev 8049)
@@ -239,6 +239,12 @@
     declareOption(ol, "value", &Variable::matValue, OptionBase::learntoption, 
                   "Current value of the variable\n");
 
+    declareOption(ol, "min_value", &Variable::min_value, OptionBase::buildoption, 
+                  "minimum value of the variable\n");
+
+    declareOption(ol, "max_value", &Variable::max_value, OptionBase::buildoption, 
+                  "maximum value of the variable\n");
+
     /*
       declareOption(ol, "gradient", &Variable::matGradient, OptionBase::learntoption, 
       "Current gradient of the variable\n");



From nouiz at mail.berlios.de  Thu Sep  6 17:20:45 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Sep 2007 17:20:45 +0200
Subject: [Plearn-commits] r8050 - trunk/scripts
Message-ID: <200709061520.l86FKjuN023956@sheep.berlios.de>

Author: nouiz
Date: 2007-09-06 17:20:45 +0200 (Thu, 06 Sep 2007)
New Revision: 8050

Modified:
   trunk/scripts/collectres
Log:
bugfix and print an error if no result is selected


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-09-05 20:36:39 UTC (rev 8049)
+++ trunk/scripts/collectres	2007-09-06 15:20:45 UTC (rev 8050)
@@ -106,6 +106,7 @@
       col = get_col_index(a,loc_specs[i])
       if col<0:
         b = array(range(minrow,maxrow))
+        b.resize(res.shape[0],1)
       else:
         b=a[minrow:maxrow,col].copy()
         la = a.length
@@ -176,6 +177,9 @@
   return distinct_names
 
 def outputres(f,mode,speclist,results):
+  if not results:
+    print "ERROR: no results selected!"
+    sys.exit(0)
   if mode=="min":
     minval = 1e36
     minfile = ""



From nouiz at mail.berlios.de  Thu Sep  6 17:23:08 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Sep 2007 17:23:08 +0200
Subject: [Plearn-commits] r8051 - trunk/plearn_learners/regressors
Message-ID: <200709061523.l86FN89K024115@sheep.berlios.de>

Author: nouiz
Date: 2007-09-06 17:23:08 +0200 (Thu, 06 Sep 2007)
New Revision: 8051

Modified:
   trunk/plearn_learners/regressors/LocalMedBoost.cc
Log:
remove compiler warning and a small optimisation


Modified: trunk/plearn_learners/regressors/LocalMedBoost.cc
===================================================================
--- trunk/plearn_learners/regressors/LocalMedBoost.cc	2007-09-06 15:20:45 UTC (rev 8050)
+++ trunk/plearn_learners/regressors/LocalMedBoost.cc	2007-09-06 15:23:08 UTC (rev 8051)
@@ -256,22 +256,19 @@
     edge = 0.0;
     capacity_too_large = true;
     capacity_too_small = true;
-    real mini_base_award; 
+    real mini_base_award = INT_MAX;
+    int sample_costs_index;
+    if (objective_function == "l1") sample_costs_index=3;
+    else sample_costs_index=2;
+
     for (each_train_sample_index = 0; each_train_sample_index < length; each_train_sample_index++)
     {
         train_set->getExample(each_train_sample_index, sample_input, sample_target, sample_weight);
         base_regressors[stage]->computeOutputAndCosts(sample_input, sample_target, sample_output, sample_costs);
-        if (objective_function == "l1")
-        {
-            base_rewards[each_train_sample_index] = sample_costs[3];
-        }
-        else
-        {
-            base_rewards[each_train_sample_index] = sample_costs[2];
-        }
+        base_rewards[each_train_sample_index] = sample_costs[sample_costs_index];
+
         base_confidences[each_train_sample_index] = sample_costs[1];
         base_awards[each_train_sample_index] = base_rewards[each_train_sample_index] * base_confidences[each_train_sample_index];
-        if (each_train_sample_index == 0) mini_base_award = base_awards[each_train_sample_index];
         if (base_awards[each_train_sample_index] < mini_base_award) mini_base_award = base_awards[each_train_sample_index];
         edge += sample_weight * base_awards[each_train_sample_index];
         if (base_awards[each_train_sample_index] < robustness) capacity_too_large = false;



From lysiane at mail.berlios.de  Thu Sep  6 19:16:45 2007
From: lysiane at mail.berlios.de (lysiane at BerliOS)
Date: Thu, 6 Sep 2007 19:16:45 +0200
Subject: [Plearn-commits] r8052 -
	trunk/plearn_learners/distributions/EXPERIMENTAL
Message-ID: <200709061716.l86HGj5t025518@sheep.berlios.de>

Author: lysiane
Date: 2007-09-06 19:16:42 +0200 (Thu, 06 Sep 2007)
New Revision: 8052

Modified:
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
   trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
Log:
for LISA MEETING, 7/09/2007


Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-09-06 15:23:08 UTC (rev 8051)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.cc	2007-09-06 17:16:42 UTC (rev 8052)
@@ -848,12 +848,13 @@
     mstd_v.resize(inputSpaceDim);
     mstd_target.resize(inputSpaceDim);
     mstd_neighbor.resize(inputSpaceDim);
-
+    mstd_pivots.resize(inputSpaceDim);
+    
     //put more emphasis on diversity among transformation?
     if(emphasisOnDiversity){
         PLASSERT(!withBias);
         if(diversityFactor<=0){
-            diversityFactor = (nbTransforms - 1)*1.0/transformsVariance;  
+            diversityFactor = 1.0/transformsVariance;  
         }
     }
     else{
@@ -2266,10 +2267,12 @@
             mstd_D += transforms[t];
         }
     }
-    mstd_D *= -2*diversityFactor;
+    mstd_D *= -2*diversityFactor*noiseVariance;
    
 
-    real lambda = 1.0*noiseVariance*(1.0/transformsVariance -2*(nbTransforms - 1)*diversityFactor);
+    //real lambda = noiseVariance*(1.0/transformsVariance -2*(nbTransforms - 1)*diversityFactor);
+    real lambda = noiseVariance/transformsVariance ;
+    
     for(int idx=0 ; idx<nbReconstructions ; idx++){
         
         //catch a view on the next entry of our dataset, that is, a  triple:
@@ -2303,7 +2306,7 @@
     addToDiagonal(mstd_C,lambda);
     //transforms[t] << solveLinearSystem(C[t], B[t]); 
     mstd_B += mstd_D;
-    lapackSolveLinearSystem(mstd_C,mstd_B, mst_pivots);
+    lapackSolveLinearSystem(mstd_C,mstd_B, mstd_pivots);
     transforms[transformIdx] << mstd_B;
     
 }
@@ -2311,6 +2314,9 @@
 
 
 
+
+
+
 //!maximization step with respect to transformation bias
 //!(MAP version)
 void TransformationLearner::MStepBias(){

Modified: trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h
===================================================================
--- trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-09-06 15:23:08 UTC (rev 8051)
+++ trunk/plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h	2007-09-06 17:16:42 UTC (rev 8052)
@@ -676,6 +676,7 @@
     mutable Vec mstd_v;
     mutable Vec mstd_target;
     mutable Vec mstd_neighbor;
+    mutable TVec<int> mstd_pivots;
 
 protected:
     //#####  Protected Member Functions  ######################################



From louradou at mail.berlios.de  Thu Sep  6 22:06:04 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 6 Sep 2007 22:06:04 +0200
Subject: [Plearn-commits] r8053 - in
	trunk/python_modules/plearn/learners/modulelearners: .
	sampler sampler/example sampler/example/data
Message-ID: <200709062006.l86K649f024925@sheep.berlios.de>

Author: louradou
Date: 2007-09-06 22:06:03 +0200 (Thu, 06 Sep 2007)
New Revision: 8053

Removed:
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
Modified:
   trunk/python_modules/plearn/learners/modulelearners/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py
   trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py
Log:


Modified: trunk/python_modules/plearn/learners/modulelearners/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/__init__.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -4,8 +4,8 @@
 plearn.bridgemode.useserver= False
 from plearn.bridge import *
 #from plearn.pyplearn import *
+from plearn.learners.modulelearners.pyplearn_read import *
 
-
 tmp_file='/tmp/modulelearner.py'
 
 if plearn.bridgemode.useserver:
@@ -41,7 +41,6 @@
     extension = os.path.splitext(filename)[1]
     
     if extension == '.pyplearn':
-       from plearn.learners.modulelearners.pyplearn_read import *
        object_dict = read_objects( filename, ['HyperLearner', 'PTester', 'MemoryVMatrix', 'AutoVMatrix'] , tmp_file)
        execfile(tmp_file)
        modules=[]

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/__init__.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -1,7 +1,7 @@
 from pygame import *
 from numarray import *
 import math
-import sys
+import sys, os
 
 EXITCODE = -2
 NEXTCODE = -1
@@ -34,6 +34,40 @@
        raise TypeError, "the first argument of sampler::init_screen() must be of type <int>, <tuple> or <list> (not "+str(type(input_size))+")"
     return display.set_mode([height, width])
 
+def draw_and_save_image(values_in_01,screen,zoom_factor,filename):
+    """ Draw a 2D image where the gray level corresponds to a value scaled in [0,1]
+        (a warning is given when at least one of the value does not lie in the interval)
+        - values_in_01 : list of values in [0,1]
+        - screen  : output of init_screen()
+	- zoom_factor : int > 0
+    """
+    GiveWarning=True
+    
+    width = screen.get_width()
+    height = screen.get_height()
+
+    surface = Surface((height, width),0,8)
+    surface.set_palette([(i,i,i) for i in range(2**8)])
+    for x in range(width/zoom_factor):
+       for y in range(height/zoom_factor):
+           if 'numarray' in str(type(values_in_01)) and len(values_in_01.shape)==2:
+              value = values_in_01[x,y]
+	   else:
+              value = values_in_01[x*width/zoom_factor+y]
+	   if value < 0. or value > 1.:
+	      if GiveWarning:
+	         GiveWarning=False
+	         print "Warning: In draw image : value "+str(value)+" is not in [0,1]"
+	      value = min(max(0.,value),1.)
+           graycol = int(255.0*value)
+           for i in range(zoom_factor):
+               for j in range(zoom_factor):
+                   surface.set_at((y*zoom_factor+i,x*zoom_factor+j),(graycol,graycol,graycol,255))
+    screen.blit(surface, (0,0))
+    display.update()
+    os.system('import -window "pygame window" ' + filename )
+
+
 def draw_image(values_in_01,screen,zoom_factor):
     """ Draw a 2D image where the gray level corresponds to a value scaled in [0,1]
         (a warning is given when at least one of the value does not lie in the interval)
@@ -46,7 +80,7 @@
     width = screen.get_width()
     height = screen.get_height()
 
-    surface = Surface((width, width),0,8)
+    surface = Surface((height, width),0,8)
     surface.set_palette([(i,i,i) for i in range(2**8)])
     for x in range(width/zoom_factor):
        for y in range(height/zoom_factor):
@@ -62,7 +96,7 @@
            graycol = int(255.0*value)
            for i in range(zoom_factor):
                for j in range(zoom_factor):
-                   surface.set_at((x*zoom_factor+i,y*zoom_factor+j),(graycol,graycol,graycol,255))
+                   surface.set_at((y*zoom_factor+i,x*zoom_factor+j),(graycol,graycol,graycol,255))
     screen.blit(surface, (0,0))
     display.update()
     return pause()

Deleted: trunk/python_modules/plearn/learners/modulelearners/sampler/example/data/DBN-3RBM.babyAI-1obj.psave
===================================================================
(Binary files differ)

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/example/do_sampling.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -13,11 +13,21 @@
 default_DIRECTORY = os.path.join(PLEARNDIR,'python_modules','plearn','learners','modulelearners','sampler','example')
 
 learner_filename = default_DIRECTORY + '/data/DBN-3RBM.babyAI-1obj.psave'
+#learner_filename = '/u/louradoj/PRGM/babyAI/convolution/expes/models/UNSUP_dbn-1RBMimage-handinit-conv_BABYAI_gray_1250000x1obj_32x32.color-size-location-shape.train.3gram01_slope1.0_N1-8-7_LRs1e-05_0.1_ns1250000_ng1/init_learner.psave'
+
+
+#plarg_defaults.width         = 32
+#plarg_defaults.imageSize     = int(plargs.width)**2
+#plarg_defaults.data_filename = default_DIRECTORY + '/data/babyAI-1obj.dmat'
+
+#width                    = plargs.width
+#imageSize                = plargs.imageSize
+#data_filename            = plargs.data_filename
+
 data_filename = default_DIRECTORY + '/data/babyAI-1obj.dmat'
-width = 32
+width         = 32
 imageSize = width*width
 
-
 if len(sys.argv)>=2:
   learner_filename = sys.argv[1]
 if len(sys.argv)>=3:
@@ -68,17 +78,25 @@
       print "(to quit, type 'q' or 'Q')\n"
       c = pause()
 
+   # NOT to save the images...
+   #
+   save_dir=None
+   #   
+   # to save the images...
+   #
+   save_dir=None
+   save_dir='~/PRGM/babyAI/pres/'+os.path.basename(os.path.dirname(learner_filename))
 
    if c == 1:
-      view_sample_from_visible(learner, imageSize, dataSet, 1)
-#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_visible.py '+' '.join([ learner_filename, str(Size*Size), data_filename, 'gibbs_step='+str(gibbs_step) ]))
+      view_sample_from_visible(learner, imageSize, dataSet, 1, save_dir)
+#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_visible.py '+' '.join([ learner_filename, str(imageSize), data_filename, 'gibbs_step=1' ]))
    elif c == 2:
       view_sample_from_hidden(learner, imageSize, 1)
-#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(Size*Size), 'gibbs_step='+str(gibbs_step) ]))
+#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/sample_from_hidden.py '+' '.join([ learner_filename, str(imageSize), 'gibbs_step=1' ]))
    elif c == 3:
       view_reconstruct( learner, imageSize , dataSet)
-#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/reconstruct.py '+' '.join([ learner_filename, str(Size*Size), data_filename]))
+#      os.system('python '+os.path.dirname(os.path.abspath(sys.argv[0]))+'/reconstruct.py '+' '.join([ learner_filename, str(imageSize), data_filename]))
    elif c == 4:
-      view_inputweights(learner, imageSize)
+      view_inputweights(learner, imageSize, save_dir)
    elif c == EXITCODE:
       break

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/inputweights.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -6,7 +6,24 @@
 import sys, os.path
 
 
-def view_inputweights(learner, Nim):
+def view_inputweights(learner, Nim, save_dir):
+
+  save_image=False
+  if save_dir<>None and len(save_dir)<>0:
+    print "\nDo you want to save learner in the directory "+save_dir+"?"
+    print "1.[default] No"
+    print "2. Yes"
+    c = pause()
+    while c not in [0,1,2,EXITCODE]:
+          c = pause()
+    if c==2:
+       save_image=True
+    elif c==EXITCODE:
+       return
+       
+  if save_image:
+     print "\nChecking/creating directory "+save_dir+"\n"
+     os.system('mkdir -p '+save_dir)
   
   inputweights_man()
   #
@@ -29,6 +46,9 @@
         weights=image_RBM.connection.weights[i]
         print str(i+1)+"/"+str(len(image_RBM.connection.weights))
         c = draw_normalized_image( weights, screen, zoom_factor )
+	if save_image:
+	   fname = save_dir+'/filters-%05d.jpg' % i
+	   os.system('import -window "pygame window" ' + fname )
         if c==EXITCODE:
            return
        
@@ -39,6 +59,26 @@
     size_filter = image_RBM.connection.sub_connections[0][0].kernel.shape
     zoom_factor **= 2
 
+#
+# to complete.... see all the weights at the same time
+#
+#    N=math.ceil(math.sqrt(N_filter))
+#    print N
+#    print (size_filter[0]*N_inputim+(N_inputim-1))*N_filter
+#    print size_filter[1]*N_filter
+#    print zoom_factor
+#    return
+#    screen=init_screen( (size_filter[0]*N_inputim*N , size_filter[1]*N) , zoom_factor)
+#    for i in range(N_filter):
+#        X = math.fmod(N_filter,i)
+#	weights = image_RBM.connection.sub_connections[i][0].kernel
+#        print str(i+1)+"/"+str(N_filter)
+#        for j in range(1,N_inputim):
+#	   weights.resize( size_filter[0]*(j+1)*(i+1)+j*(i+1), size_filter[1]*(i+1) )
+#	   weights[size_filter[0]*j*i+1:]=image_RBM.connection.sub_connections[i][j].kernel
+#    draw_normalized_image( weights, screen, zoom_factor )
+#    return
+
     screen=init_screen( (size_filter[0]*N_inputim+(N_inputim-1) , size_filter[1]) , zoom_factor)
     for i in range(N_filter):
 	weights = image_RBM.connection.sub_connections[i][0].kernel
@@ -48,6 +88,11 @@
 	   weights[size_filter[0]*j]=[0]*size_filter[1]
 	   weights[size_filter[0]*j+1:]=image_RBM.connection.sub_connections[i][j].kernel
         c = draw_normalized_image( weights, screen, zoom_factor )
+	
+	if save_image:
+	   fname = save_dir+'/filters-%05d.jpg' % i
+	   os.system('import -window "pygame window" ' + fname )
+	
         if c==EXITCODE:
            return
 	   

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/reconstruct.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -112,7 +112,11 @@
        if module.name == top_RBM_name:
           top_RBM = RBMmodel.module.modules[i]
 
+  RBMmodelInit.build()
+  RBMmodel.build()
+
   reconstruct_man()
+
   while True:
  
    random_index=random.randint(0,dataSet.length)
@@ -155,7 +159,7 @@
   if 'HyperLearner' in str(type(learner)):
      learner=learner.learner
   
-  if os.path.isfile(data_filename) == False:
+  if os.path.isfile(data_filename) == False and os.path.isdir(data_filename) == False:
      raise TypeError, "Cannot find file "+data_filename
   print " loading... "+data_filename
   dataSet = pl.AutoVMatrix( specification = data_filename )

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_hidden.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -123,7 +123,10 @@
           RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = init_gibbs_step
           top_RBM = RBMmodel.module.modules[i]
 
+  RBMmodel.build()
+
   sample_from_hidden_man()
+  
   while True:
  
    init_hidden=[random.randint(0,1) for i in range(NH)]

Modified: trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py
===================================================================
--- trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py	2007-09-06 17:16:42 UTC (rev 8052)
+++ trunk/python_modules/plearn/learners/modulelearners/sampler/sample_from_visible.py	2007-09-06 20:06:03 UTC (rev 8053)
@@ -1,13 +1,15 @@
 from plearn.learners.modulelearners import *
+import os
 
 zoom_factor = 5
 from plearn.learners.modulelearners.sampler import *
 
 import random
 
-def view_sample_from_visible(learner, Nim, dataSet, init_gibbs_step):
+def view_sample_from_visible(learner, Nim, dataSet, init_gibbs_step, save_dir):
 
-  print "analyzing learner..."
+  
+  print "\nAnalyzing learner..."
   #
   # Getting the RBMmodule which sees the image (looking at size of the down layer)
   #
@@ -19,18 +21,18 @@
         nRBM += 1
         if module.connection.down_size == Nim:
            image_RBM=learner.module.modules[i]
-           break
   image_RBM_name=image_RBM.name
   #
   # Getting the top RBMmodule
   #
-
   top_RBM = getTopRBMModule( learner )
   top_RBM_name = top_RBM.name
   
   NH=top_RBM.connection.up_size
 
-  if nRBM == 1: MeanField=False
+
+  if nRBM == 1:
+     MeanField=False
   else:
     print "\nChoose betweem these options:"
     print "1.[default] Gibbs sampling in the top RBM + mean field"
@@ -38,26 +40,51 @@
     c = pause()
     while c not in [0,1,2,EXITCODE]:
         c = pause()
-    MeanField = False
-    if c==1:
-       MeanField = True
+    MeanField = True
+    if c==2:
+       MeanField = False
     elif c==EXITCODE:
        return
+
+  save_image=False
+  if save_dir<>None and len(save_dir)<>0:
+    print "\nDo you want to save learner in the directory "+save_dir+"?"
+    print "1.[default] No"
+    print "2. Yes"
+    c = pause()
+    while c not in [0,1,2,EXITCODE]:
+          c = pause()
+    if c==2:
+       save_image=True
+    elif c==EXITCODE:
+       return
+       
+  if save_image:
+     N_IMAGE_MAX=5
+     N_GIBBS_MAX=800
+     print "\nChecking/creating directory "+save_dir+"\n"
+     os.system('mkdir -p '+save_dir)
+  else:
+     N_IMAGE_MAX=dataSet.length
+     N_GIBBS_MAX=100000
+     
+
+# Constructing the network to sample
   
   if MeanField:
      init_ports = [ ('input',  image_RBM_name+'.visible'),
                     ('output', top_RBM_name+'.hidden.state')
                   ]
-     ports = [ ('input', top_RBM_name+'.hidden_sample' ),
+     ports = [ ('input', top_RBM_name+'.hidden_sample'),
                ('output', image_RBM_name+'.visible_reconstruction.state')
              ]
   else:	
-     init_ports = [ ('input',  image_RBM_name+'.visible'),
+     init_ports = [ ('input',  image_RBM_name+'.visible_sample'),
                     ('output', top_RBM_name+'.hidden_sample')
                   ]
      ports = [ ('input', top_RBM_name+'.hidden_sample' ),
                ('output', image_RBM_name+'.visible_expectation')
-             ]
+             ]     
 
   #
   # Removing useless connections for sampling
@@ -67,8 +94,8 @@
   connections_list_down=[]
   connections_list_up=[]
   for connection in old_connections_list:
-      source_module = getModule( learner, port2moduleName( connection.source ))
-      dest_module   = getModule( learner, port2moduleName( connection.destination ))
+      source_module = getModule( learner, port2moduleName( connection.source ) )
+      dest_module   = getModule( learner, port2moduleName( connection.destination ) )
       if isModule( source_module, 'RBM') and isModule( dest_module,'RBM'):
          if MeanField:
             connections_list_up.append ( pl.NetworkConnection(source = port2moduleName( connection.source )+'.hidden.state',
@@ -150,39 +177,69 @@
           RBMmodel.module.modules[i].n_Gibbs_steps_per_generated_sample = init_gibbs_step
           top_RBM = RBMmodel.module.modules[i]
 
+  RBMmodelInit.build()
+  RBMmodel.build()
+  
   sample_from_visible_man()
-  while True:
+  
+  i_image = 0
+  while i_image < N_IMAGE_MAX:
 
       random_index=random.randint(0,dataSet.length)
       init_image=[dataSet.getRow(random_index)[i] for i in range(Nim)]
       
-      c = draw_image( init_image, screen, zoom_factor )
-      if c==NEXTCODE:
-         continue
-      elif c==EXITCODE:
-         return
-      elif c>0:
-         top_RBM.n_Gibbs_steps_per_generated_sample = c
+      i_image+=1
+      iteration = 0
+      if save_image:
+         fname = save_dir+'/sample_from_visible%02d-%05d.jpg' % (i_image,iteration)
+         draw_and_save_image( init_image , screen, zoom_factor, fname )
+      else:
+         c = draw_image( init_image, screen, zoom_factor )
+         if c==NEXTCODE:
+            continue
+         elif c==EXITCODE:
+            break
+         elif c>0:
+            top_RBM.n_Gibbs_steps_per_generated_sample = c
       
       init_hidden = RBMmodelInit.computeOutput(init_image)
-      c = draw_image( RBMmodel.computeOutput(init_hidden), screen, zoom_factor )
-      if c==NEXTCODE:
-         continue
-      elif c==EXITCODE:
-         return
-      elif c>0:
-         top_RBM.n_Gibbs_steps_per_generated_sample = c
+      sampled_image = RBMmodel.computeOutput(init_hidden)
+      
+      iteration += top_RBM.n_Gibbs_steps_per_generated_sample
+      if save_image:
+         fname = save_dir+'/sample_from_visible%02d-%05d.jpg' % (i_image,iteration)
+         draw_and_save_image( sampled_image , screen, zoom_factor, fname )
+	 top_RBM.n_Gibbs_steps_per_generated_sample=2
+      else:
+         c = draw_image( sampled_image, screen, zoom_factor )
+         if c==NEXTCODE:
+            continue
+         elif c==EXITCODE:
+            break
+         elif c>0:
+            top_RBM.n_Gibbs_steps_per_generated_sample = c
 
 
-      while True:
-          c = draw_image( RBMmodel.computeOutput([]) , screen, zoom_factor )
-          if c==NEXTCODE:
-               break
-          elif c==EXITCODE:
-               return
-          elif c>0:
-             top_RBM.n_Gibbs_steps_per_generated_sample = c
-    
+      while iteration < N_GIBBS_MAX:
+	  iteration += top_RBM.n_Gibbs_steps_per_generated_sample
+          if save_image:
+	     fname = save_dir+'/sample_from_visible%02d-%05d.jpg' % (i_image,iteration)
+             draw_and_save_image( RBMmodel.computeOutput([]) , screen, zoom_factor, fname )
+          else:
+             c = draw_image( RBMmodel.computeOutput([]) , screen, zoom_factor )
+	     if c==NEXTCODE:
+                break
+             elif c==EXITCODE:
+                break
+             elif c>0:
+                top_RBM.n_Gibbs_steps_per_generated_sample = c
+		
+      if c==EXITCODE:
+         break
+
+#  if save_image:
+#     os.system('mencoder ')
+	 
 def sample_from_visible_man():
      print "\nPlease type:"
      print ":    <ENTER>   : to continue Gibbs Sampling (same gibbs step)"
@@ -215,9 +272,9 @@
   if 'HyperLearner' in str(type(learner)):
      learner=learner.learner
   
-  if os.path.isfile(data_filename) == False:
+  if os.path.isfile(data_filename) == False and os.path.isdir(data_filename) == False:
      raise TypeError, "Cannot find file "+data_filename
   print " loading... "+data_filename
   dataSet = pl.AutoVMatrix( specification = data_filename )
 
-  view_sample_from_visible(learner, Nim, dataSet)
+  view_sample_from_visible(learner, Nim, dataSet, init_gibbs_step, None)



From nouiz at mail.berlios.de  Thu Sep  6 22:50:10 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 6 Sep 2007 22:50:10 +0200
Subject: [Plearn-commits] r8054 - trunk/python_modules/plearn/parallel
Message-ID: <200709062050.l86KoAoT028356@sheep.berlios.de>

Author: nouiz
Date: 2007-09-06 22:50:10 +0200 (Thu, 06 Sep 2007)
New Revision: 8054

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
..


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-06 20:06:03 UTC (rev 8053)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-06 20:50:10 UTC (rev 8054)
@@ -201,7 +201,7 @@
             self.unique_id = get_new_sid('')#compation intense
             self.log_file = truncate( os.path.join(log_dir, self.unique_id +'_'+ formatted_command), 200) + ".log"
         else:
-            self.unique_id = formatted_command+str(datetime.datetime.now()).replace(' ','_')
+            self.unique_id = formatted_command+'_'+str(datetime.datetime.now()).replace(' ','_')
             self.log_file = os.path.join(log_dir, self.unique_id) + ".log"
 
         if self.add_unique_id:



From plearner at mail.berlios.de  Fri Sep  7 23:14:34 2007
From: plearner at mail.berlios.de (plearner at BerliOS)
Date: Fri, 7 Sep 2007 23:14:34 +0200
Subject: [Plearn-commits] r8055 - in trunk: commands commands/EXPERIMENTAL
	doc scripts/EXPERIMENTAL
Message-ID: <200709072114.l87LEY6K007407@sheep.berlios.de>

Author: plearner
Date: 2007-09-07 23:14:30 +0200 (Fri, 07 Sep 2007)
New Revision: 8055

Added:
   trunk/commands/EXPERIMENTAL/
   trunk/commands/EXPERIMENTAL/plearn_exp.cc
   trunk/scripts/EXPERIMENTAL/TLTester.py
   trunk/scripts/EXPERIMENTAL/generators.py
   trunk/scripts/EXPERIMENTAL/iTraining.py
   trunk/scripts/EXPERIMENTAL/itest.py
   trunk/scripts/EXPERIMENTAL/itest2.py
Modified:
   trunk/doc/installation_guide.tex
   trunk/scripts/EXPERIMENTAL/deepnetplot.py
Log:
Updated script and command for today's demo


Added: trunk/commands/EXPERIMENTAL/plearn_exp.cc
===================================================================
--- trunk/commands/EXPERIMENTAL/plearn_exp.cc	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/commands/EXPERIMENTAL/plearn_exp.cc	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,405 @@
+// -*- C++ -*-
+
+// plearn.cc
+// Copyright (C) 2002 Pascal Vincent, Julien Keable, Xavier Saint-Mleux, Rejean Ducharme
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+// 
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+// 
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+// 
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+// 
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+// 
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+
+/* *******************************************************      
+ * $Id: plearn_light.cc 3995 2005-08-25 13:58:23Z chapados $
+ ******************************************************* */
+
+//! All includes should go into plearn_inc.h.
+#include <commands/plearn_version.h>
+#include <commands/PLearnCommands/plearn_main.h>
+
+
+/*****************
+ * Miscellaneous *
+ *****************/
+// #include <plearn/db/UCISpecification.h>
+// #include <plearn/io/openUrl.h>
+// #include <plearn/math/ManualBinner.h>
+// #include <plearn/math/SoftHistogramBinner.h>
+// #include <plearn/misc/ShellScript.h>
+// #include <plearn/misc/RunObject.h>
+// #include <plearn_learners/misc/Grapher.h>
+// #include <plearn_learners/misc/VariableSelectionWithDirectedGradientDescent.h>
+#include <plearn_learners/testers/PTester.h>
+
+/***********
+ * Command *
+ ***********/
+#include <commands/PLearnCommands/VMatCommand.h>
+#include <commands/PLearnCommands/VMatViewCommand.h>
+// #include <commands/PLearnCommands/AutoRunCommand.h>
+// #include <commands/PLearnCommands/DiffCommand.h>
+// #include <commands/PLearnCommands/FieldConvertCommand.h>
+#include <commands/PLearnCommands/HelpCommand.h>
+// #include <commands/PLearnCommands/JulianDateCommand.h>
+// #include <commands/PLearnCommands/KolmogorovSmirnovCommand.h>
+// #include <commands/PLearnCommands/LearnerCommand.h>
+// #include <commands/PLearnCommands/PairwiseDiffsCommand.h>
+#include <commands/PLearnCommands/ReadAndWriteCommand.h>
+#include <commands/PLearnCommands/RunCommand.h>
+#include <commands/PLearnCommands/ServerCommand.h>
+// #include <commands/PLearnCommands/TestDependenciesCommand.h>
+// #include <commands/PLearnCommands/TestDependencyCommand.h>
+
+// * extra stuff from Boost to generate help *
+// #include <commands/PLearnCommands/HTMLHelpCommand.h>
+
+// //#include <commands/PLearnCommands/TxtmatCommand.h>
+
+
+// /**************
+//  * Dictionary *
+//  **************/
+// #include <plearn/dict/Dictionary.h>
+// #include <plearn/dict/FileDictionary.h>
+// #include <plearn/dict/VecDictionary.h>
+// #include <plearn/dict/ConditionalDictionary.h>
+
+// /****************
+//  * HyperCommand *
+//  ****************/
+#include <plearn_learners/hyper/HyperOptimize.h>
+#include <plearn_learners/hyper/HyperRetrain.h>
+#include <plearn_learners/hyper/HyperSetOption.h>
+
+// /**********
+//  * Kernel *
+//  **********/
+// #include <plearn/ker/AdditiveNormalizationKernel.h>
+// #include <plearn/ker/DistanceKernel.h>
+// #include <plearn/ker/DotProductKernel.h>
+// #include <plearn/ker/EpanechnikovKernel.h>
+// #include <plearn/ker/GaussianKernel.h>
+// #include <plearn/ker/GeodesicDistanceKernel.h>
+// #include <plearn/ker/IIDNoiseKernel.h>
+// #include <plearn/ker/NegOutputCostFunction.h>
+// #include <plearn/ker/NeuralNetworkARDKernel.h>
+// #include <plearn/ker/PolynomialKernel.h>
+// #include <plearn/ker/RationalQuadraticARDKernel.h>
+// #include <plearn/ker/SquaredExponentialARDKernel.h>
+// #include <plearn/ker/SummationKernel.h>
+// #include <plearn/ker/ThresholdedKernel.h>
+// #include <plearn/ker/VMatKernel.h>
+
+// /*************
+//  * Optimizer *
+//  *************/
+// #include <plearn/opt/AdaptGradientOptimizer.h>
+// #include <plearn/opt/ConjGradientOptimizer.h>
+#include <plearn/opt/GradientOptimizer.h>
+
+// /****************
+//  * OptionOracle *
+//  ****************/
+// #include <plearn_learners/hyper/CartesianProductOracle.h>
+#include <plearn_learners/hyper/EarlyStoppingOracle.h>
+#include <plearn_learners/hyper/ExplicitListOracle.h>
+// #include <plearn_learners/hyper/OptimizeOptionOracle.h>
+
+// /************
+//  * PLearner *
+//  ************/
+
+// // Classifiers
+// #include <plearn_learners/classifiers/BinaryStump.h>
+// #include <plearn_learners/classifiers/ClassifierFromConditionalPDistribution.h>
+// #include <plearn_learners/classifiers/ClassifierFromDensity.h>
+// #include <plearn_learners/classifiers/KNNClassifier.h>
+// //#include <plearn_learners/classifiers/SVMClassificationTorch.h>
+// #include <plearn_learners/classifiers/MultiInstanceNNet.h>
+// //#include <plearn_learners/classifiers/OverlappingAdaBoost.h> // Does not currently compile.
+
+// // Generic
+// #include <plearn_learners/generic/AddCostToLearner.h>
+// #include <plearn_learners/generic/AddLayersNNet.h>
+// #include <plearn_learners/generic/BestAveragingPLearner.h>
+// //#include <plearn_learners/generic/DistRepNNet.h>
+// #include <plearn_learners/generic/NNet.h>
+// #include <plearn_learners/generic/SelectInputSubsetLearner.h>
+// #include <plearn_learners/generic/ChainedLearners.h>
+// #include <plearn_learners/generic/StackedLearner.h>
+// #include <plearn_learners/generic/TestingLearner.h>
+// #include <plearn_learners/generic/VPLPreprocessedLearner.h>
+// #include <plearn_learners/generic/VPLPreprocessedLearner2.h>
+// #include <plearn_learners/generic/VPLCombinedLearner.h>
+
+// // Hyper
+#include <plearn_learners/hyper/HyperLearner.h>
+
+// // Meta
+// #include <plearn_learners/meta/AdaBoost.h>
+// #include <plearn_learners/meta/BaggingLearner.h>
+
+// // Regressors
+// #include <plearn_learners/regressors/ConstantRegressor.h>
+// #include <plearn_learners/regressors/GaussianProcessRegressor.h>
+// #include <plearn_learners/regressors/KernelRidgeRegressor.h>
+// #include <plearn_learners/regressors/KNNRegressor.h>
+// #include <plearn_learners/regressors/RankLearner.h>
+// #include <plearn_learners/regressors/RegressorFromDistribution.h>
+// // Unsupervised
+// #include <plearn_learners/unsupervised/UniformizeLearner.h>
+
+// // PDistribution
+// #include <plearn_learners/distributions/SpiralDistribution.h>
+// #include <plearn_learners/distributions/UniformDistribution.h>
+
+// // Nearest-Neighbors
+// #include <plearn_learners/nearest_neighbors/BallTreeNearestNeighbors.h>
+// #include <plearn_learners/nearest_neighbors/ExhaustiveNearestNeighbors.h>
+// #include <plearn_learners/nearest_neighbors/GenericNearestNeighbors.h>
+
+// // Experimental
+// #include <plearn_learners_experimental/DeepFeatureExtractorNNet.h>
+
+// // Online
+// #include <plearn_learners/online/BackConvolution2DModule.h>
+// #include <plearn_learners/online/ClassErrorCostModule.h>
+// #include <plearn_learners/online/CombiningCostsModule.h>
+// #include <plearn_learners/online/Convolution2DModule.h>
+// #include <plearn_learners/online/CostModule.h>
+// #include <plearn_learners/online/DeepBeliefNet.h>
+// #include <plearn_learners/online/GradNNetLayerModule.h>
+// #include <plearn_learners/online/ModulesLearner.h>
+// #include <plearn_learners/online/ModuleStackModule.h>
+// #include <plearn_learners/online/NLLCostModule.h>
+// #include <plearn_learners/online/OnlineLearningModule.h>
+// #include <plearn_learners/online/ProcessInputCostModule.h>
+// #include <plearn_learners/online/RBMBinomialLayer.h>
+// #include <plearn_learners/online/RBMClassificationModule.h>
+// #include <plearn_learners/online/RBMConnection.h>
+// #include <plearn_learners/online/RBMConv2DConnection.h>
+// #include <plearn_learners/online/RBMGaussianLayer.h>
+// #include <plearn_learners/online/RBMLayer.h>
+// #include <plearn_learners/online/RBMMatrixConnection.h>
+// #include <plearn_learners/online/RBMMatrixTransposeConnection.h>
+// #include <plearn_learners/online/RBMMixedConnection.h>
+// #include <plearn_learners/online/RBMMixedLayer.h>
+// #include <plearn_learners/online/RBMMultinomialLayer.h>
+// #include <plearn_learners/online/RBMTruncExpLayer.h>
+// #include <plearn_learners/online/SoftmaxModule.h>
+// #include <plearn_learners/online/SquaredErrorCostModule.h>
+// #include <plearn_learners/online/StackedAutoassociatorsNet.h>
+// #include <plearn_learners/online/Subsampling2DModule.h>
+// #include <plearn_learners/online/Supersampling2DModule.h>
+// #include <plearn_learners/online/TanhModule.h>
+
+// /************
+//  * Splitter *
+//  ************/
+// #include <plearn/vmat/BinSplitter.h>
+// #include <plearn/vmat/BootstrapSplitter.h>
+// #include <plearn/vmat/ClassSeparationSplitter.h>
+// #include <plearn/vmat/ConcatSetsSplitter.h>
+// #include <plearn/vmat/DBSplitter.h>
+#include <plearn/vmat/ExplicitSplitter.h>
+// #include <plearn/vmat/FilterSplitter.h>
+#include <plearn/vmat/FractionSplitter.h>
+// #include <plearn/vmat/KFoldSplitter.h>
+#include <plearn/vmat/NoSplitSplitter.h>
+// #include <plearn/vmat/MultiTaskSeparationSplitter.h>
+// #include <plearn/vmat/RepeatSplitter.h>
+// #include <plearn/vmat/SourceVMatrixSplitter.h>
+// #include <plearn/vmat/StackedSplitter.h>
+// #include <plearn/vmat/TestInTrainSplitter.h>
+// #include <plearn/vmat/ToBagSplitter.h>
+#include <plearn/vmat/TrainTestSplitter.h>
+// #include <plearn/vmat/TrainValidTestSplitter.h>
+
+// /************
+//  * Variable *
+//  ************/
+// #include <plearn/var/MatrixElementsVariable.h>
+
+// /*********************
+//  * VecStatsCollector *
+//  *********************/
+// #include <plearn/math/LiftStatsCollector.h>
+
+// /***********
+//  * VMatrix *
+//  ***********/
+// #include <plearn/vmat/AddMissingVMatrix.h>
+// #include <plearn/vmat/AppendNeighborsVMatrix.h>
+// #include <plearn/vmat/AsciiVMatrix.h>
+#include <plearn/vmat/AutoVMatrix.h>
+// #include <plearn/vmat/BootstrapVMatrix.h>
+// #include <plearn/vmat/CenteredVMatrix.h>
+// #include <plearn/vmat/ClassSubsetVMatrix.h>
+// #include <plearn/vmat/CompactVMatrix.h>
+// #include <plearn/vmat/CompressedVMatrix.h>
+// #include <plearn/vmat/CumVMatrix.h>
+// #include <plearn/vmat/DatedJoinVMatrix.h>
+// // #include <plearn/vmat/DictionaryVMatrix.h>
+// #include <plearn/vmat/DisregardRowsVMatrix.h>
+// #include <plearn/vmat/ExtractNNetParamsVMatrix.h>
+#include <plearn/vmat/FilteredVMatrix.h>
+// #include <plearn/vmat/FinancePreprocVMatrix.h>
+// #include <plearn/vmat/GaussianizeVMatrix.h>
+// #include <plearn/vmat/GeneralizedOneHotVMatrix.h>
+// #include <plearn/vmat/GetInputVMatrix.h>
+// #include <plearn/vmat/GramVMatrix.h>
+// #include <plearn/vmat/IndexedVMatrix.h>
+// #include <plearn/vmat/JulianizeVMatrix.h>
+// #include <plearn/vmat/KNNVMatrix.h>
+// #include <plearn/vmat/KNNImputationVMatrix.h>
+// // Commented out because triggers WordNet, which does not work really fine yet.
+// //#include <plearn/vmat/LemmatizeVMatrix.h>
+// #include <plearn/vmat/LocalNeighborsDifferencesVMatrix.h>
+// #include <plearn/vmat/LocallyPrecomputedVMatrix.h>
+// #include <plearn/vmat/MeanImputationVMatrix.h>
+// //#include <plearn/vmat/MixUnlabeledNeighbourVMatrix.h>
+// #include <plearn/vmat/MultiInstanceVMatrix.h>
+// #include <plearn/vmat/MultiTargetOneHotVMatrix.h>
+// #include <plearn/vmat/MultiToUniInstanceSelectRandomVMatrix.h>
+// #include <plearn/vmat/OneHotVMatrix.h>
+// #include <plearn/vmat/PLearnerOutputVMatrix.h>
+// #include <plearn/vmat/PairsVMatrix.h>
+// #include <plearn/vmat/PrecomputedVMatrix.h>
+// #include <plearn/vmat/ProcessDatasetVMatrix.h>
+#include <plearn/vmat/ProcessingVMatrix.h>
+// #include <plearn/vmat/ProcessSymbolicSequenceVMatrix.h>
+// #include <plearn/vmat/RandomSamplesVMatrix.h>
+// #include <plearn/vmat/RandomSamplesFromVMatrix.h>
+// #include <plearn/vmat/RankedVMatrix.h>
+// #include <plearn/vmat/RegularGridVMatrix.h>
+// #include <plearn/vmat/RemoveDuplicateVMatrix.h>
+// #include <plearn/vmat/ReorderByMissingVMatrix.h>
+// //#include <plearn/vmat/SelectAttributsSequenceVMatrix.h>
+// #include <plearn/vmat/SelectRowsMultiInstanceVMatrix.h>
+// #include <plearn/vmat/ShuffleColumnsVMatrix.h>
+// #include <plearn/vmat/SortRowsVMatrix.h>
+// #include <plearn/vmat/SparseVMatrix.h>
+// #include <plearn/vmat/SplitWiseValidationVMatrix.h>
+// #include <plearn/vmat/SubInputVMatrix.h>
+// #include <plearn/vmat/TemporaryDiskVMatrix.h>
+// #include <plearn/vmat/TemporaryFileVMatrix.h>
+// #include <plearn/vmat/TextFilesVMatrix.h>
+// #include <plearn/vmat/ThresholdVMatrix.h>
+// #include <plearn/vmat/TransposeVMatrix.h>
+// #include <plearn/vmat/UCIDataVMatrix.h>
+// #include <plearn/vmat/UniformizeVMatrix.h>
+// #include <plearn/vmat/VariableDeletionVMatrix.h>
+// #include <plearn/vmat/ViewSplitterVMatrix.h>
+// #include <plearn/vmat/VMatrixFromDistribution.h>
+
+
+
+// **** Require LAPACK and BLAS
+
+// Unsupervised/KernelProjection
+// #include <plearn_learners/unsupervised/Isomap.h>
+// #include <plearn_learners/unsupervised/KernelPCA.h>
+// #include <plearn_learners/unsupervised/LLE.h>
+// #include <plearn_learners/unsupervised/PCA.h>
+// #include <plearn_learners/unsupervised/SpectralClustering.h>
+
+// Kernels
+// #include <plearn/ker/LLEKernel.h>
+// #include <plearn/ker/ReconstructionWeightsKernel.h>
+
+// Regressors
+// #include <plearn_learners/regressors/LinearRegressor.h>
+// #include <plearn_learners/regressors/PLS.h>
+
+// PDistribution
+// #include <plearn_learners/distributions/GaussianDistribution.h>
+// #include <plearn_learners/distributions/GaussMix.h>
+// #include <plearn_learners/distributions/RandomGaussMix.h>
+// #include <plearn_learners/distributions/ParzenWindow.h>
+// #include <plearn_learners/distributions/ManifoldParzen2.h>
+
+// Experimental
+// #include <plearn_learners_experimental/LinearInductiveTransferClassifier.h>
+
+// SurfaceTemplate
+// #include <plearn_learners_experimental/SurfaceTemplate/SurfaceTemplateLearner.h>
+
+// ***************************************************
+// ***   New EXPERIMENTAL stuff
+
+// includes Pascal's gradient hack
+#include <plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h>
+
+
+// Stuff used for DeepReconstrctorNet experiments
+#include <plearn/var/Variable.h>
+#include <plearn/var/SquareVariable.h>
+#include <plearn/math/TVec_impl.h>
+#include <plearn/var/EXPERIMENTAL/MultiMaxVariable.h>
+#include <plearn/var/SoftmaxVariable.h>
+#include <plearn/var/SumSquareVariable.h>
+#include <plearn/var/Func.h>
+#include <plearn/var/EXPERIMENTAL/DoubleProductVariable.h>
+#include <plearn/var/EXPERIMENTAL/TransposedDoubleProductVariable.h>
+#include <plearn/var/EXPERIMENTAL/ProbabilityPairsVariable.h>
+#include <plearn/var/EXPERIMENTAL/ProbabilityPairsInverseVariable.h>
+#include <plearn/var/EXPERIMENTAL/SoftSoftMaxVariable.h>
+#include <plearn/var/EXPERIMENTAL/LogSoftSoftMaxVariable.h>
+#include <plearn_learners/generic/EXPERIMENTAL/DeepReconstructorNet.h>
+#include <plearn/var/SourceVariable.h>
+#include <plearn/var/ExpVariable.h>
+#include <plearn/var/SigmoidVariable.h>
+#include <plearn/var/ProductTransposeVariable.h>
+#include <plearn/var/NegCrossEntropySigmoidVariable.h>
+#include <plearn/var/LogSoftmaxVariable.h>
+#include <plearn/var/ClassificationLossVariable.h>
+// #include <plearn/var/EXPERIMENTAL/MultiSampleVariable.h>
+
+// Stuff used for transformationLearner experiments
+#include <plearn_learners/distributions/EXPERIMENTAL/TransformationLearner.h>
+
+using namespace PLearn;
+
+int main(int argc, char** argv)
+{
+    return plearn_main( argc, argv, 
+                        PLEARN_MAJOR_VERSION, 
+                        PLEARN_MINOR_VERSION, 
+                        PLEARN_FIXLEVEL       );
+}
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/doc/installation_guide.tex
===================================================================
--- trunk/doc/installation_guide.tex	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/doc/installation_guide.tex	2007-09-07 21:14:30 UTC (rev 8055)
@@ -177,7 +177,7 @@
 python (which calls upon plearn) using essentially the following software
 packages: \\
 \begin{itemize}
-\item {\bf numarray} for efficient numeric array operations in python.
+\item {\bf numpy} (part of scipy) for efficient numeric array operations in python.
 \item {\bf matplotlib} for 2D plots.
 \item {\bf mayavi} for 3D interactive plots.
 \item {\bf pygtk} with {\bf gtk+2} for sophisticated GUIs.
@@ -505,6 +505,14 @@
 % which I couldn't get to compile and link from the source
 % http://ftp.gnome.org/pub/GNOME/sources/gnome-python-extras/
 
+% For python 2.5:
+% \item {\tt python25}
+% \item {\tt boost-jam}
+% \item {\tt boost1.34.nopython}
+% \item {\tt scipy-py25} for efficient matrix/vector manipulations in python
+% \item {\tt matplotlib-py25} for 2D graphics
+
+
 \subsection{Environment setup}
 
 You should make sure the following variables and paths are correctly defined in your

Added: trunk/scripts/EXPERIMENTAL/TLTester.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/TLTester.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/TLTester.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,165 @@
+import os ,sys, time, matplotlib, math, copy
+from numpy import *
+from matplotlib.pylab import *
+from matplotlib.colors import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from copy import *
+
+from generators import *
+
+UNDEFINED = -1
+EMPTY_MAT = TMat()
+
+class TLTester(object):
+    
+    #iLearner = UNDEFINED
+    #generator = UNDEFINED
+    #dim = 2
+    
+    def __init__(self,
+                 iLearner,
+                 generator):
+        assert(2*generator.nbTransforms == iLearner.nbTransforms)
+        assert(generator.dim == iLearner.dim)
+        self.dim = generator.dim
+        self.iLearner = iLearner
+        self.generator = generator
+
+
+    def biasAreNull(self,
+                    biasSet):
+        if(type(biasSet)==type(EMPTY_MAT)):
+            if(biasSet.nrows == 0 and biasSet.ncols == 0):
+                return True
+            else:
+                return False
+        elif(len(biasSet)==0):
+            return True
+        else:
+            for i in range(biasSet.shape[0]):
+                for j in range(biasSet.shape[1]):
+                    if(biasSet[i][j] != 0):
+                        return false
+            return True
+
+
+            
+        
+    def linearToLinearIncrement(self,transform,bias):
+        newTransform = copy(transform)
+        newBias = copy(bias)
+        for i in range(newTransform.shape[0]):
+            newTransform[i][i] = newTransform[i][i] - 1;
+        return (newTransform, newBias)
+
+    def linearIncrementToLinear(self,transform,bias):
+        newTransform = copy(transform)
+        newBias = copy(bias)
+        for i in range(newTransform.shape[0]):
+            newTransform[i][i] = newTransform[i][i] + 1;
+        return (newTransform,newBias)
+    
+    
+    def inverseTransformation(self,transform,bias,transformFamily):
+        if(transformFamily == LINEAR):
+            invTransform = self.zeroMatrix(self.dim,self.dim)
+            inv = inverse(transform)
+            for i in range(self.dim):
+                for j in range(self.dim):
+                    invTransform[i,j]=inv[i,j]
+            if(len(bias) >0):
+                invBias = -1*dot(invTransform ,bias)
+            else:
+                invBias = []
+            return (invTransform,invBias)
+        else:
+            id = identity(len(transform))
+            invTransform = zeros((self.dim,self.dim))
+            inv = inverse(transform + id)
+            for i in range(self.dim):
+                for j in range(self.dim):
+                    invTransform[i][j]=inv[i][j]
+            if(len(bias) > 0):
+                invBias = -1*dot(invTransform,bias)
+            else:
+                invBias = []
+            invTransform = invTransform - id
+            return (invTransform,invBias)
+
+
+    def conversionGeneratorToLearner(self,transform,bias):
+        if(self.generator.transformFamily == self.iLearner.transformFamily):
+            newTransform = copy(transform)
+            newBias = copy(bias)
+            return(newTransform,newBias)
+        elif(self.generator.transformFamily == LINEAR):
+            return self.linearToLinearIncrement(transform,bias)
+        else:
+            return self.linearIncrementToLinear(transform,bias)
+
+
+    def prepareILearner(self):
+        self.transmitParametersToLearn()
+        self.transmitDataSet()
+
+        
+    def transmitDataSet(self):
+        self.iLearner.setTrainingSet(self.generator.newDataSet())
+
+
+         
+    
+
+    def transmitParametersToLearn(self):
+        #transformations: 
+        transformsToLearn = []
+        temp = []
+        biasToLearn = array(zeros(self.iLearner.nbTransforms*self.dim), 'd')
+        biasToLearn.resize(self.iLearner.nbTransforms,self.dim)
+        
+        if(not self.iLearner.withBias and self.generator.withBias):
+            assert(self.biasAreNull(self.generator.biasSet))
+        K = self.generator.nbTransforms
+        for i in range(K):
+            if(self.generator.withBias):
+                biasToLearn[i] = copy(self.generator.biasSet[i,:])
+            (t,b)=self.conversionGeneratorToLearner(self.generator.transforms[i],biasToLearn[i])
+            transformsToLearn.append(t.copy())
+            biasToLearn[i]=copy(b)
+            (t,b)= self.inverseTransformation(t,                                       
+                                              b,
+                                              self.iLearner.transformFamily)
+            temp.append(t.copy())
+            biasToLearn[i + K]=copy(b)
+        for i in range(K):
+            transformsToLearn.append(temp[i])
+            
+    
+        
+        #noise variance
+        noiseVariance = UNDEFINED
+        if(self.iLearner.learnNoiseVariance):
+            noiseVariance=self.generator.noiseVariance
+        #transform distribution
+        transformDistribution = []
+        if(self.iLearner.learnTransformDistribution):
+            transformDistribution = [0.0,0.0]
+            for i in range(self.generator.nbTransforms):
+                p = 0.5*exp(self.generator.transformDistribution[i])
+                transformDistribution[i]=log(p)
+                transformDistribution[i + self.generator.nbTransforms] = log(p)
+        #transmission
+        self.iLearner.setParametersToLearn(transformsToLearn,
+                                      biasToLearn,
+                                      noiseVariance,
+                                      transformDistribution)
+            
+    def run(self):
+        self.prepareILearner()
+        self.iLearner.run()
+        
+            
+            

Modified: trunk/scripts/EXPERIMENTAL/deepnetplot.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/deepnetplot.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -50,6 +50,8 @@
     print 'w : plot the weight matrices associated with the current pixel'
     print 'W : same as w but for all a group'
     print 'C : same as w but for the hidden unit that has the highest value in each group'
+    print 'Z : set all the pixels of the current layer to zero'
+    print 'B : set all the pixels of the current layer to 1'
     print 'o : set the current hidden layer to its original state'
     print 'O : same as o but for every layer'
     print 't : now we have the same scale for  W, C'
@@ -107,6 +109,9 @@
         n = self.matrixToLayer(x,y)
         return self.hidden_layer[n]
 
+    def fill(self,value):
+        self.hidden_layer.fill(value)
+
     def setElement(self, x,y, value):
         n = self.matrixToLayer(x,y)
         self.hidden_layer[n] = value
@@ -379,6 +384,16 @@
                 
             # set pixel -- z,x,c,v,b
 
+            elif char=='Z':
+                hl.fill(0.)
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+
+            elif char=='B':
+                hl.fill(1.)
+                self.rep_axes[i].imshow(hl.getMatrix(), interpolation = self.interpolation, cmap = self.cmap)
+                draw()
+
             elif char in ['z', 'x', 'c', 'v', 'b']:
                 
                 x,y = event.xdata, event.ydata
@@ -922,7 +937,7 @@
 ### main ###
 ############
 
-server_command = "slearn server"
+server_command = "plearn_exp server"
 serv = launch_plearn_server(command = server_command)
 
 #print "Press Enter to continue"

Added: trunk/scripts/EXPERIMENTAL/generators.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/generators.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/generators.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,625 @@
+#generators2.py : re-implantation of generators1.py with
+#                 numpy array module instead numarray
+
+#generation procedures with 'TransformationLearner' distribution
+import os, sys, time, matplotlib, math, numpy ,copy
+
+from numpy import *
+from math import *
+from random import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from copy import *
+
+UNDEFINED = -1
+BEHAVIOR_GENERATOR = 1
+LINEAR = 0
+LINEAR_INCREMENT = 1
+EMPTY_BIAS = TMat()
+
+#------------------- TREE GENERATOR --------------------------------------
+
+class TreeGenerator(object):
+
+    #DESCRIPTION:
+
+    #generates a data set :
+    #     equivalent in building a tree 
+    #
+    #            0      1        2     ...         
+    #  
+    #            r -> child1  -> child1  ...       
+    #                         -> child2  ...
+    #                             ...    ...
+    #                         -> childn  ...
+    #
+    #              -> child2  -> child1  ...
+    #                         -> child2  ...
+    #                              ...   ...
+    #                         -> childn  ...
+    #                      ...
+    #              -> childn  -> child1  ...
+    #                         -> child2  ...
+    #                              ...   ...
+    #                         -> childn  ... 
+    #
+    # The child are generated by the same following process:
+    #  1) choose a transformation  
+    #  2) apply the transformation to the parent
+    #  3) add noise to the result 
+    
+
+
+    #PARAMETERS OF THE  GENERATOR
+
+    #parameters of the transformation learner used in the generation process:
+    tl=UNDEFINED                   #c++ transformation learner object
+    dim=2                          #dimension of input space
+    nbTransforms = UNDEFINED       #number of transformations
+    transformFamily=UNDEFINED      #transformation function family
+    transforms = []                #transformation matrices
+    withBias = False               #add a bias to the transformation function ?
+    biasSet = UNDEFINED            #transformation bias (if any)
+    noiseVariance = UNDEFINED      #noise variance 
+    transformDistribution = []     #transformation distribution (in log form) 
+
+    #shape of the tree data set to generate
+    root = []
+    deepness = 1
+    branchingFactor = 1
+
+
+
+    #CONSTRUCTOR
+    
+    def __init__(self,
+                 tl,
+                 dim=2,
+                 builded=False,
+                 deepness=1,
+                 branchingFactor=1,
+                 root=[]):
+        #c++ transformation learner object
+        self.tl = tl
+        #number of transformations
+        self.nbTransforms = self.tl.getOption("nbTransforms")
+        #transformation function family
+        self.transformFamily = self.tl.getOption("transformFamily")
+        #add a bias to the transformation function ? 
+        self.withBias = self.tl.getOption("withBias")
+        #if the generator is not builded, some parameters might be undefined.
+        #We call the default generator building procedure
+        #(use default initialization procedures when initial values are needed)
+        if(not builded):
+            self.dim = dim
+            self.tl.generatorBuild(self.dim,
+                                   [],
+                                   EMPTY_BIAS,
+                                   UNDEFINED,
+                                   [])
+        else:
+            self.dim = self.tl.getOption("trainingSetLength")
+            assert(self.dim == dim)
+        #transformation matrices 
+        self.transforms=self.tl.getOption("transforms")
+        #transformation bias (if any)
+        if(self.withBias):
+            self.biasSet= self.tl.getOption("biasSet")
+        else:
+            self.biasSet = EMPTY_BIAS
+        #noise variance
+        self.noiseVariance = self.tl.getOption("noiseVariance")
+        #transformation distribution (in log form)
+        self.transformDistribution  = self.tl.getOption("transformDistribution")
+        #generation process default parameters
+        if(len(root) == 0):
+            self.root = array(zeros(self.dim), 'd')
+            for i in range(self.dim):
+                self.root[i]=uniform(0,10)
+        else:
+            self.setRoot(root)
+        assert(deepness >=1)
+        self.deepness = deepness
+        assert(branchingFactor >= 1)
+        self.branchingFactor = branchingFactor
+
+
+    #changes the default root of the tree     
+    def setRoot(self,root):
+        assert(len(root) == self.dim)
+        self.root = copy(root)
+            
+
+    #builds the TransformationLearner object (our generator) 
+    #that is, call method 'TransformationLearner::generatorBuild'      
+    def build(self,
+              transforms=[],
+              biasSet=EMPTY_BIAS,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        #verifications fot the transformation matrices and bias
+        if(len(transforms)>0):
+            assert(self.nbTransforms == len(transforms))
+            for i in range(self.nbTransforms):
+                assert(transforms[i].shape[0] == self.dim)
+                assert(transforms[i].shape[1] == self.dim)
+            if(self.withBias):
+                assert(biasSet.shape[0]==self.nbTransforms)
+                assert(baisSet.shape[1]==self.dim)
+        else:
+            biasSet=EMPTY_BIAS
+        #verifications for the transformation distribution
+        if(len(transformDistribution)>0):
+            assert(len(transformDistribution)==self.nbTransforms)
+            sum=0
+            w=0
+            for i in range(self.nbTransforms):
+                w = exp(transformDistribution[i])
+                assert(0<=w<=1)
+                sum += w
+            assert(sum == 1)
+        #we are ready to call the building method:
+        self.tl.generatorBuild(self.dim,
+                               transforms,
+                               biasSet,
+                               noiseVariance,
+                               transformDistribution)
+        #capture new parameters of the Transformation Learner
+        self.transforms = self.tl.getOption("transforms")
+        if(self.withBias):
+            self.biasSet = self.tl.getOption("biasSet")
+        else:
+            self.biasSet= EMPTY_BIAS
+        self.noiseVariance = self.tl.getOption("noiseVariance")
+        self.transformDistribution = self.tl.getOption("transformDistribution")
+
+
+
+    #sets the transformations matrices (and bias if any) of the TransformationLearner
+    def setTransforms(self,
+                      transforms,
+                      biasSet=EMPTY_BIAS):
+        assert(self.nbTransforms==len(transforms))
+        for i in range(self.nbTransforms):
+            assert(self.dim == transforms[i].shape[0])
+            assert(self.dim == transforms[i].shape[1])
+        self.transforms = copy(transforms)
+        if(self.withBias):
+            assert(biasSet.shape[0]==self.nbTransforms)
+            assert(biasSet.shape[1]==self.dim)
+            self.biasSet = biasSet.copy()
+        else:
+            biasSet = EMPTY_BIAS
+        self.tl.setTransformsParameters(transforms,biasSet)
+             
+
+    #sets the noise variance of the TransformationLearner
+    def setNoiseVariance(self,
+                         noiseVariance):
+        assert(noiseVariance > 0)
+        self.noiseVariance = noiseVariance
+        self.tl.setNoiseVariance(noiseVariance)
+        
+
+    #sets the transformation distribution of the TransformationLearner
+    def setTransformDistribution(self,
+                                 transformDistribution):
+        assert(self.nbTransforms == len(transformDistribution))
+        sum = 0
+        for i in range(self.nbTransforms):
+            p = exp(transformDistribution[i])
+            assert(0<=p and p<=1)
+            sum += p
+        assert(sum == 1)
+        self.transformDistribution  = copy(transformDistribution)
+        self.tl.setTransformDistribution(transformDistribution)
+
+
+    #computes the length of a generation tree 
+    def computeTreeLength(self,
+                          deepness,
+                          branchingFactor):
+        if(branchingFactor == 1):
+            return deepness + 1
+        else:
+            return ((1.0 - pow(branchingFactor,deepness + 1))/(1.0 - b))
+
+
+    #changes the default generation process parameters
+    def setDefaultTreeParameters(self,
+                                 deepness,
+                                 branchingFactor,
+                                 root):
+        self.setRoot(root)
+        assert(deepness >=1)
+        self.deepness = deepness
+        assert(branchingFactor >= 1)
+        self.branchingFactor = branchingFactor
+    
+    #returns a rotation matrix 
+    #(theta here must be given in Rad units)
+    def get2DRotationMatrix(self,theta):
+        M = array([cos(theta) ,-sin(theta),sin(theta) ,cos(theta)], 'd')
+        M.resize(2,2)
+        if(self.transformFamily == LINEAR_INCREMENT):
+                M[0][0] -= 1;
+                M[1][1] -= 1;
+        return M
+
+    
+    #generates a data set and stores it in a file 
+    def generateAndStoreDataSet(self,
+                                filename,
+                                deepness=UNDEFINED,
+                                branchingFactor=UNDEFINED,
+                                root =[]):
+        dataSet = self.newDataSet( False, deepness,branchingFactor,root)
+        self.writeDataSet(dataSet, filename)
+    
+
+
+    
+    #stores a data set in a file 
+    def writeDataSet(self,dataSet,filename):
+        out = open(filename, 'w')
+        out.write(str(dataSet.shape[1]) + " 0 0\n")
+        out.write(str(dataSet))
+        out.close()
+
+
+    #creates a new data set
+    #(use default generation parameters if the given generation parameters are not well defined
+    # or unspecified)
+    def newDataSet(self,
+                   returnTransforms=False,
+                   deepness=UNDEFINED,
+                   branchingFactor=UNDEFINED,
+                   root=[],
+                   transformIndex = UNDEFINED):
+        if(len(root)==0):
+            root= copy(self.root)
+        else:
+            assert(len(root)==self.dim)
+        if(deepness<1):
+            deepness=self.deepness
+        if(branchingFactor<1):
+            branchingFactor=self.branchingFactor
+        dataSet = self.tl.returnTreeDataSet(root,
+                                            deepness,
+                                            branchingFactor,
+                                            transformIndex)
+        if(returnTransforms):
+            return (dataSet, self.transforms, self.biasSet)
+        else:
+            return dataSet 
+
+
+# ----------------- SEQUENTIAL GENERATOR -----------------------------------------
+
+class SequentialGenerator(TreeGenerator):
+
+    nbDataPoints = 40
+
+
+    def __init__(self,
+                 tl,
+                 dim=2,
+                 builded=False,
+                 nbDataPoints=40,
+                 root=[]):
+        assert(nbDataPoints >=2)
+        self.nbDataPoints = nbDataPoints
+        TreeGenerator.__init__(self,
+                               tl,
+                               dim,
+                               builded,
+                               nbDataPoints - 1,
+                               1,
+                               root)
+
+
+    def setTreeParameters(self,
+                          deepness,
+                          branchingFactor,
+                          root):
+        assert(branchingFactor == 1)
+        TreeGenerator.setTreeParameters(self,
+                                        deepness,
+                                        branchingFactor,
+                                        root)
+        self.nbDataPoints=self.computeTreeLength(deepness,
+                                                 branchingFactor)
+
+
+    def setSequenceParameters(self,
+                              nbDataPoints,
+                              root):
+        self.setTreeParameters(nbDataPoints - 1,1,root)
+
+
+    def newDataSet(self,
+                   returnTransforms=False,
+                   nbDataPoints=UNDEFINED,
+                   root=[],
+                   transformIndex =UNDEFINED):
+        if(nbDataPoints<2):
+            nbDataPoints = self.nbDataPoints
+        return TreeGenerator.newDataSet(self,
+                                        returnTransforms,
+                                        nbDataPoints - 1,
+                                        1,
+                                        root,
+                                        transformIndex)
+
+
+    
+#-------------CIRCLE GENERATOR ----------------------------------
+class CircleGenerator(SequentialGenerator):
+
+    ray = 10
+    center =array([0,0],'d')
+
+    def __init__(self,
+                 tl,
+                 builded=False,
+                 nbDataPoints=40,
+                 center=array([0,0],'d'),
+                 ray=10):
+        assert(tl.getOption("nbTransforms") == 1)
+        assert(ray>0)
+        self.ray = ray
+        assert(len(center)==2)
+        root = copy(center)
+        root[1] += ray
+        SequentialGenerator.__init__(self,
+                                     tl,
+                                     2,
+                                     builded,
+                                     nbDataPoints,
+                                     root)
+        self.setTransforms(nbDataPoints)
+
+
+    def setTreeParameters(self,
+                          deepness,
+                          branchingFactor,
+                          root):
+        SequentialGenerator.settreeParameters(self,
+                                              deepness,
+                                              branchingFactor,
+                                              root)
+        self.setTransforms(self.nbDataPoints) 
+
+
+
+    def build(self,
+              nbDataPoints,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        assert(nbDataPoints>=2)
+        self.nbDataPoints = nbDataPoints
+        theta=2*pi/nbDataPoints
+        M = self.get2DRotationMatrix(theta)
+        transforms = [M]
+        biasSet = EMPTY_BIAS
+        if(self.withBias):
+            biasSet = array([0,0],'d')
+            biasSet.resize(1,2)
+        SequentialGenerator.build(self,
+                                  transforms,
+                                  biasSet,
+                                  noiseVariance,
+                                  transformDistribution)
+
+    def setRoot(self,
+                root):
+        SequentialGenerator.setRoot(self,
+                                    root)
+        self.center = copy(root)
+        self.center[1] -= self.ray
+
+
+    def setCircleParameters(self,
+                            center,
+                            ray,
+                            nbDataPoints):
+        assert(len(center)==2)
+        assert(ray>0)
+        assert(nbDataPoints >=2)
+        self.center = copy(center)        #center
+        self.ray = ray                    #ray
+        self.root = copy(center)          #root
+        self.root[1] += ray
+        self.setTransforms(nbDataPoints)
+    
+
+    def setTransforms(self,
+                      nbDataPoints):
+        assert(nbDataPoints >=2)
+        self.nbDataPoints = nbDataPoints
+        self.defaultDeepness = nbDataPoints - 1
+        theta = 2*pi/nbDataPoints
+        transforms = [self.get2DRotationMatrix(theta)]
+        biasSet = EMPTY_BIAS
+        if(self.withBias):
+            biasSet = array([0,0],'d')
+            biasSet.resize(1,2)
+        SequentialGenerator.setTransforms(self,
+                                          transforms,
+                                          biasSet)
+        
+
+    def newDataSet(self,
+                   returnTransforms=False,
+                   center=[],
+                   ray=UNDEFINED,
+                   nbDataPoints=UNDEFINED):
+        if(len(center) != 2):
+            center = copy(self.center)
+        if(ray<=0):
+            ray = self.ray
+        if(nbDataPoints < 2):
+            nbDataPoints = self.nbDataPoints
+        self.setCircleParameters(center,
+                                 ray,
+                                 nbDataPoints)
+        return SequentialGenerator.newDataSet(self,
+                                              returnTransforms)
+
+    
+# ------------- Spiral Generator ---------------------------------------
+
+class SpiralGenerator(SequentialGenerator):
+
+    alpha = 1.01
+    theta = 0.1
+
+
+    def __init__(self,
+                 tl,
+                 builded=False,
+                 nbDataPoints=40,
+                 root=[],
+                 alpha = 1.01,
+                 theta = 0.1
+                 ):
+
+        assert(tl.getOption("nbTransforms") ==1)
+        assert(alpha > 0)
+        assert(theta > 0)
+        self.alpha = alpha
+        self.theta = theta
+        SequentialGenerator.__init__(self,
+                                     tl,
+                                     2,
+                                     builded,
+                                     nbDataPoints,
+                                     root)
+        self.setTransforms(alpha,theta)
+
+
+    def setTransforms(self,
+                      alpha,
+                      theta):
+        assert(alpha > 0)
+        assert(theta > 0)
+        self.alpha = alpha
+        self.theta = theta
+        transforms = self.get2DRotationMatrix(theta)
+        transforms = alpha*transforms
+        biasSet=EMPTY_BIAS
+        if(self.withBias):
+            biasSet = array([0,0], 'd')
+            biasSet.resize(1,2)
+        SequentialGenerator.setTransforms(self,[transforms],biasSet) 
+
+
+    def setSpiralParameters(self,
+                            nbDataPoints,
+                            root,
+                            alpha,
+                            theta):
+        SequentialGenerator.setDefaultSequenceParameters(self,
+                                                         nbDataPoints,
+                                                         root)
+        self.setTransforms(alpha,theta)
+
+
+    def build(self,
+              alpha,
+              theta,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        self.setTransforms(alpha,theta)
+        SequentialGenerator.build(self,
+                                  self.transforms,
+                                  self.biasSet,
+                                  noiseVariance,
+                                  transformDistribution)
+
+
+    def newDataSet(self,
+                   returnTransforms=False,
+                   nbDataPoints=UNDEFINED,
+                   root=[],
+                   alpha=UNDEFINED,
+                   theta=UNDEFINED):
+        if(alpha <= 0):
+            alpha = self.alpha
+        if(theta <= 0):
+            theta = self.theta
+        if(nbDataPoints < 2):
+            nbDataPoints = self.nbDataPoints
+        if(len(root) != self.dim):
+            root = self.root
+        self.setSpiralParameters(self,nbDataPoints,root,alpha,theta)        
+        return SequentialGenerator.newDataSet(self,
+                                              returnTransforms,
+                                              nbDataPoints,
+                                              root) 
+
+#--------------- LINE GENERATOR ------------------------------------
+class LineGenerator(SequentialGenerator):
+    
+
+    
+    def __init__(self,
+                 tl,
+                 dim=2,
+                 builded=False,
+                 nbDataPoints=40,
+                 increment=[]
+                 ):
+        assert(tl.getOption("nbTransforms") == 1)
+        assert(tl.getOption("withBias"))
+        t = array(zeros(dim*dim),'d')
+        t.resize(dim,dim)
+        transforms = [t]
+        self.transforms = transforms
+        biasSet = array(increment, 'd')
+        biasSet.resize(1,dim)
+        SequentialGenerator__init__(self,
+                                    tl,
+                                    dim,
+                                    builded,
+                                    transforms,
+                                    biasSet)
+
+    def setTransforms(self,
+                      increment):
+        t = array(zeros(self.dim*self.dim),d)
+        t.resize(self.dim,self.dim)
+        transforms = [t]
+        biasSet = array(increment, 'd')
+        biasSet.resize(1,self.dim)
+        SequentialGenerator.setTransforms(self,
+                                          transforms,
+                                          biasSet)
+
+    def build(self,
+              increment,
+              noiseVariance=UNDEFINED,
+              transformDistribution=[]):
+        assert(len(increment)==self.dim)
+        biasSet = array(increment, 'd')
+        biasSet.resize(1,self.dim)
+        SequentialGenerator.build(self,
+                                  [zeros(self.dim,self.dim)],
+                                  biasSet,
+                                  noiseVariance,
+                                  transformDistribution)
+        
+        
+    def newDataSet(self,
+                   returnDataSet,
+                   root=[],
+                   nbDataPoints=UNDEFINED,
+                   increment=array([])):
+        if(len(increment)!= self.dim):
+            increment = copy(self.biasSet[0,:])
+        self.setTransforms(increment)
+        return SequentialGenerator.newDataSet(self,
+                                              returnDataSet,
+                                              root,
+                                              nbDataPoints) 

Added: trunk/scripts/EXPERIMENTAL/iTraining.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/iTraining.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/iTraining.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,602 @@
+#iTraining2.py
+#to control interactively the training process
+
+import os, sys, time, matplotlib, math, numpy ,copy
+
+from matplotlib.pylab import plot,show,draw,close,text,scatter,colorbar
+from matplotlib.colors import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from numpy import *
+from pickle import *
+from copy import *
+
+
+
+
+#RECONSTRUCTION CANDIDATE
+
+#consists in a 4-element tuple:
+#   (target, target<s neighbor, transformation, weight)
+TARGET_IDX = 0
+NEIGHBOR_IDX = 1
+TRANSFORM_IDX = 2
+WEIGHT = 3
+
+
+#OTHER CONSTANTS
+DEFAULT = 0
+UNDEFINED = -1
+EMPTY_BIAS = TMat()
+
+#ITERATIVE LEARNER ----------------------------------------------------------
+class IterativeLearner(object):
+
+
+    #GRAPHICAL OUTPUT FORMAT (constants)
+
+    #a data point is represented graphically by a dot
+    #    his size/shape/color might change according to his nature:
+    #          -target ?
+    #          -neighbor ?
+    #          -ordinary training point ?
+    #          -reconstruction ?
+    #we also need some formats to draw the transformations
+
+    #sizes
+    MIN_SIZE = 10.0
+    MAX_SIZE = 100.0
+    TARGET_SIZE = MAX_SIZE
+    NEIGHBOR_SIZE = MIN_SIZE
+    DEFAULT_SIZE = MIN_SIZE
+    RECONSTRUCTION_SIZE_FACTOR = 1
+
+    #colors
+    TARGET_COLOR = 'b'
+    NEIGHBOR_COLOR = 'k'
+    DEFAULT_COLOR = 'w'
+    RECONSTRUCTION_COLOR = 'y'
+    TRANSFORMATION_COLOR  = 'k'
+
+    #shapes
+    TARGET_SHAPE = 'o'
+    NEIGHBOR_SHAPE = 'o'
+    DEFAULT_SHAPE = 'o'
+    RECONSTRUCTION_SHAPE = 'd'
+
+    #index of a transformation : size of the police
+    TRANSFORMATION_DIGIT_SIZE = 12
+
+    #LEARNER
+    
+    learner = UNDEFINED                    #TransformationLearner object
+    dim = 2                                #dimension of input space
+    nbTransforms = UNDEFINED               #number  of transforms to learn
+    withBias = False                       #includes a bias addition in the transformation function?
+    transformsToLearn = []                 #the transformations matrices to learn
+    biasToLearn = EMPTY_BIAS                #the transformations bias to learn, if any
+    learnedTransforms = []                 #learned transformations matrices
+    learnedBias = EMPTY_BIAS                #learned transformations bias, if any
+    learnNoiseVariance = False             #noise variance = learned parameter ?
+    noiseVariance = UNDEFINED              #noise variance (fixed or learned)
+    noiseVarianceToLearn=UNDEFINED         #noise variance to learn (if any)
+    learnTransformDistribution = False     #transformation distribution = learned parameter ?
+    transformDistribution = []             #transformation distribution (fixed or learned)
+    transformDistributionToLearn = []      #transformation distribution to learn (if any)
+    data = array([])                       #training data points
+    transformFamily=0                      #type of transformation functions used
+    
+
+    #TARGET AND CORRESPONDING RECONSTRUCTION CANDIDATES
+    testTargetIdx = 0
+    target = []
+    neighbors = array([])
+    reconstructions= array([])
+    weights = array([])
+    recSizes = array([])
+    recColors = array([])
+    choosenTransforms = array([])
+
+
+    #INITIALIZATION PROCEDURES
+
+    #constructor
+    def __init__(self,
+                 learner,
+                 dim=2,
+                 testTargetIdx=0,
+                 transformsToLearn=[],
+                 bias=array([]),
+                 noiseVarianceToLearn=UNDEFINED,
+                 transformDistributionToLearn=[],
+                 data=array([])):
+        self.learner = learner
+        self.dim = dim
+        self.nbTransforms = self.learner.getOption("nbTransforms")
+        self.withBias = self.learner.getOption("withBias")
+        self.transformFamily = self.learner.getOption("transformFamily")
+        self.testTargetIdx = testTargetIdx
+        self.learnNoiseVariance = self.learner.getOption("learnNoiseVariance")
+        if(self.learnNoiseVariance and noiseVarianceToLearn>0):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        else:
+            self.updateNoiseVariance()
+        self.learnTransformDistribution = self.learner.getOption("learnTransformDistribution")
+        if(self.learnTransformDistribution and len(transformDistributionToLearn) >0):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)        
+        if(len(transformsToLearn) != 0):
+            self.setTransformsToLearn(transformsToLearn, bias)
+        if(len(data)!=0):
+            self.setTrainingSet(data)
+
+
+    def setNoiseVarianceToLearn(self,
+                                noiseVarianceToLearn):
+        assert(noiseVarianceToLearn>0)
+        self.noiseVarianceToLearn = noiseVarianceToLearn
+
+    def setTransformDistributionToLearn(self,
+                                        transformDistributionToLearn):
+        assert(len(transformDistributionToLearn) == self.nbTransforms)
+        sum = 0
+        for i in range(self.nbTransforms):
+            p = exp(transformDistributionToLearn[i])
+            assert( 0<= p <=1)
+            sum = sum + p
+        assert(sum == 1)
+        self.transformDistributionToLearn = copy(transformDistributionToLearn)
+        
+        
+
+    #specifies the set of transformation functions that might be learned        
+    def setTransformsToLearn(self,
+                             transformsToLearn,
+                             bias=EMPTY_BIAS):
+        assert(len(transformsToLearn)==self.nbTransforms)
+        if(self.withBias):
+            assert(bias.shape[0] == self.nbTransforms)
+            assert(bias.shape[1] == self.dim)
+            self.biasToLearn= bias.copy()
+        else:
+            self.biasToLearn = EMPTY_BIAS
+        for i in range(self.nbTransforms):
+            assert(self.dim == transformsToLearn[i].shape[0])
+            assert(self.dim == transformsToLearn[i].shape[1])
+        self.transformsToLearn = copy(transformsToLearn)
+
+
+    #defines the training set of the learner with the given datas
+    def setTrainingSet(self,
+                       datas):
+        assert(self.dim == datas.shape[1])
+        self.data = copy(datas)
+        trainset = pl.MemoryVMatrix(data=self.data,
+                                    inputsize = self.dim,
+                                    targetsize = 0,
+                                    weightsize = 0,
+                                    length = datas.shape[0],
+                                    width = datas.shape[1])
+        self.learner.setTrainingSet(trainset,True)
+        self.updateLearnedParameters()
+        if(not self.learnTransformDistribution):
+            self.updateTransformDistribution()
+        
+
+
+    #gets the current value of the learner's transformations , noise variance(optional)
+    #and transformation distribution(optional)
+    def updateLearnedParameters(self):
+        self.updateTransforms()
+        if(self.learnNoiseVariance):
+            self.updateNoiseVariance()
+        if(self.learnTransformDistribution):
+            self.updateTransformDistribution()
+
+    
+    #registers the "real values" of the parameters to learn
+    def setParametersToLearn(self,
+                             transformsToLearn,
+                             biasToLearn=array([]),
+                             noiseVarianceToLearn = -1,
+                             transformDistributionToLearn = []):
+        self.setTransformsToLearn(transformsToLearn, biasToLearn)
+        if(self.learnNoiseVariance):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        if(self.learnTransformDistribution):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)
+        
+        
+
+    #updates the variables 'learnedTransforms' and 'learnedBias' to ensure that
+    #they correspond to the learner 's transformation matrices and bias 
+    def updateTransforms(self):
+        self.learnedTransforms = self.learner.getOption("transforms")
+        if(self.withBias):
+            self.learnedBias = self.learner.getOption("biasSet")
+
+
+    #updates the variable 'noiseVariance' to ensure it correspond to the learner's noise variance
+    def updateNoiseVariance(self):
+        self.noiseVariance = self.learner.getOption("noiseVariance")
+
+
+    #updates the variable 'transformDistribution' to ensure it correspond to the learner's transformation distribution
+    def updateTransformDistribution(self):
+        self.transformDistribution = self.learner.getOption("transformDistribution")
+
+
+    #updates reconstruction datas 
+    def updateReconstructionDatas(self):
+        self.target = self.learner.returnTrainingPoint(self.testTargetIdx)
+        temp = self.learner.returnReconstructionCandidates(self.testTargetIdx)
+        reconstructionCandidates = array(temp, 'd')
+        reconstructionCandidates.resize(len(temp),4)
+        self.weights = copy(reconstructionCandidates[:,WEIGHT])
+        for i in range(self.weights.shape[0]):
+            self.weights[i]=exp(self.weights[i])
+            self.recSizes = (multiply(self.weights,self.MAX_SIZE - self.MIN_SIZE)
+                             +
+                             self.MIN_SIZE)
+        self.choosenTransforms = copy(reconstructionCandidates[:,TRANSFORM_IDX])
+        self.neighbors = self.learner.returnNeighbors(self.testTargetIdx)
+        self.reconstructions = self.learner.returnReconstructions(self.testTargetIdx)
+        
+
+    #extracts the data points from a file and returns them
+    #(in a matricial form)
+    def load_data(self,
+                  filename):
+        data_in = open(filename)
+        (inputsize, targetsize, weightsize) = [int(x) for x in data_in.readline().split()]
+        assert(targetsize == 0)
+        assert(weightsize == 0)
+        data = []
+        n_samples = 0
+        for line in data_in.readlines():
+            data +=[float(x) for x in line.split()]
+            n_samples = n_samples + 1
+        data_in.close()
+        data = array(data, 'd')
+        data.resize(n_samples,inputsize)
+        return data
+
+
+    #sets the training set (extracts first the data points from a file)
+    def setTrainingSetFromFile(self,
+                               filename):
+        self.setTrainingSet(self.load_data(filename))
+
+
+    #re-initializes the present object    
+    def reset(self,
+              dim = 2,
+              testTargetIdx = 0,
+              transformsToLearn=[],
+              biasToLearn=array([]),
+              noiseVarianceToLearn=UNDEFINED,
+              transformDistributionToLearn =[],
+              data=array([])):
+        self.__init__(self.learner,
+                      dim,
+                      testTargetIdx,
+                      transformsToLearn,
+                      biasToLearn,
+                      noiseVarianceToLearn,
+                      transformDistributionToLearn,
+                      data)
+
+
+    #GRAPHICAL PROCEDURES
+    
+    #draws the target test point
+    #(big blue dot)
+    def drawTarget(self):
+        scatter([self.target[0]],
+                [self.target[1]],
+                [self.TARGET_SIZE],
+                c=self.TARGET_COLOR,
+                marker = self.TARGET_SHAPE)
+
+
+    #draws the neighbors associated to the reconstruction candidates
+    #(small red dots)
+    def drawNeighbors(self):
+        scatter(self.neighbors[:,0].tolist(),
+                self.neighbors[:,1].tolist(),
+                self.NEIGHBOR_SIZE,
+                c =  self.NEIGHBOR_COLOR,
+                marker=self.NEIGHBOR_SHAPE)
+
+    #draws all the training data points
+    #(small black circles)
+    def drawTrainingSet(self):
+        scatter(self.data[:,0].tolist(),
+                self.data[:,1].tolist(),
+                self.DEFAULT_SIZE,
+                c = self.DEFAULT_COLOR,
+                marker = self.DEFAULT_SHAPE)
+
+    #draws the reconstructions of the test target point
+    #(yellow diamonds, the most probable the reconstruction, the bigger the diamond shape)
+    def drawReconstructions(self):
+        scatter(self.reconstructions[:,0].tolist(),
+                self.reconstructions[:,1].tolist(),
+                self.recSizes.tolist(),
+                c=self.RECONSTRUCTION_COLOR,
+                marker=self.RECONSTRUCTION_SHAPE)
+
+        
+    #draws the transformations associated to the reconstruction candidates
+    #   -each transformation is represented as an arrow, and an integer
+    #    (the transformation index)
+    def drawChoosenTransforms(self):
+        for i in range(self.reconstructions.shape[0]):
+            mid_X = 0.5*(self.neighbors[i][0] + self.reconstructions[i][0])
+            mid_Y = 0.5*(self.neighbors[i][1] + self.reconstructions[i][1])
+            label = str(int(self.choosenTransforms[i]))
+            text(mid_X,
+                 mid_Y,
+                 label,
+                 fontsize=self.TRANSFORMATION_DIGIT_SIZE)
+            plot([self.neighbors[i][0],self.reconstructions[i][0]],
+                 [self.neighbors[i][1],self.reconstructions[i][1]],
+                 c=self.TRANSFORMATION_COLOR)
+
+
+
+    # draws the graph representing the different reconstruction candidates
+    # of the test  target point
+    def drawLearningGraph(self):
+        clf()
+        self.updateReconstructionDatas()
+        self.drawTrainingSet()
+        self.drawTarget()
+        self.drawNeighbors()
+        self.drawChoosenTransforms()
+        self.drawReconstructions()
+        draw()
+      
+
+    #CONTROL PROCEDURES
+
+
+    #prints the list of control keys
+    def help(self):
+        print "CONTROL PROCEDURES AND CORRESPONDING KEYS:\n"
+        
+        print "printLearnState() ........... 'p'"
+        print "initEStep() ................. 'i'"
+        print "smallEStep() ................ 's'"
+        print "largeEStepA() ............... 'a'"
+        print "largeEStepB() ............... 'b'"
+        print "MStep() ..................... 'm'"
+        print "MStepTransformations() ...... 't'"
+        print "MStepTransformationsDiv() ... 'y'"
+        print "MStepNoiseVariance() ........ 'n'"
+        print "MStepTransformDistribution()  'd'"
+        print "printReconstructionsProbas()  'r'"
+        print "printTransforms() ........... 'z'"
+        print "printNoiseVariance() ........ 'x'"
+        print "printDistribution() ......... 'c'"
+        print "nextStage() ................. ' '"
+        print "routine1() .................. '1'"
+        print "help() ...................... 'h'"
+        
+
+
+
+    r1_NB_ITERATIONS = 1
+    def routine1(self):
+        for i in range(self.r1_NB_ITERATIONS):
+            self.MStep()
+            self.smallEStep()
+            self.nextStage()
+        self.printLearnState()
+    
+
+    #prints the learned parameters and the values of the parameters to learn
+    #(control key == 'p')
+    def printLearnState(self): 
+        self.updateLearnedParameters()
+        n = len(self.transformsToLearn)
+        print "transformations to learn:"
+        for i in range(n):
+            print "\n"
+            print self.transformsToLearn[i]
+            if(self.withBias):
+                print "bias: ", self.biasToLearn[i,:]
+        print "\n"
+        print "learned transformations:"
+        for i in range(n):
+            print "\n"
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print "bias: ", self.learnedBias[i,:]
+        print "\n"
+        if(self.learnNoiseVariance):
+            print "noise variance to learn:  ", self.noiseVarianceToLearn
+            print "learned noise variance: " , self.noiseVariance
+            print "\n"
+        if(self.learnTransformDistribution):
+            print "transformation distribution to learn, format =(log,proba):"
+            print [["(",x," ", exp(x),")"] for x in self.transformDistributionToLearn]
+            print "learned transformation distribution (log/proba) :"
+            print [(x, exp(x)) for x in self.transformDistribution]
+            print "\n"
+            
+
+
+    #initEStep  (control key == 'i')
+    def initEStep(self):
+        assert(len(self.data!= 0))
+        #print "** initEStep **"
+        self.learner.initEStep()
+        self.updateTransforms()
+    
+        
+    #smallEStep   (control key == 's')
+    def smallEStep(self):   
+        #print "** smallEStep **"
+        self.learner.smallEStep()
+        self.printReconstructionsProbas()
+
+
+    #largeEStepA   (control key == 'a')
+    def largeEStepA(self):
+        #print "** largeEStepA **"
+        self.learner.largeEStepA()
+        self.printReconstructionsProbas()
+     
+
+    #largeEStepB   (control key == 'b')
+    def largeEStepB(self):
+        #print "** largeEStepB **"
+        self.learner.largeEStepB()
+        self.printReconstructionsProbas()
+
+
+    #MStep (control key == 'm')
+    def MStep(self):
+        #print "** MStep **"
+        self.learner.MStep()
+        #self.printLearnState()
+
+    #MStepTransformations (control key == 'T')
+    def MStepTransformations(self):
+        #print "** MStepTransformations **"
+        self.learner.MStepTransformations()
+        self.printTransforms()
+
+    mstd_t = 0
+    #MStepTransformationDiv (control key == 'H')
+    def MStepTransformationDiv(self):
+        #print "** MStepTransformations**"
+        #print "transform: ", mstd_t
+        self.learner.MStepTransformationDiv(self.mstd_t)
+        self.mstd_t = (self.mstd_t + 1) % self.nbTransforms
+
+    #MStepNoiseVariance (control key == 'N')
+    def MStepNoiseVariance(self):
+        #print "** MStepNoiseVariance **"
+        self.learner.MStepNoiseVariance()
+        self.printNoiseVariance()
+
+    #MStepDistribution (control key == 'D')
+    def MStepTransformDistribution(self):
+        #print "** MStepDistribution **"
+        self.learner.MStepTransformDistribution()
+        self.printLearnState()
+
+    #prints the  probabilities of the reconstructions associated to the present test target point
+    #(control key == r)
+    def printReconstructionsProbas(self):
+        self.updateReconstructionDatas()
+        print "reconstructions and their weights (weight format : (log,proba))"
+        for i in range(self.reconstructions.shape[0]):
+            print  self.reconstructions[i], " (" , log(self.weights[i]),", ", self.weights[i], ")"
+        print "\n"
+    
+    #prints the current learned transformations (control key == 't')
+    def printTransforms(self):
+        self.updateLearnedParameters()
+        print "current transformations:\n"
+        for i in range(self.nbTransforms):
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print self.learnedBias[i,:]
+            print "\n"
+        
+    #prints the value of the learner's noise variance (control key == 'n')
+    def printNoiseVariance(self):
+        self.updateLearnedParameters()
+        print "noise variance: " , self.noiseVariance , "\n"
+        
+
+    #prints the value of the learner's transformation distribution
+    #(control key == 'd')
+    def printDistribution(self):
+        self.updateLearnedParameters()
+        print "transformation distribution (log, proba): \n"
+        print [(x, exp(x)) for x in self.transformDistribution ]
+        print "\n"
+
+
+    #increment the learner variable 'stages' of 1 (control key == 'n')
+    def nextStage(self):
+        self.learner.nextStage()
+
+
+    #GENERAL USE PROCEDURES
+
+
+    #returns the square euclidean distance between data points x and y 
+    def squareEuclideanDistance(self,x,y):
+        return pow(x[0] - y[0],2) + pow(x[1] - y[1],2)   
+    
+
+    #RUN PROCEDURE (main)
+
+    i=1
+    def run(self):
+        self.learner.buildLearnedParameters()
+        self.initEStep()
+        self.drawLearningGraph()
+        self.i = 1
+        def mouse_press(event):
+            if(event.button == 2):
+                p = [event.xdata,event.ydata]
+                min_idx = 0
+                min_d = self.squareEuclideanDistance(self.data[0],p)
+                for i in range(1,len(self.data)):
+                    d = self.squareEuclideanDistance(self.data[i],p)
+                    if(d< min_d):
+                        min_d = d
+                        min_idx = i
+                self.testTargetIdx = min_idx
+                self.target = self.data[min_idx]
+                print "new target: ", min_idx, "\n"
+                self.drawLearningGraph()
+        
+        def key_press(event):
+            if(event.key == '1'):
+                self.routine1()
+            if(event.key == 'p' ):
+                self.printLearnState()
+            if(event.key == 'i'):
+                self.initEStep()
+            if(event.key == 's'):
+                self.smallEStep()
+            if(event.key == 'a'):
+                self.largeEStepA()
+            if(event.key == 'b'):
+
+                self.largeEStepB()
+            if(event.key == 'm'):
+                self.MStep()
+            if(event.key == 't' ):
+                self.MStepTransformations()
+            if(event.key == 'n'):
+                self.MStepNoiseVariance()
+            if(event.key == 'd'):
+                self.MStepTransformDistribution()
+            if(event.key == 'y'):
+                self.MStepTransformationDiv()
+            if(event.key == 'r'):
+                self.printReconstructionsProbas()
+            if(event.key == 'z'):
+                self.printTransforms()
+            if(event.key == 'x'):
+                self.printNoiseVariance()
+            if(event.key == 'c' ):
+                self.printDistribution()
+            if(event.key == 'h'):
+                self.help()
+            if(event.key == ' '):
+                self.nextStage()
+            self.drawLearningGraph()
+            
+        connect('button_press_event',mouse_press)
+        connect('key_press_event', key_press)
+        show()   

Added: trunk/scripts/EXPERIMENTAL/itest.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/itest.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/itest.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,770 @@
+import os ,sys, time, matplotlib, math, copy
+
+from matplotlib.pylab import *
+from matplotlib.colors import *
+from numpy.numarray import *
+# from numarray import *
+# from numarray.linear_algebra import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from copy import *
+
+from iTraining import *
+from generators import *
+from TLTester import *
+#exec open("iTraining.py").read()
+#exec open("generators.py").read()
+#exec open("TLTester.py").read()
+
+server_command = 'plearn_exp server'
+serv = launch_plearn_server(command = server_command)
+
+#iTraining2.py
+#to control interactively the training process
+
+import os, sys, time, matplotlib, math, numpy ,copy
+
+from matplotlib.pylab import plot,show,draw,close,text,scatter,colorbar
+from matplotlib.colors import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from numpy import *
+from pickle import *
+from copy import *
+
+
+
+
+#RECONSTRUCTION CANDIDATE
+
+#consists in a 4-element tuple:
+#   (target, target<s neighbor, transformation, weight)
+TARGET_IDX = 0
+NEIGHBOR_IDX = 1
+TRANSFORM_IDX = 2
+WEIGHT = 3
+
+
+#OTHER CONSTANTS
+DEFAULT = 0
+UNDEFINED = -1
+EMPTY_BIAS = TMat()
+
+#ITERATIVE LEARNER ----------------------------------------------------------
+class IterativeLearner(object):
+
+
+    #GRAPHICAL OUTPUT FORMAT (constants)
+
+    #a data point is represented graphically by a dot
+    #    his size/shape/color might change according to his nature:
+    #          -target ?
+    #          -neighbor ?
+    #          -ordinary training point ?
+    #          -reconstruction ?
+    #we also need some formats to draw the transformations
+
+    #sizes
+    MIN_SIZE = 10.0
+    MAX_SIZE = 100.0
+    TARGET_SIZE = MAX_SIZE
+    NEIGHBOR_SIZE = MIN_SIZE
+    DEFAULT_SIZE = MIN_SIZE
+    RECONSTRUCTION_SIZE_FACTOR = 1
+
+    #colors
+    TARGET_COLOR = 'b'
+    NEIGHBOR_COLOR = 'k'
+    DEFAULT_COLOR = 'w'
+    RECONSTRUCTION_COLOR = 'y'
+    TRANSFORMATION_COLOR  = 'k'
+
+    #shapes
+    TARGET_SHAPE = 'o'
+    NEIGHBOR_SHAPE = 'o'
+    DEFAULT_SHAPE = 'o'
+    RECONSTRUCTION_SHAPE = 'd'
+
+    #index of a transformation : size of the police
+    TRANSFORMATION_DIGIT_SIZE = 12
+
+    #LEARNER
+    
+    learner = UNDEFINED                    #TransformationLearner object
+    dim = 2                                #dimension of input space
+    nbTransforms = UNDEFINED               #number  of transforms to learn
+    withBias = False                       #includes a bias addition in the transformation function?
+    transformsToLearn = []                 #the transformations matrices to learn
+    biasToLearn = EMPTY_BIAS                #the transformations bias to learn, if any
+    learnedTransforms = []                 #learned transformations matrices
+    learnedBias = EMPTY_BIAS                #learned transformations bias, if any
+    learnNoiseVariance = False             #noise variance = learned parameter ?
+    noiseVariance = UNDEFINED              #noise variance (fixed or learned)
+    noiseVarianceToLearn=UNDEFINED         #noise variance to learn (if any)
+    learnTransformDistribution = False     #transformation distribution = learned parameter ?
+    transformDistribution = []             #transformation distribution (fixed or learned)
+    transformDistributionToLearn = []      #transformation distribution to learn (if any)
+    data = array([])                       #training data points
+    transformFamily=0                      #type of transformation functions used
+    
+
+    #TARGET AND CORRESPONDING RECONSTRUCTION CANDIDATES
+    testTargetIdx = 0
+    target = []
+    neighbors = array([])
+    reconstructions= array([])
+    weights = array([])
+    recSizes = array([])
+    recColors = array([])
+    choosenTransforms = array([])
+
+
+    #INITIALIZATION PROCEDURES
+
+    #constructor
+    def __init__(self,
+                 learner,
+                 dim=2,
+                 testTargetIdx=0,
+                 transformsToLearn=[],
+                 bias=array([]),
+                 noiseVarianceToLearn=UNDEFINED,
+                 transformDistributionToLearn=[],
+                 data=array([])):
+        self.learner = learner
+        self.dim = dim
+        self.nbTransforms = self.learner.getOption("nbTransforms")
+        self.withBias = self.learner.getOption("withBias")
+        self.transformFamily = self.learner.getOption("transformFamily")
+        self.testTargetIdx = testTargetIdx
+        self.learnNoiseVariance = self.learner.getOption("learnNoiseVariance")
+        if(self.learnNoiseVariance and noiseVarianceToLearn>0):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        else:
+            self.updateNoiseVariance()
+        self.learnTransformDistribution = self.learner.getOption("learnTransformDistribution")
+        if(self.learnTransformDistribution and len(transformDistributionToLearn) >0):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)        
+        if(len(transformsToLearn) != 0):
+            self.setTransformsToLearn(transformsToLearn, bias)
+        if(len(data)!=0):
+            self.setTrainingSet(data)
+
+
+    def setNoiseVarianceToLearn(self,
+                                noiseVarianceToLearn):
+        assert(noiseVarianceToLearn>0)
+        self.noiseVarianceToLearn = noiseVarianceToLearn
+
+    def setTransformDistributionToLearn(self,
+                                        transformDistributionToLearn):
+        assert(len(transformDistributionToLearn) == self.nbTransforms)
+        sum = 0
+        for i in range(self.nbTransforms):
+            p = exp(transformDistributionToLearn[i])
+            assert( 0<= p <=1)
+            sum = sum + p
+        assert(sum == 1)
+        self.transformDistributionToLearn = copy(transformDistributionToLearn)
+        
+        
+
+    #specifies the set of transformation functions that might be learned        
+    def setTransformsToLearn(self,
+                             transformsToLearn,
+                             bias=EMPTY_BIAS):
+        assert(len(transformsToLearn)==self.nbTransforms)
+        if(self.withBias):
+            assert(bias.shape[0] == self.nbTransforms)
+            assert(bias.shape[1] == self.dim)
+            self.biasToLearn= bias.copy()
+        else:
+            self.biasToLearn = EMPTY_BIAS
+        for i in range(self.nbTransforms):
+            assert(self.dim == transformsToLearn[i].shape[0])
+            assert(self.dim == transformsToLearn[i].shape[1])
+        self.transformsToLearn = copy(transformsToLearn)
+
+
+    #defines the training set of the learner with the given datas
+    def setTrainingSet(self,
+                       datas):
+        assert(self.dim == datas.shape[1])
+        self.data = copy(datas)
+        trainset = pl.MemoryVMatrix(data=self.data,
+                                    inputsize = self.dim,
+                                    targetsize = 0,
+                                    weightsize = 0,
+                                    length = datas.shape[0],
+                                    width = datas.shape[1])
+        self.learner.setTrainingSet(trainset,True)
+        self.updateLearnedParameters()
+        if(not self.learnTransformDistribution):
+            self.updateTransformDistribution()
+        
+
+
+    #gets the current value of the learner's transformations , noise variance(optional)
+    #and transformation distribution(optional)
+    def updateLearnedParameters(self):
+        self.updateTransforms()
+        if(self.learnNoiseVariance):
+            self.updateNoiseVariance()
+        if(self.learnTransformDistribution):
+            self.updateTransformDistribution()
+
+    
+    #registers the "real values" of the parameters to learn
+    def setParametersToLearn(self,
+                             transformsToLearn,
+                             biasToLearn=array([]),
+                             noiseVarianceToLearn = -1,
+                             transformDistributionToLearn = []):
+        self.setTransformsToLearn(transformsToLearn, biasToLearn)
+        if(self.learnNoiseVariance):
+            self.setNoiseVarianceToLearn(noiseVarianceToLearn)
+        if(self.learnTransformDistribution):
+            self.setTransformDistributionToLearn(transformDistributionToLearn)
+        
+        
+
+    #updates the variables 'learnedTransforms' and 'learnedBias' to ensure that
+    #they correspond to the learner 's transformation matrices and bias 
+    def updateTransforms(self):
+        self.learnedTransforms = self.learner.getOption("transforms")
+        if(self.withBias):
+            self.learnedBias = self.learner.getOption("biasSet")
+
+
+    #updates the variable 'noiseVariance' to ensure it correspond to the learner's noise variance
+    def updateNoiseVariance(self):
+        self.noiseVariance = self.learner.getOption("noiseVariance")
+
+
+    #updates the variable 'transformDistribution' to ensure it correspond to the learner's transformation distribution
+    def updateTransformDistribution(self):
+        self.transformDistribution = self.learner.getOption("transformDistribution")
+
+
+    #updates reconstruction datas 
+    def updateReconstructionDatas(self):
+        self.target = self.learner.returnTrainingPoint(self.testTargetIdx)
+        temp = self.learner.returnReconstructionCandidates(self.testTargetIdx)
+        reconstructionCandidates = array(temp, 'd')
+        reconstructionCandidates.resize(len(temp),4)
+        self.weights = copy(reconstructionCandidates[:,WEIGHT])
+        for i in range(self.weights.shape[0]):
+            self.weights[i]=exp(self.weights[i])
+            self.recSizes = (multiply(self.weights,self.MAX_SIZE - self.MIN_SIZE)
+                             +
+                             self.MIN_SIZE)
+        self.choosenTransforms = copy(reconstructionCandidates[:,TRANSFORM_IDX])
+        self.neighbors = self.learner.returnNeighbors(self.testTargetIdx)
+        self.reconstructions = self.learner.returnReconstructions(self.testTargetIdx)
+        
+
+    #extracts the data points from a file and returns them
+    #(in a matricial form)
+    def load_data(self,
+                  filename):
+        data_in = open(filename)
+        (inputsize, targetsize, weightsize) = [int(x) for x in data_in.readline().split()]
+        assert(targetsize == 0)
+        assert(weightsize == 0)
+        data = []
+        n_samples = 0
+        for line in data_in.readlines():
+            data +=[float(x) for x in line.split()]
+            n_samples = n_samples + 1
+        data_in.close()
+        data = array(data, 'd')
+        data.resize(n_samples,inputsize)
+        return data
+
+
+    #sets the training set (extracts first the data points from a file)
+    def setTrainingSetFromFile(self,
+                               filename):
+        self.setTrainingSet(self.load_data(filename))
+
+
+    #re-initializes the present object    
+    def reset(self,
+              dim = 2,
+              testTargetIdx = 0,
+              transformsToLearn=[],
+              biasToLearn=array([]),
+              noiseVarianceToLearn=UNDEFINED,
+              transformDistributionToLearn =[],
+              data=array([])):
+        self.__init__(self.learner,
+                      dim,
+                      testTargetIdx,
+                      transformsToLearn,
+                      biasToLearn,
+                      noiseVarianceToLearn,
+                      transformDistributionToLearn,
+                      data)
+
+
+    #GRAPHICAL PROCEDURES
+    
+    #draws the target test point
+    #(big blue dot)
+    def drawTarget(self):
+        scatter([self.target[0]],
+                [self.target[1]],
+                [self.TARGET_SIZE],
+                c=self.TARGET_COLOR,
+                marker = self.TARGET_SHAPE)
+
+
+    #draws the neighbors associated to the reconstruction candidates
+    #(small red dots)
+    def drawNeighbors(self):
+        scatter(self.neighbors[:,0].tolist(),
+                self.neighbors[:,1].tolist(),
+                self.NEIGHBOR_SIZE,
+                c =  self.NEIGHBOR_COLOR,
+                marker=self.NEIGHBOR_SHAPE)
+
+    #draws all the training data points
+    #(small black circles)
+    def drawTrainingSet(self):
+        scatter(self.data[:,0].tolist(),
+                self.data[:,1].tolist(),
+                self.DEFAULT_SIZE,
+                c = self.DEFAULT_COLOR,
+                marker = self.DEFAULT_SHAPE)
+
+    #draws the reconstructions of the test target point
+    #(yellow diamonds, the most probable the reconstruction, the bigger the diamond shape)
+    def drawReconstructions(self):
+        scatter(self.reconstructions[:,0].tolist(),
+                self.reconstructions[:,1].tolist(),
+                self.recSizes.tolist(),
+                c=self.RECONSTRUCTION_COLOR,
+                marker=self.RECONSTRUCTION_SHAPE)
+
+        
+    #draws the transformations associated to the reconstruction candidates
+    #   -each transformation is represented as an arrow, and an integer
+    #    (the transformation index)
+    def drawChoosenTransforms(self):
+        for i in range(self.reconstructions.shape[0]):
+            mid_X = 0.5*(self.neighbors[i][0] + self.reconstructions[i][0])
+            mid_Y = 0.5*(self.neighbors[i][1] + self.reconstructions[i][1])
+            label = str(int(self.choosenTransforms[i]))
+            text(mid_X,
+                 mid_Y,
+                 label,
+                 fontsize=self.TRANSFORMATION_DIGIT_SIZE)
+            plot([self.neighbors[i][0],self.reconstructions[i][0]],
+                 [self.neighbors[i][1],self.reconstructions[i][1]],
+                 c=self.TRANSFORMATION_COLOR)
+
+
+
+    # draws the graph representing the different reconstruction candidates
+    # of the test  target point
+    def drawLearningGraph(self):
+        clf()
+        self.updateReconstructionDatas()
+        self.drawTrainingSet()
+        self.drawTarget()
+        self.drawNeighbors()
+        self.drawChoosenTransforms()
+        self.drawReconstructions()
+        draw()
+      
+
+    #CONTROL PROCEDURES
+
+
+    #prints the list of control keys
+    def help(self):
+        print "CONTROL PROCEDURES AND CORRESPONDING KEYS:\n"
+        
+        print "printLearnState() ........... 'p'"
+        print "initEStep() ................. 'i'"
+        print "smallEStep() ................ 's'"
+        print "largeEStepA() ............... 'a'"
+        print "largeEStepB() ............... 'b'"
+        print "MStep() ..................... 'm'"
+        print "MStepTransformations() ...... 't'"
+        print "MStepTransformationsDiv() ... 'y'"
+        print "MStepNoiseVariance() ........ 'n'"
+        print "MStepTransformDistribution()  'd'"
+        print "printReconstructionsProbas()  'r'"
+        print "printTransforms() ........... 'z'"
+        print "printNoiseVariance() ........ 'x'"
+        print "printDistribution() ......... 'c'"
+        print "nextStage() ................. ' '"
+        print "routine1() .................. '1'"
+        print "help() ...................... 'h'"
+        
+
+
+
+    r1_NB_ITERATIONS = 1
+    def routine1(self):
+        for i in range(self.r1_NB_ITERATIONS):
+            self.MStep()
+            self.smallEStep()
+            self.nextStage()
+        self.printLearnState()
+    
+
+    #prints the learned parameters and the values of the parameters to learn
+    #(control key == 'p')
+    def printLearnState(self): 
+        self.updateLearnedParameters()
+        n = len(self.transformsToLearn)
+        print "transformations to learn:"
+        for i in range(n):
+            print "\n"
+            print self.transformsToLearn[i]
+            if(self.withBias):
+                print "bias: ", self.biasToLearn[i,:]
+        print "\n"
+        print "learned transformations:"
+        for i in range(n):
+            print "\n"
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print "bias: ", self.learnedBias[i,:]
+        print "\n"
+        if(self.learnNoiseVariance):
+            print "noise variance to learn:  ", self.noiseVarianceToLearn
+            print "learned noise variance: " , self.noiseVariance
+            print "\n"
+        if(self.learnTransformDistribution):
+            print "transformation distribution to learn, format =(log,proba):"
+            print [["(",x," ", exp(x),")"] for x in self.transformDistributionToLearn]
+            print "learned transformation distribution (log/proba) :"
+            print [(x, exp(x)) for x in self.transformDistribution]
+            print "\n"
+            
+
+
+    #initEStep  (control key == 'i')
+    def initEStep(self):
+        assert(len(self.data!= 0))
+        #print "** initEStep **"
+        self.learner.initEStep()
+        self.updateTransforms()
+    
+        
+    #smallEStep   (control key == 's')
+    def smallEStep(self):   
+        #print "** smallEStep **"
+        self.learner.smallEStep()
+        self.printReconstructionsProbas()
+
+
+    #largeEStepA   (control key == 'a')
+    def largeEStepA(self):
+        #print "** largeEStepA **"
+        self.learner.largeEStepA()
+        self.printReconstructionsProbas()
+     
+
+    #largeEStepB   (control key == 'b')
+    def largeEStepB(self):
+        #print "** largeEStepB **"
+        self.learner.largeEStepB()
+        self.printReconstructionsProbas()
+
+
+    #MStep (control key == 'm')
+    def MStep(self):
+        #print "** MStep **"
+        self.learner.MStep()
+        #self.printLearnState()
+
+    #MStepTransformations (control key == 'T')
+    def MStepTransformations(self):
+        #print "** MStepTransformations **"
+        self.learner.MStepTransformations()
+        self.printTransforms()
+
+    mstd_t = 0
+    #MStepTransformationDiv (control key == 'H')
+    def MStepTransformationDiv(self):
+        #print "** MStepTransformations**"
+        #print "transform: ", mstd_t
+        self.learner.MStepTransformationDiv(self.mstd_t)
+        self.mstd_t = (self.mstd_t + 1) % self.nbTransforms
+
+    #MStepNoiseVariance (control key == 'N')
+    def MStepNoiseVariance(self):
+        #print "** MStepNoiseVariance **"
+        self.learner.MStepNoiseVariance()
+        self.printNoiseVariance()
+
+    #MStepDistribution (control key == 'D')
+    def MStepTransformDistribution(self):
+        #print "** MStepDistribution **"
+        self.learner.MStepTransformDistribution()
+        self.printLearnState()
+
+    #prints the  probabilities of the reconstructions associated to the present test target point
+    #(control key == r)
+    def printReconstructionsProbas(self):
+        self.updateReconstructionDatas()
+        print "reconstructions and their weights (weight format : (log,proba))"
+        for i in range(self.reconstructions.shape[0]):
+            print  self.reconstructions[i], " (" , log(self.weights[i]),", ", self.weights[i], ")"
+        print "\n"
+    
+    #prints the current learned transformations (control key == 't')
+    def printTransforms(self):
+        self.updateLearnedParameters()
+        print "current transformations:\n"
+        for i in range(self.nbTransforms):
+            print self.learnedTransforms[i]
+            if(self.withBias):
+                print self.learnedBias[i,:]
+            print "\n"
+        
+    #prints the value of the learner's noise variance (control key == 'n')
+    def printNoiseVariance(self):
+        self.updateLearnedParameters()
+        print "noise variance: " , self.noiseVariance , "\n"
+        
+
+    #prints the value of the learner's transformation distribution
+    #(control key == 'd')
+    def printDistribution(self):
+        self.updateLearnedParameters()
+        print "transformation distribution (log, proba): \n"
+        print [(x, exp(x)) for x in self.transformDistribution ]
+        print "\n"
+
+
+    #increment the learner variable 'stages' of 1 (control key == 'n')
+    def nextStage(self):
+        self.learner.nextStage()
+
+
+    #GENERAL USE PROCEDURES
+
+
+    #returns the square euclidean distance between data points x and y 
+    def squareEuclideanDistance(self,x,y):
+        return pow(x[0] - y[0],2) + pow(x[1] - y[1],2)   
+    
+
+    #RUN PROCEDURE (main)
+
+    i=1
+    def run(self):
+        self.learner.buildLearnedParameters()
+        self.initEStep()
+        self.drawLearningGraph()
+        self.i = 1
+        def mouse_press(event):
+            if(event.button == 2):
+                p = [event.xdata,event.ydata]
+                min_idx = 0
+                min_d = self.squareEuclideanDistance(self.data[0],p)
+                for i in range(1,len(self.data)):
+                    d = self.squareEuclideanDistance(self.data[i],p)
+                    if(d< min_d):
+                        min_d = d
+                        min_idx = i
+                self.testTargetIdx = min_idx
+                self.target = self.data[min_idx]
+                print "new target: ", min_idx, "\n"
+                self.drawLearningGraph()
+        
+        def key_press(event):
+            if(event.key == '1'):
+                self.routine1()
+            if(event.key == 'p' ):
+                self.printLearnState()
+            if(event.key == 'i'):
+                self.initEStep()
+            if(event.key == 's'):
+                self.smallEStep()
+            if(event.key == 'a'):
+                self.largeEStepA()
+            if(event.key == 'b'):
+
+                self.largeEStepB()
+            if(event.key == 'm'):
+                self.MStep()
+            if(event.key == 't' ):
+                self.MStepTransformations()
+            if(event.key == 'n'):
+                self.MStepNoiseVariance()
+            if(event.key == 'd'):
+                self.MStepTransformDistribution()
+            if(event.key == 'y'):
+                self.MStepTransformationDiv()
+            if(event.key == 'r'):
+                self.printReconstructionsProbas()
+            if(event.key == 'z'):
+                self.printTransforms()
+            if(event.key == 'x'):
+                self.printNoiseVariance()
+            if(event.key == 'c' ):
+                self.printDistribution()
+            if(event.key == 'h'):
+                self.help()
+            if(event.key == ' '):
+                self.nextStage()
+            self.drawLearningGraph()
+            
+        connect('button_press_event',mouse_press)
+        connect('key_press_event', key_press)
+        show()   
+DEFAULT = 0
+BEHAVIOR_LEARNER = 0
+BEHAVIOR_GENERATOR = 1
+
+DEFAULT_DIM = 2
+
+
+generator_TRANSFORMS = []
+generator_BIAS_SET = array([])
+generator_NOISE_VARIANCE = 0.0001
+generator_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a tree generator
+generator_MODE_TREE = 0
+generator_ROOT = [] #[1,1]
+generator_DEEPNESS = 3
+generator_BRANCHING_FACTOR = 3
+generator_TREE_TRANSFORMS = []
+generator_TREE_BIAS_SET = array([])
+generator_TREE_NOISE_VARIANCE = 0.00001
+generator_TREE_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a sequential generator
+generator_MODE_SEQUENTIAL = 1
+generator_START = [1,1]
+generator_SEQUENCE_LENGTH = 40
+
+
+#parameters of a circle generator
+generator_MODE_CIRCLE = 2
+generator_CENTER = [0,0]
+generator_NB_CIRCLE_POINTS = 40
+generator_RAY = 10
+
+#parameters of a spiral generator
+generator_MODE_SPIRAL = 3
+generator_SPIRAL_ROOT = [1,1]
+generator_ALPHA = 1.01
+generator_THETA = 0.1
+generator_NB_SPIRAL_POINTS = 40
+
+
+
+#We suppose that the noisePrecision follows a gamma distribution
+#with parameters alpha, and beta
+#      (reminds that noisePrecision = 1/noiseVariance)
+#
+#Accorging to that distribution:
+#          E(noisePrecision) = alpha/beta
+#          Var(noisePrecision)=alpha/(beta^2)
+#
+#Given those 2 last values, we can deduce the value of alpha and beta:
+#
+#          beta = mean/var
+#          alpha = (mean^2)/var
+#
+#It is what the following procedure is doing:
+# -find alpha and beta,
+# and then returns them in a pair
+def computeNoiseVarianceParameters(mean,var):
+    beta = (1.0*mean)/var
+    alpha = mean*beta
+    return [alpha,beta]
+
+
+
+
+ALPHA = 0
+BETA = 1
+#learner_NOISE_PRECISION_MEAN = 1.0/0.0001
+#learner_NOISE_PRECISION_VAR = 1.0
+#learner_NOISE_DISTRIBUTION_PARAMETERS = (UNDEFINED,UNDEFINED)
+#if(learner_NOISE_PRECISION_MEAN > 0 and learner_NOISE_PRECISION_VAR > 0):
+#   learner_NOISE_DISTRIBUTION_PARAMETERS = computeNoiseVarianceParameters(learner_NOISE_PRECISION_MEAN,
+#                                                                           learner_NOISE_PRECISION_VAR)
+#print "noise distribution parameters: "
+#print learner_NOISE_DISTRIBUTION_PARAMETERS
+learnerSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_LEARNER,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    learnNoiseVariance = True,
+    regOnNoiseVariance = False,
+    emphasisOnDiversity=True,
+    diversityFactor=0.25,
+    noiseAlpha =  1,
+    noiseBeta = 1,
+    learnTransformDistribution = False,
+    regOnTransformDistribution = False,
+    transformDistributionAlpha = 2,
+    noiseVariance =3.,
+    transformsVariance =4.0 ,
+    nbTransforms = 2,
+    nbNeighbors = 2,
+    initializationMode = DEFAULT)
+learner = serv.new(learnerSpec)
+iLearner = IterativeLearner(learner)
+
+
+generatorSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_GENERATOR,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    noiseAlpha = UNDEFINED,
+    noiseBeta = UNDEFINED,
+    transformDistributionAlpha = UNDEFINED,
+    noiseVariance = generator_NOISE_VARIANCE,
+    transformsVariance = 1,
+    nbTransforms = 1,
+    nbNeighbors = 1)
+
+gen = serv.new(generatorSpec)
+
+
+generatorMode = generator_MODE_CIRCLE
+
+#generator = TreeGenerator(gen,
+ #                         DEFAULT_DIM,
+  #                        False,
+   #                       generator_DEEPNESS,
+    #                      generator_BRANCHING_FACTOR,
+     #                     generator_ROOT)
+#generator= SequentialGenerator(gen,
+ #                              DEFAULT_DIM,
+  #                             False,
+   #                            generator_SEQUENCE_LENGTH,
+    #                           generator_START
+     #                          )
+
+generator = CircleGenerator(gen,
+                            False,
+                            generator_NB_CIRCLE_POINTS,
+                            generator_CENTER,
+                            generator_RAY)
+#generator = SpiralGenerator(gen,
+ #                           False,
+  #                          generator_NB_SPIRAL_POINTS,
+   #                         generator_SPIRAL_ROOT,
+    #                        generator_ALPHA,
+     #                       generator_THETA)
+
+tester = TLTester(iLearner,generator)
+if(generator_NOISE_VARIANCE > 0):
+    generator.setNoiseVariance(generator_NOISE_VARIANCE)
+tester.run()

Added: trunk/scripts/EXPERIMENTAL/itest2.py
===================================================================
--- trunk/scripts/EXPERIMENTAL/itest2.py	2007-09-06 20:50:10 UTC (rev 8054)
+++ trunk/scripts/EXPERIMENTAL/itest2.py	2007-09-07 21:14:30 UTC (rev 8055)
@@ -0,0 +1,169 @@
+import os ,sys, time, matplotlib, math, copy
+
+from matplotlib.pylab import *
+from matplotlib.colors import *
+from numpy.numarray import *
+# from numarray import *
+#from numarray.linear_algebra import *
+
+from plearn.io.server import *
+from plearn.pyplearn import *
+from plearn.plotting import *
+from copy import *
+
+from iTraining import *
+from generators import *
+from TLTester import *
+#exec open("iTraining.py").read()
+#exec open("generators.py").read()
+#exec open("TLTester.py").read()
+
+server_command = 'plearn_exp server'
+serv = launch_plearn_server(command = server_command)
+
+LINEAR = 0
+LINEAR_INCREMENT = 1
+UNDEFINED = -1
+DEFAULT = 0
+BEHAVIOR_LEARNER = 0
+BEHAVIOR_GENERATOR = 1
+
+DEFAULT_DIM = 2
+
+
+generator_TRANSFORMS = []
+generator_BIAS_SET = array([])
+generator_NOISE_VARIANCE = 0.0001
+generator_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a tree generator
+generator_MODE_TREE = 0
+generator_ROOT = [] #[1,1]
+generator_DEEPNESS = 3
+generator_BRANCHING_FACTOR = 3
+generator_TREE_TRANSFORMS = []
+generator_TREE_BIAS_SET = array([])
+generator_TREE_NOISE_VARIANCE = 0.00001
+generator_TREE_TRANSFORM_DISTRIBUTION = []
+
+#parameters of a sequential generator
+generator_MODE_SEQUENTIAL = 1
+generator_START = [1,1]
+generator_SEQUENCE_LENGTH = 40
+
+
+#parameters of a circle generator
+generator_MODE_CIRCLE = 2
+generator_CENTER = [0,0]
+generator_NB_CIRCLE_POINTS = 40
+generator_RAY = 10
+
+#parameters of a spiral generator
+generator_MODE_SPIRAL = 3
+generator_SPIRAL_ROOT = [1,1]
+generator_ALPHA = 1.01
+generator_THETA = 0.1
+generator_NB_SPIRAL_POINTS = 40
+
+
+
+#We suppose that the noisePrecision follows a gamma distribution
+#with parameters alpha, and beta
+#      (reminds that noisePrecision = 1/noiseVariance)
+#
+#Accorging to that distribution:
+#          E(noisePrecision) = alpha/beta
+#          Var(noisePrecision)=alpha/(beta^2)
+#
+#Given those 2 last values, we can deduce the value of alpha and beta:
+#
+#          beta = mean/var
+#          alpha = (mean^2)/var
+#
+#It is what the following procedure is doing:
+# -find alpha and beta,
+# and then returns them in a pair
+def computeNoiseVarianceParameters(mean,var):
+    beta = (1.0*mean)/var
+    alpha = mean*beta
+    return [alpha,beta]
+
+
+
+
+ALPHA = 0
+BETA = 1
+#learner_NOISE_PRECISION_MEAN = 1.0/0.0001
+#learner_NOISE_PRECISION_VAR = 1.0
+#learner_NOISE_DISTRIBUTION_PARAMETERS = (UNDEFINED,UNDEFINED)
+#if(learner_NOISE_PRECISION_MEAN > 0 and learner_NOISE_PRECISION_VAR > 0):
+#   learner_NOISE_DISTRIBUTION_PARAMETERS = computeNoiseVarianceParameters(learner_NOISE_PRECISION_MEAN,
+#                                                                           learner_NOISE_PRECISION_VAR)
+#print "noise distribution parameters: "
+#print learner_NOISE_DISTRIBUTION_PARAMETERS
+learnerSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_LEARNER,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    learnNoiseVariance = True,
+    regOnNoiseVariance = False,
+    noiseAlpha =  1,
+    noiseBeta = 1,
+    learnTransformDistribution = False,
+    regOnTransformDistribution = False,
+    transformDistributionAlpha = 2,
+    noiseVariance =3.,
+    transformsVariance =4.0 ,
+    nbTransforms = 2,
+    nbNeighbors = 2,
+    initializationMode = DEFAULT)
+learner = serv.new(learnerSpec)
+iLearner = IterativeLearner(learner)
+
+
+generatorSpec = pl.TransformationLearner(
+    behavior = BEHAVIOR_GENERATOR,
+    transformFamily = LINEAR_INCREMENT,
+    withBias = False,
+    noiseAlpha = UNDEFINED,
+    noiseBeta = UNDEFINED,
+    transformDistributionAlpha = UNDEFINED,
+    noiseVariance = generator_NOISE_VARIANCE,
+    transformsVariance = 1,
+    nbTransforms = 1,
+    nbNeighbors = 1)
+
+gen = serv.new(generatorSpec)
+
+
+generatorMode = generator_MODE_CIRCLE
+
+#generator = TreeGenerator(gen,
+ #                         DEFAULT_DIM,
+  #                        False,
+   #                       generator_DEEPNESS,
+    #                      generator_BRANCHING_FACTOR,
+     #                     generator_ROOT)
+#generator= SequentialGenerator(gen,
+ #                              DEFAULT_DIM,
+  #                             False,
+   #                            generator_SEQUENCE_LENGTH,
+    #                           generator_START
+     #                          )
+
+generator = CircleGenerator(gen,
+                            False,
+                            generator_NB_CIRCLE_POINTS,
+                            generator_CENTER,
+                            generator_RAY)
+#generator = SpiralGenerator(gen,
+ #                           False,
+  #                          generator_NB_SPIRAL_POINTS,
+   #                         generator_SPIRAL_ROOT,
+    #                        generator_ALPHA,
+     #                       generator_THETA)
+
+tester = TLTester(iLearner,generator)
+if(generator_NOISE_VARIANCE > 0):
+    generator.setNoiseVariance(generator_NOISE_VARIANCE)
+tester.run()



From larocheh at mail.berlios.de  Sat Sep  8 00:07:34 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sat, 8 Sep 2007 00:07:34 +0200
Subject: [Plearn-commits] r8056 - trunk/plearn_learners/online
Message-ID: <200709072207.l87M7Y2L012230@sheep.berlios.de>

Author: larocheh
Date: 2007-09-08 00:07:33 +0200 (Sat, 08 Sep 2007)
New Revision: 8056

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Added decrease constant option and corrected partially supervised option.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-07 21:14:30 UTC (rev 8055)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-07 22:07:33 UTC (rev 8056)
@@ -58,6 +58,7 @@
 ///////////////////
 DeepBeliefNet::DeepBeliefNet() :
     cd_learning_rate( 0. ),
+    cd_decrease_ct( 0. ),
     grad_learning_rate( 0. ),
     batch_size( 1 ),
     grad_decrease_ct( 0. ),
@@ -97,6 +98,11 @@
                   "The learning rate used during contrastive divergence"
                   " learning");
 
+    declareOption(ol, "cd_decrease_ct", &DeepBeliefNet::cd_decrease_ct,
+                  OptionBase::buildoption,
+                  "The decrease constant of the learning rate used during"
+                  " contrastive divergence");
+
     declareOption(ol, "grad_learning_rate", &DeepBeliefNet::grad_learning_rate,
                   OptionBase::buildoption,
                   "The learning rate used during gradient descent");
@@ -104,7 +110,7 @@
     declareOption(ol, "grad_decrease_ct", &DeepBeliefNet::grad_decrease_ct,
                   OptionBase::buildoption,
                   "The decrease constant of the learning rate used during"
-                  "gradient descent");
+                  " gradient descent");
 
     declareOption(ol, "batch_size", &DeepBeliefNet::batch_size,
                   OptionBase::buildoption,
@@ -372,6 +378,10 @@
     if( partial_costs )
     {
         int n_partial_costs = partial_costs.length();
+        if( n_partial_costs != n_layers - 1)
+            PLERROR("DeepBeliefNet::build_costs() - \n"
+                    "partial_costs.length() (%d) != n_layers-1 (%d).\n",
+                    n_partial_costs, n_layers-1);
         partial_costs_indices.resize(n_partial_costs);
 
         for( int i=0; i<n_partial_costs; i++ )
@@ -807,12 +817,16 @@
             pb = new ProgressBar( "Training "+classname(),
                                   nstages - stage );
 
+        setLearningRate( grad_learning_rate );
         for( ; stage<nstages; stage++)
         {
             initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
             // Do a step every 'minibatch_size' examples.
             if (stage % minibatch_size == 0) {
                 int sample_start = stage % nsamples;
+                if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                    setLearningRate( grad_learning_rate
+                                     / (1. + grad_decrease_ct * stage ));
                 if (batch_size > 1 || minibatch_hack) {
                     train_set->getExamples(sample_start, minibatch_size,
                                            inputs, targets, weights, NULL, true);
@@ -858,6 +872,17 @@
 
             for( ; stage<end_stage ; stage++ )
             {
+                 if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                 {
+                     real lr = cd_learning_rate 
+                         / (1. + cd_decrease_ct * 
+                            (stage - cumulative_schedule[i]));
+                     
+                     layers[i]->setLearningRate( lr );
+                     connections[i]->setLearningRate( lr );
+                     layers[i+1]->setLearningRate( lr );
+                 }
+                 
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
                 // Do a step every 'minibatch_size' examples.
                 if (stage % minibatch_size == 0) {
@@ -875,7 +900,6 @@
                         train_set->getExample(sample_start, input, target, weight);
                         greedyStep( input, target, i );
                     }
-
                 }
                 if( pb )
                     pb->update( stage - cumulative_schedule[i] + 1 );
@@ -907,6 +931,15 @@
             int previous_stage = cumulative_schedule[n_layers-2];
             for( ; stage<end_stage ; stage++ )
             {
+                if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                {
+                    real lr = cd_learning_rate / 
+                        (1. + cd_decrease_ct * 
+                         (stage - cumulative_schedule[n_layers-2]));
+                    joint_layer->setLearningRate( lr );
+                    classification_module->joint_connection->setLearningRate( lr );
+                    layers[n_layers-1]->setLearningRate( lr );
+                }
                 initialize_gibbs_chain=(stage%gibbs_chain_reinit_freq==0);
                 int sample = stage % nsamples;
                 train_set->getExample( sample, input, target, weight );
@@ -949,7 +982,8 @@
 
                 if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
                     setLearningRate( grad_learning_rate
-                            / (1. + grad_decrease_ct * (stage - init_stage) ) );
+                            / (1. + grad_decrease_ct * 
+                               (stage - cumulative_schedule[n_layers-1])) );
 
                 if (minibatch_size > 1 || minibatch_hack) {
                     train_set->getExamples(sample_start, minibatch_size, inputs,
@@ -999,6 +1033,7 @@
 void DeepBeliefNet::onlineStep( const Vec& input, const Vec& target,
                                 Vec& train_costs)
 {
+    real lr;
     PLASSERT(batch_size == 1);
 
     TVec<Vec> cost;
@@ -1074,8 +1109,13 @@
 
     if (final_cost || (!partial_costs.isEmpty() && partial_costs[n_layers-2]))
     {
-        layers[n_layers-1]->setLearningRate( grad_learning_rate );
-        connections[n_layers-2]->setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+        else
+            lr = grad_learning_rate;
+        
+        layers[n_layers-1]->setLearningRate( lr );
+        connections[n_layers-2]->setLearningRate( lr );
 
         layers[ n_layers-1 ]->bpropUpdate( layers[ n_layers-1 ]->activation,
                                            layers[ n_layers-1 ]->expectation,
@@ -1124,11 +1164,15 @@
             Vec target_exp = classification_module->target_layer->expectation;
             fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
 
-            joint_layer->setLearningRate( cd_learning_rate );
-            layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
-            classification_module->joint_connection->setLearningRate(
-                cd_learning_rate );
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+                lr = cd_learning_rate;
 
+            joint_layer->setLearningRate( lr );
+            layers[ n_layers-1 ]->setLearningRate( lr );
+            classification_module->joint_connection->setLearningRate( lr );
+
             save_layer_activation.resize(layers[ n_layers-2 ]->size);
             save_layer_activation << layers[ n_layers-2 ]->activation;
             save_layer_expectation.resize(layers[ n_layers-2 ]->size);
@@ -1150,57 +1194,66 @@
     for( int i=n_layers-2 ; i>=0 ; i-- )
     {
         if (i <= n_layers - 3) {
-        connections[ i ]->setLearningRate( grad_learning_rate );
-        layers[ i+1 ]->setLearningRate( grad_learning_rate );
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+            else
+                lr = grad_learning_rate;
+            
+            connections[ i ]->setLearningRate( lr );
+            layers[ i+1 ]->setLearningRate( lr );
+            
 
-        layers[i+1]->bpropUpdate( layers[i+1]->activation,
-                                  layers[i+1]->expectation,
-                                  activation_gradients[i+1],
-                                  expectation_gradients[i+1] );
-
-        connections[i]->bpropUpdate( layers[i]->expectation,
-                                     layers[i+1]->activation,
-                                     expectation_gradients[i],
-                                     activation_gradients[i+1],
-                                     true);
+            layers[i+1]->bpropUpdate( layers[i+1]->activation,
+                                      layers[i+1]->expectation,
+                                      activation_gradients[i+1],
+                                      expectation_gradients[i+1] );
+            
+            connections[i]->bpropUpdate( layers[i]->expectation,
+                                         layers[i+1]->activation,
+                                         expectation_gradients[i],
+                                         activation_gradients[i+1],
+                                         true);
         }
 
         if (i <= n_layers - 3 || !use_classification_cost ||
-                                 !top_layer_joint_cd) {
+            !top_layer_joint_cd) {
 
-        // N.B. the contrastiveDivergenceStep changes the activation and
-        // expectation fields of top layer of the RBM, so it must be
-        // done last
-        layers[i]->setLearningRate( cd_learning_rate );
-        layers[i+1]->setLearningRate( cd_learning_rate );
-        connections[i]->setLearningRate( cd_learning_rate );
-
-        if( i > 0 )
-        {
-            save_layer_activation.resize(layers[i]->size);
-            save_layer_activation << layers[i]->activation;
-            save_layer_expectation.resize(layers[i]->size);
-            save_layer_expectation << layers[i]->expectation;
+            // N.B. the contrastiveDivergenceStep changes the activation and
+            // expectation fields of top layer of the RBM, so it must be
+            // done last
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+                lr = cd_learning_rate;
+            
+            layers[i]->setLearningRate( lr );
+            layers[i+1]->setLearningRate( lr );
+            connections[i]->setLearningRate( lr );
+            
+            if( i > 0 )
+            {
+                save_layer_activation.resize(layers[i]->size);
+                save_layer_activation << layers[i]->activation;
+                save_layer_expectation.resize(layers[i]->size);
+                save_layer_expectation << layers[i]->expectation;
+            }
+            contrastiveDivergenceStep( layers[ i ],
+                                       connections[ i ],
+                                       layers[ i+1 ] ,
+                                       i, true);
+            if( i > 0 )
+            {
+                layers[i]->activation << save_layer_activation;
+                layers[i]->expectation << save_layer_expectation;
+            }
         }
-        contrastiveDivergenceStep( layers[ i ],
-                                   connections[ i ],
-                                   layers[ i+1 ] ,
-                                   i, true);
-        if( i > 0 )
-        {
-            layers[i]->activation << save_layer_activation;
-            layers[i]->expectation << save_layer_expectation;
-        }
-        }
     }
-
-
-
 }
 
 void DeepBeliefNet::onlineStep(const Mat& inputs, const Mat& targets,
                                Mat& train_costs)
 {
+    real lr;
     // TODO Can we avoid this memory allocation?
     TVec<Mat> cost;
     Vec optimized_cost(inputs.length());
@@ -1280,9 +1333,14 @@
 
     if (final_cost || (!partial_costs.isEmpty() && partial_costs[n_layers-2]))
     {
-        layers[n_layers-1]->setLearningRate( grad_learning_rate );
-        connections[n_layers-2]->setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+        else
+            lr = grad_learning_rate;
 
+        layers[n_layers-1]->setLearningRate( lr );
+        connections[n_layers-2]->setLearningRate( lr );
+
         layers[ n_layers-1 ]->bpropUpdate(
                 layers[ n_layers-1 ]->activations,
                 layers[ n_layers-1 ]->getExpectations(),
@@ -1335,11 +1393,15 @@
             Vec target_exp = classification_module->target_layer->expectation;
             fill_one_hot( target_exp, (int) round(target[0]), real(0.), real(1.) );
 
-            joint_layer->setLearningRate( cd_learning_rate );
-            layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
-            classification_module->joint_connection->setLearningRate(
-                cd_learning_rate );
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+               lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+               lr = cd_learning_rate;
 
+            joint_layer->setLearningRate( lr );
+            layers[ n_layers-1 ]->setLearningRate( lr );
+            classification_module->joint_connection->setLearningRate( lr );
+
             save_layer_activation.resize(layers[ n_layers-2 ]->size);
             save_layer_activation << layers[ n_layers-2 ]->activation;
             save_layer_expectation.resize(layers[ n_layers-2 ]->size);
@@ -1370,8 +1432,13 @@
     for( int i=n_layers-2 ; i>=0 ; i-- )
     {
         if (i <= n_layers - 3) {
-            connections[ i ]->setLearningRate( grad_learning_rate );
-            layers[ i+1 ]->setLearningRate( grad_learning_rate );
+            if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+                lr = grad_learning_rate / (1. + grad_decrease_ct * stage );
+            else
+                lr = grad_learning_rate;
+            
+            connections[ i ]->setLearningRate( lr );
+            layers[ i+1 ]->setLearningRate( lr );
 
             layers[i+1]->bpropUpdate( layers[i+1]->activations,
                                       layers[i+1]->getExpectations(),
@@ -1393,9 +1460,13 @@
             // N.B. the contrastiveDivergenceStep changes the activation and
             // expectation fields of top layer of the RBM, so it must be
             // done last
-            layers[i]->setLearningRate( cd_learning_rate );
-            layers[i+1]->setLearningRate( cd_learning_rate );
-            connections[i]->setLearningRate( cd_learning_rate );
+            if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+                lr = cd_learning_rate / (1. + cd_decrease_ct * stage );
+            else
+                lr = cd_learning_rate;
+            layers[i]->setLearningRate( lr );
+            layers[i+1]->setLearningRate( lr );
+            connections[i]->setLearningRate( lr );
 
             if( i > 0 )
             {
@@ -1438,6 +1509,7 @@
 ////////////////
 void DeepBeliefNet::greedyStep( const Vec& input, const Vec& target, int index )
 {
+    real lr;
     PLASSERT( index < n_layers );
 
     layers[0]->expectation << input;
@@ -1448,13 +1520,20 @@
         layers[i+1]->computeExpectation();
     }
 
-    // TODO: add another learning rate?
     if( !partial_costs.isEmpty() && partial_costs[ index ] )
     {
         // put appropriate learning rate
-        connections[ index ]->setLearningRate( grad_learning_rate );
-        layers[ index+1 ]->setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / 
+                (1. + grad_decrease_ct * 
+                 (stage - cumulative_schedule[index]));
+        else
+            lr = grad_learning_rate;
 
+        partial_costs[ index ]->setLearningRate( lr );
+        connections[ index ]->setLearningRate( lr );
+        layers[ index+1 ]->setLearningRate( lr );
+
         // Backward pass
         real cost;
         partial_costs[ index ]->fprop( layers[ index+1 ]->expectation,
@@ -1476,8 +1555,14 @@
                                            activation_gradients[ index+1 ] );
 
         // put back old learning rate
-        connections[ index ]->setLearningRate( cd_learning_rate );
-        layers[ index+1 ]->setLearningRate( cd_learning_rate );
+        if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+                                     (stage - cumulative_schedule[index]));
+        else
+            lr = cd_learning_rate;
+
+        connections[ index ]->setLearningRate( lr );
+        layers[ index+1 ]->setLearningRate( lr );
     }
 
     contrastiveDivergenceStep( layers[ index ],
@@ -1491,6 +1576,7 @@
 /////////////////
 void DeepBeliefNet::greedyStep( const Mat& inputs, const Mat& targets, int index, Mat& train_costs_m )
 {
+    real lr;
     PLASSERT( index < n_layers );
 
     layers[0]->setExpectations(inputs);
@@ -1501,13 +1587,20 @@
         layers[i+1]->computeExpectations();
     }
 
-    // TODO: add another learning rate?
     if( !partial_costs.isEmpty() && partial_costs[ index ] )
     {
         // put appropriate learning rate
-        connections[ index ]->setLearningRate( grad_learning_rate );
-        layers[ index+1 ]->setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate / 
+                (1. + grad_decrease_ct * 
+                 (stage - cumulative_schedule[index]));
+        else
+            lr = grad_learning_rate;
 
+        partial_costs[ index ]->setLearningRate( lr );
+        connections[ index ]->setLearningRate( lr );
+        layers[ index+1 ]->setLearningRate( lr );
+
         // Backward pass
         Vec costs;
         partial_costs[ index ]->fprop( layers[ index+1 ]->getExpectations(),
@@ -1529,8 +1622,13 @@
                                            activations_gradients[ index+1 ] );
 
         // put back old learning rate
-        connections[ index ]->setLearningRate( cd_learning_rate );
-        layers[ index+1 ]->setLearningRate( cd_learning_rate );
+        if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+            lr = cd_learning_rate / (1. + cd_decrease_ct * 
+                                     (stage - cumulative_schedule[index]));
+        else
+            lr = cd_learning_rate;
+        connections[ index ]->setLearningRate( lr );
+        layers[ index+1 ]->setLearningRate( lr );
     }
 
     if (reconstruct_layerwise)
@@ -1557,6 +1655,7 @@
 /////////////////////
 void DeepBeliefNet::jointGreedyStep( const Vec& input, const Vec& target )
 {
+    real lr;
     PLASSERT( joint_layer );
     PLASSERT_MSG(batch_size == 1, "Not implemented for mini-batches");
 
@@ -1577,8 +1676,17 @@
         layers[ n_layers-1 ]->computeExpectation();
 
         // put appropriate learning rate
-        connections[ n_layers-2 ]->setLearningRate( grad_learning_rate );
-        layers[ n_layers-1 ]->setLearningRate( grad_learning_rate );
+        if( !fast_exact_is_equal( grad_decrease_ct, 0. ) )
+            lr = grad_learning_rate 
+                / (1. + grad_decrease_ct * 
+                   (stage - cumulative_schedule[n_layers-2]));
+        else
+            lr = grad_learning_rate;
+        
+        partial_costs[ n_layers-2 ]->setLearningRate( lr );
+        connections[ n_layers-2 ]->setLearningRate( lr );
+        layers[ n_layers-1 ]->setLearningRate( lr );
+        
 
         // Backward pass
         real cost;
@@ -1602,8 +1710,15 @@
             activation_gradients[ n_layers-1 ] );
 
         // put back old learning rate
-        connections[ n_layers-2 ]->setLearningRate( cd_learning_rate );
-        layers[ n_layers-1 ]->setLearningRate( cd_learning_rate );
+        if( !fast_exact_is_equal( cd_decrease_ct, 0. ) )
+            lr = cd_learning_rate 
+                / (1. + cd_decrease_ct * 
+                   (stage - cumulative_schedule[n_layers-2]));
+        else
+            lr = cd_learning_rate;
+
+        connections[ n_layers-2 ]->setLearningRate( lr );
+        layers[ n_layers-1 ]->setLearningRate( lr );
     }
 
     Vec target_exp = classification_module->target_layer->expectation;
@@ -2227,6 +2342,8 @@
     {
         layers[i]->setLearningRate( the_learning_rate );
         connections[i]->setLearningRate( the_learning_rate );
+        if( partial_costs.length() != 0 && partial_costs[i] )
+            partial_costs[i]->setLearningRate( the_learning_rate );
     }
     layers[n_layers-1]->setLearningRate( the_learning_rate );
 

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-09-07 21:14:30 UTC (rev 8055)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-09-07 22:07:33 UTC (rev 8056)
@@ -68,6 +68,10 @@
     //! The learning rate used during contrastive divergence learning
     real cd_learning_rate;
 
+    //! The decrease constant of the learning rate used during 
+    //! contrastive divergence learning
+    real cd_decrease_ct;
+
     //! The learning rate used during the gradient descent
     real grad_learning_rate;
 
@@ -184,7 +188,6 @@
     //! inside classification_module), if use_classification_cost
     PP<RBMMixedLayer> joint_layer;
 
-
 public:
     //#####  Public Member Functions  #########################################
 



From larocheh at mail.berlios.de  Sat Sep  8 00:08:43 2007
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Sat, 8 Sep 2007 00:08:43 +0200
Subject: [Plearn-commits] r8057 - trunk/plearn_learners/online
Message-ID: <200709072208.l87M8hbl012274@sheep.berlios.de>

Author: larocheh
Date: 2007-09-08 00:08:43 +0200 (Sat, 08 Sep 2007)
New Revision: 8057

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.h
Log:
Corrected partial_costs option and added online version.


Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-09-07 22:07:33 UTC (rev 8056)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-09-07 22:08:43 UTC (rev 8057)
@@ -59,7 +59,9 @@
     fine_tuning_decrease_ct( 0. ),
     l1_neuron_decay( 0. ),
     l1_neuron_decay_center( 0 ),
+    online( false ),
     compute_all_test_costs( false ),
+    reconstruct_hidden( false ),
     n_layers( 0 ),
     currently_trained_layer( 0 )
 {
@@ -123,7 +125,7 @@
                   "The layers of units in the network. The first element\n"
                   "of this vector should be the input layer and the\n"
                   "subsequent elements should be the hidden layers. The\n"
-                  "output should not be included in this layer.\n");
+                  "output layer should not be included in layers.\n");
 
     declareOption(ol, "connections", &StackedAutoassociatorsNet::connections,
                   OptionBase::buildoption,
@@ -142,6 +144,13 @@
                   "in the hidden layers. They must have the same input and\n"
                   "output sizes, compatible with their corresponding layers.");
 
+    declareOption(ol, "direct_connections", 
+                  &StackedAutoassociatorsNet::direct_connections,
+                  OptionBase::buildoption,
+                  "Optional weights from each inputs to all other inputs'\n"
+                  "reconstruction, which can capture simple (linear or log-linear)\n"
+                  "correlations between inputs.");
+
     declareOption(ol, "final_module", &StackedAutoassociatorsNet::final_module,
                   OptionBase::buildoption,
                   "Module that takes as input the output of the last layer\n"
@@ -167,6 +176,11 @@
                   "previous layers.\n"
         );
 
+    declareOption(ol, "online", &StackedAutoassociatorsNet::online,
+                  OptionBase::buildoption,
+                  "If true then all unsupervised training stages (as well as\n"
+                  "the fine-tuning stage) are done simultaneously.\n");
+
     declareOption(ol, "partial_costs_weights", 
                   &StackedAutoassociatorsNet::partial_costs_weights,
                   OptionBase::buildoption,
@@ -181,6 +195,13 @@
                   "(up to the currently trained layer) should be computed.\n"
         );
 
+    declareOption(ol, "reconstruct_hidden", 
+                  &StackedAutoassociatorsNet::reconstruct_hidden,
+                  OptionBase::buildoption,
+                  "Indication that the autoassociators are also trained to\n"
+                  "reconstruct their hidden layers (inspired from CD1 in an RBM).\n"
+        );
+
     declareOption(ol, "greedy_stages", 
                   &StackedAutoassociatorsNet::greedy_stages,
                   OptionBase::learntoption,
@@ -228,7 +249,7 @@
                     "usage of weighted samples (weight size > 0) is not\n"
                     "implemented yet.\n");
 
-        if( training_schedule.length() != n_layers-1 )        
+        if( !online && training_schedule.length() != n_layers-1 )        
             PLERROR("StackedAutoassociatorsNet::build_() - \n"
                     "training_schedule should have %d elements.\n",
                     n_layers-1);
@@ -244,22 +265,34 @@
                     "partial_costs_weights should have %d elements.\n",
                     n_layers-1);
 
-        if(greedy_stages.length() == 0)
+        if( online && reconstruct_hidden )
+            PLERROR("StackedAutoassociatorsNet::build_()"
+                    " - \n"
+                    "cannot use online setting with reconstruct_hidden=true.\n");
+
+        if( !online )
         {
-            greedy_stages.resize(n_layers-1);
-            greedy_stages.clear();
+            if( greedy_stages.length() == 0)
+            {
+                greedy_stages.resize(n_layers-1);
+                greedy_stages.clear();
+            }
+            
+            if(stage > 0)
+                currently_trained_layer = n_layers;
+            else
+            {            
+                currently_trained_layer = n_layers-1;
+                while(currently_trained_layer>1
+                      && greedy_stages[currently_trained_layer-1] <= 0)
+                    currently_trained_layer--;
+            }
         }
-
-        if(stage > 0)
+        else
+        {
             currently_trained_layer = n_layers;
-        else
-        {            
-            currently_trained_layer = n_layers-1;
-            while(currently_trained_layer>1
-                  && greedy_stages[currently_trained_layer-1] <= 0)
-                currently_trained_layer--;
         }
-
+    
         build_layers_and_connections();
         build_costs();
     }
@@ -285,8 +318,24 @@
                 "there should be either %d correlation connections or none.\n",
                 n_layers-1);
     
+    if( direct_connections.length() != 0 &&
+        direct_connections.length() != n_layers-1 )
+        PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "there should be either %d direct connections or none.\n",
+                n_layers-1);
+
+    if(reconstruct_hidden && compute_all_test_costs )
+        PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                "compute_all_test_costs option is not implemented for\n"
+                "reconstruct_hidden option.");
+
+    
     if(correlation_connections.length() != 0)
     {
+        if( compute_all_test_costs )
+            PLERROR("StackedAutoassociatorsNet::build_layers_and_connections() - \n"
+                    "compute_all_test_costs option is not implemented for\n"
+                    "correlation_connections.");
         correlation_layers.resize( layers.length()-1 );
         for( int i=0 ; i<n_layers-1 ; i++ )
         {
@@ -344,6 +393,11 @@
 
         if(correlation_connections.length() != 0)
         {
+            if(reconstruct_hidden)
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "cannot use correlation_connections with reconstruct_hidden=true.\n");
+
             if( correlation_connections[i]->up_size != layers[i+1]->size ||
                 correlation_connections[i]->down_size != layers[i+1]->size )
                 PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
@@ -368,6 +422,33 @@
             }
         }
 
+        if(direct_connections.length() != 0)
+        {
+            if( online )
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "cannot use direct_connections in the online setting.\n");
+
+
+            if(reconstruct_hidden)
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "cannot use direct_connections with reconstruct_hidden=true.\n");
+
+            if( direct_connections[i]->up_size != layers[i]->size ||
+                direct_connections[i]->down_size != layers[i]->size )
+                PLERROR("StackedAutoassociatorsNet::build_layers_and_connections()"
+                        " - \n"
+                        "direct_connections[%i] should have a up_size and "
+                        "down_size of %d.\n",
+                        i, layers[i]->size);
+            if( !(direct_connections[i]->random_gen) )
+            {
+                direct_connections[i]->random_gen = random_gen;
+                direct_connections[i]->forget();
+            }
+        }
+
         if( !(layers[i]->random_gen) )
         {
             layers[i]->random_gen = random_gen;
@@ -406,6 +487,10 @@
 {
     MODULE_LOG << "build_final_cost() called" << endl;
 
+    if( !final_cost )
+        PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                "final_cost should be provided.\n");
+
     final_cost_gradient.resize( final_cost->input_size );
     final_cost->setLearningRate( fine_tuning_learning_rate );
 
@@ -445,6 +530,11 @@
     
     if(partial_costs)
     {
+
+        if( correlation_connections.length() != 0 )
+            PLERROR("StackedAutoassociatorsNet::build_costs() - \n"
+                    "correlation_connections cannot be used with partial costs.");
+            
         partial_costs_positions.resize(partial_costs.length());
         partial_costs_positions.clear();
         for(int i=0; i<partial_costs.length(); i++)
@@ -485,28 +575,37 @@
 
     // deepCopyField(, copies);
 
+    // Public options
     deepCopyField(training_schedule, copies);
     deepCopyField(layers, copies);
     deepCopyField(connections, copies);
     deepCopyField(reconstruction_connections, copies);
+    deepCopyField(correlation_connections, copies);
+    deepCopyField(direct_connections, copies);
     deepCopyField(final_module, copies);
     deepCopyField(final_cost, copies);
     deepCopyField(partial_costs, copies);
     deepCopyField(partial_costs_weights, copies);
+
+    // Protected options
     deepCopyField(activations, copies);
     deepCopyField(expectations, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
     deepCopyField(reconstruction_activations, copies);
-    deepCopyField(reconstruction_expectations, copies);
     deepCopyField(reconstruction_activation_gradients, copies);
-    deepCopyField(reconstruction_expectation_gradients, copies);
-    deepCopyField(correlation_connections, copies);
+    deepCopyField(reconstruction_activation_gradients_from_hid_rec, copies);
+    deepCopyField(reconstruction_expectation_gradients_from_hid_rec, copies);
+    deepCopyField(hidden_reconstruction_activations, copies);
+    deepCopyField(hidden_reconstruction_activation_gradients, copies);
     deepCopyField(correlation_activations, copies);
     deepCopyField(correlation_expectations, copies);
     deepCopyField(correlation_activation_gradients, copies);
     deepCopyField(correlation_expectation_gradients, copies);
     deepCopyField(correlation_layers, copies);
+    deepCopyField(direct_activations, copies);
+    deepCopyField(direct_and_reconstruction_activations, copies);
+    deepCopyField(direct_and_reconstruction_activation_gradients, copies);
     deepCopyField(partial_costs_positions, copies);
     deepCopyField(partial_cost_value, copies);
     deepCopyField(final_cost_input, copies);
@@ -587,115 +686,189 @@
     real lr = 0;
     int init_stage;
 
-    /***** initial greedy training *****/
-    for( int i=0 ; i<n_layers-1 ; i++ )
+    if( !online )
     {
-        MODULE_LOG << "Training connection weights between layers " << i
-            << " and " << i+1 << endl;
 
-        int end_stage = training_schedule[i];
-        int* this_stage = greedy_stages.subVec(i,1).data();
-        init_stage = *this_stage;
+        /***** initial greedy training *****/
+        for( int i=0 ; i<n_layers-1 ; i++ )
+        {
+            MODULE_LOG << "Training connection weights between layers " << i
+                       << " and " << i+1 << endl;
 
-        MODULE_LOG << "  stage = " << *this_stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  greedy_learning_rate = " << greedy_learning_rate << endl;
+            int end_stage = training_schedule[i];
+            int* this_stage = greedy_stages.subVec(i,1).data();
+            init_stage = *this_stage;
 
-        if( report_progress && *this_stage < end_stage )
-            pb = new ProgressBar( "Training layer "+tostring(i)
-                                  +" of "+classname(),
-                                  end_stage - init_stage );
+            MODULE_LOG << "  stage = " << *this_stage << endl;
+            MODULE_LOG << "  end_stage = " << end_stage << endl;
+            MODULE_LOG << "  greedy_learning_rate = " << greedy_learning_rate << endl;
 
-        train_costs.fill(MISSING_VALUE);
-        lr = greedy_learning_rate;
-        layers[i]->setLearningRate( lr );
-        connections[i]->setLearningRate( lr );
-        reconstruction_connections[i]->setLearningRate( lr );
-        if(correlation_connections.length() != 0)
-        {
-            correlation_connections[i]->setLearningRate( lr );
-            correlation_layers[i]->setLearningRate( lr );
-        }
-        layers[i+1]->setLearningRate( lr );
+            if( report_progress && *this_stage < end_stage )
+                pb = new ProgressBar( "Training layer "+tostring(i)
+                                      +" of "+classname(),
+                                      end_stage - init_stage );
 
-        reconstruction_activations.resize(layers[i]->size);
-        reconstruction_expectations.resize(layers[i]->size);
-        reconstruction_activation_gradients.resize(layers[i]->size);
-        reconstruction_expectation_gradients.resize(layers[i]->size);
+            train_costs.fill(MISSING_VALUE);
+            lr = greedy_learning_rate;
+            layers[i]->setLearningRate( lr );
+            connections[i]->setLearningRate( lr );
+            reconstruction_connections[i]->setLearningRate( lr );
+            if(correlation_connections.length() != 0)
+            {
+                correlation_connections[i]->setLearningRate( lr );
+                correlation_layers[i]->setLearningRate( lr );
+            }
+            if(direct_connections.length() != 0)
+            {
+                direct_connections[i]->setLearningRate( lr );
+            }
+            layers[i+1]->setLearningRate( lr );
+            if(partial_costs.length() != 0 && partial_costs[i])
+                        partial_costs[i]->setLearningRate( lr );
 
-        for( ; *this_stage<end_stage ; (*this_stage)++ )
-        {
-            if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+            // Make sure that storage not null, will be resized anyways by bprop calls
+            reconstruction_activations.resize(layers[i]->size);
+            reconstruction_activation_gradients.resize(layers[i]->size);
+            reconstruction_expectation_gradients.resize(layers[i]->size);
+
+            if(reconstruct_hidden)
             {
-                lr = greedy_learning_rate/(1 + greedy_decrease_ct 
-                                           * (*this_stage)); 
-                layers[i]->setLearningRate( lr );
-                connections[i]->setLearningRate( lr );
-                reconstruction_connections[i]->setLearningRate( lr );
-                layers[i+1]->setLearningRate( lr );
-                if(correlation_connections.length() != 0)
+                reconstruction_activation_gradients_from_hid_rec.resize(
+                    layers[i+1]->size);
+                reconstruction_expectation_gradients_from_hid_rec.resize(
+                    layers[i+1]->size);
+                hidden_reconstruction_activations.resize(layers[i+1]->size);
+                hidden_reconstruction_activation_gradients.resize(layers[i+1]->size);
+            }
+
+            if(direct_connections.length() != 0)
+            {
+                direct_activations.resize(layers[i]->size);
+                direct_and_reconstruction_activations.resize(layers[i]->size);
+                direct_and_reconstruction_activation_gradients.resize(layers[i]->size);
+            }
+            for( ; *this_stage<end_stage ; (*this_stage)++ )
+            {
+                if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
                 {
-                    correlation_connections[i]->setLearningRate( lr );
-                    correlation_layers[i]->setLearningRate( lr );
+                    lr = greedy_learning_rate/(1 + greedy_decrease_ct 
+                                               * (*this_stage)); 
+                    layers[i]->setLearningRate( lr );
+                    connections[i]->setLearningRate( lr );
+                    reconstruction_connections[i]->setLearningRate( lr );
+                    layers[i+1]->setLearningRate( lr );
+                    if(correlation_connections.length() != 0)
+                    {
+                        correlation_connections[i]->setLearningRate( lr );
+                        correlation_layers[i]->setLearningRate( lr );
+                    }
+                    if(direct_connections.length() != 0)
+                    {
+                        direct_connections[i]->setLearningRate( lr );
+                    }
+                    if(partial_costs.length() != 0 && partial_costs[i])
+                        partial_costs[i]->setLearningRate( lr );
                 }
+
+                sample = *this_stage % nsamples;
+                train_set->getExample(sample, input, target, weight);
+                greedyStep( input, target, i, train_costs );
+                train_stats->update( train_costs );
+
+                if( pb )
+                    pb->update( *this_stage - init_stage + 1 );
             }
+        }
 
-            sample = *this_stage % nsamples;
-            train_set->getExample(sample, input, target, weight);
-            greedyStep( input, target, i, train_costs );
-            train_stats->update( train_costs );
+        /***** fine-tuning by gradient descent *****/
+        if( stage < nstages )
+        {
 
-            if( pb )
-                pb->update( *this_stage - init_stage + 1 );
+            MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
+            MODULE_LOG << "  stage = " << stage << endl;
+            MODULE_LOG << "  nstages = " << nstages << endl;
+            MODULE_LOG << "  fine_tuning_learning_rate = " << 
+                fine_tuning_learning_rate << endl;
+
+            init_stage = stage;
+            if( report_progress && stage < nstages )
+                pb = new ProgressBar( "Fine-tuning parameters of all layers of "
+                                      + classname(),
+                                      nstages - init_stage );
+
+            setLearningRate( fine_tuning_learning_rate );
+            train_costs.fill(MISSING_VALUE);
+            for( ; stage<nstages ; stage++ )
+            {
+                sample = stage % nsamples;
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( fine_tuning_learning_rate
+                                     / (1. + fine_tuning_decrease_ct * stage ) );
+
+                train_set->getExample( sample, input, target, weight );
+                fineTuningStep( input, target, train_costs );
+                train_stats->update( train_costs );
+
+                if( pb )
+                    pb->update( stage - init_stage + 1 );
+            }
         }
+    
+        train_stats->finalize();
+        MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
+
+        // Update currently_trained_layer
+        if(stage > 0)
+            currently_trained_layer = n_layers;
+        else
+        {            
+            currently_trained_layer = n_layers-1;
+            while(currently_trained_layer>1 
+                  && greedy_stages[currently_trained_layer-1] <= 0)
+                currently_trained_layer--;
+        }
     }
-
-    /***** fine-tuning by gradient descent *****/
-    if( stage < nstages )
+    else
     {
+        // Train all layers simultaneously AND fine-tuning as well!
+        if( stage < nstages )
+        {
 
-        MODULE_LOG << "Fine-tuning all parameters, by gradient descent" << endl;
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  nstages = " << nstages << endl;
-        MODULE_LOG << "  fine_tuning_learning_rate = " << 
-            fine_tuning_learning_rate << endl;
+            MODULE_LOG << "Training all layers greedy layer-wise AND "
+                       << "fine-tuning all parameters, by gradient descent" 
+                       << endl;
+            MODULE_LOG << "  stage = " << stage << endl;
+            MODULE_LOG << "  nstages = " << nstages << endl;
+            MODULE_LOG << "  fine_tuning_learning_rate = " 
+                       << fine_tuning_learning_rate << endl;
+            MODULE_LOG << "  greedy_learning_rate = " 
+                       << greedy_learning_rate << endl;
 
-        init_stage = stage;
-        if( report_progress && stage < nstages )
-            pb = new ProgressBar( "Fine-tuning parameters of all layers of "
-                                  + classname(),
-                                  nstages - init_stage );
+            init_stage = stage;
+            if( report_progress && stage < nstages )
+                pb = new ProgressBar( 
+                    "Greedy layer-wise training AND fine-tuning parameters of "
+                                      + classname(),
+                                      nstages - init_stage );
 
-        setLearningRate( fine_tuning_learning_rate );
-        train_costs.fill(MISSING_VALUE);
-        for( ; stage<nstages ; stage++ )
-        {
-            sample = stage % nsamples;
-            if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
-                setLearningRate( fine_tuning_learning_rate
-                                 / (1. + fine_tuning_decrease_ct * stage ) );
+            setLearningRate( fine_tuning_learning_rate );
+            train_costs.fill(MISSING_VALUE);
+            for( ; stage<nstages ; stage++ )
+            {
+                sample = stage % nsamples;
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct, 0. ) )
+                    setLearningRate( fine_tuning_learning_rate
+                                     / (1. + fine_tuning_decrease_ct * stage ) );
 
-            train_set->getExample( sample, input, target, weight );
-            fineTuningStep( input, target, train_costs );
-            train_stats->update( train_costs );
+                train_set->getExample( sample, input, target, weight );
+                onlineStep( input, target, train_costs );
+                train_stats->update( train_costs );
 
-            if( pb )
-                pb->update( stage - init_stage + 1 );
+                if( pb )
+                    pb->update( stage - init_stage + 1 );
+            }
         }
-    }
-    
-    train_stats->finalize();
-    MODULE_LOG << "  train costs = " << train_stats->getMean() << endl;
 
-    // Update currently_trained_layer
-    if(stage > 0)
-        currently_trained_layer = n_layers;
-    else
-    {            
-        currently_trained_layer = n_layers-1;
-        while(currently_trained_layer>1 
-              && greedy_stages[currently_trained_layer-1] <= 0)
-            currently_trained_layer--;
     }
 }
 
@@ -733,7 +906,7 @@
 
         // Update partial cost (might contain some weights for example)
         partial_costs[ index ]->bpropUpdate( expectations[ index + 1 ],
-                                             target, partial_cost_value,
+                                             target, partial_cost_value[0],
                                              expectation_gradients[ index + 1 ]
                                              );
 
@@ -757,30 +930,99 @@
 
     reconstruction_connections[ index ]->fprop( expectations[ index + 1],
                                                 reconstruction_activations);
-    layers[ index ]->fprop( reconstruction_activations,
-                            layers[ index ]->expectation);
+    if(direct_connections.length() != 0)
+    {
+        direct_connections[ index ]->fprop( expectations[ index ], 
+                                            direct_activations );
+        direct_and_reconstruction_activations.clear();
+        direct_and_reconstruction_activations += direct_activations;
+        direct_and_reconstruction_activations += reconstruction_activations;
 
-    layers[ index ]->activation << reconstruction_activations;
-    layers[ index ]->expectation_is_up_to_date = true;
-    train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
-    
-    layers[ index ]->bpropNLL(expectations[index], train_costs[index],
-                              reconstruction_activation_gradients);
+        layers[ index ]->fprop( direct_and_reconstruction_activations,
+                                layers[ index ]->expectation);
+        
+        layers[ index ]->activation << direct_and_reconstruction_activations;
+        layers[ index ]->expectation_is_up_to_date = true;
+        train_costs[index] = layers[ index ]->fpropNLL(expectations[index]);
+        
+        layers[ index ]->bpropNLL(expectations[index], train_costs[index],
+                                  direct_and_reconstruction_activation_gradients);
 
-    layers[ index ]->update(reconstruction_activation_gradients);
+        layers[ index ]->update(direct_and_reconstruction_activation_gradients);
 
-    // // This is a bad update! Propagates gradient through sigmoid again!
-    // layers[ index ]->bpropUpdate( reconstruction_activations, 
-    //                                   layers[ index ]->expectation,
-    //                                   reconstruction_activation_gradients,
-    //                                   reconstruction_expectation_gradients);
+        direct_connections[ index ]->bpropUpdate( 
+            expectations[ index ],
+            direct_activations,
+            reconstruction_expectation_gradients, // Will be overwritten later
+            direct_and_reconstruction_activation_gradients);
+        
+        reconstruction_connections[ index ]->bpropUpdate( 
+            expectations[ index + 1], 
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            direct_and_reconstruction_activation_gradients);
+    }
+    else
+    {
+        layers[ index ]->fprop( reconstruction_activations,
+                                layers[ index ]->expectation);
+        
+        layers[ index ]->activation << reconstruction_activations;
+        layers[ index ]->expectation_is_up_to_date = true;
+        real rec_err = layers[ index ]->fpropNLL(expectations[index]);
+        train_costs[index] = rec_err;
 
-    reconstruction_connections[ index ]->bpropUpdate( 
-        expectations[ index + 1], 
-        reconstruction_activations, 
-        reconstruction_expectation_gradients, 
-        reconstruction_activation_gradients);
+        layers[ index ]->bpropNLL(expectations[index], rec_err,
+                                  reconstruction_activation_gradients);
 
+        if(reconstruct_hidden)
+        {
+            connections[ index ]->fprop( layers[ index ]->expectation, 
+                                         hidden_reconstruction_activations );
+            layers[ index+1 ]->fprop( hidden_reconstruction_activations,
+                layers[ index+1 ]->expectation );
+            layers[ index+1 ]->activation << hidden_reconstruction_activations;
+            layers[ index+1 ]->expectation_is_up_to_date = true;
+            real hid_rec_err = layers[ index+1 ]->fpropNLL(expectations[index+1]);
+            train_costs[index] += hid_rec_err;
+
+            layers[ index+1 ]->bpropNLL(expectations[index+1], hid_rec_err,
+                                        hidden_reconstruction_activation_gradients);
+            layers[ index+1 ]->update(hidden_reconstruction_activation_gradients);
+            
+            connections[ index ]->bpropUpdate( 
+                layers[ index ]->expectation, 
+                hidden_reconstruction_activations,
+                reconstruction_expectation_gradients_from_hid_rec,
+                hidden_reconstruction_activation_gradients);
+
+            layers[ index ]->bpropUpdate( 
+                reconstruction_activations,
+                layers[ index ]->expectation,
+                reconstruction_activation_gradients_from_hid_rec,
+                reconstruction_expectation_gradients_from_hid_rec);
+        }
+        
+        layers[ index ]->update(reconstruction_activation_gradients);
+
+        if(reconstruct_hidden)
+            reconstruction_activation_gradients +=
+                reconstruction_activation_gradients_from_hid_rec;
+
+        // // This is a bad update! Propagates gradient through sigmoid again!
+        // layers[ index ]->bpropUpdate( reconstruction_activations, 
+        //                                   layers[ index ]->expectation,
+        //                                   reconstruction_activation_gradients,
+        //                                   reconstruction_expectation_gradients);
+        
+        reconstruction_connections[ index ]->bpropUpdate( 
+            expectations[ index + 1], 
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+    }
+
     if(!fast_exact_is_equal(l1_neuron_decay,0))
     {
         // Compute L1 penalty gradient on neurons
@@ -932,6 +1174,276 @@
     }
 }
 
+void StackedAutoassociatorsNet::onlineStep( const Vec& input, 
+                                            const Vec& target,
+                                            Vec& train_costs )
+{
+    real lr;
+    // fprop
+    expectations[0] << input;
+
+    if(correlation_connections.length() != 0)
+    {
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop( expectations[i], correlation_activations[i] );
+            layers[i+1]->fprop( correlation_activations[i],
+                                correlation_expectations[i] );
+            correlation_connections[i]->fprop( correlation_expectations[i], 
+                                               activations[i+1] );
+            correlation_layers[i]->fprop( activations[i+1], 
+                                          expectations[i+1] );
+        }
+    }
+    else
+    {
+        for( int i=0 ; i<n_layers-1; i++ )
+        {
+            connections[i]->fprop( expectations[i], activations[i+1] );
+            layers[i+1]->fprop(activations[i+1],expectations[i+1]);
+            
+            if( partial_costs.length() != 0 && partial_costs[ i ] )
+            {
+                // Set learning rates
+
+                if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+                    lr = greedy_learning_rate / 
+                        (1 + greedy_decrease_ct * stage);
+                else
+                    lr = greedy_learning_rate;
+
+                partial_costs[ i ]->setLearningRate( lr );
+                layers[ i+1 ]->setLearningRate( lr );
+                connections[ i ]->setLearningRate( lr );
+
+                partial_costs[ i ]->fprop( expectations[ i + 1],
+                                           target, partial_cost_value );
+                
+                // Update partial cost (might contain some weights for example)
+                partial_costs[ i ]->bpropUpdate( 
+                    expectations[ i + 1 ],
+                    target, partial_cost_value[0],
+                    expectation_gradients[ i + 1 ]
+                    );
+                
+                train_costs.subVec(partial_costs_positions[i],
+                                   partial_cost_value.length()) 
+                    << partial_cost_value;
+                
+                if( !fast_exact_is_equal( partial_costs_weights.length(), 0 ) )
+                    expectation_gradients[ i + 1 ] *= partial_costs_weights[i];
+                
+                // Update hidden layer bias and weights
+                layers[ i+1 ]->bpropUpdate( activations[ i + 1 ],
+                                            expectations[ i + 1 ],
+                                            activation_gradients[ i + 1 ],
+                                            expectation_gradients[ i + 1 ] );
+                
+                connections[ i ]->bpropUpdate( expectations[ i ],
+                                               activations[ i + 1 ],
+                                               expectation_gradients[ i ],
+                                               activation_gradients[ i + 1 ] );
+
+                if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
+                    lr = fine_tuning_learning_rate / 
+                        (1 + fine_tuning_decrease_ct * stage);
+                else
+                    lr = fine_tuning_learning_rate;
+
+                layers[ i+1 ]->setLearningRate( lr );
+                connections[ i ]->setLearningRate( lr );
+            }
+        }
+    }
+
+    final_module->fprop( expectations[ n_layers-1 ],
+                         final_cost_input );
+    final_cost->fprop( final_cost_input, target, final_cost_value );
+
+    train_costs.subVec(train_costs.length()-final_cost_value.length(),
+                       final_cost_value.length()) <<
+        final_cost_value;
+
+    final_cost->bpropUpdate( final_cost_input, target,
+                             final_cost_value[0],
+                             final_cost_gradient );
+    final_module->bpropUpdate( expectations[ n_layers-1 ],
+                               final_cost_input,
+                               expectation_gradients[ n_layers-1 ],
+                               final_cost_gradient );
+
+    // Unsupervised greedy layer-wise cost
+
+    // Set learning rates
+    if( !fast_exact_is_equal( greedy_decrease_ct , 0 ) )
+        lr = greedy_learning_rate / (1 + greedy_decrease_ct * stage) ;
+    else
+        lr = greedy_learning_rate;
+
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( lr );
+        connections[i]->setLearningRate( lr );
+        reconstruction_connections[i]->setLearningRate( lr );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]->setLearningRate( lr );
+            correlation_connections[i]->setLearningRate( lr );
+        }
+    }
+    layers[n_layers-1]->setLearningRate( lr );
+
+    // Backpropagate unsupervised gradient, layer-wise
+    for( int i=n_layers-1 ; i>0 ; i-- )
+    {
+        reconstruction_connections[ i-1 ]->fprop( 
+            expectations[ i ],
+            reconstruction_activations);
+
+        layers[ i-1 ]->fprop( reconstruction_activations,
+                                layers[ i-1 ]->expectation);
+        
+        layers[ i-1 ]->activation << reconstruction_activations;
+        layers[ i-1 ]->expectation_is_up_to_date = true;
+        real rec_err = layers[ i-1 ]->fpropNLL(expectations[i-1]);
+        train_costs[i-1] = rec_err;
+
+        layers[ i-1 ]->bpropNLL(expectations[i-1], rec_err,
+                                  reconstruction_activation_gradients);
+
+        layers[ i-1 ]->update(reconstruction_activation_gradients);
+
+        reconstruction_connections[ i-1 ]->bpropUpdate( 
+            expectations[ i ], 
+            reconstruction_activations, 
+            reconstruction_expectation_gradients, 
+            reconstruction_activation_gradients);
+
+        if(!fast_exact_is_equal(l1_neuron_decay,0))
+        {
+            // Compute L1 penalty gradient on neurons
+            real* hid = expectations[ i ].data();
+            real* grad = reconstruction_expectation_gradients.data();
+            int len = expectations[ i ].length();
+            for(int j=0; j<len; j++)
+            {
+                if(*hid > l1_neuron_decay_center)
+                    *grad -= l1_neuron_decay;
+                else if(*hid < l1_neuron_decay_center)
+                    *grad += l1_neuron_decay;
+                hid++;
+                grad++;
+            }
+        }
+
+        if( correlation_connections.length() != 0 )
+        {
+            correlation_layers[i-1]->bpropUpdate( 
+                activations[i],
+                expectations[i],
+                reconstruction_activation_gradients,
+                reconstruction_expectation_gradients );
+            
+            correlation_connections[i-1]->bpropUpdate( 
+                correlation_expectations[i-1],
+                activations[i],
+                correlation_expectation_gradients[i-1],
+                reconstruction_activation_gradients);
+
+            layers[i]->bpropUpdate( correlation_activations[i-1],
+                                    correlation_expectations[i-1],
+                                    correlation_activation_gradients[i-1],
+                                    correlation_expectation_gradients[i-1] );
+            
+            connections[i-1]->bpropUpdate( expectations[i-1],
+                                           correlation_activations[i-1],
+                                           reconstruction_expectation_gradients,
+                                           correlation_activation_gradients[i-1] );
+        }
+        else
+        {
+            layers[i]->bpropUpdate( 
+                activations[i],
+                expectations[i],
+                reconstruction_activation_gradients,
+                reconstruction_expectation_gradients );
+            
+            connections[i-1]->bpropUpdate( 
+                expectations[i-1],
+                activations[i],
+                reconstruction_expectation_gradients,
+                reconstruction_activation_gradients);
+        }
+    }
+
+    // Put back fine-tuning learning rate
+    // Set learning rates
+    if( !fast_exact_is_equal( fine_tuning_decrease_ct , 0 ) )
+        lr = fine_tuning_learning_rate 
+            / (1 + fine_tuning_decrease_ct * stage) ;
+    else
+        lr = fine_tuning_learning_rate ;
+
+    // Set learning rate back for fine-tuning
+    for( int i=0 ; i<n_layers-1 ; i++ )
+    {
+        layers[i]->setLearningRate( lr );
+        connections[i]->setLearningRate( lr );
+        //reconstruction_connections[i]->setLearningRate( lr );
+        if(correlation_layers.length() != 0)
+        {
+            correlation_layers[i]->setLearningRate( lr );
+            correlation_connections[i]->setLearningRate( lr );
+        }
+    }
+    layers[n_layers-1]->setLearningRate( lr );
+
+    // Fine-tuning backpropagation
+    if( correlation_connections.length() != 0 )
+    {
+        for( int i=n_layers-1 ; i>0 ; i-- )
+        {
+            correlation_layers[i-1]->bpropUpdate( 
+                activations[i],
+                expectations[i],
+                activation_gradients[i],
+                expectation_gradients[i] );
+
+            correlation_connections[i-1]->bpropUpdate( 
+                correlation_expectations[i-1],
+                activations[i],
+                correlation_expectation_gradients[i-1],
+                activation_gradients[i] );
+
+            layers[i]->bpropUpdate( correlation_activations[i-1],
+                                    correlation_expectations[i-1],
+                                    correlation_activation_gradients[i-1],
+                                    correlation_expectation_gradients[i-1] );
+            
+            connections[i-1]->bpropUpdate( 
+                expectations[i-1],
+                correlation_activations[i-1],
+                expectation_gradients[i-1],
+                correlation_activation_gradients[i-1] );
+        }
+    }
+    else
+    {
+        for( int i=n_layers-1 ; i>0 ; i-- )
+        {
+            layers[i]->bpropUpdate( activations[i],
+                                    expectations[i],
+                                    activation_gradients[i],
+                                    expectation_gradients[i] );
+            
+            connections[i-1]->bpropUpdate( expectations[i-1],
+                                           activations[i],
+                                           expectation_gradients[i-1],
+                                           activation_gradients[i] );
+        }        
+    }
+}
+
 void StackedAutoassociatorsNet::computeOutput(const Vec& input, Vec& output) const
 {
     // fprop
@@ -1009,11 +1521,20 @@
         {
             reconstruction_connections[ i ]->fprop( expectations[ i+1 ],
                                                     reconstruction_activations);
+            if( direct_connections.length() != 0 )
+            {
+                direct_connections[ i ]->fprop( 
+                    expectations[ i ], 
+                    direct_activations );
+                reconstruction_activations += direct_activations;
+            }
+
             layers[ i ]->fprop( reconstruction_activations,
-                                    layers[ i ]->expectation);
+                                layers[ i ]->expectation);
             
             layers[ i ]->activation << reconstruction_activations;
             layers[ i ]->expectation_is_up_to_date = true;
+
             costs[i] = layers[ i ]->fpropNLL(expectations[ i ]);
 
             if( partial_costs && partial_costs[i])
@@ -1032,6 +1553,13 @@
         reconstruction_connections[ currently_trained_layer-1 ]->fprop( 
             output,
             reconstruction_activations);
+        if( direct_connections.length() != 0 )
+        {
+            direct_connections[ currently_trained_layer-1 ]->fprop( 
+                expectations[ currently_trained_layer-1 ], 
+                direct_activations );
+            reconstruction_activations += direct_activations;
+        }
         layers[ currently_trained_layer-1 ]->fprop( 
             reconstruction_activations,
             layers[ currently_trained_layer-1 ]->expectation);
@@ -1043,6 +1571,22 @@
             layers[ currently_trained_layer-1 ]->fpropNLL(
                 expectations[ currently_trained_layer-1 ]);
 
+        if(reconstruct_hidden)
+        {
+            connections[ currently_trained_layer-1 ]->fprop( 
+                layers[ currently_trained_layer-1 ]->expectation, 
+                hidden_reconstruction_activations );
+            layers[ currently_trained_layer ]->fprop( 
+                hidden_reconstruction_activations,
+                layers[ currently_trained_layer ]->expectation );
+            layers[ currently_trained_layer ]->activation << 
+                hidden_reconstruction_activations;
+            layers[ currently_trained_layer ]->expectation_is_up_to_date = true;
+            costs[ currently_trained_layer-1 ] += 
+                layers[ currently_trained_layer ]->fpropNLL(
+                    output);
+        }
+
         if( partial_costs && partial_costs[ currently_trained_layer-1 ] )
         {
             partial_costs[ currently_trained_layer-1 ]->fprop( 
@@ -1074,10 +1618,10 @@
     
     for( int i=0 ; i<partial_costs.size() ; i++ )
     {
-        TVec<string> cost_names = partial_costs[i]->name();
-        for(int j=0; j<cost_names.length(); j++)
-            cost_names.push_back("partial_cost_" + tostring(i+1) + "_" + 
-                cost_names[j]);
+        TVec<string> names = partial_costs[i]->name();
+        for(int j=0; j<names.length(); j++)
+            cost_names.push_back("partial" + tostring(i) + "." + 
+                names[j]);
     }
 
     cost_names.append( final_cost->name() );
@@ -1104,6 +1648,10 @@
             correlation_layers[i]->setLearningRate( the_learning_rate );
             correlation_connections[i]->setLearningRate( the_learning_rate );
         }
+        if(direct_connections.length() != 0)
+        {
+            direct_connections[i]->setLearningRate( the_learning_rate );
+        }
     }
     layers[n_layers-1]->setLearningRate( the_learning_rate );
 

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.h
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-09-07 22:07:33 UTC (rev 8056)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.h	2007-09-07 22:08:43 UTC (rev 8057)
@@ -96,6 +96,9 @@
     //! The number of fine-tunig steps is defined by nstages.
     TVec<int> training_schedule;
 
+    //! Whether to do things by stages, including fine-tuning, or on-line
+    bool online;
+
     //! The layers of units in the network
     TVec< PP<RBMLayer> > layers;
 
@@ -110,6 +113,11 @@
     //! output sizes, compatible with their corresponding layers.
     TVec< PP<RBMConnection> > correlation_connections;
 
+    //! Optional weights from each inputs to all other inputs'
+    //! reconstruction, which can capture simple (linear or log-linear)
+    //! correlations between inputs.
+    mutable TVec< PP<RBMConnection> > direct_connections;
+
     //! Module that takes as input the output of the last layer
     //! (layers[n_layers-1), and feeds its output to final_cost
     //! which defines the fine-tuning criteria.
@@ -134,6 +142,10 @@
     //! layers (up to the currently trained layer) should be computed.
     bool compute_all_test_costs;
 
+    //! Indication that the autoassociators are also trained to
+    //! reconstruct their hidden layers (inspired from CD1 in an RBM)
+    bool reconstruct_hidden;
+
     //#####  Public Learnt Options  ###########################################
 
     //! Number of layers
@@ -184,6 +196,9 @@
     void fineTuningStep( const Vec& input, const Vec& target,
                          Vec& train_costs );
 
+    void onlineStep( const Vec& input, const Vec& target,
+                         Vec& train_costs );
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -224,13 +239,25 @@
     
     //! Reconstruction expectations
     mutable Vec reconstruction_expectations;
-    
+        
     //! Reconstruction activation gradients
     mutable Vec reconstruction_activation_gradients;
-    
+
     //! Reconstruction expectation gradients
     mutable Vec reconstruction_expectation_gradients;
 
+    //! Reconstruction activation gradients coming from hidden reconstruction
+    mutable Vec reconstruction_activation_gradients_from_hid_rec;
+    
+    //! Reconstruction expectation gradients coming from hidden reconstruction
+    mutable Vec reconstruction_expectation_gradients_from_hid_rec;
+
+    //! Hidden reconstruction activations
+    mutable Vec hidden_reconstruction_activations;
+    
+    //! Hidden reconstruction activation gradients
+    mutable Vec hidden_reconstruction_activation_gradients;
+    
     //! Activations before the correlation layer
     mutable TVec<Vec> correlation_activations;
     
@@ -246,6 +273,15 @@
     //! Hidden layers for the correlation connections
     mutable TVec< PP<RBMLayer> > correlation_layers;
 
+    //! Activations from the direct connections
+    mutable Vec direct_activations;
+
+    //! Sum of activations from the direct and reconstruction connections
+    mutable Vec direct_and_reconstruction_activations;
+
+    //! Gradient of sum of activations from the direct and reconstruction connections
+    mutable Vec direct_and_reconstruction_activation_gradients;
+
     //! Position in the total cost vector of the different partial costs
     mutable TVec<int> partial_costs_positions;
     
@@ -273,7 +309,7 @@
     
     //! Indication whether final_cost has learning rate
     bool final_cost_has_learning_rate;
-    
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From tihocan at mail.berlios.de  Mon Sep 10 18:13:16 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 10 Sep 2007 18:13:16 +0200
Subject: [Plearn-commits] r8058 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709101613.l8AGDGfE013074@sheep.berlios.de>

Author: tihocan
Date: 2007-09-10 18:13:16 +0200 (Mon, 10 Sep 2007)
New Revision: 8058

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
Log:
Added declareMethods so we can manually free shared memory, in case the object destructor is not called

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-09-07 22:08:43 UTC (rev 8057)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.cc	2007-09-10 16:13:16 UTC (rev 8058)
@@ -44,6 +44,9 @@
 #include <sys/sem.h>
 #include <sys/shm.h>
 
+#define PL_LOG_MODULE_NAME "NatGradSMPNNet"
+#include <plearn/io/pl_log.h>
+
 namespace PLearn {
 using namespace std;
 
@@ -354,6 +357,17 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void NatGradSMPNNet::declareMethods(RemoteMethodMap& rmm)
+{
+    declareMethod(rmm, "freeSharedMemory", &NatGradSMPNNet::freeSharedMemory,
+        (BodyDoc("Free shared memory ressources.")));
+
+    inherited::declareMethods(rmm);
+}
+
 ////////////
 // build_ //
 ////////////
@@ -421,11 +435,16 @@
     freeSharedMemory(); // First deallocate memory if needed.
     long total_memory_needed = long(n_params) * sizeof(real);
     params_id = shmget(IPC_PRIVATE, total_memory_needed, 0666 | IPC_CREAT);
-    PLCHECK( params_id != -1 );
+    DBG_MODULE_LOG << "params_id = " << params_id << endl;
+    if (params_id == -1) {
+        PLERROR("In NatGradSMPNNet::build_ - Error while allocating shared "
+                "memory (errno = %d)", errno);
+    }
     params_ptr = (real*) shmat(params_id, 0, 0);
     PLCHECK( params_ptr );
     long total_int_memory_needed = 1 * sizeof(int);
     params_int_id = shmget(IPC_PRIVATE, total_int_memory_needed, 0666 | IPC_CREAT);
+    DBG_MODULE_LOG << "params_int_id = " << params_int_id << endl;
     PLCHECK( params_int_id != -1 );
     params_int_ptr = (int*) shmat(params_int_id, 0, 0);
     PLCHECK( params_int_ptr );
@@ -575,6 +594,7 @@
 //////////////////////
 void NatGradSMPNNet::freeSharedMemory()
 {
+    DBG_MODULE_LOG << "Freeing shared memory" << endl;
     if (params_ptr) {
         shmctl(params_id, IPC_RMID, 0);
         params_ptr = NULL;
@@ -962,7 +982,7 @@
     const Profiler::Stats& synch_stats = Profiler::getStats("Synchronization");
     real synch_time = (synch_stats.user_duration + synch_stats.system_duration)
         / real(Profiler::ticksPerSecond());
-    pout << "Synch time: " << synch_time << endl;
+    DBG_MODULE_LOG << "Synch time: " << synch_time << endl;
     */
 
     // Get current stage (for debug purpose).

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-09-07 22:08:43 UTC (rev 8057)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradSMPNNet.h	2007-09-10 16:13:16 UTC (rev 8058)
@@ -283,9 +283,11 @@
     //#####  Protected Member Functions  ######################################
 
     //! Declares the class options.
-    // (PLEASE IMPLEMENT IN .cc)
     static void declareOptions(OptionList& ol);
 
+    //! Declares the class methods.
+    static void declareMethods(RemoteMethodMap& rmm);
+
     //! one minibatch training step
     void onlineStep(int t, const Mat& targets, Mat& train_costs, Vec example_weights);
 



From nouiz at mail.berlios.de  Mon Sep 10 21:04:41 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Sep 2007 21:04:41 +0200
Subject: [Plearn-commits] r8059 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200709101904.l8AJ4fOi011134@sheep.berlios.de>

Author: nouiz
Date: 2007-09-10 21:04:40 +0200 (Mon, 10 Sep 2007)
New Revision: 8059

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
-Made the bqtools back-end work correctly.
-Better help message.
-Replaced the file function with open


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-10 16:13:16 UTC (rev 8058)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-10 19:04:40 UTC (rev 8059)
@@ -116,6 +116,8 @@
             self.__dict__[key] = args[key]
 
         # check if log directory exists, if not create it
+        if (not os.path.exists('LOGS')):
+            os.mkdir('LOGS')
         if (not os.path.exists(self.log_dir)):
 #            if self.dolog or self.file_redirect_stdout or self.file_redirect_stderr:
             os.mkdir(self.log_dir)
@@ -143,11 +145,11 @@
         output = PIPE
         error = PIPE
         if int(self.file_redirect_stdout):
-            output = file(stdout_file, 'w')
+            output = open(stdout_file, 'w')
         if self.redirect_stderr_to_stdout:
             error = STDOUT
         elif int(self.file_redirect_stderr):
-            error = file(stderr_file, 'w')
+            error = open(stderr_file, 'w')
         return (output,error)
             
     def exec_pre_batch(self):
@@ -372,20 +374,43 @@
 class DBIbqtools(DBIBase):
 
     def __init__( self, commands, **args ):
+        self.nb_proc = 1
+        self.clean_up = True
+        self.micro = 1
+        self.queue = "qwork at ms"
+        self.long = False
+        self.duree = "12:00:00"
+
         DBIBase.__init__(self, commands, **args)
 
+        self.nb_proc = int(self.nb_proc)
+        self.micro = int(self.micro)
+        
+### We can't accept the symbols "," as this cause trouble with bqtools
+        if self.log_dir.find(',')!=-1 or self.log_file.find(',')!=-1:
+            print "[DBI] ERROR: The log file and the log dir should not have the symbol ','"
+            print "[DBI] log file=",self.log_file
+            print "[DBI] log dir=",self.log_dir
+            sys.exit(1)
+
         # create directory in which all the temp files will be created
-        self.temp_dir = 'batch_' + self.unique_id + '_tmp'
+        self.temp_dir = 'bqtools_tmp_' + os.path.split(self.log_dir)[1]
+        print "[DBI] All bqtools file will be in ",self.temp_dir
         os.mkdir(self.temp_dir)
         os.chdir(self.temp_dir)
+        
+        if self.long:
+            self.queue = "qlong at ms"
+            # Get max job duration from environment variable if it is set.
+            max = os.getenv("BQ_MAX_JOB_DURATION")
+            if max:
+                self.duree = max
+            else:
+                self.duree = "1200:00:00" #50 days
 
-        # create the right symlink for parent in self.temp_dir_name
-        self.parent_dir = 'parent'
-        os.symlink( '..', self.parent_dir )
-
         # create the information about the tasks
         args['temp_dir'] = self.temp_dir
-        
+        self.args=args
         self.add_commands(commands)
 
     def add_commands(self,commands):
@@ -395,9 +420,8 @@
         # create the information about the tasks
         for command in commands:
             self.tasks.append(Task(command, self.tmp_dir, self.log_dir,
-                                   self.time_format,
-                                   [self.pre_tasks, 'cd parent;'],
-                                   self.post_tasks,self.dolog,False,args))
+                                   self.time_format,self.pre_tasks,
+                                   self.post_tasks,self.dolog,False,self.args))
     def run(self):
         pre_batch_command = ';'.join( self.pre_batch );
         post_batch_command = ';'.join( self.post_batch );
@@ -415,6 +439,7 @@
                 HOME=%s
                 export HOME
 
+                cd ../../../
                 (%s '~~task~~')'''
                 % (bq_cluster_home, bq_shell_cmd)
                 ) )
@@ -438,31 +463,35 @@
         # create the bqsubmit.dat, with
         bqsubmit_dat = open( 'bqsubmit.dat', 'w' )
         bqsubmit_dat.write( dedent('''\
-                batchName = dbi_batch
+                batchName = dbi_batch_%s
                 command = sh launcher
                 templateFiles = launcher
-                linkFiles = parent;parent/utils.py
-                remoteHost = ss3
+                submitOptions = -q %s -l walltime=%s
                 param1 = (task, logfile) = load tasks, logfiles
-                concurrentJobs = 200
-
-                ''') )
+                linkFiles = launcher
+                concurrentJobs = %d
+                preBatch = rm -f _*.BQ
+                microJobs = %d
+                '''%(self.unique_id[1:6],self.queue,self.duree,self.nb_proc,self.micro)) )
+        print self.unique_id
+        if self.clean_up:
+            bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;')
         bqsubmit_dat.close()
 
         # Execute pre-batch
         if len(self.pre_batch)>0:
             exec_pre_batch()
 
+        print "[DBI] All the log will be in the directory: ",self.log_dir
         # Launch bqsubmit
         if not self.test:
-            (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
             task.set_scheduled_time()
-            self.p = Popen( 'bqsubmit', shell=True, stdout=output, stderr=error)
+            self.p = Popen( 'bqsubmit', shell=True)
+            self.p.wait()
         else:
             print "[DBI] Test mode, we generate all the file, but we do not execute bqsubmit"
             if self.dolog:
                 print "[DBI] The scheduling time will not be logged when you will submit the generated file" 
-        os.chdir('parent')
 
         # Execute post-batchs
         if len(self.post_batch)>0:

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-10 16:13:16 UTC (rev 8058)
+++ trunk/scripts/cdispatch	2007-09-10 19:04:40 UTC (rev 8059)
@@ -2,18 +2,56 @@
 import sys,os,re,time,datetime
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--log|*--nolog] [--cluster[=nb_process]|--local[=nb_process]|*--condor] [--nb_proc=nb_process] [--test] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
+ShortHelp='Usage: cdispatch [--help|-h] [--dbilog|*--nodbilog] [--cluster[=nb_process]|--local[=nb_process]|--bqtools[=nb_process]|*--condor] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
 LongHelp="""
 Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
 
 %s
-parameter for local and cluster:--nb_proc=INT, give the maximum number of concurent jobs running
-parameter only for cluster:--duree,--wait,--3264,--32,--64,
-parameter only for condor:--req=X
 
-parameter --local=X is the same as --local --nb_proc=X
-parameter --cluster=X is the same as --cluster --nb_proc=X
+option not explained:
+  --help, -h
+  --dbilog, --nodbilog
+  --cluster,--local,--bqtools,--condor
+  --file=FILEPATH
+  
+common option:
+  The '--test' option make that cdispatch generate the file $ScriptName, but do not execute it. That way you can see what cdispatch generate. Also, this file make dbi in test mode, so dbi do not execute automaticaly the experiment il $ScriptName is executer
+local, bqtools and cluster parameter:
+  --nb_proc=nb_process, give the maximum number of concurent jobs running
+    --local=X is the same as --local --nb_proc=X
+    --cluster=X is the same as --cluster --nb_proc=X
+    --bqtools=X is the same as --bqtools --nb_proc=X
 
+
+bqtools and cluster option:
+  The '--duree' option tell the maximum length of the jobs. The have the cluster syntaxe of accepted value 'cluster --help'. The bqtools syntaxe is '--duree=12:13:15'. This give 12 hours, 13 minutes and 15 seconds
+
+bqtools only option:
+  The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
+  have a very short duration. This may prevent some queue crashes. The nb_batch value
+  is the number of experience to group together in a batch.(default 20)
+
+  The '--long' option must be used with BqTools to launch jobs whose duration
+  is more than 5 days. The maximum duration of a job will be either the
+  BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
+  set, and 1200:00:00 (50 days) otherwise.
+  Since long jobs are launched on a different queue with few nodes, please make
+  sure you are not using too many nodes at once.
+  If this option is not set, the maximum duration of each job will be 120 hours
+  (5 days).
+
+cluster only option:
+  The '--wait' is transfered to cluster. This must be enabled if their is not nb_process available compute node. Otherwise when their is no compute node available, the launch of that command fail.
+  The '--3264', '--32' or '--64' tell the type of cpu the compute node must have to execute the commands.
+
+condor only option:
+  The '--req=\"CONDOR_REQUIREMENT\"' option make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
+
+  cdispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
+     or
+  cdispatch '--req=Machine==\\\"computer.example.com\\\"' 
+
+
 where <command-template> is interpreted as follows: the first argument
 is the <command> above, and the rest are interpreted as <arguments>.
 The arguments may contain segments of the form {{a,b,c,d}}, which trigger
@@ -43,14 +81,6 @@
   aplearn myscript.plearn numhidden=25 wd=0.01
   aplearn myscript.plearn numhidden=25 wd=0.001
 
-The optional parameter '--test' make that cdispatch generate the file $ScriptName, but do not execute it. That way you can see what cdispatch generate.
-
-The optional parameter '--req=\"CONDOR_REQUIREMENT\"' make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
-
-cdispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
-or
-cdispatch '--req=Machine==\\\"computer.example.com\\\"' 
-
 If the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
 
 cdispatch --test --file=tests
@@ -71,10 +101,17 @@
     if argv == "--help" or argv == "-h":
         print LongHelp
         sys.exit(1)
-    elif argv == "--nolog":
+    elif argv == "--nodbilog":
         dbi_param["dolog"]=False
-    elif argv == "--log":
+    elif argv == "--dbilog":
         dbi_param["dolog"]=True
+    elif argv.startswith("--bqtools"):
+        optionargs.append(argv[2:9])
+        if len(argv)>9:
+            assert(argv[9]=="=")
+            dbi_param["nb_proc"]=argv[10:]
+        dbi_param["file_redirect_stderr"]=True
+        dbi_param["file_redirect_stderr"]=True
     elif argv.startswith("--cluster"):
         optionargs.append(argv[2:9])
         if len(argv)>9:
@@ -103,6 +140,15 @@
         dbi_param["wait"]=True
     elif argv[0:6] == "--req=":
         dbi_param["requirements"]="\"%s\""%argv[6:]
+    elif argv == "--no_clean_up":
+        dbi_param["clean_up"]=False
+    elif argv == "long":
+        dbi_param["long"] = True
+    elif argv.startswith("--micro"):
+        dbi_param["micro"]=20
+        if len(argv)>7:
+            assert(argv[7]=="=")
+            dbi_param["micro"]=argv[8:]
     elif argv[0:1] == '-':
 	print "Unknow parameter (%s)",argv
 	print ShortHelp
@@ -172,6 +218,8 @@
     launch_cmd='Cluster'
 elif "local" in optionargs:
     launch_cmd='Local'
+elif "bqtools" in optionargs:
+    launch_cmd='bqtools'
 else:
     launch_cmd='Condor'
 
@@ -180,8 +228,9 @@
     t[0]=os.path.split(t[0])[1]
     tmp="_".join(t)
     tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
-    tmp+=str(datetime.datetime.now()).replace(' ','_')
-    print "tmp:",tmp
+    ### We need to remove the symbols "," as this cause trouble with bqtools
+    tmp=re.sub( ',', '-', tmp )
+    tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
     dbi_param["log_dir"]=os.path.join("LOGS",tmp)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 else:



From nouiz at mail.berlios.de  Mon Sep 10 21:35:33 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 10 Sep 2007 21:35:33 +0200
Subject: [Plearn-commits] r8060 - in trunk: python_modules/plearn/parallel
	python_modules/plearn/utilities scripts
Message-ID: <200709101935.l8AJZXI5013194@sheep.berlios.de>

Author: nouiz
Date: 2007-09-10 21:35:32 +0200 (Mon, 10 Sep 2007)
New Revision: 8060

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/python_modules/plearn/utilities/toolkit.py
   trunk/scripts/cdispatch
Log:
-"smart" auto selection of backend
-better batch name for bqtools
-new function that look for file in string like the PATH environnement variable


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-10 19:04:40 UTC (rev 8059)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-10 19:35:32 UTC (rev 8060)
@@ -463,7 +463,7 @@
         # create the bqsubmit.dat, with
         bqsubmit_dat = open( 'bqsubmit.dat', 'w' )
         bqsubmit_dat.write( dedent('''\
-                batchName = dbi_batch_%s
+                batchName = dbi_%s
                 command = sh launcher
                 templateFiles = launcher
                 submitOptions = -q %s -l walltime=%s
@@ -472,7 +472,7 @@
                 concurrentJobs = %d
                 preBatch = rm -f _*.BQ
                 microJobs = %d
-                '''%(self.unique_id[1:6],self.queue,self.duree,self.nb_proc,self.micro)) )
+                '''%(self.unique_id[1:12],self.queue,self.duree,self.nb_proc,self.micro)) )
         print self.unique_id
         if self.clean_up:
             bqsubmit_dat.write('postBatch = rm -rf dbi_batch*.BQ ; rm -f logfiles tasks launcher bqsubmit.dat ;')

Modified: trunk/python_modules/plearn/utilities/toolkit.py
===================================================================
--- trunk/python_modules/plearn/utilities/toolkit.py	2007-09-10 19:04:40 UTC (rev 8059)
+++ trunk/python_modules/plearn/utilities/toolkit.py	2007-09-10 19:35:32 UTC (rev 8060)
@@ -6,6 +6,8 @@
 I{similar_tasks.py} L{utilities} submodule to move those functions to.
 """
 import inspect, os, popen2, shutil, string, sys, time, types
+from os.path import exists, join, abspath
+from string import split
 
 def boxed_lines(s, box_width, indent=''):
     if len(s) <= box_width:
@@ -541,6 +543,22 @@
         print prefix, cmd
         os.system( cmd )
 
+#original version: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52224
+def search_file(filename, search_path):
+    """Given a search path, find file
+     Can be used with the PATH variable as search_path
+    """
+    file_found = 0
+    paths = split(search_path, os.pathsep)
+    for path in paths:
+        if exists(join(path, filename)):
+            file_found = 1
+            break
+    if file_found:
+        return abspath(join(path, filename))
+    else:
+        return None
+    
 class ListMap(dict):
     def __getitem__(self, key):
         if not key in self:

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-10 19:04:40 UTC (rev 8059)
+++ trunk/scripts/cdispatch	2007-09-10 19:35:32 UTC (rev 8060)
@@ -1,10 +1,11 @@
 #!/usr/bin/env python
 import sys,os,re,time,datetime
+from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--dbilog|*--nodbilog] [--cluster[=nb_process]|--local[=nb_process]|--bqtools[=nb_process]|*--condor] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
+ShortHelp='Usage: cdispatch [--help|-h] [--dbilog|*--nodbilog] [--cluster[=nb_process]|--local[=nb_process]|--bqtools[=nb_process]|--condor] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
 LongHelp="""
-Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools.
+Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools. If no system is selected on the command line, we try them in this order: condor, bqsubmit, cluster local
 
 %s
 
@@ -91,39 +92,46 @@
 if len(sys.argv) == 1:
     print ShortHelp
     sys.exit(1)
-optionargs = []
-otherargs = []
 FILE = ""
 dbi_param={}
 
-for argv in sys.argv[1:]:
 
+PATH=os.getenv('PATH')
+if search_file('condor_submit',PATH):
+    launch_cmd = 'Condor'
+elif search_file('bqsubmit',PATH):
+    launch_cmd = 'bqtools'
+elif search_file('cluster',PATH):
+    launch_cmd = 'Cluster'
+else:
+    launch_cmd = 'Local'
+
+command_argv = sys.argv[1:]
+for argv in command_argv:
+
     if argv == "--help" or argv == "-h":
         print LongHelp
-        sys.exit(1)
+        sys.exit(0)
     elif argv == "--nodbilog":
         dbi_param["dolog"]=False
     elif argv == "--dbilog":
         dbi_param["dolog"]=True
     elif argv.startswith("--bqtools"):
-        optionargs.append(argv[2:9])
+        launch_cmd = "bqtools"
         if len(argv)>9:
             assert(argv[9]=="=")
             dbi_param["nb_proc"]=argv[10:]
-        dbi_param["file_redirect_stderr"]=True
-        dbi_param["file_redirect_stderr"]=True
     elif argv.startswith("--cluster"):
-        optionargs.append(argv[2:9])
+        launch_cmd = "Cluster"
         if len(argv)>9:
             assert(argv[9]=="=")
             dbi_param["nb_proc"]=argv[10:]
     elif argv == "--condor":
-        #it is the default
-        optionargs.append(argv[2:])
+        launch_cmd = "Condor"
     elif argv.startswith("--duree="):
         dbi_param["duree"]=argv[8:]
     elif argv.startswith("--local"):
-        optionargs.append(argv[2:7])
+        launch_cmd = "Local"
         if len(argv)>7:
             assert(argv[7]=="=")
             dbi_param["nb_proc"]=argv[8:]
@@ -133,7 +141,6 @@
         dbi_param["test"]=True
     elif argv.startswith("--file="):
         FILE = argv[7:]
-        optionargs.append(argv[2:])
     elif argv == "--32"  or argv == "--64" or argv == "--3264":
         dbi_param["arch"]=argv[2:]
     elif argv == "--wait":
@@ -154,17 +161,17 @@
 	print ShortHelp
         sys.exit(1)
     else:
-        otherargs.append(argv)
-        
-if len(otherargs) == 0 and FILE == "":
-    print ShortHelp
-    sys.exit(1)
+        break
+    command_argv.remove(argv)
 
+print command_argv
+print "\n\nThe jobs will be launched on the system:", launch_cmd,"\n\n"
 
-if "local" in optionargs and "cluster" in optionargs:
-    print "--cluster and --local can't be used together"
+if len(command_argv) == 0 and FILE == "":
+    print ShortHelp
     sys.exit(1)
 
+
 def generate_combination(repl):
     if repl == []:
         return []
@@ -212,17 +219,8 @@
         commands+=generate_commands(sp)
     FD.close
 else:
-    commands=generate_commands(otherargs)
+    commands=generate_commands(command_argv)
 
-if "cluster" in optionargs:
-    launch_cmd='Cluster'
-elif "local" in optionargs:
-    launch_cmd='Local'
-elif "bqtools" in optionargs:
-    launch_cmd='bqtools'
-else:
-    launch_cmd='Condor'
-
 if FILE == "":    
     t = [x for x in sys.argv[1:] if not x[:2]=="--"]
     t[0]=os.path.split(t[0])[1]



From nouiz at mail.berlios.de  Tue Sep 11 16:37:58 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 16:37:58 +0200
Subject: [Plearn-commits] r8061 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200709111437.l8BEbwHQ024794@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 16:37:57 +0200 (Tue, 11 Sep 2007)
New Revision: 8061

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
-Added the wait() function and modified run() to do not block after starting the jobs


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-10 19:35:32 UTC (rev 8060)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 14:37:57 UTC (rev 8061)
@@ -180,6 +180,9 @@
     def run(self):
         pass
 
+    def wait(self):
+        print "[DBI] WARNING the wait function was not overrided by the sub class!"
+        
 class Task:
 
     def __init__(self, command, tmp_dir, log_dir, time_format, pre_tasks=[], post_tasks=[], dolog = True, gen_unique_id = True, args = {}):
@@ -299,10 +302,11 @@
     def __init__(self, commands, **args ):
         self.duree=None
         self.arch=None
-        self.wait=None
+        self.cluster_wait=None
         self.threads=[]
         self.started=0
         self.nb_proc=50
+        self.mt=None
         DBIBase.__init__(self, commands, **args)
         self.add_commands(commands)
         self.nb_proc=int(self.nb_proc)
@@ -330,7 +334,7 @@
             command += " --typecpu all"
         if self.duree:
             command += " --duree "+self.duree
-        if self.wait:
+        if self.cluster_wait:
             command += " --wait"
         command += " --execute '"+string.join(task.commands,';') + "'"
         self.started+=1
@@ -357,9 +361,8 @@
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        mt= MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
-        mt.start()
-        mt.join()
+        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
+        self.mt.start()
 
         # Execute post-batchs
         if len(self.post_batch)>0:
@@ -371,6 +374,12 @@
         #TODO: delete all log files for the current batch
         pass
 
+    def wait(self):
+        if self.mt:
+            self.mt.join()
+        else:
+            print "[DBI] WARNING jobs not started!"
+                
 class DBIbqtools(DBIBase):
 
     def __init__( self, commands, **args ):
@@ -497,7 +506,9 @@
         if len(self.post_batch)>0:
             exec_post_batch()
 
-            
+    def wait(self):
+        print "[DBI] WARNING cannot wait until all jobs are done for bqtools, use bqwatch or bqstatus"
+                
 class DBICondor(DBIBase):
 
     def __init__( self, commands, **args ):
@@ -723,9 +734,9 @@
         if len(self.post_batch)>0:
             exec_post_batch()
 
-
-
-
+    def wait(self):
+        print "[DBI] WARNING no waiting for all job to finish implemented for condor, use 'condor_q' or 'condor_wait %s/condor.log'"%(self.log_dir)
+                
     def clean(self):
         pass
 
@@ -734,10 +745,9 @@
     def __init__( self, commands, **args ):
         self.nb_proc=1
         DBIBase.__init__(self, commands, **args)
-        if isinstance(self.nb_proc,basestring):
-            self.nb_proc=int(self.nb_proc)
         self.args=args
         self.threads=[]
+        self.mt = None
         self.started=0
         self.nb_proc=int(self.nb_proc)
         self.add_commands(commands)
@@ -819,9 +829,8 @@
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
-        mt.start()
-        mt.join()
+        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
+        self.mt.start()
         
         # Execute post-batchs
         if len(self.post_batch)>0:
@@ -832,6 +841,12 @@
     def clean(self):
         pass
 
+    def wait(self):
+        if self.mt:
+            self.mt.join()
+        else:
+            print "[DBI] WARNING jobs not started!"
+                
 class SshHost:
     def __init__(self, hostname):
         self.hostname= hostname
@@ -936,6 +951,9 @@
         if len(self.pre_batch)>0:
             exec_pre_batch()
 
+        if self.test:
+            print "[DBI] In testmode, we only print the command that would be executed."
+            
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         print "[DBI] tasks= ", self.tasks
         for task in self.tasks:
@@ -949,10 +967,15 @@
         #TODO: delete all log files for the current batch
         pass
 
+    def wait(self):
+        #TODO
+        print "[DBI] WARNING the wait function was not implement for the ssh backend!"
 
-
 # creates an object of type ('DBI' + launch_system) if it exists
 def DBI(commands, launch_system, **args):
+"""The Distributed Batch Interface is a collection of python classes
+that make it easy to execute commands in parallel using different
+systems like condor, bqtools on Mammoth, the cluster command or localy."""
     try:
         jobs = eval('DBI'+launch_system+'(commands,**args)')
     except NameError:

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-10 19:35:32 UTC (rev 8060)
+++ trunk/scripts/cdispatch	2007-09-11 14:37:57 UTC (rev 8061)
@@ -144,7 +144,7 @@
     elif argv == "--32"  or argv == "--64" or argv == "--3264":
         dbi_param["arch"]=argv[2:]
     elif argv == "--wait":
-        dbi_param["wait"]=True
+        dbi_param["cluster_wait"]=True
     elif argv[0:6] == "--req=":
         dbi_param["requirements"]="\"%s\""%argv[6:]
     elif argv == "--no_clean_up":
@@ -257,6 +257,7 @@
     SCRIPT.write(
 """)
 jobs.run()
+jobs.wait()
 # There is %d command in the script"""%(len(commands)))
         
     SCRIPT.close()



From nouiz at mail.berlios.de  Tue Sep 11 16:56:27 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 16:56:27 +0200
Subject: [Plearn-commits] r8062 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200709111456.l8BEuRSN025835@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 16:56:26 +0200 (Tue, 11 Sep 2007)
New Revision: 8062

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
bugfix


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 14:37:57 UTC (rev 8061)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 14:56:26 UTC (rev 8062)
@@ -973,9 +973,10 @@
 
 # creates an object of type ('DBI' + launch_system) if it exists
 def DBI(commands, launch_system, **args):
-"""The Distributed Batch Interface is a collection of python classes
-that make it easy to execute commands in parallel using different
-systems like condor, bqtools on Mammoth, the cluster command or localy."""
+    """The Distributed Batch Interface is a collection of python classes
+    that make it easy to execute commands in parallel using different
+    systems like condor, bqtools on Mammoth, the cluster command or localy.
+    """
     try:
         jobs = eval('DBI'+launch_system+'(commands,**args)')
     except NameError:

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-11 14:37:57 UTC (rev 8061)
+++ trunk/scripts/cdispatch	2007-09-11 14:56:26 UTC (rev 8062)
@@ -107,7 +107,7 @@
     launch_cmd = 'Local'
 
 command_argv = sys.argv[1:]
-for argv in command_argv:
+for argv in sys.argv[1:]:
 
     if argv == "--help" or argv == "-h":
         print LongHelp



From nouiz at mail.berlios.de  Tue Sep 11 16:58:33 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 16:58:33 +0200
Subject: [Plearn-commits] r8063 - trunk/scripts
Message-ID: <200709111458.l8BEwX8t025994@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 16:58:32 +0200 (Tue, 11 Sep 2007)
New Revision: 8063

Modified:
   trunk/scripts/cdispatch
Log:
bugfix


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-11 14:56:26 UTC (rev 8062)
+++ trunk/scripts/cdispatch	2007-09-11 14:58:32 UTC (rev 8063)
@@ -272,6 +272,7 @@
     t2=time.time()
     print "it took %f s to create the DBI objects"%(t2-t1)
     jobs.run()
+    jobs.wait()
     t3=time.time()
     print "it took %f s to launch all the commands"%(t3-t2)
 



From nouiz at mail.berlios.de  Tue Sep 11 19:31:25 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 19:31:25 +0200
Subject: [Plearn-commits] r8064 - trunk/scripts
Message-ID: <200709111731.l8BHVPFK031569@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 19:31:25 +0200 (Tue, 11 Sep 2007)
New Revision: 8064

Modified:
   trunk/scripts/cdispatch
Log:
remove unneeded print


Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-11 14:58:32 UTC (rev 8063)
+++ trunk/scripts/cdispatch	2007-09-11 17:31:25 UTC (rev 8064)
@@ -164,7 +164,6 @@
         break
     command_argv.remove(argv)
 
-print command_argv
 print "\n\nThe jobs will be launched on the system:", launch_cmd,"\n\n"
 
 if len(command_argv) == 0 and FILE == "":



From nouiz at mail.berlios.de  Tue Sep 11 20:30:38 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 20:30:38 +0200
Subject: [Plearn-commits] r8065 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200709111830.l8BIUcjv005219@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 20:30:38 +0200 (Tue, 11 Sep 2007)
New Revision: 8065

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/cdispatch
Log:
les options --32 --64 et --3264 sont fonctionne pour condor

Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 17:31:25 UTC (rev 8064)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 18:30:38 UTC (rev 8065)
@@ -544,17 +544,25 @@
                 # architecture we execute on both. Otherwise we execute on the
                 # same architecture as the architecture of the launch computer
             self.cplat = get_condor_platform()
-            if c.endswith('.32'):
+            if self.arch == "32":
                 self.targetcondorplatform='INTEL'
-                self.targetplatform='linux-i386'
                 newcommand=command
+            elif self.arch == "64":
+                self.targetcondorplatform='X86_64'
+                newcommand=command
+            elif self.arch == "3264":
+                #the same executable will be executed on all computer
+                #So it should be a 32 bits executable
+                self.targetcondorplatform='BOTH'
+                newcommand=command
+            elif c.endswith('.32'):
+                self.targetcondorplatform='INTEL'
+                newcommand=command
             elif c.endswith('.64'):
                 self.targetcondorplatform='X86_64'
-                self.targetplatform='linux-x86_64'
                 newcommand=command
             elif os.path.exists(c+".32") and os.path.exists(c+".64"):
                 self.targetcondorplatform='BOTH'
-                self.targetplatform='linux-i386'
                 #newcommand=c+".32"+c2
                 newcommand='if [ $CPUTYPE == \'x86_64\' ]; then'
                 newcommand+='  '+c+'.64'+c2
@@ -566,20 +574,14 @@
                 c+=".32"
             elif self.cplat=="INTEL" and os.path.exists(c+".32"):
                 self.targetcondorplatform='INTEL'
-                self.targetplatform='linux-i386'
                 c+=".32"
                 newcommand=c+c2
             elif self.cplat=="X86_64" and os.path.exists(c+".64"):
                 self.targetcondorplatform='X86_64'
-                self.targetplatform='linux-x86_64'
                 c+=".64"
                 newcommand=c+c2
             else:
                 self.targetcondorplatform=self.cplat
-                if self.cplat=='INTEL':
-                    self.targetplatform='linux-i386'
-                else:
-                    self.targetplatform='linux-x86_64'
                 newcommand=command
             
             if not os.path.exists(c):
@@ -621,14 +623,9 @@
         req=""
         if self.targetcondorplatform == 'BOTH':
             req="((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
-        elif self.targetcondorplatform == 'INTEL':
-            req="(Arch == \"INTEL\")"
-        elif self.targetcondorplatform == 'X86_64':
-            req="(Arch == \"X86_64\")"
-            
+        elif :
+            req="(Arch == \"%s\")"%(self.targetcondorplatform)
 
-        tplat=self.targetplatform
-
         if self.requirements != "":
             req = req+'&&('+self.requirements+')'
 

Modified: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-11 17:31:25 UTC (rev 8064)
+++ trunk/scripts/cdispatch	2007-09-11 18:30:38 UTC (rev 8065)
@@ -41,9 +41,11 @@
   If this option is not set, the maximum duration of each job will be 120 hours
   (5 days).
 
+cluster and condor option:
+  The '--3264', '--32' or '--64' tell the type of cpu the compute node must have to execute the commands.
+
 cluster only option:
   The '--wait' is transfered to cluster. This must be enabled if their is not nb_process available compute node. Otherwise when their is no compute node available, the launch of that command fail.
-  The '--3264', '--32' or '--64' tell the type of cpu the compute node must have to execute the commands.
 
 condor only option:
   The '--req=\"CONDOR_REQUIREMENT\"' option make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:



From nouiz at mail.berlios.de  Tue Sep 11 20:33:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 20:33:23 +0200
Subject: [Plearn-commits] r8066 - trunk/scripts
Message-ID: <200709111833.l8BIXNaC005936@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 20:33:23 +0200 (Tue, 11 Sep 2007)
New Revision: 8066

Added:
   trunk/scripts/dbidispatch
Removed:
   trunk/scripts/cdispatch
Log:
Changed the name of cdispatch to dbidispatch as it better reflect what it does


Deleted: trunk/scripts/cdispatch
===================================================================
--- trunk/scripts/cdispatch	2007-09-11 18:30:38 UTC (rev 8065)
+++ trunk/scripts/cdispatch	2007-09-11 18:33:23 UTC (rev 8066)
@@ -1,279 +0,0 @@
-#!/usr/bin/env python
-import sys,os,re,time,datetime
-from plearn.utilities.toolkit import search_file
-
-ScriptName="launchdbi.py"
-ShortHelp='Usage: cdispatch [--help|-h] [--dbilog|*--nodbilog] [--cluster[=nb_process]|--local[=nb_process]|--bqtools[=nb_process]|--condor] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
-LongHelp="""
-Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools. If no system is selected on the command line, we try them in this order: condor, bqsubmit, cluster local
-
-%s
-
-option not explained:
-  --help, -h
-  --dbilog, --nodbilog
-  --cluster,--local,--bqtools,--condor
-  --file=FILEPATH
-  
-common option:
-  The '--test' option make that cdispatch generate the file $ScriptName, but do not execute it. That way you can see what cdispatch generate. Also, this file make dbi in test mode, so dbi do not execute automaticaly the experiment il $ScriptName is executer
-local, bqtools and cluster parameter:
-  --nb_proc=nb_process, give the maximum number of concurent jobs running
-    --local=X is the same as --local --nb_proc=X
-    --cluster=X is the same as --cluster --nb_proc=X
-    --bqtools=X is the same as --bqtools --nb_proc=X
-
-
-bqtools and cluster option:
-  The '--duree' option tell the maximum length of the jobs. The have the cluster syntaxe of accepted value 'cluster --help'. The bqtools syntaxe is '--duree=12:13:15'. This give 12 hours, 13 minutes and 15 seconds
-
-bqtools only option:
-  The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
-  have a very short duration. This may prevent some queue crashes. The nb_batch value
-  is the number of experience to group together in a batch.(default 20)
-
-  The '--long' option must be used with BqTools to launch jobs whose duration
-  is more than 5 days. The maximum duration of a job will be either the
-  BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
-  set, and 1200:00:00 (50 days) otherwise.
-  Since long jobs are launched on a different queue with few nodes, please make
-  sure you are not using too many nodes at once.
-  If this option is not set, the maximum duration of each job will be 120 hours
-  (5 days).
-
-cluster and condor option:
-  The '--3264', '--32' or '--64' tell the type of cpu the compute node must have to execute the commands.
-
-cluster only option:
-  The '--wait' is transfered to cluster. This must be enabled if their is not nb_process available compute node. Otherwise when their is no compute node available, the launch of that command fail.
-
-condor only option:
-  The '--req=\"CONDOR_REQUIREMENT\"' option make that cdispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
-
-  cdispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
-     or
-  cdispatch '--req=Machine==\\\"computer.example.com\\\"' 
-
-
-where <command-template> is interpreted as follows: the first argument
-is the <command> above, and the rest are interpreted as <arguments>.
-The arguments may contain segments of the form {{a,b,c,d}}, which trigger
-parallel dispatch: a separate 'cluster --execute' command is issued for
-the rest of the command template, the first time with value a, the second
-time with value b, etc.  For example, the command (NOTE: THERE MUST NOT
-BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
-important to avoid shell misinterpretation) :
-
-  dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'
-
-is equivalent to launching three jobs in parallel on the cluster:
-
-  aplearn myscript.plearn numhidden=5
-  aplearn myscript.plearn numhidden=10
-  aplearn myscript.plearn numhidden=25
-
-If several arguments contain {{ }} forms, all combinations of arguments
-are taken, and the jobs are all launched in parallel.  For instance
-
-  dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'
-
-is equivalent to:
-
-  aplearn myscript.plearn numhidden=10 wd=0.01
-  aplearn myscript.plearn numhidden=10 wd=0.001
-  aplearn myscript.plearn numhidden=25 wd=0.01
-  aplearn myscript.plearn numhidden=25 wd=0.001
-
-If the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
-
-cdispatch --test --file=tests
-
-In the file, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
-"""%ShortHelp
-
-if len(sys.argv) == 1:
-    print ShortHelp
-    sys.exit(1)
-FILE = ""
-dbi_param={}
-
-
-PATH=os.getenv('PATH')
-if search_file('condor_submit',PATH):
-    launch_cmd = 'Condor'
-elif search_file('bqsubmit',PATH):
-    launch_cmd = 'bqtools'
-elif search_file('cluster',PATH):
-    launch_cmd = 'Cluster'
-else:
-    launch_cmd = 'Local'
-
-command_argv = sys.argv[1:]
-for argv in sys.argv[1:]:
-
-    if argv == "--help" or argv == "-h":
-        print LongHelp
-        sys.exit(0)
-    elif argv == "--nodbilog":
-        dbi_param["dolog"]=False
-    elif argv == "--dbilog":
-        dbi_param["dolog"]=True
-    elif argv.startswith("--bqtools"):
-        launch_cmd = "bqtools"
-        if len(argv)>9:
-            assert(argv[9]=="=")
-            dbi_param["nb_proc"]=argv[10:]
-    elif argv.startswith("--cluster"):
-        launch_cmd = "Cluster"
-        if len(argv)>9:
-            assert(argv[9]=="=")
-            dbi_param["nb_proc"]=argv[10:]
-    elif argv == "--condor":
-        launch_cmd = "Condor"
-    elif argv.startswith("--duree="):
-        dbi_param["duree"]=argv[8:]
-    elif argv.startswith("--local"):
-        launch_cmd = "Local"
-        if len(argv)>7:
-            assert(argv[7]=="=")
-            dbi_param["nb_proc"]=argv[8:]
-    elif argv.startswith("--nb_proc="):
-        dbi_param["nb_proc"]=argv[10:]
-    elif argv == "--test":
-        dbi_param["test"]=True
-    elif argv.startswith("--file="):
-        FILE = argv[7:]
-    elif argv == "--32"  or argv == "--64" or argv == "--3264":
-        dbi_param["arch"]=argv[2:]
-    elif argv == "--wait":
-        dbi_param["cluster_wait"]=True
-    elif argv[0:6] == "--req=":
-        dbi_param["requirements"]="\"%s\""%argv[6:]
-    elif argv == "--no_clean_up":
-        dbi_param["clean_up"]=False
-    elif argv == "long":
-        dbi_param["long"] = True
-    elif argv.startswith("--micro"):
-        dbi_param["micro"]=20
-        if len(argv)>7:
-            assert(argv[7]=="=")
-            dbi_param["micro"]=argv[8:]
-    elif argv[0:1] == '-':
-	print "Unknow parameter (%s)",argv
-	print ShortHelp
-        sys.exit(1)
-    else:
-        break
-    command_argv.remove(argv)
-
-print "\n\nThe jobs will be launched on the system:", launch_cmd,"\n\n"
-
-if len(command_argv) == 0 and FILE == "":
-    print ShortHelp
-    sys.exit(1)
-
-
-def generate_combination(repl):
-    if repl == []:
-        return []
-    else:
-        res = []
-        x = repl[0]
-        res1 = generate_combination(repl[1:])
-        for y in x:
-            if res1 == []:
-                res.append(y)
-            else:
-                for r in res1:
-                    res.append(y+" "+r)
-        return res
-
-def generate_commands(sp):
-### Find replacement lists in the arguments
-    repl = []
-    for arg in sp:
-        p = re.compile('\{\{\S*\}\}')
-        reg = p.search(arg)
-        if reg:
-#            print "reg:",reg.group()[2:-2]
-            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
-#            print "curargs:",curargs
-            newcurargs = []
-            for curarg in curargs:
-                new = p.sub(curarg,arg)
-#                print "new:",new
-                newcurargs.append(new)
-            repl.append(newcurargs)
-        else:
-            repl.append([arg])
-#    print "repl: ",repl
-    argscombination = generate_combination(repl)
-    return argscombination
-
-#generate the command
-if FILE != "":
-    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
-    commands=[]
-    for line in FD.readlines():
-        line = line.rstrip()
-	sp = line.split(" ")
-        commands+=generate_commands(sp)
-    FD.close
-else:
-    commands=generate_commands(command_argv)
-
-if FILE == "":    
-    t = [x for x in sys.argv[1:] if not x[:2]=="--"]
-    t[0]=os.path.split(t[0])[1]
-    tmp="_".join(t)
-    tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
-    ### We need to remove the symbols "," as this cause trouble with bqtools
-    tmp=re.sub( ',', '-', tmp )
-    tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
-    dbi_param["log_dir"]=os.path.join("LOGS",tmp)
-    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
-else:
-    dbi_param["log_dir"]=os.path.join("LOGS",FILE)
-    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
-
-if "test" in dbi_param:
-    print "We generated %s command in the file"% len(commands)
-    print "The script %s was not launched"% ScriptName
-    SCRIPT=open(ScriptName,'w');
-    SCRIPT.write(
-"""#! /usr/bin/env python
-#%s
-from plearn.parallel.dbi import DBI
-jobs = DBI([
-"""% " ".join(sys.argv))
-    for arg in commands:
-        cmdstr = "".join(arg);
-        SCRIPT.write("   '%s',\n"%cmdstr)
-    SCRIPT.write("   ],'%s'"%(launch_cmd))
-    for key in dbi_param.keys():
-        if isinstance(dbi_param[key],str):
-            SCRIPT.write(","+str(key)+"='"+str(dbi_param[key])+"'")
-        else:
-            SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
-    SCRIPT.write(
-""")
-jobs.run()
-jobs.wait()
-# There is %d command in the script"""%(len(commands)))
-        
-    SCRIPT.close()
-    os.system("chmod +x %s"%(ScriptName));
-
-else:
-    print "We generate the DBI object with %s command"%(len(commands))
-    from plearn.parallel.dbi import *
-    print time.ctime()
-    t1=time.time()
-    jobs = DBI(commands,launch_cmd,**dbi_param)
-    t2=time.time()
-    print "it took %f s to create the DBI objects"%(t2-t1)
-    jobs.run()
-    jobs.wait()
-    t3=time.time()
-    print "it took %f s to launch all the commands"%(t3-t2)
-

Copied: trunk/scripts/dbidispatch (from rev 8065, trunk/scripts/cdispatch)
===================================================================
--- trunk/scripts/cdispatch	2007-09-11 18:30:38 UTC (rev 8065)
+++ trunk/scripts/dbidispatch	2007-09-11 18:33:23 UTC (rev 8066)
@@ -0,0 +1,279 @@
+#!/usr/bin/env python
+import sys,os,re,time,datetime
+from plearn.utilities.toolkit import search_file
+
+ScriptName="launchdbi.py"
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--cluster[=nb_process]|--local[=nb_process]|--bqtools[=nb_process]|--condor] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
+LongHelp="""
+Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools. If no system is selected on the command line, we try them in this order: condor, bqsubmit, cluster local
+
+%s
+
+option not explained:
+  --help, -h
+  --dbilog, --nodbilog
+  --cluster,--local,--bqtools,--condor
+  --file=FILEPATH
+  
+common option:
+  The '--test' option make that dbidispatch generate the file $ScriptName, but do not execute it. That way you can see what dbidispatch generate. Also, this file make dbi in test mode, so dbi do not execute automaticaly the experiment il $ScriptName is executer
+local, bqtools and cluster parameter:
+  --nb_proc=nb_process, give the maximum number of concurent jobs running
+    --local=X is the same as --local --nb_proc=X
+    --cluster=X is the same as --cluster --nb_proc=X
+    --bqtools=X is the same as --bqtools --nb_proc=X
+
+
+bqtools and cluster option:
+  The '--duree' option tell the maximum length of the jobs. The have the cluster syntaxe of accepted value 'cluster --help'. The bqtools syntaxe is '--duree=12:13:15'. This give 12 hours, 13 minutes and 15 seconds
+
+bqtools only option:
+  The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
+  have a very short duration. This may prevent some queue crashes. The nb_batch value
+  is the number of experience to group together in a batch.(default 20)
+
+  The '--long' option must be used with BqTools to launch jobs whose duration
+  is more than 5 days. The maximum duration of a job will be either the
+  BQ_MAX_JOB_DURATION environment variable (in the form hour:min:sec) if it is
+  set, and 1200:00:00 (50 days) otherwise.
+  Since long jobs are launched on a different queue with few nodes, please make
+  sure you are not using too many nodes at once.
+  If this option is not set, the maximum duration of each job will be 120 hours
+  (5 days).
+
+cluster and condor option:
+  The '--3264', '--32' or '--64' tell the type of cpu the compute node must have to execute the commands.
+
+cluster only option:
+  The '--wait' is transfered to cluster. This must be enabled if their is not nb_process available compute node. Otherwise when their is no compute node available, the launch of that command fail.
+
+condor only option:
+  The '--req=\"CONDOR_REQUIREMENT\"' option make that dbidispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
+
+  dbidispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
+     or
+  dbidispatch '--req=Machine==\\\"computer.example.com\\\"' 
+
+
+where <command-template> is interpreted as follows: the first argument
+is the <command> above, and the rest are interpreted as <arguments>.
+The arguments may contain segments of the form {{a,b,c,d}}, which trigger
+parallel dispatch: a separate 'cluster --execute' command is issued for
+the rest of the command template, the first time with value a, the second
+time with value b, etc.  For example, the command (NOTE: THERE MUST NOT
+BE ANY SPACES WITHIN THE 'numhidden={{5,10,25}}' part and the quotes are
+important to avoid shell misinterpretation) :
+
+  dbidispatch aplearn myscript.plearn 'numhidden={{5,10,25}}'
+
+is equivalent to launching three jobs in parallel on the cluster:
+
+  aplearn myscript.plearn numhidden=5
+  aplearn myscript.plearn numhidden=10
+  aplearn myscript.plearn numhidden=25
+
+If several arguments contain {{ }} forms, all combinations of arguments
+are taken, and the jobs are all launched in parallel.  For instance
+
+  dbidispatch aplearn myscript.plearn 'numhidden={{10,25}}' 'wd={{0.01,0.001}}'
+
+is equivalent to:
+
+  aplearn myscript.plearn numhidden=10 wd=0.01
+  aplearn myscript.plearn numhidden=10 wd=0.001
+  aplearn myscript.plearn numhidden=25 wd=0.01
+  aplearn myscript.plearn numhidden=25 wd=0.001
+
+If the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
+
+dbidispatch --test --file=tests
+
+In the file, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
+"""%ShortHelp
+
+if len(sys.argv) == 1:
+    print ShortHelp
+    sys.exit(1)
+FILE = ""
+dbi_param={}
+
+
+PATH=os.getenv('PATH')
+if search_file('condor_submit',PATH):
+    launch_cmd = 'Condor'
+elif search_file('bqsubmit',PATH):
+    launch_cmd = 'bqtools'
+elif search_file('cluster',PATH):
+    launch_cmd = 'Cluster'
+else:
+    launch_cmd = 'Local'
+
+command_argv = sys.argv[1:]
+for argv in sys.argv[1:]:
+
+    if argv == "--help" or argv == "-h":
+        print LongHelp
+        sys.exit(0)
+    elif argv == "--nodbilog":
+        dbi_param["dolog"]=False
+    elif argv == "--dbilog":
+        dbi_param["dolog"]=True
+    elif argv.startswith("--bqtools"):
+        launch_cmd = "bqtools"
+        if len(argv)>9:
+            assert(argv[9]=="=")
+            dbi_param["nb_proc"]=argv[10:]
+    elif argv.startswith("--cluster"):
+        launch_cmd = "Cluster"
+        if len(argv)>9:
+            assert(argv[9]=="=")
+            dbi_param["nb_proc"]=argv[10:]
+    elif argv == "--condor":
+        launch_cmd = "Condor"
+    elif argv.startswith("--duree="):
+        dbi_param["duree"]=argv[8:]
+    elif argv.startswith("--local"):
+        launch_cmd = "Local"
+        if len(argv)>7:
+            assert(argv[7]=="=")
+            dbi_param["nb_proc"]=argv[8:]
+    elif argv.startswith("--nb_proc="):
+        dbi_param["nb_proc"]=argv[10:]
+    elif argv == "--test":
+        dbi_param["test"]=True
+    elif argv.startswith("--file="):
+        FILE = argv[7:]
+    elif argv == "--32"  or argv == "--64" or argv == "--3264":
+        dbi_param["arch"]=argv[2:]
+    elif argv == "--wait":
+        dbi_param["cluster_wait"]=True
+    elif argv[0:6] == "--req=":
+        dbi_param["requirements"]="\"%s\""%argv[6:]
+    elif argv == "--no_clean_up":
+        dbi_param["clean_up"]=False
+    elif argv == "long":
+        dbi_param["long"] = True
+    elif argv.startswith("--micro"):
+        dbi_param["micro"]=20
+        if len(argv)>7:
+            assert(argv[7]=="=")
+            dbi_param["micro"]=argv[8:]
+    elif argv[0:1] == '-':
+	print "Unknow parameter (%s)",argv
+	print ShortHelp
+        sys.exit(1)
+    else:
+        break
+    command_argv.remove(argv)
+
+print "\n\nThe jobs will be launched on the system:", launch_cmd,"\n\n"
+
+if len(command_argv) == 0 and FILE == "":
+    print ShortHelp
+    sys.exit(1)
+
+
+def generate_combination(repl):
+    if repl == []:
+        return []
+    else:
+        res = []
+        x = repl[0]
+        res1 = generate_combination(repl[1:])
+        for y in x:
+            if res1 == []:
+                res.append(y)
+            else:
+                for r in res1:
+                    res.append(y+" "+r)
+        return res
+
+def generate_commands(sp):
+### Find replacement lists in the arguments
+    repl = []
+    for arg in sp:
+        p = re.compile('\{\{\S*\}\}')
+        reg = p.search(arg)
+        if reg:
+#            print "reg:",reg.group()[2:-2]
+            curargs = reg.group()[2:-2].split(",")# if arg =~ /{{(.*)}}/
+#            print "curargs:",curargs
+            newcurargs = []
+            for curarg in curargs:
+                new = p.sub(curarg,arg)
+#                print "new:",new
+                newcurargs.append(new)
+            repl.append(newcurargs)
+        else:
+            repl.append([arg])
+#    print "repl: ",repl
+    argscombination = generate_combination(repl)
+    return argscombination
+
+#generate the command
+if FILE != "":
+    FD = open(FILE,'r')#|| die "couldn't open the file $FILE!";
+    commands=[]
+    for line in FD.readlines():
+        line = line.rstrip()
+	sp = line.split(" ")
+        commands+=generate_commands(sp)
+    FD.close
+else:
+    commands=generate_commands(command_argv)
+
+if FILE == "":    
+    t = [x for x in sys.argv[1:] if not x[:2]=="--"]
+    t[0]=os.path.split(t[0])[1]
+    tmp="_".join(t)
+    tmp=re.sub( '[^a-zA-Z0-9-.,]', '_', tmp )
+    ### We need to remove the symbols "," as this cause trouble with bqtools
+    tmp=re.sub( ',', '-', tmp )
+    tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
+    dbi_param["log_dir"]=os.path.join("LOGS",tmp)
+    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
+else:
+    dbi_param["log_dir"]=os.path.join("LOGS",FILE)
+    dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
+
+if "test" in dbi_param:
+    print "We generated %s command in the file"% len(commands)
+    print "The script %s was not launched"% ScriptName
+    SCRIPT=open(ScriptName,'w');
+    SCRIPT.write(
+"""#! /usr/bin/env python
+#%s
+from plearn.parallel.dbi import DBI
+jobs = DBI([
+"""% " ".join(sys.argv))
+    for arg in commands:
+        cmdstr = "".join(arg);
+        SCRIPT.write("   '%s',\n"%cmdstr)
+    SCRIPT.write("   ],'%s'"%(launch_cmd))
+    for key in dbi_param.keys():
+        if isinstance(dbi_param[key],str):
+            SCRIPT.write(","+str(key)+"='"+str(dbi_param[key])+"'")
+        else:
+            SCRIPT.write(","+str(key)+"="+str(dbi_param[key]))
+    SCRIPT.write(
+""")
+jobs.run()
+jobs.wait()
+# There is %d command in the script"""%(len(commands)))
+        
+    SCRIPT.close()
+    os.system("chmod +x %s"%(ScriptName));
+
+else:
+    print "We generate the DBI object with %s command"%(len(commands))
+    from plearn.parallel.dbi import *
+    print time.ctime()
+    t1=time.time()
+    jobs = DBI(commands,launch_cmd,**dbi_param)
+    t2=time.time()
+    print "it took %f s to create the DBI objects"%(t2-t1)
+    jobs.run()
+    jobs.wait()
+    t3=time.time()
+    print "it took %f s to launch all the commands"%(t3-t2)
+



From nouiz at mail.berlios.de  Tue Sep 11 20:53:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 20:53:34 +0200
Subject: [Plearn-commits] r8067 - trunk/python_modules/plearn/parallel
Message-ID: <200709111853.l8BIrYNB009337@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 20:53:34 +0200 (Tue, 11 Sep 2007)
New Revision: 8067

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
bugfix and now print the correctime with we print left running


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 18:33:23 UTC (rev 8066)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 18:53:34 UTC (rev 8067)
@@ -56,8 +56,11 @@
             self._function( args )
         self.running-=1
         if self.print_when_finish:
-            print self.print_when_finish,"left running:",self.running
-            
+            if callable(self.print_when_finish):
+                print self.print_when_finish(),"left running:",self.running
+            else:
+                print self.print_when_finish,"left running:",self.running
+                    
     def start( self  ):
         for thread in self._threadPool:
             time.sleep( 0 ) # necessary to give other threads a chance to run
@@ -361,7 +364,7 @@
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
+        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :"[DBI,%s]"%time.ctime())
         self.mt.start()
 
         # Execute post-batchs
@@ -623,7 +626,7 @@
         req=""
         if self.targetcondorplatform == 'BOTH':
             req="((Arch == \"INTEL\")||(Arch == \"X86_64\"))"
-        elif :
+        else :
             req="(Arch == \"%s\")"%(self.targetcondorplatform)
 
         if self.requirements != "":
@@ -826,7 +829,7 @@
             exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,"[DBI,%s]"%time.ctime())
+        self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :("[DBI,%s]"%time.ctime()))
         self.mt.start()
         
         # Execute post-batchs



From nouiz at mail.berlios.de  Tue Sep 11 21:08:03 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 11 Sep 2007 21:08:03 +0200
Subject: [Plearn-commits] r8068 - trunk/python_modules/plearn/parallel
Message-ID: <200709111908.l8BJ83vE011724@sheep.berlios.de>

Author: nouiz
Date: 2007-09-11 21:08:02 +0200 (Tue, 11 Sep 2007)
New Revision: 8068

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
refactoring of execution of {pre,post} batch command


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 18:53:34 UTC (rev 8067)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-11 19:08:02 UTC (rev 8068)
@@ -157,25 +157,23 @@
             
     def exec_pre_batch(self):
         # Execute pre-batch
-        pre_batch_command = ';'.join( self.pre_batch )
-
-        (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
+        if len(self.pre_batch)>0:
+            pre_batch_command = ';'.join( self.pre_batch )
+            if not self.test:
+                (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
+                self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
+            else:
+                print "[DBI] pre_batch_command:",pre_batch_command
             
-        if not self.test:
-            self.pre = Popen(pre_batch_command, shell=True, stdout=output, stderr=error)
-        else:
-            print "[DBI] pre_batch_command:",pre_batch_command
-            
     def exec_post_batch(self):
         # Execute post-batch
-        post_batch_command = ';'.join( self.post_batch )
-        
-        (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
-        
-        if not self.test:
-            self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
-        else:
-            print "[DBI] post_batch_command:",post_batch_command
+        if len(self.post_batch)>0:
+            post_batch_command = ';'.join( self.post_batch )        
+            if not self.test:
+                (output,error)=self.get_redirection(self.log_file + '.out',self.log_file + '.err')
+                self.post = Popen(post_batch_command, shell=True, stdout=output, stderr=error)
+            else:
+                print "[DBI] post_batch_command:",post_batch_command
             
     def clean(self):
         pass
@@ -360,16 +358,14 @@
         if self.test:
             print "[DBI] Test mode, we only print the command to be executed, we don't execute them"
         # Execute pre-batch
-        if len(self.pre_batch)>0:
-            exec_pre_batch()
+        self.exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :"[DBI,%s]"%time.ctime())
         self.mt.start()
 
         # Execute post-batchs
-        if len(self.post_batch)>0:
-            exec_post_batch()
+        self.exec_post_batch()
 
         print "[DBI] The Log file are under %s"%self.log_dir
 
@@ -491,8 +487,7 @@
         bqsubmit_dat.close()
 
         # Execute pre-batch
-        if len(self.pre_batch)>0:
-            exec_pre_batch()
+        self.exec_pre_batch()
 
         print "[DBI] All the log will be in the directory: ",self.log_dir
         # Launch bqsubmit
@@ -506,8 +501,7 @@
                 print "[DBI] The scheduling time will not be logged when you will submit the generated file" 
 
         # Execute post-batchs
-        if len(self.post_batch)>0:
-            exec_post_batch()
+        self.exec_post_batch()
 
     def wait(self):
         print "[DBI] WARNING cannot wait until all jobs are done for bqtools, use bqwatch or bqstatus"
@@ -728,12 +722,12 @@
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
 
-        if len(self.pre_batch)>0:
-            exec_pre_batch()
+        self.exec_pre_batch()
+        
         self.run_all_job()
-        if len(self.post_batch)>0:
-            exec_post_batch()
 
+        self.exec_post_batch()
+
     def wait(self):
         print "[DBI] WARNING no waiting for all job to finish implemented for condor, use 'condor_q' or 'condor_wait %s/condor.log'"%(self.log_dir)
                 
@@ -825,16 +819,14 @@
         print "[DBI] The Log file are under %s"%self.log_dir
 
         # Execute pre-batch
-        if len(self.pre_batch)>0:
-            exec_pre_batch()
+        self.exec_pre_batch()
 
         # Execute all Tasks (including pre_tasks and post_tasks if any)
         self.mt=MultiThread(self.run_one_job,self.tasks,self.nb_proc,lambda :("[DBI,%s]"%time.ctime()))
         self.mt.start()
         
         # Execute post-batchs
-        if len(self.post_batch)>0:
-            exec_post_batch()
+        self.exec_post_batch()
             
         print "[DBI] The Log file are under %s"%self.log_dir
         
@@ -948,8 +940,7 @@
         print "[DBI] The Log file are under %s"%self.log_dir
 
         # Execute pre-batch
-        if len(self.pre_batch)>0:
-            exec_pre_batch()
+        self.exec_pre_batch()
 
         if self.test:
             print "[DBI] In testmode, we only print the command that would be executed."
@@ -960,8 +951,7 @@
             self.run_one_job(task)
 
         # Execute post-batchs
-        if len(self.post_batch)>0:
-            exec_post_batch()
+        self.exec_post_batch()
 
     def clean(self):
         #TODO: delete all log files for the current batch



From tihocan at mail.berlios.de  Tue Sep 11 21:41:53 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 11 Sep 2007 21:41:53 +0200
Subject: [Plearn-commits] r8069 - trunk/python_modules/plearn/pymake
Message-ID: <200709111941.l8BJfrGg014449@sheep.berlios.de>

Author: tihocan
Date: 2007-09-11 21:41:53 +0200 (Tue, 11 Sep 2007)
New Revision: 8069

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
The final executable is now also linked under Windows. To be honest I have no idea why this had not been the case in the past...

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-09-11 19:08:02 UTC (rev 8068)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-09-11 19:41:53 UTC (rev 8069)
@@ -1053,12 +1053,11 @@
             if verbose>=2:
                 print '[ LINKING',ccfile.filebase,']'
             link_exit_code = ccfile.launch_linking()
-            if platform!='win32':
-                if create_so or create_pyso:
-                    so_filename = os.path.basename(ccfile.corresponding_output)
-                    ccfile.make_symbolic_link(so_filename, so_filename)
-                else:
-                    ccfile.make_symbolic_link(linkname)
+            if create_so or create_pyso:
+                so_filename = os.path.basename(ccfile.corresponding_output)
+                ccfile.make_symbolic_link(so_filename, so_filename)
+            else:
+                ccfile.make_symbolic_link(linkname)
             return link_exit_code
 
 def sequential_dll(target_file_info):



From louradou at mail.berlios.de  Wed Sep 12 19:24:38 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 12 Sep 2007 19:24:38 +0200
Subject: [Plearn-commits] r8070 - in trunk: commands plearn_learners/online
Message-ID: <200709121724.l8CHOcCV012160@sheep.berlios.de>

Author: louradou
Date: 2007-09-12 19:24:37 +0200 (Wed, 12 Sep 2007)
New Revision: 8070

Added:
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Added a new module to compute (and propagate the gradient of) a new cost
during the training of a RBM with Binomial hidden units.
This additionial cost encourages to have more diversity in the filters.



Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-09-11 19:41:53 UTC (rev 8069)
+++ trunk/commands/plearn_noblas_inc.h	2007-09-12 17:24:37 UTC (rev 8070)
@@ -205,6 +205,7 @@
 #include <plearn_learners/online/ForwardModule.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
 #include <plearn_learners/online/IdentityModule.h>
+#include <plearn_learners/online/LayerCostModule.h>
 #include <plearn_learners/online/LinearCombinationModule.h>
 #include <plearn_learners/online/MatrixModule.h>
 #include <plearn_learners/online/MaxSubsampling2DModule.h>

Added: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-09-11 19:41:53 UTC (rev 8069)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-09-12 17:24:37 UTC (rev 8070)
@@ -0,0 +1,1350 @@
+// -*- C++ -*-
+
+// LayerCostModule.cc
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Author: Jerome Louradour
+
+/*! \file LayerCostModule.cc */
+
+
+
+#include "LayerCostModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    LayerCostModule,
+    "Computes a cost function on Layer, given:    \n",
+    "* Expectations for a binomial RBM upper layer\n"
+    "* sigmoid(activation) for a Neural Network   \n"
+    "and Back-propagates the gradient.            \n"
+    "\n"
+    "This function can be:                        \n"
+    "* The average Cross-Entropy                  \n"
+    "* The average Kullback-Leibler Divergence    \n"
+    "* Pascal's function...                       \n");
+
+LayerCostModule::LayerCostModule():
+    histo_size(10),
+    alpha(0.0),
+    momentum(0.0)
+{
+    output_size = 1;
+/*
+    ntest = 0;
+*/
+}
+
+void LayerCostModule::declareOptions(OptionList& ol)
+{
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+
+    redeclareOption(ol, "input_size", &LayerCostModule::input_size,
+                     OptionBase::nosave,
+        "Size of the layer.");
+
+    declareOption(ol, "cost_function", &LayerCostModule::cost_function,
+                  OptionBase::buildoption,
+        "The cost function applied to the layer:\n"
+        "- \"stochastic_cross_entropy\" [default]: average cross-entropy between pairs of binomial units\n"
+        "- \"stochastic_kl_div\": average KL divergence between pairs of binomial units\n"
+        "- \"kl_div\": KL divergence between distrubution of expectations (sampled with x)\n"
+        "- \"kl_div_2\": good version of kl_div\n"
+        "- \"kl_div_simple\": simple version of kl_div where we count at least one sample per histogram's bin\n");
+
+    declareOption(ol, "histo_size", &LayerCostModule::histo_size,
+                  OptionBase::buildoption,
+        "For \"kl_div*\" cost functions,\n"
+        "number of bins for the histograms (to estimate distributions of probabilities for expectations).\n"
+        "The higher is histo_size, the more precise is the estimation.\n");
+
+    declareOption(ol, "alpha", &LayerCostModule::alpha,
+                  OptionBase::buildoption,
+        "(>=0) For \"pascal\" cost function,\n"
+        "number of bins for the histograms (to estimate distributions of probabilities for expectations).\n"
+        "The higher is histo_size, the more precise is the estimation.\n");
+
+    declareOption(ol, "momentum", &LayerCostModule::momentum,
+                  OptionBase::buildoption,
+        "(in [0,1[) For \"pascal\" cost function, momentum for the moving means\n");
+
+}
+
+void LayerCostModule::build_()
+{
+    PLASSERT( input_size > 1 );
+    PLASSERT( histo_size > 1 );
+    PLASSERT( momentum >= 0.0);
+    PLASSERT( momentum < 1);
+    
+    string im = lowerstring( cost_function );
+    // choose HERE the *default* cost function
+    if( im == "" )
+        cost_function = "pascal";
+    else
+        cost_function = im;
+
+     // list HERE all *stochastic* cost functions
+    if( ( cost_function == "stochastic_cross_entropy")
+     || ( cost_function == "stochastic_kl_div") )
+        is_cost_function_stochastic = true;
+	
+    // list HERE all *non stochastic* cost functions
+    // and the specific initialization
+    else if( ( cost_function == "kl_div")
+          || ( cost_function == "kl_div_simple")
+	  || ( cost_function == "kl_div_2") )
+    {
+        is_cost_function_stochastic = false;
+        expectations_histo.resize(input_size,histo_size);
+	LINHISTO_STEP = 1.0/(real)histo_size;
+        LOGHISTO_BASE = 10.0;
+        LOGHISTO_MIN = (real)pow(LOGHISTO_BASE,-(real)histo_size);
+    }
+    else if ( ( cost_function == "pascal") )
+    {
+        is_cost_function_stochastic = false;
+	expectations_expectation.resize(input_size);
+	expectations_cross_quadratic_mean.resize(input_size,input_size);
+/*
+        expectations_expectation_testMemory.resize(input_size);
+        expectations_cross_quadratic_mean_testMemory.resize(input_size,input_size);
+*/
+	if( momentum > 0.0)
+	{
+            expectations_expectation_trainMemory.resize(input_size);
+            expectations_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
+	}
+    }
+    else
+        PLERROR("LayerCostModule::build_() does not recognize cost function %s",
+                 cost_function.c_str());
+
+    // The port story...
+    ports.resize(0);
+    portname_to_index.clear();
+    addPortName("expectations");
+    addPortName("cost");
+
+    port_sizes.resize(nPorts(), 2);
+    port_sizes.fill(-1);
+    port_sizes(getPortIndex("expectations"), 1) = input_size;
+    port_sizes(getPortIndex("cost"), 1) = 1;
+    
+}
+
+
+// ### Nothing to add here, simply calls build_
+void LayerCostModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void LayerCostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(expectations_histo, copies);
+    deepCopyField(expectations_expectation, copies);
+    deepCopyField(expectations_cross_quadratic_mean, copies);
+    deepCopyField(expectations_expectation_trainMemory, copies);
+    deepCopyField(expectations_cross_quadratic_mean_trainMemory, copies);
+/*
+    deepCopyField(expectations_expectation_testMemory, copies);
+    deepCopyField(expectations_cross_quadratic_mean_testMemory, copies);
+*/
+    deepCopyField(ports, copies);
+}
+
+
+
+
+///////////
+// fprop //
+///////////
+
+
+void LayerCostModule::fprop(const TVec<Mat*>& ports_value)
+{
+
+    Mat* expectations = ports_value[getPortIndex("expectations")];
+    Mat* cost = ports_value[getPortIndex("cost")];
+
+    PLASSERT( ports_value.length() == nPorts() );
+
+    if ( cost && cost->isEmpty() )
+    {
+        PLASSERT( expectations && !expectations->isEmpty() );
+	cout << "1 regular fprop!!!" << endl;
+        fprop(*expectations, *cost);
+    }
+}
+
+void LayerCostModule::fprop(const Mat& expectations, Mat& costs)
+{
+    int batch_size = expectations.length();
+    costs.resize( batch_size, output_size );
+    
+    if( !is_cost_function_stochastic )
+    {
+        costs.clear();
+
+        if( cost_function == "kl_div" )
+        {
+        //! ************************************************************
+        //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
+        //! between probabilities of expectations vectors for all units
+        //! ************************************************************
+        //! 
+        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!
+        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //!
+        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
+        //!        Px(.): empirical probability (given data x, we sample the q's)
+        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
+        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
+        //!
+        //! Note: one q{i} *entirely* determines one binomial densities of probability
+        //!       ( Bijection {binomial Proba functions} <-> |R )
+        //!
+        //! ************************************************************
+
+            // Filling the histogram (i.e. emperical distribution)
+            // of the expectations
+	    computeHisto(expectations);
+	    
+            // Computing the KL divergence
+            for (int i = 0; i < input_size; i++)
+                for (int j = 0; j < i; j++)
+		{
+		    // These variables are used in case one bin of 
+		    // the histogram is empty for one unit
+		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+		    // In such case, we ''differ'' the count for the next bin and so on.
+                    real differ_count_i = 0.0;
+                    real differ_count_j = 0.0;
+		    for (int k = 0; k < histo_size; k++)
+		    {
+                        real Ni_k = expectations_histo(i,k) + differ_count_i;
+			real Nj_k = expectations_histo(j,k) + differ_count_j;
+			if( fast_exact_is_equal(Ni_k, 0.0) )
+			{
+                         // differ_count_j += expectations_histo(j,k);
+                            differ_count_j = Nj_k;
+			    continue;
+			}
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+			{
+                            differ_count_i = Ni_k;
+			    continue;
+			}
+                        else
+			{
+			    costs(0,0) += KLdivTerm(Ni_k,Nj_k);
+                            differ_count_i = 0.0;
+			    differ_count_j = 0.0;
+			}
+                    }
+		    if( differ_count_i > 0.0 )
+		        "cas ou on regroupe avec le dernier";
+		    else if ( differ_count_j > 0.0 )
+		        "cas ou on regroupe avec le dernier";		    
+                }
+            // Normalization w.r.t. number of units
+            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+        }
+        else if( cost_function == "kl_div_2" )
+        {
+        //! ************************************************************
+        //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
+        //! between probabilities of expectations vectors for all units
+        //! ************************************************************
+        //! 
+        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!
+        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //!
+        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
+        //!        Px(.): empirical probability (given data x, we sample the q's)
+        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
+        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
+        //!
+        //! Note: one q{i} *entirely* determines one binomial densities of probability
+        //!       ( Bijection {binomial Proba functions} <-> |R )
+        //!
+        //! ************************************************************
+
+            // Filling the histogram (i.e. emperical distribution)
+            // of the expectations
+	    computeHisto(expectations);
+	    
+            // Computing the KL divergence
+            for (int i = 0; i < input_size; i++)
+                for (int j = 0; j < i; j++)
+		{
+		    // These variables are used in case one bin of 
+		    // the histogram is empty for one unit
+		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+		    // In such case, we ''differ'' the count for the next bin and so on.
+                    real differ_count_i = 0.0;
+                    real differ_count_j = 0.0;
+		    int n_differ = 0;
+		    for (int k = 0; k < histo_size; k++)
+		    {
+                        real Ni_k = expectations_histo(i,k) + differ_count_i;
+			real Nj_k = expectations_histo(j,k) + differ_count_j;
+			if( fast_exact_is_equal(Ni_k, 0.0) )
+			{
+                         // differ_count_j += expectations_histo(j,k);
+                            differ_count_j = Nj_k;
+			    n_differ += 1;
+			}
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+			{
+                            differ_count_i = Ni_k;
+			    n_differ += 1;
+			}
+                        else
+			{
+			    costs(0,0) += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ)/(real)histo_size;
+                            differ_count_i = 0.0;
+			    differ_count_j = 0.0;
+			}
+                    }
+		    if( differ_count_i > 0.0 )
+		        "cas ou on regroupe avec le dernier";
+		    else if ( differ_count_j > 0.0 )
+		        "cas ou on regroupe avec le dernier";		    
+                }
+            // Normalization w.r.t. number of units
+            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+        }
+        else if( cost_function == "kl_div_simple" )
+        {
+            // Filling the histogram (i.e. emperical distribution)
+            // of the expectations
+	    computeSafeHisto(expectations);
+	    
+            // Computing the KL divergence
+            for (int i = 0; i < input_size; i++)
+                for (int j = 0; j < i; j++)
+		    for (int k = 0; k < histo_size; k++)
+		    {
+                        real Ni_k = expectations_histo(i,k);
+			real Nj_k = expectations_histo(j,k);
+			costs(0,0) += KLdivTerm(Ni_k,Nj_k);
+                    }
+            // Normalization w.r.t. number of units
+            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+        }
+        else if( cost_function == "pascal" )
+        {
+        //! ************************************************************
+        //! a god-given similarity measure
+        //! between expectations vectors for all units
+        //! ************************************************************
+        //! 
+        //!      cost = \sum_{i} \sum_{j#i} exp( Ex[q{i}.q{j}] ) - alpha. \sum_{i} exp( Ex[q{i}] )
+        //!
+        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
+        //!        Ex(.): empirical esperance (given data x, we sample the q's)
+        //!
+        //! ************************************************************
+
+            // Computing statistics on expectations
+	    computePascalStatistics(expectations, false);
+	    
+	    cout << "1 fprop" << endl;
+	    	    
+            // Computing the cost
+            for (int i = 0; i < input_size; i++)
+	    {
+	        if (alpha > 0.0 )
+		    costs(0,0) -= alpha*exp(expectations_expectation[i]);
+                for (int j = 0; j < i; j++)
+                    costs(0,0) += exp(expectations_cross_quadratic_mean(i,j)) / (real)(input_size-1);
+            }
+
+            // Normalization w.r.t. number of units
+            costs(0,0) /= (real)input_size;
+        }
+	
+        return; // Do not fprop with the conventional stochastic fprop...
+    }
+    
+    for (int isample = 0; isample < batch_size; isample++)
+        fprop(expectations(isample), costs(isample,0));
+}
+
+void LayerCostModule::fprop(const Vec& expectation, real& cost) const
+{
+    PLASSERT( expectation.size() == input_size );
+    PLASSERT( is_cost_function_stochastic );
+
+    cost = 0.0;
+    real  qi, qj, comp_qi, comp_qj; // The expectations qi=p(h_i=1)
+                                      //     and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
+				      
+    if( cost_function == "stochastic_cross_entropy" )
+    {
+    //! ************************************************************
+    //! average *** CROSS ENTROPY ***
+    //! between pairs of units (given expectations = sigmoid(act) )
+    //! ************************************************************
+    //!
+    //!      cost = - \sum_{i} \sum_{j#i} CrossEntropy[( P(h_{i}|x) | P(h_{j}|x) )]
+    //!
+    //!           = - \sum_{i} \sum_{j#i} [ q{i}.log(q{j}) + (1-q{i}).log(1-q{j}) ]
+    //!
+    //! where |  h_{i}: i^th units of the layer
+    //!       \  P(.|x): output for input data x
+    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!
+    //! ************************************************************
+
+        for( int i = 0; i < input_size; i++ )
+        {
+           qi = expectation[i];
+           comp_qi = 1.0 - qi;
+           for( int j = 0; j < i; j++ )
+           {
+               qj = expectation[j];
+               comp_qj = 1.0 - qj;
+	       
+               // H(pi||pj) = H(pi) + D_{KL}(pi||pj)
+               cost += qi*safeflog(qj) + comp_qi*safeflog(comp_qj);
+	       
+               // The symetric part (loop  j=i+1...size)
+               cost += qj*safeflog(qi) + comp_qj*safeflog(comp_qi);
+           }
+        }
+        // Normalization w.r.t. number of units
+        cost /= ((real)input_size *(real)(input_size-1));
+    }
+    
+    else if( cost_function == "stochastic_kl_div" )
+    {
+    //! ************************************************************
+    //! average SYMETRIC *** K-L DIVERGENCE ***
+    //! between pairs of units (given expectations = sigmoid(act) )
+    //! ************************************************************
+    //!
+    //!      cost = - \sum_{i} \sum_{j#i} Div_{KL} [( P(h_{i}|v) | P(h_{j}|v) )]
+    //!
+    //!           = - \sum_{i} \sum_{j#i} [ ( q{j} - q{i} ) log( q{i}/(1-q{i})*(1-q{j})/q{j} ) ]
+    //!
+    //! where |  h_{i}: i^th units of the layer
+    //!       \  P(.|v):  output for input data x
+    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!
+    //! ************************************************************
+
+        for( int i = 0; i < input_size; i++ )
+        {
+           qi = expectation[i];
+           if(fast_exact_is_equal(qi, 1.0))
+               comp_qi = REAL_MAX;
+           else
+               comp_qi = qi/(1.0 - qi);
+       
+           for( int j = 0; j < i; j++ )
+           {
+               qj = expectation[j];
+               if(fast_exact_is_equal(qj, 1.0))
+                   comp_qj = REAL_MAX;
+               else
+                   comp_qj = qj/(1.0 - qj);
+	       
+               //     - D_{KL}(pi||pj) - D_{KL}(pj||pi)
+               cost += (qj-qi)*safeflog(comp_qi/comp_qj);
+           }
+        }
+        // Normalization w.r.t. number of units
+        cost /= ((real)input_size *(real)(input_size-1));   
+    }
+
+    else
+        PLERROR("LayerCostModule::fprop() not implemented for cost function %s\n"
+	        "- It may be a printing error\n"
+		"- You can try to call LayerCostModule::fprop(const Mat& expectations, Mat& costs)\n"
+		"- Or else write the code corresponding to your cost function",
+                 cost_function.c_str());
+}
+
+
+
+
+////////////////////
+// bpropAccUpdate //
+////////////////////
+
+
+void LayerCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                   const TVec<Mat*>& ports_gradient)
+{
+    PLASSERT( ports_value.length() == nPorts() );
+    PLASSERT( ports_gradient.length() == nPorts() );
+
+    const Mat* expectations = ports_value[getPortIndex("expectations")];
+    Mat* expectations_grad = ports_gradient[getPortIndex("expectations")];
+    Mat* cost = ports_value[getPortIndex("cost")];
+    Mat* cost_grad = ports_gradient[getPortIndex("cost")];
+
+    if( expectations_grad && expectations_grad->isEmpty()
+        && cost_grad && !cost_grad->isEmpty() )
+    {
+        int batch_size = expectations->length();
+
+        PLASSERT( expectations && !expectations->isEmpty());
+        PLASSERT( expectations->length() == batch_size );
+        PLASSERT( cost_grad->length() == batch_size );
+
+        expectations_grad->resize(batch_size, input_size);
+	expectations_grad->clear();
+
+        real qi, qj, comp_qi, comp_qj;
+        Vec comp_q(input_size), log_term(input_size);
+
+        if( cost_function == "stochastic_cross_entropy" )
+        {
+            for (int isample = 0; isample < batch_size; isample++)
+            {
+        	for (int i = 0 ; i < input_size ; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+            	    comp_qi = 1.0 - qi;
+                    comp_q[i] = comp_qi;
+                    log_term[i] = safeflog(qi) - safeflog(comp_qi);
+                }
+                for (int i = 0; i < input_size; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+                    comp_qi = comp_q[i];
+                    (*expectations_grad)(isample,i) = 0.0;
+                    for (int j = 0; j < i; j++ )
+                    {
+                        qj = (*expectations)(isample,j);
+                        comp_qj=comp_q[j];
+
+                        // log(pj) - log(1-pj) + pj/pi - (1-pj)/(1-pi)
+                        (*expectations_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
+
+                        // The symetric part (loop  j=i+1...input_size)
+                        (*expectations_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
+                    }
+                }
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i < input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == "stochastic_kl_div" )
+        {
+            for (int isample = 0; isample < batch_size; isample++)
+            {
+        	for (int i = 0; i < input_size; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+            	    comp_qi = 1.0 - qi;
+                    if(fast_exact_is_equal(qi, 1.0) || fast_exact_is_equal(qi, 0.0))
+                        comp_q[i] = REAL_MAX;
+                    else
+                        comp_q[i] = 1.0/(qi*comp_qi);
+                    log_term[i] = safeflog(qi) - safeflog(comp_qi);
+                }
+                for (int i = 0; i < input_size; i++ )
+                {
+                    qi = (*expectations)(isample,i);
+                    comp_qi = comp_q[i];
+
+                    (*expectations_grad)(isample,i) = 0.0;
+                    for (int j = 0; j < i ; j++ )
+                    {
+                        qj = (*expectations)(isample,j);
+                        comp_qj=comp_q[j];
+
+                        //   [qj - qi]/[qi (1-qi)] - log[ qi/(1-qi) * (1-qj)/qj]
+                        (*expectations_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
+
+                        // The symetric part (loop  j=i+1...input_size)
+                        (*expectations_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
+                    }
+                }
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i < input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+
+        else if( cost_function == "kl_div" )
+        {
+	    computeHisto(*expectations);
+	    real one_count = 1. / (real)batch_size;
+	    
+            for (int isample = 0; isample < batch_size; isample++)
+            {
+
+                // Computing the difference of KL divergence
+                // for d_q
+                for (int i = 0; i < input_size; i++)
+		{
+                    (*expectations_grad)(isample, i) = 0.0;
+		    
+		    qi = (*expectations)(isample,i);
+		    int index_i = histo_index(qi);
+		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+		        continue;
+		    real over_dqi=1.0/dq(qi);
+		    int shift_i;
+		    if( over_dqi > 0.0)
+		        shift_i = 1;
+		    else
+		        shift_i = -1;
+		    // qi + dq(qi) ==> | expectations_histo(i,index_i)   - one_count
+		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
+		    
+		    for (int j = 0; j < i; j++)
+		    {
+			(*expectations_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi, one_count);
+			
+                        qj = (*expectations)(isample,j);
+			int index_j = histo_index(qj);
+		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+		            continue;
+			real over_dqj=1.0/dq(qj);
+ 		        int shift_j;
+		        if( over_dqj > 0.0)
+		            shift_j = 1;
+		        else
+		            shift_j = -1;
+            	        // qj + dq(qj) ==> | expectations_histo(j,index_j)   - one_count
+  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
+			
+			(*expectations_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj, one_count);
+                    }
+		}
+
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i < input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == "kl_div_2" )
+        {
+	    computeHisto(*expectations);
+	    real one_count = 1. / (real)batch_size;
+	    
+            for (int isample = 0; isample < batch_size; isample++)
+            {
+
+                // Computing the difference of KL divergence
+                // for d_q
+                for (int i = 0; i < input_size; i++)
+		{
+                    (*expectations_grad)(isample, i) = 0.0;
+		    
+		    qi = (*expectations)(isample,i);
+		    int index_i = histo_index(qi);
+		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+		        continue;
+		    real over_dqi=1.0/dq(qi);
+		    int shift_i;
+		    if( over_dqi > 0.0)
+		        shift_i = 1;
+		    else
+		        shift_i = -1;
+		    // qi + dq(qi) ==> | expectations_histo(i,index_i)   - one_count
+		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
+		    
+		    for (int j = 0; j < i; j++)
+		    {
+			(*expectations_grad)(isample, i) += delta_KLdivTerm_2(i, j, index_i, over_dqi, one_count);
+			
+                        qj = (*expectations)(isample,j);
+			int index_j = histo_index(qj);
+		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+		            continue;
+			real over_dqj=1.0/dq(qj);
+ 		        int shift_j;
+		        if( over_dqj > 0.0)
+		            shift_j = 1;
+		        else
+		            shift_j = -1;
+            	        // qj + dq(qj) ==> | expectations_histo(j,index_j)   - one_count
+  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
+			
+			(*expectations_grad)(isample, j) += delta_KLdivTerm_2(j, i, index_j, over_dqj, one_count);
+                    }
+		}
+
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i < input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == "kl_div_simple" )
+        {
+	    computeSafeHisto(*expectations);
+	    real one_count = 1. / (real)(batch_size+histo_size);
+	    
+            for (int isample = 0; isample < batch_size; isample++)
+            {
+
+                // Computing the difference of KL divergence
+                // for d_q
+                for (int i = 0; i < input_size; i++)
+		{
+                    (*expectations_grad)(isample, i) = 0.0;
+		    
+		    qi = (*expectations)(isample,i);
+		    int index_i = histo_index(qi);
+		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+		        continue;
+		    real over_dqi=1.0/dq(qi);
+		    int shift_i;
+		    if( over_dqi > 0.0)
+		        shift_i = 1;
+		    else
+		        shift_i = -1;
+		    // qi + dq(qi) ==> | expectations_histo(i,index_i)   - one_count
+		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
+		    
+		    for (int j = 0; j < i; j++)
+		    {
+			(*expectations_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi, one_count);
+			
+                        qj = (*expectations)(isample,j);
+			int index_j = histo_index(qj);
+		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+		            continue;
+			real over_dqj=1.0/dq(qj);
+ 		        int shift_j;
+		        if( over_dqj > 0.0)
+		            shift_j = 1;
+		        else
+		            shift_j = -1;
+            	        // qj + dq(qj) ==> | expectations_histo(j,index_j)   - one_count
+  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
+			
+			(*expectations_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj, one_count);
+                    }
+		}
+
+                // Normalization
+                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
+                for (int i = 0; i < input_size; i++ )
+                {
+                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                }
+            }
+        }
+
+        else if( cost_function == "pascal" )
+        {
+	    computePascalStatistics(*expectations, true);
+	    
+	    cout << "1 BPropAccUpdate" << endl;
+	    
+	    real one_count = 1. / (real)batch_size;
+	    if( momentum > 0.0 )
+	        for (int isample = 0; isample < batch_size; isample++)
+		{
+                    for (int i = 0; i < input_size; i++)
+                    {
+                        qi = (*expectations)(isample, i);
+			if (alpha > 0.0 )
+			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *(1.0-momentum)*one_count;
+                        for (int j = 0; j < i; j++)
+                        {
+			    qj = (*expectations)(isample,j);
+                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*(1.0-momentum)*one_count / (real)(input_size-1);
+                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*(1.0-momentum)*one_count / (real)(input_size-1);
+                        }
+                    }
+                    for (int i = 0; i < input_size; i++)
+                    {
+	                (*expectations_grad)(isample, i) /= (real)input_size;
+	            }
+		}
+	    else
+	        for (int isample = 0; isample < batch_size; isample++)
+		{
+                    for (int i = 0; i < input_size; i++)
+                    {
+                        qi = (*expectations)(isample, i);
+			if (alpha > 0.0 )
+			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *one_count;
+                        for (int j = 0; j < i; j++)
+                        {
+			    qj = (*expectations)(isample,j);
+                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*one_count / (real)(input_size-1);
+                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*one_count / (real)(input_size-1);
+                        }
+                    }
+                    for (int i = 0; i < input_size; i++)
+                    {
+	                (*expectations_grad)(isample, i) /= (real)input_size;
+	            }
+		}
+        }
+	
+        else
+            PLERROR("LayerCostModule::bpropAccUpdate() not implemented for cost function %s",
+                     cost_function.c_str());
+
+/*
+        ntest = 0;
+*/
+
+        checkProp(ports_gradient);
+    }
+    else if( !expectations_grad && !cost_grad )
+        return;
+    else
+        PLERROR("In LayerCostModule::bpropAccUpdate - Port configuration not implemented ");
+
+}
+
+
+////////////////////////////////////////////////////
+// Auxiliary Functions for Pascal's cost function //
+////////////////////////////////////////////////////
+void LayerCostModule::computePascalStatistics(const Mat& expectations, bool duringTraining)
+{
+    int batch_size = expectations.length();
+    real one_count = 1. / (real)batch_size;
+    Vec expectation;
+    
+    expectations_expectation.clear(); 
+    expectations_cross_quadratic_mean.clear(); 
+
+    for (int isample = 0; isample < batch_size; isample++)
+    {
+        expectation = expectations(isample);
+        for (int i = 0; i < input_size; i++)
+	{
+	    expectations_expectation[i] += expectation[i];
+	    for (int j = 0; j < i; j++)
+                 expectations_cross_quadratic_mean(i,j) += expectation[i] * expectation[j];
+        }
+    }
+    expectations_cross_quadratic_mean *= one_count;
+    
+    for (int i = 0; i < input_size; i++)
+    {
+        expectations_expectation[i] *= one_count;
+        for (int j = 0; j < i; j++)
+        {
+             expectations_cross_quadratic_mean(i,j) *= one_count;
+//    	     expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+        }
+    }
+    if( ( momentum > 0.0 ) && duringTraining )
+    {
+        for (int i = 0; i < input_size; i++)
+        {
+	    if(i == 0)
+	       cout << ".Check momentum....: expectations_expectation_trainMemory[0] = " << expectations_expectation_trainMemory[0] << endl;
+
+            expectations_expectation[i] = momentum*expectations_expectation_trainMemory[i]
+	                                 +(1.0-momentum)*expectations_expectation[i];
+            expectations_expectation_trainMemory[i] = expectations_expectation[i];
+            for (int j = 0; j < i; j++)
+            {
+                 expectations_cross_quadratic_mean(i,j) = momentum*expectations_cross_quadratic_mean_trainMemory(i,j)
+		                                       +(1.0-momentum)*expectations_cross_quadratic_mean(i,j);
+//        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+        	 expectations_cross_quadratic_mean_trainMemory(i,j) = expectations_cross_quadratic_mean(i,j);
+//        	 expectations_cross_quadratic_mean_trainMemory(j,i) = expectations_cross_quadratic_mean(i,j);
+            }
+        }
+    }
+/*    else if( !duringTraining )
+    {
+	PLASSERT( ntest+batch_size > 0 );
+	for (int i = 0; i < input_size; i++)
+        {
+            expectations_expectation[i] = ( (real)ntest*expectations_expectation_testMemory[i]
+	                                   +(real)batch_size*expectations_expectation[i] )/(real)(ntest+batch_size);
+            expectations_expectation_testMemory[i] = expectations_expectation[i];
+            for (int j = 0; j < i; j++)
+            {
+                 expectations_cross_quadratic_mean(i,j) = ( (real)ntest*expectations_cross_quadratic_mean_testMemory(i,j)
+		                                           +(real)batch_size*expectations_cross_quadratic_mean(i,j) );
+        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+        	 expectations_cross_quadratic_mean_testMemory(i,j) = expectations_cross_quadratic_mean(i,j);
+        	 expectations_cross_quadratic_mean_testMemory(j,i) = expectations_cross_quadratic_mean(i,j);
+            }
+        }
+	ntest += batch_size;
+    }
+*/
+}
+
+/////////////////////////
+// Auxiliary Functions //
+/////////////////////////
+real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+{
+    PLASSERT( over_dq > 0.0 );
+
+    real grad_update = 0.0;
+
+    real Ni_ki = expectations_histo(i,index_i);
+    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
+    real Nj_ki        = expectations_histo(j,index_i);
+    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+
+    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
+                                                  // if expectations_histo is up to date,
+                                                  // the expectation(isample,i) has been counted
+    real differ_count_j_before = 0.0;
+    real differ_count_j_after = 0.0;
+    real differ_count_i_before = 0.0;
+    real differ_count_i_after = 0.0;
+
+    // What follows is only valuable when the qi's are increased (dq>0).
+
+    if( !fast_exact_is_equal(Nj_ki, 0.0) )
+    // if it is zero, then INCREASING qi will not change anything
+    // (it was already counted in the next histograms's bin
+    {
+        // removing the term of the sum that will be modified
+        grad_update -= KLdivTerm( Ni_ki, Nj_ki );
+							       
+        if( fast_exact_is_equal(Ni_ki, one_count) )
+            differ_count_j_after = Nj_ki;
+        else
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
+
+        if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
+        {
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after );
+
+            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas o? on regroupe avec le dernier";
+            {
+                // removing the term of the sum that will be modified
+                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );		
+            }
+	    else
+	    {
+	        // We search   ki' > k(i)+1   such that   n(i,ki') > 0
+		differ_count_j_before = Nj_ki_shift1;
+		int ki;
+		for (ki = index_i+2; ki < histo_size; ki++)
+		{
+		    differ_count_j_before += expectations_histo( j, ki );
+		    if( expectations_histo( i, ki )>0 )
+		        break;
+		}
+		if( ki < histo_size )
+		{
+                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before );
+
+		    if( differ_count_j_before > Nj_ki_shift1 )		
+                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 );
+		        // pb avec differ_count_j_after plus haut??? semble pas
+		}
+		else
+		    "cas o? on regroupe avec le dernier";
+	    }
+        }
+        else
+        {
+            differ_count_i_before = Ni_ki_shift1;
+            differ_count_i_after  = Ni_ki_shift1+one_count;
+	    int kj;
+	    for( kj = index_i+2; kj < histo_size; kj++)
+	    {
+	        differ_count_i_after += expectations_histo( i, kj );
+		if( differ_count_i_before > 0 )
+		    differ_count_i_before += expectations_histo( i, kj );
+		if( expectations_histo( j, kj ) > 0 )
+		    break;
+	    }
+	    if( kj < histo_size )
+            {
+                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) );
+
+		if( differ_count_i_before > 0 )
+                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) );
+	    }
+	    else
+		"cas o? on regroupe avec le dernier";   
+        }
+    }
+    return grad_update *over_dq;
+}
+
+real LayerCostModule::delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count)
+{
+    PLASSERT( over_dq > 0.0 );
+
+    real grad_update = 0.0;
+
+    real Ni_ki = expectations_histo(i,index_i);
+    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
+    real Nj_ki        = expectations_histo(j,index_i);
+    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+
+    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
+                                                  // if expectations_histo is up to date,
+                                                  // the expectation(isample,i) has been counted
+    real differ_count_j_before = 0.0;
+    real differ_count_j_after = 0.0;
+    real differ_count_i_before = 0.0;
+    real differ_count_i_after = 0.0;
+    int n_differ_j_before = 0;
+    int n_differ_j_after = 0;
+    int n_differ_i_before = 0;
+    int n_differ_i_after = 0;
+
+    // What follows is only valuable when the qi's are increased (dq>0).
+
+    if( !fast_exact_is_equal(Nj_ki, 0.0) )
+    // if it is zero, then INCREASING qi will not change anything
+    // (it was already counted in the next histograms's bin
+    {
+        // removing the term of the sum that will be modified
+        grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
+							       
+        if( fast_exact_is_equal(Ni_ki, one_count) )
+	{
+            differ_count_j_after = Nj_ki;
+	    n_differ_j_after += 1;
+	}
+        else
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki ) *over_dq;
+
+        if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
+        {
+            // adding the term of the sum with its modified value
+            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after ) *(real)(n_differ_j_after+1)*over_dq ;
+
+            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas o? on regroupe avec le dernier";
+            {
+                // removing the term of the sum that will be modified
+                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )*over_dq;		
+            }
+	    else
+	    {
+	        // We search   ki' > k(i)+1   such that   n(i,ki') > 0
+		differ_count_j_before = Nj_ki_shift1;
+                n_differ_j_before += 1;
+		int ki;
+		for (ki = index_i+2; ki < histo_size; ki++)
+		{
+		    differ_count_j_before += expectations_histo( j, ki );
+		    if( expectations_histo( i, ki )>0 )
+		        break;
+                    n_differ_j_before += 1;
+		}
+		if( ki < histo_size )
+		{
+                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before )*(real)(1+n_differ_j_before)*over_dq;
+
+		    if( differ_count_j_before > Nj_ki_shift1 )		
+                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )*(real)(n_differ_j_before)*over_dq;
+		        // pb avec differ_count_j_after plus haut??? semble pas
+		}
+		else
+		    "cas o? on regroupe avec le dernier";
+	    }
+        }
+        else
+        {
+            differ_count_i_before = Ni_ki_shift1;
+	    if( differ_count_i_before>0.0 )
+	       n_differ_i_before += 1;
+            differ_count_i_after  = Ni_ki_shift1+one_count;
+	    n_differ_i_after += 1;
+	    int kj;
+	    for( kj = index_i+2; kj < histo_size; kj++)
+	    {
+	        differ_count_i_after += expectations_histo( i, kj );
+		if( differ_count_i_before > 0 )
+		    differ_count_i_before += expectations_histo( i, kj );
+		if( expectations_histo( j, kj ) > 0 )
+		    break;
+		n_differ_i_after += 1;
+		if( differ_count_i_before > 0 )
+		    n_differ_i_before += 1;
+	    }
+	    if( kj < histo_size )
+            {
+                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) ) *(real)(n_differ_i_after+1)*over_dq;
+
+		if( differ_count_i_before > 0 )
+                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) ) *(real)(n_differ_i_before+1)*over_dq;
+	    }
+	    else
+		"cas o? on regroupe avec le dernier";   
+        }
+    }
+    return grad_update;
+}
+
+real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+{
+    //PLASSERT( over_dq > 0.0 )
+
+    real grad_update = 0.0;
+		    
+    real Ni_ki = expectations_histo(i,index_i);
+    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
+                                                  // if expectations_histo is up to date,
+                                                  // the expectation(isample,i) has been counted
+    real Ni_ki_shift1 = expectations_histo(i,index_i+1);
+		    
+    real Nj_ki        = expectations_histo(j,index_i);
+    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+
+
+        // removing the term of the sum that will be modified
+        grad_update -= KLdivTerm( Ni_ki, Nj_ki );
+							       
+        // adding the term of the sum with its modified value
+        grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
+
+        grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1 );
+	
+        grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );
+
+    return grad_update *over_dq;
+}
+
+
+real LayerCostModule::KLdivTerm(real pi, real pj)
+{
+    return ( pj - pi ) * safeflog( pi/pj );
+}
+
+void LayerCostModule::computeHisto(const Mat& expectations)
+{
+    int index, batch_size = expectations.length();
+    real one_count = 1. / (real)batch_size;
+    Vec expectation;
+    
+    expectations_histo.clear(); 
+    for (int isample = 0; isample < batch_size; isample++)
+    {
+        expectation = expectations(isample);
+        for (int i = 0; i < input_size; i++)
+        {
+	    index = histo_index(expectation[i]);
+            expectations_histo(i,index) += one_count;
+        }
+    }
+}
+
+
+
+void LayerCostModule::computeSafeHisto(const Mat& expectations)
+{
+    int index, batch_size = expectations.length();
+    real one_count = 1. / (real)(batch_size+histo_size);
+    Vec expectation;
+    
+    expectations_histo.fill(one_count);
+/*
+    for (int k = 0; k < histo_size; k++)
+        for (int i = 0; i < input_size; i++)
+            expectations_histo(i,k) = one_count;
+*/
+    for (int isample = 0; isample < batch_size; isample++)
+    {
+        expectation = expectations(isample);
+        for (int i = 0; i < input_size; i++)
+        {
+	    index = histo_index(expectation[i]);
+            expectations_histo(i,index) += one_count;
+        }
+    }
+}
+
+
+// Return the index of the (1D) histogram
+// corresponding to the real input value q in [0,1]
+//
+int LayerCostModule::histo_index(real q)
+{
+    if( q >=1.0 )
+       return histo_size-1;
+
+    if( !(q >= 0.0) || !(q < 1.0) )
+        PLERROR("LayerCostModule detected an anormal expectation value (%f)", q);
+
+// LINEAR SCALE
+    return (int)floor(q*(real)histo_size);
+
+// LOG SCALE
+    return max(  (int)floor(log(LOGHISTO_BASE, q))+histo_size , 0);
+}
+
+// Returns the minimum amount dq which have to be added/removed to q
+// so that q+dq will be counted in the next/previous bin of the histogram
+//   (cf. LayerCostModule::histo_index)
+//
+// Note: we do not care about cases where histo_index(q)=histo_size
+//      (this is done in the bpropAccUpdate code)
+//
+real LayerCostModule::dq(real q)
+{
+// LINEAR SCALE
+    // ** Simple version **
+    return LINHISTO_STEP;
+
+    // ** Elaborated version **
+    if( fast_exact_is_equal( round(q*(real)histo_size) , ceil(q*(real)histo_size) ) )
+       return LINHISTO_STEP;
+    else
+       return -LINHISTO_STEP;
+
+    // ** BAD VERSION: too unstable **
+    // return (real)histo_index(q+1.0/(real)histo_size)/(real)histo_size - q;
+
+// LOG SCALE
+    // ** Simple version **
+    //if( q < LOGHISTO_MIN )
+    //  return LOGHISTO_BASE * LOGHISTO_MIN - q;
+    //return q*(LOGHISTO_BASE-1.0);
+    
+    // ** BAD VERSION: too unstable **
+    // real REF = LOGHISTO_BASE * LOGHISTO_MIN;
+    // while( true )
+    // {
+    //     if( q < REF )
+    //         return REF - q;
+    //     REF *= LOGHISTO_BASE;
+    // }
+}
+
+
+
+
+
+////////////
+// forget //
+////////////
+void LayerCostModule::forget()
+{
+    if( momentum > 0.0)
+    {
+        expectations_expectation_trainMemory.clear();
+        expectations_cross_quadratic_mean_trainMemory.clear();
+    }
+/*
+    expectations_expectation_testMemory.clear();
+    expectations_cross_quadratic_mean_testMemory.clear();
+    ntest = 0;
+*/
+}
+
+
+/////////////////
+// addPortName //
+/////////////////
+void LayerCostModule::addPortName(const string& name)
+{
+    PLASSERT( portname_to_index.find(name) == portname_to_index.end() );
+    portname_to_index[name] = ports.length();
+    ports.append(name);
+}
+
+//////////////
+// getPorts //
+//////////////
+const TVec<string>& LayerCostModule::getPorts()
+{
+    return ports;
+}
+
+///////////////////
+// getPortsSizes //
+///////////////////
+const TMat<int>& LayerCostModule::getPortSizes()
+{
+    return port_sizes;
+}
+
+//////////////////
+// getPortIndex //
+//////////////////
+int LayerCostModule::getPortIndex(const string& port)
+{
+    map<string, int>::const_iterator it = portname_to_index.find(port);
+    if (it == portname_to_index.end())
+        return -1;
+    else
+        return it->second;
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-09-11 19:41:53 UTC (rev 8069)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-09-12 17:24:37 UTC (rev 8070)
@@ -0,0 +1,191 @@
+// -*- C++ -*-
+
+// LayerCostModule.h
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file LayerCostModule.h */
+
+#ifndef LayerCostModule_INC
+#define LayerCostModule_INC
+
+#include <map>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn/vmat/VMat.h>
+
+namespace PLearn {
+
+/**
+ * Computes a cost function for a (hidden) representation, given two "expectation" vectors. Backpropagates it.
+ */
+class LayerCostModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    string cost_function;
+    
+    int histo_size;
+    
+    real alpha;
+    
+    real momentum;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LayerCostModule();
+
+
+    //! given the input and target, compute the cost
+    virtual void fprop(const Vec& expectation, real& cost) const;
+    virtual void fprop(const Mat& expectations, Mat& costs);
+    //! Overridden.
+    virtual void fprop(const TVec<Mat*>& ports_value);
+    
+    //! backpropagate the derivative w.r.t. activation
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    //! Some auxiliary function to deal with empirical histograms
+    virtual void computeHisto(const Mat& expectations);
+    virtual void computeSafeHisto(const Mat& expectations);
+    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real KLdivTerm(real pi, real pj);
+    virtual int histo_index(real q);
+    virtual real dq(real q);
+
+    //! Auxiliary function for the pascal's cost function
+    virtual void computePascalStatistics(const Mat& expectations, bool duringTraining);
+
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Returns all ports in a RBMModule.
+    virtual const TVec<string>& getPorts();
+
+    //! The ports' sizes are given by the corresponding RBM layers.
+    virtual const TMat<int>& getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    virtual int getPortIndex(const string& port);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(LayerCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    virtual void forget();
+
+protected:
+
+    //! Does stochastic gradient makes sense with our cost function?
+    bool is_cost_function_stochastic;
+
+    //! Histograms of expectations (estimated empiricially on the data)
+    Mat expectations_histo;
+
+    //! Some features of the histogram of expectations
+    real LINHISTO_STEP;
+    real LOGHISTO_BASE;
+    real LOGHISTO_MIN;
+
+    //! Statistics on matrix of expectations (estimated empiricially on the data)
+    Vec expectations_expectation;
+    Mat expectations_cross_quadratic_mean;
+    Vec expectations_expectation_trainMemory;
+    Mat expectations_cross_quadratic_mean_trainMemory;
+    Vec expectations_expectation_testMemory;
+    Mat expectations_cross_quadratic_mean_testMemory;
+    int ntest;
+
+    //! Map from a port name to its index in the 'ports' vector.
+    map<string, int> portname_to_index;
+
+    //! List of port names.
+    TVec<string> ports;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
+    void addPortName(const string& name);
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LayerCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Wed Sep 12 21:24:04 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Sep 2007 21:24:04 +0200
Subject: [Plearn-commits] r8071 - trunk/plearn/python
Message-ID: <200709121924.l8CJO4IQ019823@sheep.berlios.de>

Author: nouiz
Date: 2007-09-12 21:24:03 +0200 (Wed, 12 Sep 2007)
New Revision: 8071

Modified:
   trunk/plearn/python/
Log:
Added OBJS directory to ignore file



Property changes on: trunk/plearn/python
___________________________________________________________________
Name: svn:ignore
   + OBJS





From nouiz at mail.berlios.de  Wed Sep 12 21:25:20 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Sep 2007 21:25:20 +0200
Subject: [Plearn-commits] r8072 -
	trunk/plearn_learners_experimental/SurfaceTemplate
Message-ID: <200709121925.l8CJPKUW019908@sheep.berlios.de>

Author: nouiz
Date: 2007-09-12 21:25:20 +0200 (Wed, 12 Sep 2007)
New Revision: 8072

Modified:
   trunk/plearn_learners_experimental/SurfaceTemplate/
Log:
Added directory OBJS to svn:ignore



Property changes on: trunk/plearn_learners_experimental/SurfaceTemplate
___________________________________________________________________
Name: svn:ignore
   + OBJS





From nouiz at mail.berlios.de  Wed Sep 12 22:10:17 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 12 Sep 2007 22:10:17 +0200
Subject: [Plearn-commits] r8073 - trunk/python_modules/plearn/learners
Message-ID: <200709122010.l8CKAHOA022391@sheep.berlios.de>

Author: nouiz
Date: 2007-09-12 22:10:17 +0200 (Wed, 12 Sep 2007)
New Revision: 8073

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
-Added function computeOutputAndCosts
-Now receive the weak_learner to use as a parameter



Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-12 19:25:20 UTC (rev 8072)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-12 20:10:17 UTC (rev 8073)
@@ -3,46 +3,37 @@
 import time
 
 class AdaBoostMultiClasses:
-#class AdaBoost3PLearner(pl.PLearner):
-    def __init__(self,trainSet1,trainSet2):
+    def __init__(self,trainSet1,trainSet2,weakLearner):
+#        """
+#        Initialize a AdaBoost for 3 classes learner
+#        trainSet1 is used for the first sub AdaBoost learner,
+#        trainSet2 is used for the second sub learner
+#        weakLearner should be a function that return a new weak learner
+#        """
         self.trainSet1=trainSet1
         self.trainSet2=trainSet2
-        self.learner1 = self.myAdaBoostLearner(self.weakLearner(),trainSet1)
+            
+        self.learner1 = self.myAdaBoostLearner(weakLearner(),trainSet1)
         self.learner1.expdir=plargs.expdirr+"/learner1"
         self.learner1.setTrainingSet(trainSet1,True)
         
-        self.learner2 = self.myAdaBoostLearner(self.weakLearner(),trainSet2)
+        self.learner2 = self.myAdaBoostLearner(weakLearner(),trainSet2)
         self.learner2.expdir=plargs.expdirr+"/learner2"
         self.learner2.setTrainingSet(trainSet2,True)
         self.nstages = 0
         self.stage = 0
         self.train_time = 0
-#        self.confusion_target=plargs.confusion_target
+        #        self.confusion_target=plargs.confusion_target
         
-    def weakLearner(self):
-        """ Return a new instance of the weak learner to use"""
-        return pl.RegressionTree(
-            nstages = plargs.subnstages
-            ,loss_function_weight = 1
-            ,missing_is_valid = plargs.missing_is_valid
-            ,multiclass_outputs = plargs.multiclass_output
-            ,maximum_number_of_nodes = 250
-            ,compute_train_stats = 0
-            ,complexity_penalty_factor = 0.0
-            ,verbosity = 0
-            ,report_progress = 1
-            ,forget_when_training_set_changes = 1
-            ,conf_rated_adaboost = plargs.conf_rated_adaboost
-            ,leave_template = pl.RegressionTreeLeave( )
-            )
-    
     def myAdaBoostLearner(self,sublearner,trainSet):
         l = pl.AdaBoost()
         l.weak_learner_template=sublearner
-        l.pseudo_loss_adaboost=True
+        l.pseudo_loss_adaboost=plargs.pseudo_loss_adaboost
         l.weight_by_resampling=plargs.weight_by_resampling
         l.setTrainingSet(trainSet,True)
         l.setTrainStatsCollector(VecStatsCollector())
+        l.early_stopping=False
+        l.compute_training_error=False
         return l
 
     def train(self):
@@ -104,7 +95,12 @@
             costs.append(0)
         
         return costs
-        
+
+    def computeOutputAndCosts(self,input,target):
+        output=self.computeOutput(input)
+        costs=self.computeCostsFromOutput(input,output,target)
+        return (output,costs)
+
     def outputsize(self):
         return len(self.getTestCostNames())
 



From nouiz at mail.berlios.de  Thu Sep 13 15:43:37 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 13 Sep 2007 15:43:37 +0200
Subject: [Plearn-commits] r8074 - trunk/scripts
Message-ID: <200709131343.l8DDhbmI014782@sheep.berlios.de>

Author: nouiz
Date: 2007-09-13 15:43:37 +0200 (Thu, 13 Sep 2007)
New Revision: 8074

Modified:
   trunk/scripts/dbidispatch
Log:
-Added option for the ssh back-end
-log all the exection of the script in the home directory
-better help message


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-09-12 20:10:17 UTC (rev 8073)
+++ trunk/scripts/dbidispatch	2007-09-13 13:43:37 UTC (rev 8074)
@@ -3,27 +3,28 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--cluster[=nb_process]|--local[=nb_process]|--bqtools[=nb_process]|--condor] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
-LongHelp="""
-Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, cluster, local, ssh and bqtools. If no system is selected on the command line, we try them in this order: condor, bqsubmit, cluster local
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
+LongHelp="""Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the gived order. ssh is never automaticaly selected.
 
 %s
 
-option not explained:
-  --help, -h
-  --dbilog, --nodbilog
-  --cluster,--local,--bqtools,--condor
-  --file=FILEPATH
-  
 common option:
+  The -h, --help print the long help(this)
+  The --condor, --bqtools, --cluster, --local or --ssh option tell witch on system the jobs will be send. If not present, we will use the first available in the order gived. ssh is never automaticaly selected.
+  The --dbilog (--nodbilog) ask dbi to (to don't) generate additional log
   The '--test' option make that dbidispatch generate the file $ScriptName, but do not execute it. That way you can see what dbidispatch generate. Also, this file make dbi in test mode, so dbi do not execute automaticaly the experiment il $ScriptName is executer
-local, bqtools and cluster parameter:
+  The --file=FILEPATH option make this script use the jobs to expand in the file instead of the command line. Their must be one jobs by line.
+
+dbidispatch --test --file=tests
+
+
+bqtools, cluster, local and ssh parameter:
   --nb_proc=nb_process, give the maximum number of concurent jobs running
     --local=X is the same as --local --nb_proc=X
     --cluster=X is the same as --cluster --nb_proc=X
     --bqtools=X is the same as --bqtools --nb_proc=X
+    --ssh=X is the same as --ssh --nb_proc=X
 
-
 bqtools and cluster option:
   The '--duree' option tell the maximum length of the jobs. The have the cluster syntaxe of accepted value 'cluster --help'. The bqtools syntaxe is '--duree=12:13:15'. This give 12 hours, 13 minutes and 15 seconds
 
@@ -84,11 +85,7 @@
   aplearn myscript.plearn numhidden=25 wd=0.01
   aplearn myscript.plearn numhidden=25 wd=0.001
 
-If the optinal parameter '--file=FILEPATH' is set, the script will take commands from the file instead of taking one as parameter. This file must have one command to execute by line and each of them will be expended.
-
-dbidispatch --test --file=tests
-
-In the file, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
+In the file of the parameter --file=FILEPATH, there must not be double quotes around the {{}} as they are for the shell and if the command is in the file, they are not interpreted by the shell.
 """%ShortHelp
 
 if len(sys.argv) == 1:
@@ -139,6 +136,13 @@
             dbi_param["nb_proc"]=argv[8:]
     elif argv.startswith("--nb_proc="):
         dbi_param["nb_proc"]=argv[10:]
+    elif argv.startswith("--ssh"):
+        launch_cmd = "Ssh"
+        if len(argv)>5:
+            assert(argv[5]=="=")
+            dbi_param["nb_proc"]=argv[6:]
+        dbi_param["file_redirect_stdout"]=False
+        dbi_param["file_redirect_stderr"]=False
     elif argv == "--test":
         dbi_param["test"]=True
     elif argv.startswith("--file="):
@@ -236,6 +240,11 @@
     dbi_param["log_dir"]=os.path.join("LOGS",FILE)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
+
+SCRIPT=open(os.getenv("HOME")+"/.dbidispatch.launched",'a');
+SCRIPT.write("["+time.ctime()+"] "+str(sys.argv)+"\n")
+SCRIPT.close()
+
 if "test" in dbi_param:
     print "We generated %s command in the file"% len(commands)
     print "The script %s was not launched"% ScriptName



From manzagop at mail.berlios.de  Thu Sep 13 20:53:55 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 13 Sep 2007 20:53:55 +0200
Subject: [Plearn-commits] r8075 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709131853.l8DIrtiS018599@sheep.berlios.de>

Author: manzagop
Date: 2007-09-13 20:53:54 +0200 (Thu, 13 Sep 2007)
New Revision: 8075

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
- Added a deceleration coefficient for the step sizes.
- Added limits on the step sizes.
- Default values for step size related values now based upon '94 rprop tech report.
- Now performing the step size update before taking the step.
- In pvGradUpdate, now checking to see if m and e (gradient mean and stderror) are
inferior to 1e-15. If so we skip the iteration (continue). This cures the symptom,
but isn't a real fix to the cause (saturating weights?).



Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-13 13:43:37 UTC (rev 8074)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-13 18:53:54 UTC (rev 8075)
@@ -75,8 +75,12 @@
       //corr_profiling_start(0), 
       //corr_profiling_end(0),
       use_pvgrad(false),
-      pv_initial_stepsize(1e-6),
-      pv_acceleration(2),
+      // Next 5 values based on those used in the '94 rprop tech report
+      pv_initial_stepsize(1e-1),
+      pv_min_stepsize(1e-6),
+      pv_max_stepsize(50.0),
+      pv_acceleration(1.2),
+      pv_deceleration(0.5),
       pv_min_samples(2),
       pv_required_confidence(0.80),
       pv_random_sample_step(false),
@@ -295,8 +299,13 @@
     declareOption(ol, "pv_acceleration",
                   &NatGradNNet::pv_acceleration,
                   OptionBase::buildoption,
-                  "Coefficient by which to multiply/divide the step sizes");
+                  "Coefficient by which to multiply the step sizes.");
 
+    declareOption(ol, "pv_deceleration",
+                  &NatGradNNet::pv_deceleration,
+                  OptionBase::buildoption,
+                  "Coefficient by which to multiply the step sizes.");
+
     declareOption(ol, "pv_min_samples",
                   &NatGradNNet::pv_min_samples,
                   OptionBase::buildoption,
@@ -823,39 +832,54 @@
 
 void NatGradNNet::pvGradUpdate()
 {
-    int n = all_params_gradient.length();
+    int np = all_params_gradient.length();
     if(pv_stepsizes.length()==0)
     {
-        pv_stepsizes.resize(n);
+        pv_stepsizes.resize(np);
         pv_stepsizes.fill(pv_initial_stepsize);
-        pv_stepsigns.resize(n);
+        pv_stepsigns.resize(np);
         pv_stepsigns.fill(true);
     }
     pv_gradstats->update(all_params_gradient);
-    real pv_deceleration = 1.0/pv_acceleration;
-    for(int k=0; k<n; k++)
+    for(int k=0; k<np; k++)
     {
         StatsCollector& st = pv_gradstats->getStats(k);
-        int n = (int)st.nnonmissing();
-        if(n>pv_min_samples)
+        int ns = (int)st.nnonmissing();
+        if(ns>pv_min_samples)
         {
             real m = st.mean();
             real e = st.stderror();
+
+            // test to see if solve numerical problems
+            if( fabs(m) < 1e-15 || e < 1e-15 )
+                continue;
+
             real prob_pos = gauss_01_cum(m/e);
             real prob_neg = 1.-prob_pos;
             if(!pv_random_sample_step)
             {
+                // We adapt the stepsize before taking the step
+                // gradient is positive
                 if(prob_pos>=pv_required_confidence)
                 {
+                    pv_stepsizes[k] *= (pv_stepsigns[k]?pv_acceleration:pv_deceleration);
+                    if( pv_stepsizes[k] > pv_max_stepsize )
+                        pv_stepsizes[k] = pv_max_stepsize;
+                    else if( pv_stepsizes[k] < pv_min_stepsize )
+                        pv_stepsizes[k] = pv_min_stepsize;
                     all_params[k] += pv_stepsizes[k];
-                    pv_stepsizes[k] *= (pv_stepsigns[k]?pv_acceleration:pv_deceleration);
                     pv_stepsigns[k] = true;
                     st.forget();
                 }
+                // gradient is negative
                 else if(prob_neg>=pv_required_confidence)
                 {
+                    pv_stepsizes[k] *= ((!pv_stepsigns[k])?pv_acceleration:pv_deceleration);
+                    if( pv_stepsizes[k] > pv_max_stepsize )
+                        pv_stepsizes[k] = pv_max_stepsize;
+                    else if( pv_stepsizes[k] < pv_min_stepsize )
+                        pv_stepsizes[k] = pv_min_stepsize;
                     all_params[k] -= pv_stepsizes[k];
-                    pv_stepsizes[k] *= ((!pv_stepsigns[k])?pv_acceleration:pv_deceleration);
                     pv_stepsigns[k] = false;
                     st.forget();
                 }
@@ -873,6 +897,14 @@
             }
         }
     }
+
+    // Ouput for profiling: step sizes and number of samples
+    // horribly inefficient!
+//    ofstream fd_ss;
+//    fd_ss.open("step_sizes.txt", ios::app);
+//    fd_ss << pv_stepsizes << endl;
+//    fd_ss.close();
+
 }
 
 void NatGradNNet::computeOutput(const Vec& input, Vec& output) const

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-09-13 13:43:37 UTC (rev 8074)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-09-13 18:53:54 UTC (rev 8075)
@@ -153,9 +153,12 @@
     //! Initial size of steps in parameter space
     real pv_initial_stepsize;
 
-    //! Coefficient by which to multiply/divide the step sizes  
-    real pv_acceleration;
+    //! Bounds for the step sizes
+    real pv_min_stepsize, pv_max_stepsize;
 
+    //! Coefficients by which to multiply the step sizes  
+    real pv_acceleration, pv_deceleration;
+
     //! PV's gradient minimum number of samples to estimate confidence
     int pv_min_samples;
 



From louradou at mail.berlios.de  Thu Sep 13 21:25:05 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 13 Sep 2007 21:25:05 +0200
Subject: [Plearn-commits] r8076 - trunk/plearn_learners/online
Message-ID: <200709131925.l8DJP57g020131@sheep.berlios.de>

Author: louradou
Date: 2007-09-13 21:25:05 +0200 (Thu, 13 Sep 2007)
New Revision: 8076

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-13 18:53:54 UTC (rev 8075)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-13 19:25:05 UTC (rev 8076)
@@ -2155,6 +2155,12 @@
         layers[i+1]->getAllActivations( connections[i] );
         layers[i+1]->computeExpectation();
 
+        if( i_output_layer==i && (!use_classification_cost && !final_module))
+        {
+            output.resize(outputsize());
+            output << layers[ i ]->expectation;
+        }
+
         if (reconstruct_layerwise)
         {
             layer_input.resize(layers[i]->size);
@@ -2165,8 +2171,19 @@
             reconstruction_costs[0] += rc;
         }
     }
+    if( i_output_layer>=n_layers-2 && (!use_classification_cost && !final_module))
+    {
+        //! We haven't computed the expectations of the top layer
+	if(i_output_layer==n_layers-1)
+	{
+            connections[ n_layers-2 ]->setAsDownInput(layers[ n_layers-2 ]->expectation );
+            layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
+            layers[ n_layers-1 ]->computeExpectation();
+        }
+        output.resize(outputsize());
+        output << layers[ i_output_layer ]->expectation;
+    }	    
 
-
     if( use_classification_cost )
         classification_module->fprop( layers[ n_layers-2 ]->expectation,
                                       output );
@@ -2200,17 +2217,9 @@
         }
     }
 
-    if( !use_classification_cost && !final_module)
+    if(!use_classification_cost && !final_module)
     {
-        output.resize(outputsize());
-	
-	connections[ n_layers-2 ]->setAsDownInput(
-            layers[ n_layers-2 ]->expectation );
-        layers[ n_layers-1 ]->getAllActivations( connections[ n_layers-2 ] );
-        layers[ n_layers-1 ]->computeExpectation();
-        output << layers[ i_output_layer ]->expectation;
-
-        //! Copy of the part above: hope it makes sense
+        //! Reconstruction error of the top layer
         if (reconstruct_layerwise)
         {
             layer_input.resize(layers[n_layers-2]->size);



From lamblin at mail.berlios.de  Thu Sep 13 22:03:36 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Thu, 13 Sep 2007 22:03:36 +0200
Subject: [Plearn-commits] r8077 - trunk/plearn_learners/online
Message-ID: <200709132003.l8DK3aL9023111@sheep.berlios.de>

Author: lamblin
Date: 2007-09-13 22:03:35 +0200 (Thu, 13 Sep 2007)
New Revision: 8077

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fix results of fprop when sampling


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2007-09-13 19:25:05 UTC (rev 8076)
+++ trunk/plearn_learners/online/RBMModule.cc	2007-09-13 20:03:35 UTC (rev 8077)
@@ -1059,13 +1059,13 @@
         {
             hidden->resize(hidden_layer->samples.length(),
                            hidden_layer->samples.width());
-            *hidden << hidden_layer->samples;
+            *hidden << hidden_layer->getExpectations();
         }
         if (hidden_act && hidden_act_is_output)
         {
             hidden_act->resize(hidden_layer->samples.length(),
                                hidden_layer->samples.width());
-            *hidden_act << hidden_layer->getExpectations();
+            *hidden_act << hidden_layer->activations;
         }
         found_a_valid_configuration = true;
     }// END SAMPLING



From manzagop at mail.berlios.de  Fri Sep 14 16:08:20 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Fri, 14 Sep 2007 16:08:20 +0200
Subject: [Plearn-commits] r8078 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709141408.l8EE8Kok027102@sheep.berlios.de>

Author: manzagop
Date: 2007-09-14 16:08:20 +0200 (Fri, 14 Sep 2007)
New Revision: 8078

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
- Bug fix in pvGradUpdate - the weight updates were WRONG. If the gradient 
is positive (resp. negative) the we must add a NEGATIVE (resp. positive) 
value to the weight.
- Added some coded for outputing info during development of the pvGrad, 
commented out.



Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-13 20:03:35 UTC (rev 8077)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-14 14:08:20 UTC (rev 8078)
@@ -75,10 +75,11 @@
       //corr_profiling_start(0), 
       //corr_profiling_end(0),
       use_pvgrad(false),
-      // Next 5 values based on those used in the '94 rprop tech report
-      pv_initial_stepsize(1e-1),
+      // Next 5 values inspired those used in the '94 rprop tech report
+      // but we are in stochastic case
+      pv_initial_stepsize(1e-3),
       pv_min_stepsize(1e-6),
-      pv_max_stepsize(50.0),
+      pv_max_stepsize(1e-1),
       pv_acceleration(1.2),
       pv_deceleration(0.5),
       pv_min_samples(2),
@@ -592,6 +593,7 @@
         pv_stepsizes.fill(pv_initial_stepsize);
         pv_stepsigns.resize(n);
         pv_stepsigns.fill(true);
+        all_ns.resize(n);
     }
 
 }
@@ -828,6 +830,18 @@
         }
     }
 
+    // Ouput for profiling: weights
+    // horribly inefficient!
+/*    ofstream fd_params;
+    fd_params.open("params.txt", ios::app);
+    fd_params << all_params << endl;
+    fd_params.close();
+
+    ofstream fd_gradients;
+    fd_gradients.open("gradients.txt", ios::app);
+    fd_gradients << all_params_gradient << endl;
+    fd_gradients.close();
+*/
 }
 
 void NatGradNNet::pvGradUpdate()
@@ -839,6 +853,8 @@
         pv_stepsizes.fill(pv_initial_stepsize);
         pv_stepsigns.resize(np);
         pv_stepsigns.fill(true);
+        // profiling
+        all_ns.resize(np);
     }
     pv_gradstats->update(all_params_gradient);
     for(int k=0; k<np; k++)
@@ -867,7 +883,7 @@
                         pv_stepsizes[k] = pv_max_stepsize;
                     else if( pv_stepsizes[k] < pv_min_stepsize )
                         pv_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] += pv_stepsizes[k];
+                    all_params[k] -= pv_stepsizes[k];
                     pv_stepsigns[k] = true;
                     st.forget();
                 }
@@ -879,7 +895,7 @@
                         pv_stepsizes[k] = pv_max_stepsize;
                     else if( pv_stepsizes[k] < pv_min_stepsize )
                         pv_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] -= pv_stepsizes[k];
+                    all_params[k] += pv_stepsizes[k];
                     pv_stepsigns[k] = false;
                     st.forget();
                 }
@@ -896,14 +912,22 @@
                 st.forget();
             }
         }
+
+        // profiling
+        all_ns[k] = ns;
     }
 
     // Ouput for profiling: step sizes and number of samples
     // horribly inefficient!
-//    ofstream fd_ss;
-//    fd_ss.open("step_sizes.txt", ios::app);
-//    fd_ss << pv_stepsizes << endl;
-//    fd_ss.close();
+/*    ofstream fd_ss;
+    fd_ss.open("step_sizes.txt", ios::app);
+    fd_ss << pv_stepsizes << endl;
+    fd_ss.close();
+    ofstream fd_ns;
+    fd_ns.open("nsamples.txt", ios::app);
+    fd_ns << all_ns << endl;
+    fd_ns.close();
+*/
 
 }
 

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-09-13 20:03:35 UTC (rev 8077)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-09-14 14:08:20 UTC (rev 8078)
@@ -181,6 +181,9 @@
     //! Indicates whether the previous step was positive (true) or negative (false)
     TVec<bool> pv_stepsigns;
 
+    // profiling
+    TVec<int> all_ns;
+
 public:
     //#####  Public Member Functions  #########################################
 



From nouiz at mail.berlios.de  Fri Sep 14 16:09:44 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Sep 2007 16:09:44 +0200
Subject: [Plearn-commits] r8079 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200709141409.l8EE9idK027171@sheep.berlios.de>

Author: nouiz
Date: 2007-09-14 16:09:44 +0200 (Fri, 14 Sep 2007)
New Revision: 8079

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
-In the cluster backend, by default give the wait option to the cluster command.
-Added the option --nowait to dbidispatch that make the --wait option not passed to the cluster command


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-14 14:08:20 UTC (rev 8078)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-14 14:09:44 UTC (rev 8079)
@@ -303,7 +303,7 @@
     def __init__(self, commands, **args ):
         self.duree=None
         self.arch=None
-        self.cluster_wait=None
+        self.cluster_wai=True
         self.threads=[]
         self.started=0
         self.nb_proc=50

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-09-14 14:08:20 UTC (rev 8078)
+++ trunk/scripts/dbidispatch	2007-09-14 14:09:44 UTC (rev 8079)
@@ -3,7 +3,7 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
 LongHelp="""Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the gived order. ssh is never automaticaly selected.
 
 %s
@@ -47,7 +47,8 @@
 
 cluster only option:
   The '--wait' is transfered to cluster. This must be enabled if their is not nb_process available compute node. Otherwise when their is no compute node available, the launch of that command fail.
-
+  The '--nowait' make that the --wait option is not gived to cluster as is the default
+  
 condor only option:
   The '--req=\"CONDOR_REQUIREMENT\"' option make that dbidispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
 
@@ -151,6 +152,8 @@
         dbi_param["arch"]=argv[2:]
     elif argv == "--wait":
         dbi_param["cluster_wait"]=True
+    elif argv == "--nowait":
+        dbi_param["cluster_wait"]=False
     elif argv[0:6] == "--req=":
         dbi_param["requirements"]="\"%s\""%argv[6:]
     elif argv == "--no_clean_up":



From nouiz at mail.berlios.de  Fri Sep 14 16:40:57 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Sep 2007 16:40:57 +0200
Subject: [Plearn-commits] r8080 - trunk/python_modules/plearn/learners
Message-ID: <200709141440.l8EEevEY029884@sheep.berlios.de>

Author: nouiz
Date: 2007-09-14 16:40:56 +0200 (Fri, 14 Sep 2007)
New Revision: 8080

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
-added costs about the class gived as output. This allow to do stat about the output


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-14 14:09:44 UTC (rev 8079)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-14 14:40:56 UTC (rev 8080)
@@ -55,6 +55,7 @@
                 costnames.append("conf_matrix_%d_%d"%(i,j))
         costnames.append("train_time")
         costnames.append("conflict")
+        costnames.extend(["class0","class1","class2"])
         return costnames
     
     def computeOutput(self,example):
@@ -89,11 +90,18 @@
                 costs.append(0)
         costs[output[0]*3+target+3]=1
         costs.append(self.train_time)
+        if output[0]==0:
+            costs.extend([0,1,0,0])
+        if output[0]==1:
+            costs.extend([0,0,1,0])
+        if output[0]==2:
+            costs.extend([0,0,0,1])
         if output[0]==3:
             costs.append(1)
-        else:
-            costs.append(0)
-        
+            t=[0,0,0]
+            t[plargs.confusion_target]=1
+            costs.extend(t)
+            
         return costs
 
     def computeOutputAndCosts(self,input,target):



From nouiz at mail.berlios.de  Fri Sep 14 17:00:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Sep 2007 17:00:34 +0200
Subject: [Plearn-commits] r8081 - trunk/python_modules/plearn/parallel
Message-ID: <200709141500.l8EF0Yxe031261@sheep.berlios.de>

Author: nouiz
Date: 2007-09-14 17:00:34 +0200 (Fri, 14 Sep 2007)
New Revision: 8081

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
fix typo


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-14 14:40:56 UTC (rev 8080)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-14 15:00:34 UTC (rev 8081)
@@ -303,7 +303,7 @@
     def __init__(self, commands, **args ):
         self.duree=None
         self.arch=None
-        self.cluster_wai=True
+        self.cluster_wait=True
         self.threads=[]
         self.started=0
         self.nb_proc=50



From nouiz at mail.berlios.de  Fri Sep 14 22:13:10 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 14 Sep 2007 22:13:10 +0200
Subject: [Plearn-commits] r8082 - trunk/scripts
Message-ID: <200709142013.l8EKDAvj001075@sheep.berlios.de>

Author: nouiz
Date: 2007-09-14 22:13:10 +0200 (Fri, 14 Sep 2007)
New Revision: 8082

Modified:
   trunk/scripts/collectres
Log:
created a function collectres, so that we can use it in script


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-09-14 15:00:34 UTC (rev 8081)
+++ trunk/scripts/collectres	2007-09-14 20:13:10 UTC (rev 8082)
@@ -243,7 +243,19 @@
   else:
     raise ValueError("Invalid <spec> mode, expected 'min', 'sort', or 'plot', got "+mode)
 
-
+def collectres(outputfile,speclist,filenames,printcommand=True):
+  specs= string.split(speclist)
+  mode = specs[0]
+  f=open(outputfile,"w")
+  if printcommand:
+    f.write("# "+sys.argv[0]+" "+outputfile+" "+'"'+speclist+'" ')
+    for file in filenames:
+      f.write(file+" ")
+  f.write("\n")
+  outputres(f,mode,specs[1:],getres(specs[1:],filenames))
+  f.flush()
+  f.close()
+  
 if __name__=='__main__':
   args = sys.argv[:]
   if len(args)<=3:
@@ -272,14 +284,8 @@
     print '  mincol <mcol> [<col1> <col2>...] : keep only the minimum value in colun <mcol> of each pmat,'
     print '                          keeping track at the same time of the <coli> values at the min-selected row.'
     sys.exit(1)
-  output = args[1]
-  filenames = args[3:]
-  speclist = string.split(args[2])
-  mode = speclist[0]
-  f=open(output,"w")
-  f.write("# "+args[0]+" "+output+" "+'"'+args[2]+'" ')
-  for file in filenames:
-    f.write(file+" ")
-  f.write("\n")
-  outputres(f,mode,speclist[1:],getres(speclist[1:],filenames))
-  f.flush()
+  printcommand=True
+  if args[1]=="--printcommand":
+    printcommand=False
+    del args[1]
+  collectres(args[1],args[2],args[3:],printcommand)



From manzagop at mail.berlios.de  Tue Sep 18 14:12:11 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Tue, 18 Sep 2007 14:12:11 +0200
Subject: [Plearn-commits] r8083 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709181212.l8ICCB6O002351@sheep.berlios.de>

Author: manzagop
Date: 2007-09-18 14:12:10 +0200 (Tue, 18 Sep 2007)
New Revision: 8083

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:
- pv_min/max_stepsizes are now build options.
- changed some default values for pv options


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-14 20:13:10 UTC (rev 8082)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-18 12:12:10 UTC (rev 8083)
@@ -77,9 +77,9 @@
       use_pvgrad(false),
       // Next 5 values inspired those used in the '94 rprop tech report
       // but we are in stochastic case
-      pv_initial_stepsize(1e-3),
+      pv_initial_stepsize(1e-1),
       pv_min_stepsize(1e-6),
-      pv_max_stepsize(1e-1),
+      pv_max_stepsize(50.0),
       pv_acceleration(1.2),
       pv_deceleration(0.5),
       pv_min_samples(2),
@@ -297,6 +297,16 @@
                   OptionBase::buildoption,
                   "Initial size of steps in parameter space");
 
+    declareOption(ol, "pv_min_stepsize",
+                  &NatGradNNet::pv_min_stepsize,
+                  OptionBase::buildoption,
+                  "Minimal size of steps in parameter space");
+
+    declareOption(ol, "pv_max_stepsize",
+                  &NatGradNNet::pv_max_stepsize,
+                  OptionBase::buildoption,
+                  "Maximal size of steps in parameter space");
+
     declareOption(ol, "pv_acceleration",
                   &NatGradNNet::pv_acceleration,
                   OptionBase::buildoption,



From nouiz at mail.berlios.de  Tue Sep 18 19:43:55 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 18 Sep 2007 19:43:55 +0200
Subject: [Plearn-commits] r8084 - trunk/plearn_learners/online
Message-ID: <200709181743.l8IHhtl1013110@sheep.berlios.de>

Author: nouiz
Date: 2007-09-18 19:43:54 +0200 (Tue, 18 Sep 2007)
New Revision: 8084

Modified:
   trunk/plearn_learners/online/LayerCostModule.h
Log:
changed from dos end of lines to unix end of line as pymake support completly only the unix format


Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-09-18 12:12:10 UTC (rev 8083)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-09-18 17:43:54 UTC (rev 8084)
@@ -1,191 +1,191 @@
-// -*- C++ -*-
-
-// LayerCostModule.h
-//
-// Copyright (C) 2007 Jerome Louradour
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Jerome Louradour
-
-/*! \file LayerCostModule.h */
-
-#ifndef LayerCostModule_INC
-#define LayerCostModule_INC
-
-#include <map>
-#include <plearn_learners/online/OnlineLearningModule.h>
-#include <plearn/vmat/VMat.h>
-
-namespace PLearn {
-
-/**
- * Computes a cost function for a (hidden) representation, given two "expectation" vectors. Backpropagates it.
- */
-class LayerCostModule : public OnlineLearningModule
-{
-    typedef OnlineLearningModule inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-    string cost_function;
-    
-    int histo_size;
-    
-    real alpha;
-    
-    real momentum;
-    
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    LayerCostModule();
-
-
-    //! given the input and target, compute the cost
-    virtual void fprop(const Vec& expectation, real& cost) const;
-    virtual void fprop(const Mat& expectations, Mat& costs);
-    //! Overridden.
-    virtual void fprop(const TVec<Mat*>& ports_value);
-    
-    //! backpropagate the derivative w.r.t. activation
-    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
-                                const TVec<Mat*>& ports_gradient);
-
-    //! Some auxiliary function to deal with empirical histograms
-    virtual void computeHisto(const Mat& expectations);
-    virtual void computeSafeHisto(const Mat& expectations);
-    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real KLdivTerm(real pi, real pj);
-    virtual int histo_index(real q);
-    virtual real dq(real q);
-
-    //! Auxiliary function for the pascal's cost function
-    virtual void computePascalStatistics(const Mat& expectations, bool duringTraining);
-
-    //! Overridden to do nothing (in particular, no warning).
-    virtual void setLearningRate(real dynamic_learning_rate) {}
-
-    //! Returns all ports in a RBMModule.
-    virtual const TVec<string>& getPorts();
-
-    //! The ports' sizes are given by the corresponding RBM layers.
-    virtual const TMat<int>& getPortSizes();
-
-    //! Return the index (as in the list of ports returned by getPorts()) of
-    //! a given port.
-    //! If 'port' does not exist, -1 is returned.
-    virtual int getPortIndex(const string& port);
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
-    PLEARN_DECLARE_OBJECT(LayerCostModule);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-    virtual void forget();
-
-protected:
-
-    //! Does stochastic gradient makes sense with our cost function?
-    bool is_cost_function_stochastic;
-
-    //! Histograms of expectations (estimated empiricially on the data)
-    Mat expectations_histo;
-
-    //! Some features of the histogram of expectations
-    real LINHISTO_STEP;
-    real LOGHISTO_BASE;
-    real LOGHISTO_MIN;
-
-    //! Statistics on matrix of expectations (estimated empiricially on the data)
-    Vec expectations_expectation;
-    Mat expectations_cross_quadratic_mean;
-    Vec expectations_expectation_trainMemory;
-    Mat expectations_cross_quadratic_mean_trainMemory;
-    Vec expectations_expectation_testMemory;
-    Mat expectations_cross_quadratic_mean_testMemory;
-    int ntest;
-
-    //! Map from a port name to its index in the 'ports' vector.
-    map<string, int> portname_to_index;
-
-    //! List of port names.
-    TVec<string> ports;
-
-    //#####  Protected Member Functions  ######################################
-
-    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
-    void addPortName(const string& name);
-
-    //! Declares the class options.
-    static void declareOptions(OptionList& ol);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(LayerCostModule);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :
+// -*- C++ -*-
+
+// LayerCostModule.h
+//
+// Copyright (C) 2007 Jerome Louradour
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Jerome Louradour
+
+/*! \file LayerCostModule.h */
+
+#ifndef LayerCostModule_INC
+#define LayerCostModule_INC
+
+#include <map>
+#include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn/vmat/VMat.h>
+
+namespace PLearn {
+
+/**
+ * Computes a cost function for a (hidden) representation, given two "expectation" vectors. Backpropagates it.
+ */
+class LayerCostModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    string cost_function;
+    
+    int histo_size;
+    
+    real alpha;
+    
+    real momentum;
+    
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LayerCostModule();
+
+
+    //! given the input and target, compute the cost
+    virtual void fprop(const Vec& expectation, real& cost) const;
+    virtual void fprop(const Mat& expectations, Mat& costs);
+    //! Overridden.
+    virtual void fprop(const TVec<Mat*>& ports_value);
+    
+    //! backpropagate the derivative w.r.t. activation
+    virtual void bpropAccUpdate(const TVec<Mat*>& ports_value,
+                                const TVec<Mat*>& ports_gradient);
+
+    //! Some auxiliary function to deal with empirical histograms
+    virtual void computeHisto(const Mat& expectations);
+    virtual void computeSafeHisto(const Mat& expectations);
+    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual real KLdivTerm(real pi, real pj);
+    virtual int histo_index(real q);
+    virtual real dq(real q);
+
+    //! Auxiliary function for the pascal's cost function
+    virtual void computePascalStatistics(const Mat& expectations, bool duringTraining);
+
+    //! Overridden to do nothing (in particular, no warning).
+    virtual void setLearningRate(real dynamic_learning_rate) {}
+
+    //! Returns all ports in a RBMModule.
+    virtual const TVec<string>& getPorts();
+
+    //! The ports' sizes are given by the corresponding RBM layers.
+    virtual const TMat<int>& getPortSizes();
+
+    //! Return the index (as in the list of ports returned by getPorts()) of
+    //! a given port.
+    //! If 'port' does not exist, -1 is returned.
+    virtual int getPortIndex(const string& port);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(LayerCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    virtual void forget();
+
+protected:
+
+    //! Does stochastic gradient makes sense with our cost function?
+    bool is_cost_function_stochastic;
+
+    //! Histograms of expectations (estimated empiricially on the data)
+    Mat expectations_histo;
+
+    //! Some features of the histogram of expectations
+    real LINHISTO_STEP;
+    real LOGHISTO_BASE;
+    real LOGHISTO_MIN;
+
+    //! Statistics on matrix of expectations (estimated empiricially on the data)
+    Vec expectations_expectation;
+    Mat expectations_cross_quadratic_mean;
+    Vec expectations_expectation_trainMemory;
+    Mat expectations_cross_quadratic_mean_trainMemory;
+    Vec expectations_expectation_testMemory;
+    Mat expectations_cross_quadratic_mean_testMemory;
+    int ntest;
+
+    //! Map from a port name to its index in the 'ports' vector.
+    map<string, int> portname_to_index;
+
+    //! List of port names.
+    TVec<string> ports;
+
+    //#####  Protected Member Functions  ######################################
+
+    //! Add a new port to the 'portname_to_index' map and 'ports' vector.
+    void addPortName(const string& name);
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LayerCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Sep 19 20:55:57 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 19 Sep 2007 20:55:57 +0200
Subject: [Plearn-commits] r8085 - trunk/python_modules/plearn/pymake
Message-ID: <200709191855.l8JItvAb005773@sheep.berlios.de>

Author: lamblin
Date: 2007-09-19 20:55:57 +0200 (Wed, 19 Sep 2007)
New Revision: 8085

Modified:
   trunk/python_modules/plearn/pymake/minicpreproc.g
   trunk/python_modules/plearn/pymake/minicpreproc.py
Log:
Now accepts DOS-style endlines


Modified: trunk/python_modules/plearn/pymake/minicpreproc.g
===================================================================
--- trunk/python_modules/plearn/pymake/minicpreproc.g	2007-09-18 17:43:54 UTC (rev 8084)
+++ trunk/python_modules/plearn/pymake/minicpreproc.g	2007-09-19 18:55:57 UTC (rev 8085)
@@ -6,6 +6,9 @@
 # by Yapps (http://theory.stanford.edu/~amitp/Yapps/), that should also
 # be distributed with pymake.
 #
+# The command line to build minicpreproc.py is:
+# % python yapps.py -fembed-error-printer -fembed-scanner minicpreproc.g
+#
 # Copyright (C) 2006 Pascal Lamblin
 #
 #  Redistribution and use in source and binary forms, with or without
@@ -88,7 +91,7 @@
     ignore: '[ \t]+'
 
     token END: "$"
-    token ENDLINE: "[\n\r]|$"
+    token ENDLINE: "\r\n|\n|\r|$"
 
     token FALSE: "0"
     token TRUE: "1"

Modified: trunk/python_modules/plearn/pymake/minicpreproc.py
===================================================================
--- trunk/python_modules/plearn/pymake/minicpreproc.py	2007-09-18 17:43:54 UTC (rev 8084)
+++ trunk/python_modules/plearn/pymake/minicpreproc.py	2007-09-19 18:55:57 UTC (rev 8085)
@@ -6,6 +6,9 @@
 # by Yapps (http://theory.stanford.edu/~amitp/Yapps/), that should also
 # be distributed with pymake.
 #
+# The command line to build minicpreproc.py is:
+# % python yapps.py -fembed-error-printer -fembed-scanner minicpreproc.g
+#
 # Copyright (C) 2006 Pascal Lamblin
 #
 #  Redistribution and use in source and binary forms, with or without
@@ -191,7 +194,7 @@
             ('"\\("', '\\('),
             ("'[ \\t]+'", '[ \t]+'),
             ('END', '$'),
-            ('ENDLINE', '[\n\r]|$'),
+            ('ENDLINE', '\r\n|\n|\r|$'),
             ('FALSE', '0'),
             ('TRUE', '1'),
             ('UNKNOWN', '@'),



From nouiz at mail.berlios.de  Thu Sep 20 17:21:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 20 Sep 2007 17:21:06 +0200
Subject: [Plearn-commits] r8086 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709201521.l8KFL6ov005159@sheep.berlios.de>

Author: nouiz
Date: 2007-09-20 17:21:06 +0200 (Thu, 20 Sep 2007)
New Revision: 8086

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:
-Added weighting of the gradient with the wights of the examples
-Added the cost weithted_class_error that is the class_error weighted by the example



Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-19 18:55:57 UTC (rev 8085)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-20 15:21:06 UTC (rev 8086)
@@ -1088,8 +1088,10 @@
             costs(i,0) = -outp[target_class];
             costs(i,1) = (target_class == argmax(outp))?0:1;
             grad[target_class]-=1;
-            if (example_weight[i]!=1.0)
-                costs(i,0) *= example_weight[i];
+
+            costs(i,0) *= example_weight[i];
+            costs(i,2) = costs(i,1) * example_weight[i];
+            grad *= example_weight[i];
         }
     }
     else if(output_type=="cross_entropy")   {
@@ -1107,8 +1109,10 @@
                 costs(i,1) = (grad[0]>0.5)?1:0;
             }
             grad[0] -= (real)target_class;
-            if (example_weight[i]!=1.0)
-                costs(i,0) *= example_weight[i];
+
+            costs(i,0) *= example_weight[i];
+            costs(i,2) = costs(i,1) * example_weight[i];
+            grad *= example_weight[i];
         }
 //cout << "costs\t" << costs(0) << endl;
 //cout << "gradient\t" << out_grad(0) << endl;
@@ -1186,14 +1190,16 @@
     TVec<string> costs;
     if (output_type=="NLL")
     {
-        costs.resize(2);
+        costs.resize(3);
         costs[0]="NLL";
         costs[1]="class_error";
+        costs[2]="weighted_class_error";
     }
     else if (output_type=="cross_entropy")  {
-        costs.resize(2);
+        costs.resize(3);
         costs[0]="cross_entropy";
         costs[1]="class_error";
+        costs[2]="weighted_class_error";
     }
     else if (output_type=="MSE")
     {



From tihocan at mail.berlios.de  Thu Sep 20 22:11:52 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 20 Sep 2007 22:11:52 +0200
Subject: [Plearn-commits] r8087 - trunk/plearn/io
Message-ID: <200709202011.l8KKBq2m006656@sheep.berlios.de>

Author: tihocan
Date: 2007-09-20 22:11:51 +0200 (Thu, 20 Sep 2007)
New Revision: 8087

Modified:
   trunk/plearn/io/PStream.cc
Log:
Bug fix when reading a quoted string with no extra character after the last quote (there is a reason why the doc of putback says 'if you put back the result of a call to get(), make sure it is not EOF').


Modified: trunk/plearn/io/PStream.cc
===================================================================
--- trunk/plearn/io/PStream.cc	2007-09-20 15:21:06 UTC (rev 8086)
+++ trunk/plearn/io/PStream.cc	2007-09-20 20:11:51 UTC (rev 8087)
@@ -1067,7 +1067,7 @@
                 PLERROR("In read(istream&, string&) unterminated quoted string");
             c = get();
             if(!isspace(c)) // skip following blank if any
-                putback(c);
+                unget();
         }
         else // it's a single word without quotes
         {



From tihocan at mail.berlios.de  Thu Sep 20 22:14:26 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 20 Sep 2007 22:14:26 +0200
Subject: [Plearn-commits] r8088 - trunk/plearn/io/test
Message-ID: <200709202014.l8KKEQYG006805@sheep.berlios.de>

Author: tihocan
Date: 2007-09-20 22:14:26 +0200 (Thu, 20 Sep 2007)
New Revision: 8088

Modified:
   trunk/plearn/io/test/PStreamBufTest.cc
Log:
Added an extra test so that the bug fixed in r8087 does not appear again in the future

Modified: trunk/plearn/io/test/PStreamBufTest.cc
===================================================================
--- trunk/plearn/io/test/PStreamBufTest.cc	2007-09-20 20:11:51 UTC (rev 8087)
+++ trunk/plearn/io/test/PStreamBufTest.cc	2007-09-20 20:14:26 UTC (rev 8088)
@@ -211,6 +211,13 @@
     s.unread("1234");
     s.read(temp, 8);
     test("Unread after EOF", temp, "1234");
+
+    // Test bug fixed in r8087 when reading a quoted string whose last
+    // character is also the last quote.
+    string quoted_str("\"123\"");
+    s = PLearn::openString(quoted_str, PLearn::PStream::plearn_ascii);
+    s >> temp;
+    test("EOF at end of quoted string", s.peek(), EOF);
 }
 
 



From tihocan at mail.berlios.de  Thu Sep 20 22:45:27 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 20 Sep 2007 22:45:27 +0200
Subject: [Plearn-commits] r8089 - trunk/plearn/io
Message-ID: <200709202045.l8KKjRvm008457@sheep.berlios.de>

Author: tihocan
Date: 2007-09-20 22:45:27 +0200 (Thu, 20 Sep 2007)
New Revision: 8089

Modified:
   trunk/plearn/io/PPath.cc
Log:
Cosmetic change (splitting a line that was doing more than 80 characters)

Modified: trunk/plearn/io/PPath.cc
===================================================================
--- trunk/plearn/io/PPath.cc	2007-09-20 20:14:26 UTC (rev 8088)
+++ trunk/plearn/io/PPath.cc	2007-09-20 20:45:27 UTC (rev 8089)
@@ -260,7 +260,8 @@
 
         if (isfile(config_file_path))
         {
-            PStream ppath_config     = openFile(config_file_path, PStream::plearn_ascii );
+            PStream ppath_config = openFile(config_file_path,
+                                            PStream::plearn_ascii);
 
             string  next_metaprotocol;
             PPath   next_metapath;    



From louradou at mail.berlios.de  Sat Sep 22 00:47:01 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Sat, 22 Sep 2007 00:47:01 +0200
Subject: [Plearn-commits] r8090 - in trunk: plearn/base
	plearn_learners/online
Message-ID: <200709212247.l8LMl1Ju005514@sheep.berlios.de>

Author: louradou
Date: 2007-09-22 00:47:00 +0200 (Sat, 22 Sep 2007)
New Revision: 8090

Modified:
   trunk/plearn/base/stringutils.h
   trunk/plearn_learners/online/LayerCostModule.cc
   trunk/plearn_learners/online/LayerCostModule.h
Log:
LayerCostModule (a module to plug a cost function to a hidden layer):
The code was cleaned, and some functionalities were added.
NB: If someone knows how to compare properly 2 strings with PLearn, please let
me know!



Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2007-09-20 20:45:27 UTC (rev 8089)
+++ trunk/plearn/base/stringutils.h	2007-09-21 22:47:00 UTC (rev 8090)
@@ -140,9 +140,14 @@
 inline bool string_begins_with(const string& s, const string& beginning)
 {
     string::size_type n = beginning.size();
-    return (s.size() >= n && s.substr(0,n) == beginning);
+    return (s.size() >= n  &&  beginning == s.substr(0,n-1) );
 }
-
+inline bool string_ends_with(const string& s, const string& end)
+{
+    string::size_type n = end.size();
+    string::size_type m = s.size();
+    return (m >= n  &&  end == s.substr(m-n,m) );
+}
   
 //! replaces all occurences of searchstr in the text by replacestr
 //! returns the number of matches that got replaced

Modified: trunk/plearn_learners/online/LayerCostModule.cc
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.cc	2007-09-20 20:45:27 UTC (rev 8089)
+++ trunk/plearn_learners/online/LayerCostModule.cc	2007-09-21 22:47:00 UTC (rev 8090)
@@ -45,25 +45,23 @@
 
 PLEARN_IMPLEMENT_OBJECT(
     LayerCostModule,
-    "Computes a cost function on Layer, given:    \n",
-    "* Expectations for a binomial RBM upper layer\n"
-    "* sigmoid(activation) for a Neural Network   \n"
-    "and Back-propagates the gradient.            \n"
+    "Computes a cost function on Layer, given:            \n",
+    "* Expectations for a binomial RBM hidden layer, or   \n"
+    "* sigmoid(activation) for a layer of a Neural Net, or\n"
+    "* real outputs of any layer                          \n"
+    "and Back-propagates the gradient.                    \n"
     "\n"
-    "This function can be:                        \n"
-    "* The average Cross-Entropy                  \n"
-    "* The average Kullback-Leibler Divergence    \n"
-    "* Pascal's function...                       \n");
+    "Several cost functions can be chosen.\n"
+    "Some only apply for binomial layers. \n");
 
 LayerCostModule::LayerCostModule():
     histo_size(10),
     alpha(0.0),
-    momentum(0.0)
+    momentum(0.0),
+    cost_function(""),
+    cost_function_completename("")
 {
     output_size = 1;
-/*
-    ntest = 0;
-*/
 }
 
 void LayerCostModule::declareOptions(OptionList& ol)
@@ -78,75 +76,124 @@
     declareOption(ol, "cost_function", &LayerCostModule::cost_function,
                   OptionBase::buildoption,
         "The cost function applied to the layer:\n"
-        "- \"stochastic_cross_entropy\" [default]: average cross-entropy between pairs of binomial units\n"
-        "- \"stochastic_kl_div\": average KL divergence between pairs of binomial units\n"
-        "- \"kl_div\": KL divergence between distrubution of expectations (sampled with x)\n"
-        "- \"kl_div_2\": good version of kl_div\n"
-        "- \"kl_div_simple\": simple version of kl_div where we count at least one sample per histogram's bin\n");
+        "- \"pascal\":"
+	                    " Pascal Vincent's God given cost function.\n"
+        "- \"correlation\":"
+	                    " average of a function applied to the correlations between outputs.\n"
+        "- \"kl_div\":"
+	                    " KL divergence between distrubution of outputs (sampled with x)\n"
+        "- \"kl_div_simple\":"
+	                    " simple version of kl_div where we count at least one sample per histogram's bin\n"
+        "- \"stochastic_cross_entropy\" [default]:"
+	                    " average cross-entropy between pairs of binomial units\n"
+        "- \"stochastic_kl_div\":"
+	                    " average KL divergence between pairs of binomial units\n"
+                 );
 
     declareOption(ol, "histo_size", &LayerCostModule::histo_size,
                   OptionBase::buildoption,
-        "For \"kl_div*\" cost functions,\n"
-        "number of bins for the histograms (to estimate distributions of probabilities for expectations).\n"
+        "For \"kl_div\" cost functions,\n"
+        "number of bins for the histograms (to estimate distributions of outputs).\n"
         "The higher is histo_size, the more precise is the estimation.\n");
 
     declareOption(ol, "alpha", &LayerCostModule::alpha,
                   OptionBase::buildoption,
         "(>=0) For \"pascal\" cost function,\n"
-        "number of bins for the histograms (to estimate distributions of probabilities for expectations).\n"
+        "number of bins for the histograms (to estimate distributions of outputs).\n"
         "The higher is histo_size, the more precise is the estimation.\n");
 
     declareOption(ol, "momentum", &LayerCostModule::momentum,
                   OptionBase::buildoption,
-        "(in [0,1[) For \"pascal\" cost function, momentum for the moving means\n");
+        "(in [0,1[) For \"pascal\" cost function, momentum for the moving means.\n");
 
+
+
+    declareOption(ol, "inputs_histo", &LayerCostModule::inputs_histo,
+                  OptionBase::learntoption,
+                  "Histograms (empirical ditribution) of the output, for all units.\n"
+        );
+
+    declareOption(ol, "inputs_expectation", &LayerCostModule::inputs_expectation,
+                  OptionBase::learntoption,
+                  "Expectation of the output (in [0,1[), for all units.\n"
+        );
+
+    declareOption(ol, "inputs_stds", &LayerCostModule::inputs_stds,
+                  OptionBase::learntoption,
+                  "Standard Deviation of the output, for all units.\n"
+        );
+
+    declareOption(ol, "inputs_correlations", &LayerCostModule::inputs_correlations,
+                  OptionBase::learntoption,
+                  "Correlation of the outputs, for all pairs of units.\n"
+        );
+
+    declareOption(ol, "inputs_cross_quadratic_mean", &LayerCostModule::inputs_cross_quadratic_mean,
+                  OptionBase::learntoption,
+                  "Expectation of the cross products between outputs, for all pairs of units.\n"
+        );
+
+    declareOption(ol, "cost_function_completename", &LayerCostModule::cost_function_completename,
+                  OptionBase::learntoption,
+                  "complete name of cost_function (take into account some internal settings).\n"
+        );
 }
 
 void LayerCostModule::build_()
 {
-    PLASSERT( input_size > 1 );
     PLASSERT( histo_size > 1 );
     PLASSERT( momentum >= 0.0);
     PLASSERT( momentum < 1);
     
+    norm_factor = 1./(real)(input_size*(input_size-1));
+    
     string im = lowerstring( cost_function );
     // choose HERE the *default* cost function
     if( im == "" )
         cost_function = "pascal";
     else
         cost_function = im;
+    if( ( cost_function_completename == "" ) || !string_ends_with(cost_function_completename, cost_function) )
+        cost_function_completename = string(cost_function);
 
      // list HERE all *stochastic* cost functions
     if( ( cost_function == "stochastic_cross_entropy")
      || ( cost_function == "stochastic_kl_div") )
         is_cost_function_stochastic = true;
-	
+        
     // list HERE all *non stochastic* cost functions
     // and the specific initialization
     else if( ( cost_function == "kl_div")
-          || ( cost_function == "kl_div_simple")
-	  || ( cost_function == "kl_div_2") )
+          || ( cost_function == "kl_div_simple") )
     {
         is_cost_function_stochastic = false;
-        expectations_histo.resize(input_size,histo_size);
-	LINHISTO_STEP = 1.0/(real)histo_size;
-        LOGHISTO_BASE = 10.0;
-        LOGHISTO_MIN = (real)pow(LOGHISTO_BASE,-(real)histo_size);
+        if( input_size > 1 )
+            inputs_histo.resize(input_size,histo_size);
+        HISTO_STEP = 1.0/(real)histo_size;
     }
-    else if ( ( cost_function == "pascal") )
+    else if( (cost_function == "pascal" )
+          || (cost_function == "correlation" ) )
     {
         is_cost_function_stochastic = false;
-	expectations_expectation.resize(input_size);
-	expectations_cross_quadratic_mean.resize(input_size,input_size);
-/*
-        expectations_expectation_testMemory.resize(input_size);
-        expectations_cross_quadratic_mean_testMemory.resize(input_size,input_size);
-*/
-	if( momentum > 0.0)
-	{
-            expectations_expectation_trainMemory.resize(input_size);
-            expectations_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
-	}
+        if( input_size > 1 )
+        {
+            inputs_expectation.resize(input_size);
+            inputs_cross_quadratic_mean.resize(input_size,input_size);
+            if( cost_function == "correlation" )
+            {
+                inputs_stds.resize(input_size);
+                inputs_correlations.resize(input_size,input_size);
+            }
+            if( momentum > 0.0)
+            {
+                inputs_expectation_trainMemory.resize(input_size);
+                inputs_cross_quadratic_mean_trainMemory.resize(input_size,input_size);
+            }
+            if( cost_function == "pascal" )
+                cost_function_completename = addprepostfix( func_pascal_prefix(), "_", cost_function );
+            else if( cost_function == "correlation" )
+                cost_function_completename = addprepostfix( func_correlation_prefix(), "_", cost_function );
+        }
     }
     else
         PLERROR("LayerCostModule::build_() does not recognize cost function %s",
@@ -155,14 +202,13 @@
     // The port story...
     ports.resize(0);
     portname_to_index.clear();
-    addPortName("expectations");
+    addPortName("input");
     addPortName("cost");
 
     port_sizes.resize(nPorts(), 2);
     port_sizes.fill(-1);
-    port_sizes(getPortIndex("expectations"), 1) = input_size;
+    port_sizes(getPortIndex("input"), 1) = input_size;
     port_sizes(getPortIndex("cost"), 1) = 1;
-    
 }
 
 
@@ -178,15 +224,17 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(expectations_histo, copies);
-    deepCopyField(expectations_expectation, copies);
-    deepCopyField(expectations_cross_quadratic_mean, copies);
-    deepCopyField(expectations_expectation_trainMemory, copies);
-    deepCopyField(expectations_cross_quadratic_mean_trainMemory, copies);
-/*
-    deepCopyField(expectations_expectation_testMemory, copies);
-    deepCopyField(expectations_cross_quadratic_mean_testMemory, copies);
-*/
+    deepCopyField(inputs_histo, copies);
+    
+    deepCopyField(inputs_expectation, copies);
+    deepCopyField(inputs_stds, copies);
+    
+    deepCopyField(inputs_correlations, copies);
+    deepCopyField(inputs_cross_quadratic_mean, copies);
+    
+    deepCopyField(inputs_expectation_trainMemory, copies);
+    deepCopyField(inputs_cross_quadratic_mean_trainMemory, copies);
+    
     deepCopyField(ports, copies);
 }
 
@@ -200,283 +248,211 @@
 
 void LayerCostModule::fprop(const TVec<Mat*>& ports_value)
 {
+    PLASSERT( input_size > 1 );
 
-    Mat* expectations = ports_value[getPortIndex("expectations")];
-    Mat* cost = ports_value[getPortIndex("cost")];
+    Mat* p_inputs = ports_value[getPortIndex("input")];
+    Mat* p_costs = ports_value[getPortIndex("cost")];
 
     PLASSERT( ports_value.length() == nPorts() );
 
-    if ( cost && cost->isEmpty() )
+    if ( p_costs && p_costs->isEmpty() )
     {
-        PLASSERT( expectations && !expectations->isEmpty() );
-	cout << "1 regular fprop!!!" << endl;
-        fprop(*expectations, *cost);
+        PLASSERT( p_inputs && !p_inputs->isEmpty() );
+        cout << "fprop" << endl;
+        fprop(*p_inputs, *p_costs);
     }
 }
 
-void LayerCostModule::fprop(const Mat& expectations, Mat& costs)
+void LayerCostModule::fprop(const Mat& inputs, Mat& costs)
 {
-    int batch_size = expectations.length();
-    costs.resize( batch_size, output_size );
+    int n_samples = inputs.length();
+    costs.resize( n_samples, output_size );
     
     if( !is_cost_function_stochastic )
     {
-        costs.clear();
+        costs.clear(); // costs(i,0) = 0
 
         if( cost_function == "kl_div" )
         {
         //! ************************************************************
         //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
-        //! between probabilities of expectations vectors for all units
+        //! between probabilities of outputs vectors for all units
         //! ************************************************************
         //! 
-        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!      cost = - MEAN_{i,j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
         //!
-        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //!           = - MEAN_{i,j#i} SUM_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
         //!
-        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
-        //!        Px(.): empirical probability (given data x, we sample the q's)
-        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
-        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
+        //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer.
+        //!        Px(.): empirical probability (given data x, we sample the q's).
+        //!        Q: interval in [0,1] = one bin of the histogram of the outputs q's.
+        //!           Q has size HISTO_STEP
+        //!        Nx_i(Q): proportion of q{i} that belong to Q, given data x.
         //!
-        //! Note: one q{i} *entirely* determines one binomial densities of probability
-        //!       ( Bijection {binomial Proba functions} <-> |R )
+        //! Note1: one q{i} *entirely* determines one binomial densities of probability.
+        //!        ( Bijection {binomial Proba functions} <-> |R )
         //!
+        //! Note2: there is a special processing for cases when
+        //!        NO outputs q{i} were observed for a given unit i
+        //!        at a given bin Q of the histograms whereas another q{j}
+        //!        has been observed in Q  (normally, KLdiv -> infinity ).
+        //!        SEE function computeKLdiv().
         //! ************************************************************
 
-            // Filling the histogram (i.e. emperical distribution)
-            // of the expectations
-	    computeHisto(expectations);
-	    
+            computeHisto(inputs);
+            
+            costs(0,0) = computeKLdiv();
+        }
+        else if( cost_function == "kl_div_simple" )
+        {
+        //! ************************************************************
+        //! same as above with a very simple version of the KL-div:
+        //! when computing the histogram of the outputs for all units.
+        //! we add one count per histogram's bin so as to avoid
+        //! numerical problems with zeros.
+        //!
+        //! SEE function computeSafeHisto(real ).
+        //! ************************************************************
+
+            computeSafeHisto(inputs);
+            
             // Computing the KL divergence
             for (int i = 0; i < input_size; i++)
                 for (int j = 0; j < i; j++)
-		{
-		    // These variables are used in case one bin of 
-		    // the histogram is empty for one unit
-		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
-		    // In such case, we ''differ'' the count for the next bin and so on.
-                    real differ_count_i = 0.0;
-                    real differ_count_j = 0.0;
-		    for (int k = 0; k < histo_size; k++)
-		    {
-                        real Ni_k = expectations_histo(i,k) + differ_count_i;
-			real Nj_k = expectations_histo(j,k) + differ_count_j;
-			if( fast_exact_is_equal(Ni_k, 0.0) )
-			{
-                         // differ_count_j += expectations_histo(j,k);
-                            differ_count_j = Nj_k;
-			    continue;
-			}
-                        else if( fast_exact_is_equal(Nj_k, 0.0) )
-			{
-                            differ_count_i = Ni_k;
-			    continue;
-			}
-                        else
-			{
-			    costs(0,0) += KLdivTerm(Ni_k,Nj_k);
-                            differ_count_i = 0.0;
-			    differ_count_j = 0.0;
-			}
-                    }
-		    if( differ_count_i > 0.0 )
-		        "cas ou on regroupe avec le dernier";
-		    else if ( differ_count_j > 0.0 )
-		        "cas ou on regroupe avec le dernier";		    
-                }
+                    for (int k = 0; k < histo_size; k++)
+                        costs(0,0) += KLdivTerm( inputs_histo(i,k), inputs_histo(j,k));
+
             // Normalization w.r.t. number of units
-            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+            costs(0,0) *= norm_factor;
         }
-        else if( cost_function == "kl_div_2" )
+        else if( cost_function == "pascal" )
         {
         //! ************************************************************
-        //! (non stochastic) SYMETRIC *** K-L DIVERGENCE ***
-        //! between probabilities of expectations vectors for all units
+        //! a Pascal Vincent's god-given similarity measure
+        //! between outputs vectors for all units
         //! ************************************************************
         //! 
-        //!      cost = - \sum_{i} \sum_{j#i} Div_{KL}[ Px(q{i}) | Px(q{j}) ]
+        //!      cost = MEAN_{i,j#i} f( Ex[q{i}.q{j}] ) - alpha. MEAN_{i} f( Ex[q{i}] )
         //!
-        //!           = - \sum_{i} \sum_{j#i} \sum_Q (Nx_j(Q) - Nx_j(Q)) log( Nx_i(Q) / Nx_j(Q) )
+        //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer
+        //!        Ex(.): empirical expectation (given data x)
         //!
-        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
-        //!        Px(.): empirical probability (given data x, we sample the q's)
-        //!        Q: interval in [0,1] = one bin of the histogram of the expectations q's
-        //!        Nx_i(Q): proportion of q{i} that belong to Q, with input data x
-        //!
-        //! Note: one q{i} *entirely* determines one binomial densities of probability
-        //!       ( Bijection {binomial Proba functions} <-> |R )
-        //!
         //! ************************************************************
 
-            // Filling the histogram (i.e. emperical distribution)
-            // of the expectations
-	    computeHisto(expectations);
-	    
-            // Computing the KL divergence
+            computePascalStatistics(inputs);
+                                    
+            // Computing the cost
             for (int i = 0; i < input_size; i++)
+            {
+                if (alpha > 0.0 )
+                    costs(0,0) -= alpha * func_pascal(inputs_expectation[i]) *(real)(input_size-1);
                 for (int j = 0; j < i; j++)
-		{
-		    // These variables are used in case one bin of 
-		    // the histogram is empty for one unit
-		    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
-		    // In such case, we ''differ'' the count for the next bin and so on.
-                    real differ_count_i = 0.0;
-                    real differ_count_j = 0.0;
-		    int n_differ = 0;
-		    for (int k = 0; k < histo_size; k++)
-		    {
-                        real Ni_k = expectations_histo(i,k) + differ_count_i;
-			real Nj_k = expectations_histo(j,k) + differ_count_j;
-			if( fast_exact_is_equal(Ni_k, 0.0) )
-			{
-                         // differ_count_j += expectations_histo(j,k);
-                            differ_count_j = Nj_k;
-			    n_differ += 1;
-			}
-                        else if( fast_exact_is_equal(Nj_k, 0.0) )
-			{
-                            differ_count_i = Ni_k;
-			    n_differ += 1;
-			}
-                        else
-			{
-			    costs(0,0) += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ)/(real)histo_size;
-                            differ_count_i = 0.0;
-			    differ_count_j = 0.0;
-			}
-                    }
-		    if( differ_count_i > 0.0 )
-		        "cas ou on regroupe avec le dernier";
-		    else if ( differ_count_j > 0.0 )
-		        "cas ou on regroupe avec le dernier";		    
-                }
-            // Normalization w.r.t. number of units
-            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
+                    costs(0,0) += func_pascal(inputs_cross_quadratic_mean(i,j));
+            }
+
+            costs(0,0) *= norm_factor;
         }
-        else if( cost_function == "kl_div_simple" )
+        else if( cost_function == "correlation" )
         {
-            // Filling the histogram (i.e. emperical distribution)
-            // of the expectations
-	    computeSafeHisto(expectations);
-	    
-            // Computing the KL divergence
-            for (int i = 0; i < input_size; i++)
-                for (int j = 0; j < i; j++)
-		    for (int k = 0; k < histo_size; k++)
-		    {
-                        real Ni_k = expectations_histo(i,k);
-			real Nj_k = expectations_histo(j,k);
-			costs(0,0) += KLdivTerm(Ni_k,Nj_k);
-                    }
-            // Normalization w.r.t. number of units
-            costs(0,0) /= ((real)input_size *(real)(input_size-1)); // / (real)batch_size;
-        }
-        else if( cost_function == "pascal" )
-        {
         //! ************************************************************
-        //! a god-given similarity measure
-        //! between expectations vectors for all units
+        //! a correlation measure
+        //! between outputs for all units
         //! ************************************************************
-        //! 
-        //!      cost = \sum_{i} \sum_{j#i} exp( Ex[q{i}.q{j}] ) - alpha. \sum_{i} exp( Ex[q{i}] )
         //!
-        //! where  q{i} = P(h{i}=1|x): expectation of the i^th units of the layer
-        //!        Ex(.): empirical esperance (given data x, we sample the q's)
+        //!                            ( Ex[q{i}.q{j}] - Ex[q{i}]Ex[q{j}] )
+        //!      cost = MEAN_{i,j#i} f(  -------------------------------- )
+        //!                           (      StDx(q{i}) * StDx(q{j})     )
         //!
+        //! where  q{i} = P(h{i}=1|x): output of the i^th units of the layer
+        //!        Ex(.): empirical esperance (given data x)
+        //!        StDx(.): empirical standard deviation (given data x)
+        //!
         //! ************************************************************
 
-            // Computing statistics on expectations
-	    computePascalStatistics(expectations, false);
-	    
-	    cout << "1 fprop" << endl;
-	    	    
+            computeCorrelationStatistics(inputs);
+                        
             // Computing the cost
             for (int i = 0; i < input_size; i++)
-	    {
-	        if (alpha > 0.0 )
-		    costs(0,0) -= alpha*exp(expectations_expectation[i]);
                 for (int j = 0; j < i; j++)
-                    costs(0,0) += exp(expectations_cross_quadratic_mean(i,j)) / (real)(input_size-1);
-            }
+                    costs(0,0) += func_correlation( inputs_correlations(i,j) );
 
-            // Normalization w.r.t. number of units
-            costs(0,0) /= (real)input_size;
+            costs(0,0) *= norm_factor;
         }
-	
+
+        
         return; // Do not fprop with the conventional stochastic fprop...
     }
     
-    for (int isample = 0; isample < batch_size; isample++)
-        fprop(expectations(isample), costs(isample,0));
+    for (int isample = 0; isample < n_samples; isample++)
+        fprop(inputs(isample), costs(isample,0));
 }
 
-void LayerCostModule::fprop(const Vec& expectation, real& cost) const
+void LayerCostModule::fprop(const Vec& input, real& cost) const
 {
-    PLASSERT( expectation.size() == input_size );
+    PLASSERT( input.size() == input_size );
     PLASSERT( is_cost_function_stochastic );
 
     cost = 0.0;
-    real  qi, qj, comp_qi, comp_qj; // The expectations qi=p(h_i=1)
-                                      //     and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
-				      
+    real  qi, qj, comp_qi, comp_qj; // The outputs (units i,j)
+                                    // and some basic operations on it (e.g.: 1-qi, qi/(1-qi)) 
+                                      
     if( cost_function == "stochastic_cross_entropy" )
     {
     //! ************************************************************
     //! average *** CROSS ENTROPY ***
-    //! between pairs of units (given expectations = sigmoid(act) )
+    //! between pairs of units (given output = sigmoid(act) )
     //! ************************************************************
     //!
-    //!      cost = - \sum_{i} \sum_{j#i} CrossEntropy[( P(h_{i}|x) | P(h_{j}|x) )]
+    //!      cost = - MEAN_{i,j#i} CrossEntropy[( P(h_{i}|x) | P(h_{j}|x) )]
     //!
-    //!           = - \sum_{i} \sum_{j#i} [ q{i}.log(q{j}) + (1-q{i}).log(1-q{j}) ]
+    //!           = - MEAN_{i,j#i} [ q{i}.log(q{j}) + (1-q{i}).log(1-q{j}) ]
     //!
     //! where |  h_{i}: i^th units of the layer
     //!       \  P(.|x): output for input data x
-    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!        \ q{i}=P(h{i}=1|v): output of the i^th units of the layer
     //!
     //! ************************************************************
 
         for( int i = 0; i < input_size; i++ )
         {
-           qi = expectation[i];
+           qi = input[i];
            comp_qi = 1.0 - qi;
            for( int j = 0; j < i; j++ )
            {
-               qj = expectation[j];
+               qj = input[j];
                comp_qj = 1.0 - qj;
-	       
+               
                // H(pi||pj) = H(pi) + D_{KL}(pi||pj)
                cost += qi*safeflog(qj) + comp_qi*safeflog(comp_qj);
-	       
+               
                // The symetric part (loop  j=i+1...size)
                cost += qj*safeflog(qi) + comp_qj*safeflog(comp_qi);
            }
         }
         // Normalization w.r.t. number of units
-        cost /= ((real)input_size *(real)(input_size-1));
+        cost *= norm_factor;
     }
     
     else if( cost_function == "stochastic_kl_div" )
     {
     //! ************************************************************
     //! average SYMETRIC *** K-L DIVERGENCE ***
-    //! between pairs of units (given expectations = sigmoid(act) )
+    //! between pairs of units (given outputs = sigmoid(act) )
     //! ************************************************************
     //!
-    //!      cost = - \sum_{i} \sum_{j#i} Div_{KL} [( P(h_{i}|v) | P(h_{j}|v) )]
+    //!      cost = - MEAN_{i,j#i} Div_{KL} [( P(h_{i}|v) | P(h_{j}|v) )]
     //!
-    //!           = - \sum_{i} \sum_{j#i} [ ( q{j} - q{i} ) log( q{i}/(1-q{i})*(1-q{j})/q{j} ) ]
+    //!           = - MEAN_{i,j#i} [ ( q{j} - q{i} ) log( q{i}/(1-q{i})*(1-q{j})/q{j} ) ]
     //!
     //! where |  h_{i}: i^th units of the layer
     //!       \  P(.|v):  output for input data x
-    //!        \ q{i}=P(h{i}=1|v): expectation of the i^th units of the layer
+    //!        \ q{i}=P(h{i}=1|v): output of the i^th units of the layer
     //!
     //! ************************************************************
 
         for( int i = 0; i < input_size; i++ )
         {
-           qi = expectation[i];
+           qi = input[i];
            if(fast_exact_is_equal(qi, 1.0))
                comp_qi = REAL_MAX;
            else
@@ -484,25 +460,26 @@
        
            for( int j = 0; j < i; j++ )
            {
-               qj = expectation[j];
+               qj = input[j];
                if(fast_exact_is_equal(qj, 1.0))
                    comp_qj = REAL_MAX;
                else
                    comp_qj = qj/(1.0 - qj);
-	       
+               
                //     - D_{KL}(pi||pj) - D_{KL}(pj||pi)
                cost += (qj-qi)*safeflog(comp_qi/comp_qj);
            }
         }
         // Normalization w.r.t. number of units
-        cost /= ((real)input_size *(real)(input_size-1));   
+        cost *= norm_factor;   
     }
 
     else
-        PLERROR("LayerCostModule::fprop() not implemented for cost function %s\n"
-	        "- It may be a printing error\n"
-		"- You can try to call LayerCostModule::fprop(const Mat& expectations, Mat& costs)\n"
-		"- Or else write the code corresponding to your cost function",
+        PLERROR("LayerCostModule::fprop() not implemented for cost_cfunction %s\n"
+                "- It may be a printing error.\n"
+                "- You can try to call LayerCostModule::fprop(const Mat& inputs, Mat& costs)"
+                "  if your cost function is non stochastic.\n"
+                "- Or else write the code corresponding to your cost function.\n",
                  cost_function.c_str());
 }
 
@@ -517,74 +494,75 @@
 void LayerCostModule::bpropAccUpdate(const TVec<Mat*>& ports_value,
                                    const TVec<Mat*>& ports_gradient)
 {
+    PLASSERT( input_size > 1 );
     PLASSERT( ports_value.length() == nPorts() );
     PLASSERT( ports_gradient.length() == nPorts() );
 
-    const Mat* expectations = ports_value[getPortIndex("expectations")];
-    Mat* expectations_grad = ports_gradient[getPortIndex("expectations")];
-    Mat* cost = ports_value[getPortIndex("cost")];
-    Mat* cost_grad = ports_gradient[getPortIndex("cost")];
+    cout << "bpropAccUpdate" << endl;
 
-    if( expectations_grad && expectations_grad->isEmpty()
-        && cost_grad && !cost_grad->isEmpty() )
+    const Mat* p_inputs = ports_value[getPortIndex("input")];
+    Mat* p_inputs_grad = ports_gradient[getPortIndex("input")];
+    Mat* p_cost_grad = ports_gradient[getPortIndex("cost")];
+
+    if( p_inputs_grad && p_inputs_grad->isEmpty()
+        && p_cost_grad && !p_cost_grad->isEmpty() )
     {
-        int batch_size = expectations->length();
+        int n_samples = p_inputs->length();
 
-        PLASSERT( expectations && !expectations->isEmpty());
-        PLASSERT( expectations->length() == batch_size );
-        PLASSERT( cost_grad->length() == batch_size );
+        PLASSERT( p_inputs && !p_inputs->isEmpty());
+        PLASSERT( p_inputs->length() == n_samples );
+        PLASSERT( p_cost_grad->length() == n_samples );
 
-        expectations_grad->resize(batch_size, input_size);
-	expectations_grad->clear();
+        p_inputs_grad->resize(n_samples, input_size);
+        p_inputs_grad->clear();
 
         real qi, qj, comp_qi, comp_qj;
         Vec comp_q(input_size), log_term(input_size);
 
         if( cost_function == "stochastic_cross_entropy" )
         {
-            for (int isample = 0; isample < batch_size; isample++)
+            for (int isample = 0; isample < n_samples; isample++)
             {
-        	for (int i = 0 ; i < input_size ; i++ )
+                for (int i = 0 ; i < input_size ; i++ )
                 {
-                    qi = (*expectations)(isample,i);
-            	    comp_qi = 1.0 - qi;
+                    qi = (*p_inputs)(isample,i);
+                        comp_qi = 1.0 - qi;
                     comp_q[i] = comp_qi;
                     log_term[i] = safeflog(qi) - safeflog(comp_qi);
                 }
                 for (int i = 0; i < input_size; i++ )
                 {
-                    qi = (*expectations)(isample,i);
+                    qi = (*p_inputs)(isample,i);
                     comp_qi = comp_q[i];
-                    (*expectations_grad)(isample,i) = 0.0;
+                    (*p_inputs_grad)(isample,i) = 0.0;
                     for (int j = 0; j < i; j++ )
                     {
-                        qj = (*expectations)(isample,j);
+                        qj = (*p_inputs)(isample,j);
                         comp_qj=comp_q[j];
 
                         // log(pj) - log(1-pj) + pj/pi - (1-pj)/(1-pi)
-                        (*expectations_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
+                        (*p_inputs_grad)(isample,i) += log_term[j] + qj/qi - comp_qi/comp_qj;
 
                         // The symetric part (loop  j=i+1...input_size)
-                        (*expectations_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
+                        (*p_inputs_grad)(isample,j) += log_term[i] + qi/qj - comp_qj/comp_qi;
                     }
                 }
-                // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i < input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0)
+		                                    * norm_factor /(real)n_samples;
                 }
             }
         }
 
         else if( cost_function == "stochastic_kl_div" )
         {
-            for (int isample = 0; isample < batch_size; isample++)
+            for (int isample = 0; isample < n_samples; isample++)
             {
-        	for (int i = 0; i < input_size; i++ )
+                for (int i = 0; i < input_size; i++ )
                 {
-                    qi = (*expectations)(isample,i);
-            	    comp_qi = 1.0 - qi;
+                    qi = (*p_inputs)(isample,i);
+                        comp_qi = 1.0 - qi;
                     if(fast_exact_is_equal(qi, 1.0) || fast_exact_is_equal(qi, 0.0))
                         comp_q[i] = REAL_MAX;
                     else
@@ -593,263 +571,273 @@
                 }
                 for (int i = 0; i < input_size; i++ )
                 {
-                    qi = (*expectations)(isample,i);
+                    qi = (*p_inputs)(isample,i);
                     comp_qi = comp_q[i];
 
-                    (*expectations_grad)(isample,i) = 0.0;
+                    (*p_inputs_grad)(isample,i) = 0.0;
                     for (int j = 0; j < i ; j++ )
                     {
-                        qj = (*expectations)(isample,j);
+                        qj = (*p_inputs)(isample,j);
                         comp_qj=comp_q[j];
 
                         //   [qj - qi]/[qi (1-qi)] - log[ qi/(1-qi) * (1-qj)/qj]
-                        (*expectations_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
+                        (*p_inputs_grad)(isample,i) += (qj - qi)*comp_qi - log_term[i] + log_term[j];
 
                         // The symetric part (loop  j=i+1...input_size)
-                        (*expectations_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
+                        (*p_inputs_grad)(isample,j) += (qi - qj)*comp_qj - log_term[j] + log_term[i];
                     }
                 }
-                // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i < input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0)
+		                                     * norm_factor /(real)n_samples;
                 }
             }
         }
 
-
         else if( cost_function == "kl_div" )
         {
-	    computeHisto(*expectations);
-	    real one_count = 1. / (real)batch_size;
-	    
-            for (int isample = 0; isample < batch_size; isample++)
+            computeHisto(*p_inputs);
+            
+            for (int isample = 0; isample < n_samples; isample++)
             {
 
                 // Computing the difference of KL divergence
                 // for d_q
                 for (int i = 0; i < input_size; i++)
-		{
-                    (*expectations_grad)(isample, i) = 0.0;
-		    
-		    qi = (*expectations)(isample,i);
-		    int index_i = histo_index(qi);
-		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-		        continue;
-		    real over_dqi=1.0/dq(qi);
-		    int shift_i;
-		    if( over_dqi > 0.0)
-		        shift_i = 1;
-		    else
-		        shift_i = -1;
-		    // qi + dq(qi) ==> | expectations_histo(i,index_i)   - one_count
-		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
-		    
-		    for (int j = 0; j < i; j++)
-		    {
-			(*expectations_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi, one_count);
-			
-                        qj = (*expectations)(isample,j);
-			int index_j = histo_index(qj);
-		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-		            continue;
-			real over_dqj=1.0/dq(qj);
- 		        int shift_j;
-		        if( over_dqj > 0.0)
-		            shift_j = 1;
-		        else
-		            shift_j = -1;
-            	        // qj + dq(qj) ==> | expectations_histo(j,index_j)   - one_count
-  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
-			
-			(*expectations_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj, one_count);
+                {
+                    (*p_inputs_grad)(isample, i) = 0.0;
+                    
+                    qi = (*p_inputs)(isample,i);
+                    int index_i = histo_index(qi);
+                    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                        continue;
+                    real over_dqi=1.0/dq(qi);
+                    int shift_i;
+                    if( over_dqi > 0.0)
+                        shift_i = 1;
+                    else
+                        shift_i = -1;
+                    // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
+                    //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
+                    
+                    for (int j = 0; j < i; j++)
+                    {
+                        (*p_inputs_grad)(isample, i) += delta_KLdivTerm(i, j, index_i, over_dqi);
+                        
+                        qj = (*p_inputs)(isample,j);
+                        int index_j = histo_index(qj);
+                        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+                            continue;
+                        real over_dqj=1.0/dq(qj);
+                         int shift_j;
+                        if( over_dqj > 0.0)
+                            shift_j = 1;
+                        else
+                            shift_j = -1;
+                            // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
+                          //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
+                        
+                        (*p_inputs_grad)(isample, j) += delta_KLdivTerm(j, i, index_j, over_dqj);
                     }
-		}
+                }
 
                 // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i < input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
                 }
             }
-        }
-
-        else if( cost_function == "kl_div_2" )
-        {
-	    computeHisto(*expectations);
-	    real one_count = 1. / (real)batch_size;
-	    
-            for (int isample = 0; isample < batch_size; isample++)
+            
+            
+            // debug Check
+            int i=0;
+            real cost_before = computeKLdiv();
+            for (int isample = 0; isample < n_samples; isample++)
             {
-
-                // Computing the difference of KL divergence
-                // for d_q
-                for (int i = 0; i < input_size; i++)
-		{
-                    (*expectations_grad)(isample, i) = 0.0;
-		    
-		    qi = (*expectations)(isample,i);
-		    int index_i = histo_index(qi);
-		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-		        continue;
-		    real over_dqi=1.0/dq(qi);
-		    int shift_i;
-		    if( over_dqi > 0.0)
-		        shift_i = 1;
-		    else
-		        shift_i = -1;
-		    // qi + dq(qi) ==> | expectations_histo(i,index_i)   - one_count
-		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
-		    
-		    for (int j = 0; j < i; j++)
-		    {
-			(*expectations_grad)(isample, i) += delta_KLdivTerm_2(i, j, index_i, over_dqi, one_count);
-			
-                        qj = (*expectations)(isample,j);
-			int index_j = histo_index(qj);
-		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-		            continue;
-			real over_dqj=1.0/dq(qj);
- 		        int shift_j;
-		        if( over_dqj > 0.0)
-		            shift_j = 1;
-		        else
-		            shift_j = -1;
-            	        // qj + dq(qj) ==> | expectations_histo(j,index_j)   - one_count
-  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
-			
-			(*expectations_grad)(isample, j) += delta_KLdivTerm_2(j, i, index_j, over_dqj, one_count);
-                    }
-		}
-
-                // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
-                for (int i = 0; i < input_size; i++ )
+                real qi=(*p_inputs)(isample,i);
+                if( histo_index(qi) < histo_size-1 )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                  (*p_inputs)(isample,i) += dq(qi);
+                  computeHisto(*p_inputs);
+                  real cost_after = computeKLdiv();
+                  (*p_inputs)(isample,i) -= dq(qi);                  
+                  cout << "\tglobal cost comparison:" << cost_after - cost_before;
+                  cout << "  <?>  " << (*p_inputs_grad)(isample, i)*dq(qi) << endl;
                 }
             }
+            
+            
         }
 
         else if( cost_function == "kl_div_simple" )
         {
-	    computeSafeHisto(*expectations);
-	    real one_count = 1. / (real)(batch_size+histo_size);
-	    
-            for (int isample = 0; isample < batch_size; isample++)
+            computeSafeHisto(*p_inputs);
+            
+            for (int isample = 0; isample < n_samples; isample++)
             {
 
                 // Computing the difference of KL divergence
                 // for d_q
                 for (int i = 0; i < input_size; i++)
-		{
-                    (*expectations_grad)(isample, i) = 0.0;
-		    
-		    qi = (*expectations)(isample,i);
-		    int index_i = histo_index(qi);
-		    if( ( index_i == histo_size-1 ) ) // we do not care about this...
-		        continue;
-		    real over_dqi=1.0/dq(qi);
-		    int shift_i;
-		    if( over_dqi > 0.0)
-		        shift_i = 1;
-		    else
-		        shift_i = -1;
-		    // qi + dq(qi) ==> | expectations_histo(i,index_i)   - one_count
-		    //                 \ expectations_histo(i,index_i+shift_i) + one_count
-		    
-		    for (int j = 0; j < i; j++)
-		    {
-			(*expectations_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi, one_count);
-			
-                        qj = (*expectations)(isample,j);
-			int index_j = histo_index(qj);
-		        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
-		            continue;
-			real over_dqj=1.0/dq(qj);
- 		        int shift_j;
-		        if( over_dqj > 0.0)
-		            shift_j = 1;
-		        else
-		            shift_j = -1;
-            	        // qj + dq(qj) ==> | expectations_histo(j,index_j)   - one_count
-  		        //                 \ expectations_histo(j,index_j+shift_j) + one_count
-			
-			(*expectations_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj, one_count);
+                {
+                    (*p_inputs_grad)(isample, i) = 0.0;
+                    
+                    qi = (*p_inputs)(isample,i);
+                    int index_i = histo_index(qi);
+                    if( ( index_i == histo_size-1 ) ) // we do not care about this...
+                        continue;
+                    real over_dqi=1.0/dq(qi);
+                    int shift_i;
+                    if( over_dqi > 0.0)
+                        shift_i = 1;
+                    else
+                        shift_i = -1;
+                    // qi + dq(qi) ==> | p_inputs_histo(i,index_i)   - one_count
+                    //                 \ p_inputs_histo(i,index_i+shift_i) + one_count
+                    
+                    for (int j = 0; j < i; j++)
+                    {
+                        (*p_inputs_grad)(isample, i) += delta_SafeKLdivTerm(i, j, index_i, over_dqi);
+                        
+                        qj = (*p_inputs)(isample,j);
+                        int index_j = histo_index(qj);
+                        if( ( index_j == histo_size-1 ) || ( index_j == 0 ) )
+                            continue;
+                        real over_dqj=1.0/dq(qj);
+                         int shift_j;
+                        if( over_dqj > 0.0)
+                            shift_j = 1;
+                        else
+                            shift_j = -1;
+                            // qj + dq(qj) ==> | p_inputs_histo(j,index_j)   - one_count
+                          //                 \ p_inputs_histo(j,index_j+shift_j) + one_count
+                        
+                        (*p_inputs_grad)(isample, j) += delta_SafeKLdivTerm(j, i, index_j, over_dqj);
                     }
-		}
+                }
 
                 // Normalization
-                real norm_factor = 1.0 / ((real)input_size *(real)(input_size-1));
                 for (int i = 0; i < input_size; i++ )
                 {
-                    (*expectations_grad)(isample, i) *= (*cost_grad)(isample,0) * norm_factor;
+                    (*p_inputs_grad)(isample, i) *= (*p_cost_grad)(isample,0) * norm_factor;
                 }
             }
         }
 
         else if( cost_function == "pascal" )
         {
-	    computePascalStatistics(*expectations, true);
-	    
-	    cout << "1 BPropAccUpdate" << endl;
-	    
-	    real one_count = 1. / (real)batch_size;
-	    if( momentum > 0.0 )
-	        for (int isample = 0; isample < batch_size; isample++)
-		{
+            computePascalStatistics(*p_inputs);
+            
+            if( momentum > 0.0 )
+                for (int isample = 0; isample < n_samples; isample++)
+                {
                     for (int i = 0; i < input_size; i++)
                     {
-                        qi = (*expectations)(isample, i);
-			if (alpha > 0.0 )
-			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *(1.0-momentum)*one_count;
+                        qi = (*p_inputs)(isample, i);
+                        if (alpha > 0.0 )
+                            (*p_inputs_grad)(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
+			                                         *(1.0-momentum) *one_count
+                                                                 *(real)(input_size-1);
                         for (int j = 0; j < i; j++)
                         {
-			    qj = (*expectations)(isample,j);
-                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*(1.0-momentum)*one_count / (real)(input_size-1);
-                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*(1.0-momentum)*one_count / (real)(input_size-1);
+                            real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
+                            qj = (*p_inputs)(isample,j);
+                            (*p_inputs_grad)(isample, i) += d_temp *qj*(1.0-momentum)*one_count;
+                            (*p_inputs_grad)(isample, j) += d_temp *qi*(1.0-momentum)*one_count;
                         }
                     }
                     for (int i = 0; i < input_size; i++)
                     {
-	                (*expectations_grad)(isample, i) /= (real)input_size;
-	            }
-		}
-	    else
-	        for (int isample = 0; isample < batch_size; isample++)
-		{
+                        (*p_inputs_grad)(isample, i) *= norm_factor;
+                    }
+                }
+            else
+                for (int isample = 0; isample < n_samples; isample++)
+                {
                     for (int i = 0; i < input_size; i++)
                     {
-                        qi = (*expectations)(isample, i);
-			if (alpha > 0.0 )
-			    (*expectations_grad)(isample, i) -= alpha*exp(expectations_expectation[i]) *one_count;
+                        qi = (*p_inputs)(isample, i);
+                        if (alpha > 0.0 )
+                            (*p_inputs_grad)(isample, i) -= alpha*deriv_func_pascal(inputs_expectation[i])
+			                                         *one_count
+                                                                 *(real)(input_size-1);
                         for (int j = 0; j < i; j++)
                         {
-			    qj = (*expectations)(isample,j);
-                            (*expectations_grad)(isample, i) += exp(expectations_cross_quadratic_mean(i,j)) *qj*one_count / (real)(input_size-1);
-                            (*expectations_grad)(isample, j) += exp(expectations_cross_quadratic_mean(i,j)) *qi*one_count / (real)(input_size-1);
+                            real d_temp = deriv_func_pascal(inputs_cross_quadratic_mean(i,j));
+                            qj = (*p_inputs)(isample,j);
+                            (*p_inputs_grad)(isample, i) += d_temp *qj *one_count;
+                            (*p_inputs_grad)(isample, j) += d_temp *qi *one_count;
                         }
                     }
                     for (int i = 0; i < input_size; i++)
                     {
-	                (*expectations_grad)(isample, i) /= (real)input_size;
-	            }
-		}
+                        (*p_inputs_grad)(isample, i) *= norm_factor;
+                    }
+                }
         }
-	
+
+        else if( cost_function == "correlation")
+        {
+            computeCorrelationStatistics(*p_inputs);
+            
+            if( momentum > 0.0 )
+                PLERROR( "not implemented yet");
+            else
+                for (int isample = 0; isample < n_samples; isample++)
+                {
+                    Vec dSTDi_dqi, dCROSSij_dqj;
+                    dSTDi_dqi.resize( input_size );
+                    dCROSSij_dqj.resize( input_size );
+                    
+                    for (int i = 0; i < input_size; i++)
+                    {
+                        qi = (*p_inputs)(isample, i);
+
+                        //!  dCROSSij_dqj[i] = d[ E(QiQj)-E(Qi)E(Qj) ]/d[qj(t)]
+                        //!                  = ( qi(t) - E(Qi) ) / n_samples 
+                        //!
+                        //!  dSTDi_dqi[i] = d[ STD(Qi) ]/d[qi(t)]
+                        //!               = d[ sqrt( E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
+                        //!               = 1 / [ 2.STD(Qi) ] * d[ E(Qi^2) -E(Qi)^2 ]/d[qi(t)]
+                        //!               = 1 / [ 2.STD(Qi) ] * [ 2*qi(t) / n_samples - 2*E(Qi) / n_samples ]
+                        //!               = ( qi(t) - E(Qi) ) / ( n_samples * STD(Qi) )
+                        //!               = dCROSSij_dqj[i] / STD(Qi)
+                        //!
+                        dCROSSij_dqj[i] = ( qi - inputs_expectation[i] )*one_count;
+                        dSTDi_dqi[i] = dCROSSij_dqj[i] / inputs_stds[i];
+                        
+                        for (int j = 0; j < i; j++)
+                        {
+                            qj = (*p_inputs)(isample,j);
+
+                            real correlation_denum = inputs_stds[i]*inputs_stds[j];
+                            real dfunc_dCorr = deriv_func_correlation( inputs_correlations(i,j) );
+                            real correlation_num = ( inputs_cross_quadratic_mean(i,j)
+                                                     - inputs_expectation[i]*inputs_expectation[j] );
+                                  
+                            (*p_inputs_grad)(isample, i) += dfunc_dCorr * ( 
+                                                                    correlation_denum * dCROSSij_dqj[j]
+                                                                  - correlation_num * dSTDi_dqi[i] * inputs_stds[j]
+                                                                    ) / (correlation_denum * correlation_denum);
+
+                            (*p_inputs_grad)(isample, j) += dfunc_dCorr * ( 
+                                                                    correlation_denum * dCROSSij_dqj[i]
+                                                                  - correlation_num * dSTDi_dqi[j] * inputs_stds[i]
+                                                                    ) / (correlation_denum * correlation_denum);
+                        }
+                    }
+                    for (int i = 0; i < input_size; i++)
+                        (*p_inputs_grad)(isample, i) *= norm_factor;
+                }
+        }
         else
             PLERROR("LayerCostModule::bpropAccUpdate() not implemented for cost function %s",
                      cost_function.c_str());
 
-/*
-        ntest = 0;
-*/
-
         checkProp(ports_gradient);
     }
-    else if( !expectations_grad && !cost_grad )
+    else if( !p_inputs_grad && !p_cost_grad )
         return;
     else
         PLERROR("In LayerCostModule::bpropAccUpdate - Port configuration not implemented ");
@@ -860,189 +848,150 @@
 ////////////////////////////////////////////////////
 // Auxiliary Functions for Pascal's cost function //
 ////////////////////////////////////////////////////
-void LayerCostModule::computePascalStatistics(const Mat& expectations, bool duringTraining)
+void LayerCostModule::computePascalStatistics(const Mat& inputs)
 {
-    int batch_size = expectations.length();
-    real one_count = 1. / (real)batch_size;
-    Vec expectation;
+    int n_samples = inputs.length();
+    one_count = 1. / (real)n_samples;
+    Vec input;
     
-    expectations_expectation.clear(); 
-    expectations_cross_quadratic_mean.clear(); 
+    inputs_expectation.clear(); 
+    inputs_cross_quadratic_mean.clear(); 
 
-    for (int isample = 0; isample < batch_size; isample++)
+    for (int isample = 0; isample < n_samples; isample++)
     {
-        expectation = expectations(isample);
+        input = inputs(isample);
         for (int i = 0; i < input_size; i++)
-	{
-	    expectations_expectation[i] += expectation[i];
-	    for (int j = 0; j < i; j++)
-                 expectations_cross_quadratic_mean(i,j) += expectation[i] * expectation[j];
+        {
+            inputs_expectation[i] += input[i];
+            for (int j = 0; j < i; j++)
+                 inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
         }
     }
-    expectations_cross_quadratic_mean *= one_count;
     
     for (int i = 0; i < input_size; i++)
     {
-        expectations_expectation[i] *= one_count;
+        inputs_expectation[i] *= one_count;
         for (int j = 0; j < i; j++)
         {
-             expectations_cross_quadratic_mean(i,j) *= one_count;
-//    	     expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
+             inputs_cross_quadratic_mean(i,j) *= one_count;
         }
     }
-    if( ( momentum > 0.0 ) && duringTraining )
+    if( ( momentum > 0.0 ) && during_training )
     {
         for (int i = 0; i < input_size; i++)
         {
-	    if(i == 0)
-	       cout << ".Check momentum....: expectations_expectation_trainMemory[0] = " << expectations_expectation_trainMemory[0] << endl;
-
-            expectations_expectation[i] = momentum*expectations_expectation_trainMemory[i]
-	                                 +(1.0-momentum)*expectations_expectation[i];
-            expectations_expectation_trainMemory[i] = expectations_expectation[i];
+            inputs_expectation[i] = momentum*inputs_expectation_trainMemory[i]
+                                         +(1.0-momentum)*inputs_expectation[i];
+            inputs_expectation_trainMemory[i] = inputs_expectation[i];
             for (int j = 0; j < i; j++)
             {
-                 expectations_cross_quadratic_mean(i,j) = momentum*expectations_cross_quadratic_mean_trainMemory(i,j)
-		                                       +(1.0-momentum)*expectations_cross_quadratic_mean(i,j);
-//        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
-        	 expectations_cross_quadratic_mean_trainMemory(i,j) = expectations_cross_quadratic_mean(i,j);
-//        	 expectations_cross_quadratic_mean_trainMemory(j,i) = expectations_cross_quadratic_mean(i,j);
+                 inputs_cross_quadratic_mean(i,j) = momentum*inputs_cross_quadratic_mean_trainMemory(i,j)
+                                                       +(1.0-momentum)*inputs_cross_quadratic_mean(i,j);
+                 inputs_cross_quadratic_mean_trainMemory(i,j) = inputs_cross_quadratic_mean(i,j);
             }
         }
     }
-/*    else if( !duringTraining )
-    {
-	PLASSERT( ntest+batch_size > 0 );
-	for (int i = 0; i < input_size; i++)
-        {
-            expectations_expectation[i] = ( (real)ntest*expectations_expectation_testMemory[i]
-	                                   +(real)batch_size*expectations_expectation[i] )/(real)(ntest+batch_size);
-            expectations_expectation_testMemory[i] = expectations_expectation[i];
-            for (int j = 0; j < i; j++)
-            {
-                 expectations_cross_quadratic_mean(i,j) = ( (real)ntest*expectations_cross_quadratic_mean_testMemory(i,j)
-		                                           +(real)batch_size*expectations_cross_quadratic_mean(i,j) );
-        	 expectations_cross_quadratic_mean(j,i) = expectations_cross_quadratic_mean(i,j);
-        	 expectations_cross_quadratic_mean_testMemory(i,j) = expectations_cross_quadratic_mean(i,j);
-        	 expectations_cross_quadratic_mean_testMemory(j,i) = expectations_cross_quadratic_mean(i,j);
-            }
-        }
-	ntest += batch_size;
-    }
-*/
 }
-
-/////////////////////////
-// Auxiliary Functions //
-/////////////////////////
-real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+string LayerCostModule::func_pascal_prefix()
 {
-    PLASSERT( over_dq > 0.0 );
+    string prefix = "exp";
+    return prefix;
+}
+real LayerCostModule::func_pascal(real value)
+{
+    return exp(value);
+}
+real LayerCostModule::deriv_func_pascal(real value)
+{
+    return exp(value);
+}
 
-    real grad_update = 0.0;
 
-    real Ni_ki = expectations_histo(i,index_i);
-    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
-    real Nj_ki        = expectations_histo(j,index_i);
-    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+void LayerCostModule::computeCorrelationStatistics(const Mat& inputs)
+{
+    int n_samples = inputs.length();
+    one_count = 1. / (real)n_samples;
+    Vec input;
+    
+    inputs_expectation.clear();
+    inputs_cross_quadratic_mean.clear(); 
+    inputs_correlations.clear();
 
-    PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if expectations_histo is up to date,
-                                                  // the expectation(isample,i) has been counted
-    real differ_count_j_before = 0.0;
-    real differ_count_j_after = 0.0;
-    real differ_count_i_before = 0.0;
-    real differ_count_i_after = 0.0;
-
-    // What follows is only valuable when the qi's are increased (dq>0).
-
-    if( !fast_exact_is_equal(Nj_ki, 0.0) )
-    // if it is zero, then INCREASING qi will not change anything
-    // (it was already counted in the next histograms's bin
+    for (int isample = 0; isample < n_samples; isample++)
     {
-        // removing the term of the sum that will be modified
-        grad_update -= KLdivTerm( Ni_ki, Nj_ki );
-							       
-        if( fast_exact_is_equal(Ni_ki, one_count) )
-            differ_count_j_after = Nj_ki;
-        else
-            // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
-
-        if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
+        input = inputs(isample);
+        for (int i = 0; i < input_size; i++)
         {
-            // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after );
+            inputs_expectation[i] += input[i];
+            inputs_cross_quadratic_mean(i,i) += input[i] * input[i];
+            for (int j = 0; j < i; j++)
+                 inputs_cross_quadratic_mean(i,j) += input[i] * input[j];
+        }
+    }
+    
+    for (int i = 0; i < input_size; i++)
+    {
+        //! Normalization
+        inputs_expectation[i] *= one_count;
+        inputs_cross_quadratic_mean(i,i) *= one_count;
 
-            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas o? on regroupe avec le dernier";
-            {
-                // removing the term of the sum that will be modified
-                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );		
-            }
-	    else
-	    {
-	        // We search   ki' > k(i)+1   such that   n(i,ki') > 0
-		differ_count_j_before = Nj_ki_shift1;
-		int ki;
-		for (ki = index_i+2; ki < histo_size; ki++)
-		{
-		    differ_count_j_before += expectations_histo( j, ki );
-		    if( expectations_histo( i, ki )>0 )
-		        break;
-		}
-		if( ki < histo_size )
-		{
-                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before );
+            inputs_stds[i] = sqrt( inputs_cross_quadratic_mean(i,i)
+                              - inputs_expectation[i] * inputs_expectation[i] );
 
-		    if( differ_count_j_before > Nj_ki_shift1 )		
-                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 );
-		        // pb avec differ_count_j_after plus haut??? semble pas
-		}
-		else
-		    "cas o? on regroupe avec le dernier";
-	    }
-        }
-        else
+        for (int j = 0; j < i; j++)
         {
-            differ_count_i_before = Ni_ki_shift1;
-            differ_count_i_after  = Ni_ki_shift1+one_count;
-	    int kj;
-	    for( kj = index_i+2; kj < histo_size; kj++)
-	    {
-	        differ_count_i_after += expectations_histo( i, kj );
-		if( differ_count_i_before > 0 )
-		    differ_count_i_before += expectations_histo( i, kj );
-		if( expectations_histo( j, kj ) > 0 )
-		    break;
-	    }
-	    if( kj < histo_size )
-            {
-                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) );
+            //! Normalization
+            inputs_cross_quadratic_mean(i,j) *= one_count;
 
-		if( differ_count_i_before > 0 )
-                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) );
-	    }
-	    else
-		"cas o? on regroupe avec le dernier";   
+            //! Correlations
+            inputs_correlations(i,j) = (
+                                  inputs_cross_quadratic_mean(i,j)
+                                  - inputs_expectation[i]*inputs_expectation[j]
+                                  ) / ( inputs_stds[i] * inputs_stds[j] );
         }
     }
-    return grad_update *over_dq;
+    //! Be careful: 'inputs_correlations' matrix is only computed
+    //!  on the triangle subpart 'i' > 'j' 
+    //!  ('i'/'j': first/second argument)
+
+    if(  during_training )
+    {
+        if( momentum > 0.0 )
+            PLERROR("not implemented yet");
+    }
 }
+string LayerCostModule::func_correlation_prefix()
+{
+    string prefix = "squared";
+    return "square";
+}
+real LayerCostModule::func_correlation(real correlation)
+{
+    return correlation * correlation;
+}
+real LayerCostModule::deriv_func_correlation(real correlation)
+{
+    return 2 * correlation;
+}
+/////////////////////////
+// Auxiliary Functions //
+/////////////////////////
 
-real LayerCostModule::delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count)
+
+real LayerCostModule::delta_KLdivTerm(int i, int j, int index_i, real over_dq)
 {
     PLASSERT( over_dq > 0.0 );
 
     real grad_update = 0.0;
 
-    real Ni_ki = expectations_histo(i,index_i);
-    real Ni_ki_shift1 = expectations_histo(i,index_i+1);	    
-    real Nj_ki        = expectations_histo(j,index_i);
-    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+    real Ni_ki = inputs_histo(i,index_i);
+    real Ni_ki_shift1 = inputs_histo(i,index_i+1);            
+    real Nj_ki        = inputs_histo(j,index_i);
+    real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
     PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if expectations_histo is up to date,
-                                                  // the expectation(isample,i) has been counted
+                                                  // if inputs_histo is up to date,
+                                                  // the input(isample,i) has been counted
     real differ_count_j_before = 0.0;
     real differ_count_j_after = 0.0;
     real differ_count_i_before = 0.0;
@@ -1060,108 +1009,115 @@
     {
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki ) *over_dq;
-							       
+                                                               
         if( fast_exact_is_equal(Ni_ki, one_count) )
-	{
+        {
             differ_count_j_after = Nj_ki;
-	    n_differ_j_after += 1;
-	}
+            n_differ_j_after += 1;
+        }
         else
             // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki ) *over_dq;
+            grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki )
+	                   *over_dq;
 
         if( !fast_exact_is_equal(Nj_ki_shift1,0.0) )
         {
             // adding the term of the sum with its modified value
-            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after ) *(real)(n_differ_j_after+1)*over_dq ;
+            grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1+differ_count_j_after )
+	                  *(real)(1+n_differ_j_after)*over_dq ;
 
-            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas o? on regroupe avec le dernier";
+            if( !fast_exact_is_equal(Ni_ki_shift1, 0.0) ) // "cas ou on regroupe avec le dernier";
             {
                 // removing the term of the sum that will be modified
-                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )*over_dq;		
+                grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 )
+		               *over_dq;                
             }
-	    else
-	    {
-	        // We search   ki' > k(i)+1   such that   n(i,ki') > 0
-		differ_count_j_before = Nj_ki_shift1;
+            else
+            {
+                // We search   ki' > k(i)+1   such that   n(i,ki') > 0
+                differ_count_j_before = Nj_ki_shift1;
                 n_differ_j_before += 1;
-		int ki;
-		for (ki = index_i+2; ki < histo_size; ki++)
-		{
-		    differ_count_j_before += expectations_histo( j, ki );
-		    if( expectations_histo( i, ki )>0 )
-		        break;
+                int ki;
+                for (ki = index_i+2; ki < histo_size; ki++)
+                {
+                    differ_count_j_before += inputs_histo( j, ki );
+                    if( inputs_histo( i, ki )>0 )
+                        break;
                     n_differ_j_before += 1;
-		}
-		if( ki < histo_size )
-		{
-                    grad_update -= KLdivTerm( expectations_histo( i, ki ), differ_count_j_before )*(real)(1+n_differ_j_before)*over_dq;
+                }
+                if( ki < histo_size )
+                {
+                    grad_update -= KLdivTerm( inputs_histo( i, ki ), differ_count_j_before )
+		                   *(real)(1+n_differ_j_before)*over_dq;
 
-		    if( differ_count_j_before > Nj_ki_shift1 )		
-                        grad_update += KLdivTerm( expectations_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )*(real)(n_differ_j_before)*over_dq;
-		        // pb avec differ_count_j_after plus haut??? semble pas
-		}
-		else
-		    "cas o? on regroupe avec le dernier";
-	    }
+                    if( differ_count_j_before > Nj_ki_shift1 )                
+                        grad_update += KLdivTerm( inputs_histo( i, ki ), differ_count_j_before - Nj_ki_shift1 )
+			               *(real)(n_differ_j_before)*over_dq;
+                        // pb avec differ_count_j_after plus haut??? semble pas
+                }
+                else
+                    "cas ou on regroupe avec le dernier (easy)";
+            }
         }
         else
         {
             differ_count_i_before = Ni_ki_shift1;
-	    if( differ_count_i_before>0.0 )
-	       n_differ_i_before += 1;
+            if( differ_count_i_before>0.0 )
+               n_differ_i_before += 1;
             differ_count_i_after  = Ni_ki_shift1+one_count;
-	    n_differ_i_after += 1;
-	    int kj;
-	    for( kj = index_i+2; kj < histo_size; kj++)
-	    {
-	        differ_count_i_after += expectations_histo( i, kj );
-		if( differ_count_i_before > 0 )
-		    differ_count_i_before += expectations_histo( i, kj );
-		if( expectations_histo( j, kj ) > 0 )
-		    break;
-		n_differ_i_after += 1;
-		if( differ_count_i_before > 0 )
-		    n_differ_i_before += 1;
-	    }
-	    if( kj < histo_size )
+            n_differ_i_after += 1;
+            int kj;
+            for( kj = index_i+2; kj < histo_size; kj++)
             {
-                grad_update += KLdivTerm( differ_count_i_after, expectations_histo( j, kj ) ) *(real)(n_differ_i_after+1)*over_dq;
+                differ_count_i_after += inputs_histo( i, kj );
+                if( differ_count_i_before > 0 )
+                    differ_count_i_before += inputs_histo( i, kj );
+                if( inputs_histo( j, kj ) > 0 )
+                    break;
+                n_differ_i_after += 1;
+                if( differ_count_i_before > 0 )
+                    n_differ_i_before += 1;
+            }
+            if( kj < histo_size )
+            {
+                grad_update += KLdivTerm( differ_count_i_after, inputs_histo( j, kj ) )
+		               *(real)(1+n_differ_i_after)*over_dq;
 
-		if( differ_count_i_before > 0 )
-                    grad_update -= KLdivTerm( differ_count_i_before, expectations_histo( j, kj ) ) *(real)(n_differ_i_before+1)*over_dq;
-	    }
-	    else
-		"cas o? on regroupe avec le dernier";   
+                if( differ_count_i_before > 0 )
+                    grad_update -= KLdivTerm( differ_count_i_before, inputs_histo( j, kj ) )
+		                   *(real)(1+n_differ_i_before)*over_dq;
+            }
+            else
+                "cas ou on regroupe avec le dernier";   
         }
     }
-    return grad_update;
+    return grad_update*over_dq;
 }
 
-real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count)
+real LayerCostModule::delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq)
 {
     //PLASSERT( over_dq > 0.0 )
 
     real grad_update = 0.0;
-		    
-    real Ni_ki = expectations_histo(i,index_i);
+                    
+    real Ni_ki = inputs_histo(i,index_i);
     PLASSERT( !fast_exact_is_equal(Ni_ki, 0.0) ); // Verification:
-                                                  // if expectations_histo is up to date,
-                                                  // the expectation(isample,i) has been counted
-    real Ni_ki_shift1 = expectations_histo(i,index_i+1);
-		    
-    real Nj_ki        = expectations_histo(j,index_i);
-    real Nj_ki_shift1 = expectations_histo(j,index_i+1);
+                                                  // if inputs_histo is up to date,
+                                                  // the input(isample,i) has been counted
+    real Ni_ki_shift1 = inputs_histo(i,index_i+1);
+                    
+    real Nj_ki        = inputs_histo(j,index_i);
+    real Nj_ki_shift1 = inputs_histo(j,index_i+1);
 
 
         // removing the term of the sum that will be modified
         grad_update -= KLdivTerm( Ni_ki, Nj_ki );
-							       
+                                                               
         // adding the term of the sum with its modified value
         grad_update += KLdivTerm( Ni_ki-one_count, Nj_ki );
 
         grad_update += KLdivTerm( Ni_ki_shift1+one_count, Nj_ki_shift1 );
-	
+        
         grad_update -= KLdivTerm( Ni_ki_shift1, Nj_ki_shift1 );
 
     return grad_update *over_dq;
@@ -1173,46 +1129,99 @@
     return ( pj - pi ) * safeflog( pi/pj );
 }
 
-void LayerCostModule::computeHisto(const Mat& expectations)
+real LayerCostModule::computeKLdiv()
 {
-    int index, batch_size = expectations.length();
-    real one_count = 1. / (real)batch_size;
-    Vec expectation;
+            real cost = 0;
+            for (int i = 0; i < input_size; i++)
+                for (int j = 0; j < i; j++)
+                {
+                    // These variables are used in case one bin of 
+                    // the histogram is empty for one unit
+                    // and not for another one ( (Nj-Ni).log(Ni/Nj) = nan ).
+                    // In such case, we ''differ'' the count for the next bin and so on.
+                    real differ_count_i = 0.0;
+                    real differ_count_j = 0.0;
+                    int n_differ = 0;
+                    real last_positive_Ni_k, last_positive_Nj_k;
+                    int last_n_differ;
+                    for (int k = 0; k < histo_size; k++)
+                    {
+                        real Ni_k = inputs_histo(i,k) + differ_count_i;
+                        real Nj_k = inputs_histo(j,k) + differ_count_j;
+                        if( fast_exact_is_equal(Ni_k, 0.0) )
+                        {
+                         // differ_count_j += inputs_histo(j,k);
+                            differ_count_j = Nj_k;
+                            n_differ += 1;
+                        }
+                        else if( fast_exact_is_equal(Nj_k, 0.0) )
+                        {
+                            differ_count_i = Ni_k;
+                            n_differ += 1;
+                        }
+                        else
+                        {
+                            cost += KLdivTerm(Ni_k,Nj_k) *(real)(1+n_differ) *HISTO_STEP;
+                            differ_count_i = 0.0;
+                            differ_count_j = 0.0;
+                            n_differ = 0;
+                            last_positive_Ni_k = Ni_k;
+                            last_positive_Nj_k = Nj_k;
+                            last_n_differ = n_differ;
+                        }
+                    }
+                    if( differ_count_i > 0.0 )
+                    {   
+                        "cas ou on regroupe avec le dernier";   
+//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
+//                                  *(real)(1+last_n_differ) *HISTO_STEP;
+//                        cost += KLdivTerm(last_positive_Ni_k+differ_count_i,last_positive_Nj_k)
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP; 
+                    }
+                     
+                    else if ( differ_count_j > 0.0 )
+                    {
+                        "cas ou on regroupe avec le dernier";
+//                        cost -= KLdivTerm(last_positive_Ni_k,last_positive_Nj_k)
+//                                 *(real)(1+last_n_differ) *HISTO_STEP;
+//                        cost += KLdivTerm(last_positive_Ni_k,last_positive_Nj_k+differ_count_j)
+//                                 *(real)(1+last_n_differ+n_differ) *HISTO_STEP;
+                    }    
+                }
+            // Normalization w.r.t. number of units
+            return cost *norm_factor;
+}
+
+void LayerCostModule::computeHisto(const Mat& inputs)
+{
+    int n_samples = inputs.length();
+    one_count = 1. / (real)n_samples;
+    Vec input;
     
-    expectations_histo.clear(); 
-    for (int isample = 0; isample < batch_size; isample++)
+    inputs_histo.clear(); 
+    for (int isample = 0; isample < n_samples; isample++)
     {
-        expectation = expectations(isample);
+        input = inputs(isample);
         for (int i = 0; i < input_size; i++)
-        {
-	    index = histo_index(expectation[i]);
-            expectations_histo(i,index) += one_count;
-        }
+            inputs_histo(i, histo_index(input[i]) ) += one_count;
     }
 }
 
 
 
-void LayerCostModule::computeSafeHisto(const Mat& expectations)
+void LayerCostModule::computeSafeHisto(const Mat& inputs)
 {
-    int index, batch_size = expectations.length();
-    real one_count = 1. / (real)(batch_size+histo_size);
-    Vec expectation;
+    int n_samples = inputs.length();
+    one_count = 1. / (real)(n_samples+histo_size);
+    Vec input;
     
-    expectations_histo.fill(one_count);
-/*
-    for (int k = 0; k < histo_size; k++)
-        for (int i = 0; i < input_size; i++)
-            expectations_histo(i,k) = one_count;
-*/
-    for (int isample = 0; isample < batch_size; isample++)
+    inputs_histo.fill(one_count);
+
+    for (int isample = 0; isample < n_samples; isample++)
     {
-        expectation = expectations(isample);
+        input = inputs(isample);
         for (int i = 0; i < input_size; i++)
-        {
-	    index = histo_index(expectation[i]);
-            expectations_histo(i,index) += one_count;
-        }
+            inputs_histo(i, histo_index(input[i])) += one_count;
     }
 }
 
@@ -1222,17 +1231,13 @@
 //
 int LayerCostModule::histo_index(real q)
 {
-    if( q >=1.0 )
-       return histo_size-1;
+    PLASSERT( (q >= 0.) && (q < 1.) );
 
-    if( !(q >= 0.0) || !(q < 1.0) )
-        PLERROR("LayerCostModule detected an anormal expectation value (%f)", q);
+    if( fast_exact_is_equal( q, 1. ) )
+       return histo_size - 1;
 
 // LINEAR SCALE
     return (int)floor(q*(real)histo_size);
-
-// LOG SCALE
-    return max(  (int)floor(log(LOGHISTO_BASE, q))+histo_size , 0);
 }
 
 // Returns the minimum amount dq which have to be added/removed to q
@@ -1244,54 +1249,38 @@
 //
 real LayerCostModule::dq(real q)
 {
-// LINEAR SCALE
     // ** Simple version **
-    return LINHISTO_STEP;
+    return HISTO_STEP;
 
     // ** Elaborated version **
-    if( fast_exact_is_equal( round(q*(real)histo_size) , ceil(q*(real)histo_size) ) )
-       return LINHISTO_STEP;
-    else
-       return -LINHISTO_STEP;
+    //if( fast_exact_is_equal( round(q*(real)histo_size) , ceil(q*(real)histo_size) ) )
+    //   return HISTO_STEP;
+    //else
+    //   return -HISTO_STEP;
 
     // ** BAD VERSION: too unstable **
     // return (real)histo_index(q+1.0/(real)histo_size)/(real)histo_size - q;
-
-// LOG SCALE
-    // ** Simple version **
-    //if( q < LOGHISTO_MIN )
-    //  return LOGHISTO_BASE * LOGHISTO_MIN - q;
-    //return q*(LOGHISTO_BASE-1.0);
-    
-    // ** BAD VERSION: too unstable **
-    // real REF = LOGHISTO_BASE * LOGHISTO_MIN;
-    // while( true )
-    // {
-    //     if( q < REF )
-    //         return REF - q;
-    //     REF *= LOGHISTO_BASE;
-    // }
 }
 
 
-
-
-
 ////////////
 // forget //
 ////////////
 void LayerCostModule::forget()
 {
+    inputs_histo.clear();
+
+    inputs_expectation.clear();
+    inputs_stds.clear();
+    
+    inputs_correlations.clear();
+    inputs_cross_quadratic_mean.clear();
     if( momentum > 0.0)
     {
-        expectations_expectation_trainMemory.clear();
-        expectations_cross_quadratic_mean_trainMemory.clear();
+        inputs_expectation_trainMemory.clear();
+        inputs_cross_quadratic_mean_trainMemory.clear();
     }
-/*
-    expectations_expectation_testMemory.clear();
-    expectations_cross_quadratic_mean_testMemory.clear();
-    ntest = 0;
-*/
+    one_count = 0.;
 }
 
 

Modified: trunk/plearn_learners/online/LayerCostModule.h
===================================================================
--- trunk/plearn_learners/online/LayerCostModule.h	2007-09-20 20:45:27 UTC (rev 8089)
+++ trunk/plearn_learners/online/LayerCostModule.h	2007-09-21 22:47:00 UTC (rev 8090)
@@ -46,7 +46,7 @@
 namespace PLearn {
 
 /**
- * Computes a cost function for a (hidden) representation, given two "expectation" vectors. Backpropagates it.
+ * Computes a cost function for a (hidden) representation. Backpropagates it.
  */
 class LayerCostModule : public OnlineLearningModule
 {
@@ -62,17 +62,34 @@
     real alpha;
     
     real momentum;
+
+    //#####  Public Learnt Options  ###########################################
+
+    //! Histograms of inputs (estimated empiricially on some data)
+    //! Computed only when cost_function == 'kl_div' or 'kl_div_simpe'
+    Mat inputs_histo;
+
+    //! Statistics on inputs (estimated empiricially on some data)
+    //! Computed only when cost_function == 'correlation'
+    //! or (for some) 'pascal'
+    Vec inputs_expectation;
+    Vec inputs_stds;         //! only for 'correlation' cost function
     
+    Mat inputs_correlations; //! only for 'correlation' cost function
+    Mat inputs_cross_quadratic_mean;
+
+    //! The generic name of the cost function
+    string cost_function_completename;
+    
 public:
     //#####  Public Member Functions  #########################################
 
     //! Default constructor
     LayerCostModule();
 
-
     //! given the input and target, compute the cost
-    virtual void fprop(const Vec& expectation, real& cost) const;
-    virtual void fprop(const Mat& expectations, Mat& costs);
+    virtual void fprop(const Vec& input, real& cost) const;
+    virtual void fprop(const Mat& inputs, Mat& costs);
     //! Overridden.
     virtual void fprop(const TVec<Mat*>& ports_value);
     
@@ -81,18 +98,27 @@
                                 const TVec<Mat*>& ports_gradient);
 
     //! Some auxiliary function to deal with empirical histograms
-    virtual void computeHisto(const Mat& expectations);
-    virtual void computeSafeHisto(const Mat& expectations);
-    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real delta_KLdivTerm_2(int i, int j, int index_i, real over_dq, real one_count);
-    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq, real one_count);
+    virtual void computeHisto(const Mat& inputs);
+    virtual void computeSafeHisto(const Mat& inputs);
+    virtual real delta_KLdivTerm(int i, int j, int index_i, real over_dq);
+    virtual real delta_SafeKLdivTerm(int i, int j, int index_i, real over_dq);
     virtual real KLdivTerm(real pi, real pj);
+    virtual real computeKLdiv();
     virtual int histo_index(real q);
     virtual real dq(real q);
 
     //! Auxiliary function for the pascal's cost function
-    virtual void computePascalStatistics(const Mat& expectations, bool duringTraining);
+    virtual void computePascalStatistics(const Mat& inputs);
+    virtual string func_pascal_prefix();
+    virtual real   func_pascal(real correlation);
+    virtual real   deriv_func_pascal(real correlation);
 
+    //! Auxiliary function for the correlation's cost function
+    virtual void computeCorrelationStatistics(const Mat& inputs);
+    virtual string func_correlation_prefix();
+    virtual real   func_correlation(real correlation);
+    virtual real   deriv_func_correlation(real correlation);
+
     //! Overridden to do nothing (in particular, no warning).
     virtual void setLearningRate(real dynamic_learning_rate) {}
 
@@ -127,23 +153,23 @@
     //! Does stochastic gradient makes sense with our cost function?
     bool is_cost_function_stochastic;
 
-    //! Histograms of expectations (estimated empiricially on the data)
-    Mat expectations_histo;
+    //! Normalizing factor applied to the cost function
+    //! to take into acount the number of weights
+    real norm_factor;
 
-    //! Some features of the histogram of expectations
-    real LINHISTO_STEP;
-    real LOGHISTO_BASE;
-    real LOGHISTO_MIN;
+    //! Variables for (non stochastic) KL Div cost function
+    //! ---------------------------------------------------
+    //! Range of a histogram's bin ( HISTO_STEP = 1/histo_size )
+    real HISTO_STEP;
+    //! the weight of a sample within a batch (usually, 1/n_samples)
+    real one_count; 
 
-    //! Statistics on matrix of expectations (estimated empiricially on the data)
-    Vec expectations_expectation;
-    Mat expectations_cross_quadratic_mean;
-    Vec expectations_expectation_trainMemory;
-    Mat expectations_cross_quadratic_mean_trainMemory;
-    Vec expectations_expectation_testMemory;
-    Mat expectations_cross_quadratic_mean_testMemory;
-    int ntest;
-
+    //! Variables for (non stochastic) Pascal's/correlation function
+    //! -------------------------------------------------------------
+    //! Statistics on outputs (estimated empiricially on the data)    
+    Vec inputs_expectation_trainMemory;
+    Mat inputs_cross_quadratic_mean_trainMemory;
+        
     //! Map from a port name to its index in the 'ports' vector.
     map<string, int> portname_to_index;
 



From nouiz at mail.berlios.de  Sat Sep 22 23:49:55 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sat, 22 Sep 2007 23:49:55 +0200
Subject: [Plearn-commits] r8091 - trunk/python_modules/plearn/parallel
Message-ID: <200709222149.l8MLnt2o010242@sheep.berlios.de>

Author: nouiz
Date: 2007-09-22 23:49:54 +0200 (Sat, 22 Sep 2007)
New Revision: 8091

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
partial version of ssh back-end


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-21 22:47:00 UTC (rev 8090)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-22 21:49:54 UTC (rev 8091)
@@ -15,7 +15,21 @@
 from threading import Thread,Lock
 from time import sleep
 import datetime
+from plearn.pymake.pymake import get_list_of_hosts
+#from plearn.pymake.pymake import get_distcc_hosts
+from plearn.pymake.pymake import locateconfigfile
+from plearn.pymake.pymake import get_platform
 
+try:
+    from random import shuffle
+except ImportError:
+    import whrandom
+    def shuffle(list):
+        l = len(list)
+        for i in range(0,l-1):
+            j = whrandom.randint(i+1,l-1)
+            list[i], list[j] = list[j], list[i]
+
 STATUS_FINISHED = 0
 STATUS_RUNNING = 1
 STATUS_WAITING = 2
@@ -29,6 +43,14 @@
         
     def __iter__( self ):
         return self
+
+    def get(self):
+        try:
+            self._lock.acquire()
+            return self._iterator.next()
+        finally:
+            self._lock.release()
+ 
     
     def next( self ):
         try:
@@ -36,7 +58,36 @@
             return self._iterator.next()
         finally:
             self._lock.release()
+
+class LockedListIter:
+    def __init__( self, list ):
+        self._lock     = Lock()
+        self._list     = list
+        self._last     = -1
+
+    def __iter__( self ):
+        return self
+
+    def next(self):
+        try:
+            self._lock.acquire()
+            self._last+=1
+            if len(self._list)>self._last:
+                return 
+            else:
+                return self._list[self._last]
+        finally:
+            self._lock.release()
+ 
+    
+    def append( self, a ):
+        try:
+            self._lock.acquire()
+            list.append(a)
+        finally:
+            self._lock.release()
             
+
 #original version from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/196618
 class MultiThread:
     def __init__( self, function, argsVector, maxThreads=5, print_when_finished=None):
@@ -840,51 +891,91 @@
             print "[DBI] WARNING jobs not started!"
                 
 class SshHost:
-    def __init__(self, hostname):
+    def __init__(self, hostname,nice=19,get_avail=True):
         self.hostname= hostname
-        self.lastupd= -16
-        self.getAvailability()
+        self.minupdate=15
+        self.lastupd= -1-self.minupdate
+        self.working=True
+        (self.bogomips,self.ncores,self.loadavg)=(-1.,-1,-1.)
+        self.nice=nice
+        if get_avail:
+            self.getAvailability()
         
     def getAvailability(self):
         # simple heuristic: mips / load
         t= time.time()
-        if t - self.lastupd > 15: # min. 15 sec. before update
-            self.bogomips= self.getBogomips()
-            self.loadavg= self.getLoadavg()
+        if t - self.lastupd > self.minupdate: # min. 15 sec. before update
+            (self.bogomips,self.ncores,self.loadavg)=self.getAllHostInfo()
             self.lastupd= t
             #print  self.hostname, self.bogomips, self.loadavg, (self.bogomips / (self.loadavg + 0.5))
         return self.bogomips / (self.loadavg + 0.5)
         
-    def getBogomips(self):
-        cmd= ["ssh", self.hostname ,"cat /proc/cpuinfo"]
+    def getAllHostInfo(self):
+        cmd= ["ssh", self.hostname ,"cat /proc/cpuinfo;cat /proc/loadavg"]
         p= Popen(cmd, stdout=PIPE)
-        bogomips= 0.0
+        bogomips= -1
+        ncores=-1
+        loadavg=-1
+        returncode = p.returncode
+        wait = p.wait()
+        if returncode:
+            self.working=False
+            return (-1.,-1,-1.)
+        elif wait!=0:
+            self.working=False
+            return (-1.,-1,-1.)
+
         for l in p.stdout:
             if l.startswith('bogomips'):
                 s= l.split(' ')
                 bogomips+= float(s[-1])
-        return bogomips
+            if l.startswith('processor'):
+                s= l.split(' ')
+                ncores=int(s[-1])+1
 
-    def getLoadavg(self):
-        cmd= ["ssh", self.hostname,"cat /proc/loadavg"]
-        p= Popen(cmd, stdout=PIPE)
-        l= p.stdout.readline().split(' ')
-        return float(l[0])
-        
+        if l:
+            loadavg=float(l[0])
+        #(bogomips,ncores,load average)
+        return (bogomips,ncores,loadavg)
+
     def addToLoadavg(self,n):
         self.loadavg+= n
         self.lastupd= time.time()
 
     def __str__(self):
-        return "SshHost("+self.hostname+" <"+str(self.bogomips) \
-               +','+str(self.loadavg) +','+str(self.getAvailability()) \
-               +','+str(self.lastupd) + '>)'
+        return "SshHost("+self.hostname+" <nice: "+str(self.nice)\
+               +"bogomips:"+str(self.bogomips)\
+               +',ncores:'+str(self.ncores)\
+               +',loadavg'+str(self.loadavg)\
+               +',avail:'+str(self.getAvailability())\
+               +',lastupd:'+str(self.lastupd) + '>)'
 
     def __repr__(self):
         return str(self)
         
 def find_all_ssh_hosts():
-    return [SshHost(h) for h in set(pymake.get_distcc_hosts())]
+    hostspath_list = [os.path.join(os.getenv("HOME"),".pymake",get_platform()+'.hosts')]
+    if os.path.exists(hostspath_list[0])==0:
+        print "[DBI] no host file %s for the ssh backend"%(hostspath_list[0])
+        sys.exit(1)
+    print "[DBI] using file %s for the list of host"%(hostspath_list[0])
+    from plearn.pymake.pymake import process_hostspath_list
+    from plearn.pymake.pymake import get_hostname
+    (list_of_hosts, nice_values) = process_hostspath_list(hostspath_list,19,get_hostname())
+    shuffle(list_of_hosts)
+    print list_of_hosts
+    print nice_values
+    h=[]
+    for host in list_of_hosts:
+        print "connecting to",host
+        s=SshHost(host,nice_values[host],False)
+        if s.working:
+            h.append(s)
+        else:
+            print "[DBI] host not working:",s.hostname            
+        print s
+    print h
+    return h
 
 def cmp_ssh_hosts(h1, h2):
     return cmp(h2.getAvailability(), h1.getAvailability())
@@ -894,11 +985,13 @@
     def __init__(self, commands, **args ):
         print "[DBI] WARNING: The SSH DBI is not fully implemented!"
         print "[DBI] Use at your own risk!"
+        self.nb_proc=1
         DBIBase.__init__(self, commands, **args)
 
         self.add_commands(commands)
         self.hosts= find_all_ssh_hosts()
-        
+        print "[DBI] hosts: ",self.hosts
+
     def add_commands(self,commands):
         if not isinstance(commands, list):
             commands=[commands]
@@ -911,8 +1004,8 @@
             
     def getHost(self):
         self.hosts.sort(cmp= cmp_ssh_hosts)
-        #print "hosts= "
-        #for h in self.hosts: print h
+        print "hosts= "
+        for h in self.hosts: print h
         self.hosts[0].addToLoadavg(1.0)
         return self.hosts[0]
     
@@ -920,8 +1013,7 @@
         DBIBase.run(self)
 
         host= self.getHost()
-
-
+        
         cwd= os.getcwd()
         command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + "'"
         print "[DBI] "+command
@@ -935,20 +1027,69 @@
         (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
         
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
+        task.p.wait()
 
+
+    def run_one_job2(self, host):
+        DBIBase.run(self)
+
+        cwd= os.getcwd()
+        print self._locked_iter
+        for task in self._locked_iter:
+            print "task",task
+            command = "ssh " + host.hostname + " 'cd " + cwd + "; " + string.join(task.commands,';') + " ; echo $?'"
+            print "[DBI, %s] %s"%(time.ctime(),command)
+            
+            if self.test:
+                return
+        
+            task.launch_time = time.time()
+            task.set_scheduled_time()
+        
+###            (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
+        
+            task.p = Popen(command, shell=True,stdout=PIPE,stderr=PIPE)
+            wait = task.p.wait()
+            returncode = p.returncode
+            if returncode:
+                self.working=False
+                
+            elif wait!=0:
+                self.working=False
+                #redo it
+            return -1.
+
+            out=task.p.stdout.readlines()
+            err=task.p.stderr.readlines()
+            self.echo_result=None
+            if out:
+                self.echo_result=int(out[-1])
+                del out[-1]
+            print "out",out
+            print "err",err
+            print "echo result",self.echo_result
+            if err:
+                task.return_status = int(err[-1])  # last line was an echo $? (because rsh doesn't transmit the status byte correctly)
+                del err[-1]
+                print "return status", task.return_status
+            sleep(1)
+      
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
+        if not self.file_redirect_stdout and self.nb_proc>1:
+            print "[DBI] WARNING: many process but all their stdout are redirected to the parent"
+        if not self.file_redirect_stderr and self.nb_proc>1:
+            print "[DBI] WARNING: many process but all their stderr are redirected to the parent"
 
         # Execute pre-batch
         self.exec_pre_batch()
-
+        self._locked_iter=LockedListIter(iter(self.tasks))
         if self.test:
             print "[DBI] In testmode, we only print the command that would be executed."
-            
+        print "in run",self.hosts
         # Execute all Tasks (including pre_tasks and post_tasks if any)
-        print "[DBI] tasks= ", self.tasks
-        for task in self.tasks:
-            self.run_one_job(task)
+        self.mt=MultiThread(self.run_one_job2,self.hosts,self.nb_proc,lambda :("[DBI,%s]"%time.ctime()))
+        self.mt.start()
 
         # Execute post-batchs
         self.exec_post_batch()
@@ -959,8 +1100,9 @@
 
     def wait(self):
         #TODO
-        print "[DBI] WARNING the wait function was not implement for the ssh backend!"
+        self.mt.join()
 
+
 # creates an object of type ('DBI' + launch_system) if it exists
 def DBI(commands, launch_system, **args):
     """The Distributed Batch Interface is a collection of python classes



From nouiz at mail.berlios.de  Sun Sep 23 01:00:40 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 23 Sep 2007 01:00:40 +0200
Subject: [Plearn-commits] r8092 - trunk/python_modules/plearn/parallel
Message-ID: <200709222300.l8MN0eCq025775@sheep.berlios.de>

Author: nouiz
Date: 2007-09-23 01:00:39 +0200 (Sun, 23 Sep 2007)
New Revision: 8092

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
detect and warn if the cluster back-end failed during the launching/executing of a jobs. The jobs is no automatically relaunched.


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-22 21:49:54 UTC (rev 8091)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-22 23:00:39 UTC (rev 8092)
@@ -360,6 +360,8 @@
         self.nb_proc=50
         self.mt=None
         DBIBase.__init__(self, commands, **args)
+        self.pre_tasks=["echo '[DBI] executing on host' $HOSTNAME"]+self.pre_tasks
+        self.post_tasks=["echo '[DBI] exit status' $?"]+self.post_tasks
         self.add_commands(commands)
         self.nb_proc=int(self.nb_proc)
 
@@ -400,9 +402,18 @@
 
         (output,error)=self.get_redirection(task.log_file + '.out',task.log_file + '.err')
         task.p = Popen(command, shell=True,stdout=output,stderr=error)
-        ret=task.p.wait()
-        if task.p.returncode!=0:
-            print "[DBI,%d/%d,%s] Failed to launch: '%s' returned %d,%d"%(started,len(self.tasks),time.ctime(),command,task.p.returncode,ret)
+        task.p_wait_ret=task.p.wait()
+        task.dbi_return_status=None
+        if output!=PIPE:#TODO what do to if = PIPE?
+            fd=open(task.log_file+'.out','r')
+            last=""
+            for l in fd.readlines():
+                last=l
+            if last.startswith("[DBI] exit status "):
+                task.dbi_return_status=int(last.split()[-1])
+#        print "[DBI,%d/%d,%s] Job ended, popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),task.p.returncode,task.p_wait_ret,task.dbi_return_status)
+        if task.dbi_return_status==None:
+            print "[DBI,%d/%d,%s] Trouble with launching/executing '%s'. Its execution did not finished. Probable cause is the back-end itself. Meaby you want to rerun it. popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),command,task.p.returncode,task.p_wait_ret,task.dbi_return_status)
             
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir



From nouiz at mail.berlios.de  Sun Sep 23 01:08:24 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 23 Sep 2007 01:08:24 +0200
Subject: [Plearn-commits] r8093 - in trunk: python_modules/plearn/parallel
	scripts
Message-ID: <200709222308.l8MN8Ofs002232@sheep.berlios.de>

Author: nouiz
Date: 2007-09-23 01:08:23 +0200 (Sun, 23 Sep 2007)
New Revision: 8093

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
   trunk/scripts/dbidispatch
Log:
if nb_proc=-1 will try to execute all jobs concurently


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-22 23:00:39 UTC (rev 8092)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-22 23:08:23 UTC (rev 8093)
@@ -96,7 +96,13 @@
         self._threadPool   = []
         self.print_when_finish = print_when_finished
         self.running = 0
-        nb_thread=maxThreads
+        if maxThreads==-1:
+            nb_thread=len(argsVector)
+        elif maxThreads<=0:
+            print "[DBI] you set %d concurent jobs. Must be higher then 0!!"%(maxThreads)
+            sys.exit(1)
+        else:
+            nb_thread=maxThreads
         if nb_thread>len(argsVector):
             nb_thread=len(argsVector)
         for i in range( nb_thread ):

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-09-22 23:00:39 UTC (rev 8092)
+++ trunk/scripts/dbidispatch	2007-09-22 23:08:23 UTC (rev 8093)
@@ -19,7 +19,7 @@
 
 
 bqtools, cluster, local and ssh parameter:
-  --nb_proc=nb_process, give the maximum number of concurent jobs running
+  --nb_proc=nb_process, give the maximum number of concurent jobs running. The valud -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
     --local=X is the same as --local --nb_proc=X
     --cluster=X is the same as --cluster --nb_proc=X
     --bqtools=X is the same as --bqtools --nb_proc=X



From nouiz at mail.berlios.de  Sun Sep 23 01:59:14 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 23 Sep 2007 01:59:14 +0200
Subject: [Plearn-commits] r8094 - trunk/plearn/vmat
Message-ID: <200709222359.l8MNxEap001062@sheep.berlios.de>

Author: nouiz
Date: 2007-09-23 01:59:14 +0200 (Sun, 23 Sep 2007)
New Revision: 8094

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
-Added field type: num, char
-better test before loading existing mapping
-always create metadatadir


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2007-09-22 23:08:23 UTC (rev 8093)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2007-09-22 23:59:14 UTC (rev 8094)
@@ -178,8 +178,10 @@
 // isValidNonSkipFieldType //
 /////////////////////////////
 bool TextFilesVMatrix::isValidNonSkipFieldType(const string& ftype) const {
-    return (ftype=="auto" || ftype=="date" || ftype=="jdate" || ftype=="postal" ||
-            ftype=="dollar" || ftype=="YYYYMM" || ftype=="sas_date" || ftype == "bell_range");
+    return (ftype=="auto" || ftype=="num" || ftype=="date" || ftype=="jdate" ||
+            ftype=="postal" || ftype=="dollar" || ftype=="dollar-comma" ||
+            ftype=="YYYYMM" || ftype=="sas_date" || ftype == "bell_range" ||
+            ftype == "char" );
 }
 
 void TextFilesVMatrix::setColumnNamesAndWidth()
@@ -222,11 +224,11 @@
 
         metadatadir = metadatapath;
         setMetaDataDir(metadatapath);
+    }
 
-        if(!force_mkdir(getMetaDataDir()))
-            PLERROR("In TextFilesVMatrix::build_: could not create directory '%s'",
-                    getMetaDataDir().absolute().c_str());
-    }
+    if(!force_mkdir(getMetaDataDir()))
+        PLERROR("In TextFilesVMatrix::build_: could not create directory '%s'",
+                getMetaDataDir().absolute().c_str());
     
     PPath metadir = getMetaDataDir();
     PPath idxfname = metadir/"txtmat.idx";
@@ -294,7 +296,7 @@
                 string real_val_str = map_line.substr(end_of_string + 1);
                 real real_val;
                 if (!pl_isnumber(real_val_str, &real_val))
-                    PLERROR("In TextFilesVMatrix::loadMappings - Found a mapping to something that is not a number");
+                    PLERROR("In TextFilesVMatrix::loadMappings - Found a mapping to something that is not a number (%s) in file %s at non-black line %d", map_line.c_str(), fname.c_str(), i);
                 mapping[k][strval] = real_val;
             }
         }
@@ -309,12 +311,15 @@
     // For now we just create them if they do not exist yet.
 
     // First make sure there is no existing mapping.
-    bool already_exist = false;
-    for (int i = 0; !already_exist && i < mapping.length(); i++) {
+    int nb_already_exist = 0;
+    int nb_type_no_mapping = 0;
+    for (int i = 0;  i < mapping.length(); i++) {
         if (!mapping[i].empty())
-            already_exist = true;
+            nb_already_exist++;
+        else if(fieldspec[i].second!="char")//should add auto when it is char that are selected
+            nb_type_no_mapping++;
     }
-    if (!already_exist) {
+    if(nb_already_exist == 0){
         // Mappings need to be built.
         // We do this by reading the whole data.
         Vec row(width());
@@ -326,7 +331,13 @@
             pb.update(i + 1);
         }
         auto_extend_map = auto_extend_map_backup;
-    }
+    }else if (nb_already_exist+nb_type_no_mapping < mapping.length()) {
+        for (int i = 0;  i < mapping.length(); i++) 
+            if(fieldspec[i].second=="char" && mapping[i].empty())//should add auto when it is char that are selected
+                PLWARNING("In TextFilesVMatrix::autoBuildMappings - mapping already existing but not for field %d (%s)",i,fieldspec[i].first.c_str());
+
+        PLWARNING("In TextFilesVMatrix::autoBuildMappings - The existing mapping is not complete! Their is %d fields with build mapping and their is %d fields that do not need mapping with a total of %d fields. Erase the mapping directory in the metadatadir to have it regenerated next time!",nb_already_exist,nb_type_no_mapping,mapping.length());
+    }//else already build
 }
 
 void TextFilesVMatrix::generateMapCounts()
@@ -524,6 +535,23 @@
         else
             dest[0] = getMapping(k, strval);
     }
+    else if(fieldtype=="char")
+    {
+        if(strval=="")  // missing
+            dest[0] = MISSING_VALUE;
+        else
+            dest[0] = getMapping(k, strval);
+    }
+    else if(fieldtype=="num")
+    {
+        if(strval=="")  // missing
+            dest[0] = MISSING_VALUE;
+        else if(pl_isnumber(strval,&val))
+            dest[0] = real(val);
+        else
+            PLERROR("In TextFilesVMatrix::transformStringToValue - expedted a number as the value for field %d(%s). Got %s",k,fieldname.c_str(),strval.c_str());
+                
+    }
     else if(fieldtype=="date")
     {
         if(strval=="")  // missing
@@ -566,24 +594,27 @@
     {
         dest[0] = getPostalEncoding(strval);
     }
-    else if(fieldtype=="dollar")
+    else if(fieldtype=="dollar" || fieldtype=="dollar-comma")
     {
+        char char_torm = ' ';
+        if(fieldtype=="dollar-comma")
+            char_torm = ',';
         if(strval=="")  // missing
             dest[0] = MISSING_VALUE;
         else if(strval[0]=='$')
         {
             string s = "";
             for(unsigned int pos=1; pos<strval.size(); pos++)
-                if(!isspace(strval[pos]))
+                if(strval[pos]!=char_torm)
                     s += strval[pos];
 
             if(pl_isnumber(s,&val))
                 dest[0] = real(val);
             else
-                dest[0] = getMapping(k, strval);
+                PLERROR("In TextFilesVMatrix::transformStringToValue - Goat as value '%s' while parsing field %d (%s) with fieldtype %s",strval.c_str(),k,fieldname.c_str(),fieldtype.c_str());
         }
         else
-            dest[0] = getMapping(k, strval);
+            PLERROR("In TextFilesVMatrix::transformStringToValue - Got as value '%s' while expecting a value beggining with '$' while parsing field %d (%s) with fieldtype %s",strval.c_str(),k,fieldname.c_str(),fieldtype.c_str());
     }
     else if(fieldtype=="bell_range") {
         if (strval == "") {
@@ -671,6 +702,8 @@
                   "Currently supported types: \n"
                   "- skip       : Ignore the content of the field, won't be inserted in the resulting VMat\n"
                   "- auto       : If a numeric value, keep it as is, if not, look it up in the mapping (possibly inserting a new mapping if it's not there) \n"
+                  "- num        : numeric value, keep as is\n"
+                  "- char       : look it up in the mapping (possibly inserting a new mapping if it's not there)\n"
                   "- date       : date of the form 25DEC2003 or 25-dec-2003 or 2003/12/25 or 20031225, will be mapped to float date format 1031225\n"
                   "- jdate      : date of the form 25DEC2003 or 25-dec-2003 or 2003/12/25 or 20031225, will be mapped to *julian* date format\n"
                   "- sas_date   : date used by SAS = number of days since Jan. 1st, 1960 (with 0 = missing)\n"
@@ -678,6 +711,7 @@
                   "               other than a number or lower than 197000 is considered as nan\n"
                   "- postal     : canadian postal code \n"
                   "- dollar     : strangely formatted field with dollar amount. Format is sth like '$12 003'\n"
+                  "- dollar-comma : strangely formatted field with dollar amount. Format is sth like '$12,003'\n"
                   "- bell_range : a range like \"A: $0- 250\", replaced by the average of the two bounds;\n"
                   "               if the \"Negative Value\" string is found, it is replaced by -100\n"
         );



From nouiz at mail.berlios.de  Sun Sep 23 02:10:20 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Sun, 23 Sep 2007 02:10:20 +0200
Subject: [Plearn-commits] r8095 - trunk/python_modules/plearn/learners
Message-ID: <200709230010.l8N0AKOX001729@sheep.berlios.de>

Author: nouiz
Date: 2007-09-23 02:10:19 +0200 (Sun, 23 Sep 2007)
New Revision: 8095

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
use confusion_target correctly
forward the weaklearner test cost


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-22 23:59:14 UTC (rev 8094)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-23 00:10:19 UTC (rev 8095)
@@ -3,7 +3,7 @@
 import time
 
 class AdaBoostMultiClasses:
-    def __init__(self,trainSet1,trainSet2,weakLearner):
+    def __init__(self,trainSet1,trainSet2,weakLearner,confusion_target=1):
 #        """
 #        Initialize a AdaBoost for 3 classes learner
 #        trainSet1 is used for the first sub AdaBoost learner,
@@ -23,7 +23,7 @@
         self.nstages = 0
         self.stage = 0
         self.train_time = 0
-        #        self.confusion_target=plargs.confusion_target
+        self.confusion_target=confusion_target
         
     def myAdaBoostLearner(self,sublearner,trainSet):
         l = pl.AdaBoost()
@@ -31,9 +31,12 @@
         l.pseudo_loss_adaboost=plargs.pseudo_loss_adaboost
         l.weight_by_resampling=plargs.weight_by_resampling
         l.setTrainingSet(trainSet,True)
-        l.setTrainStatsCollector(VecStatsCollector())
+        tmp=VecStatsCollector()
+        tmp.setFieldNames(l.getTrainCostNames())
+        l.setTrainStatsCollector(tmp)
         l.early_stopping=False
-        l.compute_training_error=False
+        l.compute_training_error=True
+        l.forward_sub_learner_costs=True
         return l
 
     def train(self):
@@ -48,14 +51,17 @@
         
     def getTestCostNames(self):
         costnames = ["class_error","linear_class_error","square_class_error"]
-        #    for i in range(len(conf_matrix)):
-        #        for j in range(len(conf_matrix[i])):
-        for i in range(4):
+        for i in range(3):
             for j in range(3):
                 costnames.append("conf_matrix_%d_%d"%(i,j))
         costnames.append("train_time")
         costnames.append("conflict")
         costnames.extend(["class0","class1","class2"])
+
+        for c in self.learner1.getTestCostNames():
+            costnames.append("subweaklearner1."+c)
+        for c in self.learner2.getTestCostNames():
+            costnames.append("subweaklearner2."+c)
         return costnames
     
     def computeOutput(self,example):
@@ -74,7 +80,7 @@
         elif ind1==ind2==1:
             ind=2
         else:
-            ind=3
+            ind=self.confusion_target
         return (ind,out1,out2)
     
     def computeCostsFromOutput(self,input,output,target,costs=[]):
@@ -85,23 +91,36 @@
         costs.append(class_error)
         costs.append(linear_class_error)
         costs.append(square_class_error)
-        for i in range(4):
+        for i in range(3):
             for j in range(3):
                 costs.append(0)
         costs[output[0]*3+target+3]=1
         costs.append(self.train_time)
-        if output[0]==0:
-            costs.extend([0,1,0,0])
-        if output[0]==1:
-            costs.extend([0,0,1,0])
-        if output[0]==2:
-            costs.extend([0,0,0,1])
-        if output[0]==3:
+
+        #append conflict cost
+        if int(round(output[1]))==0 and int(round(output[2]))==1:
             costs.append(1)
-            t=[0,0,0]
-            t[plargs.confusion_target]=1
-            costs.extend(t)
-            
+        else:
+            costs.append(0)
+        
+        #append class output cost
+        t=[0,0,0]
+        t[output[0]]=1
+        costs.extend(t)
+        if target==0:
+            t1=array([0.])
+        else:
+            t1=array([1.])
+        if target==2:
+            t2=array([1.])
+        else:
+            t2=array([0.])
+        o1=array([output[1]])
+        o2=[output[2]]
+        c1=self.learner1.computeCostsFromOutputs(input,o1,t1)
+        c2=self.learner2.computeCostsFromOutputs(input,o2,t2)
+        costs.extend(c1)
+        costs.extend(c2)
         return costs
 
     def computeOutputAndCosts(self,input,target):
@@ -109,6 +128,34 @@
         costs=self.computeCostsFromOutput(input,output,target)
         return (output,costs)
 
+    def test(self,testset,test_stats,return_outputs,return_costs):
+        print "In AdaBoostMultiClasses.py::test Not implemented"
+        sys.exit(1)
+        
+        stats1=pl.VecStatsCollector()
+        stats2=pl.VecStatsCollector()
+        (test_stats1, testoutputs1, testcosts1)=self.learner1.test(test_stats,stats1,True,return_costs)
+        (test_stats2, testoutputs2, testcosts2)=self.learner2.test(test_stats,stats2,True,return_costs)
+        outputs=[]
+        costs=[]
+        #calculate stats, outputs, costs
+        test_mat=testset.getMat()
+        for i in range(testset.length()):
+            out1=testoutputs1[i][0]
+            out2=testoutputs2[i][0]
+            ind1=int(round(out1))
+            ind2=int(round(out2))
+            if ind1==ind2==0:
+                ind=0
+            elif ind1==1 and ind2==0:
+                ind=1
+            elif ind1==ind2==1:
+                ind=2
+            else:
+                ind=self.confusion_target
+            outputs.append([ind,out1,out2])
+            self.computeCostsFromOutput(test_mat[i][:-2],ind,test_mat[i][-1])
+        
     def outputsize(self):
         return len(self.getTestCostNames())
 



From nouiz at mail.berlios.de  Mon Sep 24 20:19:45 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 24 Sep 2007 20:19:45 +0200
Subject: [Plearn-commits] r8096 - trunk/plearn/math
Message-ID: <200709241819.l8OIJjbF021603@sheep.berlios.de>

Author: nouiz
Date: 2007-09-24 20:19:45 +0200 (Mon, 24 Sep 2007)
New Revision: 8096

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Added remote call from python to getFieldNames() function


Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-09-23 00:10:19 UTC (rev 8095)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-09-24 18:19:45 UTC (rev 8096)
@@ -213,6 +213,10 @@
                  "A vector of strings corresponding to the names of each field"
                  " in the VecStatsCollector.\n")));
 
+    declareMethod(
+        rmm, "getFieldNames", &VecStatsCollector::getFieldNames,
+        (BodyDoc("Get field names.\n")));
+
    declareMethod(
         rmm, "length", &VecStatsCollector::length,
         (BodyDoc("Returns the number of statistics collected.\n"),



From manzagop at mail.berlios.de  Mon Sep 24 20:22:45 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 24 Sep 2007 20:22:45 +0200
Subject: [Plearn-commits] r8097 - trunk/scripts
Message-ID: <200709241822.l8OIMj0K021782@sheep.berlios.de>

Author: manzagop
Date: 2007-09-24 20:22:44 +0200 (Mon, 24 Sep 2007)
New Revision: 8097

Modified:
   trunk/scripts/dbidispatch
Log:
Minor changes to the long help message.

Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2007-09-24 18:19:45 UTC (rev 8096)
+++ trunk/scripts/dbidispatch	2007-09-24 18:22:44 UTC (rev 8097)
@@ -3,32 +3,32 @@
 from plearn.utilities.toolkit import search_file
 
 ScriptName="launchdbi.py"
-ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- mean that it is the default'
-LongHelp="""Dispatches jobs with dbi.py. dbi allow to dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the gived order. ssh is never automaticaly selected.
+ShortHelp='Usage: dbidispatch [--help|-h] [--dbilog|*--nodbilog] [--condor|--bqtools[=nb_process]|--cluster[=nb_process]|--local[=nb_process]|--ssh[=nb_process]] [--nb_proc=nb_process] [--test] [--long] [--micro[=nb_batch]] [--duree=X] [--wait|--nowait] [--req="CONDOR_REQUIREMENT"] [--32|--64|--3264] {--file=FILEPATH | <command-template>} \n An * before -- signals the default option value.'
+LongHelp="""Dispatches jobs with dbi.py. dbi can dispatch jobs on condor, bqtools, cluster, local and ssh. If no system is selected on the command line, we try them in the previous order. ssh is never automaticaly selected.
 
 %s
 
-common option:
+common options:
   The -h, --help print the long help(this)
-  The --condor, --bqtools, --cluster, --local or --ssh option tell witch on system the jobs will be send. If not present, we will use the first available in the order gived. ssh is never automaticaly selected.
-  The --dbilog (--nodbilog) ask dbi to (to don't) generate additional log
-  The '--test' option make that dbidispatch generate the file $ScriptName, but do not execute it. That way you can see what dbidispatch generate. Also, this file make dbi in test mode, so dbi do not execute automaticaly the experiment il $ScriptName is executer
-  The --file=FILEPATH option make this script use the jobs to expand in the file instead of the command line. Their must be one jobs by line.
+  The --condor, --bqtools, --cluster, --local or --ssh option specify on which system the jobs will be sent. If not present, we will use the first available in the previously given order. ssh is never automaticaly selected.
+  The --dbilog (--nodbilog) tells dbi to generate (or not) an additional log
+  The '--test' option makes dbidispatch generate the file $ScriptName, without executing it. That way you can see what dbidispatch generates. Also, this file calls dbi in test mode, so dbi executes everything in the script except the experiment in $ScriptName (so you can check the script).
+  The --file=FILEPATH specifies a file containing the jobs to execute, one per line. This is instead of specifying only one job on the command line.
 
 dbidispatch --test --file=tests
 
 
-bqtools, cluster, local and ssh parameter:
-  --nb_proc=nb_process, give the maximum number of concurent jobs running. The valud -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
+bqtools, cluster, local and ssh options:
+  --nb_proc=nb_process, specifies the maximum number of concurrent jobs running. The value -1 will try to execute all jobs concurently. Use with care as some back-end or configuration do not handle this correctly.
     --local=X is the same as --local --nb_proc=X
     --cluster=X is the same as --cluster --nb_proc=X
     --bqtools=X is the same as --bqtools --nb_proc=X
     --ssh=X is the same as --ssh --nb_proc=X
 
 bqtools and cluster option:
-  The '--duree' option tell the maximum length of the jobs. The have the cluster syntaxe of accepted value 'cluster --help'. The bqtools syntaxe is '--duree=12:13:15'. This give 12 hours, 13 minutes and 15 seconds
+  The '--duree' option specifies the maximum duration of the jobs. The syntax depends on where the job is dispatched. For the cluster syntax, see 'cluster --help'. For bqtools, the syntax is '--duree=12:13:15', giving 12 hours, 13 minutes and 15 seconds.
 
-bqtools only option:
+bqtools only options:
   The '--micro[=nb_batch]' option can be used with BqTools when launching many jobs that
   have a very short duration. This may prevent some queue crashes. The nb_batch value
   is the number of experience to group together in a batch.(default 20)
@@ -42,15 +42,15 @@
   If this option is not set, the maximum duration of each job will be 120 hours
   (5 days).
 
-cluster and condor option:
-  The '--3264', '--32' or '--64' tell the type of cpu the compute node must have to execute the commands.
+cluster and condor options:
+  The '--3264', '--32' or '--64' specify which type of cpu the node must have to execute the commands.
 
-cluster only option:
-  The '--wait' is transfered to cluster. This must be enabled if their is not nb_process available compute node. Otherwise when their is no compute node available, the launch of that command fail.
-  The '--nowait' make that the --wait option is not gived to cluster as is the default
+cluster only options:
+  The '--wait' is transfered to cluster. This must be enabled if there is not nb_process available nodes. Otherwise when there are no nodes available, the launch of that command fails.
+  The '--nowait' means the --wait option is not given to the cluster command, as in the default.
   
-condor only option:
-  The '--req=\"CONDOR_REQUIREMENT\"' option make that dbidispatch send additional option to DBI that will be used to generate addtional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writed like that:
+condor only options:
+  The '--req=\"CONDOR_REQUIREMENT\"' option makes dbidispatch send additional option to DBI that will be used to generate additional requirement for condor. CONDOR_REQUIREMENT must follow the syntax of requirement for condor with one exception. The symbol '\"' must be escaped 3 times! So the requirement (Machine == \"computer.example.com\") must be writen in the following way:
 
   dbidispatch \"--req=Machine==\\\\\\\"computer.example.com\\\\\\\"\"
      or



From nouiz at mail.berlios.de  Mon Sep 24 20:45:47 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 24 Sep 2007 20:45:47 +0200
Subject: [Plearn-commits] r8098 - trunk/python_modules/plearn/learners
Message-ID: <200709241845.l8OIjlA5022918@sheep.berlios.de>

Author: nouiz
Date: 2007-09-24 20:45:47 +0200 (Mon, 24 Sep 2007)
New Revision: 8098

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
better hangling of sublearner expdir


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-24 18:22:44 UTC (rev 8097)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2007-09-24 18:45:47 UTC (rev 8098)
@@ -14,11 +14,11 @@
         self.trainSet2=trainSet2
             
         self.learner1 = self.myAdaBoostLearner(weakLearner(),trainSet1)
-        self.learner1.expdir=plargs.expdirr+"/learner1"
+        self.learner1.setExperimentDirectory(plargs.expdirr+"/learner1")
         self.learner1.setTrainingSet(trainSet1,True)
         
         self.learner2 = self.myAdaBoostLearner(weakLearner(),trainSet2)
-        self.learner2.expdir=plargs.expdirr+"/learner2"
+        self.learner2.setExperimentDirectory(plargs.expdirr+"/learner2")
         self.learner2.setTrainingSet(trainSet2,True)
         self.nstages = 0
         self.stage = 0
@@ -36,7 +36,8 @@
         l.setTrainStatsCollector(tmp)
         l.early_stopping=False
         l.compute_training_error=True
-        l.forward_sub_learner_costs=True
+        l.forward_sub_learner_test_costs=True
+        l.provide_learner_expdir=True
         return l
 
     def train(self):



From nouiz at mail.berlios.de  Mon Sep 24 20:55:21 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 24 Sep 2007 20:55:21 +0200
Subject: [Plearn-commits] r8099 - trunk/plearn_learners/meta
Message-ID: <200709241855.l8OItLYl023320@sheep.berlios.de>

Author: nouiz
Date: 2007-09-24 20:55:20 +0200 (Mon, 24 Sep 2007)
New Revision: 8099

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
Added an option to get the test costs from the weaklearner


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-24 18:45:47 UTC (rev 8098)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-24 18:55:20 UTC (rev 8099)
@@ -194,6 +194,10 @@
                   &AdaBoost::compute_training_error, OptionBase::buildoption,
                   "Whether to compute training error at each stage.\n");
 
+    declareOption(ol, "forward_sub_learner_test_costs", 
+                  &AdaBoost::forward_sub_learner_test_costs, OptionBase::buildoption,
+                  "Did we add the sub_learner_costs to our costs.\n");
+
     declareOption(ol, "found_zero_error_weak_learner", 
                   &AdaBoost::found_zero_error_weak_learner, 
                   OptionBase::learntoption,
@@ -699,11 +703,25 @@
                  "either 0 or 1; current target=%f", target[0]);
     costs[1] = exp(-1.0*sum_voting_weights*(2*output[0]-1)*(2*target[0]-1));
     costs[2] = costs[0];
+    Vec tmp(weak_learner_template->nTestCosts());
+    if(forward_sub_learner_test_costs){
+        weak_learners.last()->computeCostsFromOutputs(input,output,target,tmp);
+        costs.append(tmp);
+    }
 }
 
 TVec<string> AdaBoost::getTestCostNames() const
 {
-    return getTrainCostNames();
+    TVec<string> costs=getTrainCostNames();
+
+    if(forward_sub_learner_test_costs){
+        TVec<string> subcosts=weak_learner_template->getTestCostNames();
+        for(int i=0;i<subcosts.length();i++){
+            subcosts[i]="weak_learner."+subcosts[i];
+        }
+        costs.append(subcosts);
+    }
+    return costs;
 }
 
 TVec<string> AdaBoost::getTrainCostNames() const

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2007-09-24 18:45:47 UTC (rev 8098)
+++ trunk/plearn_learners/meta/AdaBoost.h	2007-09-24 18:55:20 UTC (rev 8099)
@@ -119,6 +119,9 @@
     // save model after each stage into <expdir>/model.psave
     bool save_often;
 
+    // Did we add the sub_learner_costs to our costs
+    bool forward_sub_learner_test_costs;
+
     // ****************
     // * Constructors *
     // ****************



From nouiz at mail.berlios.de  Mon Sep 24 21:24:52 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 24 Sep 2007 21:24:52 +0200
Subject: [Plearn-commits] r8100 - trunk/plearn_learners/meta
Message-ID: <200709241924.l8OJOqf4025061@sheep.berlios.de>

Author: nouiz
Date: 2007-09-24 21:24:52 +0200 (Mon, 24 Sep 2007)
New Revision: 8100

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
-Added default value to forward_sub_learner_test_costs
-Added safe comparison of real value



Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-24 18:55:20 UTC (rev 8099)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-24 19:24:52 UTC (rev 8100)
@@ -64,7 +64,8 @@
       conf_rated_adaboost(0), 
       weight_by_resampling(1), 
       early_stopping(1),
-      save_often(0)
+      save_often(0),
+      forward_sub_learner_test_costs(false)
 { }
 
 PLEARN_IMPLEMENT_OBJECT(
@@ -432,7 +433,7 @@
                 if(report_progress) pb->update(i);
                 train_set->getExample(i, input, target, weight);
 #ifdef BOUNDCHECK
-                if(!(target[0]==0||target[0]==1))
+                if(!(is_equal(target[0],0)||is_equal(target[0],1)))
                     PLERROR("In AdaBoost::train() - target is not 0 or 1 in the training set. We implement only two class boosting.");
 #endif
                 new_weak_learner->computeOutput(input,output);



From nouiz at mail.berlios.de  Mon Sep 24 21:28:52 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 24 Sep 2007 21:28:52 +0200
Subject: [Plearn-commits] r8101 - trunk/plearn_learners/meta
Message-ID: <200709241928.l8OJSqfW025270@sheep.berlios.de>

Author: nouiz
Date: 2007-09-24 21:28:52 +0200 (Mon, 24 Sep 2007)
New Revision: 8101

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
-Added default initialization of some variable


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-24 19:24:52 UTC (rev 8100)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-24 19:28:52 UTC (rev 8101)
@@ -65,7 +65,8 @@
       weight_by_resampling(1), 
       early_stopping(1),
       save_often(0),
-      forward_sub_learner_test_costs(false)
+      forward_sub_learner_test_costs(false),
+      provide_learner_expdir(false)
 { }
 
 PLEARN_IMPLEMENT_OBJECT(



From nouiz at mail.berlios.de  Tue Sep 25 16:16:06 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 16:16:06 +0200
Subject: [Plearn-commits] r8102 - trunk/python_modules/plearn/parallel
Message-ID: <200709251416.l8PEG6g6017038@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 16:16:06 +0200 (Tue, 25 Sep 2007)
New Revision: 8102

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
print more info


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-24 19:28:52 UTC (rev 8101)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-25 14:16:06 UTC (rev 8102)
@@ -96,6 +96,8 @@
         self._threadPool   = []
         self.print_when_finish = print_when_finished
         self.running = 0
+        self.init_len_list = len(argsVector)
+        
         if maxThreads==-1:
             nb_thread=len(argsVector)
         elif maxThreads<=0:
@@ -114,9 +116,9 @@
         self.running-=1
         if self.print_when_finish:
             if callable(self.print_when_finish):
-                print self.print_when_finish(),"left running:",self.running
+                print self.print_when_finish(),"left running: %d/%d"%(self.running,self.init_len_list)
             else:
-                print self.print_when_finish,"left running:",self.running
+                print self.print_when_finish,"left running: %d/%d"%(self.running,self.init_len_list)
                     
     def start( self  ):
         for thread in self._threadPool:



From nouiz at mail.berlios.de  Tue Sep 25 16:58:26 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 16:58:26 +0200
Subject: [Plearn-commits] r8103 - trunk/plearn_learners/hyper
Message-ID: <200709251458.l8PEwQbx019290@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 16:58:26 +0200 (Tue, 25 Sep 2007)
New Revision: 8103

Modified:
   trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
Log:
-Better comment and one more warning


Modified: trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2007-09-25 14:16:06 UTC (rev 8102)
+++ trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2007-09-25 14:58:26 UTC (rev 8103)
@@ -73,7 +73,8 @@
     declareOption(ol, "values", &EarlyStoppingOracle::values, OptionBase::buildoption,
                   "a list of values to try in sequence ");
     declareOption(ol, "range", &EarlyStoppingOracle::range, OptionBase::buildoption,
-                  "a numerical range of the form [ start, end ] or [ start, end, step ] ");
+                  "a numerical range of the form [ start, end ] or [ start, end, step ]\n"
+                  "WARNING: end is not included! ");
 
     declareOption(ol, "min_value", &EarlyStoppingOracle::min_value, OptionBase::buildoption,
                   "minimum allowed error beyond which we stop\n");
@@ -119,6 +120,8 @@
             step = range[2];
         for(real x = range[0]; x<range[1]; x+=step)
             option_values.append(tostring(x));
+        if(range[0]==range[1])
+            PLWARNING("In EarlyStoppingOracle::build_ - no range selected. Meaby you forgot that the end part of the range is not included!");
     }
 }
 



From nouiz at mail.berlios.de  Tue Sep 25 18:06:25 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 18:06:25 +0200
Subject: [Plearn-commits] r8104 - trunk/python_modules/plearn/parallel
Message-ID: <200709251606.l8PG6Pht024852@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 18:06:24 +0200 (Tue, 25 Sep 2007)
New Revision: 8104

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
print more info


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2007-09-25 14:58:26 UTC (rev 8103)
+++ trunk/python_modules/plearn/parallel/dbi.py	2007-09-25 16:06:24 UTC (rev 8104)
@@ -372,6 +372,8 @@
         self.post_tasks=["echo '[DBI] exit status' $?"]+self.post_tasks
         self.add_commands(commands)
         self.nb_proc=int(self.nb_proc)
+        self.backend_failed=0
+        self.jobs_failed=0
 
     def add_commands(self,commands):
         if not isinstance(commands, list):
@@ -422,6 +424,9 @@
 #        print "[DBI,%d/%d,%s] Job ended, popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),task.p.returncode,task.p_wait_ret,task.dbi_return_status)
         if task.dbi_return_status==None:
             print "[DBI,%d/%d,%s] Trouble with launching/executing '%s'. Its execution did not finished. Probable cause is the back-end itself. Meaby you want to rerun it. popen returncode:%d, popen.wait.return:%d, dbi echo return code:%s"%(started,len(self.tasks),time.ctime(),command,task.p.returncode,task.p_wait_ret,task.dbi_return_status)
+            self.backend_failed+=1
+        elif task.dbi_return_status!=0:
+            self.jobs_failed+=1
             
     def run(self):
         print "[DBI] The Log file are under %s"%self.log_dir
@@ -448,7 +453,9 @@
             self.mt.join()
         else:
             print "[DBI] WARNING jobs not started!"
-                
+        print "[DBI] Their was %d jobs where the back-end failled"%(self.backend_failed)
+        print "[DBI] Their was %d jobs that returned a failure status."%(self.jobs_failed)
+        
 class DBIbqtools(DBIBase):
 
     def __init__( self, commands, **args ):



From nouiz at mail.berlios.de  Tue Sep 25 18:36:51 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 18:36:51 +0200
Subject: [Plearn-commits] r8105 - trunk/scripts
Message-ID: <200709251636.l8PGapms015206@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 18:36:50 +0200 (Tue, 25 Sep 2007)
New Revision: 8105

Modified:
   trunk/scripts/collectres
Log:
check for more error


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-09-25 16:06:24 UTC (rev 8104)
+++ trunk/scripts/collectres	2007-09-25 16:36:50 UTC (rev 8105)
@@ -118,6 +118,8 @@
         i=6
       else:
         i+=2 # skip the column name
+    if not res:
+      raise ValueError("No value selected for this file")
   else:
     raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    
   return res



From nouiz at mail.berlios.de  Tue Sep 25 20:11:34 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 20:11:34 +0200
Subject: [Plearn-commits] r8106 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709251811.l8PIBYI1017109@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 20:11:33 +0200 (Tue, 25 Sep 2007)
New Revision: 8106

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
Log:
BUGFIX on the progress bar


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-25 16:36:50 UTC (rev 8105)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-25 18:11:33 UTC (rev 8106)
@@ -634,6 +634,7 @@
     if( report_progress && stage < nstages )
         pb = new ProgressBar( "Training "+classname(),
                               nstages - stage );
+    int start_stage=stage;
 
     Vec costs_plus_time(train_costs.width()+2);
     costs_plus_time[train_costs.width()] = MISSING_VALUE;
@@ -666,7 +667,7 @@
             multiplyScaledAdd(all_params, 1-params_averaging_coeff,
                               params_averaging_coeff, all_mparams);
         if( pb )
-            pb->update( stage + 1 );
+            pb->update( stage + 1 - start_stage);
     }
     Profiler::end("training");
     Profiler::pl_profile_end("Totaltraining");



From nouiz at mail.berlios.de  Tue Sep 25 21:34:48 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 21:34:48 +0200
Subject: [Plearn-commits] r8107 - trunk/plearn_learners/meta
Message-ID: <200709251934.l8PJYmgo025433@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 21:34:48 +0200 (Tue, 25 Sep 2007)
New Revision: 8107

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
remove warning


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 18:11:33 UTC (rev 8106)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:34:48 UTC (rev 8107)
@@ -58,6 +58,7 @@
       initial_sum_weights(0.0),
       found_zero_error_weak_learner(0),
       target_error(0.5), 
+      provide_learner_expdir(false),
       output_threshold(0.5), 
       compute_training_error(1), 
       pseudo_loss_adaboost(1), 
@@ -65,8 +66,7 @@
       weight_by_resampling(1), 
       early_stopping(1),
       save_often(0),
-      forward_sub_learner_test_costs(false),
-      provide_learner_expdir(false)
+      forward_sub_learner_test_costs(false)
 { }
 
 PLEARN_IMPLEMENT_OBJECT(



From nouiz at mail.berlios.de  Tue Sep 25 21:41:58 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 21:41:58 +0200
Subject: [Plearn-commits] r8108 - trunk/plearn_learners/meta
Message-ID: <200709251941.l8PJfwY5025806@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 21:41:58 +0200 (Tue, 25 Sep 2007)
New Revision: 8108

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
better warning and added assert


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:34:48 UTC (rev 8107)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:41:58 UTC (rev 8108)
@@ -435,13 +435,14 @@
                 train_set->getExample(i, input, target, weight);
 #ifdef BOUNDCHECK
                 if(!(is_equal(target[0],0)||is_equal(target[0],1)))
-                    PLERROR("In AdaBoost::train() - target is not 0 or 1 in the training set. We implement only two class boosting.");
+                    PLERROR("In AdaBoost::train() - target is %f in the training set. It should be 0 or 1 as we implement only two class boosting.",target[0]);
 #endif
                 new_weak_learner->computeOutput(input,output);
                 real y_i=target[0];
                 real f_i=output[0];
                 if(conf_rated_adaboost)
                 {
+                    PLASSERT_MSG(f_i>=0,"In AdaBoost.cc::train() - output[0] should be >= 0 ");
                     // an error between 0 and 1 (before weighting)
                     examples_error[i] = 2*(f_i+y_i-2*f_i*y_i);
                     learners_error[stage] += example_weights[i]*
@@ -452,6 +453,7 @@
                     // an error between 0 and 1 (before weighting)
                     if (pseudo_loss_adaboost) 
                     {
+                        PLASSERT_MSG(f_i>=0,"In AdaBoost.cc::train() - output[0] should be >= 0 ");
                         examples_error[i] = 2*(f_i+y_i-2*f_i*y_i);
                         learners_error[stage] += example_weights[i]*
                             examples_error[i]/2;



From nouiz at mail.berlios.de  Tue Sep 25 21:51:18 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 21:51:18 +0200
Subject: [Plearn-commits] r8109 - trunk/plearn_learners/meta
Message-ID: <200709251951.l8PJpI7Q026197@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 21:51:17 +0200 (Tue, 25 Sep 2007)
New Revision: 8109

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
Added train cost nb_class_0, nb_class_1, avg_weight_0 and avg_weight_1


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:41:58 UTC (rev 8108)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:51:17 UTC (rev 8109)
@@ -649,12 +649,26 @@
                 PP<ProgressBar> pb;
                 if(report_progress) pb = new ProgressBar("computing weighted training error of whole model",n);
                 train_stats->forget();
-                static Vec err(1);
+                static Vec err(nTrainCosts());
+                int nb_class_0=0;
+                int nb_class_1=0;
+                real cum_weights_0=0;
+                real cum_weights_1=0;
+
                 for (int i=0;i<n;i++)
                 {
                     if(report_progress) pb->update(i);
                     train_set->getExample(i, input, target, weight);
                     computeCostsOnly(input,target,err);
+                    if(fast_is_equal(target[0],0.)){
+                        cum_weights_0 += example_weights[i];
+                        nb_class_0++;
+                    }else{
+                        cum_weights_1 += example_weights[i];
+                        nb_class_1++;
+                    }
+                    err[3]=cum_weights_0/nb_class_0;
+                    err[4]=cum_weights_1/nb_class_1;
                     train_stats->update(err);
                 }
                 train_stats->finalize();
@@ -690,7 +704,7 @@
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                        const Vec& target, Vec& costs) const
 {
-    costs.resize(3);
+    costs.resize(5);
 
     // First cost is negative log-likelihood...  output[0] is the likelihood
     // of the first class
@@ -707,6 +721,8 @@
                  "either 0 or 1; current target=%f", target[0]);
     costs[1] = exp(-1.0*sum_voting_weights*(2*output[0]-1)*(2*target[0]-1));
     costs[2] = costs[0];
+    costs[3] = train_stats->getStat("E[avg_weight_class_0]");
+    costs[4] = train_stats->getStat("E[avg_weight_class_1]");
     Vec tmp(weak_learner_template->nTestCosts());
     if(forward_sub_learner_test_costs){
         weak_learners.last()->computeCostsFromOutputs(input,output,target,tmp);
@@ -730,10 +746,12 @@
 
 TVec<string> AdaBoost::getTrainCostNames() const
 {
-    TVec<string> costs(3);
+    TVec<string> costs(5);
     costs[0] = "binary_class_error";
     costs[1] = "exp_neg_margin";
     costs[2] = "class_error";
+    costs[3] = "avg_weight_class_0";
+    costs[4] = "avg_weight_class_1";
     return costs;
 }
 



From nouiz at mail.berlios.de  Tue Sep 25 21:58:50 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 21:58:50 +0200
Subject: [Plearn-commits] r8110 - trunk/plearn_learners/meta
Message-ID: <200709251958.l8PJwoYI026655@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 21:58:50 +0200 (Tue, 25 Sep 2007)
New Revision: 8110

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
Added void computeOutput(input,output,nstage) that compute the output after the first nstage.
remotely exported this function as ouput computeOutput_at_stage(input,nstage)


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:51:17 UTC (rev 8109)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-25 19:58:50 UTC (rev 8110)
@@ -220,6 +220,23 @@
     inherited::declareOptions(ol);
 }
 
+////////////////////
+// declareMethods //
+////////////////////
+void AdaBoost::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "computeOutput_at_stage", &AdaBoost::remote_computeOutput_at_stage,
+        (BodyDoc("On a trained learner, this computes the output from the input with the first stage weaklearner. Their must be enought weaklearner that have been trained"),
+         ArgDoc ("input", "Input vector (should have width inputsize)"),
+         ArgDoc ("stage", "The number of stage to use to calculate the output"),
+         RetDoc ("Computed output (will have width outputsize)")));
+
+}
 void AdaBoost::build_()
 {
     if(conf_rated_adaboost && pseudo_loss_adaboost)
@@ -685,10 +702,26 @@
 
 void AdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
+    computeOutput(input,output,voting_weights.length());
+}
+void AdaBoost::computeOutput(const Vec& input, Vec& output, int nb_learner) const
+{
+    PLASSERT(nb_learner>0);
+    real local_sum_weight = sum_voting_weights;
+    if (nb_learner>voting_weights.length() and not found_zero_error_weak_learner){
+        PLERROR("AdaBoost::computeOutput - Asked to compute the output with more learner(%d) then currently learned %d",
+                nb_learner,voting_weights.length());
+    }else if(nb_learner>voting_weights.length()){
+        nb_learner=voting_weights.length();
+    }else if(nb_learner != voting_weights.length()){
+        local_sum_weight = 0;
+        for (int i=0;i<nb_learner;i++)
+            local_sum_weight += voting_weights[i];
+    }
     output.resize(weak_learner_template->outputsize());
     real sum_out=0;
     weak_learner_output.resize(output.size());
-    for (int i=0;i<voting_weights.length();i++)
+    for (int i=0;i<nb_learner;i++)
     {
         weak_learners[i]->computeOutput(input,weak_learner_output);
         if(!pseudo_loss_adaboost && !conf_rated_adaboost)
@@ -697,7 +730,7 @@
         else
             sum_out += weak_learner_output[0]*voting_weights[i];
     }
-    output[0] = sum_out/sum_voting_weights;
+    output[0] = sum_out/local_sum_weight;
     output.resize(1);
 }
 
@@ -755,6 +788,14 @@
     return costs;
 }
 
+//! Version of computeOutput that returns a result by value
+Vec AdaBoost::remote_computeOutput_at_stage(const Vec& input,const int stage) const
+{
+    tmp_output2.resize(outputsize());
+    computeOutput(input, tmp_output2, stage);
+    return tmp_output2;
+}
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2007-09-25 19:51:17 UTC (rev 8109)
+++ trunk/plearn_learners/meta/AdaBoost.h	2007-09-25 19:58:50 UTC (rev 8110)
@@ -52,7 +52,10 @@
 class AdaBoost: public PLearner
 {
     typedef PLearner inherited;
-  
+
+    //! Global storage to save memory allocations.
+    mutable Vec tmp_output2;
+    
 protected:
     // average weighted error of each learner
     Vec learners_error;
@@ -140,11 +143,18 @@
     // (Please implement in .cc)
     void build_();
 
+    // List of methods that are called by Remote Method Invocation.  Our
+    // convention is to have them start with the remote_ prefix.
+    Vec remote_computeOutput_at_stage(const Vec& input,const int stage) const;
+
 protected: 
     //! Declares this class' options
     // (Please implement in .cc)
     static void declareOptions(OptionList& ol);
 
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
+
 public:
 
     // ************************
@@ -186,6 +196,10 @@
     //! Computes the output from the input
     virtual void computeOutput(const Vec& input, Vec& output) const;
 
+    //! Computes the output from the input with a specific number of learner
+    //! This way we don't need to save the learner at each stage, we can save just the last one
+    void computeOutput(const Vec& input, Vec& output, int nb_learner) const;
+
     //! Computes the costs from already computed output. 
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output, 
                                          const Vec& target, Vec& costs) const;



From nouiz at mail.berlios.de  Tue Sep 25 22:30:23 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 25 Sep 2007 22:30:23 +0200
Subject: [Plearn-commits] r8111 - trunk/scripts
Message-ID: <200709252030.l8PKUN1g028320@sheep.berlios.de>

Author: nouiz
Date: 2007-09-25 22:30:23 +0200 (Tue, 25 Sep 2007)
New Revision: 8111

Modified:
   trunk/scripts/collectres
Log:
bugfix: The truth value for a non-empty array is ambiguous


Modified: trunk/scripts/collectres
===================================================================
--- trunk/scripts/collectres	2007-09-25 19:58:50 UTC (rev 8110)
+++ trunk/scripts/collectres	2007-09-25 20:30:23 UTC (rev 8111)
@@ -118,7 +118,7 @@
         i=6
       else:
         i+=2 # skip the column name
-    if not res:
+    if not res.any():
       raise ValueError("No value selected for this file")
   else:
     raise ValueError("Invalid <location-spec> mode, expected 'pos', 'mincol', or 'col', got "+loc_mode)    



From louradou at mail.berlios.de  Wed Sep 26 19:13:55 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Wed, 26 Sep 2007 19:13:55 +0200
Subject: [Plearn-commits] r8112 - trunk/plearn_learners/online
Message-ID: <200709261713.l8QHDtmP015645@sheep.berlios.de>

Author: louradou
Date: 2007-09-26 19:13:54 +0200 (Wed, 26 Sep 2007)
New Revision: 8112

Modified:
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
Log:
resolved a bug in StackedAutoassociators
(asking the train stats was a problem)



Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-09-25 20:30:23 UTC (rev 8111)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2007-09-26 17:13:54 UTC (rev 8112)
@@ -680,6 +680,12 @@
 
     PP<ProgressBar> pb;
 
+    if( !train_stats )
+    {
+        train_stats = new VecStatsCollector();
+        train_stats->setFieldNames(getTrainCostNames());
+    }
+    
     // clear stats of previous epoch
     train_stats->forget();
 



From manzagop at mail.berlios.de  Thu Sep 27 16:23:27 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 27 Sep 2007 16:23:27 +0200
Subject: [Plearn-commits] r8113 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709271423.l8RENRuX027765@sheep.berlios.de>

Author: manzagop
Date: 2007-09-27 16:23:27 +0200 (Thu, 27 Sep 2007)
New Revision: 8113

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
   trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
Log:
Update related to work on pvgrad.
- added (commented out) code for gathering soeme development stats.
- renamed some pv variables related to the parameters so as to have both
the "all" and the "layer" views on them (same grouping as for usual parameters).


Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-26 17:13:54 UTC (rev 8112)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.cc	2007-09-27 14:23:27 UTC (rev 8113)
@@ -36,7 +36,7 @@
 
 /*! \file NatGradNNet.cc */
 
-
+//#include <sstream>  // *stat* for output
 #include "NatGradNNet.h"
 #include <plearn/math/pl_erf.h>
 
@@ -398,6 +398,7 @@
     all_mparams.resize(n_params);
     all_params_gradient.resize(n_params);
     all_params_delta.resize(n_params);
+    //all_params_cum_gradient.resize(n_params); // *stat*
 
     // depending on how parameters are grouped on the first layer
     int n_groups = params_natgrad_per_input_template ? (n_neurons-layer_sizes[1]+layer_sizes[0]+1) : n_neurons;
@@ -560,10 +561,11 @@
     deepCopyField(layer_params_delta, copies);
 
     deepCopyField(pv_gradstats, copies);
-    deepCopyField(pv_stepsizes, copies);
-    deepCopyField(pv_stepsigns, copies);
+    deepCopyField(pv_all_stepsizes, copies);
+    deepCopyField(pv_all_stepsigns, copies);
+    deepCopyField(pv_all_intstepsigns, copies);
+    //deepCopyField(pv_all_nsamples, copies); // *stat*
 
-
 /*
     deepCopyField(, copies);
 */
@@ -599,13 +601,42 @@
     {
         pv_gradstats->forget();
         int n = all_params.length();
-        pv_stepsizes.resize(n);
-        pv_stepsizes.fill(pv_initial_stepsize);
-        pv_stepsigns.resize(n);
-        pv_stepsigns.fill(true);
-        all_ns.resize(n);
+        pv_all_stepsizes.resize(n);
+        pv_all_stepsizes.fill(pv_initial_stepsize);
+        pv_all_stepsigns.resize(n);
+        pv_all_stepsigns.fill(true);    // TODO should be init to undetermined
+        pv_all_intstepsigns.resize(n);
+        pv_all_intstepsigns.fill(0);
+        //pv_all_nsamples.resize(n);    // *stat*
+
+        // Get some structure on the previous Vecs
+        pv_layer_stepsizes.resize(n_layers-1);
+        pv_layer_stepsigns.resize(n_layers-1);
+        pv_layer_intstepsigns.resize(n_layers-1);
+        //pv_layer_nsamples.resize(n_layers-1); // *stat*
+        for (int i=0,p=0;i<n_layers-1;i++)
+        {
+            int np=layer_sizes[i+1]*(1+layer_sizes[i]);
+            pv_layer_stepsizes[i]=pv_all_stepsizes.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            pv_layer_stepsigns[i]=pv_all_stepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            pv_layer_intstepsigns[i]=pv_all_intstepsigns.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);
+            //pv_layer_nsamples[i]=pv_all_nsamples.subVec(p,np).toMat(layer_sizes[i+1],layer_sizes[i]+1);   // *stat*
+            p+=np;
+        }
     }
 
+    // *stat*
+    /*if( pa_gradstats.length() == 0 )    {
+        pa_gradstats.resize(noutputs);
+        for(int i=0; i<noutputs; i++)   {
+            (pa_gradstats[i]).compute_covariance = true;
+        }
+    }   else    {
+        for(int i=0; i<noutputs; i++)   {
+            (pa_gradstats[i]).forget();
+        }
+    }*/
+
 }
 
 void NatGradNNet::train()
@@ -642,6 +673,10 @@
     Vec costs = costs_plus_time.subVec(0,train_costs.width());
     int nsamples = train_set->length();
 
+    // *stat* - Need some stats for pvgrad analysis
+    //sum_gradient_norms = 0.0;
+    //all_params_cum_gradient.fill(0.0);
+    
     for( ; stage<nstages; stage++)
     {
         int sample = stage % nsamples;
@@ -668,6 +703,10 @@
                               params_averaging_coeff, all_mparams);
         if( pb )
             pb->update( stage + 1 - start_stage);
+
+        // *stat*
+        //(pa_gradstats[(int)targets(0,0)]).update( all_params_gradient );
+
     }
     Profiler::end("training");
     Profiler::pl_profile_end("Totaltraining");
@@ -683,6 +722,7 @@
     train_stats->update( costs_plus_time );
     train_stats->finalize(); // finalize statistics for this epoch
 
+    // *stat*
     // profiling gradient correlation
     //if( g_corrprof )    {
     //    PLASSERT( corr_profiling_end <= nstages );
@@ -690,6 +730,22 @@
     //    ng_corrprof->printAndReset();
     //}
 
+    // *stat* - Need some stats for pvgrad analysis
+    // The SGrad stats include the learning rate.
+    //cout << "sum_gradient_norms " << sum_gradient_norms 
+    //     << " norm(all_params_cum_gradient,2.0) " << norm(all_params_cum_gradient,2.0) << endl;
+
+    // *stat*
+    //for(int i=0; i<noutputs; i++)   {
+    //    ofstream fd_cov;
+    //    stringstream ss;
+    //    ss << "cov" << i+1 << ".txt";
+    //    fd_cov.open(ss.str().c_str());
+    //    fd_cov << (pa_gradstats[i]).getCovariance();
+    //    fd_cov.close();
+    //}
+    
+
 }
 
 void NatGradNNet::onlineStep(int t, const Mat& targets,
@@ -787,6 +843,18 @@
                             neuron_extended_outputs_per_layer[i-1],false,
                             -layer_lrate_factor*lrate/minibatch_size,1);
             Profiler::pl_profile_end("ProducScaleAccOnlineStep");
+
+            // Don't do the stochastic trick - remember the gradient times its
+            // learning rate
+            /*productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            -layer_lrate_factor*lrate/minibatch_size,0);
+            layer_params[i-1] += layer_params_gradient[i-1];*/
+  
+            // *stat* - compute and store the gradient
+            /*productScaleAcc(layer_params_gradient[i-1],next_neurons_gradient,true,
+                            neuron_extended_outputs_per_layer[i-1],false,
+                            1,0);*/
         }
     }
     if (use_pvgrad)
@@ -818,12 +886,6 @@
         multiplyAcc(all_params,all_params_delta,-lrate); // update
     }
 
-    // profiling gradient correlation
-    //if( (t>=corr_profiling_start) && (t<=corr_profiling_end) && g_corrprof )    {
-    //    (*g_corrprof)(all_params_gradient);
-    //    (*ng_corrprof)(all_params_delta);
-    //}
-
     // Output layer L1 regularization
     if( output_layer_L1_penalty_factor != 0. )    {
         real L1_delta = lrate * output_layer_L1_penalty_factor;
@@ -841,33 +903,60 @@
         }
     }
 
+    // profiling gradient correlation
+    //if( (t>=corr_profiling_start) && (t<=corr_profiling_end) && g_corrprof )    {
+    //    (*g_corrprof)(all_params_gradient);
+    //    (*ng_corrprof)(all_params_delta);
+    //}
+
+    // temporary - Need some stats for pvgrad analysis
+    // SGrad stats. This includes the learning rate.
+    /*if( ! use_pvgrad )  {
+        sum_gradient_norms += norm(all_params_gradient,2.0);
+        all_params_cum_gradient += all_params_gradient;
+    }*/
+
+
     // Ouput for profiling: weights
-    // horribly inefficient!
+    // horribly inefficient! Anyway the Mat output is done one number at a
+    // time...
+    // do it locally, say on /part/01/Tmp
 /*    ofstream fd_params;
     fd_params.open("params.txt", ios::app);
-    fd_params << all_params << endl;
+    fd_params << layer_params[0](0) << " " << layer_params[1](0) << endl;
     fd_params.close();
 
     ofstream fd_gradients;
     fd_gradients.open("gradients.txt", ios::app);
-    fd_gradients << all_params_gradient << endl;
+    //fd_gradients << all_params_gradient << endl;
+    fd_gradients << layer_params_gradient[0](0) << " " <<layer_params_gradient[1](0) << endl;
     fd_gradients.close();
 */
 }
 
+void NatGradNNet::paGradUpdate()
+{
+
+}
+
 void NatGradNNet::pvGradUpdate()
 {
     int np = all_params_gradient.length();
-    if(pv_stepsizes.length()==0)
+    if(pv_all_stepsizes.length()==0)
     {
-        pv_stepsizes.resize(np);
-        pv_stepsizes.fill(pv_initial_stepsize);
-        pv_stepsigns.resize(np);
-        pv_stepsigns.fill(true);
-        // profiling
-        all_ns.resize(np);
+        pv_all_stepsizes.resize(np);
+        pv_all_stepsizes.fill(pv_initial_stepsize);
+        pv_all_stepsigns.resize(np);
+        pv_all_stepsigns.fill(true);
+        pv_all_intstepsigns.resize(np);
+        pv_all_intstepsigns.fill(0);
+        //pv_all_nsamples.resize(np);   // *stat*
     }
     pv_gradstats->update(all_params_gradient);
+
+    // *stat* - Need some stats for pvgrad analysis
+    //real gradient_squared_sum = 0.0;
+
     for(int k=0; k<np; k++)
     {
         StatsCollector& st = pv_gradstats->getStats(k);
@@ -878,8 +967,10 @@
             real e = st.stderror();
 
             // test to see if solve numerical problems
-            if( fabs(m) < 1e-15 || e < 1e-15 )
+            if( fabs(m) < 1e-15 || e < 1e-15 )  {
+                cout << "small mean and error ratio." << endl;
                 continue;
+            }
 
             real prob_pos = gauss_01_cum(m/e);
             real prob_neg = 1.-prob_pos;
@@ -889,55 +980,76 @@
                 // gradient is positive
                 if(prob_pos>=pv_required_confidence)
                 {
-                    pv_stepsizes[k] *= (pv_stepsigns[k]?pv_acceleration:pv_deceleration);
-                    if( pv_stepsizes[k] > pv_max_stepsize )
-                        pv_stepsizes[k] = pv_max_stepsize;
-                    else if( pv_stepsizes[k] < pv_min_stepsize )
-                        pv_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] -= pv_stepsizes[k];
-                    pv_stepsigns[k] = true;
+                    pv_all_stepsizes[k] *= (pv_all_stepsigns[k]?pv_acceleration:pv_deceleration);
+                    if( pv_all_stepsizes[k] > pv_max_stepsize )
+                        pv_all_stepsizes[k] = pv_max_stepsize;
+                    else if( pv_all_stepsizes[k] < pv_min_stepsize )
+                        pv_all_stepsizes[k] = pv_min_stepsize;
+                    all_params[k] -= pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = true;
+                    pv_all_intstepsigns[k] = -1;
                     st.forget();
+
+                    // *stat* - Need some stats for pvgrad analysis
+                    //gradient_squared_sum += pv_all_stepsizes[k]*pv_all_stepsizes[k];
+                    //all_params_cum_gradient[k] -= pv_all_stepsizes[k];
+
                 }
                 // gradient is negative
                 else if(prob_neg>=pv_required_confidence)
                 {
-                    pv_stepsizes[k] *= ((!pv_stepsigns[k])?pv_acceleration:pv_deceleration);
-                    if( pv_stepsizes[k] > pv_max_stepsize )
-                        pv_stepsizes[k] = pv_max_stepsize;
-                    else if( pv_stepsizes[k] < pv_min_stepsize )
-                        pv_stepsizes[k] = pv_min_stepsize;
-                    all_params[k] += pv_stepsizes[k];
-                    pv_stepsigns[k] = false;
+                    pv_all_stepsizes[k] *= ((!pv_all_stepsigns[k])?pv_acceleration:pv_deceleration);
+                    if( pv_all_stepsizes[k] > pv_max_stepsize )
+                        pv_all_stepsizes[k] = pv_max_stepsize;
+                    else if( pv_all_stepsizes[k] < pv_min_stepsize )
+                        pv_all_stepsizes[k] = pv_min_stepsize;
+                    all_params[k] += pv_all_stepsizes[k];
+                    pv_all_stepsigns[k] = false;
+                    pv_all_intstepsigns[k] = 1;
                     st.forget();
+
+                    // *stat* - Need some stats for pvgrad analysis
+                    //gradient_squared_sum += pv_all_stepsizes[k]*pv_all_stepsizes[k];
+                    //all_params_cum_gradient[k] += pv_all_stepsizes[k];
+                
+                }   
+                // no step
+                else    {
+                    pv_all_intstepsigns[k] = 0;
                 }
             }
             else  // random sample update direction (sign)
             {
                 bool ispos = (random_gen->binomial_sample(prob_pos)>0);
                 if(ispos) // picked positive
-                    all_params[k] += pv_stepsizes[k];
+                    all_params[k] += pv_all_stepsizes[k];
                 else  // picked negative
-                    all_params[k] -= pv_stepsizes[k];
-                pv_stepsizes[k] *= (pv_stepsigns[k]==ispos) ?pv_acceleration :pv_deceleration;
-                pv_stepsigns[k] = ispos;
+                    all_params[k] -= pv_all_stepsizes[k];
+                pv_all_stepsizes[k] *= (pv_all_stepsigns[k]==ispos) ?pv_acceleration :pv_deceleration;
+                pv_all_stepsigns[k] = ispos;
                 st.forget();
             }
         }
-
-        // profiling
-        all_ns[k] = ns;
+        //pv_all_nsamples[k] = ns; // *stat*
     }
 
+    // *stat* - Need some stats for pvgrad analysis
+    //sum_gradient_norms += sqrt( gradient_squared_sum );
+
     // Ouput for profiling: step sizes and number of samples
     // horribly inefficient!
 /*    ofstream fd_ss;
     fd_ss.open("step_sizes.txt", ios::app);
-    fd_ss << pv_stepsizes << endl;
+    fd_ss << pv_layer_stepsizes[0](0) << " " << pv_layer_stepsizes[1](0) << endl;
     fd_ss.close();
     ofstream fd_ns;
     fd_ns.open("nsamples.txt", ios::app);
-    fd_ns << all_ns << endl;
+    fd_ns << pv_layer_nsamples[0](0) << " " << pv_layer_nsamples[1](0) << endl;
     fd_ns.close();
+    ofstream fd_ssgn;
+    fd_ssgn.open("step_signs.txt", ios::app);
+    fd_ssgn << pv_layer_intstepsigns[0](0) << " " << pv_layer_intstepsigns[1](0) << endl;
+    fd_ssgn.close();
 */
 
 }

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-09-26 17:13:54 UTC (rev 8112)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/NatGradNNet.h	2007-09-27 14:23:27 UTC (rev 8113)
@@ -43,7 +43,7 @@
 #include <plearn_learners/generic/PLearner.h>
 #include <plearn_learners/generic/GradientCorrector.h>
 #include <plearn/sys/Profiler.h>
-//#include "CorrelationProfiler.h"
+//#include "CorrelationProfiler.h" // *stat*
 
 namespace PLearn {
 
@@ -140,9 +140,28 @@
     // average with this coefficient (near 0 for very slow averaging)
     real activation_statistics_moving_average_coefficient;
 
+    // *stat*
+    // Temporary stuff for getting a clue as to what's going on
+    // Look for the marker '*stat*' in the code
+
+    // -Options-
     //! Stages for profiling the correlation between the gradients' elements
     //int corr_profiling_start, corr_profiling_end;
 
+    // -Not options-
+    //PP<CorrelationProfiler> g_corrprof, ng_corrprof;    // for optional gradient correlation profiling
+    //real sum_gradient_norms;     // holds sum of the gradient norms - reset at each epoch
+    //Vec all_params_cum_gradient; // holds the sum of the gradients - reset at each epoch
+
+    //! To see differences between class gradient covariances
+    //TVec<VecStatsCollector> pa_gradstats;   // one VecStatsCollector per output class
+
+    //! Holds the number of samples gathered for each weight
+    //TVec<int> pv_all_nsamples;
+    //TVec< TMat<int> > pv_layer_nsamples;
+
+    // *stat* end
+
 public:
     //*************************************************************
     //*** Members used for Pascal Vincent's gradient technique  ***
@@ -169,21 +188,24 @@
     // each parameter based on the estimated probability of it being positive or
     // negative.
     bool pv_random_sample_step;
-    
 
 protected:
     //! accumulated statistics of gradients on each parameter.
     PP<VecStatsCollector> pv_gradstats;
 
     //! The step size (absolute value) to be taken for each parameter.
-    Vec pv_stepsizes;
+    Vec pv_all_stepsizes;
+    TVec<Mat> pv_layer_stepsizes;
 
     //! Indicates whether the previous step was positive (true) or negative (false)
-    TVec<bool> pv_stepsigns;
+    TVec<bool> pv_all_stepsigns;
+    TVec< TMat<bool> > pv_layer_stepsigns;
 
-    // profiling
-    TVec<int> all_ns;
+    //! Temporary add-on. Allows an undetermined signed value (zero).
+    TVec<int> pv_all_intstepsigns;
+    TVec< TMat<int> > pv_layer_intstepsigns;
 
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -304,6 +326,10 @@
     //! gradient computation and weight update in Pascal Vincent's gradient technique
     void pvGradUpdate();
 
+    //! a related idea 
+    void paGradUpdate();
+
+
 private:
     //#####  Private Member Functions  ########################################
 
@@ -333,7 +359,8 @@
     Vec example_weights; // one element per example in a minibatch
     Mat train_costs; // one row per example in a minibatch
 
-    //PP<CorrelationProfiler> g_corrprof, ng_corrprof;    // for optional gradient correlation profiling
+
+    
 };
 
 // Declares a few other classes and functions related to this class



From manzagop at mail.berlios.de  Thu Sep 27 16:24:51 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 27 Sep 2007 16:24:51 +0200
Subject: [Plearn-commits] r8114 - trunk/plearn_learners/generic/EXPERIMENTAL
Message-ID: <200709271424.l8REOpjq027813@sheep.berlios.de>

Author: manzagop
Date: 2007-09-27 16:24:51 +0200 (Thu, 27 Sep 2007)
New Revision: 8114

Modified:
   trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc
Log:
Minor changes to comments and output.

Modified: trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc
===================================================================
--- trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc	2007-09-27 14:23:27 UTC (rev 8113)
+++ trunk/plearn_learners/generic/EXPERIMENTAL/CorrelationProfiler.cc	2007-09-27 14:24:51 UTC (rev 8114)
@@ -87,7 +87,7 @@
 void CorrelationProfiler::printAndReset()
 {
 
-    cout << its_name << " - correlation based on " << n << " samples." << endl;
+    cout << its_name << " - stats based on " << n << " samples." << endl;
 
     // *** Get mean
     sum_v /= n;
@@ -98,7 +98,15 @@
     // *** Get the centered covariance
     externalProductScaleAcc(A, sum_v, sum_v, -1.0);
 
-    // *** Get correlation by dividing by the product of the standard deviations
+    // TODO check opening
+    string file_name;
+    file_name = its_name + "_covariance.txt";
+    ofstream fd;
+    fd.open( file_name.c_str() );
+    A.print(fd);
+    fd.close();
+
+/*    // *** Get correlation by dividing by the product of the standard deviations
     Vec diagA = diag( A );
     // TODO Not very efficient. Isn't there a lapack function for this. 
     for(int i=0; i<A.length(); i++)   {
@@ -117,7 +125,7 @@
     ofstream fd;
     fd.open( file_name.c_str() );
     A.print(fd);
-    fd.close();
+    fd.close();*/
 
     // *** Reset
     reset();



From tihocan at mail.berlios.de  Thu Sep 27 16:25:42 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Sep 2007 16:25:42 +0200
Subject: [Plearn-commits] r8115 - trunk/plearn_learners/hyper
Message-ID: <200709271425.l8REPgRd027888@sheep.berlios.de>

Author: tihocan
Date: 2007-09-27 16:25:42 +0200 (Thu, 27 Sep 2007)
New Revision: 8115

Modified:
   trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
Log:
Typo and indent fix

Modified: trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2007-09-27 14:24:51 UTC (rev 8114)
+++ trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2007-09-27 14:25:42 UTC (rev 8115)
@@ -74,7 +74,7 @@
                   "a list of values to try in sequence ");
     declareOption(ol, "range", &EarlyStoppingOracle::range, OptionBase::buildoption,
                   "a numerical range of the form [ start, end ] or [ start, end, step ]\n"
-                  "WARNING: end is not included! ");
+                  "WARNING: end is not included!");
 
     declareOption(ol, "min_value", &EarlyStoppingOracle::min_value, OptionBase::buildoption,
                   "minimum allowed error beyond which we stop\n");
@@ -120,8 +120,10 @@
             step = range[2];
         for(real x = range[0]; x<range[1]; x+=step)
             option_values.append(tostring(x));
-        if(range[0]==range[1])
-            PLWARNING("In EarlyStoppingOracle::build_ - no range selected. Meaby you forgot that the end part of the range is not included!");
+        if(fast_exact_is_equal(range[0], range[1]))
+            PLWARNING("In EarlyStoppingOracle::build_ - no range selected. "
+                    "Maybe you forgot that the end part of the range is not "
+                    "included!");
     }
 }
 



From manzagop at mail.berlios.de  Thu Sep 27 16:30:58 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 27 Sep 2007 16:30:58 +0200
Subject: [Plearn-commits] r8116 - trunk/plearn_learners/generic
Message-ID: <200709271430.l8REUwN6028272@sheep.berlios.de>

Author: manzagop
Date: 2007-09-27 16:30:58 +0200 (Thu, 27 Sep 2007)
New Revision: 8116

Modified:
   trunk/plearn_learners/generic/NatGradEstimator.cc
   trunk/plearn_learners/generic/NatGradEstimator.h
Log:
Added a minimal lambda value as an option that is enforced.

Modified: trunk/plearn_learners/generic/NatGradEstimator.cc
===================================================================
--- trunk/plearn_learners/generic/NatGradEstimator.cc	2007-09-27 14:25:42 UTC (rev 8115)
+++ trunk/plearn_learners/generic/NatGradEstimator.cc	2007-09-27 14:30:58 UTC (rev 8116)
@@ -71,6 +71,7 @@
     /* ### Initialize all fields to their default value */
     : cov_minibatch_size(10),
       init_lambda(1.),
+      min_lambda(0.001),
       n_eigen(10),
       gamma(0.99),
       renormalize(true),
@@ -126,6 +127,10 @@
                   OptionBase::buildoption,
                   "Initial variance. The first covariance is assumed to be\n"
                   "init_lambda times the identity. Default = 1.\n");
+    declareOption(ol, "min_lambda", &NatGradEstimator::min_lambda,
+                  OptionBase::buildoption,
+                  "Minimal lambda value allowed in lambda's update from an eigendecomposition.\n");
+
     declareOption(ol, "regularizer", &NatGradEstimator::init_lambda,
                   OptionBase::buildoption,
                   "Proxy for option init_lambda (different name to avoid python problems).\n");
@@ -258,9 +263,15 @@
         // get eigen-decomposition, with one more eigen-x than necessary to check if coherent with lambda
         //if (save_G)
         //    saveAscii("G.amat",G);
-        eigenVecOfSymmMat(G,n_eigen+1,D,Vt);
+
+        // try to regularize G
+//        for (int j=0;j<n+1;j++)
+//            G(j,j) += 0.001;
+
+
+//        eigenVecOfSymmMat(G,n_eigen,D,Vt);
         // Get all eigenvalues -> this resizes D and Vt, but it doesn't matter
-//        eigenVecOfSymmMat(G,G.width(),D,Vt);
+        eigenVecOfSymmMat(G,G.width(),D,Vt);
 //        cout << "-= " << t << " =-" << endl;
 //        cout << D.length() << " eigenvalues = " << D << endl;
 
@@ -302,6 +313,7 @@
         if( update_lambda_from_eigen )    {
 //            if (D[n_eigen-1]>lambda)
 //                cout << " *** Last lambda too small? *** lambda, last eigen : " << lambda << ", " << D[n_eigen-1] << endl;
+
 /*            float big_eig = D[0];
             bool cont = true;
             for (int j=0;j<n_eigen && cont;j++) {
@@ -310,8 +322,19 @@
                     cont = false;
                 }
             }
-            if(cont)*/
+            if(cont)
                 lambda = D[n_eigen-1];
+
+*/
+            
+            lambda =  D[n_eigen-1];
+
+
+
+            if( lambda < min_lambda )
+                lambda = min_lambda;
+
+
 //          if (D[n_eigen]<1e-6)
 //              PLWARNING("NatGradEstimator: updating lambda with small value %g\n",D[n_eigen]);
 //          lambda = D[n_eigen-1];

Modified: trunk/plearn_learners/generic/NatGradEstimator.h
===================================================================
--- trunk/plearn_learners/generic/NatGradEstimator.h	2007-09-27 14:25:42 UTC (rev 8115)
+++ trunk/plearn_learners/generic/NatGradEstimator.h	2007-09-27 14:30:58 UTC (rev 8116)
@@ -67,6 +67,9 @@
 
     //! regularization coefficient of covariance matrix (initial values on diagonal)
     real init_lambda;
+    //! Minimal value allowed for lambda in its update from an
+    //eigendecomposition
+    real min_lambda;
 
     //! number of eigenvectors-eigenvalues that is preserved of the covariance matrix
     int n_eigen;



From manzagop at mail.berlios.de  Thu Sep 27 16:32:45 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Thu, 27 Sep 2007 16:32:45 +0200
Subject: [Plearn-commits] r8117 - trunk/plearn_learners_experimental/netflix
Message-ID: <200709271432.l8REWj2f028375@sheep.berlios.de>

Author: manzagop
Date: 2007-09-27 16:32:45 +0200 (Thu, 27 Sep 2007)
New Revision: 8117

Modified:
   trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
Log:
Change warning msg.

Modified: trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc
===================================================================
--- trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-09-27 14:30:58 UTC (rev 8116)
+++ trunk/plearn_learners_experimental/netflix/NxProfileLearner.cc	2007-09-27 14:32:45 UTC (rev 8117)
@@ -138,10 +138,10 @@
 
     if( L1_penalty_factor < 0. )
         PLWARNING("NxProfileLearner::build:\n"
-                    "L1_penalty_factor is negative!\n", slr);
+                    "L1_penalty_factor is negative!\n");
     if( L2_penalty_factor < 0. )
         PLWARNING("NxProfileLearner::build:\n"
-                    "L2_penalty_factor is negative!\n", slr);
+                    "L2_penalty_factor is negative!\n");
     if( (slr*L2_penalty_factor) > 1. )
         PLWARNING("NxProfileLearner::build:\n"
                     "slr = %f is too large for L2_penalty_factor!\n", slr);



From tihocan at mail.berlios.de  Thu Sep 27 16:39:35 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Sep 2007 16:39:35 +0200
Subject: [Plearn-commits] r8118 - trunk/plearn_learners/meta
Message-ID: <200709271439.l8REdZg3028830@sheep.berlios.de>

Author: tihocan
Date: 2007-09-27 16:39:35 +0200 (Thu, 27 Sep 2007)
New Revision: 8118

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
   trunk/plearn_learners/meta/AdaBoost.h
Log:
- Typo and indentation fixes
- Added missing deep copy statement
- Added comment for new method 'remote_computeOutput_at_stage'


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2007-09-27 14:32:45 UTC (rev 8117)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2007-09-27 14:39:35 UTC (rev 8118)
@@ -231,9 +231,11 @@
 
     declareMethod(
         rmm, "computeOutput_at_stage", &AdaBoost::remote_computeOutput_at_stage,
-        (BodyDoc("On a trained learner, this computes the output from the input with the first stage weaklearner. Their must be enought weaklearner that have been trained"),
+        (BodyDoc("On a trained learner, this computes the output from the "
+                 "input with the first stage weak learner. There must be "
+                 "enough weak learners that have been trained"),
          ArgDoc ("input", "Input vector (should have width inputsize)"),
-         ArgDoc ("stage", "The number of stage to use to calculate the output"),
+         ArgDoc ("stage", "The number of stage to use to compute the output"),
          RetDoc ("Computed output (will have width outputsize)")));
 
 }
@@ -255,6 +257,7 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
+    deepCopyField(tmp_output2,              copies);
     deepCopyField(learners_error,           copies);
     deepCopyField(example_weights,          copies);
     deepCopyField(weak_learner_output,      copies);
@@ -789,7 +792,8 @@
 }
 
 //! Version of computeOutput that returns a result by value
-Vec AdaBoost::remote_computeOutput_at_stage(const Vec& input,const int stage) const
+Vec AdaBoost::remote_computeOutput_at_stage(const Vec& input,
+                                            const int stage) const
 {
     tmp_output2.resize(outputsize());
     computeOutput(input, tmp_output2, stage);

Modified: trunk/plearn_learners/meta/AdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.h	2007-09-27 14:32:45 UTC (rev 8117)
+++ trunk/plearn_learners/meta/AdaBoost.h	2007-09-27 14:39:35 UTC (rev 8118)
@@ -140,13 +140,16 @@
 
 private: 
     //! This does the actual building. 
-    // (Please implement in .cc)
     void build_();
 
     // List of methods that are called by Remote Method Invocation.  Our
     // convention is to have them start with the remote_ prefix.
-    Vec remote_computeOutput_at_stage(const Vec& input,const int stage) const;
 
+    //! Compute output at a given stage. Note that the returned vector may
+    //! be modified by subsequent use of this object, and thus should be copied
+    //! if it needs to be stored safely.
+    Vec remote_computeOutput_at_stage(const Vec& input, const int stage) const;
+
 protected: 
     //! Declares this class' options
     // (Please implement in .cc)



From tihocan at mail.berlios.de  Thu Sep 27 16:42:14 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Sep 2007 16:42:14 +0200
Subject: [Plearn-commits] r8119 - trunk/plearn/base
Message-ID: <200709271442.l8REgEVf029008@sheep.berlios.de>

Author: tihocan
Date: 2007-09-27 16:42:14 +0200 (Thu, 27 Sep 2007)
New Revision: 8119

Modified:
   trunk/plearn/base/stringutils.h
Log:
- Fixed bug recently added in 'string_begins_with'
- Fixed illogical switch between 'm' and 'n' in 'string_ends_with' (although that was not a bug)
- Added comments for methods 'string_begins_with' and 'string_ends_with'


Modified: trunk/plearn/base/stringutils.h
===================================================================
--- trunk/plearn/base/stringutils.h	2007-09-27 14:39:35 UTC (rev 8118)
+++ trunk/plearn/base/stringutils.h	2007-09-27 14:42:14 UTC (rev 8119)
@@ -137,16 +137,19 @@
 //! replaces all backslashes with slash
 string backslash_to_slash(string str);
 
+//! Return true iff string 's' begins with string 'beginning'.
 inline bool string_begins_with(const string& s, const string& beginning)
 {
     string::size_type n = beginning.size();
-    return (s.size() >= n  &&  beginning == s.substr(0,n-1) );
+    return (s.size() >= n && beginning == s.substr(0, n));
 }
+
+//! Return true iff string 's' ends with string 'end'.
 inline bool string_ends_with(const string& s, const string& end)
 {
     string::size_type n = end.size();
     string::size_type m = s.size();
-    return (m >= n  &&  end == s.substr(m-n,m) );
+    return (m >= n  &&  end == s.substr(m-n, n) );
 }
   
 //! replaces all occurences of searchstr in the text by replacestr



From tihocan at mail.berlios.de  Thu Sep 27 17:02:42 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 27 Sep 2007 17:02:42 +0200
Subject: [Plearn-commits] r8120 - in trunk: commands plearn/base/test
	plearn/base/test/.pytest plearn/base/test/.pytest/PL_stringutils
	plearn/base/test/.pytest/PL_stringutils/expected_results
Message-ID: <200709271502.l8RF2g5w029873@sheep.berlios.de>

Author: tihocan
Date: 2007-09-27 17:02:41 +0200 (Thu, 27 Sep 2007)
New Revision: 8120

Added:
   trunk/plearn/base/test/.pytest/PL_stringutils/
   trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/
   trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/PLStringutilsTest.psave
   trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/RUN.log
   trunk/plearn/base/test/PLStringutilsTest.cc
   trunk/plearn/base/test/PLStringutilsTest.h
Modified:
   trunk/commands/plearn_tests_inc.h
   trunk/plearn/base/test/pytest.config
Log:
Added regression test for bug fixed in r8119

Modified: trunk/commands/plearn_tests_inc.h
===================================================================
--- trunk/commands/plearn_tests_inc.h	2007-09-27 14:42:14 UTC (rev 8119)
+++ trunk/commands/plearn_tests_inc.h	2007-09-27 15:02:41 UTC (rev 8120)
@@ -54,6 +54,7 @@
  * PTest *
  *********/
 #include <plearn/base/test/PLCheckTest.h>
+#include <plearn/base/test/PLStringutilsTest.h>
 #include <plearn/base/test/PP/PPTest.h>
 #include <plearn/base/test/ObjectGraphIterator/ObjectGraphIteratorTest.h>
 #include <plearn/io/test/PLLogTest.h>


Property changes on: trunk/plearn/base/test/.pytest/PL_stringutils
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/PLStringutilsTest.psave
===================================================================
--- trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/PLStringutilsTest.psave	2007-09-27 14:42:14 UTC (rev 8119)
+++ trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/PLStringutilsTest.psave	2007-09-27 15:02:41 UTC (rev 8120)
@@ -0,0 +1,3 @@
+*1 ->PLStringutilsTest(
+save = 1 ;
+save_path = ""  )

Added: trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/RUN.log	2007-09-27 14:42:14 UTC (rev 8119)
+++ trunk/plearn/base/test/.pytest/PL_stringutils/expected_results/RUN.log	2007-09-27 15:02:41 UTC (rev 8120)
@@ -0,0 +1,5 @@
+1
+0
+1
+0
+0

Added: trunk/plearn/base/test/PLStringutilsTest.cc
===================================================================
--- trunk/plearn/base/test/PLStringutilsTest.cc	2007-09-27 14:42:14 UTC (rev 8119)
+++ trunk/plearn/base/test/PLStringutilsTest.cc	2007-09-27 15:02:41 UTC (rev 8120)
@@ -0,0 +1,136 @@
+// -*- C++ -*-
+
+// PLStringutilsTest.cc
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file PLStringutilsTest.cc */
+
+
+#include "PLStringutilsTest.h"
+#include <plearn/base/stringutils.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    PLStringutilsTest,
+    "Test functions declared in stringutils.h",
+    ""
+);
+
+///////////////////////
+// PLStringutilsTest //
+///////////////////////
+PLStringutilsTest::PLStringutilsTest()
+{}
+
+///////////
+// build //
+///////////
+void PLStringutilsTest::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void PLStringutilsTest::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("PLStringutilsTest::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void PLStringutilsTest::declareOptions(OptionList& ol)
+{
+    // ### ex:
+    // declareOption(ol, "myoption", &PLStringutilsTest::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void PLStringutilsTest::build_()
+{
+}
+
+/////////////
+// perform //
+/////////////
+void PLStringutilsTest::perform()
+{
+    // Currently only test the 'begins_with' and 'ends_with' functions.
+    // More tests may (should!) be added in the future.
+    string s1 = "Hello";
+    string s2 = "Hello World";
+    string s3 = "Hell no!";
+    string s4 = "Say Hello";
+    string s5 = "Say Hello!";
+    string s6 = "Say Jello";
+    pout << string_begins_with(s2, s1) << endl;
+    pout << string_begins_with(s3, s1) << endl;
+    pout << string_ends_with(s4, s1) << endl;
+    pout << string_ends_with(s5, s1) << endl;
+    pout << string_ends_with(s6, s1) << endl;
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/base/test/PLStringutilsTest.h
===================================================================
--- trunk/plearn/base/test/PLStringutilsTest.h	2007-09-27 14:42:14 UTC (rev 8119)
+++ trunk/plearn/base/test/PLStringutilsTest.h	2007-09-27 15:02:41 UTC (rev 8120)
@@ -0,0 +1,143 @@
+// -*- C++ -*-
+
+// PLStringutilsTest.h
+//
+// Copyright (C) 2007 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file PLStringutilsTest.h */
+
+
+#ifndef PLStringutilsTest_INC
+#define PLStringutilsTest_INC
+
+#include <plearn/misc/PTest.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class PLStringutilsTest : public PTest
+{
+    typedef PTest inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    // ### declare public option fields (such as build options) here
+    // Start your comments with Doxygen-compatible comments such as //!
+    // ### Typically, a PTest options are used to store the test results.
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    PLStringutilsTest();
+
+    // Your other public member functions go here
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(PLStringutilsTest);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    //#####  PLearn::PTest Protocol  ##########################################
+
+    //! The method performing the test. A typical test consists in some output
+    //! (to pout and / or perr), and updates of this object's options.
+    virtual void perform();
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    // ### Declare protected option fields (such as learned parameters) here
+    // ...
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(PLStringutilsTest);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn/base/test/pytest.config
===================================================================
--- trunk/plearn/base/test/pytest.config	2007-09-27 14:42:14 UTC (rev 8119)
+++ trunk/plearn/base/test/pytest.config	2007-09-27 15:02:41 UTC (rev 8120)
@@ -122,3 +122,21 @@
     pfileprg = None,
     disabled = False
     )
+
+Test(
+    name = "PL_stringutils",
+    description = "Test functions declared in stringutils.h",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "PLEARNDIR:scripts/command_line_object.plearn " \
+        "'object=PLStringutilsTest()'",
+    resources = [ ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = False
+    )
+
+



From louradou at mail.berlios.de  Thu Sep 27 18:17:38 2007
From: louradou at mail.berlios.de (louradou at BerliOS)
Date: Thu, 27 Sep 2007 18:17:38 +0200
Subject: [Plearn-commits] r8121 - trunk/plearn_learners/online
Message-ID: <200709271617.l8RGHcHN001549@sheep.berlios.de>

Author: louradou
Date: 2007-09-27 18:17:37 +0200 (Thu, 27 Sep 2007)
New Revision: 8121

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-27 15:02:41 UTC (rev 8120)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-09-27 16:17:37 UTC (rev 8121)
@@ -2223,7 +2223,7 @@
         if (reconstruct_layerwise)
         {
             layer_input.resize(layers[n_layers-2]->size);
-            layer_input << layers[n_layers-2]->expectation.subVec(0,layers[n_layers-2]->expectation.length()-2);
+            layer_input << layers[n_layers-2]->expectation;
             connections[n_layers-2]->setAsUpInput(layers[n_layers-1]->expectation);
             layers[n_layers-2]->getAllActivations(connections[n_layers-2]);
             real rc = reconstruction_costs[n_layers-1] = layers[n_layers-2]->fpropNLL( layer_input );
@@ -2317,6 +2317,11 @@
         // We just need to put a value in one of the rows of that column.
         testcosts->put(0,cumulative_testing_time_cost_index,cumulative_testing_time);
 
+    if( !test_stats )
+    {
+        test_stats = new VecStatsCollector();
+        test_stats->setFieldNames(getTestCostNames());
+    }
     if (test_stats) {
         // Here we simply update the corresponding stat index
         Vec test_time_stats(test_stats->length(), MISSING_VALUE);



From nouiz at mail.berlios.de  Thu Sep 27 21:21:11 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 27 Sep 2007 21:21:11 +0200
Subject: [Plearn-commits] r8122 - trunk/plearn/vmat
Message-ID: <200709271921.l8RJLBxl026534@sheep.berlios.de>

Author: nouiz
Date: 2007-09-27 21:21:10 +0200 (Thu, 27 Sep 2007)
New Revision: 8122

Modified:
   trunk/plearn/vmat/Splitter.cc
   trunk/plearn/vmat/Splitter.h
Log:
set 2 functions as remote callable


Modified: trunk/plearn/vmat/Splitter.cc
===================================================================
--- trunk/plearn/vmat/Splitter.cc	2007-09-27 16:17:37 UTC (rev 8121)
+++ trunk/plearn/vmat/Splitter.cc	2007-09-27 19:21:10 UTC (rev 8122)
@@ -70,6 +70,25 @@
     inherited::declareOptions(ol);
 }
 
+void Splitter::declareMethods(RemoteMethodMap& rmm)
+{
+    // Insert a backpointer to remote methods; note that this
+    // different than for declareOptions()
+    rmm.inherited(inherited::_getRemoteMethodMap_());
+
+    declareMethod(
+        rmm, "nSetsPerSplit", &Splitter::nSetsPerSplit,
+        (BodyDoc("Returns the number of sets per split\n"),
+         RetDoc ("the numer of sets per split")));
+    declareMethod(
+        rmm, "nsplits", &Splitter::nsplits,
+        (BodyDoc(" Returns the number of available different 'splits'\n"),
+         RetDoc (" the numer of available splits")));
+
+
+///TODO export    virtual TVec<VMat> getSplit(int i=0) = 0;
+
+}
 void Splitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);

Modified: trunk/plearn/vmat/Splitter.h
===================================================================
--- trunk/plearn/vmat/Splitter.h	2007-09-27 16:17:37 UTC (rev 8121)
+++ trunk/plearn/vmat/Splitter.h	2007-09-27 19:21:10 UTC (rev 8122)
@@ -100,6 +100,9 @@
 protected:    
     //! Declares the class options.
     static void declareOptions(OptionList& ol);
+
+    //! Declare the methods that are remote-callable
+    static void declareMethods(RemoteMethodMap& rmm);
 };
 
 // Declares a few other classes and functions related to this class



From nouiz at mail.berlios.de  Thu Sep 27 22:09:48 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 27 Sep 2007 22:09:48 +0200
Subject: [Plearn-commits] r8123 - trunk/plearn/base
Message-ID: <200709272009.l8RK9mfo029210@sheep.berlios.de>

Author: nouiz
Date: 2007-09-27 22:09:48 +0200 (Thu, 27 Sep 2007)
New Revision: 8123

Modified:
   trunk/plearn/base/PDate.cc
   trunk/plearn/base/PDate.h
Log:
-Now all date after 1581 is valid. We choosed this date as it is the limit of the julien format.
-Added a mechanisme so that invalid string date gived to the constructor, will initialize the object as missing instead of an executing an PLERROR.


Modified: trunk/plearn/base/PDate.cc
===================================================================
--- trunk/plearn/base/PDate.cc	2007-09-27 19:21:10 UTC (rev 8122)
+++ trunk/plearn/base/PDate.cc	2007-09-27 20:09:48 UTC (rev 8123)
@@ -112,7 +112,7 @@
     PLASSERT( isValid() );
 }
 
-PDate::PDate(string date)
+PDate::PDate(string date,bool invalid_value_as_missing)
 {
     date = removeblanks(date);
     search_replace(date, "-", "");      // remove dashes
@@ -169,6 +169,8 @@
             month = 11;
         else if(mo=="DEC")
             month = 12;
+        else if(invalid_value_as_missing)
+            setMissing();
         else
             PLERROR("Invalid month string: '%s'",mo.c_str());
     }
@@ -195,11 +197,17 @@
         month = toint(date.substr(2,2));
         day = toint(date.substr(4,2));
     }
+    else if(invalid_value_as_missing)
+        setMissing();
     else
         PLERROR("PDate::PDate: the passed date string is not in a known format: %s", date.c_str());
 
     if( !isValid() )
-        PLERROR("Invalid date string: %s",date.c_str());
+        if(invalid_value_as_missing){
+            setMissing();
+            PLWARNING("Invalid date string: %s",date.c_str());
+        }else
+            PLERROR("Invalid date string: %s",date.c_str());
 }
 
 bool PDate::isMissing() const
@@ -226,7 +234,7 @@
         return valid;
     }
 
-    return year  >= 1900  &&  year  <= 3000 &&
+    return year  >= 1582  &&  year  <= 3000 &&
            month >=    1  &&  month <=   12 &&
            day   >=    1  &&  day   <= lastDayOfMonth();
 }

Modified: trunk/plearn/base/PDate.h
===================================================================
--- trunk/plearn/base/PDate.h	2007-09-27 19:21:10 UTC (rev 8122)
+++ trunk/plearn/base/PDate.h	2007-09-27 20:09:48 UTC (rev 8123)
@@ -87,7 +87,8 @@
 
     //!  Initialize a date from a string. 
     //!  Currently recognized formats are: "2003/01/27", "27JAN2003"
-    PDate(string date);
+    //!  if invalid_value_as_missing is true and the string is an invalid date, we set the date as missing
+    PDate(string date,bool invalid_value_as_missing=false);
 
     //!  Missing date handling
     bool isMissing() const;



From nouiz at mail.berlios.de  Fri Sep 28 20:50:08 2007
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 28 Sep 2007 20:50:08 +0200
Subject: [Plearn-commits] r8124 - trunk/plearn/vmat
Message-ID: <200709281850.l8SIo8or015742@sheep.berlios.de>

Author: nouiz
Date: 2007-09-28 20:50:08 +0200 (Fri, 28 Sep 2007)
New Revision: 8124

Modified:
   trunk/plearn/vmat/NoSplitSplitter.cc
Log:
Implemented makeDeepCopyFromShallowCopy. It do not need any copy.Also added an assert


Modified: trunk/plearn/vmat/NoSplitSplitter.cc
===================================================================
--- trunk/plearn/vmat/NoSplitSplitter.cc	2007-09-27 20:09:48 UTC (rev 8123)
+++ trunk/plearn/vmat/NoSplitSplitter.cc	2007-09-28 18:50:08 UTC (rev 8124)
@@ -104,15 +104,6 @@
 void NoSplitSplitter::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
-
-    // ### Call deepCopyField on all "pointer-like" fields
-    // ### that you wish to be deepCopied rather than
-    // ### shallow-copied.
-    // ### ex:
-    // deepCopyField(trainvec, copies);
-
-    // ### Remove this line when you have fully implemented this method.
-    PLERROR("NoSplitSplitter::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
 /////////////
@@ -136,6 +127,7 @@
 //////////////
 TVec<VMat> NoSplitSplitter::getSplit(int k)
 {
+    PLASSERT_MSG(k==0,"In NoSplitSplitter::getSplit - asked for a split that don't exist!");
     TVec<VMat> result(1);
     result[0] = dataset;
     return result;



