From nouiz at mail.berlios.de  Tue Jul  1 08:03:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 1 Jul 2008 08:03:35 +0200
Subject: [Plearn-commits] r9190 - trunk/plearn_learners/regressors
Message-ID: <200807010603.m6163ZAZ020549@sheep.berlios.de>

Author: nouiz
Date: 2008-07-01 08:03:34 +0200 (Tue, 01 Jul 2008)
New Revision: 9190

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
good implementation of check that handle correctly the case of unsigned and signed type


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-06-27 19:07:14 UTC (rev 9189)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-07-01 06:03:34 UTC (rev 9190)
@@ -42,6 +42,7 @@
 #include "RegressionTreeRegisters.h"
 #include <plearn/vmat/TransposeVMatrix.h>
 #include <plearn/vmat/MemoryVMatrixNoSave.h>
+#include <limits>
 
 namespace PLearn {
 using namespace std;
@@ -123,7 +124,7 @@
 {   
     //check that we can put all the examples of the train_set
     //with respect to the size of RTR_type who limit the capacity
-    PLCHECK(the_train_set.length()<pow((real)2,(real)(sizeof(RTR_type)*8)));
+    PLCHECK(the_train_set.length()<=std::numeric_limits<RTR_type>::max());
 
     if(the_train_set==source && tsource)
         //we set the existing source file



From nouiz at mail.berlios.de  Wed Jul  2 18:46:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 2 Jul 2008 18:46:00 +0200
Subject: [Plearn-commits] r9191 - trunk/plearn_learners/online
Message-ID: <200807021646.m62Gk0tp030846@sheep.berlios.de>

Author: nouiz
Date: 2008-07-02 18:45:58 +0200 (Wed, 02 Jul 2008)
New Revision: 9191

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
bugfix. && bind more tightly then ||. Gcc 4.3.0 was giving a warning, so this also remove the warning.


Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-07-01 06:03:34 UTC (rev 9190)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-07-02 16:45:58 UTC (rev 9191)
@@ -2304,8 +2304,8 @@
                 // The length of 'visible_grad' must be either 0 (if not computed
                 // previously) or the size of the mini-batches (otherwise).
                 PLASSERT( visible_grad->width() == visible_layer->size &&
-                          visible_grad->length() == 0 ||
-                          visible_grad->length() == mbs );
+                          (visible_grad->length() == 0 ||
+                           visible_grad->length() == mbs) );
                 visible_grad->resize(mbs, visible_grad->width());
                 connection->bpropUpdate(
                     *visible, *hidden_act,



From nouiz at mail.berlios.de  Wed Jul  2 18:48:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 2 Jul 2008 18:48:47 +0200
Subject: [Plearn-commits] r9192 - in trunk: commands/PLearnCommands
	plearn/misc plearn/vmat plearn_learners/generic
	plearn_learners/online plearn_learners/regressors
Message-ID: <200807021648.m62GmlCv001076@sheep.berlios.de>

Author: nouiz
Date: 2008-07-02 18:48:44 +0200 (Wed, 02 Jul 2008)
New Revision: 9192

Modified:
   trunk/commands/PLearnCommands/FieldConvertCommand.cc
   trunk/plearn/misc/PRange.h
   trunk/plearn/vmat/MeanImputationVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
   trunk/plearn/vmat/MissingInstructionVMatrix.cc
   trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
   trunk/plearn/vmat/RemoveDuplicateVMatrix.cc
   trunk/plearn/vmat/ThresholdVMatrix.cc
   trunk/plearn_learners/generic/AddCostToLearner.cc
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
   trunk/plearn_learners/regressors/RankLearner.cc
Log:
removed new warning generated by gcc 4.3.0 by adding {} or (). I check that their is no bug.


Modified: trunk/commands/PLearnCommands/FieldConvertCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/FieldConvertCommand.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/commands/PLearnCommands/FieldConvertCommand.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -504,7 +504,7 @@
             }
 
             // If we're still not sure (that is to say, type==unknown && message=="").
-            if(type==unknown && message=="")
+            if(type==unknown && message==""){
                 // is data 'uncorrelated + discrete + sparse'? Yes : Flag 
                 if((real)(sc[i].max()-sc[i].min()+1) > (real)(count)*2 ) {
                     type=continuous;
@@ -518,6 +518,7 @@
                     type = discrete_uncorr;
                     // cout << "Discrete uncorrelated: " << i << endl;
                 }
+            }
         }
 
         // Now find out which actions to perform according to type.

Modified: trunk/plearn/misc/PRange.h
===================================================================
--- trunk/plearn/misc/PRange.h	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/misc/PRange.h	2008-07-02 16:48:44 UTC (rev 9192)
@@ -143,14 +143,15 @@
     //! Intersection-assignment operator.
     PRange& operator&=(const PRange<T>& r)
     {
-        if (! isEmpty())
+        if (! isEmpty()){
             if (! r.isEmpty()) {
                 lower_ = max(lower_, r.lower());
                 upper_ = min(upper_, r.upper());
             }
             else
                 clear();
-    
+        }
+            
         return (*this);
     }
   

Modified: trunk/plearn/vmat/MeanImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanImputationVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/MeanImputationVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -259,7 +259,7 @@
     PLASSERT( source );
     source->getRow(i, v);
 
-    if (v.hasMissing())
+    if (v.hasMissing()){
         if (distribution) {
             Vec target;
             bool restore_target = false;
@@ -284,6 +284,7 @@
             for (int j = 0; j < v.length(); j++)
                 if (is_missing(v[j]))
                     v[j] = variable_mean[j];
+    }
 }
 
 ///////////////////

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -167,7 +167,7 @@
 {  
   source->getSubRow(i, j, v);
   for (int source_col = 0; source_col < v->length(); source_col++) 
-    if (is_missing(v[source_col]))
+    if (is_missing(v[source_col])){
       if (variable_imputation_instruction[source_col + j] == 1)
 	v[source_col] = variable_mean[source_col + j];
       else if (variable_imputation_instruction[source_col + j] == 2)
@@ -183,6 +183,7 @@
 	PLERROR("In MeanMedianModeImputationVMatrix::getSubRow(%d,%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,j,
 		variable_imputation_instruction[source_col + j] );
+    }
 
 }
 
@@ -190,7 +191,7 @@
 {  
   source-> getRow(i, v);
   for (int source_col = 0; source_col < v->length(); source_col++)
-    if (is_missing(v[source_col]))
+    if (is_missing(v[source_col])){
       if (variable_imputation_instruction[source_col] == 1)
 	v[source_col] = variable_mean[source_col];
       else if (variable_imputation_instruction[source_col] == 2)
@@ -206,14 +207,14 @@
 	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,
 		variable_imputation_instruction[source_col] );
-  
+    }
 }
 
 void MeanMedianModeImputationVMatrix::getColumn(int i, Vec v) const
 {  
   source-> getColumn(i, v);
   for (int source_row = 0; source_row < v->length(); source_row++)
-    if (is_missing(v[source_row]))
+    if (is_missing(v[source_row])){
       if (variable_imputation_instruction[i] == 1) v[source_row] = variable_mean[i];
       else if (variable_imputation_instruction[i] == 2) v[source_row] = variable_median[i];
       else if (variable_imputation_instruction[i] == 3) v[source_row] = variable_mode[i];
@@ -226,7 +227,7 @@
 	PLERROR("In MeanMedianModeImputationVMatrix::getRow(%d, Vec) - "
 		"unknow variable_imputation_instruction value of %d",i,
 		variable_imputation_instruction[i] );
-
+    }
 }
 
 
@@ -243,9 +244,10 @@
     int train_length = train_set->length();
     int train_width = train_set->width();
 
-    if (number_of_train_samples_to_use > 0.0)
+    if (number_of_train_samples_to_use > 0.0){
         if (number_of_train_samples_to_use < 1.0) train_length = (int) (number_of_train_samples_to_use * (real) train_length);
         else train_length = (int) number_of_train_samples_to_use;
+    }
     if (train_length > train_set->length()) train_length = train_set->length();
     if(train_length < 1) 
       PLERROR("In MeanMedianModeImputationVMatrix::length of the number of"

Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -168,9 +168,10 @@
 
     if(train_set){
       int train_length = train_set->length();
-      if (number_of_train_samples_to_use > 0.0)
+      if (number_of_train_samples_to_use > 0.0){
         if (number_of_train_samples_to_use < 1.0) train_length = (int) (number_of_train_samples_to_use * (real) train_length);
         else train_length = (int) number_of_train_samples_to_use;
+      }
       if (train_length > train_set->length()) train_length = train_set->length();
 
       int train_width = train_set->width();

Modified: trunk/plearn/vmat/MissingInstructionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/MissingInstructionVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -195,12 +195,13 @@
             PLWARNING("In MergeDond2Files::build_() - merge instruction empty for field '%s', we keep the previous instruction who could be the default_instruction",(missing_instructions[source_col].first).c_str());
         else PLERROR("In MergeDond2Files::build_() - unsupported merge instruction: '%s'", 
                      (missing_instructions[ins_col].second).c_str());
-        if (ins[source_col] == "skip")
+        if (ins[source_col] == "skip"){
             if(source_col<source->inputsize())
                 skip_instruction_input++;
             else if(source_col<(source->inputsize()+source->targetsize()))
                 skip_instruction_target++;
             else skip_instruction_weight++;
+        }
     }
     setMetaInfoFromSource();
     inputsize_ = source->inputsize() - skip_instruction_input;

Modified: trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/MultiTargetOneHotVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -389,7 +389,7 @@
 ///////////////
 void MultiTargetOneHotVMatrix::getNewRow(int i, const Vec& v) const
 {
-  PLASSERT( source && source_target || source_and_target);
+  PLASSERT( (source && source_target) || source_and_target);
   int is = source_inputsize;
   int ts;
   if(source_and_target)

Modified: trunk/plearn/vmat/RemoveDuplicateVMatrix.cc
===================================================================
--- trunk/plearn/vmat/RemoveDuplicateVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/RemoveDuplicateVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -207,12 +207,13 @@
             if (!removed[i])
                 indices.append(i);
         inherited::build();
-        if (verbosity >= 2)
+        if (verbosity >= 2){
             if (count > 0)
                 pout << "Removed a total of " << count << " duplicated samples (new length: "
                      << length() << ")" << endl;
             else
                 pout << "No duplicated samples found." << endl;
+        }
     }
 }
 

Modified: trunk/plearn/vmat/ThresholdVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ThresholdVMatrix.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn/vmat/ThresholdVMatrix.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -74,7 +74,8 @@
 #endif
     source->getRow(i,v);
     int p= v.size()-1;
-    if(gt_threshold && v[p] <= threshold || !gt_threshold && v[p] < threshold)
+    if((gt_threshold && v[p] <= threshold) 
+       || (!gt_threshold && v[p] < threshold))
         v[p]= cold_value;
     else
         v[p]= hot_value;

Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -289,12 +289,13 @@
     costs.resize(nTestCosts());
     Vec sub_costs = costs.subVec(0, n_original_costs);
     int target_length = target.length();
-    if(add_sub_learner_costs)
+    if(add_sub_learner_costs){
         if (compute_costs_on_bags) {
             learner_->computeCostsFromOutputs(input, output, target.subVec(0, target_length - 1), sub_costs);
         } else {
             learner_->computeCostsFromOutputs(input, output, target, sub_costs);
         }
+    }
 
     if (compute_costs_on_bags) {
         // We only need to compute the costs when the whole bag has been seen,

Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -946,12 +946,13 @@
                 }
 
                 // Update stats if we are in the last n_train_stats_samples
-                if (stage >= nstages - n_train_stats_samples)
+                if (stage >= nstages - n_train_stats_samples){
                     if (minibatch_size > 1 || minibatch_hack)
                         for (int k = 0; k < minibatch_size; k++)
                             train_stats->update(train_costs_m(k));
                     else
                         train_stats->update(train_costs);
+                }
             }
 
             if( pb )
@@ -1160,13 +1161,14 @@
                 }
 
                 // Update stats if we are in the last n_train_stats_samples samples
-                if (stage >= end_stage - n_train_stats_samples)
+                if (stage >= end_stage - n_train_stats_samples){
                     if (minibatch_size > 1 || minibatch_hack)
                         for (int k = 0; k < minibatch_size; k++)
                             train_stats->update(train_costs_m(k));
                     else
                         train_stats->update(train_costs);
                 }
+            }
 
             if( pb )
                 pb->update( stage - init_stage + 1 );

Modified: trunk/plearn_learners/online/StackedAutoassociatorsNet.cc
===================================================================
--- trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn_learners/online/StackedAutoassociatorsNet.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -1107,12 +1107,13 @@
                     }
 
                     // Update stats if we are in the last n_train_stats_samples
-                    if (stage >= nstages - n_train_stats_samples)
+                    if (stage >= nstages - n_train_stats_samples){
                         if (minibatch_size > 1 || minibatch_hack)
                             for (int k = 0; k < minibatch_size; k++)
                                 train_stats->update(train_costs_m(k));
                         else
                             train_stats->update(train_costs);
+                    }
                 }
 
                 if (pb)

Modified: trunk/plearn_learners/regressors/RankLearner.cc
===================================================================
--- trunk/plearn_learners/regressors/RankLearner.cc	2008-07-02 16:45:58 UTC (rev 9191)
+++ trunk/plearn_learners/regressors/RankLearner.cc	2008-07-02 16:48:44 UTC (rev 9192)
@@ -138,11 +138,12 @@
             else
                 left = mid;
         }
-        if (right == left)
+        if (right == left){
             if (left == n - 1)
                 left--;
             else
                 right++;
+        }
         frac = sorted_targets[right] - sorted_targets[left];
         if (frac < 1e-30)
             // Equal targets, up to numerical precision.



From tihocan at mail.berlios.de  Wed Jul  2 20:23:43 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 2 Jul 2008 20:23:43 +0200
Subject: [Plearn-commits] r9193 - trunk/plearn_learners/online
Message-ID: <200807021823.m62INh0G007480@sheep.berlios.de>

Author: tihocan
Date: 2008-07-02 20:23:42 +0200 (Wed, 02 Jul 2008)
New Revision: 9193

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
More fun to play with!

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-07-02 16:48:44 UTC (rev 9192)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-07-02 18:23:42 UTC (rev 9193)
@@ -108,6 +108,8 @@
     "     gradient updates agree on the sign, followed by the fraction of\n"
     "     weights for which the CD update has same sign as the difference\n"
     "     between the NLL gradient and the CD update.\n"
+    "   - 'agreement_stoch': same as the first half of above, except that\n"
+    "     it is for the stochastic CD update rather than its expected value.\n"
     "   - 'bound_cd_nll': bound on the difference between the CD and NLL\n"
     "     gradient updates, as computed in (Bengio & Delalleau, 2008)\n"
     "   - 'weights_stats': first element is the median of the absolute value\n"
@@ -118,7 +120,9 @@
     "   - 'ratio_cd_leftout': median ratio between the absolute value of the\n"
     "     CD update and the absolute value of the term left out in CD (i.e.\n"
     "     the difference between NLL gradient and CD).\n"
-    "   - 'abs_cd': average absolute value of the CD update\n"
+    "   - 'abs_cd': average absolute value of the CD update. First for the\n"
+    "     expected CD update, then its stochastic (sampled) version.\n"
+    "   - 'nll_grad': NLL gradient.\n"
     "    \n"
     "\n"
     "The RBM can be trained by gradient descent (wrt to gradients provided on\n"
@@ -404,10 +408,12 @@
     addPortName("median_reldiff_cd_nll");
     addPortName("mean_diff_cd_nll");
     addPortName("agreement_cd_nll");
+    addPortName("agreement_stoch");
     addPortName("bound_cd_nll");
     addPortName("weights_stats");
     addPortName("ratio_cd_leftout");
     addPortName("abs_cd");
+    addPortName("nll_grad");
     if(reconstruction_connection)
     {
         addPortName("visible_reconstruction.state");
@@ -816,6 +822,12 @@
                     exp(all_p_visible[i] - log_partition_function);
             //pout << "All P(x): " << all_p_visible << endl;
             //pout << "Sum_x P(x) = " << sum(all_p_visible) << endl;
+            if (!is_equal(sum(all_p_visible), 1)) {
+                PLWARNING("The sum of all probability is not 1: %f",
+                        sum(all_p_visible));
+                // Renormalize.
+                all_p_visible /= sum(all_p_visible);
+            }
             PLCHECK( is_equal(sum(all_p_visible), 1) );
         }
     }
@@ -886,6 +898,8 @@
     bool mean_diff_cd_nll_is_output = mean_diff_cd_nll && mean_diff_cd_nll->isEmpty();
     Mat* agreement_cd_nll = ports_value[getPortIndex("agreement_cd_nll")];
     bool agreement_cd_nll_is_output = agreement_cd_nll && agreement_cd_nll->isEmpty();
+    Mat* agreement_stoch = ports_value[getPortIndex("agreement_stoch")];
+    bool agreement_stoch_is_output = agreement_stoch && agreement_stoch->isEmpty();
     Mat* bound_cd_nll = ports_value[getPortIndex("bound_cd_nll")];
     bool bound_cd_nll_is_output = bound_cd_nll && bound_cd_nll->isEmpty();
     Mat* weights_stats = ports_value[getPortIndex("weights_stats")];
@@ -894,6 +908,8 @@
     bool ratio_cd_leftout_is_output = ratio_cd_leftout && ratio_cd_leftout->isEmpty();
     Mat* abs_cd = ports_value[getPortIndex("abs_cd")];
     bool abs_cd_is_output = abs_cd && abs_cd->isEmpty();
+    Mat* nll_grad = ports_value[getPortIndex("nll_grad")];
+    bool nll_grad_is_output = nll_grad && nll_grad->isEmpty();
     hidden_bias = ports_value[getPortIndex("hidden_bias")];
     //bool hidden_bias_is_output = hidden_bias && hidden_bias->isEmpty();
     weights = ports_value[getPortIndex("weights")];
@@ -1482,6 +1498,7 @@
         Mat input_mat = input.toMat(1, input.length());
         Mat grad_nll(hidden_layer->size, visible_layer->size);
         Mat grad_cd(hidden_layer->size, visible_layer->size);
+        Mat grad_stoch_cd(hidden_layer->size, visible_layer->size);
         Mat grad_first_term(hidden_layer->size, visible_layer->size);
         grad_nll.fill(0);
         if (median_reldiff_cd_nll_is_output)
@@ -1490,6 +1507,8 @@
             mean_diff_cd_nll->resize(visible->length(), n_steps_compare);
         if (agreement_cd_nll_is_output)
             agreement_cd_nll->resize(visible->length(), 2 * n_steps_compare);
+        if (agreement_stoch_is_output)
+            agreement_stoch->resize(visible->length(), n_steps_compare);
         real bound_coeff = MISSING_VALUE;
         if (bound_cd_nll_is_output || weights_stats_is_output) {
             if (bound_cd_nll_is_output)
@@ -1499,7 +1518,10 @@
             if (ratio_cd_leftout_is_output)
                 ratio_cd_leftout->resize(visible->length(), n_steps_compare);
             if (abs_cd_is_output)
-                abs_cd->resize(visible->length(), n_steps_compare);
+                abs_cd->resize(visible->length(), 2 * n_steps_compare);
+            if (nll_grad_is_output)
+                nll_grad->resize(visible->length(),
+                        visible_layer->size * hidden_layer->size);
             // Compute main bound coefficient:
             // (1 - N_x N_h sigm(-alpha)^d_x sigm(-beta)^d_h).
             PP<RBMMatrixConnection> matrix_conn =
@@ -1579,6 +1601,13 @@
                     ") for x = " << (*visible)(0) << ":" <<
                     endl << tmp << endl;
                 */
+                int stoch_idx = -1;
+                if (abs_cd_is_output) {
+                    grad_stoch_cd.fill(0);
+                    // Pick a random X_t drawn from X_t | x.
+                    stoch_idx = random_gen->multinomial_sample(
+                            p_xt_given_x.toVecCopy());
+                }
                 // Compute E_{X_t}[dF(X_t)/dWij | x].
                 grad_cd.fill(0);
                 for (int k = 0; k < n_visible_conf; k++) {
@@ -1599,11 +1628,20 @@
                                 -all_p_visible[k],
                                 real(1));
                     }
+                    if (k == stoch_idx) {
+                        transposeProduct(grad_stoch_cd,
+                                hidden_layer->getExpectations(),
+                                input_mat);
+                        negateElements(grad_stoch_cd);
+                    }
                 }
                 // Compute difference between CD and NLL updates.
                 Mat diff = grad_nll.copy();
                 diff -= grad_cd;
                 grad_cd += grad_first_term;
+                if (abs_cd_is_output) {
+                    grad_stoch_cd += grad_first_term;
+                }
                 //pout << "Grad_CD_" << t+1 << "=" << endl << grad_cd << endl;
                 //pout << "Diff =" << endl << diff << endl;
                 // Compute average relative difference.
@@ -1625,32 +1663,48 @@
                 // agree.
                 int agree = 0;
                 int agree2 = 0;
+                int agree_stoch = 0;
                 real mean_abs_updates = 0;
+                real mean_abs_stoch_updates = 0;
                 for (int p = 0; p < grad_cd.length(); p++)
                     for (int q = 0; q < grad_cd.width(); q++) {
                         if (grad_cd(p, q) *
-                                (grad_first_term(p, q) + grad_nll(p, q)) > 0)
+                                (grad_first_term(p, q) + grad_nll(p, q)) >= 0)
                         {
                             agree++;
                         }
-                        if (grad_cd(p, q) * diff(p, q) > 0)
+                        if (grad_cd(p, q) * diff(p, q) >= 0)
                             agree2++;
-                        mean_abs_updates += abs(grad_cd(p, q));
+                        if (abs_cd_is_output) {
+                            mean_abs_updates += abs(grad_cd(p, q));
+                            mean_abs_stoch_updates += abs(grad_stoch_cd(p, q));
+                        }
+                        if (agreement_stoch_is_output &&
+                                grad_stoch_cd(p, q) *
+                                (grad_first_term(p, q) + grad_nll(p, q)) >= 0)
+                        {
+                            agree_stoch++;
+                        }
                     }
                 mean_abs_updates /= real(grad_cd.size());
+                mean_abs_stoch_updates /= real(grad_cd.size());
                 if (agreement_cd_nll_is_output) {
                     (*agreement_cd_nll)(i, t) = agree / real(grad_cd.size());
                     (*agreement_cd_nll)(i, t + n_steps_compare) =
                         agree2 / real(grad_cd.size());
                 }
+                if (agreement_stoch_is_output)
+                    (*agreement_stoch)(i, t) = agree_stoch / real(grad_cd.size());
                 if (bound_cd_nll_is_output)
                     (*bound_cd_nll)(i, t) =
                         visible_layer->getConfigurationCount() *
                         ipow(bound_coeff, t + 1);
                 if (ratio_cd_leftout_is_output)
                     (*ratio_cd_leftout)(i, t) = median(all_ratios);
-                if (abs_cd_is_output)
+                if (abs_cd_is_output) {
                     (*abs_cd)(i, t) = mean_abs_updates;
+                    (*abs_cd)(i, t + n_steps_compare) = mean_abs_stoch_updates;
+                }
                 /*
                 pout << "Median relative difference: "
                     << median(all_relative_diffs) << endl;
@@ -1663,6 +1717,16 @@
             }
             //pout << "P(x)=" << endl << all_p_visible << endl;
             grad_nll += grad_first_term;
+            if (nll_grad_is_output) {
+                //real mean_nll_grad = 0;
+                int idx = 0;
+                for (int p = 0; p < grad_nll.length(); p++)
+                    for (int q = 0; q < grad_nll.width(); q++, idx++)
+                        (*nll_grad)(i, idx) = grad_nll(p, q);
+                        //mean_nll_grad += abs(grad_nll(p, q));
+                //mean_nll_grad /= real(grad_nll.size());
+                //(*nll_grad)(i, 0) = mean_nll_grad;
+            }
             //pout << "Grad_NLL=" << endl << grad_nll << endl;
             //pout << "Grad first term=" << endl << grad_first_term << endl;
         }
@@ -1684,6 +1748,11 @@
         agreement_cd_nll->resize(visible->length(), 2 * n_steps_compare);
         agreement_cd_nll->fill(MISSING_VALUE);
     }
+    if (agreement_stoch_is_output && agreement_stoch->isEmpty()) {
+        PLASSERT( during_training );
+        agreement_stoch->resize(visible->length(), n_steps_compare);
+        agreement_stoch->fill(MISSING_VALUE);
+    }
     if (bound_cd_nll_is_output && bound_cd_nll->isEmpty()) {
         PLASSERT( during_training );
         bound_cd_nll->resize(visible->length(), n_steps_compare);
@@ -1701,9 +1770,15 @@
     }
     if (abs_cd_is_output && abs_cd->isEmpty()) {
         PLASSERT( during_training );
-        abs_cd->resize(visible->length(), n_steps_compare);
+        abs_cd->resize(visible->length(), 2 * n_steps_compare);
         abs_cd->fill(MISSING_VALUE);
     }
+    if (nll_grad_is_output && nll_grad->isEmpty()) {
+        PLASSERT( during_training );
+        nll_grad->resize(visible->length(),
+                         visible_layer->size * hidden_layer->size);
+        nll_grad->fill(MISSING_VALUE);
+    }
 
     // UGLY HACK TO DEAL WITH THE PROBLEM THAT XXX.state MAY NOT BE NEEDED
     // BUT IS ALWAYS EXPECTED BECAUSE IT IS A STATE (!@#$%!!!)



From tihocan at mail.berlios.de  Wed Jul  2 20:24:03 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 2 Jul 2008 20:24:03 +0200
Subject: [Plearn-commits] r9194 - trunk/plearn/io
Message-ID: <200807021824.m62IO3p1007681@sheep.berlios.de>

Author: tihocan
Date: 2008-07-02 20:24:02 +0200 (Wed, 02 Jul 2008)
New Revision: 9194

Modified:
   trunk/plearn/io/PrPStreamBuf.cc
Log:
Fixed warning and line length

Modified: trunk/plearn/io/PrPStreamBuf.cc
===================================================================
--- trunk/plearn/io/PrPStreamBuf.cc	2008-07-02 18:23:42 UTC (rev 9193)
+++ trunk/plearn/io/PrPStreamBuf.cc	2008-07-02 18:24:02 UTC (rev 9194)
@@ -95,7 +95,8 @@
 {
     streamsize nwritten = ::PR_Write(out, p, PRInt32(n));
     if (nwritten != n)
-        PLERROR("In PrPStreamBuf::write_ failed to write the requested number of bytes: wrote %d instead of %d",nwritten,n);
+        PLERROR("In PrPStreamBuf::write_ failed to write the requested number "
+                "of bytes: wrote %ld instead of %ld", nwritten, n);
 }
   
 } // end of namespace PLearn



From nouiz at mail.berlios.de  Wed Jul  2 20:46:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 2 Jul 2008 20:46:15 +0200
Subject: [Plearn-commits] r9195 - trunk/plearn_learners/meta
Message-ID: <200807021846.m62IkFc0010453@sheep.berlios.de>

Author: nouiz
Date: 2008-07-02 20:46:15 +0200 (Wed, 02 Jul 2008)
New Revision: 9195

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
Log:
bugfix introduced in revision 9179.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-02 18:24:02 UTC (rev 9194)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-02 18:46:15 UTC (rev 9195)
@@ -376,20 +376,20 @@
                                              TVec<Vec> sub_target) 
 {
     if(fast_is_equal(target[0],0.)){
-        sub_target[0]=0;
-        sub_target[1]=0;
+        sub_target[0][0]=0;
+        sub_target[1][0]=0;
     }else if(fast_is_equal(target[0],1.)){
-        sub_target[0]=1;
-        sub_target[1]=0;
+        sub_target[0][0]=1;
+        sub_target[1][0]=0;
     }else if(fast_is_equal(target[0],2.)){
-        sub_target[0]=1;
-        sub_target[1]=1;
+        sub_target[0][0]=1;
+        sub_target[1][0]=1;
     }else if(target[0]>2){
         PLWARNING("In MultiClassAdaBoost::getSubLearnerTarget - "
                   "We only support target 0/1/2. We got %f. We transform "
                   "it to a target of 2.", target[0]);
-        sub_target[0]=1;
-        sub_target[1]=1;
+        sub_target[0][0]=1;
+        sub_target[1][0]=1;
     }else
         PLERROR("In MultiClassAdaBoost::getSubLearnerTarget - "
                   "We only support target 0/1/2. We got %f.", target[0]); 



From nouiz at mail.berlios.de  Wed Jul  2 20:47:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 2 Jul 2008 20:47:37 +0200
Subject: [Plearn-commits] r9196 - trunk/plearn_learners/cgi
Message-ID: <200807021847.m62Ilbf6010572@sheep.berlios.de>

Author: nouiz
Date: 2008-07-02 20:47:36 +0200 (Wed, 02 Jul 2008)
New Revision: 9196

Modified:
   trunk/plearn_learners/cgi/AnalyzeFieldStats.cc
   trunk/plearn_learners/cgi/ComputeDond2Target.cc
   trunk/plearn_learners/cgi/MergeDond2Files.cc
   trunk/plearn_learners/cgi/TestImputations.cc
Log:
removed warning du to bad code. The warning was added with __attribute__(format,..) in a previous commit


Modified: trunk/plearn_learners/cgi/AnalyzeFieldStats.cc
===================================================================
--- trunk/plearn_learners/cgi/AnalyzeFieldStats.cc	2008-07-02 18:46:15 UTC (rev 9195)
+++ trunk/plearn_learners/cgi/AnalyzeFieldStats.cc	2008-07-02 18:47:36 UTC (rev 9196)
@@ -153,7 +153,7 @@
             if (fields[fields_col] == main_names[main_col]) break;
         }
         if (main_col >= main_width) 
-            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in input dataset: %", (fields[fields_col]).c_str());
+            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in input dataset: %s", (fields[fields_col]).c_str());
         fields_selected[main_col] = 1;
     }
     
@@ -243,7 +243,7 @@
             if (fields[fields_col] == main_names[main_col]) break;
         }
         if (main_col >= main_width) 
-            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in input dataset: %", (fields[fields_col]).c_str());
+            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in input dataset: %s", (fields[fields_col]).c_str());
         if (fields_col != to_deal_with_next && header_record[main_col] != 1)
         {
             output_variable_src[output_col] = main_col;
@@ -284,7 +284,7 @@
             if (fields[fields_col] == targeted_names[main_col]) break;
         }
         if (main_col >= targeted_width) 
-            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in targeted dataset: %", (fields[fields_col]).c_str());
+            PLERROR("In AnalyzeFieldStats::analyzeVariableStats() no field with this name in targeted dataset: %s", (fields[fields_col]).c_str());
         if (fields_col != to_deal_with_next && header_record[main_col] != 1)
         {
             train_test_variable_src[output_col] = main_col;

Modified: trunk/plearn_learners/cgi/ComputeDond2Target.cc
===================================================================
--- trunk/plearn_learners/cgi/ComputeDond2Target.cc	2008-07-02 18:46:15 UTC (rev 9195)
+++ trunk/plearn_learners/cgi/ComputeDond2Target.cc	2008-07-02 18:47:36 UTC (rev 9196)
@@ -160,7 +160,7 @@
         {
             if (input_vector[ins_col] == main_names[main_col]) break;
         }
-        if (main_col >= main_width) PLERROR("In ComputeDond2Target: no field with this name in input dataset: %", (input_vector[ins_col]).c_str());
+        if (main_col >= main_width) PLERROR("In ComputeDond2Target: no field with this name in input dataset: %s", (input_vector[ins_col]).c_str());
         output_variable_src[ins_col] = main_col;
         output_names[ins_col] = input_vector[ins_col];
     }

Modified: trunk/plearn_learners/cgi/MergeDond2Files.cc
===================================================================
--- trunk/plearn_learners/cgi/MergeDond2Files.cc	2008-07-02 18:46:15 UTC (rev 9195)
+++ trunk/plearn_learners/cgi/MergeDond2Files.cc	2008-07-02 18:47:36 UTC (rev 9196)
@@ -181,12 +181,12 @@
         {
             if (merge_instructions[ins_col].first == sec_names[sec_col]) break;
         }
-        if (sec_col >= sec_width) PLERROR("In MergeDond2Files: no field with this name in external_dataset data set: %", (merge_instructions[ins_col].first).c_str());
+        if (sec_col >= sec_width) PLERROR("In MergeDond2Files: no field with this name in external_dataset data set: %s", (merge_instructions[ins_col].first).c_str());
         if (merge_instructions[ins_col].second == "skip") sec_ins[sec_col] = "skip";
         else if (merge_instructions[ins_col].second == "mean") sec_ins[sec_col] = "mean";
         else if (merge_instructions[ins_col].second == "mode") sec_ins[sec_col] = "mode";
         else if (merge_instructions[ins_col].second == "present") sec_ins[sec_col] = "present";
-        else PLERROR("In MergeDond2Files: unsupported merge instruction: %", (merge_instructions[ins_col].second).c_str());
+        else PLERROR("In MergeDond2Files: unsupported merge instruction: %s", (merge_instructions[ins_col].second).c_str());
         if (sec_ins[sec_col] != "skip") extension_width += 1;
     }
     extension_pos.resize(sec_width);
@@ -232,13 +232,13 @@
         {
             if (missing_instructions[ins_col].first == main_names[main_col]) break;
         }
-        if (main_col >= main_width) PLERROR("In MergeDond2Files: no field with this name in external_dataset data set: %", (missing_instructions[ins_col].first).c_str());
+        if (main_col >= main_width) PLERROR("In MergeDond2Files: no field with this name in external_dataset data set: %s", (missing_instructions[ins_col].first).c_str());
         if (missing_instructions[ins_col].second == "skip") main_ins[main_col] = "skip";
         else if (missing_instructions[ins_col].second == "as_is") main_ins[main_col] = "as_is";
         else if (missing_instructions[ins_col].second == "zero_is_missing") main_ins[main_col] = "zero_is_missing";
         else if (missing_instructions[ins_col].second == "2436935_is_missing") main_ins[main_col] = "2436935_is_missing";
         else if (missing_instructions[ins_col].second == "present") main_ins[main_col] = "present";
-        else PLERROR("In MergeDond2Files: unsupported merge instruction: %", (missing_instructions[ins_col].second).c_str());
+        else PLERROR("In MergeDond2Files: unsupported merge instruction: %s", (missing_instructions[ins_col].second).c_str());
         if (main_ins[main_col] != "skip") primary_width += 1;
     }
     primary_names.resize(primary_width);

Modified: trunk/plearn_learners/cgi/TestImputations.cc
===================================================================
--- trunk/plearn_learners/cgi/TestImputations.cc	2008-07-02 18:46:15 UTC (rev 9195)
+++ trunk/plearn_learners/cgi/TestImputations.cc	2008-07-02 18:47:36 UTC (rev 9196)
@@ -555,7 +555,7 @@
         if (train_missing <= 0.0) header_record[train_col] = 0.0;                       // no missing, noting to do.
         else if (train_present < min_number_of_samples){
             header_record[train_col] = -1.0; // should not happen
-            PLERROR("In TestImputations::createHeaderFile: train_present(%d) < min_number_of_samples (%d) for variable %d()",
+            PLERROR("In TestImputations::createHeaderFile: train_present(%d) < min_number_of_samples (%d) for variable %d(%s)",
                     train_present,min_number_of_samples,train_col,train_set.fieldName(train_col).c_str());
         }
         else header_record[train_col] = 1.0;                                            // test imputations



From nouiz at mail.berlios.de  Wed Jul  2 20:53:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 2 Jul 2008 20:53:15 +0200
Subject: [Plearn-commits] r9197 - trunk/plearn_learners/online
Message-ID: <200807021853.m62IrFCr011205@sheep.berlios.de>

Author: nouiz
Date: 2008-07-02 20:53:14 +0200 (Wed, 02 Jul 2008)
New Revision: 9197

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
corrected bad printf format in a PLERROR


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2008-07-02 18:47:36 UTC (rev 9196)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2008-07-02 18:53:14 UTC (rev 9197)
@@ -666,8 +666,8 @@
 
         if( final_module->output_size != final_cost->input_size )
             PLERROR("DeepBeliefNet::build_final_cost() - "
-                    "final_module->output_size (%d) != final_cost->input_size."
-                    "\n", n_layers-1, layers[n_layers-1]->size,
+                    "final_module->output_size (%d) != final_cost->input_size (%d)."
+                    "\n", final_module->output_size,
                     final_module->input_size);
 
         final_module->setLearningRate( grad_learning_rate );



From laulysta at mail.berlios.de  Thu Jul  3 01:06:02 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Thu, 3 Jul 2008 01:06:02 +0200
Subject: [Plearn-commits] r9198 - trunk/plearn_learners_experimental
Message-ID: <200807022306.m62N62gx011874@sheep.berlios.de>

Author: laulysta
Date: 2008-07-03 01:06:01 +0200 (Thu, 03 Jul 2008)
New Revision: 9198

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
Development progressing...


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-02 18:53:14 UTC (rev 9197)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-02 23:06:01 UTC (rev 9198)
@@ -58,7 +58,7 @@
 //     * du code pour entra?ner s?par?ment les hidden_connections (si pr?sentes)
 // - pourrait avoir le gradient du denoising recurrent net en m?me temps que
 //   celui du "fine-tuning"
-// - add dynamic_activations_list and use it in recurrent_update
+// - add dynamic_activations_list and use it in recurrentUpdate
 
 
 namespace PLearn {
@@ -70,13 +70,20 @@
     ""
     );
 
-
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
-    recurrent_net_learning_rate( 0.01),
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
     encoding("raw_masked_supervised"),
-    input_window_size(1)
+    input_window_size(1),
+    tied_input_reconstruction_weights( true ),
+    input_noise_prob( 0.15 ),
+    input_reconstruction_lr( 0 ),
+    hidden_noise_prob( 0.15 ),
+    hidden_reconstruction_lr( 0 ),
+    tied_hidden_reconstruction_weights( false ),
+    noisy_recurrent_lr( 0),
+    dynamic_gradient_scale_factor( 0.5 ),
+    recurrent_lr( 0.01 )
 {
     random_gen = new PRandom();
 }
@@ -88,11 +95,6 @@
 //                  "The learning rate used during RBM contrastive "
 //                  "divergence learning phase.\n");
 
-    declareOption(ol, "recurrent_net_learning_rate", 
-                  &DenoisingRecurrentNet::recurrent_net_learning_rate,
-                  OptionBase::buildoption,
-                  "The learning rate used during the recurrent phase.\n");
-
 //    declareOption(ol, "rbm_nstages", &DenoisingRecurrentNet::rbm_nstages,
 //                  OptionBase::buildoption,
 //                  "Number of epochs for rbm phase.\n");
@@ -103,12 +105,6 @@
                   OptionBase::buildoption,
                   "The training weights of each target layers.\n");
 
-    declareOption(ol, "use_target_layers_masks", 
-                  &DenoisingRecurrentNet::use_target_layers_masks,
-                  OptionBase::buildoption,
-                  "Indication that a mask indicating which target to predict\n"
-                  "is present in the input part of the VMatrix dataset.\n");
-
     declareOption(ol, "end_of_sequence_symbol", 
                   &DenoisingRecurrentNet::end_of_sequence_symbol,
                   OptionBase::buildoption,
@@ -153,19 +149,6 @@
                   OptionBase::buildoption,
                   "The RBMConnection from input_layer to hidden_layer.\n");
 
-    declareOption(ol, "encoding", 
-                  &DenoisingRecurrentNet::encoding,
-                  OptionBase::buildoption,
-                  "Chooses what type of encoding to apply to an input sequence\n"
-                  "Possibilities: timeframe, note_duration, note_octav_duration, raw_masked_supervised");
-
-    declareOption(ol, "input_window_size", 
-                  &DenoisingRecurrentNet::input_window_size,
-                  OptionBase::buildoption,
-                  "How many time steps to present as input\n"
-                  "This option is ignored when mode is raw_masked_supervised,"
-                  "since in this mode the full expanded and preprocessed input and target are given explicitly.");
-
     declareOption(ol, "target_layers_n_of_target_elements", 
                   &DenoisingRecurrentNet::target_layers_n_of_target_elements,
                   OptionBase::learntoption,
@@ -182,7 +165,73 @@
                   OptionBase::learntoption,
                   "Number of symbols for each symbolic field of train_set.\n");
 
-    /*
+
+
+
+
+    
+    declareOption(ol, "encoding", 
+                  &DenoisingRecurrentNet::encoding,
+                  OptionBase::buildoption,
+                  "Chooses what type of encoding to apply to an input sequence\n"
+                  "Possibilities: timeframe, note_duration, note_octav_duration, raw_masked_supervised");
+
+    declareOption(ol, "input_window_size", 
+                  &DenoisingRecurrentNet::input_window_size,
+                  OptionBase::buildoption,
+                  "How many time steps to present as input\n"
+                  "This option is ignored when mode is raw_masked_supervised,"
+                  "since in this mode the full expanded and preprocessed input and target are given explicitly.");
+
+    declareOption(ol, "tied_input_reconstruction_weights", 
+                  &DenoisingRecurrentNet::tied_input_reconstruction_weights,
+                  OptionBase::buildoption,
+                  "Do we want the input reconstruction weights tied or not\n"
+                  "Boolean, yes or no");
+
+    declareOption(ol, "input_noise_prob", 
+                  &DenoisingRecurrentNet::input_noise_prob,
+                  OptionBase::buildoption,
+                  "Probability, for each neurone of each input, to be set to zero\n");
+
+    declareOption(ol, "input_reconstruction_lr", 
+                  &DenoisingRecurrentNet::input_reconstruction_lr,
+                  OptionBase::buildoption,
+                  "The learning rate used for the reconstruction\n");
+
+    declareOption(ol, "hidden_noise_prob", 
+                  &DenoisingRecurrentNet::hidden_noise_prob,
+                  OptionBase::buildoption,
+                  "Probability, for each neurone of each hidden layer, to be set to zero\n");
+
+    declareOption(ol, "hidden_reconstruction_lr", 
+                  &DenoisingRecurrentNet::hidden_reconstruction_lr,
+                  OptionBase::buildoption,
+                  "The learning rate used for the dynamic reconstruction through time\n");
+
+    declareOption(ol, "tied_hidden_reconstruction_weights", 
+                  &DenoisingRecurrentNet::tied_hidden_reconstruction_weights,
+                  OptionBase::buildoption,
+                  "Do we want the dynamic reconstruction weights tied or not\n"
+                  "Boolean, yes or no");
+
+    declareOption(ol, "noisy_recurrent_lr", 
+                  &DenoisingRecurrentNet::noisy_recurrent_lr,
+                  OptionBase::buildoption,
+                  "The learning rate used in the noisy recurrent phase for the input reconstruction\n");
+
+    declareOption(ol, "dynamic_gradient_scale_factor", 
+                  &DenoisingRecurrentNet::dynamic_gradient_scale_factor,
+                  OptionBase::buildoption,
+                  "The scale factor of the learning rate used in the noisy recurrent phase for the dynamic hidden reconstruction\n");
+
+    declareOption(ol, "recurrent_lr", 
+                  &DenoisingRecurrentNet::recurrent_lr,
+                  OptionBase::buildoption,
+                  "The learning rate used in the fine tuning phase\n");
+
+
+ /*
     declareOption(ol, "", &DenoisingRecurrentNet::,
                   OptionBase::learntoption,
                   "");
@@ -209,6 +258,8 @@
 
     if(train_set)
     {
+        use_target_layers_masks = (encoding=="raw_masked_supervised");
+
         PLASSERT( target_layers_weights.length() == target_layers.length() );
         PLASSERT( target_connections.length() == target_layers.length() );
         PLASSERT( target_layers.length() > 0 );
@@ -497,15 +548,19 @@
 
         MODULE_LOG << "  stage = " << stage << endl;
         MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  recurrent_net_learning_rate = " << recurrent_net_learning_rate << endl;
+        MODULE_LOG << "  input_noise_prob = " <<                input_noise_prob  << endl;              
+        MODULE_LOG << "  input_reconstruction_lr = " <<         input_reconstruction_lr  << endl;       
+        MODULE_LOG << "  hidden_noise_prob = " <<               hidden_noise_prob  << endl;             
+        MODULE_LOG << "  hidden_reconstruction_lr = " <<        hidden_reconstruction_lr  << endl;      
+        MODULE_LOG << "  noisy_recurrent_lr = " <<              noisy_recurrent_lr  << endl;            
+        MODULE_LOG << "  dynamic_gradient_scale_factor = " <<   dynamic_gradient_scale_factor  << endl; 
+        MODULE_LOG << "  recurrent_lr = " <<                    recurrent_lr  << endl;                  
 
+
         if( report_progress && stage < end_stage )
             pb = new ProgressBar( "Recurrent training phase of "+classname(),
                                   end_stage - init_stage );
 
-        // TO DO: check this line
-        setLearningRate( recurrent_net_learning_rate );
-
         while(stage < end_stage)
         {
             train_costs.clear();
@@ -516,8 +571,36 @@
             {
                 getSequence(i, seq);
                 encodeSequenceAndPopulateLists(seq);
-                fprop(train_costs, train_n_items);
-                recurrent_update();
+              
+                bool corrupt_input = input_noise_prob!=0 && (noisy_recurrent_lr!=0 || input_reconstruction_lr!=0);
+
+                clean_encoded_seq.resize(encoded_seq.length(), encoded_seq.width());
+                clean_encoded_seq << encoded_seq;
+
+                if(corrupt_input)  // WARNING: encoded_sequence will be dirty!!!!
+                    inject_zero_forcing_noise(encoded_seq, input_noise_prob);
+                
+                // greedy phase
+                if(input_reconstruction_lr!=0 || hidden_reconstruction_lr!=0)
+                    performGreedyDenoisingPhase();
+
+                // recurrent noisy phase
+                if(noisy_recurrent_lr!=0)
+                {
+                    setLearningRate( noisy_recurrent_lr );
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate();
+                }
+
+                // recurrent no noise phase
+                if(recurrent_lr!=0)
+                {
+                    if(corrupt_input) // need to recover the clean sequence
+                        encoded_seq << clean_encoded_seq;                    
+                    setLearningRate( recurrent_lr );
+                    recurrentFprop(train_costs, train_n_items);
+                    recurrentUpdate();
+                }
             }
 
             if( pb )
@@ -549,6 +632,13 @@
 }
 
 
+void DenoisingRecurrentNet::performGreedyDenoisingPhase()
+{
+    // TO DO!
+    PLERROR("performGreedyDenoisingPhase not yet implemented");
+}
+
+
 //! does encoding if needed and populates the list.
 void DenoisingRecurrentNet::encodeSequenceAndPopulateLists(Mat seq) const
 {
@@ -558,24 +648,30 @@
         encodeAndCreateSupervisedSequence(seq);
 }
 
-// TO DO: penser a gestion des prepended dans ce cas
-// encodes sequ, then populates: inputslist, targets_list, masks_list
+// encodes sequ, then populates: input_list, targets_list, masks_list
 void DenoisingRecurrentNet::encodeAndCreateSupervisedSequence(Mat seq) const
 {
+    if(use_target_layers_masks)
+        PLERROR("Bug: use_target_layers_masks is expected to be false (no masks) when in encodeAndCreateSupervisedSequence");
+
     encodeSequence(seq, encoded_seq);
     // now work with encoded_seq
     int l = encoded_seq.length();
     resize_lists(l);
 
-    // TO DO: populate lists
-    // ....
-
-    PLERROR("Not implemented yet");
+    Mat targets = targets_list[0];
+    targets.resize(l, encoded_seq.width());
+                   
+    for(int t=input_window_size; t<l; t++)
+    {
+        input_list[t] = encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec();
+        // target is copied so that when adding noise to input, it doesn't modify target 
+        targets(t) << encoded_seq(t);
+    }
 }
 
 
 
-// TO DO: penser a prepend dans ce cas
 
 // For the (backward testing) raw_masked_supervised case. Populates: input_list, targets_list, masks_list
 void DenoisingRecurrentNet::splitRawMaskedSupervisedSequence(Mat seq) const
@@ -633,7 +729,7 @@
 // TODO: think properly about prepended stuff
 
 // fprop accumulates costs in costs and counts in n_items
-void DenoisingRecurrentNet::fprop(Vec train_costs, Vec train_n_items) const
+void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items) const
 {
     int l = input_list.length();
     int ntargets = target_layers.length();
@@ -709,7 +805,7 @@
 nll_list
 */
 
-void DenoisingRecurrentNet::recurrent_update()
+void DenoisingRecurrentNet::recurrentUpdate()
 {
     hidden_temporal_gradient.resize(hidden_layer->size);
     hidden_temporal_gradient.clear();
@@ -813,413 +909,10 @@
     
 }
 
-/*
-void DenoisingRecurrentNet::old_recurrent_update()
-{
-    hidden_temporal_gradient.resize(hidden_layer->size);
-    hidden_temporal_gradient.clear();
-    for(int i=hidden_list.length()-1; i>=0; i--){   
 
-        if( hidden_layer2 )
-            hidden_gradient.resize(hidden_layer2->size);
-        else
-            hidden_gradient.resize(hidden_layer->size);
-        hidden_gradient.clear();
-        if(use_target_layers_masks)
-        {
-            for( int tar=0; tar<target_layers.length(); tar++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
-                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    bias_gradient *= masks_list[tar][i];
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true);
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true);
-                }
-            }
-        }
-        else
-        {
-            for( int tar=0; tar<target_layers.length(); tar++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar][i];
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(target_prediction_list[tar][i]);
-                    target_layers[tar]->bpropNLL(targets_list[tar][i],nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true); 
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list[i],target_prediction_act_no_bias_list[tar][i],
-                                                             hidden_gradient, bias_gradient,true); 
-                        
-                }
-            }
-        }
-
-        if (hidden_layer2)
-        {
-            hidden_layer2->bpropUpdate(
-                hidden2_act_no_bias_list[i], hidden2_list[i],
-                bias_gradient, hidden_gradient);
-                
-            hidden_connections->bpropUpdate(
-                hidden_list[i],
-                hidden2_act_no_bias_list[i], 
-                hidden_gradient, bias_gradient);
-        }
-            
-        if(i!=0 && dynamic_connections )
-        {   
-            hidden_gradient += hidden_temporal_gradient;
-                
-            hidden_layer->bpropUpdate(
-                hidden_act_no_bias_list[i], hidden_list[i],
-                hidden_temporal_gradient, hidden_gradient);
-                
-            dynamic_connections->bpropUpdate(
-                hidden_list[i-1],
-                hidden_act_no_bias_list[i], // Here, it should be cond_bias, but doesn't matter
-                hidden_gradient, hidden_temporal_gradient);
-                
-            hidden_temporal_gradient << hidden_gradient;
-                
-            input_connections->bpropUpdate(
-                input_list[i],
-                hidden_act_no_bias_list[i], 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-                
-        }
-        else
-        {
-            hidden_layer->bpropUpdate(
-                hidden_act_no_bias_list[i], hidden_list[i],
-                hidden_temporal_gradient, hidden_gradient); // Not really temporal gradient, but this is the final iteration...
-            input_connections->bpropUpdate(
-                input_list[i],
-                hidden_act_no_bias_list[i], 
-                visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
-
-        }
-    }
-    
-}
-*/
-
-
-/*
-void DenoisingRecurrentNet::oldtrain()
-{
-    MODULE_LOG << "train() called " << endl;
-
-    Vec input( inputsize() );
-    Vec target( targetsize() );
-    real weight = 0; // Unused
-    Vec train_costs( getTrainCostNames().length() );
-    train_costs.clear();
-    Vec train_n_items( getTrainCostNames().length() );
-
-    if( !initTrain() )
-    {
-        MODULE_LOG << "train() aborted" << endl;
-        return;
-    }
-
-    ProgressBar* pb = 0;
-
-    // clear stats of previous epoch
-    train_stats->forget();
-
-
-//    if(rbm_stage < rbm_nstages)
-//    {
-//    }
-
-
-    if( stage >= nstages )
-        return;
-
-    if( stage < nstages )
-    {        
-
-        MODULE_LOG << "Training the whole model" << endl;
-
-        int init_stage = stage;
-        //int end_stage = max(0,nstages-(rbm_nstages + dynamic_nstages));
-        int end_stage = nstages;
-
-        MODULE_LOG << "  stage = " << stage << endl;
-        MODULE_LOG << "  end_stage = " << end_stage << endl;
-        MODULE_LOG << "  recurrent_net_learning_rate = " << recurrent_net_learning_rate << endl;
-
-        if( report_progress && stage < end_stage )
-            pb = new ProgressBar( "Recurrent training phase of "+classname(),
-                                  end_stage - init_stage );
-
-        setLearningRate( recurrent_net_learning_rate );
-
-        int ith_sample_in_sequence = 0;
-        int inputsize_without_masks = inputsize() 
-            - ( use_target_layers_masks ? targetsize() : 0 );
-        int sum_target_elements = 0;
-        while(stage < end_stage)
-        {
-            train_costs.clear();
-            train_n_items.clear();
-            for(int sample=0 ; sample<train_set->length() ; sample++ )
-            {
-                train_set->getExample(sample, input, target, weight);
-
-                if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
-                {
-                    //update
-                    recurrent_update();
-                    
-                    ith_sample_in_sequence = 0;
-                    hidden_list.resize(0);
-                    hidden_act_no_bias_list.resize(0);
-                    hidden2_list.resize(0);
-                    hidden2_act_no_bias_list.resize(0);
-                    target_prediction_list.resize(0);
-                    target_prediction_act_no_bias_list.resize(0);
-                    input_list.resize(0);
-                    targets_list.resize(0);
-                    nll_list.resize(0,0);
-                    masks_list.resize(0);
-                    continue;
-                }
-
-                // Resize internal variables
-                hidden_list.resize(ith_sample_in_sequence+1);
-                hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
-                if( hidden_layer2 )
-                {
-                    hidden2_list.resize(ith_sample_in_sequence+1);
-                    hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
-                }
-                 
-                input_list.resize(ith_sample_in_sequence+1);
-                input_list[ith_sample_in_sequence].resize(input_layer->size);
-
-                targets_list.resize( target_layers.length() );
-                target_prediction_list.resize( target_layers.length() );
-                target_prediction_act_no_bias_list.resize( target_layers.length() );
-                for( int tar=0; tar < target_layers.length(); tar++ )
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {                        
-                        targets_list[tar].resize( ith_sample_in_sequence+1);
-                        targets_list[tar][ith_sample_in_sequence].resize( 
-                            target_layers[tar]->size);
-                        target_prediction_list[tar].resize(
-                            ith_sample_in_sequence+1);
-                        target_prediction_act_no_bias_list[tar].resize(
-                            ith_sample_in_sequence+1);
-                    }
-                }
-                nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
-                if( use_target_layers_masks )
-                {
-                    masks_list.resize( target_layers.length() );
-                    for( int tar=0; tar < target_layers.length(); tar++ )
-                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                            masks_list[tar].resize( ith_sample_in_sequence+1 );
-                }
-
-                // Forward propagation
-
-                // Fetch right representation for input
-                clamp_units(input.subVec(0,inputsize_without_masks),
-                            input_layer,
-                            input_symbol_sizes);                
-                input_list[ith_sample_in_sequence] << input_layer->expectation;
-
-                // Fetch right representation for target
-                sum_target_elements = 0;
-                for( int tar=0; tar < target_layers.length(); tar++ )
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {
-                        if( use_target_layers_masks )
-                        {
-                            clamp_units(target.subVec(
-                                            sum_target_elements,
-                                            target_layers_n_of_target_elements[tar]),
-                                        target_layers[tar],
-                                        target_symbol_sizes[tar],
-                                        input.subVec(
-                                            inputsize_without_masks 
-                                            + sum_target_elements, 
-                                            target_layers_n_of_target_elements[tar]),
-                                        masks_list[tar][ith_sample_in_sequence]
-                                );
-                            
-                        }
-                        else
-                        {
-                            clamp_units(target.subVec(
-                                            sum_target_elements,
-                                            target_layers_n_of_target_elements[tar]),
-                                        target_layers[tar],
-                                        target_symbol_sizes[tar]);
-                        }
-                        targets_list[tar][ith_sample_in_sequence] << 
-                            target_layers[tar]->expectation;
-                    }
-                    sum_target_elements += target_layers_n_of_target_elements[tar];
-                }
-                
-                input_connections->fprop( input_list[ith_sample_in_sequence], 
-                                          hidden_act_no_bias_list[ith_sample_in_sequence]);
-                
-                if( ith_sample_in_sequence > 0 && dynamic_connections )
-                {
-                    dynamic_connections->fprop( 
-                        hidden_list[ith_sample_in_sequence-1],
-                        dynamic_act_no_bias_contribution );
-
-                    hidden_act_no_bias_list[ith_sample_in_sequence] += 
-                        dynamic_act_no_bias_contribution;
-                }
-                 
-                hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
-                                     hidden_list[ith_sample_in_sequence] );
-                 
-                if( hidden_layer2 )
-                {
-                    hidden_connections->fprop( 
-                        hidden_list[ith_sample_in_sequence],
-                        hidden2_act_no_bias_list[ith_sample_in_sequence]);
-
-                    hidden_layer2->fprop( 
-                        hidden2_act_no_bias_list[ith_sample_in_sequence],
-                        hidden2_list[ith_sample_in_sequence] 
-                        );
-
-                    for( int tar=0; tar < target_layers.length(); tar++ )
-                    {
-                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                        {
-                            target_connections[tar]->fprop(
-                                hidden2_list[ith_sample_in_sequence],
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence]
-                                );
-                            target_layers[tar]->fprop(
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence],
-                                target_prediction_list[tar][
-                                    ith_sample_in_sequence] );
-                            if( use_target_layers_masks )
-                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                                    masks_list[tar][ith_sample_in_sequence];
-                        }
-                    }
-                }
-                else
-                {
-                    for( int tar=0; tar < target_layers.length(); tar++ )
-                    {
-                        if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                        {
-                            target_connections[tar]->fprop(
-                                hidden_list[ith_sample_in_sequence],
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence]
-                                );
-                            target_layers[tar]->fprop(
-                                target_prediction_act_no_bias_list[tar][
-                                    ith_sample_in_sequence],
-                                target_prediction_list[tar][
-                                    ith_sample_in_sequence] );
-                            if( use_target_layers_masks )
-                                target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                                    masks_list[tar][ith_sample_in_sequence];
-                        }
-                    }
-                }
-
-                sum_target_elements = 0;
-                for( int tar=0; tar < target_layers.length(); tar++ )
-                {
-                    if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    {
-                        target_layers[tar]->activation << 
-                            target_prediction_act_no_bias_list[tar][
-                                ith_sample_in_sequence];
-                        target_layers[tar]->activation += target_layers[tar]->bias;
-                        target_layers[tar]->setExpectation(
-                            target_prediction_list[tar][
-                                ith_sample_in_sequence]);
-                        nll_list(ith_sample_in_sequence,tar) = 
-                            target_layers[tar]->fpropNLL( 
-                                targets_list[tar][ith_sample_in_sequence] ); 
-                        train_costs[tar] += nll_list(ith_sample_in_sequence,tar);
-                        
-                        // Normalize by the number of things to predict
-                        if( use_target_layers_masks )
-                        {
-                            train_n_items[tar] += sum(
-                                input.subVec( inputsize_without_masks 
-                                              + sum_target_elements, 
-                                              target_layers_n_of_target_elements[tar]) );
-                        }
-                        else
-                            train_n_items[tar]++;
-                    }
-                    if( use_target_layers_masks )
-                        sum_target_elements += 
-                            target_layers_n_of_target_elements[tar];
-                    
-                }
-                ith_sample_in_sequence++;
-            }
-            if( pb )
-                pb->update( stage + 1 - init_stage);
-            
-            for(int i=0; i<train_costs.length(); i++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[i],0) )
-                    train_costs[i] /= train_n_items[i];
-                else
-                    train_costs[i] = MISSING_VALUE;
-            }
-
-            if(verbosity>0)
-                cout << "mean costs at stage " << stage << 
-                    " = " << train_costs << endl;
-            stage++;
-            train_stats->update(train_costs);
-        }    
-        if( pb )
-        {
-            delete pb;
-            pb = 0;
-        }
-
-    }
-
-
-    train_stats->finalize();
-}
-*/
-
 /* TO DO:
 verifier nombre de temps
-implementer correctmeent duration_to_number_of_timeframes(duration)
+implementer correctement duration_to_number_of_timeframes(duration)
 declare nouvelles options et valeurs par defaut correctes
 */
 
@@ -1235,8 +928,8 @@
       ou -1 (missing)
       ou -999 (fin de sequence)
 
-duree: 1 double-croche
-       2 
+duree: 0 double-croche
+       1 
 ..16   exprimee en 1/16 de mesure (resultat du quantize de Stan)
 
 
@@ -1253,11 +946,11 @@
     if(encoding=="timeframe")
         encode_onehot_timeframe(sequence, encoded_seq, prepend_zero_rows);
     else if(encoding=="note_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows);
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 0);
     else if(encoding=="note_octav_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 4);    
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 5);    
     else if(encoding=="raw_masked_supervised")
-        PLERROR("raw_masked_supervised encoding not yet implemented");
+        PLERROR("raw_masked_supervised means already encoded! You shouldnt have landed here!!!");
     else if(encoding=="generic")
         PLERROR("generic encoding not yet implemented");
     else
@@ -1370,6 +1063,7 @@
 
 int DenoisingRecurrentNet::duration_to_number_of_timeframes(int duration)
 {
+    PLERROR("duration_to_number_of_timeframes (used only when encoding==timeframe) is not yet implemented");
     return duration+1;
 }
 
@@ -1495,7 +1189,7 @@
     hidden_layer->setLearningRate( the_learning_rate );
     input_connections->setLearningRate( the_learning_rate );
     if( dynamic_connections )
-        dynamic_connections->setLearningRate( the_learning_rate ); //HUGO: multiply by dynamic_connections_learning_weight;
+        dynamic_connections->setLearningRate( dynamic_gradient_scale_factor*the_learning_rate ); 
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );
@@ -1569,7 +1263,7 @@
         seq.resize(seqlen, w);
         testset->getMat(start,0,seq);
         encodeSequenceAndPopulateLists(seq);
-        fprop(costs, n_items);
+        recurrentFprop(costs, n_items);
 
         if (testoutputs)
         {
@@ -1611,295 +1305,7 @@
         test_stats->update(costs, 1.);
 }
 
-/*
-void DenoisingRecurrentNet::oldtest(VMat testset, PP<VecStatsCollector> test_stats,
-                  VMat testoutputs, VMat testcosts)const
-{ 
 
-    int len = testset.length();
-    Vec input;
-    Vec target;
-    real weight;
-
-    Vec output(outputsize());
-    output.clear();
-    Vec costs(nTestCosts());
-    costs.clear();
-    Vec n_items(nTestCosts());
-    n_items.clear();
-
-    PP<ProgressBar> pb;
-    if (report_progress) 
-        pb = new ProgressBar("Testing learner", len);
-
-    if (len == 0) {
-        // Empty test set: we give -1 cost arbitrarily.
-        costs.fill(-1);
-        test_stats->update(costs);
-    }
-    
-    int ith_sample_in_sequence = 0;
-    int inputsize_without_masks = inputsize() 
-        - ( use_target_layers_masks ? targetsize() : 0 );
-    int sum_target_elements = 0;
-    for (int i = 0; i < len; i++)
-    {
-        testset.getExample(i, input, target, weight);
-
-        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
-        {
-            ith_sample_in_sequence = 0;
-            hidden_list.resize(0);
-            hidden_act_no_bias_list.resize(0);
-            hidden2_list.resize(0);
-            hidden2_act_no_bias_list.resize(0);
-            target_prediction_list.resize(0);
-            target_prediction_act_no_bias_list.resize(0);
-            input_list.resize(0);
-            targets_list.resize(0);
-            nll_list.resize(0,0);
-            masks_list.resize(0);
-
-            if (testoutputs)
-            {
-                output.fill(end_of_sequence_symbol);
-                testoutputs->putOrAppendRow(i, output);
-            }
-
-            continue;
-        }
-
-        // Resize internal variables
-        hidden_list.resize(ith_sample_in_sequence+1);
-        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
-        if( hidden_layer2 )
-        {
-            hidden2_list.resize(ith_sample_in_sequence+1);
-            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
-        }
-                 
-        input_list.resize(ith_sample_in_sequence+1);
-        input_list[ith_sample_in_sequence].resize(input_layer->size);
-
-        targets_list.resize( target_layers.length() );
-        target_prediction_list.resize( target_layers.length() );
-        target_prediction_act_no_bias_list.resize( target_layers.length() );
-        for( int tar=0; tar < target_layers.length(); tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                targets_list[tar].resize( ith_sample_in_sequence+1);
-                targets_list[tar][ith_sample_in_sequence].resize( 
-                    target_layers[tar]->size);
-                target_prediction_list[tar].resize(
-                    ith_sample_in_sequence+1);
-                target_prediction_act_no_bias_list[tar].resize(
-                    ith_sample_in_sequence+1);
-            }
-        }
-        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
-        if( use_target_layers_masks )
-        {
-            masks_list.resize( target_layers.length() );
-            for( int tar=0; tar < target_layers.length(); tar++ )
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                    masks_list[tar].resize( ith_sample_in_sequence+1 );
-        }
-
-        // Forward propagation
-
-        // Fetch right representation for input
-        clamp_units(input.subVec(0,inputsize_without_masks),
-                    input_layer,
-                    input_symbol_sizes);                
-        input_list[ith_sample_in_sequence] << input_layer->expectation;
-
-        // Fetch right representation for target
-        sum_target_elements = 0;
-        for( int tar=0; tar < target_layers.length(); tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                if( use_target_layers_masks )
-                {
-                    clamp_units(target.subVec(
-                                    sum_target_elements,
-                                    target_layers_n_of_target_elements[tar]),
-                                target_layers[tar],
-                                target_symbol_sizes[tar],
-                                input.subVec(
-                                    inputsize_without_masks 
-                                    + sum_target_elements, 
-                                    target_layers_n_of_target_elements[tar]),
-                                masks_list[tar][ith_sample_in_sequence]
-                        );
-                    
-                }
-                else
-                {
-                    clamp_units(target.subVec(
-                                    sum_target_elements,
-                                    target_layers_n_of_target_elements[tar]),
-                                target_layers[tar],
-                                target_symbol_sizes[tar]);
-                }
-                targets_list[tar][ith_sample_in_sequence] << 
-                    target_layers[tar]->expectation;
-            }
-            sum_target_elements += target_layers_n_of_target_elements[tar];
-        }
-                
-        input_connections->fprop( input_list[ith_sample_in_sequence], 
-                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
-                
-        if( ith_sample_in_sequence > 0 && dynamic_connections )
-        {
-            dynamic_connections->fprop( 
-                hidden_list[ith_sample_in_sequence-1],
-                dynamic_act_no_bias_contribution );
-
-            hidden_act_no_bias_list[ith_sample_in_sequence] += 
-                dynamic_act_no_bias_contribution;
-        }
-                 
-        hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
-                             hidden_list[ith_sample_in_sequence] );
-                 
-        if( hidden_layer2 )
-        {
-            hidden_connections->fprop( 
-                hidden_list[ith_sample_in_sequence],
-                hidden2_act_no_bias_list[ith_sample_in_sequence]);
-
-            hidden_layer2->fprop( 
-                hidden2_act_no_bias_list[ith_sample_in_sequence],
-                hidden2_list[ith_sample_in_sequence] 
-                );
-
-            for( int tar=0; tar < target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_connections[tar]->fprop(
-                        hidden2_list[ith_sample_in_sequence],
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence]
-                        );
-                    target_layers[tar]->fprop(
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence],
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence] );
-                    if( use_target_layers_masks )
-                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                            masks_list[tar][ith_sample_in_sequence];
-                }
-            }
-        }
-        else
-        {
-            for( int tar=0; tar < target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_connections[tar]->fprop(
-                        hidden_list[ith_sample_in_sequence],
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence]
-                        );
-                    target_layers[tar]->fprop(
-                        target_prediction_act_no_bias_list[tar][
-                            ith_sample_in_sequence],
-                        target_prediction_list[tar][
-                            ith_sample_in_sequence] );
-                    if( use_target_layers_masks )
-                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
-                            masks_list[tar][ith_sample_in_sequence];
-                }
-            }
-        }
-
-        if (testoutputs)
-        {
-            int sum_target_layers_size = 0;
-            for( int tar=0; tar < target_layers.length(); tar++ )
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    output.subVec(sum_target_layers_size,target_layers[tar]->size)
-                        << target_prediction_list[tar][ ith_sample_in_sequence ];
-                }
-                sum_target_layers_size += target_layers[tar]->size;
-            }
-            testoutputs->putOrAppendRow(i, output);
-        }
-
-        sum_target_elements = 0;
-        for( int tar=0; tar < target_layers.length(); tar++ )
-        {
-            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-            {
-                target_layers[tar]->activation << 
-                    target_prediction_act_no_bias_list[tar][
-                        ith_sample_in_sequence];
-                target_layers[tar]->activation += target_layers[tar]->bias;
-                target_layers[tar]->setExpectation(
-                    target_prediction_list[tar][
-                        ith_sample_in_sequence]);
-                nll_list(ith_sample_in_sequence,tar) = 
-                    target_layers[tar]->fpropNLL( 
-                        targets_list[tar][ith_sample_in_sequence] ); 
-                costs[tar] += nll_list(ith_sample_in_sequence,tar);
-                
-                // Normalize by the number of things to predict
-                if( use_target_layers_masks )
-                {
-                    n_items[tar] += sum(
-                        input.subVec( inputsize_without_masks 
-                                      + sum_target_elements, 
-                                      target_layers_n_of_target_elements[tar]) );
-                }
-                else
-                    n_items[tar]++;
-            }
-            if( use_target_layers_masks )
-                sum_target_elements += 
-                    target_layers_n_of_target_elements[tar];
-        }
-        ith_sample_in_sequence++;
-
-        if (report_progress)
-            pb->update(i);
-
-    }
-
-    for(int i=0; i<costs.length(); i++)
-    {
-        if( !fast_exact_is_equal(target_layers_weights[i],0) )
-            costs[i] /= n_items[i];
-        else
-            costs[i] = MISSING_VALUE;
-    }
-    if (testcosts)
-        testcosts->putOrAppendRow(0, costs);
-    
-    if (test_stats)
-        test_stats->update(costs, weight);
-    
-    ith_sample_in_sequence = 0;
-    hidden_list.resize(0);
-    hidden_act_no_bias_list.resize(0);
-    hidden2_list.resize(0);
-    hidden2_act_no_bias_list.resize(0);
-    target_prediction_list.resize(0);
-    target_prediction_act_no_bias_list.resize(0);
-    input_list.resize(0);
-    targets_list.resize(0);
-    nll_list.resize(0,0);
-    masks_list.resize(0);   
-}
-*/
-
 TVec<string> DenoisingRecurrentNet::getTestCostNames() const
 {
     TVec<string> cost_names(0);

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-02 18:53:14 UTC (rev 9197)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-02 23:06:01 UTC (rev 9198)
@@ -68,9 +68,6 @@
     ////! The learning rate used during RBM contrastive divergence learning phase
     //real rbm_learning_rate;
 
-    //! The learning rate used during the recurrent phase
-    real recurrent_net_learning_rate;
-
     ////! Number of epochs for rbm phase
     //int rbm_nstages;
 
@@ -79,6 +76,7 @@
     
     //! Indication that a mask indicating which target to predict
     //! is present in the input part of the VMatrix dataset.
+    //! no loinger an option: this is set to true only for (encoding=="raw_masked_supervised")
     bool use_target_layers_masks;
 
     //! Value of the first input component for end-of-sequence delimiter
@@ -128,12 +126,15 @@
     int input_window_size;
 
     // Phase greedy (unsupervised)
+    bool tied_input_reconstruction_weights;
     double input_noise_prob;
     double input_reconstruction_lr;
     double hidden_noise_prob;
-    double hidden_reconstruciton_lr;
+    double hidden_reconstruction_lr;
+    bool tied_hidden_reconstruction_weights;
 
     // Phase noisy recurrent (supervised): uses input_noise_prob
+    // this phase *also* uses dynamic_gradient_scale_factor;
     double noisy_recurrent_lr;
     double dynamic_gradient_scale_factor;
     
@@ -157,7 +158,7 @@
     void encodeSequence(Mat sequence, Mat& encoded_seq) const;
 
     static void encode_onehot_note_octav_duration(Mat sequence, Mat& encoded_sequence, int prepend_zero_rows,
-                                                  bool use_silence=true, int octav_nbits=0, int duration_nbits=8);
+                                                  bool use_silence, int octav_nbits, int duration_nbits=20);
     
     static void encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, 
                                         int prepend_zero_rows, bool use_silence=true);    
@@ -253,7 +254,7 @@
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
-    void recurrent_update();
+    void recurrentUpdate();
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -344,7 +345,8 @@
     mutable TVec<int> testset_boundaries;
 
     mutable Mat seq; // contains the current train or test sequence
-    mutable Mat encoded_seq; // contains encoded version of current train or test sequence
+    mutable Mat encoded_seq; // contains encoded version of current train or test sequence (possibly corrupted by noise)
+    mutable Mat clean_encoded_seq; // copy of clean sequence contains encoded version of current train or test sequence
 
 protected:
     //#####  Protected Member Functions  ######################################
@@ -358,10 +360,13 @@
     //! This does the actual building.
     void build_();
 
+
+    void performGreedyDenoisingPhase();
+
     // note: the following functions are declared const because they have
     // to be called by test (which is const). Similarly, the members they 
     // manipulate are all declared mutable.
-    void fprop(Vec train_costs, Vec train_n_items) const;
+    void recurrentFprop(Vec train_costs, Vec train_n_items) const;
 
     //! does encoding if needed and populates the list.
     void encodeSequenceAndPopulateLists(Mat seq) const;



From nouiz at mail.berlios.de  Thu Jul  3 17:00:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 17:00:20 +0200
Subject: [Plearn-commits] r9199 - in trunk/plearn: base io vmat
Message-ID: <200807031500.m63F0Ktg014657@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 17:00:12 +0200 (Thu, 03 Jul 2008)
New Revision: 9199

Modified:
   trunk/plearn/base/PMemPool.cc
   trunk/plearn/io/PStreamBuf.cc
   trunk/plearn/io/PrPStreamBuf.cc
   trunk/plearn/io/fileutils.cc
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/VMatrix.cc
Log:
removed warning, printf of size_t must be done with %ld and casting the value to long to be valid. We could use %zd if we accept to require c99 as the standard for plearn. but I don't know if this is acceptable as we currently use an older standard.



Modified: trunk/plearn/base/PMemPool.cc
===================================================================
--- trunk/plearn/base/PMemPool.cc	2008-07-02 23:06:01 UTC (rev 9198)
+++ trunk/plearn/base/PMemPool.cc	2008-07-03 15:00:12 UTC (rev 9199)
@@ -58,8 +58,8 @@
       allocated_objects(0), watermark(0), free_list(0)
 {
     if (object_size < sizeof(void*))
-        PLERROR("PMemArena::PMemArena: object_size must be at least %d; passed size is %d",
-                sizeof(void*), object_size);
+        PLERROR("PMemArena::PMemArena: object_size must be at least %ld; passed size is %ld",
+                long(sizeof(void*)), long(object_size));
     size_t mem_size = object_size * max_num_objects;
     size_t num_align = mem_size / sizeof(Aligner);
     if (mem_size % sizeof(Aligner) != 0)

Modified: trunk/plearn/io/PStreamBuf.cc
===================================================================
--- trunk/plearn/io/PStreamBuf.cc	2008-07-02 23:06:01 UTC (rev 9198)
+++ trunk/plearn/io/PStreamBuf.cc	2008-07-03 15:00:12 UTC (rev 9199)
@@ -194,7 +194,7 @@
 void PStreamBuf::unread(const char* p, streamsize n)
 {
     if(streamsize(inbuf_p-inbuf)<n)
-        PLERROR("Cannot unread that many characters: %d, input buffer bound reached (you may want to increase the unget_capacity)", n);
+        PLERROR("Cannot unread that many characters: %ld, input buffer bound reached (you may want to increase the unget_capacity)", long(n));
   
     inbuf_p -= n;
     memcpy(inbuf_p,p,n);

Modified: trunk/plearn/io/PrPStreamBuf.cc
===================================================================
--- trunk/plearn/io/PrPStreamBuf.cc	2008-07-02 23:06:01 UTC (rev 9198)
+++ trunk/plearn/io/PrPStreamBuf.cc	2008-07-03 15:00:12 UTC (rev 9199)
@@ -96,7 +96,7 @@
     streamsize nwritten = ::PR_Write(out, p, PRInt32(n));
     if (nwritten != n)
         PLERROR("In PrPStreamBuf::write_ failed to write the requested number "
-                "of bytes: wrote %ld instead of %ld", nwritten, n);
+                "of bytes: wrote %ld instead of %ld",long(nwritten),long(n));
 }
   
 } // end of namespace PLearn

Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-07-02 23:06:01 UTC (rev 9198)
+++ trunk/plearn/io/fileutils.cc	2008-07-03 15:00:12 UTC (rev 9199)
@@ -418,7 +418,7 @@
         {
             in.unget(); // Match failed, unget that last character.
             PLERROR("In readWhileMatches. Failure while matching %s: "
-                    "at position %d expected a '%c', but read a '%c'",s.c_str(),i,s[i],c);
+                    "at position %ld expected a '%c', but read a '%c'",s.c_str(),long(i),s[i],c);
         }
         ++i;
         if(i==n) // passed through the whole string 

Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-02 23:06:01 UTC (rev 9198)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-03 15:00:12 UTC (rev 9199)
@@ -430,7 +430,7 @@
                 string real_val_str = map_line.substr(end_of_string + 1);
                 real real_val;
                 if (!pl_isnumber(real_val_str, &real_val))
-                    PLERROR("In TextFilesVMatrix::loadMappings - Found a mapping to something that is not a number (%s) in file %s at non-black line %d", map_line.c_str(), fname.c_str(), i);
+                    PLERROR("In TextFilesVMatrix::loadMappings - Found a mapping to something that is not a number (%s) in file %s at non-black line %ld", map_line.c_str(), fname.c_str(), long(i));
                 mapping[k][strval] = real_val;
             }
         }

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-02 23:06:01 UTC (rev 9198)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-03 15:00:12 UTC (rev 9199)
@@ -1539,8 +1539,8 @@
     if(warning_older && exist && !uptodate)
         PLWARNING("In VMatrix::isUpToDate - for class '%s'"
                   " File '%s' is older than this "
-                  "VMat's mtime of %d, and should not be re-used.",
-                  classname().c_str(), path.absolute().c_str(), getMtime());
+                  "VMat's mtime of %ld, and should not be re-used.",
+                  classname().c_str(), path.absolute().c_str(), long(getMtime()));
 
     return exist && uptodate;
 }
@@ -1564,9 +1564,9 @@
                   classname().c_str());
     if(warning_older && !uptodate)
         PLWARNING("In VMatrix::isUpToDate - for class '%s'"
-                  " The VMat with mtime of %d is older than this "
-                  "VMat's with mtime of %d, and should not be re-used.",
-                  classname().c_str(), vm->getMtime(), getMtime());
+                  " The VMat with mtime of %ld is older than this "
+                  "VMat's with mtime of %ld, and should not be re-used.",
+                  classname().c_str(), long(vm->getMtime()), long(getMtime()));
 
     return uptodate;
 }



From nouiz at mail.berlios.de  Thu Jul  3 17:01:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 17:01:31 +0200
Subject: [Plearn-commits] r9200 - trunk/plearn/math/test/VecStatsCollector
Message-ID: <200807031501.m63F1VZu014815@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 17:01:30 +0200 (Thu, 03 Jul 2008)
New Revision: 9200

Modified:
   trunk/plearn/math/test/VecStatsCollector/RemoveObservationTest.cc
Log:
removed warning in opt mode


Modified: trunk/plearn/math/test/VecStatsCollector/RemoveObservationTest.cc
===================================================================
--- trunk/plearn/math/test/VecStatsCollector/RemoveObservationTest.cc	2008-07-03 15:00:12 UTC (rev 9199)
+++ trunk/plearn/math/test/VecStatsCollector/RemoveObservationTest.cc	2008-07-03 15:01:30 UTC (rev 9200)
@@ -116,8 +116,7 @@
 compareCovariance(int t,
                   const VecStatsCollector& batch, const VecStatsCollector& online)
 {
-    int len = batch.length();
-    PLASSERT(len==online.length());
+    PLASSERT(batch.length()==online.length());
     
     batch.getCovariance(m_batch_cov);
     online.getCovariance(m_online_cov);



From nouiz at mail.berlios.de  Thu Jul  3 17:34:10 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 17:34:10 +0200
Subject: [Plearn-commits] r9201 -
	trunk/plearn_learners_experimental/SurfaceTemplate
Message-ID: <200807031534.m63FYAWK017019@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 17:34:09 +0200 (Thu, 03 Jul 2008)
New Revision: 9201

Modified:
   trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc
Log:
added missing parameter to PLERROR


Modified: trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc
===================================================================
--- trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc	2008-07-03 15:01:30 UTC (rev 9200)
+++ trunk/plearn_learners_experimental/SurfaceTemplate/Molecule.cc	2008-07-03 15:34:09 UTC (rev 9201)
@@ -118,7 +118,7 @@
     }
     else
         PLERROR("Molecule::readFromFile - File %s.vrml should contain a"
-                " 'Coordinate3' block.\n");
+                " 'Coordinate3' block.\n", filename.c_str());
 
     // Read the other geometrical informations (edges or faces)
     // Search for edges informations



From nouiz at mail.berlios.de  Thu Jul  3 17:34:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 17:34:49 +0200
Subject: [Plearn-commits] r9202 - trunk/doc
Message-ID: <200807031534.m63FYn4O017089@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 17:34:48 +0200 (Thu, 03 Jul 2008)
New Revision: 9202

Modified:
   trunk/doc/Makefile
Log:
make the tmp dir if it don't exist


Modified: trunk/doc/Makefile
===================================================================
--- trunk/doc/Makefile	2008-07-03 15:34:09 UTC (rev 9201)
+++ trunk/doc/Makefile	2008-07-03 15:34:48 UTC (rev 9202)
@@ -85,10 +85,12 @@
 
 #generic rules that transform a .dvi file to a .ps file
 %.ps: %.dvi
+	mkdir -p $(TMPDIR)
 	dvips -Pcmps -t letter $(TMPDIR)$< -o
 
 #generic rules that transform a .tex file to a .pdf file
 %.pdf: %.tex
+	mkdir -p $(TMPDIR)
 	pdflatex $(PDFTEX_OPTIONS) $<
 	cp $(TMPDIR)$@ .
 



From nouiz at mail.berlios.de  Thu Jul  3 18:39:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 18:39:09 +0200
Subject: [Plearn-commits] r9203 - in trunk/plearn: base io math misc
Message-ID: <200807031639.m63Gd9Fd002415@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 18:39:04 +0200 (Thu, 03 Jul 2008)
New Revision: 9203

Modified:
   trunk/plearn/base/PDate.cc
   trunk/plearn/base/RealMapping.cc
   trunk/plearn/io/PPath.cc
   trunk/plearn/math/pl_math.cc
   trunk/plearn/misc/viewVMat.cc
Log:
added (), {} and explicit cast to remove warning made by gcc 4.3.0


Modified: trunk/plearn/base/PDate.cc
===================================================================
--- trunk/plearn/base/PDate.cc	2008-07-03 15:34:48 UTC (rev 9202)
+++ trunk/plearn/base/PDate.cc	2008-07-03 16:39:04 UTC (rev 9203)
@@ -120,9 +120,9 @@
     // Format "2003/01/27"
     if (date.size() == 10 && date[4] == '/' && date[7] == '/') 
     {
-        year = toint(date.substr(0,4));
-        month = toint(date.substr(5,2));
-        day = toint(date.substr(8,2));
+        year = (short)toint(date.substr(0,4));
+        month = (unsigned char)toint(date.substr(5,2));
+        day = (unsigned char)toint(date.substr(8,2));
     }
     // Format "27JAN2003" or "27-jan-2003" or "27-jan-03"
     else if((date.size()==9 || date.size()==7)
@@ -202,12 +202,13 @@
     else
         PLERROR("PDate::PDate: the passed date string is not in a known format: %s", date.c_str());
 
-    if( !isValid() )
+    if( !isValid() ){
         if(invalid_value_as_missing){
             setMissing();
             PLWARNING("Invalid date string: %s",date.c_str());
         }else
             PLERROR("Invalid date string: %s",date.c_str());
+    }
 }
 
 bool PDate::isMissing() const

Modified: trunk/plearn/base/RealMapping.cc
===================================================================
--- trunk/plearn/base/RealMapping.cc	2008-07-03 15:34:48 UTC (rev 9202)
+++ trunk/plearn/base/RealMapping.cc	2008-07-03 16:39:04 UTC (rev 9203)
@@ -119,10 +119,10 @@
 }
 
 bool RealRange::operator<(real x) const
-{ return high < x || fast_exact_is_equal(high, x) && rightbracket == '['; }
+{ return high < x || (fast_exact_is_equal(high, x) && rightbracket == '['); }
 
 bool RealRange::operator>(real x) const
-{ return low > x || fast_exact_is_equal(low, x) && leftbracket == ']'; }
+{ return low > x || (fast_exact_is_equal(low, x) && leftbracket == ']'); }
 
 bool RealRange::operator<(const RealRange& x) const
 { return high < x.low || (fast_exact_is_equal(high, x.low) && (rightbracket=='[' || x.leftbracket==']')); }

Modified: trunk/plearn/io/PPath.cc
===================================================================
--- trunk/plearn/io/PPath.cc	2008-07-03 15:34:48 UTC (rev 9202)
+++ trunk/plearn/io/PPath.cc	2008-07-03 16:39:04 UTC (rev 9203)
@@ -267,7 +267,7 @@
             PPath   next_metapath;    
             while (ppath_config.good()) {
                 ppath_config >> next_metaprotocol >> next_metapath;
-                if (next_metaprotocol.empty())
+                if (next_metaprotocol.empty()){
                     if (ppath_config.good())
                         PLERROR("In PPath::metaprotocolToMetapath - Error while parsing PPath config file (%s): read "
                                 "a blank line before reaching the end of the file",
@@ -275,6 +275,7 @@
                     else
                         // Nothing left to read.
                         break;
+                }
                 // Make sure we managed to read the metapath associated with the metaprotocol.
                 if (next_metapath.empty())
                     PLERROR("In PPath::metaprotocolToMetapath - Error in PPath config file (%s): could not read the "
@@ -939,11 +940,12 @@
     if (isEmpty() || isRoot())
         return PPath(*this);
     size_t slash_pos = rfind(_slash_char());
-    if ( slash_pos == npos )
+    if ( slash_pos == npos ){
         if (_protocol.empty())
             return ".";  
         else
             return _protocol + ":.";
+    }
     PPath result = substr(0, slash_pos + 1);
     // Remove trailing slash if it is not a root directory.
     result.removeTrailingSlash();

Modified: trunk/plearn/math/pl_math.cc
===================================================================
--- trunk/plearn/math/pl_math.cc	2008-07-03 15:34:48 UTC (rev 9202)
+++ trunk/plearn/math/pl_math.cc	2008-07-03 16:39:04 UTC (rev 9203)
@@ -77,11 +77,12 @@
               real absolute_tolerance,
               real relative_tolerance)
 {
-    if (isnan(a))
+    if (isnan(a)){
         if (isnan(b))
             return true;
         else
             return false;
+    }
     if (isnan(b))
         return false;
     if (int inf_a = isinf(a))

Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2008-07-03 15:34:48 UTC (rev 9202)
+++ trunk/plearn/misc/viewVMat.cc	2008-07-03 16:39:04 UTC (rev 9203)
@@ -300,7 +300,7 @@
                         mvprintw(y, x, valstrformat, "...");
                     else if(fast_exact_is_equal(hide_sameval, 1) && i>starti &&
                             (fast_exact_is_equal(val, oldv[j]) ||
-                             is_missing(val) && is_missing(oldv[j])))
+                             (is_missing(val) && is_missing(oldv[j]))))
                         mvprintw(y, x, valstrformat, "...");
                     else
                         mvprintw(y, x, valstrformat,



From nouiz at mail.berlios.de  Thu Jul  3 18:53:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 18:53:47 +0200
Subject: [Plearn-commits] r9204 - in trunk: plearn/python plearn/vmat/test
	plearn_learners_experimental
Message-ID: <200807031653.m63GrlVu018770@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 18:53:45 +0200 (Thu, 03 Jul 2008)
New Revision: 9204

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/vmat/test/AutoVMatrixTest.cc
   trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
Log:
removed more warning from gcc 4.3.0


Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2008-07-03 16:39:04 UTC (rev 9203)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2008-07-03 16:53:45 UTC (rev 9204)
@@ -297,7 +297,7 @@
                                     function_name);
     // pFunc: Borrowed reference if not instance_method
     bool ret= pFunc && PyCallable_Check(pFunc);
-    if(instance_method) Py_DECREF(pFunc);
+    if(instance_method) {Py_DECREF(pFunc);}
     return ret;
 }
 
@@ -334,8 +334,9 @@
         return_value = PyObject_CallObject(pFunc, NULL);
         if (! return_value)
         {
-            if(instance_method)
+            if(instance_method){
                 Py_DECREF(pFunc);
+            }
             handlePythonErrors(string("Error while calling function '")
                                + function_name
                                + "' with no params.");
@@ -346,12 +347,12 @@
     }
     else
     {
-        if(instance_method) Py_DECREF(pFunc);
+        if(instance_method) {Py_DECREF(pFunc);}
         PLERROR("PythonCodeSnippet::invoke: cannot call function '%s' (not callable).",
                 function_name);
     }
 
-    if(instance_method) Py_DECREF(pFunc);
+    if(instance_method) {Py_DECREF(pFunc);}
     //return PythonObjectWrapper(return_value);
     PythonObjectWrapper r(return_value);
     Py_DECREF(return_value);
@@ -403,7 +404,7 @@
         if (! return_value)
         {
             if(instance_method)
-                Py_DECREF(pFunc);
+            {Py_DECREF(pFunc);}
             handlePythonErrors(string("Error while calling function '")
                                + function_name
                                + "' with "
@@ -416,13 +417,13 @@
     else
     {
         if(instance_method)
-            Py_DECREF(pFunc);
+        {Py_DECREF(pFunc);}
         PLERROR("PythonCodeSnippet::invoke: cannot call function '%s'",
                 function_name);
     }
 
     if(instance_method)
-        Py_DECREF(pFunc);
+    {Py_DECREF(pFunc);}
     //return PythonObjectWrapper(return_value);
     PythonObjectWrapper r(return_value);
     Py_DECREF(return_value);

Modified: trunk/plearn/vmat/test/AutoVMatrixTest.cc
===================================================================
--- trunk/plearn/vmat/test/AutoVMatrixTest.cc	2008-07-03 16:39:04 UTC (rev 9203)
+++ trunk/plearn/vmat/test/AutoVMatrixTest.cc	2008-07-03 16:53:45 UTC (rev 9204)
@@ -145,9 +145,9 @@
     AutoVMatrix reloaded( save_to );
     bool success = ( vm.toMat().isEqual( reloaded.toMat() ) );
     if ( success )
-      MAND_LOG << "Save and load suceeded on " << save_to << endl << endl;
+    {MAND_LOG << "Save and load suceeded on " << save_to << endl << endl;}
     else
-      MAND_LOG << "!!! Save and load FAILED on " << save_to << endl << endl;
+    {MAND_LOG << "!!! Save and load FAILED on " << save_to << endl << endl;}
 }
 
 void unitTest(const PPath& path)

Modified: trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc
===================================================================
--- trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2008-07-03 16:39:04 UTC (rev 9203)
+++ trunk/plearn_learners_experimental/LinearInductiveTransferClassifier.cc	2008-07-03 16:53:45 UTC (rev 9204)
@@ -808,7 +808,7 @@
             else
                 for(int j=0; j<target.length(); j++)
                 {
-                    if(fast_exact_is_equal(target[j], 1))
+                    if(fast_exact_is_equal(target[j], 1)){
                         if(weightsize()>0)
                         {
                             externalProductScaleAcc(ww,class_reps_var->matValue(j),class_reps_var->matValue(j),weight);
@@ -818,7 +818,8 @@
                         {
                             externalProductAcc(ww,class_reps_var->matValue(j),class_reps_var->matValue(j));
                             externalProductAcc(xw,input,class_reps_var->matValue(j));
-                        }   
+                        }
+                    }
                 }
         }
         if(weight_decay > 0)



From nouiz at mail.berlios.de  Thu Jul  3 18:58:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 18:58:52 +0200
Subject: [Plearn-commits] r9205 - trunk/plearn/math
Message-ID: <200807031658.m63GwqHY023796@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 18:58:51 +0200 (Thu, 03 Jul 2008)
New Revision: 9205

Modified:
   trunk/plearn/math/StatsCollector.cc
   trunk/plearn/math/StatsCollector.h
Log:
made a function that return the value inside a StatsCollectorCounts to have then in python.


Modified: trunk/plearn/math/StatsCollector.cc
===================================================================
--- trunk/plearn/math/StatsCollector.cc	2008-07-03 16:53:45 UTC (rev 9204)
+++ trunk/plearn/math/StatsCollector.cc	2008-07-03 16:58:51 UTC (rev 9205)
@@ -456,6 +456,22 @@
         (BodyDoc("Return mean_over_kurtosis of the seen value\n"),
          RetDoc ("mean_over_kurtosis")));
 
+    declareMethod(
+        rmm, "isbinary", &StatsCollector::isbinary,
+        (BodyDoc("Return true is all value seen are binary value\n"),
+         RetDoc ("binary_")));
+
+    declareMethod(
+        rmm, "isinteger", &StatsCollector::isinteger,
+        (BodyDoc("Return true is all value seen are integer value\n"),
+         RetDoc ("integer_")));
+
+    declareMethod(
+        rmm, "getCount", &StatsCollector::getCount,
+        (BodyDoc("return the value stored in a StatsCollectorCount: (n, nbellow, sum, sumsquare, id)\n"),
+         ArgDoc ("v", "The value of the counts to lookup.\n"),
+         RetDoc ("Vec(n, nbellow, sum, sumsquare, id)")));
+
 }
 ////////////
 // build_ //

Modified: trunk/plearn/math/StatsCollector.h
===================================================================
--- trunk/plearn/math/StatsCollector.h	2008-07-03 16:53:45 UTC (rev 9204)
+++ trunk/plearn/math/StatsCollector.h	2008-07-03 16:58:51 UTC (rev 9205)
@@ -381,6 +381,20 @@
     //! @return true if all value seen are integer, false otherwise and is not
     //! defined in case where we reload an old version that have maxnvalues==0
     bool isinteger(){return integer_;}
+
+    //! @return the value stored in a StatsCollectorCount:
+    //!         (n, nbellow, sum, sumsquare, id)
+    Vec getCount(real value){
+        Vec v(0,5);
+        StatsCollectorCounts c = counts[value];
+        
+        v.append(c.n);
+        v.append(c.nbelow);
+        v.append(c.sum);
+        v.append(c.sumsquare);
+        v.append(c.id);
+        return v;
+    }
 };
 
 DECLARE_OBJECT_PTR(StatsCollector);



From tihocan at mail.berlios.de  Thu Jul  3 19:21:26 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 3 Jul 2008 19:21:26 +0200
Subject: [Plearn-commits] r9206 - trunk/plearn/var
Message-ID: <200807031721.m63HLQWC016690@sheep.berlios.de>

Author: tihocan
Date: 2008-07-03 19:21:25 +0200 (Thu, 03 Jul 2008)
New Revision: 9206

Modified:
   trunk/plearn/var/VarArray.cc
   trunk/plearn/var/VarArray.h
Log:
Added constructor with 3 variables

Modified: trunk/plearn/var/VarArray.cc
===================================================================
--- trunk/plearn/var/VarArray.cc	2008-07-03 16:58:51 UTC (rev 9205)
+++ trunk/plearn/var/VarArray.cc	2008-07-03 17:21:25 UTC (rev 9206)
@@ -46,6 +46,9 @@
 namespace PLearn {
 using namespace std;
 
+//////////////
+// VarArray //
+//////////////
 VarArray::VarArray()
     : Array<Var>(0,10) 
 {}
@@ -92,6 +95,13 @@
     (*this)[1] = Var(v2); 
 }
 
+VarArray::VarArray(Variable*  v1, Variable*  v2, Variable* v3):
+    Array<Var>(3)
+{ 
+    (*this)[0] = Var(v1); 
+    (*this)[1] = Var(v2); 
+    (*this)[2] = Var(v3); 
+}
 // This is really EXTERN!  Don't try to define it...
 //template<>
 //extern void deepCopyField(Var& field, CopiesMap& copies);
@@ -103,13 +113,18 @@
 #pragma warning(default:1419)
 #endif
 
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
 void VarArray::makeDeepCopyFromShallowCopy(CopiesMap& copies)
 {
     for(int i=0; i<size(); i++)
         varDeepCopyField((*this)[i], copies);
 }
 
-
+////////////////////
+// copyValuesFrom //
+////////////////////
 void VarArray::copyValuesFrom(const VarArray& from)
 {
     if (size()!=from.size())
@@ -119,8 +134,9 @@
         (*this)[i]->value << from[i]->value;
 }
 
-
-
+//////////////
+// copyFrom //
+//////////////
 void VarArray::copyFrom(const Vec& datavec)
 {
     copyFrom(datavec.data(),datavec.length());

Modified: trunk/plearn/var/VarArray.h
===================================================================
--- trunk/plearn/var/VarArray.h	2008-07-03 16:58:51 UTC (rev 9205)
+++ trunk/plearn/var/VarArray.h	2008-07-03 17:21:25 UTC (rev 9206)
@@ -70,6 +70,7 @@
     VarArray(const Var& v1, const Var& v2);
     VarArray(Variable* v);
     VarArray(Variable* v1, Variable* v2);
+    VarArray(Variable* v1, Variable* v2, Variable* v3);
 
     void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 



From tihocan at mail.berlios.de  Thu Jul  3 19:21:47 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Thu, 3 Jul 2008 19:21:47 +0200
Subject: [Plearn-commits] r9207 - trunk/plearn_learners/generic
Message-ID: <200807031721.m63HLlE1016781@sheep.berlios.de>

Author: tihocan
Date: 2008-07-03 19:21:47 +0200 (Thu, 03 Jul 2008)
New Revision: 9207

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Added safety assert

Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2008-07-03 17:21:25 UTC (rev 9206)
+++ trunk/plearn_learners/generic/NNet.cc	2008-07-03 17:21:47 UTC (rev 9207)
@@ -978,9 +978,14 @@
 {
     // We don't need to take into account the sampleweight, because it is
     // taken care of in stats->update.
-    if (costname=="mse")
-        return sumsquare(the_output-the_target);
-    else if (costname=="mse_onehot")
+    if (costname=="mse") {
+        // The following assert may be useful since 'operator-' on variables
+        // can be used to do subtractions on Variables of different sizes,
+        // which should not be the case in a NNet.
+        PLASSERT( the_output->length() == the_target->length() &&
+                  the_output->width() == the_target->width() );
+        return sumsquare(the_output - the_target);
+    } else if (costname=="mse_onehot")
         return onehot_squared_loss(the_output, the_target);
     else if (costname=="NLL") 
     {



From nouiz at mail.berlios.de  Thu Jul  3 20:24:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 20:24:59 +0200
Subject: [Plearn-commits] r9208 - trunk/plearn_learners/regressors
Message-ID: <200807031824.m63IOx2C023485@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 20:24:59 +0200 (Thu, 03 Jul 2008)
New Revision: 9208

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
Log:
added PLASSERT and made the code more readable


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-07-03 17:21:47 UTC (rev 9207)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-07-03 18:24:59 UTC (rev 9208)
@@ -63,18 +63,29 @@
 
 void RegressionTreeMulticlassLeave::declareOptions(OptionList& ol)
 { 
-    declareOption(ol, "multiclass_outputs", &RegressionTreeMulticlassLeave::multiclass_outputs, OptionBase::buildoption,
+    declareOption(ol, "multiclass_outputs", 
+                  &RegressionTreeMulticlassLeave::multiclass_outputs,
+                  OptionBase::buildoption,
                   "A vector of possible output values when solving a multiclass problem.\n"
                   "The leave will output the value with the largest weight sum.");
-    declareOption(ol, "objective_function", &RegressionTreeMulticlassLeave::objective_function, OptionBase::buildoption,
+    declareOption(ol, "objective_function",
+                  &RegressionTreeMulticlassLeave::objective_function,
+                  OptionBase::buildoption,
                   "The function to be used to compute the leave error.\n"
                   "Current supported values are l1 and l2 (default is l1).");
       
-    declareOption(ol, "multiclass_weights_sum", &RegressionTreeMulticlassLeave::multiclass_weights_sum, OptionBase::learntoption,
-                  "A vector to count the weight sum of each possible output for the sample in this leave.\n");
-    declareOption(ol, "l1_loss_function_factor", &RegressionTreeMulticlassLeave::l1_loss_function_factor, OptionBase::learntoption,
+    declareOption(ol, "multiclass_weights_sum",
+                  &RegressionTreeMulticlassLeave::multiclass_weights_sum,
+                  OptionBase::learntoption,
+                  "A vector to count the weight sum of each possible output "
+                  "for the sample in this leave.\n");
+    declareOption(ol, "l1_loss_function_factor",
+                  &RegressionTreeMulticlassLeave::l1_loss_function_factor,
+                  OptionBase::learntoption,
                   "2 / loss_function_weight.\n");
-    declareOption(ol, "l2_loss_function_factor", &RegressionTreeMulticlassLeave::l2_loss_function_factor, OptionBase::learntoption,
+    declareOption(ol, "l2_loss_function_factor",
+                  &RegressionTreeMulticlassLeave::l2_loss_function_factor,
+                  OptionBase::learntoption,
                   "2 / pow(loss_function_weight, 2.0).\n");
     inherited::declareOptions(ol);
 }
@@ -124,17 +135,18 @@
     length += 1;
     weights_sum += weight;
     int multiclass_found = 0;
-    for (int multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+    for (int mc_ind = 0; mc_ind < multiclass_outputs.length(); mc_ind++)
     {
-        if (target == multiclass_outputs[multiclass_ind])
+        if (target == multiclass_outputs[mc_ind])
         {
-            multiclass_weights_sum[multiclass_ind] += weight;
+            multiclass_weights_sum[mc_ind] += weight;
             multiclass_found = 1;      
             break;      
         }
     }
     if (multiclass_found < 1) 
-        PLERROR("RegressionTreeMultilassLeave: Unknown target: %g row: %d\n", target, row);
+        PLERROR("RegressionTreeMultilassLeave: Unknown target: %g row: %d\n",
+                target, row);
 }
 
 void RegressionTreeMulticlassLeave::addRow(int row, Vec outputv, Vec errorv)
@@ -151,14 +163,18 @@
     weights_sum -= weight;
     PLASSERT(length>=0);
     PLASSERT(weights_sum>=0);
-    for (int multiclass_ind = 0; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+    PLASSERT(length>0 || weights_sum==0);
+    bool found=false;
+    for (int mc_ind = 0; mc_ind < multiclass_outputs.length(); mc_ind++)
     {
-        if (target == multiclass_outputs[multiclass_ind])
+        if (target == multiclass_outputs[mc_ind])
         {
-            multiclass_weights_sum[multiclass_ind] -= weight;
+            multiclass_weights_sum[mc_ind] -= weight;
+            found=true;
             break;      
         }
     }
+    PLASSERT(found);
     getOutputAndError(outputv,errorv);
 }
 
@@ -166,7 +182,8 @@
 {
 #ifdef BOUNDCHECK
     if(multiclass_outputs.length()<=0)
-        PLERROR("In RegressionTreeMulticlassLeave::getOutputAndError() - multiclass_outputs must not be empty");
+        PLERROR("In RegressionTreeMulticlassLeave::getOutputAndError() -"
+                " multiclass_outputs must not be empty");
 #endif
     if(length==0){        
         output.clear();
@@ -174,13 +191,14 @@
         error.clear();
         return;
     }
-    int multiclass_winer = 0;
-    for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+    int mc_winer = 0;
+    for (int mc_ind = 1; mc_ind < multiclass_outputs.length(); mc_ind++)
     {
-        if (multiclass_weights_sum[multiclass_ind] > multiclass_weights_sum[multiclass_winer]) multiclass_winer = multiclass_ind;
+        if (multiclass_weights_sum[mc_ind] > multiclass_weights_sum[mc_winer])
+            mc_winer = mc_ind;
     }
-    output[0] = multiclass_outputs[multiclass_winer];
-    if (missing_leave >= 1)
+    output[0] = multiclass_outputs[mc_winer];
+    if (missing_leave)
     {
         output[1] = 0.0;
         error[0] = 0.0;
@@ -189,27 +207,33 @@
     }
     else
     {
-        output[1] = multiclass_weights_sum[multiclass_winer] / weights_sum;;
+        output[1] = multiclass_weights_sum[mc_winer] / weights_sum;;
         error[0] = 0.0;
         if (objective_function == "l1")
         {
-            for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+            for (int mc_ind = 1; mc_ind < multiclass_outputs.length();mc_ind++)
             {
-                error[0] += abs(output[0] - multiclass_outputs[multiclass_ind]) * multiclass_weights_sum[multiclass_ind];
+                error[0] += abs(output[0] - multiclass_outputs[mc_ind]) 
+                    * multiclass_weights_sum[mc_ind];
             }
             error[0] *= l1_loss_function_factor * length / weights_sum;
             if (error[0] < 1E-10) error[0] = 0.0;
-            if (error[0] > weights_sum * l1_loss_function_factor) error[2] = weights_sum * l1_loss_function_factor; else error[2] = error[0];
+            if (error[0] > weights_sum * l1_loss_function_factor)
+                error[2] = weights_sum * l1_loss_function_factor;
+            else error[2] = error[0];
         }
         else
         {
-            for (int multiclass_ind = 1; multiclass_ind < multiclass_outputs.length(); multiclass_ind++)
+            for (int mc_ind = 1; mc_ind < multiclass_outputs.length();mc_ind++)
             {
-                error[0] += pow(output[0] - multiclass_outputs[multiclass_ind], 2) * multiclass_weights_sum[multiclass_ind];
+                error[0] += pow(output[0] - multiclass_outputs[mc_ind], 2) 
+                    * multiclass_weights_sum[mc_ind];
             }
             error[0] *= l2_loss_function_factor * length / weights_sum;
             if (error[0] < 1E-10) error[0] = 0.0;
-            if (error[0] > weights_sum * l2_loss_function_factor) error[2] = weights_sum * l2_loss_function_factor; else error[2] = error[0];
+            if (error[0] > weights_sum * l2_loss_function_factor) 
+                error[2] = weights_sum * l2_loss_function_factor; 
+            else error[2] = error[0];
         }
         error[1] = (1.0 - output[1]) * length;
     }



From nouiz at mail.berlios.de  Thu Jul  3 20:43:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 20:43:06 +0200
Subject: [Plearn-commits] r9209 - in trunk/plearn_learners/regressors: .
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Stra!
 t0/Trials3
 test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0
Message-ID: <200807031843.m63Ih6jg024980@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 20:43:03 +0200 (Thu, 03 Jul 2008)
New Revision: 9209

Modified:
   trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat
Log:
bugfix in RegressionTreeMulticlassLeave. New compute the error correctly. renegerated the test.


Modified: trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/RegressionTreeMulticlassLeave.cc	2008-07-03 18:43:03 UTC (rev 9209)
@@ -211,7 +211,7 @@
         error[0] = 0.0;
         if (objective_function == "l1")
         {
-            for (int mc_ind = 1; mc_ind < multiclass_outputs.length();mc_ind++)
+            for (int mc_ind = 0; mc_ind < multiclass_outputs.length();mc_ind++)
             {
                 error[0] += abs(output[0] - multiclass_outputs[mc_ind]) 
                     * multiclass_weights_sum[mc_ind];
@@ -224,7 +224,7 @@
         }
         else
         {
-            for (int mc_ind = 1; mc_ind < multiclass_outputs.length();mc_ind++)
+            for (int mc_ind = 0; mc_ind < multiclass_outputs.length();mc_ind++)
             {
                 error[0] += pow(output[0] - multiclass_outputs[mc_ind], 2) 
                     * multiclass_weights_sum[mc_ind];

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/RUN.log	2008-07-03 18:43:03 UTC (rev 9209)
@@ -4,8 +4,8 @@
 split_values: []
 
 split_cols: 1 
-split_values: 0.14412705026016423 
+split_values: 0.547790627008831965 
 split_cols: 1 0 
-split_values: 0.14412705026016423 2.81501972474946704 
+split_values: 0.547790627008831965 1.88430442629138994 
 split_cols: 1 0 0 
-split_values: 0.14412705026016423 2.81501972474946704 3.90430461537466744 
+split_values: 0.547790627008831965 1.88430442629138994 3.07754902034128275 

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test1_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 14.2702702702702666 ;
-sumsquare_ = 3.91616564589538418 ;
-sumcube_ = 1.07470658057835911 ;
-sumfourth_ = 0.294929872424830264 ;
-min_ = 0.648648648648648574 ;
-max_ = 0.923076923076923128 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = 0.648648648648648574 ;
-last_ = 0.648648648648648574 ;
+sum_ = -13.0253164556962204 ;
+sumsquare_ = 1.40213941133083142 ;
+sumcube_ = -0.150936442542046217 ;
+sumfourth_ = 0.0162478919736128886 ;
+min_ = 0.702479338842975198 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.702479338842975198 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 288 ;
-sumsquare_ = 576 ;
-sumcube_ = 1152 ;
-sumfourth_ = 2304 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 288 ;
-sumsquare_ = 576 ;
-sumcube_ = 1152 ;
-sumfourth_ = 2304 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,15 +119,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
+sum_ = 121 ;
+sumsquare_ = 121 ;
+sumcube_ = 121 ;
+sumfourth_ = 121 ;
+min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -140,17 +140,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
+sum_ = -121 ;
+sumsquare_ = 121 ;
+sumcube_ = -121 ;
+sumfourth_ = 121 ;
 min_ = 1 ;
-max_ = 1 ;
-agmemin_ = 199 ;
+max_ = 2 ;
+agmemin_ = 197 ;
 agemax_ = 199 ;
-first_ = 1 ;
+first_ = 2 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/test2_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,10 +14,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1049 ;
-sumsquare_ = 1049 ;
-sumcube_ = 1049 ;
-sumfourth_ = 1049 ;
+sum_ = 1145 ;
+sumsquare_ = 1145 ;
+sumcube_ = 1145 ;
+sumfourth_ = 1145 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -1060.3908523908317 ;
-sumsquare_ = 291.001231841168249 ;
-sumcube_ = -79.8589659106670098 ;
-sumfourth_ = 21.9155582124898878 ;
-min_ = 0.648648648648648574 ;
-max_ = 0.923076923076923128 ;
-agmemin_ = 3863 ;
+sum_ = -356.527670258402452 ;
+sumsquare_ = 38.3792209118009708 ;
+sumcube_ = -4.13141733635762964 ;
+sumfourth_ = 0.444735687740564989 ;
+min_ = 0.702479338842975198 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 3311 ;
 agemax_ = 6830 ;
-first_ = 0.923076923076923128 ;
-last_ = 0.648648648648648574 ;
+first_ = 0.810126582278481 ;
+last_ = 0.702479338842975198 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,10 +56,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -2098 ;
-sumsquare_ = 4196 ;
-sumcube_ = -8392 ;
-sumfourth_ = 16784 ;
+sum_ = -2290 ;
+sumsquare_ = 4580 ;
+sumcube_ = -9160 ;
+sumfourth_ = 18320 ;
 min_ = -1 ;
 max_ = 1 ;
 agmemin_ = 4899 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -2098 ;
-sumsquare_ = 4196 ;
-sumcube_ = -8392 ;
-sumfourth_ = 16784 ;
+sum_ = -2290 ;
+sumsquare_ = 4580 ;
+sumcube_ = -9160 ;
+sumfourth_ = 18320 ;
 min_ = -1 ;
 max_ = 1 ;
 agmemin_ = 4899 ;
@@ -98,10 +98,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1049 ;
-sumsquare_ = 1049 ;
-sumcube_ = 1049 ;
-sumfourth_ = 1049 ;
+sum_ = 1145 ;
+sumsquare_ = 1145 ;
+sumcube_ = 1145 ;
+sumfourth_ = 1145 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
@@ -119,15 +119,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
+sum_ = 3312 ;
+sumsquare_ = 3312 ;
+sumcube_ = 3312 ;
+sumfourth_ = 3312 ;
+min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
-agemax_ = 6830 ;
-first_ = 1 ;
+agemax_ = 3311 ;
+first_ = 0 ;
 last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -140,17 +140,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
+sum_ = -3312 ;
+sumsquare_ = 3312 ;
+sumcube_ = -3312 ;
+sumfourth_ = 3312 ;
 min_ = 1 ;
-max_ = 1 ;
-agmemin_ = 6830 ;
+max_ = 2 ;
+agmemin_ = 3311 ;
 agemax_ = 6830 ;
-first_ = 1 ;
+first_ = 2 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/Split0/train_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 14.2702702702702666 ;
-sumsquare_ = 3.91616564589538418 ;
-sumcube_ = 1.07470658057835911 ;
-sumfourth_ = 0.294929872424830264 ;
-min_ = 0.648648648648648574 ;
-max_ = 0.923076923076923128 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = 0.648648648648648574 ;
-last_ = 0.648648648648648574 ;
+sum_ = -13.0253164556962204 ;
+sumsquare_ = 1.40213941133083142 ;
+sumcube_ = -0.150936442542046217 ;
+sumfourth_ = 0.0162478919736128886 ;
+min_ = 0.702479338842975198 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.702479338842975198 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 288 ;
-sumsquare_ = 576 ;
-sumcube_ = 1152 ;
-sumfourth_ = 2304 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 288 ;
-sumsquare_ = 576 ;
-sumcube_ = 1152 ;
-sumfourth_ = 2304 ;
+sum_ = -102 ;
+sumsquare_ = 204 ;
+sumcube_ = -408 ;
+sumfourth_ = 816 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 51 ;
+sumsquare_ = 51 ;
+sumcube_ = 51 ;
+sumfourth_ = 51 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,15 +119,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
+sum_ = 121 ;
+sumsquare_ = 121 ;
+sumcube_ = 121 ;
+sumfourth_ = 121 ;
+min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -140,17 +140,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
+sum_ = -121 ;
+sumsquare_ = 121 ;
+sumcube_ = -121 ;
+sumfourth_ = 121 ;
 min_ = 1 ;
-max_ = 1 ;
-agmemin_ = 199 ;
+max_ = 2 ;
+agmemin_ = 197 ;
 agemax_ = 199 ;
-first_ = 1 ;
+first_ = 2 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials1/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test1_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -145 ;
-sumsquare_ = 145 ;
-sumcube_ = -145 ;
-sumfourth_ = 145 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 15.2702702702702631 ;
-sumsquare_ = 4.48764662914106438 ;
-sumcube_ = 1.3200231277098966 ;
-sumfourth_ = 0.388694875255762629 ;
-min_ = 0.648648648648648574 ;
-max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 46 ;
-first_ = 0.648648648648648574 ;
-last_ = 0.648648648648648574 ;
+sum_ = -5.02531645569621244 ;
+sumsquare_ = 0.321449501143033556 ;
+sumcube_ = -0.0289169130068278983 ;
+sumfourth_ = 0.0030033253501558126 ;
+min_ = 0.699999999999999845 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 290 ;
-sumsquare_ = 580 ;
-sumcube_ = 1160 ;
-sumfourth_ = 2320 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 290 ;
-sumsquare_ = 580 ;
-sumcube_ = 1160 ;
-sumfourth_ = 2320 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 141 ;
+sumsquare_ = 181 ;
+sumcube_ = 261 ;
+sumfourth_ = 421 ;
+min_ = 0 ;
+max_ = 2 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 190 ;
+first_ = 0 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,16 +140,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 51 ;
-sumsquare_ = 51 ;
-sumcube_ = 51 ;
-sumfourth_ = 51 ;
+sum_ = -20 ;
+sumsquare_ = 20 ;
+sumcube_ = -20 ;
+sumfourth_ = 20 ;
 min_ = 1 ;
 max_ = 2 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = 1 ;
-last_ = 1 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
 binary_ = 0 ;
 integer_ = 1 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/test2_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1507 ;
-sumsquare_ = 1507 ;
-sumcube_ = 1507 ;
-sumfourth_ = 1507 ;
+sum_ = 1229 ;
+sumsquare_ = 1229 ;
+sumcube_ = 1229 ;
+sumfourth_ = 1229 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
-agemax_ = 6775 ;
+agemax_ = 4899 ;
 first_ = 0 ;
-last_ = 0 ;
+last_ = 1 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -1094.91573926876504 ;
-sumsquare_ = 332.735287798785521 ;
-sumcube_ = -96.6024483526156672 ;
-sumfourth_ = 28.3019556147200539 ;
-min_ = 0.648648648648648574 ;
-max_ = 1 ;
-agmemin_ = 3863 ;
-agemax_ = 6775 ;
-first_ = 0.941176470588235392 ;
-last_ = 0.648648648648648574 ;
+sum_ = -187.234290011284173 ;
+sumsquare_ = 15.6584993949765128 ;
+sumcube_ = -1.58576593913658415 ;
+sumfourth_ = 0.170759912175163953 ;
+min_ = 0.699999999999999845 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 3266 ;
+agemax_ = 6830 ;
+first_ = 0.810126582278481 ;
+last_ = 0.699999999999999845 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,16 +56,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -3014 ;
-sumsquare_ = 6028 ;
-sumcube_ = -12056 ;
-sumfourth_ = 24112 ;
+sum_ = -2458 ;
+sumsquare_ = 4916 ;
+sumcube_ = -9832 ;
+sumfourth_ = 19664 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 6775 ;
+agmemin_ = 4899 ;
 agemax_ = 6830 ;
 first_ = 1 ;
-last_ = 1 ;
+last_ = -1 ;
 binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
@@ -77,16 +77,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -3014 ;
-sumsquare_ = 6028 ;
-sumcube_ = -12056 ;
-sumfourth_ = 24112 ;
+sum_ = -2458 ;
+sumsquare_ = 4916 ;
+sumcube_ = -9832 ;
+sumfourth_ = 19664 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 6775 ;
+agmemin_ = 4899 ;
 agemax_ = 6830 ;
 first_ = 1 ;
-last_ = 1 ;
+last_ = -1 ;
 binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
@@ -98,17 +98,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 2109 ;
-sumsquare_ = 3169 ;
-sumcube_ = 5289 ;
-sumfourth_ = 9529 ;
+sum_ = 1229 ;
+sumsquare_ = 1229 ;
+sumcube_ = 1229 ;
+sumfourth_ = 1229 ;
 min_ = 0 ;
-max_ = 2 ;
+max_ = 1 ;
 agmemin_ = 6830 ;
-agemax_ = 6775 ;
+agemax_ = 4899 ;
 first_ = 0 ;
-last_ = 0 ;
-binary_ = 0 ;
+last_ = 1 ;
+binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 4464 ;
+sumsquare_ = 6768 ;
+sumcube_ = 11376 ;
+sumfourth_ = 20592 ;
+min_ = 0 ;
+max_ = 2 ;
 agmemin_ = 6830 ;
-agemax_ = 6830 ;
-first_ = 1 ;
-last_ = 1 ;
-binary_ = 1 ;
+agemax_ = 3266 ;
+first_ = 0 ;
+last_ = 2 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,13 +140,13 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -4466 ;
-sumsquare_ = 4466 ;
-sumcube_ = -4466 ;
-sumfourth_ = 4466 ;
+sum_ = -1152 ;
+sumsquare_ = 1152 ;
+sumcube_ = -1152 ;
+sumfourth_ = 1152 ;
 min_ = 1 ;
 max_ = 2 ;
-agmemin_ = 6775 ;
+agmemin_ = 3266 ;
 agemax_ = 6830 ;
 first_ = 2 ;
 last_ = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/Split0/train_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -145 ;
-sumsquare_ = 145 ;
-sumcube_ = -145 ;
-sumfourth_ = 145 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 15.2702702702702631 ;
-sumsquare_ = 4.48764662914106438 ;
-sumcube_ = 1.3200231277098966 ;
-sumfourth_ = 0.388694875255762629 ;
-min_ = 0.648648648648648574 ;
-max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 46 ;
-first_ = 0.648648648648648574 ;
-last_ = 0.648648648648648574 ;
+sum_ = -5.02531645569621244 ;
+sumsquare_ = 0.321449501143033556 ;
+sumcube_ = -0.0289169130068278983 ;
+sumfourth_ = 0.0030033253501558126 ;
+min_ = 0.699999999999999845 ;
+max_ = 0.810126582278481 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 290 ;
-sumsquare_ = 580 ;
-sumcube_ = 1160 ;
-sumfourth_ = 2320 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 290 ;
-sumsquare_ = 580 ;
-sumcube_ = 1160 ;
-sumfourth_ = 2320 ;
+sum_ = -86 ;
+sumsquare_ = 172 ;
+sumcube_ = -344 ;
+sumfourth_ = 688 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 43 ;
+sumsquare_ = 43 ;
+sumcube_ = 43 ;
+sumfourth_ = 43 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 141 ;
+sumsquare_ = 181 ;
+sumcube_ = 261 ;
+sumfourth_ = 421 ;
+min_ = 0 ;
+max_ = 2 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 190 ;
+first_ = 0 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,16 +140,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 51 ;
-sumsquare_ = 51 ;
-sumcube_ = 51 ;
-sumfourth_ = 51 ;
+sum_ = -20 ;
+sumsquare_ = 20 ;
+sumcube_ = -20 ;
+sumfourth_ = 20 ;
 min_ = 1 ;
 max_ = 2 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = 1 ;
-last_ = 1 ;
+agmemin_ = 190 ;
+agemax_ = 199 ;
+first_ = 2 ;
+last_ = 2 ;
 binary_ = 0 ;
 integer_ = 1 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials2/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test1_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -146 ;
-sumsquare_ = 146 ;
-sumcube_ = -146 ;
-sumfourth_ = 146 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 15.3877551020408116 ;
-sumsquare_ = 4.4742631746576258 ;
-sumcube_ = 1.30326420202302096 ;
-sumfourth_ = 0.380403205233986463 ;
-min_ = 0.653061224489795866 ;
-max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 140 ;
-first_ = 0.653061224489795866 ;
-last_ = 0.653061224489795866 ;
+sum_ = -1.02531645569621088 ;
+sumsquare_ = 0.278532081010422594 ;
+sumcube_ = 0.021133659590955596 ;
+sumfourth_ = 0.00281881493806235587 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 197 ;
+agemax_ = 190 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 292 ;
-sumsquare_ = 584 ;
-sumcube_ = 1168 ;
-sumfourth_ = 2336 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 292 ;
-sumsquare_ = 584 ;
-sumcube_ = 1168 ;
-sumfourth_ = 2336 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 140 ;
+first_ = 0 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,10 +140,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -2 ;
-sumsquare_ = 2 ;
-sumcube_ = -2 ;
-sumfourth_ = 2 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
 min_ = 1 ;
 max_ = 2 ;
 agmemin_ = 140 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/test2_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1675 ;
-sumsquare_ = 1675 ;
-sumcube_ = 1675 ;
-sumfourth_ = 1675 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
-agemax_ = 6775 ;
+agemax_ = 4899 ;
 first_ = 0 ;
-last_ = 1 ;
+last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -1019.5798319327115 ;
-sumsquare_ = 309.470779908601799 ;
-sumcube_ = -88.2388801657493929 ;
-sumfourth_ = 25.4773413654550858 ;
-min_ = 0.653061224489795866 ;
-max_ = 1 ;
-agmemin_ = 3863 ;
-agemax_ = 6775 ;
-first_ = 0.941176470588235392 ;
-last_ = 1 ;
+sum_ = 21.2228528458643559 ;
+sumsquare_ = 10.0782152306775235 ;
+sumcube_ = 0.91717454487854666 ;
+sumfourth_ = 0.114852084949136693 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 3311 ;
+agemax_ = 3266 ;
+first_ = 0.810126582278481 ;
+last_ = 0.833333333333333259 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,16 +56,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -3350 ;
-sumsquare_ = 6700 ;
-sumcube_ = -13400 ;
-sumfourth_ = 26800 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 6775 ;
+agmemin_ = 4899 ;
 agemax_ = 6830 ;
 first_ = 1 ;
-last_ = -1 ;
+last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
@@ -77,16 +77,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -3350 ;
-sumsquare_ = 6700 ;
-sumcube_ = -13400 ;
-sumfourth_ = 26800 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 6775 ;
+agmemin_ = 4899 ;
 agemax_ = 6830 ;
 first_ = 1 ;
-last_ = -1 ;
+last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
@@ -98,17 +98,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 2445 ;
-sumsquare_ = 3841 ;
-sumcube_ = 6633 ;
-sumfourth_ = 12217 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
 min_ = 0 ;
-max_ = 2 ;
+max_ = 1 ;
 agmemin_ = 6830 ;
-agemax_ = 6775 ;
+agemax_ = 4899 ;
 first_ = 0 ;
-last_ = 2 ;
-binary_ = 0 ;
+last_ = 0 ;
+binary_ = 1 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 5040 ;
+sumsquare_ = 9648 ;
+sumcube_ = 22320 ;
+sumfourth_ = 58032 ;
+min_ = 0 ;
+max_ = 3 ;
 agmemin_ = 6830 ;
-agemax_ = 6830 ;
-first_ = 1 ;
-last_ = 1 ;
-binary_ = 1 ;
+agemax_ = 3254 ;
+first_ = 0 ;
+last_ = 3 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,13 +140,13 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -770 ;
-sumsquare_ = 770 ;
-sumcube_ = -770 ;
-sumfourth_ = 770 ;
+sum_ = -576 ;
+sumsquare_ = 576 ;
+sumcube_ = -576 ;
+sumfourth_ = 576 ;
 min_ = 1 ;
 max_ = 2 ;
-agmemin_ = 6775 ;
+agmemin_ = 3254 ;
 agemax_ = 6830 ;
 first_ = 2 ;
 last_ = 1 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/Split0/train_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -146 ;
-sumsquare_ = 146 ;
-sumcube_ = -146 ;
-sumfourth_ = 146 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 15.3877551020408116 ;
-sumsquare_ = 4.4742631746576258 ;
-sumcube_ = 1.30326420202302096 ;
-sumfourth_ = 0.380403205233986463 ;
-min_ = 0.653061224489795866 ;
-max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 140 ;
-first_ = 0.653061224489795866 ;
-last_ = 0.653061224489795866 ;
+sum_ = -1.02531645569621088 ;
+sumsquare_ = 0.278532081010422594 ;
+sumcube_ = 0.021133659590955596 ;
+sumfourth_ = 0.00281881493806235587 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 197 ;
+agemax_ = 190 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 292 ;
-sumsquare_ = 584 ;
-sumcube_ = 1168 ;
-sumfourth_ = 2336 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 292 ;
-sumsquare_ = 584 ;
-sumcube_ = 1168 ;
-sumfourth_ = 2336 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 140 ;
+first_ = 0 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,10 +140,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -2 ;
-sumsquare_ = 2 ;
-sumcube_ = -2 ;
-sumfourth_ = 2 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
 min_ = 1 ;
 max_ = 2 ;
 agmemin_ = 140 ;

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0/Trials3/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test1_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,15 +14,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 14.2702702702702666 ;
-sumsquare_ = 3.91616564589538418 ;
-sumcube_ = 1.07470658057835911 ;
-sumfourth_ = 0.294929872424830264 ;
-min_ = 0.648648648648648574 ;
-max_ = 0.923076923076923128 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = 0.648648648648648574 ;
-last_ = 0.648648648648648574 ;
+sum_ = -1.02531645569621088 ;
+sumsquare_ = 0.278532081010422594 ;
+sumcube_ = 0.021133659590955596 ;
+sumfourth_ = 0.00281881493806235587 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 197 ;
+agemax_ = 190 ;
+first_ = 0.810126582278481 ;
+last_ = 0.782178217821782096 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,15 +56,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 288 ;
-sumsquare_ = 576 ;
-sumcube_ = 1152 ;
-sumfourth_ = 2304 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -77,15 +77,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 288 ;
-sumsquare_ = 576 ;
-sumcube_ = 1152 ;
-sumfourth_ = 2304 ;
+sum_ = -78 ;
+sumsquare_ = 156 ;
+sumcube_ = -312 ;
+sumfourth_ = 624 ;
 min_ = -1 ;
 max_ = 1 ;
-agmemin_ = 199 ;
-agemax_ = 198 ;
-first_ = -1 ;
+agmemin_ = 197 ;
+agemax_ = 199 ;
+first_ = 1 ;
 last_ = 1 ;
 binary_ = 0 ;
 integer_ = 1 ;
@@ -98,15 +98,15 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = -144 ;
-sumsquare_ = 144 ;
-sumcube_ = -144 ;
-sumfourth_ = 144 ;
+sum_ = 39 ;
+sumsquare_ = 39 ;
+sumcube_ = 39 ;
+sumfourth_ = 39 ;
 min_ = 0 ;
 max_ = 1 ;
-agmemin_ = 198 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agmemin_ = 199 ;
+agemax_ = 197 ;
+first_ = 0 ;
 last_ = 0 ;
 binary_ = 1 ;
 integer_ = 1 ;
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 147 ;
+sumsquare_ = 211 ;
+sumcube_ = 375 ;
+sumfourth_ = 811 ;
+min_ = 0 ;
+max_ = 3 ;
 agmemin_ = 199 ;
-agemax_ = 199 ;
-first_ = 1 ;
+agemax_ = 140 ;
+first_ = 0 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,17 +140,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 200 ;
 sumsquarew_ = 200 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
+sum_ = -6 ;
+sumsquare_ = 6 ;
+sumcube_ = -6 ;
+sumfourth_ = 6 ;
 min_ = 1 ;
-max_ = 1 ;
-agmemin_ = 199 ;
+max_ = 2 ;
+agmemin_ = 140 ;
 agemax_ = 199 ;
-first_ = 1 ;
-last_ = 1 ;
-binary_ = 1 ;
+first_ = 2 ;
+last_ = 2 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/test2_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -14,10 +14,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1049 ;
-sumsquare_ = 1049 ;
-sumcube_ = 1049 ;
-sumfourth_ = 1049 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
@@ -35,16 +35,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -1060.3908523908317 ;
-sumsquare_ = 291.001231841168249 ;
-sumcube_ = -79.8589659106670098 ;
-sumfourth_ = 21.9155582124898878 ;
-min_ = 0.648648648648648574 ;
-max_ = 0.923076923076923128 ;
-agmemin_ = 3863 ;
-agemax_ = 6830 ;
-first_ = 0.923076923076923128 ;
-last_ = 0.648648648648648574 ;
+sum_ = 21.2228528458643559 ;
+sumsquare_ = 10.0782152306775235 ;
+sumcube_ = 0.91717454487854666 ;
+sumfourth_ = 0.114852084949136693 ;
+min_ = 0.782178217821782096 ;
+max_ = 0.928571428571428492 ;
+agmemin_ = 3311 ;
+agemax_ = 3266 ;
+first_ = 0.810126582278481 ;
+last_ = 0.833333333333333259 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -56,10 +56,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -2098 ;
-sumsquare_ = 4196 ;
-sumcube_ = -8392 ;
-sumfourth_ = 16784 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
 min_ = -1 ;
 max_ = 1 ;
 agmemin_ = 4899 ;
@@ -77,10 +77,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = -2098 ;
-sumsquare_ = 4196 ;
-sumcube_ = -8392 ;
-sumfourth_ = 16784 ;
+sum_ = -1346 ;
+sumsquare_ = 2692 ;
+sumcube_ = -5384 ;
+sumfourth_ = 10768 ;
 min_ = -1 ;
 max_ = 1 ;
 agmemin_ = 4899 ;
@@ -98,10 +98,10 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 1049 ;
-sumsquare_ = 1049 ;
-sumcube_ = 1049 ;
-sumfourth_ = 1049 ;
+sum_ = 673 ;
+sumsquare_ = 673 ;
+sumcube_ = 673 ;
+sumfourth_ = 673 ;
 min_ = 0 ;
 max_ = 1 ;
 agmemin_ = 6830 ;
@@ -119,17 +119,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
-min_ = 1 ;
-max_ = 1 ;
+sum_ = 5040 ;
+sumsquare_ = 9648 ;
+sumcube_ = 22320 ;
+sumfourth_ = 58032 ;
+min_ = 0 ;
+max_ = 3 ;
 agmemin_ = 6830 ;
-agemax_ = 6830 ;
-first_ = 1 ;
-last_ = 1 ;
-binary_ = 1 ;
+agemax_ = 3254 ;
+first_ = 0 ;
+last_ = 3 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )
@@ -140,17 +140,17 @@
 nmissing_ = 0 ;
 nnonmissing_ = 6831 ;
 sumsquarew_ = 6831 ;
-sum_ = 0 ;
-sumsquare_ = 0 ;
-sumcube_ = 0 ;
-sumfourth_ = 0 ;
+sum_ = -576 ;
+sumsquare_ = 576 ;
+sumcube_ = -576 ;
+sumfourth_ = 576 ;
 min_ = 1 ;
-max_ = 1 ;
-agmemin_ = 6830 ;
+max_ = 2 ;
+agmemin_ = 3254 ;
 agemax_ = 6830 ;
-first_ = 1 ;
+first_ = 2 ;
 last_ = 1 ;
-binary_ = 1 ;
+binary_ = 0 ;
 integer_ = 1 ;
 counts = {};
 more_than_maxnvalues = 1  )

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/Split0/train_stats.psave	2008-07-03 18:43:03 UTC (rev 9209)
@@ -18,12 +18,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.280000000000000027 ;
-max_ = 0.280000000000000027 ;
+min_ = 0.195000000000000007 ;
+max_ = 0.195000000000000007 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.280000000000000027 ;
-last_ = 0.280000000000000027 ;
+first_ = 0.195000000000000007 ;
+last_ = 0.195000000000000007 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -39,12 +39,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.719999999999999862 ;
-max_ = 0.719999999999999862 ;
+min_ = 0.805000000000000049 ;
+max_ = 0.805000000000000049 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.719999999999999862 ;
-last_ = 0.719999999999999862 ;
+first_ = 0.805000000000000049 ;
+last_ = 0.805000000000000049 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -60,12 +60,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.440000000000000002 ;
-max_ = 0.440000000000000002 ;
+min_ = 0.609999999999999987 ;
+max_ = 0.609999999999999987 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.440000000000000002 ;
-last_ = 0.440000000000000002 ;
+first_ = 0.609999999999999987 ;
+last_ = 0.609999999999999987 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -81,12 +81,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.440000000000000002 ;
-max_ = 0.440000000000000002 ;
+min_ = 0.609999999999999987 ;
+max_ = 0.609999999999999987 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.440000000000000002 ;
-last_ = 0.440000000000000002 ;
+first_ = 0.609999999999999987 ;
+last_ = 0.609999999999999987 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -102,12 +102,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.153564631825501396 ;
-max_ = 0.153564631825501396 ;
+min_ = 0.0985214463475333063 ;
+max_ = 0.0985214463475333063 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.153564631825501396 ;
-last_ = 0.153564631825501396 ;
+first_ = 0.0985214463475333063 ;
+last_ = 0.0985214463475333063 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -123,12 +123,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.767844767844770959 ;
-max_ = 0.767844767844770959 ;
+min_ = 0.813233426495413303 ;
+max_ = 0.813233426495413303 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.767844767844770959 ;
-last_ = 0.767844767844770959 ;
+first_ = 0.813233426495413303 ;
+last_ = 0.813233426495413303 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -144,12 +144,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.692870736348997207 ;
-max_ = 0.692870736348997207 ;
+min_ = 0.802957107304933415 ;
+max_ = 0.802957107304933415 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.692870736348997207 ;
-last_ = 0.692870736348997207 ;
+first_ = 0.802957107304933415 ;
+last_ = 0.802957107304933415 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};
@@ -165,12 +165,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.692870736348997207 ;
-max_ = 0.692870736348997207 ;
+min_ = 0.802957107304933415 ;
+max_ = 0.802957107304933415 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.692870736348997207 ;
-last_ = 0.692870736348997207 ;
+first_ = 0.802957107304933415 ;
+last_ = 0.802957107304933415 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-07-03 18:24:59 UTC (rev 9208)
+++ trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/metainfos.txt	2008-07-03 18:43:03 UTC (rev 9209)
@@ -1,3 +1,3 @@
-__REVISION__ = "PL8883"
+__REVISION__ = "PL9198"
 datatest                                      = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_test.amat
 datatrain                                     = PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat

Modified: trunk/plearn_learners/regressors/test/RegressionTree/.pytest/PL_RegressionTree_MultiClass/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)



From nouiz at mail.berlios.de  Thu Jul  3 22:34:13 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 3 Jul 2008 22:34:13 +0200
Subject: [Plearn-commits] r9210 - in trunk: plearn/python
	plearn_learners/distributions
Message-ID: <200807032034.m63KYDEk001832@sheep.berlios.de>

Author: nouiz
Date: 2008-07-03 22:34:12 +0200 (Thu, 03 Jul 2008)
New Revision: 9210

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn_learners/distributions/GaussMix.cc
Log:
remove more warning


Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-07-03 18:43:03 UTC (rev 9209)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-07-03 20:34:12 UTC (rev 9210)
@@ -406,7 +406,7 @@
       m_object(Py_None)
 {
     if (m_ownership == control_ownership)
-        Py_XINCREF(m_object);
+    {Py_XINCREF(m_object);}
 }
 
 //! Constructor for pre-existing PyObject
@@ -417,7 +417,7 @@
       m_object(pyobj)
 {
     if (m_ownership == control_ownership)
-        Py_XINCREF(m_object);
+    {Py_XINCREF(m_object);}
 }
 
 
@@ -428,7 +428,7 @@
       m_object(other.m_object)
 {
     if (m_ownership == control_ownership)
-        Py_XINCREF(m_object);
+    {Py_XINCREF(m_object);}
 }
 
 // Destructor decrements refcount if controlling ownership.

Modified: trunk/plearn_learners/distributions/GaussMix.cc
===================================================================
--- trunk/plearn_learners/distributions/GaussMix.cc	2008-07-03 18:43:03 UTC (rev 9209)
+++ trunk/plearn_learners/distributions/GaussMix.cc	2008-07-03 20:34:12 UTC (rev 9210)
@@ -2697,7 +2697,7 @@
     nsamples = train_set->length();
     D = train_set->inputsize();
 
-    if (f_eigen > 0)
+    if (f_eigen > 0){
         if (is_equal(f_eigen, 1))
             n_eigen = -1;
         else {
@@ -2706,6 +2706,7 @@
                 // We always want to keep at least one eigenvector.
                 n_eigen = 1;
         }
+    }
 
     alpha.resize(L);
     clust_imputed_missing.resize(0);



From nouiz at mail.berlios.de  Mon Jul  7 15:45:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 7 Jul 2008 15:45:08 +0200
Subject: [Plearn-commits] r9211 - trunk/commands
Message-ID: <200807071345.m67Dj88p014840@sheep.berlios.de>

Author: nouiz
Date: 2008-07-07 15:45:08 +0200 (Mon, 07 Jul 2008)
New Revision: 9211

Modified:
   trunk/commands/
Log:
added ignore files



Property changes on: trunk/commands
___________________________________________________________________
Name: svn:ignore
   - OBJS
plearn
plearn_curses
plearn_full
plearn_lapack
plearn_light
plearn_noblas
plearn_python
plearn_tests
plearn.lnk
plearn_tests.lnk
plearn_exp-*
plearn_desjardins-*

   + OBJS
plearn
plearn_curses
plearn_full
plearn_full-*
plearn_lapack
plearn_light
plearn_noblas
plearn_python
plearn_tests
plearn_tests-*
plearn_tests.lnk
plearn.lnk
plearn_exp*
plearn_desjardins*




From chapados at mail.berlios.de  Mon Jul  7 23:09:33 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 7 Jul 2008 23:09:33 +0200
Subject: [Plearn-commits] r9212 - in trunk: plearn/python
	plearn_learners/hyper
Message-ID: <200807072109.m67L9Xq8029933@sheep.berlios.de>

Author: chapados
Date: 2008-07-07 23:09:33 +0200 (Mon, 07 Jul 2008)
New Revision: 9212

Modified:
   trunk/plearn/python/PythonCodeSnippet.cc
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Fixed a few format strings in PLERROR

Modified: trunk/plearn/python/PythonCodeSnippet.cc
===================================================================
--- trunk/plearn/python/PythonCodeSnippet.cc	2008-07-07 13:45:08 UTC (rev 9211)
+++ trunk/plearn/python/PythonCodeSnippet.cc	2008-07-07 21:09:33 UTC (rev 9212)
@@ -136,7 +136,7 @@
       m_python_methods(4)
 {
     PyObject* compiled_code= 
-        PyObject_GetAttrString(m_instance.getPyObject(), "__dict__");
+        PyObject_GetAttrString(m_instance.getPyObject(), const_cast<char*>("__dict__"));
     m_compiled_code= PythonObjectWrapper(compiled_code, PythonObjectWrapper::transfer_ownership);
     // NOTE: build() not called
 }
@@ -495,7 +495,7 @@
     py_method->ml_name  = const_cast<char*>(python_name);
     py_method->ml_meth  = pythonTrampoline;
     py_method->ml_flags = METH_VARARGS;
-    py_method->ml_doc   = "injected-function-from-PythonCodeSnippet";
+    py_method->ml_doc   = const_cast<char*>("injected-function-from-PythonCodeSnippet");
 
     PyObject* py_funcobj = PyCFunction_NewEx(py_method,
                                              self /* info for trampoline */,
@@ -767,7 +767,8 @@
             if(!formatFunc)
                 throw PythonException("PythonCodeSnippet::handlePythonErrors :"
                                       " Can't find cgitb.text");
-            PyObject* args= Py_BuildValue("((OOO))", exception, v, traceback);
+            PyObject* args= Py_BuildValue(const_cast<char*>("((OOO))"),
+                                          exception, v, traceback);
             if(!args)
                 throw PythonException("PythonCodeSnippet::handlePythonErrors :"
                                       " Can't build args for cgitb.text");

Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-07-07 13:45:08 UTC (rev 9211)
+++ trunk/plearn/python/PythonExtension.cc	2008-07-07 21:09:33 UTC (rev 9212)
@@ -138,7 +138,7 @@
     // import python class for wrapping PLearn objects
     string importcode= "\nfrom plearn.pybridge.wrapped_plearn_object "
         "import *\n";
-    PyObject_SetAttrString(module, "__builtins__", PyEval_GetBuiltins());
+    PyObject_SetAttrString(module, const_cast<char*>("__builtins__"), PyEval_GetBuiltins());
     PyObject* res= PyRun_String(importcode.c_str(), Py_file_input, 
                                 PyModule_GetDict(module), PyModule_GetDict(module));
     if(!res)
@@ -165,11 +165,11 @@
 
     //inject unref and newCPPObj methods
     PyMethodDef* py_method= &PythonObjectWrapper::m_unref_method_def;
-    py_method->ml_name  = "_unref";
+    py_method->ml_name  = const_cast<char*>("_unref");
     py_method->ml_meth  = PythonObjectWrapper::python_del;
     py_method->ml_flags = METH_VARARGS;
-    py_method->ml_doc   = "Injected unref function from PythonObjectWrapper; "
-        "DO NOT USE THIS FUNCTION! IT MAY DEALLOCATE THE PLEARN OBJECT!";
+    py_method->ml_doc   = const_cast<char*>("Injected unref function from PythonObjectWrapper; "
+                                            "DO NOT USE THIS FUNCTION! IT MAY DEALLOCATE THE PLEARN OBJECT!");
     PyObject* py_funcobj= PyCFunction_NewEx(py_method, NULL, NULL);
     PyObject* py_methobj= PyMethod_New(py_funcobj, NULL, wrapper);
     Py_XDECREF(py_funcobj);
@@ -187,17 +187,17 @@
     // inject 'newCPPObj' and 'refCPPObj' class methods
     TVec<PyMethodDef*> classmethods(2);
     classmethods[0]= &PythonObjectWrapper::m_newCPPObj_method_def;
-    classmethods[0]->ml_name  = "_newCPPObj";
+    classmethods[0]->ml_name  = const_cast<char*>("_newCPPObj");
     classmethods[0]->ml_meth  = PythonObjectWrapper::newCPPObj;
     classmethods[0]->ml_flags = METH_VARARGS;
-    classmethods[0]->ml_doc   = "Injected new function from PythonObjectWrapper; "
-        "DO NOT USE THIS FUNCTION!";
+    classmethods[0]->ml_doc   = const_cast<char*>("Injected new function from PythonObjectWrapper; "
+                                                  "DO NOT USE THIS FUNCTION!");
     classmethods[1]= &PythonObjectWrapper::m_refCPPObj_method_def;
-    classmethods[1]->ml_name  = "_refCPPObj";
+    classmethods[1]->ml_name  = const_cast<char*>("_refCPPObj");
     classmethods[1]->ml_meth  = PythonObjectWrapper::refCPPObj;
     classmethods[1]->ml_flags = METH_VARARGS;
-    classmethods[1]->ml_doc   = "Injected new function from PythonObjectWrapper; "
-        "DO NOT USE THIS FUNCTION!";
+    classmethods[1]->ml_doc   = const_cast<char*>("Injected new function from PythonObjectWrapper; "
+                                                  "DO NOT USE THIS FUNCTION!");
 
     for(TVec<PyMethodDef*>::iterator mit= classmethods.begin();
         mit != classmethods.end(); ++mit)
@@ -297,7 +297,7 @@
         for(unsigned int i= 0; i < nopts; ++i)
             optionnames.insert(options[i]->optionname());
         the_pyclass= clit->second;
-        if(-1==PyObject_SetAttrString(the_pyclass, "_optionnames", 
+        if(-1==PyObject_SetAttrString(the_pyclass, const_cast<char*>("_optionnames"), 
                                       PythonObjectWrapper(optionnames).getPyObject()))
         {
             Py_DECREF(module);
@@ -406,7 +406,7 @@
 void addToWrappedObjectsSet(PyObject* o)
 {
     PLASSERT(the_PLearn_python_module);
-    if(-1 == PyObject_SetAttrString(the_PLearn_python_module, "_tmp_wrapped_instance", o))
+    if(-1 == PyObject_SetAttrString(the_PLearn_python_module, const_cast<char*>("_tmp_wrapped_instance"), o))
         PLERROR("in addToWrappedObjectsSet : cannot add wrapped object to module.");
     PyObject* res= PyRun_String("\nwrapped_PLearn_instances.add(_tmp_wrapped_instance)"
                                 "\ndel _tmp_wrapped_instance\n", 
@@ -423,7 +423,7 @@
 void removeFromWrappedObjectsSet(PyObject* o)
 {
     PLASSERT(the_PLearn_python_module);
-    if(-1 == PyObject_SetAttrString(the_PLearn_python_module, "_tmp_wrapped_instance", o))
+    if(-1 == PyObject_SetAttrString(the_PLearn_python_module, const_cast<char*>("_tmp_wrapped_instance"), o))
         PLERROR("in removeFromWrappedObjectsSet : cannot add wrapped object to module.");
     PyObject* res= PyRun_String("\nwrapped_PLearn_instances.remove(_tmp_wrapped_instance)"
                                 "\ndel _tmp_wrapped_instance\n", 

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-07-07 13:45:08 UTC (rev 9211)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-07-07 21:09:33 UTC (rev 9212)
@@ -380,10 +380,17 @@
     {
         auto_save_timer->startTimer("auto_save");
 
-        if(verbosity>0)
+        if(verbosity>0) {
+            // Print current option-value pairs in slightly comprehensible form
+            string kv;
+            for (int i=0, n=option_names.size() ; i<n ; ++i) {
+                kv += option_names[i] + '=' + option_vals[i];
+                if (i < n-1)
+                    kv += ", ";
+            }
             perr << "In HyperOptimize::optimize() - We optimize with "
-                "parameters " << option_names << " with value " << option_vals
-                << endl;
+                "parameters " << kv;
+        }
 
         // This will also call build and forget on the learner unless unnecessary
         // because the modified options don't require it.



From nouiz at mail.berlios.de  Tue Jul  8 16:16:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 8 Jul 2008 16:16:49 +0200
Subject: [Plearn-commits] r9213 - trunk/scripts
Message-ID: <200807081416.m68EGnUU010387@sheep.berlios.de>

Author: nouiz
Date: 2008-07-08 16:16:48 +0200 (Tue, 08 Jul 2008)
New Revision: 9213

Modified:
   trunk/scripts/dbidispatch
Log:
added an env variable DBIDISPATCH_LOGDIR that is optinal that take the name of the global log directory


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-07-07 21:09:33 UTC (rev 9212)
+++ trunk/scripts/dbidispatch	2008-07-08 14:16:48 UTC (rev 9213)
@@ -123,6 +123,7 @@
 The '--[*no_]clean_up' set the DBI option clean_up to true(false)
 
 The environnement variable DBIDISPATCH_DEFAULT_OPTION can contain default option that you always want to pass to dbidispatch. You can override them on the command line.
+The environnement variable DBIDISPATCH_LOGDIR set the name of the directory where all the individual logs directory will be put. If not present default to LOGS.
 """%{'ShortHelp':ShortHelp,'ScriptName':ScriptName}
 
 if len(sys.argv) == 1:
@@ -141,6 +142,10 @@
     launch_cmd = 'Cluster'
 else:
     launch_cmd = 'Local'
+LOGDIR=os.getenv("DBIDISPATCH_LOGDIR")
+if not LOGDIR:
+    LOGDIR="LOGS"
+
 to_parse=[]
 env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")
 if env:
@@ -321,10 +326,10 @@
     tmp=re.sub( ',', '-', tmp )
     tmp=tmp[:200]
     tmp+='_'+str(datetime.datetime.now()).replace(' ','_')
-    dbi_param["log_dir"]=os.path.join("LOGS",tmp)
+    dbi_param["log_dir"]=os.path.join(LOGDIR,tmp)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 else:
-    dbi_param["log_dir"]=os.path.join("LOGS",FILE)
+    dbi_param["log_dir"]=os.path.join(LOGDIR,FILE)
     dbi_param["log_file"]=os.path.join(dbi_param["log_dir"],'log')
 
 



From chapados at mail.berlios.de  Tue Jul  8 19:57:20 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Tue, 8 Jul 2008 19:57:20 +0200
Subject: [Plearn-commits] r9214 - trunk/plearn_learners/hyper
Message-ID: <200807081757.m68HvKFM025370@sheep.berlios.de>

Author: chapados
Date: 2008-07-08 19:57:20 +0200 (Tue, 08 Jul 2008)
New Revision: 9214

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
Fixed output message

Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-07-08 14:16:48 UTC (rev 9213)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-07-08 17:57:20 UTC (rev 9214)
@@ -389,7 +389,7 @@
                     kv += ", ";
             }
             perr << "In HyperOptimize::optimize() - We optimize with "
-                "parameters " << kv;
+                "parameters " << kv << "\n";
         }
 
         // This will also call build and forget on the learner unless unnecessary



From laulysta at mail.berlios.de  Tue Jul  8 23:36:52 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 8 Jul 2008 23:36:52 +0200
Subject: [Plearn-commits] r9215 - trunk/plearn_learners_experimental
Message-ID: <200807082136.m68LaqRl015200@sheep.berlios.de>

Author: laulysta
Date: 2008-07-08 23:36:52 +0200 (Tue, 08 Jul 2008)
New Revision: 9215

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
Log:


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-08 17:57:20 UTC (rev 9214)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-08 21:36:52 UTC (rev 9215)
@@ -73,7 +73,7 @@
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
-    encoding("raw_masked_supervised"),
+    encoding("note_duration"),
     input_window_size(1),
     tied_input_reconstruction_weights( true ),
     input_noise_prob( 0.15 ),
@@ -299,10 +299,11 @@
         // Parsing symbols in target
         int tar_layer = 0;
         int tar_layer_size = 0;
+        int lala = target_layers.length();
         target_symbol_sizes.resize(target_layers.length());
-        for( int tar_layer=0; tar_layer<target_layers.length(); 
-             tar_layer++ )
+        for( tar_layer=0; tar_layer<target_layers.length(); tar_layer++ )
             target_symbol_sizes[tar_layer].resize(0);
+
         target_layers_n_of_target_elements.resize( targetsize() );
         target_layers_n_of_target_elements.clear();
 



From Alex1024 at erobay.in  Wed Jul  9 12:57:48 2008
From: Alex1024 at erobay.in (Alex1024)
Date: Wed, 09 Jul 2008 14:57:48 +0400
Subject: [Plearn-commits] Einschreiben von Alex1024
Message-ID: <E1KGXMq-0008YR-KJ@host05.best-hosting.ru>

Hallo!

Ein(e) Bekannte(r) von Dir (Alex1024) 
hat Dich eingeladen, seine/ihre neue Auktion zu besuchen.
Was genau versteigert wird, erfaehrst Du auf der Auktionsseite.

Folge bitte diesem Link:
http://www.erobay.in/?auctionator=Alex1024

Ausser der privaten Auktion von Alex1024 findest zu noch
Dates, Parties, Swinger, Toys und mehr ab 1 EURO!

Viele Gruesse und viel Spass bei Erobay!
Ihr erobay.in Team


PS: Diese Einladung wurde einmalig ueber das EINLADEN Formular von erobay.in
einmalig verschickt. erobay ist hierfuer nicht verantwortlich.

  



From chapados at mail.berlios.de  Wed Jul  9 14:22:52 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 9 Jul 2008 14:22:52 +0200
Subject: [Plearn-commits] r9216 - trunk/python_modules/plearn/gui_tools
Message-ID: <200807091222.m69CMqZx010212@sheep.berlios.de>

Author: chapados
Date: 2008-07-09 14:22:52 +0200 (Wed, 09 Jul 2008)
New Revision: 9216

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Fixed time format string

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-07-08 21:36:52 UTC (rev 9215)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-07-09 12:22:52 UTC (rev 9216)
@@ -371,7 +371,7 @@
     def expdir_name(expdir_root):
         """Return an experiment directory from a root location."""
         return os.path.join(expdir_root,
-                            datetime.now().strftime("expdir_%Y%m%d_%H%m%S"))
+                            datetime.now().strftime("expdir_%Y%m%d_%H%M%S"))
 
 
     @staticmethod



From nouiz at mail.berlios.de  Wed Jul  9 17:44:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Jul 2008 17:44:06 +0200
Subject: [Plearn-commits] r9217 - trunk/python_modules/plearn/pymake
Message-ID: <200807091544.m69Fi6bR005743@sheep.berlios.de>

Author: nouiz
Date: 2008-07-09 17:44:05 +0200 (Wed, 09 Jul 2008)
New Revision: 9217

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Added an env variable PYMAKE_OBJS_DIR that allow to change the LOGS dir to something else. Usefull at lisa as we set it to LOGS.NOBACKUP so that they are not put on the backup


Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2008-07-09 12:22:52 UTC (rev 9216)
+++ trunk/python_modules/plearn/pymake/pymake.py	2008-07-09 15:44:05 UTC (rev 9217)
@@ -187,6 +187,7 @@
 The environment variable PYMAKE_OPTIONS is prepended to the command line 
 options. You can define your default options there if they do not conflict
 with the ones from the command line.
+The environment variable PYMAKE_OBJS_DIR can be set to change the OBJS dir to the value of this variable. This is usefull if you don't want to backup the OBJS dir.
 """
 
 
@@ -251,19 +252,25 @@
         myhostname = myhostname[0:pos]
     return myhostname
 
+def get_OBJS_dir():
+    dir=os.getenv("PYMAKE_OBJS_DIR")
+    if not dir:
+        dir="OBJS"
+    return dir
+
 def join(*path_list):
     """simply call os.path.join and convertWinToLinux"""
     return convertWinToLinux(os.path.join(*path_list))
 
 def lsdirs(basedir):
     """returns the recursive list of all subdirectories of the given directory,
-    excluding OBJS and CVS directories and those whose name starts with a dot.
+    excluding OBJS, os.getenv("PYMAKE_OBJS_DIR") and CVS directories and those whose name starts with a dot.
     The first element of the returned list is the basedir"""
     if not os.path.isdir(basedir):
         return []
     dirs = [ basedir ]
     for dname in os.listdir(basedir):
-        if dname!='CVS' and dname!='OBJS' and dname[0]!='.':
+        if dname!='CVS' and dname!='OBJS' and dname!=get_OBJS_dir() and dname[0]!='.':
             dname = join(basedir,dname)
             if os.path.isdir(dname):
                 dirs.append(dname)
@@ -917,7 +924,7 @@
     if os.path.basename(target)[0] == '.': # ignore files and directories starting with a dot
         return
     if os.path.isdir(target):
-        if os.path.basename(target) not in ['OBJS','CVS']: # skip OBJS and CVS directories
+        if os.path.basename(target) not in ['OBJS','CVS',get_OBJS_dir()]: # skip OBJS and CVS directories
             print "Entering " + target
             for direntry in os.listdir(target):
                 newtarget = join(target,direntry)
@@ -1144,7 +1151,7 @@
         print "Directory " + dist_dir + " already exists"
         sys.exit(100)
     os.makedirs(dist_dir)
-    os.makedirs(os.path.join(dist_dir,"OBJS"))
+    os.makedirs(os.path.join(dist_dir,get_OBJS_dir()))
     makefile = open(os.path.join(dist_dir,"Makefile"),"w")
     compiler = default_compiler
     compileroptions = ""
@@ -1181,7 +1188,8 @@
     for ccfile in executables_to_link:
         objsfilelist = []
         for lf in ccfile.get_ccfiles_to_link():
-            objsfilelist.append("OBJS/" + os.path.basename(lf.corresponding_ofile))
+            objsfilelist.append(os.path.join(get_OBJS_dir(),
+                                             os.path.basename(lf.corresponding_ofile)))
 
         command = linker + so_options + ' -o ' + os.path.basename(ccfile.corresponding_output) +\
                   ' ' + string.join(objsfilelist,' ') + ' ' + \
@@ -1194,10 +1202,11 @@
 
     for ccfile in ccfiles_to_compile.keys():
         [dir_cc, file_cc] = ccfile.get_rel_dir_file()
-
-        makefile.write('OBJS/' + ccfile.filebase + ".o : " + os.path.join(dir_cc,file_cc) + "\n")
+        out_file_o=os.path.join(get_OBJS_dir(),ccfile.filebase + ".o")
+        makefile.write(out_file_o+" : " + os.path.join(dir_cc,file_cc) + "\n")
         makefile.write('\t' + compiler + ' ' + compileflags + ' ' + \
-                       compileroptions + ' -o OBJS/' + ccfile.filebase + ".o -c " + os.path.join(dir_cc,file_cc))
+                           compileroptions + ' -o ' + out_file_o + " -c " + \
+                           os.path.join(dir_cc,file_cc))
         makefile.write("\n")
         makefile.write("\n")
 
@@ -1213,6 +1222,7 @@
     target = args[0]
     configpath = get_config_path(target)
     execfile( configpath, globals() )
+    global options
     options = getOptions(options_choices, optionargs)
 
     sourcedirs = unique(sourcedirs)
@@ -1524,19 +1534,24 @@
 def reportMissingObj(arg, dirpath, names):
     if 'OBJS' in names: names.remove('OBJS')
     if 'CVS' in names: names.remove('CVS')
+    objs_dir_to_use=get_OBJS_dir()
+    if objs_dir_to_use in names: names.remove(objs_dir_to_use)
     for fname in names:
         fpath = join(dirpath,fname)
         if os.path.isfile(fpath):
             basename, ext = os.path.splitext(fname)
             if ext in cpp_exts:
                 foundobj = 0 # found the .o file?
-                for f in glob.glob(join(dirpath,'OBJS','*',basename+'.o')):
+                for f in glob.glob(join(dirpath,objs_dir_to_use,
+                                        '*',basename+'.o')):
                     if os.path.isfile(f): foundobj = 1
                 if not foundobj:
                     print fpath
 
 def rmOBJS(arg, dirpath, names):
-    if os.path.basename(dirpath) == 'OBJS':
+    objs_dir_to_use=get_OBJS_dir()
+    if os.path.basename(dirpath) == 'OBJS' or\
+            os.path.basename(dirpath)==objs_dir_to_use:
         print 'Removing', dirpath
         shutil.rmtree(dirpath)
 
@@ -2768,13 +2783,18 @@
         temp_objs=1
         local_compilation = 1
         if (objsdir == ''):
-            objsdir = '/tmp/OBJS'
+            objsdir = '/tmp/'+get_OBJS_dir()
         optionargs.remove('tmp')
     else:
         temp_objs = 0
 
     ##  Special options that will not compile, but perform various operations
     if 'clean' in optionargs:
+        #remove some option that are save to have.
+        #some people can put those option in their PYMAKE_OPTIONS and 
+        #we want them to be able to do pymake -clean .
+        if 'local_ofiles' in optionargs: optionargs.remove('local_ofiles')
+        if 'logging=dbg' in optionargs: optionargs.remove('logging=dbg')
         if len(optionargs)!=1 or len(otherargs)==0:
             print 'BAD ARGUMENTS: with -clean, specify one or more directories to clean, but no other -option'
             sys.exit(100)
@@ -2881,7 +2901,7 @@
 
         # Building name of object subdirectory
         if  objspolicy== 1:
-            objsdir = join('OBJS', target_platform + '__')
+            objsdir = join(get_OBJS_dir(), target_platform + '__')
         elif objspolicy == 2:
             objsdir = join(objsdir, target_platform + '__')
         # We append options name to the objsdir name if they modify the compiled objects file



From laulysta at mail.berlios.de  Wed Jul  9 21:09:51 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 9 Jul 2008 21:09:51 +0200
Subject: [Plearn-commits] r9218 - trunk/commands/PLearnCommands
Message-ID: <200807091909.m69J9pLQ016058@sheep.berlios.de>

Author: laulysta
Date: 2008-07-09 21:09:51 +0200 (Wed, 09 Jul 2008)
New Revision: 9218

Added:
   trunk/commands/PLearnCommands/Stan.cc
   trunk/commands/PLearnCommands/Stan.h
Log:


Added: trunk/commands/PLearnCommands/Stan.cc
===================================================================
--- trunk/commands/PLearnCommands/Stan.cc	2008-07-09 15:44:05 UTC (rev 9217)
+++ trunk/commands/PLearnCommands/Stan.cc	2008-07-09 19:09:51 UTC (rev 9218)
@@ -0,0 +1,111 @@
+// -*- C++ -*-
+
+// Stan.cc
+//
+// Copyright (C) 2008 Stanislas Lauly
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Stanislas Lauly
+
+/*! \file Stan.cc */
+
+
+#include "Stan.h"
+#include <plearn_learners/hyper/HyperLearner.h>
+#include <plearn_learners_experimental/DynamicallyLinkedRBMsModel.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'Stan' command in the command registry
+PLearnCommandRegistry Stan::reg_(new Stan);
+
+Stan::Stan()
+    : PLearnCommand(
+        "stan",
+        "La commande pour les affaires de Stan",
+        "stan generate path_to_model.psave nb_notes \n"
+        "ex: stan generate /u/laulysta/recherche_maitrise/projet_GenerationDeMusique/exp/croche/00001__1a40000_80N/Split0/final_learner.psave 20 \n"
+        "\n"
+        )
+{}
+
+//! The actual implementation of the 'Stan' command
+void Stan::run(const vector<string>& args)
+{
+    string subcommand = args[0];
+    string modelpath = args[1];
+
+    DynamicallyLinkedRBMsModel* model=0;
+    Object* obj = loadObject(modelpath);
+    PP<Object> ppobj = obj;
+	
+    HyperLearner* hyper = dynamic_cast<HyperLearner*>(obj);
+    if(hyper!=0)
+    {
+        PP<PLearner> l = hyper->getLearner();
+        model = dynamic_cast<DynamicallyLinkedRBMsModel*>((PLearner*)l);
+    }
+    else
+    {
+        model = dynamic_cast<DynamicallyLinkedRBMsModel*>(obj);
+    }
+	  
+    if(model==0)
+        PLERROR("Le fichier doit contenir soit un HyperLearner contenant un DynamicallyLinkedRBMsModel, soit un DynamicallyLinkedRBMsModel directement");
+    
+    perr << "Successfully loaded " << modelpath << endl;
+    
+    if(subcommand=="generate")
+    {        
+        int nb_notes = toint(args[2]);
+        perr << "Generating " << nb_notes << " notes!" << endl;
+        model->generate(nb_notes);
+    }
+    else
+        perr << "No such subcommand: " << subcommand << endl;
+
+    // *** PLEASE COMPLETE HERE ****
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/commands/PLearnCommands/Stan.h
===================================================================
--- trunk/commands/PLearnCommands/Stan.h	2008-07-09 15:44:05 UTC (rev 9217)
+++ trunk/commands/PLearnCommands/Stan.h	2008-07-09 19:09:51 UTC (rev 9218)
@@ -0,0 +1,86 @@
+// -*- C++ -*-
+
+// Stan.h
+//
+// Copyright (C) 2008 Stanislas Lauly
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Stanislas Lauly
+
+/*! \file Stan.h */
+
+
+#ifndef Stan_INC
+#define Stan_INC
+
+#include <commands/PLearnCommands/PLearnCommand.h>
+#include <commands/PLearnCommands/PLearnCommandRegistry.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class Stan : public PLearnCommand
+{
+    typedef PLearnCommand inherited;
+
+public:
+    Stan();
+    virtual void run(const std::vector<std::string>& args);
+
+protected:
+    static PLearnCommandRegistry reg_;
+};
+
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From laulysta at mail.berlios.de  Wed Jul  9 21:20:44 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 9 Jul 2008 21:20:44 +0200
Subject: [Plearn-commits] r9219 - trunk/commands/PLearnCommands
Message-ID: <200807091920.m69JKiJo017041@sheep.berlios.de>

Author: laulysta
Date: 2008-07-09 21:20:41 +0200 (Wed, 09 Jul 2008)
New Revision: 9219

Modified:
   trunk/commands/PLearnCommands/Stan.cc
Log:
new version generation exprecive 


Modified: trunk/commands/PLearnCommands/Stan.cc
===================================================================
--- trunk/commands/PLearnCommands/Stan.cc	2008-07-09 19:09:51 UTC (rev 9218)
+++ trunk/commands/PLearnCommands/Stan.cc	2008-07-09 19:20:41 UTC (rev 9219)
@@ -1,111 +1,114 @@
-// -*- C++ -*-
-
-// Stan.cc
-//
-// Copyright (C) 2008 Stanislas Lauly
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Stanislas Lauly
-
-/*! \file Stan.cc */
-
-
-#include "Stan.h"
-#include <plearn_learners/hyper/HyperLearner.h>
-#include <plearn_learners_experimental/DynamicallyLinkedRBMsModel.h>
-
-namespace PLearn {
-using namespace std;
-
-//! This allows to register the 'Stan' command in the command registry
-PLearnCommandRegistry Stan::reg_(new Stan);
-
-Stan::Stan()
-    : PLearnCommand(
-        "stan",
-        "La commande pour les affaires de Stan",
-        "stan generate path_to_model.psave nb_notes \n"
-        "ex: stan generate /u/laulysta/recherche_maitrise/projet_GenerationDeMusique/exp/croche/00001__1a40000_80N/Split0/final_learner.psave 20 \n"
-        "\n"
-        )
-{}
-
-//! The actual implementation of the 'Stan' command
-void Stan::run(const vector<string>& args)
-{
-    string subcommand = args[0];
-    string modelpath = args[1];
-
-    DynamicallyLinkedRBMsModel* model=0;
-    Object* obj = loadObject(modelpath);
-    PP<Object> ppobj = obj;
-	
-    HyperLearner* hyper = dynamic_cast<HyperLearner*>(obj);
-    if(hyper!=0)
-    {
-        PP<PLearner> l = hyper->getLearner();
-        model = dynamic_cast<DynamicallyLinkedRBMsModel*>((PLearner*)l);
-    }
-    else
-    {
-        model = dynamic_cast<DynamicallyLinkedRBMsModel*>(obj);
-    }
-	  
-    if(model==0)
-        PLERROR("Le fichier doit contenir soit un HyperLearner contenant un DynamicallyLinkedRBMsModel, soit un DynamicallyLinkedRBMsModel directement");
-    
-    perr << "Successfully loaded " << modelpath << endl;
-    
-    if(subcommand=="generate")
-    {        
-        int nb_notes = toint(args[2]);
-        perr << "Generating " << nb_notes << " notes!" << endl;
-        model->generate(nb_notes);
-    }
-    else
-        perr << "No such subcommand: " << subcommand << endl;
-
-    // *** PLEASE COMPLETE HERE ****
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :
+// -*- C++ -*-
+
+// Stan.cc
+//
+// Copyright (C) 2008 Stanislas Lauly
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Stanislas Lauly
+
+/*! \file Stan.cc */
+
+
+#include "Stan.h"
+#include <plearn_learners/hyper/HyperLearner.h>
+#include <plearn_learners_experimental/DynamicallyLinkedRBMsModel.h>
+
+namespace PLearn {
+using namespace std;
+
+//! This allows to register the 'Stan' command in the command registry
+PLearnCommandRegistry Stan::reg_(new Stan);
+
+Stan::Stan()
+    : PLearnCommand(
+        "stan",
+        "La commande pour les affaires de Stan",
+        "stan generate path_to_model.psave nb_notes \n"
+        "ex: stan generate /home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/exp_tar_tm1__in_tm1_tp1/exp-expressive_timing-rnet_1hid-sizes=95-7-7-1-1-1-1-1-1-mds=20-stepsize=1-seed=654321-eoss=8-nhid=40-lrl=0.0001-utlm=1-20080529:185450/Split0/LearnerExpdir/final_learner.psave 1 \n"
+        "\n"
+        )
+{}
+
+//! The actual implementation of the 'Stan' command
+void Stan::run(const vector<string>& args)
+{
+    string subcommand = args[0];
+    string modelpath = args[1];
+
+    DynamicallyLinkedRBMsModel* model=0;
+    Object* obj = loadObject(modelpath);
+    PP<Object> ppobj = obj;
+	
+    HyperLearner* hyper = dynamic_cast<HyperLearner*>(obj);
+    if(hyper!=0)
+    {
+        PP<PLearner> l = hyper->getLearner();
+        model = dynamic_cast<DynamicallyLinkedRBMsModel*>((PLearner*)l);
+    }
+    else
+    {
+        model = dynamic_cast<DynamicallyLinkedRBMsModel*>(obj);
+    }
+	  
+    if(model==0)
+        PLERROR("Le fichier doit contenir soit un HyperLearner contenant un DynamicallyLinkedRBMsModel, soit un DynamicallyLinkedRBMsModel directement");
+    
+    perr << "Successfully loaded " << modelpath << endl;
+    
+    if(subcommand=="generate")
+    {        
+        int t = toint(args[2]);
+        int n = toint(args[3]);
+        //perr << "Generating " << nb_notes << " notes!" << endl;
+        perr << "Generating " << endl;
+        model->generate(t, n);
+        
+    }
+    else
+        perr << "No such subcommand: " << subcommand << endl;
+
+    // *** PLEASE COMPLETE HERE ****
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From laulysta at mail.berlios.de  Wed Jul  9 21:26:18 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 9 Jul 2008 21:26:18 +0200
Subject: [Plearn-commits] r9220 - trunk/plearn_learners_experimental
Message-ID: <200807091926.m69JQIri017737@sheep.berlios.de>

Author: laulysta
Date: 2008-07-09 21:26:16 +0200 (Wed, 09 Jul 2008)
New Revision: 9220

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
final version


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-07-09 19:20:41 UTC (rev 9219)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2008-07-09 19:26:16 UTC (rev 9220)
@@ -398,6 +398,7 @@
     
 
     deepCopyField( bias_gradient , copies);
+    deepCopyField( visi_bias_gradient , copies);
     deepCopyField( hidden_gradient , copies);
     deepCopyField( hidden_temporal_gradient , copies);
     deepCopyField( hidden_list , copies);
@@ -825,7 +826,7 @@
     hidden_layer->setLearningRate( the_learning_rate );
     input_connections->setLearningRate( the_learning_rate );
     if( dynamic_connections )
-        dynamic_connections->setLearningRate( the_learning_rate );
+        dynamic_connections->setLearningRate( the_learning_rate ); //HUGO: multiply by dynamic_connections_learning_weight;
     if( hidden_layer2 )
     {
         hidden_layer2->setLearningRate( the_learning_rate );
@@ -1259,137 +1260,474 @@
     return getTestCostNames();
 }
 
-//void DynamicallyLinkedRBMsModel::gen()
-//{
-//    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
-//    data = new AutoVMatrix();
-//    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
-//    data->defineSizes(21,0,0);
-//    //data->inputsize = 21;
-//    //data->targetsize = 0;
-//    //data->weightsize = 0;
-//    data->build();
-//
-//    
-//    int len = data->length();
-//    Vec score;
-//    Vec target;
-//    real weight;
-//    Vec bias_tempo;
-//    Vec visi_bias_tempo;
-//   
-//   
-//    
-//    previous_hidden_layer.resize(hidden_layer->size);
-//    connections_idem = connections;
-//
-//    for (int ith_sample = 0; ith_sample < len ; ith_sample++ ){
-//        
-//        data->getExample(ith_sample, score, target, weight);
-//        //score << data(ith_sample);
-//        input_prediction_list.resize(
-//            ith_sample+1,visible_layer->size);
-//        if(ith_sample > 0)
-//        {
-//            
-//            //input_list(ith_sample_in_sequence) << previous_input;
-//            //h*_{t-1}
-//            //////////////////////////////////
-//            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
-//            hidden_layer->setAllBias(cond_bias); //**************************
-//            
-//            
-//            
-//            //up phase
-//            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
-//            hidden_layer->getAllActivations( connections_idem );
-//            hidden_layer->computeExpectation();
-//            //////////////////////////////////
-//            
-//            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
-//            //previous_hidden_layer_act_no_bias << hidden_layer->activation;
-//            
-//            
-//            //h*_{t}
-//            ////////////
-//            if(dynamic_connections_copy)
-//                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-//            else
-//                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-//            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-//            hidden_layer->expectation_is_not_up_to_date();
-//            hidden_layer->computeExpectation();//h_{t}
-//            ///////////
-//            
-//            //previous_input << visible_layer->expectation;//v_{t-1}
-//            
-//        }
-//        else
-//        {
-//            
-//            previous_hidden_layer.clear();//h_{t-1}
-//            if(dynamic_connections_copy)
-//                dynamic_connections_copy->fprop( previous_hidden_layer ,
-//                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-//            else
-//                dynamic_connections->fprop(previous_hidden_layer,
-//                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
-//            
-//            hidden_layer->expectation_is_not_up_to_date();
-//            hidden_layer->computeExpectation();//h_{t}
-//            //previous_input.resize(data->inputsize);
-//            //previous_input << data(ith_sample);
-//            
-//        }
-//        
-//        //connections_transpose->setAsDownInput( hidden_layer->expectation );
-//        //visible_layer->getAllActivations( connections_idem_t );
-//        
-//        connections->setAsUpInput( hidden_layer->expectation );
-//        visible_layer->getAllActivations( connections_idem );
-//        
-//        visible_layer->computeExpectation();
-//        //visible_layer->generateSample();
-//        partition(score.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
-//        partition(score.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
-//
-//
-//        visible_layer->activation.subVec(0,14+taillePart) << score;
-//        visible_layer->expectation.subVec(0,14+taillePart) << score;
-//
-//        input_prediction_list(ith_sample) << visible_layer->expectation;
-//        
-//    }
-//    
-//    //Vec tempo;
-//    TVec<real> tempo;
-//    tempo.resize(visible_layer->size);
-//    ofstream myfile;
-//    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
-//    
-//    for (int i = 0; i < len ; i++ ){
-//        tempo << input_prediction_list(i);
-//        
-//        //cout << tempo[2] << endl;
-//       
-//        for (int j = 0; j < tempo.length() ; j++ ){
-//            
-//            
-//                
-//                
-//               myfile << tempo[j] << " ";
-//               
-//
-//               
-//           
-//        }
-//        myfile << "\n";
-//    }
-//     
-//
-//     myfile.close();
-//
-//}
+void DynamicallyLinkedRBMsModel::generate(int t, int n)
+{
+    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data = new AutoVMatrix();
+    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/listData/target_tm12_input_t_tm12_tp12/scoreGen_tar_tm12__in_tm12_tp12.amat";
+    //data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/create_data/scoreGenSuitePerf.amat";
+
+    data->defineSizes(208,16,0);
+    //data->inputsize = 21;
+    //data->targetsize = 0;
+    //data->weightsize = 0;
+    data->build();
+
+    
+    
+   
+   
+
+    int len = data->length();
+    int tarSize = outputsize();
+    int partTarSize;
+    Vec input;
+    Vec target;
+    real weight;
+
+    Vec output(outputsize());
+    output.clear();
+    /*Vec costs(nTestCosts());
+    costs.clear();
+    Vec n_items(nTestCosts());
+    n_items.clear();*/
+
+    int r,r2;
+    
+    int ith_sample_in_sequence = 0;
+    int inputsize_without_masks = inputsize() 
+        - ( use_target_layers_masks ? targetsize() : 0 );
+    int sum_target_elements = 0;
+    for (int i = 0; i < len; i++)
+    {
+        data->getExample(i, input, target, weight);
+        if(i>n)
+        {
+            for (int k = 1; k <= t; k++)
+            {
+                if(k<=i){
+                    partTarSize = outputsize();
+                    for( int tar=0; tar < target_layers.length(); tar++ )
+                    {
+                        
+                        input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+                        partTarSize -= target_layers[tar]->size;
+                        
+                        
+                    }
+                }
+            }       
+        }
+    
+/*
+        for (int k = 1; k <= t; k++)
+        {
+            partTarSize = outputsize();
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if(i>=t){
+                    input.subVec(inputsize_without_masks-(tarSize*(t-k))-partTarSize-1,target_layers[tar]->size) << target_prediction_list[tar][ith_sample_in_sequence-k];
+                    partTarSize -= target_layers[tar]->size;
+                }
+            }
+        }
+*/
+        if( fast_exact_is_equal(input[0],end_of_sequence_symbol) )
+        {
+            /*  ith_sample_in_sequence = 0;
+            hidden_list.resize(0);
+            hidden_act_no_bias_list.resize(0);
+            hidden2_list.resize(0);
+            hidden2_act_no_bias_list.resize(0);
+            target_prediction_list.resize(0);
+            target_prediction_act_no_bias_list.resize(0);
+            input_list.resize(0);
+            targets_list.resize(0);
+            nll_list.resize(0,0);
+            masks_list.resize(0);*/
+
+            
+
+            continue;
+        }
+
+        // Resize internal variables
+        hidden_list.resize(ith_sample_in_sequence+1);
+        hidden_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        if( hidden_layer2 )
+        {
+            hidden2_list.resize(ith_sample_in_sequence+1);
+            hidden2_act_no_bias_list.resize(ith_sample_in_sequence+1);
+        }
+                 
+        input_list.resize(ith_sample_in_sequence+1);
+        input_list[ith_sample_in_sequence].resize(input_layer->size);
+
+        targets_list.resize( target_layers.length() );
+        target_prediction_list.resize( target_layers.length() );
+        target_prediction_act_no_bias_list.resize( target_layers.length() );
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                targets_list[tar].resize( ith_sample_in_sequence+1);
+                targets_list[tar][ith_sample_in_sequence].resize( 
+                    target_layers[tar]->size);
+                target_prediction_list[tar].resize(
+                    ith_sample_in_sequence+1);
+                target_prediction_act_no_bias_list[tar].resize(
+                    ith_sample_in_sequence+1);
+            }
+        }
+        nll_list.resize(ith_sample_in_sequence+1,target_layers.length());
+        if( use_target_layers_masks )
+        {
+            masks_list.resize( target_layers.length() );
+            for( int tar=0; tar < target_layers.length(); tar++ )
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                    masks_list[tar].resize( ith_sample_in_sequence+1 );
+        }
+
+        // Forward propagation
+
+        // Fetch right representation for input
+        clamp_units(input.subVec(0,inputsize_without_masks),
+                    input_layer,
+                    input_symbol_sizes);                
+        input_list[ith_sample_in_sequence] << input_layer->expectation;
+
+        // Fetch right representation for target
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                if( use_target_layers_masks )
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar],
+                                input.subVec(
+                                    inputsize_without_masks 
+                                    + sum_target_elements, 
+                                    target_layers_n_of_target_elements[tar]),
+                                masks_list[tar][ith_sample_in_sequence]
+                        );
+                    
+                }
+                else
+                {
+                    clamp_units(target.subVec(
+                                    sum_target_elements,
+                                    target_layers_n_of_target_elements[tar]),
+                                target_layers[tar],
+                                target_symbol_sizes[tar]);
+                }
+                targets_list[tar][ith_sample_in_sequence] << 
+                    target_layers[tar]->expectation;
+            }
+            sum_target_elements += target_layers_n_of_target_elements[tar];
+        }
+                
+        input_connections->fprop( input_list[ith_sample_in_sequence], 
+                                  hidden_act_no_bias_list[ith_sample_in_sequence]);
+                
+        if( ith_sample_in_sequence > 0 && dynamic_connections )
+        {
+            dynamic_connections->fprop( 
+                hidden_list[ith_sample_in_sequence-1],
+                dynamic_act_no_bias_contribution );
+
+            hidden_act_no_bias_list[ith_sample_in_sequence] += 
+                dynamic_act_no_bias_contribution;
+        }
+                 
+        hidden_layer->fprop( hidden_act_no_bias_list[ith_sample_in_sequence], 
+                             hidden_list[ith_sample_in_sequence] );
+                 
+        if( hidden_layer2 )
+        {
+            hidden_connections->fprop( 
+                hidden_list[ith_sample_in_sequence],
+                hidden2_act_no_bias_list[ith_sample_in_sequence]);
+
+            hidden_layer2->fprop( 
+                hidden2_act_no_bias_list[ith_sample_in_sequence],
+                hidden2_list[ith_sample_in_sequence] 
+                );
+
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden2_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
+            }
+        }
+        else
+        {
+            for( int tar=0; tar < target_layers.length(); tar++ )
+            {
+                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+                {
+                    target_connections[tar]->fprop(
+                        hidden_list[ith_sample_in_sequence],
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence]
+                        );
+                    target_layers[tar]->fprop(
+                        target_prediction_act_no_bias_list[tar][
+                            ith_sample_in_sequence],
+                        target_prediction_list[tar][
+                            ith_sample_in_sequence] );
+                    if( use_target_layers_masks )
+                        target_prediction_list[tar][ ith_sample_in_sequence] *= 
+                            masks_list[tar][ith_sample_in_sequence];
+                }
+            }
+        }
+
+        
+
+        sum_target_elements = 0;
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
+            {
+                target_layers[tar]->activation << 
+                    target_prediction_act_no_bias_list[tar][
+                        ith_sample_in_sequence];
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(
+                    target_prediction_list[tar][
+                        ith_sample_in_sequence]);
+                nll_list(ith_sample_in_sequence,tar) = 
+                    target_layers[tar]->fpropNLL( 
+                        targets_list[tar][ith_sample_in_sequence] ); 
+                /*costs[tar] += nll_list(ith_sample_in_sequence,tar);
+                
+                // Normalize by the number of things to predict
+                if( use_target_layers_masks )
+                {
+                    n_items[tar] += sum(
+                        input.subVec( inputsize_without_masks 
+                                      + sum_target_elements, 
+                                      target_layers_n_of_target_elements[tar]) );
+                }
+                else
+                n_items[tar]++;*/
+            }
+            if( use_target_layers_masks )
+                sum_target_elements += 
+                    target_layers_n_of_target_elements[tar];
+        }
+        ith_sample_in_sequence++;
+
+        
+
+    }
+
+    /*  
+    ith_sample_in_sequence = 0;
+    hidden_list.resize(0);
+    hidden_act_no_bias_list.resize(0);
+    hidden2_list.resize(0);
+    hidden2_act_no_bias_list.resize(0);
+    target_prediction_list.resize(0);
+    target_prediction_act_no_bias_list.resize(0);
+    input_list.resize(0);
+    targets_list.resize(0);
+    nll_list.resize(0,0);
+    masks_list.resize(0);   
+
+
+    */
+
+
+
+
+
+
+
+
+
+    
+    //Vec tempo;
+    //TVec<real> tempo;
+    //tempo.resize(visible_layer->size);
+    ofstream myfile;
+    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
+    
+    for (int i = 0; i < target_prediction_list[0].length() ; i++ ){
+       
+       
+        for( int tar=0; tar < target_layers.length(); tar++ )
+        {
+            for (int j = 0; j < target_prediction_list[tar][i].length() ; j++ ){
+                
+                if(i>n){
+                    myfile << target_prediction_list[tar][i][j] << " ";
+                }
+                else{
+                    myfile << targets_list[tar][i][j] << " ";
+                }
+                       
+           
+            }
+        }
+        myfile << "\n";
+    }
+     
+
+     myfile.close();
+
+}
+/*
+void DynamicallyLinkedRBMsModel::gen()
+{
+    //PPath* the_filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data = new AutoVMatrix();
+    data->filename = "/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/scoreGen.amat";
+    data->defineSizes(21,0,0);
+    //data->inputsize = 21;
+    //data->targetsize = 0;
+    //data->weightsize = 0;
+    data->build();
+
+    
+    int len = data->length();
+    Vec score;
+    Vec target;
+    real weight;
+    Vec bias_tempo;
+    Vec visi_bias_tempo;
+   
+   
+    
+    previous_hidden_layer.resize(hidden_layer->size);
+    connections_idem = connections;
+
+    for (int ith_sample = 0; ith_sample < len ; ith_sample++ ){
+        
+        data->getExample(ith_sample, score, target, weight);
+        //score << data(ith_sample);
+        input_prediction_list.resize(
+            ith_sample+1,visible_layer->size);
+        if(ith_sample > 0)
+        {
+            
+            //input_list(ith_sample_in_sequence) << previous_input;
+            //h*_{t-1}
+            //////////////////////////////////
+            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+            hidden_layer->setAllBias(cond_bias); 
+            
+            
+            
+            //up phase
+            connections->setAsDownInput( input_prediction_list(ith_sample-1) );
+            hidden_layer->getAllActivations( connections_idem );
+            hidden_layer->computeExpectation();
+            //////////////////////////////////
+            
+            //previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour
+            //previous_hidden_layer_act_no_bias << hidden_layer->activation;
+            
+            
+            //h*_{t}
+            ////////////
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            //dynamic_connections_copy->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            ///////////
+            
+            //previous_input << visible_layer->expectation;//v_{t-1}
+            
+        }
+        else
+        {
+            
+            previous_hidden_layer.clear();//h_{t-1}
+            if(dynamic_connections_copy)
+                dynamic_connections_copy->fprop( previous_hidden_layer ,
+                                                 hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            else
+                dynamic_connections->fprop(previous_hidden_layer,
+                                           hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            
+            hidden_layer->expectation_is_not_up_to_date();
+            hidden_layer->computeExpectation();//h_{t}
+            //previous_input.resize(data->inputsize);
+            //previous_input << data(ith_sample);
+            
+        }
+        
+        //connections_transpose->setAsDownInput( hidden_layer->expectation );
+        //visible_layer->getAllActivations( connections_idem_t );
+        
+        connections->setAsUpInput( hidden_layer->expectation );
+        visible_layer->getAllActivations( connections_idem );
+        
+        visible_layer->computeExpectation();
+        //visible_layer->generateSample();
+        partition(score.subVec(14,taillePart), visible_layer->activation.subVec(14+taillePart,taillePart), visible_layer->activation.subVec(14+(taillePart*2),taillePart));
+        partition(score.subVec(14,taillePart), visible_layer->expectation.subVec(14+taillePart,taillePart), visible_layer->expectation.subVec(14+(taillePart*2),taillePart));
+
+
+        visible_layer->activation.subVec(0,14+taillePart) << score;
+        visible_layer->expectation.subVec(0,14+taillePart) << score;
+
+        input_prediction_list(ith_sample) << visible_layer->expectation;
+        
+    }
+    
+    //Vec tempo;
+    TVec<real> tempo;
+    tempo.resize(visible_layer->size);
+    ofstream myfile;
+    myfile.open ("/home/stan/Documents/recherche_maitrise/DDBN_bosendorfer/data/generate/test.txt");
+    
+    for (int i = 0; i < len ; i++ ){
+        tempo << input_prediction_list(i);
+        
+        //cout << tempo[2] << endl;
+       
+        for (int j = 0; j < tempo.length() ; j++ ){
+            
+            
+                
+                
+               myfile << tempo[j] << " ";
+               
+
+               
+           
+        }
+        myfile << "\n";
+    }
+     
+
+     myfile.close();
+
+}*/
 //void DynamicallyLinkedRBMsModel::generate(int nbNotes)
 //{
 //    

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-07-09 19:20:41 UTC (rev 9219)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2008-07-09 19:26:16 UTC (rev 9220)
@@ -165,7 +165,7 @@
     
 
 //    //! Generate music in a folder
-//    void generate(int nbNotes);
+    void generate(int t, int n);
 //
 //    //! Generate a part of the data in a folder
 //    void gen();



From nouiz at mail.berlios.de  Wed Jul  9 22:32:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 9 Jul 2008 22:32:21 +0200
Subject: [Plearn-commits] r9221 - trunk/plearn_learners/regressors
Message-ID: <200807092032.m69KWLOp024368@sheep.berlios.de>

Author: nouiz
Date: 2008-07-09 22:32:20 +0200 (Wed, 09 Jul 2008)
New Revision: 9221

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
   trunk/plearn_learners/regressors/RegressionTreeRegisters.h
Log:
removed warning of comparaison between signed and unsigned value


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-07-09 19:26:16 UTC (rev 9220)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-07-09 20:32:20 UTC (rev 9221)
@@ -146,7 +146,7 @@
     sortRows();
 }
 
-void RegressionTreeRegisters::registerLeave(int leave_id, int row)
+void RegressionTreeRegisters::registerLeave(RTR_type leave_id, int row)
 {
     leave_register[row] = leave_id;
 }
@@ -189,7 +189,7 @@
     return next_id;
 }
 
-void RegressionTreeRegisters::getAllRegisteredRow(int leave_id, int col, TVec<RTR_type> &reg)
+void RegressionTreeRegisters::getAllRegisteredRow(RTR_type leave_id, int col, TVec<RTR_type> &reg)
 {
     for(int i=0;i<length();i++)
     {

Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-07-09 19:26:16 UTC (rev 9220)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.h	2008-07-09 20:32:20 UTC (rev 9221)
@@ -49,7 +49,6 @@
 
 //!used to limit the memory used by limiting the length of the dataset.
 //!work with unsigned int, uint16_t, but fail with uint8_t???
-//!always use unsigned type! Otherwise you need to modif RegressionTreeRegisters.cc too
 #define RTR_type uint32_t 
 
 namespace PLearn {
@@ -92,13 +91,13 @@
     virtual void         build();
     void         initRegisters(VMat train_set);
     void         reinitRegisters();
-    void         registerLeave(int leave_id, int row);
+    void         registerLeave(RTR_type leave_id, int row);
     virtual real get(int row, int col) const;
     real         getTarget(int row);
     real         getWeight(int row);
     void         setWeight(int row,real val);
     RTR_type     getNextId();
-    void         getAllRegisteredRow(int leave_id, int col, TVec<RTR_type> &reg);
+    void         getAllRegisteredRow(RTR_type leave_id, int col, TVec<RTR_type> &reg);
     void         sortRows();
     void         printRegisters();
     void         getExample(int i, Vec& input, Vec& target, real& weight);



From ducharme at mail.berlios.de  Thu Jul 10 17:48:04 2008
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 10 Jul 2008 17:48:04 +0200
Subject: [Plearn-commits] r9222 - trunk/python_modules/plearn/gui_tools
Message-ID: <200807101548.m6AFm4mF031361@sheep.berlios.de>

Author: ducharme
Date: 2008-07-10 17:48:03 +0200 (Thu, 10 Jul 2008)
New Revision: 9222

Modified:
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
We can now select a "expdir" name.


Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-07-09 20:32:20 UTC (rev 9221)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-07-10 15:48:03 UTC (rev 9222)
@@ -253,8 +253,11 @@
     def _new_xp_context(self):
         """Initialize an experiment context
         """
-        expdir  = self.expdir_name(self.script_params.expdir_root)
-        context = ExperimentContext(expdir = expdir,
+        expdir = None
+        if hasattr(self.script_params, 'expdir'):
+            expdir = self.script_params.expdir
+        expdir_path = self.expdir_name(self.script_params.expdir_root, expdir)
+        context = ExperimentContext(expdir = expdir_path,
                                     gui = self.gui,
                                     script_params = self.script_params,
                                     expfunc = self.expfunc)
@@ -368,10 +371,11 @@
 
 
     @staticmethod
-    def expdir_name(expdir_root):
-        """Return an experiment directory from a root location."""
-        return os.path.join(expdir_root,
-                            datetime.now().strftime("expdir_%Y%m%d_%H%M%S"))
+    def expdir_name(expdir_root, expdir=None):
+        """Return an experiment directory from a root location and possibly a dir name."""
+        if expdir is None or expdir == '':
+            expdir = datetime.now().strftime("expdir_%Y%m%d_%H%M%S")
+        return os.path.join(expdir_root, expdir)
 
 
     @staticmethod
@@ -452,7 +456,8 @@
 if __name__ == "__main__":
     class GlobalOpt(HasStrictTraits):
         expdir_root       = Directory(".", auto_set=True,
-                                         desc="Where the experiment directory should be created")
+                                         desc="where the experiment directory should be created")
+        expdir            = Str("",      desc="experiment directory name")
         max_train_size    = Trait( -1 ,  desc="maximum size of training set (in days)")
         nhidden           = Trait(3,     desc="number of hidden units")
         weight_decay      = Trait(1e-8,  desc="weight decay to use for neural-net training")
@@ -464,6 +469,7 @@
 
     class AllOpt(HasStrictTraits):
         expdir_root = Delegate("GlobalOpt")
+        expdir      = Delegate("GlobalOpt")
         GlobalOpt   = Instance(GlobalOpt, ())
         MinorOpt    = Instance(MinorOpt,  ())
 



From nouiz at mail.berlios.de  Thu Jul 10 18:10:11 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 10 Jul 2008 18:10:11 +0200
Subject: [Plearn-commits] r9223 - trunk/scripts
Message-ID: <200807101610.m6AGABst000933@sheep.berlios.de>

Author: nouiz
Date: 2008-07-10 18:10:10 +0200 (Thu, 10 Jul 2008)
New Revision: 9223

Modified:
   trunk/scripts/dbidispatch
Log:
create the log dir if needed


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-07-10 15:48:03 UTC (rev 9222)
+++ trunk/scripts/dbidispatch	2008-07-10 16:10:10 UTC (rev 9223)
@@ -145,6 +145,7 @@
 LOGDIR=os.getenv("DBIDISPATCH_LOGDIR")
 if not LOGDIR:
     LOGDIR="LOGS"
+os.mkdir(LOGDIR)
 
 to_parse=[]
 env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")



From nouiz at mail.berlios.de  Thu Jul 10 18:21:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 10 Jul 2008 18:21:00 +0200
Subject: [Plearn-commits] r9224 - trunk/scripts
Message-ID: <200807101621.m6AGL0Qo003169@sheep.berlios.de>

Author: nouiz
Date: 2008-07-10 18:21:00 +0200 (Thu, 10 Jul 2008)
New Revision: 9224

Modified:
   trunk/scripts/dbidispatch
Log:
make log dif only if it don't exist


Modified: trunk/scripts/dbidispatch
===================================================================
--- trunk/scripts/dbidispatch	2008-07-10 16:10:10 UTC (rev 9223)
+++ trunk/scripts/dbidispatch	2008-07-10 16:21:00 UTC (rev 9224)
@@ -145,7 +145,8 @@
 LOGDIR=os.getenv("DBIDISPATCH_LOGDIR")
 if not LOGDIR:
     LOGDIR="LOGS"
-os.mkdir(LOGDIR)
+if not os.path.exists(LOGDIR):
+    os.mkdir(LOGDIR)
 
 to_parse=[]
 env=os.getenv("DBIDISPATCH_DEFAULT_OPTION")



From saintmlx at mail.berlios.de  Thu Jul 10 20:51:40 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Jul 2008 20:51:40 +0200
Subject: [Plearn-commits] r9225 - trunk/plearn/python
Message-ID: <200807101851.m6AIpeIt014656@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-10 20:51:39 +0200 (Thu, 10 Jul 2008)
New Revision: 9225

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
- added support for Array<T> and null VMats in PLearn<->Python bridge



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-07-10 16:21:00 UTC (rev 9224)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-07-10 18:51:39 UTC (rev 9225)
@@ -232,11 +232,14 @@
     PLASSERT( pyobj );
     PyObject* pyarr0= PyArray_CheckFromAny(pyobj, NULL,
                                            2, 2, NPY_CARRAY_RO, Py_None);
+    if(!pyarr0)
+        PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj,
+                                print_traceback);
     PyObject* pyarr= 
         PyArray_CastToType(reinterpret_cast<PyArrayObject*>(pyarr0),
                            PyArray_DescrFromType(PL_NPY_REAL), 0);
     Py_XDECREF(pyarr0);
-    if (! pyarr)
+    if(!pyarr)
         PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj,
                                 print_traceback);
     m.resize(PyArray_DIM(pyarr,0), PyArray_DIM(pyarr,1));
@@ -258,6 +261,8 @@
                                                        bool print_traceback)
 {
     PLASSERT(pyobj);
+    if(pyobj == Py_None)
+        return 0;
     if(PyObject_HasAttrString(pyobj, "_cptr"))
         return static_cast<VMatrix*>(
             ConvertFromPyObject<Object*>::convert(pyobj, print_traceback));
@@ -507,6 +512,8 @@
     Object* obj= args_tvec[0];
     args_tvec.subVecSelf(1, args_tvec.size()-1);
 
+    //perr << "REMOTE METHOD: " << obj->classname() << "::" << tramp->documentation().name() << endl;
+
     gc_collect1();
 
     //call, catch and send any errors to python

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2008-07-10 16:21:00 UTC (rev 9224)
+++ trunk/plearn/python/PythonObjectWrapper.h	2008-07-10 18:51:39 UTC (rev 9225)
@@ -412,6 +412,12 @@
 };
 
 template <class T>
+struct ConvertFromPyObject< Array<T> >
+{
+    static Array<T> convert(PyObject*, bool print_traceback);
+};
+
+template <class T>
 struct ConvertFromPyObject<TMat<T> >
 {
     static TMat<T> convert(PyObject*, bool print_traceback);
@@ -653,6 +659,10 @@
 struct ConvertToPyObject<tuple<T,U,V,W,X,Y,Z> >
 { static PyObject* newPyObject(const tuple<T, U, V, W, X, Y, Z>&); };
 
+//! Generic array: create a Python list of those objects recursively
+template <class T> struct ConvertToPyObject<Array<T> >
+{ static PyObject* newPyObject(const Array<T>&); };
+
 //! Generic vector: create a Python list of those objects recursively
 template <class T> struct ConvertToPyObject<TVec<T> >
 { static PyObject* newPyObject(const TVec<T>&); };
@@ -1024,6 +1034,48 @@
 PyObject* convertArrayCheck(PyObject* pyobj, int numpy_type, int ndim, bool print_traceback);
 
 template <class T>
+Array<T> ConvertFromPyObject<Array<T> >::convert(PyObject* pyobj,
+                                                  bool print_traceback)
+{
+    PLASSERT(pyobj);
+
+    // Here, we support both Python Tuples and Lists
+    if(PyTuple_Check(pyobj)) {
+        // Tuple case
+        int size = PyTuple_GET_SIZE(pyobj);
+        Array<T> v(size);
+        for (int i=0 ; i<size ; ++i) {
+            PyObject* elem_i = PyTuple_GET_ITEM(pyobj, i);
+            v[i] = ConvertFromPyObject<T>::convert(elem_i, print_traceback);
+        }
+        return v;
+    }
+    else if (PyList_Check(pyobj)) {
+        // List case
+        int size = PyList_GET_SIZE(pyobj);
+        Array<T> v(size);
+        for (int i=0 ; i<size ; ++i) {
+            PyObject* elem_i = PyList_GET_ITEM(pyobj, i);
+            v[i] = ConvertFromPyObject<T>::convert(elem_i, print_traceback);
+        }
+        return v;
+    }
+    else if (PyObject* pyarr= convertArrayCheck(pyobj, numpyType<T>(), 1, print_traceback))
+    {
+        int sz= PyArray_DIM(pyarr,0);
+        Array<T> v(sz);
+        v.copyFrom(static_cast<T*>(PyArray_DATA(pyarr)), sz);
+        Py_XDECREF(pyarr);
+        return v;
+    }
+    else
+        PLPythonConversionError("ConvertFromPyObject<Array<T> >", pyobj,
+                                print_traceback);
+
+    return Array<T>();                        // Shut up compiler
+}
+
+template <class T>
 TVec<T> ConvertFromPyObject< TVec<T> >::convert(PyObject* pyobj,
                                                 bool print_traceback)
 {
@@ -1257,6 +1309,19 @@
 }
 
 template <class T>
+PyObject* ConvertToPyObject<Array<T> >::newPyObject(const Array<T>& data)
+{
+    PyObject* newlist = PyList_New(data.size());
+    for (int i=0, n=data.size() ; i<n ; ++i) {
+        // Since PyList_SET_ITEM steals the reference to the item being set,
+        // one does not need to Py_XDECREF the inserted string as was required
+        // for the PyArrayObject code above...
+        PyList_SET_ITEM(newlist, i, ConvertToPyObject<T>::newPyObject(data[i]));
+    }
+    return newlist;
+}
+
+template <class T>
 PyObject* ConvertToPyObject<TVec<T> >::newPyObject(const TVec<T>& data)
 {
     PyObject* newlist = PyList_New(data.size());



From saintmlx at mail.berlios.de  Thu Jul 10 20:52:50 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Jul 2008 20:52:50 +0200
Subject: [Plearn-commits] r9226 - trunk/plearn/vmat
Message-ID: <200807101852.m6AIqoRX014745@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-10 20:52:49 +0200 (Thu, 10 Jul 2008)
New Revision: 9226

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VMatrix.h
Log:
- new remote methods



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-10 18:51:39 UTC (rev 9225)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-10 18:52:49 UTC (rev 9226)
@@ -160,6 +160,12 @@
         "DO NOT play with this if you don't know the implementation!\n"
         "This add a dependency mtime to the gived value.");
 
+
+    declareOption(
+        ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::learntoption,
+        "Field infos.\n");
+
+
     inherited::declareOptions(ol);
 }
 
@@ -182,6 +188,12 @@
          RetDoc ("An (input, target, weight) tuple.")));
 
     declareMethod(
+        rmm, "getExtra", &VMatrix::remote_getExtra,
+        (BodyDoc("Returns the extra part of a row.\n"),
+         ArgDoc ("i", "Position of the row to get.\n"),
+         RetDoc ("Values for extrafields.")));
+
+    declareMethod(
         rmm, "getColumn", &VMatrix::remote_getColumn,
         (BodyDoc("Returns a row of a matrix \n"),
          ArgDoc ("i", "Position of the row to get.\n"),
@@ -313,6 +325,11 @@
          ArgDoc ("extrasize", "extrasize")));
 
     declareMethod(
+        rmm, "copySizesFrom", &VMatrix::copySizesFrom,
+        (BodyDoc("Define this vmatrix's sizes from another vmatrix\n"),
+         ArgDoc ("vm", "the other vmatrix")));
+
+    declareMethod(
         rmm, "addStringMapping", static_cast<void (VMatrix::*)(int, string, real)>(&VMatrix::addStringMapping),
         (BodyDoc("Add or replace a string mapping for a column\n"),
          ArgDoc ("col", "column number"),
@@ -330,6 +347,12 @@
         (BodyDoc("Get the real->string mapping for a given column.\n"),
          ArgDoc ("col", "column number"),
          RetDoc ("map of real->string")));
+
+    declareMethod(
+        rmm, "setMetaInfoFrom", &VMatrix::setMetaInfoFrom,
+        (BodyDoc("Set this vmatrix's meta-info from another vmatrix\n"),
+         ArgDoc ("vm", "the other vmatrix")));
+
 }
 
 
@@ -697,7 +720,14 @@
         getSubRow(i,inputsize_+targetsize_+weightsize_, extra);
 }
 
+Vec VMatrix::remote_getExtra(int i)
+{
+    Vec extra;
+    getExtra(i, extra);
+    return extra;
+}
 
+
 //////////////////
 // computeStats //
 //////////////////

Modified: trunk/plearn/vmat/VMatrix.h
===================================================================
--- trunk/plearn/vmat/VMatrix.h	2008-07-10 18:51:39 UTC (rev 9225)
+++ trunk/plearn/vmat/VMatrix.h	2008-07-10 18:52:49 UTC (rev 9226)
@@ -505,6 +505,7 @@
      *  weightsize_ and extrasize_
      */
     virtual void getExtra(int i, Vec& extra);
+    Vec remote_getExtra(int i);
 
     /// This method must be implemented in all subclasses
     virtual real get(int i, int j) const = 0; ///<  Returns element (i,j).



From saintmlx at mail.berlios.de  Thu Jul 10 20:53:55 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Jul 2008 20:53:55 +0200
Subject: [Plearn-commits] r9227 - trunk/plearn/vmat
Message-ID: <200807101853.m6AIrtmN014811@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-10 20:53:55 +0200 (Thu, 10 Jul 2008)
New Revision: 9227

Modified:
   trunk/plearn/vmat/VMField.cc
   trunk/plearn/vmat/VMField.h
Log:
- added VMField serialization



Modified: trunk/plearn/vmat/VMField.cc
===================================================================
--- trunk/plearn/vmat/VMField.cc	2008-07-10 18:52:49 UTC (rev 9226)
+++ trunk/plearn/vmat/VMField.cc	2008-07-10 18:53:55 UTC (rev 9227)
@@ -65,6 +65,18 @@
     return in; // shut up compiler
 }
 
+PStream& operator>>(PStream& in, VMField& x)
+{
+    int y;
+    in >> ws >> x.name >> ws >> y;//x.fieldtype;
+    x.fieldtype= static_cast<VMField::FieldType>(y);
+    return in;
+}
+PStream& operator<<(PStream& out, const VMField& x)
+{
+    out << ' ' << x.name << ' ' << x.fieldtype << ' ';
+    return out;
+}
 
 /** VMFieldStat **/
 

Modified: trunk/plearn/vmat/VMField.h
===================================================================
--- trunk/plearn/vmat/VMField.h	2008-07-10 18:52:49 UTC (rev 9226)
+++ trunk/plearn/vmat/VMField.h	2008-07-10 18:53:55 UTC (rev 9227)
@@ -80,6 +80,9 @@
 
 PStream& operator>>(PStream& in, VMField::FieldType& x); // dummy placeholder; do not call
 
+PStream& operator>>(PStream& in, VMField& x);
+PStream& operator<<(PStream& out, const VMField& x);
+
 //!  this class holds simple statistics about a field
 class VMFieldStat
 {



From saintmlx at mail.berlios.de  Thu Jul 10 20:54:43 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Jul 2008 20:54:43 +0200
Subject: [Plearn-commits] r9228 - trunk/plearn/vmat
Message-ID: <200807101854.m6AIshG8014872@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-10 20:54:42 +0200 (Thu, 10 Jul 2008)
New Revision: 9228

Modified:
   trunk/plearn/vmat/ConcatRowsVMatrix.cc
Log:
- support extrasize



Modified: trunk/plearn/vmat/ConcatRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-07-10 18:53:55 UTC (rev 9227)
+++ trunk/plearn/vmat/ConcatRowsVMatrix.cc	2008-07-10 18:54:42 UTC (rev 9228)
@@ -162,11 +162,12 @@
     // Set length_ and width_.
     recomputeDimensions();
 
-    // Note that the 3 lines below will overwrite any provided sizes.
+    // Note that the 4 lines below will overwrite any provided sizes.
     if(inputsize_<0 && targetsize_<0 && weightsize_<0){
         inputsize_ = to_concat[0]->inputsize();
         targetsize_ = to_concat[0]->targetsize();
         weightsize_ = to_concat[0]->weightsize();
+        extrasize_ = to_concat[0]->extrasize();
         if(fieldinfos.size()!=to_concat[0].width())
             PLERROR("In ConcatRowsVMatrix::build_() - We override "
                       "inputsize, targetsize and weightsize with the value"



From saintmlx at mail.berlios.de  Thu Jul 10 20:55:39 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 10 Jul 2008 20:55:39 +0200
Subject: [Plearn-commits] r9229 - trunk/plearn/vmat
Message-ID: <200807101855.m6AItdhV014997@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-10 20:55:38 +0200 (Thu, 10 Jul 2008)
New Revision: 9229

Modified:
   trunk/plearn/vmat/KFoldSplitter.cc
Log:
- more info in error message



Modified: trunk/plearn/vmat/KFoldSplitter.cc
===================================================================
--- trunk/plearn/vmat/KFoldSplitter.cc	2008-07-10 18:54:42 UTC (rev 9228)
+++ trunk/plearn/vmat/KFoldSplitter.cc	2008-07-10 18:55:38 UTC (rev 9229)
@@ -171,7 +171,8 @@
     real start = cross_range.first;
     real end   = cross_range.second;
     int n_data = dataset->length();
-    PLASSERT( start >= 0 && end >= 0 && end > start && start < n_data && end < n_data );
+    PLASSERT_MSG(start >= 0 && end >= 0 && end > start && start < n_data && end < n_data,
+                 string("start=")+tostring(start)+", end="+tostring(end)+", n_data="+tostring(n_data));
     int i_start = 0;
     if (start > 0)
         i_start = start >= 1 ? int(round(start)) : int(round(n_data * start));



From saintmlx at mail.berlios.de  Fri Jul 11 00:15:34 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 11 Jul 2008 00:15:34 +0200
Subject: [Plearn-commits] r9230 - trunk/plearn/vmat
Message-ID: <200807102215.m6AMFY3S006008@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-11 00:15:34 +0200 (Fri, 11 Jul 2008)
New Revision: 9230

Modified:
   trunk/plearn/vmat/VMField.cc
   trunk/plearn/vmat/VMatrix.cc
Log:
- fixed serialization of fieldinfos



Modified: trunk/plearn/vmat/VMField.cc
===================================================================
--- trunk/plearn/vmat/VMField.cc	2008-07-10 18:55:38 UTC (rev 9229)
+++ trunk/plearn/vmat/VMField.cc	2008-07-10 22:15:34 UTC (rev 9230)
@@ -68,13 +68,13 @@
 PStream& operator>>(PStream& in, VMField& x)
 {
     int y;
-    in >> ws >> x.name >> ws >> y;//x.fieldtype;
+    in >> ws >> x.name >> ws >> y;
     x.fieldtype= static_cast<VMField::FieldType>(y);
     return in;
 }
 PStream& operator<<(PStream& out, const VMField& x)
 {
-    out << ' ' << x.name << ' ' << x.fieldtype << ' ';
+    out << x.name << x.fieldtype;
     return out;
 }
 

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-10 18:55:38 UTC (rev 9229)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-10 22:15:34 UTC (rev 9230)
@@ -162,7 +162,7 @@
 
 
     declareOption(
-        ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::learntoption,
+        ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::buildoption,
         "Field infos.\n");
 
 



From saintmlx at mail.berlios.de  Fri Jul 11 00:40:14 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 11 Jul 2008 00:40:14 +0200
Subject: [Plearn-commits] r9231 - trunk/python_modules/plearn/pybridge
Message-ID: <200807102240.m6AMeEEW020030@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-11 00:39:59 +0200 (Fri, 11 Jul 2008)
New Revision: 9231

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- new format for pickled PLearn Objects (__setstate__ is backward-compatible)
- new Python repr for PLearn Objects (use Python's default instead of custom __repr__)



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-07-10 22:15:34 UTC (rev 9230)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-07-10 22:39:59 UTC (rev 9231)
@@ -57,20 +57,25 @@
 
 
 class WrappedPLearnObject(object):
-
+    """
+    Wrapper class for PLearn objects used from Python.
+    """
+    
+    # for debug purposes: can print warnings when setting attributes
+    # which are not PLearn options 
     allowed_non_PLearn_options= ['_cptr']
     warn_non_PLearn_options= False # you can turn this on to help debugging
 
     def __init__(self, **kwargs):
-        #print 'WrappedPLearnObject.__init__',type(self),kwargs
+        """
+        ctor.: manage pointer to underlying C++ object
+        """
         if '_cptr' in kwargs:
             self._cptr= kwargs['_cptr'] # ptr to c++ obj
         elif hasattr(self,'_cptr'):
-            #print 'init->SETOPTIONS2',kwargs
             self.setOptions(kwargs)
             
     def setOptions(self, kwargs):
-        #print 'SETOPTIONS',kwargs
         call_build= True
         for k in kwargs:
             if k=='__call_build':
@@ -104,8 +109,9 @@
         if hasattr(self, '_cptr'):
             self._unref()
 
-    def __repr__(self):
-        return self.asString() #PLearn repr. for now
+    # DEPRECATED: now use Python's default <modulename.classname object>
+    #def __repr__(self):
+    #    return self.asString() #PLearn repr. for now
 
     def __deepcopy__(self, memo= None):
         if not memo: memo= {}
@@ -124,14 +130,64 @@
         return newone
 
     def __getstate__(self):
+        """
+        Returns self's dict, except that the value associated with the '_cptr'
+        key is replaced by 
+        """
+        PLEARN_PICKLE_PROTOCOL_VERSION= 2
         d= self.__dict__.copy()
-        if remote_pickle:
-            d['_cptr']= self.asStringRemoteTransmit()
-        else:
-            d['_cptr']= self.asString()
+        d['_cptr']= (PLEARN_PICKLE_PROTOCOL_VERSION,self.classname(),{})
+        for o in self._optionnames:
+            d['_cptr'][2][o]= self.getOption(o)
         return d
+        ##### old, deprecated version follows: (for reference only)
+        #d= self.__dict__.copy()
+        #if remote_pickle:
+        #    d['_cptr']= self.asStringRemoteTransmit()
+        #else:
+        #    d['_cptr']= self.asString()
+        #return d
     
     def __setstate__(self, dict):
+        """
+        Unpickle wrapped PLearn objects pickled using
+        pybridge's 2nd protocol; the original protocol is
+        also supported for backward compatibility
+        (through old_deprecated___setstate__)
+        Protocol #2 expects a standard pickle dictionary,
+        except that the '_cptr' element of this dict. is
+        a 3-tuple as follows:
+          ( <protocol version>, <PLearn class name>, <dict of PLearn options->values> )
+        e.g.:
+          (2, 'LinearRegressor', {'weight_decay': 1e-3})
+        Note that only protocol version 2 is allowed; the version 1 protocol
+        uses a totally different format (see old_deprecated___setstate__) 
+        """
+        d= dict['_cptr']
+        if isinstance(d, str):# Protocol v.1 (deprecated)
+            return self.old_deprecated___setstate__(dict)
+        # Protocol v.2:
+        PLEARN_PICKLE_PROTOCOL_VERSION= 2
+        if d[0] != PLEARN_PICKLE_PROTOCOL_VERSION:
+            raise RuntimeError, "PLearn pickle protocol version should be 2"
+        newone= plearn_module.newObjectFromClassname(d[1])
+        self._cptr= newone._cptr
+        self._refCPPObj(self, False)
+        for k in dict:
+            if k != '_cptr':
+                self.__setattr__(k, dict[k])
+        for o in d[1]:
+            self.setOptionFromPython(o,d[2][o])
+        self.build()
+        return dict
+
+    def old_deprecated___setstate__(self, dict):
+        """
+        Provide support to unpickle objects saved
+        in the original (PLearn) format.  This is
+        automatically called from __setstate__ when
+        needed.
+        """
         newone= plearn_module.newObject(dict['_cptr'])
         self._cptr= newone._cptr
         self._refCPPObj(self, False)
@@ -141,10 +197,11 @@
         return dict
 
 
-from numpy.numarray import *
-
 class WrappedPLearnVMat(WrappedPLearnObject):
-
+    """
+    Specialized wrapper for PLearn VMatrices;
+    supplies __len__ and __getitem__ methods.
+    """
     def __len__(self):
         return self.length
 
@@ -163,7 +220,6 @@
             if stop < 0: stop+= l
             if step==1:
                 return self.subMat(start, 0, stop-start, self.width)
-            #return pl.MemoryVMatrix(data= [self[i] for i in xrange(start, stop, step)])
             raise NotImplementedError, 'slice step != 1'
         raise TypeError, "key should be an int or a slice"
     



From saintmlx at mail.berlios.de  Fri Jul 11 01:00:53 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 11 Jul 2008 01:00:53 +0200
Subject: [Plearn-commits] r9232 - in trunk:
	plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0
	plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir
Message-ID: <200807102300.m6AN0rL7015679@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-11 01:00:51 +0200 (Fri, 11 Jul 2008)
New Revision: 9232

Modified:
   trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_costs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_outputs.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/global_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/split_stats.pmat
   trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave
Log:
- update pytest results



Modified: trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log
===================================================================
--- trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn/vmat/test/.pytest/PL_AutoVMatrix/expected_results/RUN.log	2008-07-10 23:00:51 UTC (rev 9232)
@@ -11,7 +11,8 @@
 targetsize = 0 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.amat.metadata/"  )
+metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.amat.metadata/" ;
+fieldinfos = 6 [ "x1" 0 "x2" 0 "x3" 0 "x4" 0 "y1" 0 "y2" 0 ]  )
 
 200  6  [ 
 -0.676509999999999945 	-0.354930000000000023 	-6.93189999999999973 	-1.70795000000000008 	-35.9233100000000007 	-7.30156999999999989 	
@@ -236,7 +237,8 @@
 targetsize = 2 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/"  )
+metadatadir = "PLEARNDIR:examples/data/test_suite/linear_4x_2y.pmat.metadata/" ;
+fieldinfos = 6 [ "0" 0 "1" 0 "2" 0 "3" 0 "4" 0 "5" 0 ]  )
 
 200  6  [ 
 -0.676509999999999945 	-0.354930000000000023 	-6.93189999999999973 	-1.70795000000000008 	-35.9233100000000007 	-7.30156999999999989 	
@@ -461,7 +463,8 @@
 targetsize = 0 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/"  )
+metadatadir = "PLEARNDIR:examples/data/test_suite/eslt_mixture/data_train.amat.metadata/" ;
+fieldinfos = 3 [ "x1" 0 "y2" 0 "class" 0 ]  )
 
 200  3  [ 
 2.52609296787849136 	0.32105044573636482 	0 	

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/Strat0results.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/LearnerExpdir/final_learner.psave	2008-07-10 23:00:51 UTC (rev 9232)
@@ -12,231 +12,231 @@
 weight_decay = 0.0100000000000000002 ;
 include_bias = 1 ;
 params = 225  2  [ 
-2.71513510622207166 	6.96796926050934751 	
--0.924570620987045566 	-5.50999860198858826 	
-2.74766209737875133 	16.2764471019206418 	
--0.897331116697557052 	24.1696624838867358 	
--0.257145462011077963 	-2.68785159119476535 	
--1.3282966539726655 	-18.5932956256296649 	
--5.49258347509019718 	-1.57002604298704584 	
-1.46070534954897835 	-3.53276434233635506 	
--1.038126171674709 	-1.2073115958693148 	
-1.02580172581497386 	-26.7245637328128929 	
-0.791254761045932176 	82.4469215737125865 	
-0.836570941006688051 	-16.4202630459276833 	
--1.58240826854525074 	-6.65940808938515527 	
--8.24967860415612364 	-3.49957089648365738 	
-0.9782411438628974 	-4.92116475768973771 	
-0.0717117261764370351 	-3.09067303050886988 	
--2.58531178640483761 	12.035458429525173 	
-3.211076444250756 	5.97223155883393897 	
-1.43705644733268212 	94.2974631733484614 	
-0.651016019176285488 	1.95403761231488304 	
--1.20978628334898786 	-7.36323983157156103 	
--0.676733526068380775 	-31.4356962338337986 	
--3.71894302925004316 	-5.30412867324711534 	
--2.6818491206821955 	-0.224631918494603705 	
--1.88827683784314537 	-11.2808510609389074 	
-1.56552507696732834 	-5.27853909851336311 	
--2.51225078969590498 	-3.22314543003026799 	
--8.82457968582814622 	-1.42245769348111262 	
--5.49712489654685843 	-14.7599950960045039 	
-8.47592427242672741 	8.11107085181684795 	
-2.34953305575425997 	4.63659562959487381 	
--1.64301101095052804 	11.4873476009648563 	
-0.580660833463284676 	13.1755650470951462 	
--0.650250885492095976 	1.90203710703754014 	
--3.87917543422272804 	5.03518774179621609 	
-1.70397379112897029 	1.34775554557823152 	
--2.78522303432040363 	-64.1292521132013178 	
-2.32700743878965444 	24.7436125596276284 	
--5.39566293426405519 	-9.82739376173207368 	
-12.9203582472648133 	5.29957416697557626 	
--10.7455769141731885 	89.9090483177333653 	
-4.31713435896750308 	-5.18586285076809528 	
-0.848047232583675648 	-2.17948525292697681 	
--9.46810320629667324 	-0.220748951447968828 	
-7.21414655874827737 	7.46866878382293553 	
--1.75858145600769644 	11.2871954305640294 	
-1.96560792644487359 	-6.52458829385446837 	
--2.79380808107613055 	-5.09955504191000042 	
--0.79905931023683674 	-12.925504941414971 	
--4.25437046157972976 	8.96615214816773154 	
--9.43622335236653065 	3.78339978190649218 	
-4.36697916817812093 	-0.68750387907888455 	
--2.18796130106569242 	-6.69802616230909642 	
-3.17982632559557965 	-1.4370316032819157 	
--4.31414536641474289 	-5.51667561668654649 	
--2.74519292766675793 	7.73168925235037285 	
-1.0537820640848472 	0.178110338215527242 	
-0.460217896787073477 	10.058544903984771 	
--0.0106331952212118993 	-19.6790725953963772 	
-0.424465188958454087 	-15.8551532141373048 	
-0.567218573772457435 	-2.69519041953220029 	
-4.45718670597664524 	5.62393864871897797 	
-2.19961229164894245 	-5.42279390073130685 	
--0.228762156047200543 	-0.831542078921280403 	
--2.40103186222691534 	15.8373411775187218 	
--1.34006398347233469 	-12.0359127014981127 	
-4.03810217513344138 	-2.91524838149503962 	
-0.593248196381594317 	0.218503040354157629 	
+2.7151351062236615 	6.96796926051022236 	
+-0.924570620989403458 	-5.50999860198525226 	
+2.74766209737707889 	16.2764471019291541 	
+-0.897331116702694054 	24.1696624839723277 	
+-0.257145462010700432 	-2.68785159119131878 	
+-1.3282966539723533 	-18.5932956256279311 	
+-5.49258347507660183 	-1.57002604296742554 	
+1.46070534955196196 	-3.53276434234297421 	
+-1.03812617167629262 	-1.20731159585204617 	
+1.02580172582147133 	-26.7245637329914061 	
+0.791254761045375288 	82.4469215737143202 	
+0.836570941010414737 	-16.420263045944921 	
+-1.58240826855158301 	-6.65940808940268703 	
+-8.24967860415081411 	-3.4995708964716874 	
+0.978241143861720674 	-4.92116475768050154 	
+0.0717117261694705244 	-3.09067303052573861 	
+-2.58531178640539849 	12.0354584295284699 	
+3.21107644425249639 	5.97223155883275147 	
+1.43705644733271809 	94.2974631733482198 	
+0.651016019177080518 	1.95403761232638362 	
+-1.20978628334706184 	-7.36323983158574702 	
+-0.676733526068720059 	-31.4356962338422257 	
+-3.71894302925004183 	-5.30412867323935266 	
+-2.68184912068139347 	-0.224631918495068694 	
+-1.88827683783783651 	-11.2808510609680628 	
+1.56552507696869569 	-5.27853909851396796 	
+-2.51225078969575177 	-3.22314543004070897 	
+-8.82457968581596752 	-1.42245769348021844 	
+-5.4971248965487387 	-14.759995096029046 	
+8.47592427242844337 	8.11107085181370557 	
+2.34953305575392202 	4.63659562958691041 	
+-1.64301101094937252 	11.4873476009575572 	
+0.580660833465362791 	13.1755650470871277 	
+-0.650250885491696851 	1.90203710703855133 	
+-3.87917543422287148 	5.03518774179589901 	
+1.7039737911280386 	1.34775554555850574 	
+-2.78522303432125629 	-64.1292521131980919 	
+2.32700743878955185 	24.743612559642397 	
+-5.39566293426395394 	-9.82739376173094037 	
+12.9203582472649128 	5.29957416698956862 	
+-10.7455769141728563 	89.9090483177386801 	
+4.31713435895606157 	-5.18586285077431786 	
+0.848047232584521082 	-2.17948525293295869 	
+-9.4681032062965258 	-0.22074895143857387 	
+7.21414655874796917 	7.46866878382939792 	
+-1.75858145601429361 	11.2871954305960074 	
+1.96560792644405802 	-6.52458829385219907 	
+-2.79380808107006473 	-5.09955504189952258 	
+-0.799059310236704512 	-12.9255049414569196 	
+-4.25437046158016585 	8.96615214816819339 	
+-9.43622335236651288 	3.78339978190638559 	
+4.36697916816686238 	-0.687503879096243442 	
+-2.18796130106503472 	-6.69802616230832459 	
+3.17982632559527989 	-1.43703160328240198 	
+-4.3141453664152607 	-5.51667561668627471 	
+-2.74519292766851297 	7.73168925236620286 	
+1.05378206408368125 	0.178110338223795794 	
+0.460217896787359693 	10.0585449039864159 	
+-0.0106331952235446964 	-19.6790725953361481 	
+0.424465188958869921 	-15.8551532141411631 	
+0.567218573767627854 	-2.6951904195179508 	
+4.45718670597547018 	5.62393864872558336 	
+2.19961229165357031 	-5.42279390073859879 	
+-0.228762156049838294 	-0.831542078930034179 	
+-2.4010318622261817 	15.8373411775190593 	
+-1.34006398346898026 	-12.0359127014414575 	
+4.0381021751345596 	-2.91524838149849552 	
+0.593248196395251171 	0.218503040318817232 	
 -0.894620455881181775 	1.01068418956454376 	
--0.905934974062951315 	-1.04508844319939009 	
-8.00428356105542704 	-1.26168634488815745 	
-0.619982738889116813 	54.1218393092184655 	
--0.705573849337669534 	-1.25381377885319911 	
-0.871066182005948986 	40.4927882352012602 	
--7.05074671248575058 	71.3212740911221061 	
--2.22324163214598158 	1.07345612751800834 	
--2.44557668836901909 	4.29373139551074523 	
-9.8216236808294628 	6.64093872401048912 	
--9.18335934248439933 	-3.97745257894915438 	
--0.836507465302499531 	-1.03211418641978869 	
--0.053497546963581534 	19.5696227138299683 	
-1.07568581176873801 	-5.10104965788098763 	
--0.51895274081205478 	15.8636310713846687 	
--2.84087926537020463 	-12.9191491848348825 	
-2.04826664350002785 	12.2408805586060261 	
-1.80375307384701089 	11.6859209915609643 	
--1.25621312029488119 	-2.02266034407121387 	
--0.0484853946924818577 	3.80498181684614956 	
--0.677242736032365844 	-42.6254775342041086 	
-1.5916269901049056 	-5.48763586146515259 	
--1.21972683978988194 	-1.50185397433043755 	
-1.24858166506403889 	4.87092530126864176 	
--7.04338199489057715 	17.3390887919734524 	
-3.47070361924793991 	1.42970456326488016 	
--4.26550633389315159 	-1.73980331015090517 	
-2.12547785989944504 	3.62923159122205607 	
-2.5325993386476946 	-38.0016204477384605 	
-2.21881274683430751 	-0.0138947589494949966 	
-3.14760002032383079 	-1.50792558449675762 	
-1.08730538645968133 	-5.46742192799871596 	
-0.805042144582074792 	-53.0054722631666593 	
--1.33448103050452405 	5.00293548641393393 	
--4.27305110186876824 	-0.418543428368392034 	
-1.70132006444656469 	-5.38749080327059016 	
--1.12532164909935517 	33.3451642986786609 	
--2.70415771236861247 	3.89572987273113291 	
-4.98777580381695707 	13.8821103996001245 	
-0.568511986632529109 	-6.88339363195469822 	
-2.78082104553195331 	-5.19833751432510915 	
--3.45195094389145929 	6.54989673235798353 	
--1.26923932125293937 	-82.4386151296812386 	
-6.7221655120617827 	-5.26765661015447861 	
--0.742712929107001374 	-2.30022073912255554 	
-1.55989131761590083 	-4.18345207234387129 	
-4.72158352753704147 	21.7128846359820926 	
--2.74322426786354923 	0.0600870120432775226 	
-5.66665271018074534 	-3.34386936425682135 	
--1.7405916077807202 	11.7068252411723854 	
-0.339841379787169384 	1.53151755701152847 	
--0.136156736944080564 	-12.0671867463559686 	
-2.99636501987962234 	1.68582324362498048 	
--1.96646003943842773 	-7.02906457262245166 	
--2.68253702359453339 	19.2149219172491286 	
-3.77166285715723992 	-15.0634217849763132 	
-1.10118597277121655 	11.2002484600206493 	
--0.21265096744765738 	0.824744814217108657 	
-3.73652792828045532 	-7.81218626521123305 	
--0.1517753718816853 	1.00699915467459311 	
-3.72484877108446399 	5.92264185066637783 	
--0.0755790292585322709 	-3.91733851844826209 	
--1.69431689893366721 	3.02479371219842008 	
-0.0753423487892740662 	14.9935199826461982 	
-2.23109480398673998 	-2.59140850098083764 	
-6.91777882726973914 	41.5668021912243546 	
-1.87173910300826329 	-16.2617861471825336 	
--2.61089444121170011 	-39.3944996228537363 	
+-0.905934974062951648 	-1.04508844319938632 	
+8.00428356105546257 	-1.26168634488796494 	
+0.619982738884564899 	54.1218393093347103 	
+-0.705573849334636627 	-1.2538137788610173 	
+0.871066182002223743 	40.4927882352060706 	
+-7.05074671248599127 	71.3212740911236835 	
+-2.22324163215964843 	1.0734561275325698 	
+-2.44557668837193853 	4.2937313954156906 	
+9.82162368083046999 	6.64093872400913376 	
+-9.18335934249402008 	-3.97745257891762716 	
+-0.836507465302499753 	-1.0321141864197878 	
+-0.0534975469631229703 	19.5696227138288812 	
+1.07568581177046618 	-5.10104965789005504 	
+-0.518952740814859759 	15.8636310713528932 	
+-2.84087926537195301 	-12.9191491848439277 	
+2.04826664350717103 	12.2408805584981177 	
+1.80375307384732975 	11.6859209915770084 	
+-1.25621312028974019 	-2.02266034406559303 	
+-0.0484853946913663472 	3.8049818168441556 	
+-0.677242736030050585 	-42.625477534183851 	
+1.59162699009028663 	-5.48763586147435767 	
+-1.21972683978848395 	-1.50185397433300194 	
+1.24858166506431711 	4.87092530127280199 	
+-7.04338199489003269 	17.3390887919741523 	
+3.47070361924823967 	1.42970456326448381 	
+-4.2655063338911301 	-1.73980331015853862 	
+2.12547785989874116 	3.62923159123009187 	
+2.53259933864800901 	-38.0016204477437114 	
+2.21881274683373908 	-0.0138947589423880184 	
+3.1476000203292771 	-1.50792558450150538 	
+1.08730538646035324 	-5.46742192799836069 	
+0.805042144582562846 	-53.0054722631629431 	
+-1.33448103050428291 	5.00293548641483543 	
+-4.27305110186990333 	-0.418543428366281389 	
+1.70132006444962425 	-5.38749080326675145 	
+-1.12532164909613996 	33.3451642986689407 	
+-2.70415771239980796 	3.8957298727264229 	
+4.98777580381757346 	13.8821103996191333 	
+0.568511986630580113 	-6.88339363194706877 	
+2.78082104553625609 	-5.19833751433356372 	
+-3.45195094389070523 	6.54989673235655356 	
+-1.26923932124764582 	-82.4386151297649405 	
+6.72216551206190616 	-5.26765661015598763 	
+-0.742712929107524844 	-2.30022073912311376 	
+1.55989131761279332 	-4.18345207235786098 	
+4.72158352754091215 	21.7128846360060805 	
+-2.74322426786452844 	0.0600870120549067896 	
+5.66665271018079864 	-3.34386936425720371 	
+-1.7405916077822694 	11.7068252411821998 	
+0.339841379787524656 	1.53151755701120851 	
+-0.136156736953116003 	-12.0671867463523572 	
+2.99636501990413606 	1.68582324363194558 	
+-1.96646003944210279 	-7.02906457252052252 	
+-2.68253702359536872 	19.2149219172472989 	
+3.77166285715782834 	-15.0634217849804468 	
+1.10118597276613972 	11.2002484600089556 	
+-0.212650967437634342 	0.824744814209136368 	
+3.73652792828373403 	-7.81218626519529735 	
+-0.151775371881687632 	1.00699915467459045 	
+3.72484877108447021 	5.92264185066602611 	
+-0.0755790292600937996 	-3.91733851844509173 	
+-1.69431689893300641 	3.02479371219719839 	
+0.0753423487888285753 	14.9935199826456476 	
+2.23109480398829207 	-2.59140850098041486 	
+6.91777882726934212 	41.5668021912209156 	
+1.87173910301644164 	-16.2617861474499854 	
+-2.610894441212122 	-39.3944996228497075 	
 -0.836329838029520878 	0.975444302819992171 	
-3.00474481016317796 	-21.1150530969481025 	
--8.62460330276778109 	3.04533586235480191 	
--11.9994315455979592 	-1.47919451474482977 	
-4.44069944340338374 	-18.2589018941275931 	
--2.27968651835906844 	1.02704086191163668 	
-6.31981383647014194 	1.54445289435077759 	
--5.26600895531925595 	-35.7977533157710113 	
--0.481694055896209672 	2.21195940922255518 	
-7.77036312147073716 	1.09313418809460505 	
--4.68097213985389882 	-6.20894287206248841 	
--1.0971415680584542 	-2.85536759176862898 	
-0.545700238407987648 	11.8862474186525926 	
-1.48867902407183705 	-5.48427538646079693 	
--6.1558764315134562 	6.74361516831379859 	
--0.34943300218334733 	-6.2328287440142951 	
--2.64624601969912154 	3.41977326768898227 	
--2.94286030596212944 	-2.75364076618103537 	
-5.73828756539082274 	-5.57310817627025834 	
--4.97225940962958202 	3.65509474643111654 	
--6.10148670149821548 	-3.5099263102813012 	
-1.55044181583261187 	14.6775034364680952 	
-0.782385337009910753 	25.481371320069055 	
--1.53273115443696883 	4.58635057373261112 	
-10.4172385843581061 	6.86633339432157452 	
--0.368394943949268971 	20.6264920696424987 	
-5.60810932982655341 	6.78678373501238852 	
-6.71791590246840631 	-5.80482782599815295 	
-0.388121780022244123 	-3.46219858678257397 	
-2.83447313289494707 	7.76167610817524611 	
--1.54630702794617703 	0.277670233006282186 	
-8.9837147693589845 	-0.793959411589270636 	
-2.70994943537632693 	14.8040061566573282 	
-6.68573003919279518 	0.0462762095885054375 	
--4.29261320816020042 	-2.95101472906158513 	
--4.54151110825627313 	16.1599843952082907 	
--5.15501226513589383 	6.64257668637242293 	
-1.90733106596579205 	2.8501923085260672 	
-1.43237792951475584 	11.0408267483489499 	
--1.36458349374178489 	-1.30153240656658253 	
--4.30599353659890038 	-10.9529742342083214 	
-0.0316284965238302559 	1.25596215070410944 	
--2.60581041885303533 	-6.14526724575781902 	
--0.312851078671457561 	4.8899176585817985 	
-5.29462188951335122 	-16.5770775166756401 	
--4.00703215573606908 	2.75362305989316081 	
-8.7281729037734479 	9.97806304735961547 	
-0.312638773494846978 	-2.14323866166241261 	
--1.81776211066742133 	-44.0518064486871594 	
-2.55609366479101618 	-4.74886921353842606 	
--0.833149682703483663 	11.5472059416640835 	
--1.51305776067499043 	-109.480058674132408 	
-0.96348561462821225 	-6.81413026705121183 	
--8.58496213584385792 	13.0940886747191065 	
-0.0915420976503914519 	5.86702156901551231 	
-4.79191376944235348 	-6.83937697733123517 	
-0.534268386340944978 	-12.4849809673482426 	
-2.17337508306810578 	-14.4118021194967199 	
-0.140164109164443162 	62.1647781456503097 	
-0.167570766919056896 	-24.6465822115656401 	
-1.23667489086769566 	1.84320815143338623 	
--0.440814885867638395 	-1.55400780863914356 	
-3.86906628870721292 	5.74016181018787464 	
--4.46332178439648342 	14.5109110420769749 	
-0.978359207199548941 	-0.176691538654043939 	
--0.253112600092994244 	11.6452773485220398 	
-9.90282705194954538 	7.59072425472390933 	
--1.48088213993055762 	-12.6227827040919376 	
--2.07209465738824505 	6.0289852325298714 	
-0.0851342612640778873 	65.9186373049947889 	
--4.36632696181617952 	-11.3811871241025226 	
--1.71368855874052861 	1.31764702088757302 	
-2.15686598960512166 	18.4587976762973902 	
-0.511354889660306289 	-43.8065302486330737 	
-3.79985826164594931 	2.1930211957444623 	
-2.2000165454424403 	-4.52230342001110674 	
--3.10342048881031829 	1.69054390982302927 	
-2.49515664287580075 	-12.6407876685394616 	
--2.05596705724929718 	-15.4018265269691028 	
-2.4535740130399093 	-17.9766881878857845 	
-14.0087664049954643 	-2.1415029657413216 	
--0.458146105055854791 	-94.286637940529971 	
--7.43642773069096918 	-0.417123011311428815 	
--0.401022533507097045 	12.8005812092002085 	
--0.954443961086675396 	-18.2505062496947659 	
--3.53127796312802422 	1.20073147588600682 	
--4.42051731554893923 	-59.221195467202314 	
-1.77815493364989341 	47.1410132671640412 	
--7.07469141009799074 	-2.82886733985023353 	
+3.00474481016258865 	-21.1150530969548456 	
+-8.6246033027724156 	3.04533586233992315 	
+-11.9994315456001406 	-1.47919451474942232 	
+4.44069944340081424 	-18.2589018941206973 	
+-2.27968651835379399 	1.02704086194237587 	
+6.31981383646970141 	1.54445289434102828 	
+-5.26600895531991675 	-35.797753315782785 	
+-0.481694055895810824 	2.2119594092289705 	
+7.77036312147051333 	1.09313418809397089 	
+-4.6809721398523001 	-6.20894287205482076 	
+-1.09714156805876417 	-2.85536759175997856 	
+0.54570023840243842 	11.8862474186002061 	
+1.48867902407078145 	-5.48427538647323942 	
+-6.15587643151411612 	6.74361516830442742 	
+-0.349433002185481179 	-6.23282874401665588 	
+-2.64624601969778839 	3.41977326768260959 	
+-2.94286030596189985 	-2.7536407661812845 	
+5.73828756539099682 	-5.5731081762760617 	
+-4.97225940963330615 	3.6550947464273591 	
+-6.10148670149677752 	-3.5099263102824203 	
+1.55044181583256746 	14.6775034364667842 	
+0.782385337009624093 	25.4813713200397167 	
+-1.53273115443669905 	4.58635057373372401 	
+10.4172385843634707 	6.8663333943362943 	
+-0.368394943953074205 	20.6264920696709204 	
+5.60810932980211341 	6.78678373498030751 	
+6.71791590246602066 	-5.80482782600071445 	
+0.388121780023255425 	-3.46219858678926329 	
+2.83447313288851621 	7.76167610821283915 	
+-1.54630702793932451 	0.277670233017959622 	
+8.98371476935886726 	-0.793959411588884501 	
+2.70994943537675281 	14.8040061565928838 	
+6.68573003918294617 	0.0462762095938224136 	
+-4.29261320816485004 	-2.95101472907809592 	
+-4.54151110825021309 	16.1599843952047344 	
+-5.15501226513595601 	6.64257668637025311 	
+1.9073310659665581 	2.85019230852685856 	
+1.43237792951583942 	11.0408267483431235 	
+-1.36458349374152954 	-1.30153240656581781 	
+-4.30599353659678563 	-10.9529742342064758 	
+0.0316284965242700083 	1.25596215072053163 	
+-2.60581041885561548 	-6.14526724575121097 	
+-0.312851078676800676 	4.88991765871333239 	
+5.29462188951449519 	-16.5770775166692452 	
+-4.00703215573595894 	2.75362305989386025 	
+8.72817290377258459 	9.97806304735738259 	
+0.312638773499881673 	-2.14323866156373199 	
+-1.81776211067225546 	-44.0518064486709733 	
+2.55609366479082256 	-4.74886921353495595 	
+-0.833149682701808447 	11.5472059416397173 	
+-1.51305776067360043 	-109.480058674138334 	
+0.963485614629060239 	-6.81413026706003766 	
+-8.58496213584483847 	13.0940886747311964 	
+0.0915420976403832215 	5.86702156899371197 	
+4.79191376947731396 	-6.83937697735480477 	
+0.534268386342521273 	-12.4849809673474379 	
+2.17337508306685523 	-14.4118021194800612 	
+0.140164109164259837 	62.16477814568497 	
+0.167570766918804098 	-24.6465822114029365 	
+1.23667489086611737 	1.84320815139914407 	
+-0.440814885865148109 	-1.5540078086496576 	
+3.86906628870645619 	5.74016181018549343 	
+-4.46332178439692751 	14.5109110420846736 	
+0.978359207200913406 	-0.176691538655378344 	
+-0.253112600078602312 	11.6452773486085555 	
+9.90282705194929491 	7.59072425472290746 	
+-1.48088213993077722 	-12.6227827040926677 	
+-2.07209465738966392 	6.02898523253392682 	
+0.0851342612633791129 	65.918637305018251 	
+-4.36632696181662183 	-11.3811871240997462 	
+-1.71368855874337322 	1.31764702087338548 	
+2.15686598960616616 	18.4587976763140915 	
+0.511354889667731349 	-43.8065302488140631 	
+3.7998582616395109 	2.19302119575025722 	
+2.20001654545043479 	-4.52230341999792085 	
+-3.10342048881173493 	1.69054390982317559 	
+2.49515664288340444 	-12.6407876685416056 	
+-2.05596705726674811 	-15.4018265269667936 	
+2.45357401304169231 	-17.9766881878872269 	
+14.0087664049932883 	-2.14150296573378363 	
+-0.458146105055885766 	-94.2866379405297295 	
+-7.43642773069070628 	-0.417123011304259772 	
+-0.401022533514366897 	12.800581209304859 	
+-0.954443961086527848 	-18.250506249697839 	
+-3.53127796312637265 	1.20073147588584428 	
+-4.42051731555071115 	-59.221195467169288 	
+1.77815493365281663 	47.1410132671280238 	
+-7.07469141008806623 	-2.82886733983377647 	
 ]
 ;
 training_inputs = *2 ->MemoryVMatrix(
@@ -479,7 +479,8 @@
 targetsize = 0 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = ""  )
+metadatadir = "" ;
+fieldinfos = 0 [ ]  )
 ;
 random_gen = *0 ;
 seed = 1827 ;

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/final_learner.psave	2008-07-10 23:00:51 UTC (rev 9232)
@@ -241,7 +241,8 @@
 targetsize = 2 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = ""  )
+metadatadir = "" ;
+fieldinfos = 3 [ "x" 0 "y1" 0 "y2" 0 ]  )
 ;
 splitter = *0 ;
 statnames = 2 [ "E[test.E[mse]]" "E[train.E[mse]]" ] ;
@@ -261,231 +262,231 @@
 weight_decay = 0.0100000000000000002 ;
 include_bias = 1 ;
 params = 225  2  [ 
-2.71513510622207166 	6.96796926050934751 	
--0.924570620987045566 	-5.50999860198858826 	
-2.74766209737875133 	16.2764471019206418 	
--0.897331116697557052 	24.1696624838867358 	
--0.257145462011077963 	-2.68785159119476535 	
--1.3282966539726655 	-18.5932956256296649 	
--5.49258347509019718 	-1.57002604298704584 	
-1.46070534954897835 	-3.53276434233635506 	
--1.038126171674709 	-1.2073115958693148 	
-1.02580172581497386 	-26.7245637328128929 	
-0.791254761045932176 	82.4469215737125865 	
-0.836570941006688051 	-16.4202630459276833 	
--1.58240826854525074 	-6.65940808938515527 	
--8.24967860415612364 	-3.49957089648365738 	
-0.9782411438628974 	-4.92116475768973771 	
-0.0717117261764370351 	-3.09067303050886988 	
--2.58531178640483761 	12.035458429525173 	
-3.211076444250756 	5.97223155883393897 	
-1.43705644733268212 	94.2974631733484614 	
-0.651016019176285488 	1.95403761231488304 	
--1.20978628334898786 	-7.36323983157156103 	
--0.676733526068380775 	-31.4356962338337986 	
--3.71894302925004316 	-5.30412867324711534 	
--2.6818491206821955 	-0.224631918494603705 	
--1.88827683784314537 	-11.2808510609389074 	
-1.56552507696732834 	-5.27853909851336311 	
--2.51225078969590498 	-3.22314543003026799 	
--8.82457968582814622 	-1.42245769348111262 	
--5.49712489654685843 	-14.7599950960045039 	
-8.47592427242672741 	8.11107085181684795 	
-2.34953305575425997 	4.63659562959487381 	
--1.64301101095052804 	11.4873476009648563 	
-0.580660833463284676 	13.1755650470951462 	
--0.650250885492095976 	1.90203710703754014 	
--3.87917543422272804 	5.03518774179621609 	
-1.70397379112897029 	1.34775554557823152 	
--2.78522303432040363 	-64.1292521132013178 	
-2.32700743878965444 	24.7436125596276284 	
--5.39566293426405519 	-9.82739376173207368 	
-12.9203582472648133 	5.29957416697557626 	
--10.7455769141731885 	89.9090483177333653 	
-4.31713435896750308 	-5.18586285076809528 	
-0.848047232583675648 	-2.17948525292697681 	
--9.46810320629667324 	-0.220748951447968828 	
-7.21414655874827737 	7.46866878382293553 	
--1.75858145600769644 	11.2871954305640294 	
-1.96560792644487359 	-6.52458829385446837 	
--2.79380808107613055 	-5.09955504191000042 	
--0.79905931023683674 	-12.925504941414971 	
--4.25437046157972976 	8.96615214816773154 	
--9.43622335236653065 	3.78339978190649218 	
-4.36697916817812093 	-0.68750387907888455 	
--2.18796130106569242 	-6.69802616230909642 	
-3.17982632559557965 	-1.4370316032819157 	
--4.31414536641474289 	-5.51667561668654649 	
--2.74519292766675793 	7.73168925235037285 	
-1.0537820640848472 	0.178110338215527242 	
-0.460217896787073477 	10.058544903984771 	
--0.0106331952212118993 	-19.6790725953963772 	
-0.424465188958454087 	-15.8551532141373048 	
-0.567218573772457435 	-2.69519041953220029 	
-4.45718670597664524 	5.62393864871897797 	
-2.19961229164894245 	-5.42279390073130685 	
--0.228762156047200543 	-0.831542078921280403 	
--2.40103186222691534 	15.8373411775187218 	
--1.34006398347233469 	-12.0359127014981127 	
-4.03810217513344138 	-2.91524838149503962 	
-0.593248196381594317 	0.218503040354157629 	
+2.7151351062236615 	6.96796926051022236 	
+-0.924570620989403458 	-5.50999860198525226 	
+2.74766209737707889 	16.2764471019291541 	
+-0.897331116702694054 	24.1696624839723277 	
+-0.257145462010700432 	-2.68785159119131878 	
+-1.3282966539723533 	-18.5932956256279311 	
+-5.49258347507660183 	-1.57002604296742554 	
+1.46070534955196196 	-3.53276434234297421 	
+-1.03812617167629262 	-1.20731159585204617 	
+1.02580172582147133 	-26.7245637329914061 	
+0.791254761045375288 	82.4469215737143202 	
+0.836570941010414737 	-16.420263045944921 	
+-1.58240826855158301 	-6.65940808940268703 	
+-8.24967860415081411 	-3.4995708964716874 	
+0.978241143861720674 	-4.92116475768050154 	
+0.0717117261694705244 	-3.09067303052573861 	
+-2.58531178640539849 	12.0354584295284699 	
+3.21107644425249639 	5.97223155883275147 	
+1.43705644733271809 	94.2974631733482198 	
+0.651016019177080518 	1.95403761232638362 	
+-1.20978628334706184 	-7.36323983158574702 	
+-0.676733526068720059 	-31.4356962338422257 	
+-3.71894302925004183 	-5.30412867323935266 	
+-2.68184912068139347 	-0.224631918495068694 	
+-1.88827683783783651 	-11.2808510609680628 	
+1.56552507696869569 	-5.27853909851396796 	
+-2.51225078969575177 	-3.22314543004070897 	
+-8.82457968581596752 	-1.42245769348021844 	
+-5.4971248965487387 	-14.759995096029046 	
+8.47592427242844337 	8.11107085181370557 	
+2.34953305575392202 	4.63659562958691041 	
+-1.64301101094937252 	11.4873476009575572 	
+0.580660833465362791 	13.1755650470871277 	
+-0.650250885491696851 	1.90203710703855133 	
+-3.87917543422287148 	5.03518774179589901 	
+1.7039737911280386 	1.34775554555850574 	
+-2.78522303432125629 	-64.1292521131980919 	
+2.32700743878955185 	24.743612559642397 	
+-5.39566293426395394 	-9.82739376173094037 	
+12.9203582472649128 	5.29957416698956862 	
+-10.7455769141728563 	89.9090483177386801 	
+4.31713435895606157 	-5.18586285077431786 	
+0.848047232584521082 	-2.17948525293295869 	
+-9.4681032062965258 	-0.22074895143857387 	
+7.21414655874796917 	7.46866878382939792 	
+-1.75858145601429361 	11.2871954305960074 	
+1.96560792644405802 	-6.52458829385219907 	
+-2.79380808107006473 	-5.09955504189952258 	
+-0.799059310236704512 	-12.9255049414569196 	
+-4.25437046158016585 	8.96615214816819339 	
+-9.43622335236651288 	3.78339978190638559 	
+4.36697916816686238 	-0.687503879096243442 	
+-2.18796130106503472 	-6.69802616230832459 	
+3.17982632559527989 	-1.43703160328240198 	
+-4.3141453664152607 	-5.51667561668627471 	
+-2.74519292766851297 	7.73168925236620286 	
+1.05378206408368125 	0.178110338223795794 	
+0.460217896787359693 	10.0585449039864159 	
+-0.0106331952235446964 	-19.6790725953361481 	
+0.424465188958869921 	-15.8551532141411631 	
+0.567218573767627854 	-2.6951904195179508 	
+4.45718670597547018 	5.62393864872558336 	
+2.19961229165357031 	-5.42279390073859879 	
+-0.228762156049838294 	-0.831542078930034179 	
+-2.4010318622261817 	15.8373411775190593 	
+-1.34006398346898026 	-12.0359127014414575 	
+4.0381021751345596 	-2.91524838149849552 	
+0.593248196395251171 	0.218503040318817232 	
 -0.894620455881181775 	1.01068418956454376 	
--0.905934974062951315 	-1.04508844319939009 	
-8.00428356105542704 	-1.26168634488815745 	
-0.619982738889116813 	54.1218393092184655 	
--0.705573849337669534 	-1.25381377885319911 	
-0.871066182005948986 	40.4927882352012602 	
--7.05074671248575058 	71.3212740911221061 	
--2.22324163214598158 	1.07345612751800834 	
--2.44557668836901909 	4.29373139551074523 	
-9.8216236808294628 	6.64093872401048912 	
--9.18335934248439933 	-3.97745257894915438 	
--0.836507465302499531 	-1.03211418641978869 	
--0.053497546963581534 	19.5696227138299683 	
-1.07568581176873801 	-5.10104965788098763 	
--0.51895274081205478 	15.8636310713846687 	
--2.84087926537020463 	-12.9191491848348825 	
-2.04826664350002785 	12.2408805586060261 	
-1.80375307384701089 	11.6859209915609643 	
--1.25621312029488119 	-2.02266034407121387 	
--0.0484853946924818577 	3.80498181684614956 	
--0.677242736032365844 	-42.6254775342041086 	
-1.5916269901049056 	-5.48763586146515259 	
--1.21972683978988194 	-1.50185397433043755 	
-1.24858166506403889 	4.87092530126864176 	
--7.04338199489057715 	17.3390887919734524 	
-3.47070361924793991 	1.42970456326488016 	
--4.26550633389315159 	-1.73980331015090517 	
-2.12547785989944504 	3.62923159122205607 	
-2.5325993386476946 	-38.0016204477384605 	
-2.21881274683430751 	-0.0138947589494949966 	
-3.14760002032383079 	-1.50792558449675762 	
-1.08730538645968133 	-5.46742192799871596 	
-0.805042144582074792 	-53.0054722631666593 	
--1.33448103050452405 	5.00293548641393393 	
--4.27305110186876824 	-0.418543428368392034 	
-1.70132006444656469 	-5.38749080327059016 	
--1.12532164909935517 	33.3451642986786609 	
--2.70415771236861247 	3.89572987273113291 	
-4.98777580381695707 	13.8821103996001245 	
-0.568511986632529109 	-6.88339363195469822 	
-2.78082104553195331 	-5.19833751432510915 	
--3.45195094389145929 	6.54989673235798353 	
--1.26923932125293937 	-82.4386151296812386 	
-6.7221655120617827 	-5.26765661015447861 	
--0.742712929107001374 	-2.30022073912255554 	
-1.55989131761590083 	-4.18345207234387129 	
-4.72158352753704147 	21.7128846359820926 	
--2.74322426786354923 	0.0600870120432775226 	
-5.66665271018074534 	-3.34386936425682135 	
--1.7405916077807202 	11.7068252411723854 	
-0.339841379787169384 	1.53151755701152847 	
--0.136156736944080564 	-12.0671867463559686 	
-2.99636501987962234 	1.68582324362498048 	
--1.96646003943842773 	-7.02906457262245166 	
--2.68253702359453339 	19.2149219172491286 	
-3.77166285715723992 	-15.0634217849763132 	
-1.10118597277121655 	11.2002484600206493 	
--0.21265096744765738 	0.824744814217108657 	
-3.73652792828045532 	-7.81218626521123305 	
--0.1517753718816853 	1.00699915467459311 	
-3.72484877108446399 	5.92264185066637783 	
--0.0755790292585322709 	-3.91733851844826209 	
--1.69431689893366721 	3.02479371219842008 	
-0.0753423487892740662 	14.9935199826461982 	
-2.23109480398673998 	-2.59140850098083764 	
-6.91777882726973914 	41.5668021912243546 	
-1.87173910300826329 	-16.2617861471825336 	
--2.61089444121170011 	-39.3944996228537363 	
+-0.905934974062951648 	-1.04508844319938632 	
+8.00428356105546257 	-1.26168634488796494 	
+0.619982738884564899 	54.1218393093347103 	
+-0.705573849334636627 	-1.2538137788610173 	
+0.871066182002223743 	40.4927882352060706 	
+-7.05074671248599127 	71.3212740911236835 	
+-2.22324163215964843 	1.0734561275325698 	
+-2.44557668837193853 	4.2937313954156906 	
+9.82162368083046999 	6.64093872400913376 	
+-9.18335934249402008 	-3.97745257891762716 	
+-0.836507465302499753 	-1.0321141864197878 	
+-0.0534975469631229703 	19.5696227138288812 	
+1.07568581177046618 	-5.10104965789005504 	
+-0.518952740814859759 	15.8636310713528932 	
+-2.84087926537195301 	-12.9191491848439277 	
+2.04826664350717103 	12.2408805584981177 	
+1.80375307384732975 	11.6859209915770084 	
+-1.25621312028974019 	-2.02266034406559303 	
+-0.0484853946913663472 	3.8049818168441556 	
+-0.677242736030050585 	-42.625477534183851 	
+1.59162699009028663 	-5.48763586147435767 	
+-1.21972683978848395 	-1.50185397433300194 	
+1.24858166506431711 	4.87092530127280199 	
+-7.04338199489003269 	17.3390887919741523 	
+3.47070361924823967 	1.42970456326448381 	
+-4.2655063338911301 	-1.73980331015853862 	
+2.12547785989874116 	3.62923159123009187 	
+2.53259933864800901 	-38.0016204477437114 	
+2.21881274683373908 	-0.0138947589423880184 	
+3.1476000203292771 	-1.50792558450150538 	
+1.08730538646035324 	-5.46742192799836069 	
+0.805042144582562846 	-53.0054722631629431 	
+-1.33448103050428291 	5.00293548641483543 	
+-4.27305110186990333 	-0.418543428366281389 	
+1.70132006444962425 	-5.38749080326675145 	
+-1.12532164909613996 	33.3451642986689407 	
+-2.70415771239980796 	3.8957298727264229 	
+4.98777580381757346 	13.8821103996191333 	
+0.568511986630580113 	-6.88339363194706877 	
+2.78082104553625609 	-5.19833751433356372 	
+-3.45195094389070523 	6.54989673235655356 	
+-1.26923932124764582 	-82.4386151297649405 	
+6.72216551206190616 	-5.26765661015598763 	
+-0.742712929107524844 	-2.30022073912311376 	
+1.55989131761279332 	-4.18345207235786098 	
+4.72158352754091215 	21.7128846360060805 	
+-2.74322426786452844 	0.0600870120549067896 	
+5.66665271018079864 	-3.34386936425720371 	
+-1.7405916077822694 	11.7068252411821998 	
+0.339841379787524656 	1.53151755701120851 	
+-0.136156736953116003 	-12.0671867463523572 	
+2.99636501990413606 	1.68582324363194558 	
+-1.96646003944210279 	-7.02906457252052252 	
+-2.68253702359536872 	19.2149219172472989 	
+3.77166285715782834 	-15.0634217849804468 	
+1.10118597276613972 	11.2002484600089556 	
+-0.212650967437634342 	0.824744814209136368 	
+3.73652792828373403 	-7.81218626519529735 	
+-0.151775371881687632 	1.00699915467459045 	
+3.72484877108447021 	5.92264185066602611 	
+-0.0755790292600937996 	-3.91733851844509173 	
+-1.69431689893300641 	3.02479371219719839 	
+0.0753423487888285753 	14.9935199826456476 	
+2.23109480398829207 	-2.59140850098041486 	
+6.91777882726934212 	41.5668021912209156 	
+1.87173910301644164 	-16.2617861474499854 	
+-2.610894441212122 	-39.3944996228497075 	
 -0.836329838029520878 	0.975444302819992171 	
-3.00474481016317796 	-21.1150530969481025 	
--8.62460330276778109 	3.04533586235480191 	
--11.9994315455979592 	-1.47919451474482977 	
-4.44069944340338374 	-18.2589018941275931 	
--2.27968651835906844 	1.02704086191163668 	
-6.31981383647014194 	1.54445289435077759 	
--5.26600895531925595 	-35.7977533157710113 	
--0.481694055896209672 	2.21195940922255518 	
-7.77036312147073716 	1.09313418809460505 	
--4.68097213985389882 	-6.20894287206248841 	
--1.0971415680584542 	-2.85536759176862898 	
-0.545700238407987648 	11.8862474186525926 	
-1.48867902407183705 	-5.48427538646079693 	
--6.1558764315134562 	6.74361516831379859 	
--0.34943300218334733 	-6.2328287440142951 	
--2.64624601969912154 	3.41977326768898227 	
--2.94286030596212944 	-2.75364076618103537 	
-5.73828756539082274 	-5.57310817627025834 	
--4.97225940962958202 	3.65509474643111654 	
--6.10148670149821548 	-3.5099263102813012 	
-1.55044181583261187 	14.6775034364680952 	
-0.782385337009910753 	25.481371320069055 	
--1.53273115443696883 	4.58635057373261112 	
-10.4172385843581061 	6.86633339432157452 	
--0.368394943949268971 	20.6264920696424987 	
-5.60810932982655341 	6.78678373501238852 	
-6.71791590246840631 	-5.80482782599815295 	
-0.388121780022244123 	-3.46219858678257397 	
-2.83447313289494707 	7.76167610817524611 	
--1.54630702794617703 	0.277670233006282186 	
-8.9837147693589845 	-0.793959411589270636 	
-2.70994943537632693 	14.8040061566573282 	
-6.68573003919279518 	0.0462762095885054375 	
--4.29261320816020042 	-2.95101472906158513 	
--4.54151110825627313 	16.1599843952082907 	
--5.15501226513589383 	6.64257668637242293 	
-1.90733106596579205 	2.8501923085260672 	
-1.43237792951475584 	11.0408267483489499 	
--1.36458349374178489 	-1.30153240656658253 	
--4.30599353659890038 	-10.9529742342083214 	
-0.0316284965238302559 	1.25596215070410944 	
--2.60581041885303533 	-6.14526724575781902 	
--0.312851078671457561 	4.8899176585817985 	
-5.29462188951335122 	-16.5770775166756401 	
--4.00703215573606908 	2.75362305989316081 	
-8.7281729037734479 	9.97806304735961547 	
-0.312638773494846978 	-2.14323866166241261 	
--1.81776211066742133 	-44.0518064486871594 	
-2.55609366479101618 	-4.74886921353842606 	
--0.833149682703483663 	11.5472059416640835 	
--1.51305776067499043 	-109.480058674132408 	
-0.96348561462821225 	-6.81413026705121183 	
--8.58496213584385792 	13.0940886747191065 	
-0.0915420976503914519 	5.86702156901551231 	
-4.79191376944235348 	-6.83937697733123517 	
-0.534268386340944978 	-12.4849809673482426 	
-2.17337508306810578 	-14.4118021194967199 	
-0.140164109164443162 	62.1647781456503097 	
-0.167570766919056896 	-24.6465822115656401 	
-1.23667489086769566 	1.84320815143338623 	
--0.440814885867638395 	-1.55400780863914356 	
-3.86906628870721292 	5.74016181018787464 	
--4.46332178439648342 	14.5109110420769749 	
-0.978359207199548941 	-0.176691538654043939 	
--0.253112600092994244 	11.6452773485220398 	
-9.90282705194954538 	7.59072425472390933 	
--1.48088213993055762 	-12.6227827040919376 	
--2.07209465738824505 	6.0289852325298714 	
-0.0851342612640778873 	65.9186373049947889 	
--4.36632696181617952 	-11.3811871241025226 	
--1.71368855874052861 	1.31764702088757302 	
-2.15686598960512166 	18.4587976762973902 	
-0.511354889660306289 	-43.8065302486330737 	
-3.79985826164594931 	2.1930211957444623 	
-2.2000165454424403 	-4.52230342001110674 	
--3.10342048881031829 	1.69054390982302927 	
-2.49515664287580075 	-12.6407876685394616 	
--2.05596705724929718 	-15.4018265269691028 	
-2.4535740130399093 	-17.9766881878857845 	
-14.0087664049954643 	-2.1415029657413216 	
--0.458146105055854791 	-94.286637940529971 	
--7.43642773069096918 	-0.417123011311428815 	
--0.401022533507097045 	12.8005812092002085 	
--0.954443961086675396 	-18.2505062496947659 	
--3.53127796312802422 	1.20073147588600682 	
--4.42051731554893923 	-59.221195467202314 	
-1.77815493364989341 	47.1410132671640412 	
--7.07469141009799074 	-2.82886733985023353 	
+3.00474481016258865 	-21.1150530969548456 	
+-8.6246033027724156 	3.04533586233992315 	
+-11.9994315456001406 	-1.47919451474942232 	
+4.44069944340081424 	-18.2589018941206973 	
+-2.27968651835379399 	1.02704086194237587 	
+6.31981383646970141 	1.54445289434102828 	
+-5.26600895531991675 	-35.797753315782785 	
+-0.481694055895810824 	2.2119594092289705 	
+7.77036312147051333 	1.09313418809397089 	
+-4.6809721398523001 	-6.20894287205482076 	
+-1.09714156805876417 	-2.85536759175997856 	
+0.54570023840243842 	11.8862474186002061 	
+1.48867902407078145 	-5.48427538647323942 	
+-6.15587643151411612 	6.74361516830442742 	
+-0.349433002185481179 	-6.23282874401665588 	
+-2.64624601969778839 	3.41977326768260959 	
+-2.94286030596189985 	-2.7536407661812845 	
+5.73828756539099682 	-5.5731081762760617 	
+-4.97225940963330615 	3.6550947464273591 	
+-6.10148670149677752 	-3.5099263102824203 	
+1.55044181583256746 	14.6775034364667842 	
+0.782385337009624093 	25.4813713200397167 	
+-1.53273115443669905 	4.58635057373372401 	
+10.4172385843634707 	6.8663333943362943 	
+-0.368394943953074205 	20.6264920696709204 	
+5.60810932980211341 	6.78678373498030751 	
+6.71791590246602066 	-5.80482782600071445 	
+0.388121780023255425 	-3.46219858678926329 	
+2.83447313288851621 	7.76167610821283915 	
+-1.54630702793932451 	0.277670233017959622 	
+8.98371476935886726 	-0.793959411588884501 	
+2.70994943537675281 	14.8040061565928838 	
+6.68573003918294617 	0.0462762095938224136 	
+-4.29261320816485004 	-2.95101472907809592 	
+-4.54151110825021309 	16.1599843952047344 	
+-5.15501226513595601 	6.64257668637025311 	
+1.9073310659665581 	2.85019230852685856 	
+1.43237792951583942 	11.0408267483431235 	
+-1.36458349374152954 	-1.30153240656581781 	
+-4.30599353659678563 	-10.9529742342064758 	
+0.0316284965242700083 	1.25596215072053163 	
+-2.60581041885561548 	-6.14526724575121097 	
+-0.312851078676800676 	4.88991765871333239 	
+5.29462188951449519 	-16.5770775166692452 	
+-4.00703215573595894 	2.75362305989386025 	
+8.72817290377258459 	9.97806304735738259 	
+0.312638773499881673 	-2.14323866156373199 	
+-1.81776211067225546 	-44.0518064486709733 	
+2.55609366479082256 	-4.74886921353495595 	
+-0.833149682701808447 	11.5472059416397173 	
+-1.51305776067360043 	-109.480058674138334 	
+0.963485614629060239 	-6.81413026706003766 	
+-8.58496213584483847 	13.0940886747311964 	
+0.0915420976403832215 	5.86702156899371197 	
+4.79191376947731396 	-6.83937697735480477 	
+0.534268386342521273 	-12.4849809673474379 	
+2.17337508306685523 	-14.4118021194800612 	
+0.140164109164259837 	62.16477814568497 	
+0.167570766918804098 	-24.6465822114029365 	
+1.23667489086611737 	1.84320815139914407 	
+-0.440814885865148109 	-1.5540078086496576 	
+3.86906628870645619 	5.74016181018549343 	
+-4.46332178439692751 	14.5109110420846736 	
+0.978359207200913406 	-0.176691538655378344 	
+-0.253112600078602312 	11.6452773486085555 	
+9.90282705194929491 	7.59072425472290746 	
+-1.48088213993077722 	-12.6227827040926677 	
+-2.07209465738966392 	6.02898523253392682 	
+0.0851342612633791129 	65.918637305018251 	
+-4.36632696181662183 	-11.3811871240997462 	
+-1.71368855874337322 	1.31764702087338548 	
+2.15686598960616616 	18.4587976763140915 	
+0.511354889667731349 	-43.8065302488140631 	
+3.7998582616395109 	2.19302119575025722 	
+2.20001654545043479 	-4.52230341999792085 	
+-3.10342048881173493 	1.69054390982317559 	
+2.49515664288340444 	-12.6407876685416056 	
+-2.05596705726674811 	-15.4018265269667936 	
+2.45357401304169231 	-17.9766881878872269 	
+14.0087664049932883 	-2.14150296573378363 	
+-0.458146105055885766 	-94.2866379405297295 	
+-7.43642773069070628 	-0.417123011304259772 	
+-0.401022533514366897 	12.800581209304859 	
+-0.954443961086527848 	-18.250506249697839 	
+-3.53127796312637265 	1.20073147588584428 	
+-4.42051731555071115 	-59.221195467169288 	
+1.77815493365281663 	47.1410132671280238 	
+-7.07469141008806623 	-2.82886733983377647 	
 ]
 ;
 training_inputs = *6 ->MemoryVMatrix(
@@ -728,7 +729,8 @@
 targetsize = 0 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = ""  )
+metadatadir = "" ;
+fieldinfos = 0 [ ]  )
 ;
 random_gen = *0 ;
 seed = 1827 ;
@@ -775,7 +777,8 @@
 min_n_trials = 0 ;
 oracle = *8 ->CartesianProductOracle(
 option_names = 2 [ "weight_decay" "kernel.sigma" ] ;
-option_values = 2 [ 5 [ "1e-8" "1e-6" "1e-4" "1e-2" "1e0" ] 5 [ "1e-2" "1e-1" "1e0" "1e1" "1e2" ] ]  )
+option_values = 2 [ 5 [ "1e-8" "1e-6" "1e-4" "1e-2" "1e0" ] 5 [ "1e-2" "1e-1" "1e0" "1e1" "1e2" ] ] ;
+option_values_indices = 2 [ 0 0 ]  )
 ;
 provide_tester_expdir = 0 ;
 sub_strategy = []
@@ -788,10 +791,14 @@
 append_train = 0 ;
 append_non_constant_test = 0 ;
 include_test_in_train = 0 ;
-cross_range = (0 , 1 ) )
+cross_range = (0 , 1 );
+balance_classes = 0  )
 ;
-best_objective = 0.2204549456115516 ;
-best_results = 2 [ 0.2204549456115516 0.0496004907388373079 ] ;
+auto_save = 0 ;
+auto_save_diff_time = 10800 ;
+auto_save_test = 0 ;
+best_objective = 0.220454945611602143 ;
+best_results = 2 [ 0.220454945611602143 0.0496004907388580829 ] ;
 best_learner = *4  ;
 trialnum = 25 ;
 option_vals = []

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_costs.pmat	2008-07-10 23:00:51 UTC (rev 9232)
@@ -1,3 +1,2 @@
 MATRIX 75 1 DOUBLE LITTLE_ENDIAN                               
-_hXBK?????$,?l??ia?????5m? ??)=0??b???o?y[??????i???x?>V???H???????E%v????<8dl??h???o??xe?????vS???????'???f6?x??????x????????T??-j???y?z?)	2?V??
-?kb????A??^x???(R?T??oi?H??????]?\w??????pP???,)U[??)??y:???0????O?\???uQ?~?m?????:?v????????5q?{????t??pv?c?T?D?^&z?z?R?3A????y*u?up?P??P???G?A??????E??1????g??mn??tWm?Oa?(??@?Q?????G???z???K??g?Z?7???La??????[y*(Xr?A/y?IB???<?rmx??e?x'?????u9K????B{????5r??x???$i?????????????|??^s??cx?g??6y???D???????????o:??H??[???????=??f+??????Ym;???????W?????b????%/???J91? r????~cP???*?????*??!e????C???????????
\ No newline at end of file
+jhXBK???2?$,?l?D?a???G?5m? ????<0??b???o?y[??:?????i?I?x?>V????H??????E%v??eL<8dl??d???o??e???n?vS???u???'???4?x???????x????P????T??]j???y???)	2?V?)?kb?????@??^x???(R?T??l_?H???^|?]?\w??t???pP?}X,)U[?????y:???br???O?N=???uQ??m???????v??????\?5q?{?mc???t?_?v?c?T?7?^&z?z???4A????*u?up?{G??P???F?A??????E??1??`0g??mn??qWm?Oa????@?Q??W??G???x???K????Z?7???3W??????f?y*(Xr???.y?IB?s?<?rmx?J{?x'?????u9K????F{????o??x????i?????????????M?|???c??cx?z??6y??????????????o:?~?H??[??Gi???=??g6??????~m;???????W???>
\ No newline at end of file

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test1_stats.psave	2008-07-10 23:00:51 UTC (rev 9232)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 75 ;
 sumsquarew_ = 75 ;
-sum_ = 8.32876645342397737 ;
-sumsquare_ = 7.86682227539818513 ;
-sumcube_ = 10.9548142656995928 ;
-sumfourth_ = 17.491486713979409 ;
-min_ = 0.000403386661495011372 ;
-max_ = 1.79622562496960114 ;
+sum_ = 8.32876645342359012 ;
+sumsquare_ = 7.8668222754000583 ;
+sumcube_ = 10.9548142657038454 ;
+sumfourth_ = 17.4914867139879391 ;
+min_ = 0.000403386661488822746 ;
+max_ = 1.79622562497000327 ;
 agmemin_ = 13 ;
 agemax_ = 8 ;
-first_ = 0.013327139208753001 ;
-last_ = 0.0703378652397716186 ;
+first_ = 0.0133271392087507996 ;
+last_ = 0.0703378652394482801 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_costs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_outputs.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/test2_stats.psave	2008-07-10 23:00:51 UTC (rev 9232)
@@ -14,16 +14,16 @@
 nmissing_ = 0 ;
 nnonmissing_ = 300 ;
 sumsquarew_ = 300 ;
-sum_ = 19.3910908077334234 ;
-sumsquare_ = 13.9110920621933758 ;
-sumcube_ = 15.8389851220074238 ;
-sumfourth_ = 22.117074279799926 ;
-min_ = 3.99687001153368061e-05 ;
-max_ = 1.79622562496960114 ;
+sum_ = 19.3910908077377719 ;
+sumsquare_ = 13.9110920621987386 ;
+sumcube_ = 15.8389851220146944 ;
+sumfourth_ = 22.1170742798113977 ;
+min_ = 3.99687001153608957e-05 ;
+max_ = 1.79622562497000327 ;
 agmemin_ = 232 ;
 agemax_ = 8 ;
-first_ = 0.00559245542604428977 ;
-last_ = 0.0703378652397716186 ;
+first_ = 0.00559245542604580592 ;
+last_ = 0.0703378652394482801 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/Split0/train_stats.psave	2008-07-10 23:00:51 UTC (rev 9232)
@@ -39,12 +39,12 @@
 sumsquare_ = 0 ;
 sumcube_ = 0 ;
 sumfourth_ = 0 ;
-min_ = 0.0521801135176278436 ;
-max_ = 0.0521801135176278436 ;
+min_ = 0.052180113517651637 ;
+max_ = 0.052180113517651637 ;
 agmemin_ = 0 ;
 agemax_ = 0 ;
-first_ = 0.0521801135176278436 ;
-last_ = 0.0521801135176278436 ;
+first_ = 0.052180113517651637 ;
+last_ = 0.052180113517651637 ;
 binary_ = 0 ;
 integer_ = 0 ;
 counts = {};

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/global_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/metainfos.txt	2008-07-10 23:00:51 UTC (rev 9232)
@@ -1,4 +1,4 @@
-__REVISION__ = "PL8883"
+__REVISION__ = "PL9224"
 DataOpt.data                                  = PLEARNDIR:examples/data/test_suite/sin_signcos_1x_2y.amat
 HyperKRR.kfold                                = 10
 HyperKRR.lambda_list                          = 1e-8,1e-6,1e-4,1e-2,1e0

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/split_stats.pmat
===================================================================
(Binary files differ)

Modified: trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave
===================================================================
--- trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave	2008-07-10 22:39:59 UTC (rev 9231)
+++ trunk/plearn_learners/regressors/test/KernelRidgeRegressor/.pytest/PL_kernel_ridge_hyper_regressor/expected_results/expdir/tester.psave	2008-07-10 23:00:51 UTC (rev 9232)
@@ -10,7 +10,8 @@
 targetsize = 2 ;
 weightsize = 0 ;
 extrasize = 0 ;
-metadatadir = "PLEARNDIR:examples/data/test_suite/sin_signcos_1x_2y.amat.metadata/"  )
+metadatadir = "PLEARNDIR:examples/data/test_suite/sin_signcos_1x_2y.amat.metadata/" ;
+fieldinfos = 3 [ "x" 0 "y1" 0 "y2" 0 ]  )
 ;
 splitter = *2 ->FractionSplitter(
 round_to_closest = 0 ;
@@ -93,7 +94,8 @@
 min_n_trials = 0 ;
 oracle = *8 ->CartesianProductOracle(
 option_names = 2 [ "weight_decay" "kernel.sigma" ] ;
-option_values = 2 [ 5 [ "1e-8" "1e-6" "1e-4" "1e-2" "1e0" ] 5 [ "1e-2" "1e-1" "1e0" "1e1" "1e2" ] ]  )
+option_values = 2 [ 5 [ "1e-8" "1e-6" "1e-4" "1e-2" "1e0" ] 5 [ "1e-2" "1e-1" "1e0" "1e1" "1e2" ] ] ;
+option_values_indices = 2 [ 0 0 ]  )
 ;
 provide_tester_expdir = 0 ;
 sub_strategy = []
@@ -106,8 +108,12 @@
 append_train = 0 ;
 append_non_constant_test = 0 ;
 include_test_in_train = 0 ;
-cross_range = (0 , 1 ) )
+cross_range = (0 , 1 );
+balance_classes = 0  )
 ;
+auto_save = 0 ;
+auto_save_diff_time = 10800 ;
+auto_save_test = 0 ;
 best_objective = 1.79769313486231571e+308 ;
 best_results = []
 ;



From saintmlx at mail.berlios.de  Fri Jul 11 01:17:29 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Fri, 11 Jul 2008 01:17:29 +0200
Subject: [Plearn-commits] r9233 - trunk/python_modules/plearn/pybridge
Message-ID: <200807102317.m6ANHTx3004343@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-11 01:17:28 +0200 (Fri, 11 Jul 2008)
New Revision: 9233

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- minor mod.



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-07-10 23:00:51 UTC (rev 9232)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-07-10 23:17:28 UTC (rev 9233)
@@ -141,12 +141,13 @@
             d['_cptr'][2][o]= self.getOption(o)
         return d
         ##### old, deprecated version follows: (for reference only)
-        #d= self.__dict__.copy()
-        #if remote_pickle:
-        #    d['_cptr']= self.asStringRemoteTransmit()
-        #else:
-        #    d['_cptr']= self.asString()
-        #return d
+	def old_deprecated___getstate__(self):
+		d= self.__dict__.copy()
+	        if remote_pickle:
+        	    d['_cptr']= self.asStringRemoteTransmit()
+        	else:
+        	    d['_cptr']= self.asString()
+        	return d
     
     def __setstate__(self, dict):
         """



From laulysta at mail.berlios.de  Fri Jul 11 19:46:28 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Fri, 11 Jul 2008 19:46:28 +0200
Subject: [Plearn-commits] r9234 - trunk/plearn_learners_experimental
Message-ID: <200807111746.m6BHkSLJ026071@sheep.berlios.de>

Author: laulysta
Date: 2008-07-11 19:46:27 +0200 (Fri, 11 Jul 2008)
New Revision: 9234

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
avec debug


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-10 23:17:28 UTC (rev 9233)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-11 17:46:27 UTC (rev 9234)
@@ -81,9 +81,9 @@
     hidden_noise_prob( 0.15 ),
     hidden_reconstruction_lr( 0 ),
     tied_hidden_reconstruction_weights( false ),
-    noisy_recurrent_lr( 0),
-    dynamic_gradient_scale_factor( 0.5 ),
-    recurrent_lr( 0.01 )
+    noisy_recurrent_lr( 0.000001),
+    dynamic_gradient_scale_factor( 1.0 ),
+    recurrent_lr( 0.00001 )
 {
     random_gen = new PRandom();
 }
@@ -291,11 +291,11 @@
                 input_layer_size++;
             }
         }
-
+/*
         if( input_layer->size != input_layer_size )
             PLERROR("DenoisingRecurrentNet::build_(): input_layer->size %d "
                     "should be %d", input_layer->size, input_layer_size);
-
+*/
         // Parsing symbols in target
         int tar_layer = 0;
         int tar_layer_size = 0;
@@ -658,16 +658,22 @@
     encodeSequence(seq, encoded_seq);
     // now work with encoded_seq
     int l = encoded_seq.length();
-    resize_lists(l);
+    resize_lists(l-input_window_size);
 
-    Mat targets = targets_list[0];
-    targets.resize(l, encoded_seq.width());
-                   
+
+    int ntargets = target_layers.length();
+    targets_list.resize(ntargets);
+    //Mat targets = targets_list[0];
+    //targets.resize(l, encoded_seq.width());
+    targets_list[0].resize(l-input_window_size, encoded_seq.width());   
+         
     for(int t=input_window_size; t<l; t++)
     {
-        input_list[t] = encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec();
+
+        input_list[t-input_window_size] = encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec();
         // target is copied so that when adding noise to input, it doesn't modify target 
-        targets(t) << encoded_seq(t);
+        //targets(t-input_window_size) << encoded_seq(t);
+        targets_list[0](t-input_window_size) << encoded_seq(t);
     }
 }
 
@@ -947,9 +953,9 @@
     if(encoding=="timeframe")
         encode_onehot_timeframe(sequence, encoded_seq, prepend_zero_rows);
     else if(encoding=="note_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 0);
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 0);
     else if(encoding=="note_octav_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, true, 5);    
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 5);    
     else if(encoding=="raw_masked_supervised")
         PLERROR("raw_masked_supervised means already encoded! You shouldnt have landed here!!!");
     else if(encoding=="generic")

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-10 23:17:28 UTC (rev 9233)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-11 17:46:27 UTC (rev 9234)
@@ -161,7 +161,7 @@
                                                   bool use_silence, int octav_nbits, int duration_nbits=20);
     
     static void encode_onehot_timeframe(Mat sequence, Mat& encoded_sequence, 
-                                        int prepend_zero_rows, bool use_silence=true);    
+                                        int prepend_zero_rows, bool use_silence=false);    
 
     static int duration_to_number_of_timeframes(int duration);
 



From nouiz at mail.berlios.de  Fri Jul 11 21:01:36 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 11 Jul 2008 21:01:36 +0200
Subject: [Plearn-commits] r9235 - trunk/plearn/base
Message-ID: <200807111901.m6BJ1apK001978@sheep.berlios.de>

Author: nouiz
Date: 2008-07-11 21:01:36 +0200 (Fri, 11 Jul 2008)
New Revision: 9235

Modified:
   trunk/plearn/base/Object.cc
   trunk/plearn/base/Object.h
Log:
better error message: while loading a file in plearn, print the pointer number of the object to facilitate the locating of the error in the source file.


Modified: trunk/plearn/base/Object.cc
===================================================================
--- trunk/plearn/base/Object.cc	2008-07-11 17:46:27 UTC (rev 9234)
+++ trunk/plearn/base/Object.cc	2008-07-11 19:01:36 UTC (rev 9235)
@@ -211,7 +211,7 @@
 
 //#####  Object::readOptionVal  ###############################################
 
-void Object::readOptionVal(PStream &in, const string &optionname)
+void Object::readOptionVal(PStream &in, const string &optionname, unsigned int id)
 {
     try 
     {
@@ -277,8 +277,13 @@
     }
     catch(const PLearnError& e)
     { 
-        PLERROR("Problem while attempting to read value of option \"%s\" of a \"%s\":\n %s", 
-                optionname.c_str(), classname().c_str(), e.message().c_str()); 
+        if(id == UINT_MAX)
+            PLERROR("Problem while attempting to read value of option \"%s\" of a \"%s\":\n %s", 
+                    optionname.c_str(), classname().c_str(), e.message().c_str());
+        else
+            PLERROR("Problem while reading ptr %d while attempting to read value of option \"%s\" of a \"%s\":\n %s", 
+                    id, optionname.c_str(), classname().c_str(), e.message().c_str());
+            
     }
 
     // There are bigger problems in the world but still it isn't always funny
@@ -476,7 +481,7 @@
 
 //#####  Object::newread  #####################################################
 
-void Object::newread(PStream &in)
+void Object::newread(PStream &in, unsigned int id)
 {
     PP<Object> dummy_obj = 0; // Used to read skipped options.
     string cl;
@@ -541,12 +546,12 @@
                     dummy_obj =
                         TypeFactory::instance().newObject(this->classname());
                 }
-                dummy_obj->readOptionVal(in, optionname);
+                dummy_obj->readOptionVal(in, optionname, id);
             }
             else
             {
                 // cerr << "Reading option: " << optionname << endl;
-                readOptionVal(in, optionname);
+                readOptionVal(in, optionname, id);
                 // cerr << "returned from reading optiion " << optionname << endl;
             }
             in.skipBlanksAndCommentsAndSeparators();
@@ -893,7 +898,7 @@
         in.unread(cl+'(');
 
         // Finally read the guts of the object
-        o->newread(in);
+        o->newread(in, id);
     }
 
 #if 0

Modified: trunk/plearn/base/Object.h
===================================================================
--- trunk/plearn/base/Object.h	2008-07-11 17:46:27 UTC (rev 9234)
+++ trunk/plearn/base/Object.h	2008-07-11 19:01:36 UTC (rev 9235)
@@ -707,7 +707,7 @@
      *  @param in          PStream from which to read the new option value
      *  @param optionname  Name of the option to read from the stream
      */
-    void readOptionVal(PStream &in, const string &optionname);
+    void readOptionVal(PStream &in, const string &optionname, unsigned int id = UINT_MAX);
     
     /**
      *  Writes the value of the specified option to the specified stream.
@@ -861,7 +861,7 @@
      *
      *  @param in  Stream from which read the object
      */
-    void newread(PStream& in);
+    void newread(PStream& in, unsigned int id = UINT_MAX);
 
 
     //#####  Remote Method Invocation  ########################################



From nouiz at mail.berlios.de  Fri Jul 11 21:04:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 11 Jul 2008 21:04:59 +0200
Subject: [Plearn-commits] r9236 - trunk/commands
Message-ID: <200807111904.m6BJ4xtQ002249@sheep.berlios.de>

Author: nouiz
Date: 2008-07-11 21:04:58 +0200 (Fri, 11 Jul 2008)
New Revision: 9236

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
added class that is needed when creating the prototype


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-07-11 19:01:36 UTC (rev 9235)
+++ trunk/commands/plearn_desjardins.cc	2008-07-11 19:04:58 UTC (rev 9236)
@@ -84,6 +84,7 @@
 #include <plearn/vmat/DichotomizeVMatrix.h>
 #include <plearn/vmat/FilteredVMatrix.h>
 #include <plearn/vmat/GaussianizeVMatrix.h>
+#include <plearn/vmat/ConstantVMatrix.h>
 #include <plearn/vmat/MemoryVMatrixNoSave.h>
 #include <plearn/vmat/MissingInstructionVMatrix.h>
 #include <plearn/vmat/ProcessingVMatrix.h>



From nouiz at mail.berlios.de  Fri Jul 11 21:34:31 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 11 Jul 2008 21:34:31 +0200
Subject: [Plearn-commits] r9237 - trunk/plearn/vmat
Message-ID: <200807111934.m6BJYVL9007143@sheep.berlios.de>

Author: nouiz
Date: 2008-07-11 21:34:30 +0200 (Fri, 11 Jul 2008)
New Revision: 9237

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
removed useless warning as the data is recalculated if not uptodate


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-11 19:04:58 UTC (rev 9236)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-11 19:34:30 UTC (rev 9237)
@@ -335,8 +335,8 @@
     PPath train_metadata = train_set->getMetaDataDir();
     PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
     train_set->lockMetaDataDir();
-    if (!train_set->isUpToDate(mean_median_mode_file_name,true,true)
-	||!source->isUpToDate(mean_median_mode_file_name,true,true))
+    if (!train_set->isUpToDate(mean_median_mode_file_name)
+	||!source->isUpToDate(mean_median_mode_file_name))
     {
         computeMeanMedianModeVectors();
         createMeanMedianModeFile(mean_median_mode_file_name);



From chapados at mail.berlios.de  Sat Jul 12 23:10:42 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 12 Jul 2008 23:10:42 +0200
Subject: [Plearn-commits] r9238 - trunk/python_modules/plearn/gui_tools
Message-ID: <200807122110.m6CLAgZG020103@sheep.berlios.de>

Author: chapados
Date: 2008-07-12 23:10:41 +0200 (Sat, 12 Jul 2008)
New Revision: 9238

Added:
   trunk/python_modules/plearn/gui_tools/mpl_utilities.py
Modified:
   trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py
   trunk/python_modules/plearn/gui_tools/xp_workbench.py
Log:
Some fixes so that figures can be automatically saved more easily

Modified: trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py	2008-07-11 19:34:30 UTC (rev 9237)
+++ trunk/python_modules/plearn/gui_tools/mpl_figure_editor.py	2008-07-12 21:10:41 UTC (rev 9238)
@@ -54,12 +54,19 @@
 class TraitedFigure( HasTraits ):
     """Traited object acting as a container for a single figure.
     """
-    figure = Instance(Figure, editor=MPLFigureEditor())
-    title  = "Figure"
+    figure  = Instance(Figure, editor=MPLFigureEditor())
+    __title = Str
 
-    def _figure_default(self):
-        return Figure()
+    def __init__(self, title = "Figure", **kwargs):
+        """Instantiate a TraitedFigure.
 
+        Apart from the title, the other **kwargs are passed to the
+        matplotlib Figure() constructor.
+        """
+        super(TraitedFigure,self).__init__()
+        self.__title = title
+        self.figure  = Figure(**kwargs)
+    
     traits_view = View(Item('figure', show_label=False))
 
 

Added: trunk/python_modules/plearn/gui_tools/mpl_utilities.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/mpl_utilities.py	2008-07-11 19:34:30 UTC (rev 9237)
+++ trunk/python_modules/plearn/gui_tools/mpl_utilities.py	2008-07-12 21:10:41 UTC (rev 9238)
@@ -0,0 +1,89 @@
+# mpl_utilities.py
+# Copyright (C) 2008 by Nicolas Chapados
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+# Author: Nicolas Chapados
+
+"""
+A set of utility functions for Matplotlib.
+"""
+
+import os
+
+
+def showOrSave(filename=None, use_environment=True, dpi='__AUTO__', **kwargs):
+    """Either show a matplotlib figure or save it to disk.
+
+    This function is a replacement for Matplotlib's show() function.  It
+    has the behavior or either calling Matplotlib's show function, or
+    saving the figure to disk (without showing it).  Here is how it works:
+
+    - If argument 'use_environment' is True, then the environment variable
+      PL_SAVEFIG is considered.  If defined, the figure is always saved
+      using the given filename (if provided); if not defined, the figure is
+      shown to the screen.  Note that if PL_SAVEFIG is defined but the
+      filename is NOT provided, then nothing happens (neither saved nor
+      shown); this would occur when a valid location (expdir) cannot be
+      found for saving the figure.
+
+    - Otherwise, the argument 'filename' alone is considered.  If provided,
+      the figure is saved; if not, the figure is shown.
+
+    If the function has been able to succesfully save the figure or show
+    it, it returns respectively 'saved' or 'shown'.  Otherwise it returns
+    None.
+    """
+    from pylab import savefig, show
+    if dpi == '__AUTO__':
+        dpi = int(os.environ.get('PL_SAVEFIG_DPI', 72))
+        
+    if use_environment:
+        if eval(os.environ.get('PL_SAVEFIG', "False")):
+            if filename:
+                try:
+                    savefig(filename, dpi=dpi, **kwargs)
+                    return 'saved'
+                except Exception:
+                    pass
+            return None         # Could not save: no filename or exception raised
+        else:
+            show()
+            return 'shown'
+
+    else:
+        if filename:
+            try:
+                savefig(filename, dpi=dpi, **kwargs)
+                return 'saved'
+            except Exception:
+                return None
+        else:
+            show()
+            return 'shown'

Modified: trunk/python_modules/plearn/gui_tools/xp_workbench.py
===================================================================
--- trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-07-11 19:34:30 UTC (rev 9237)
+++ trunk/python_modules/plearn/gui_tools/xp_workbench.py	2008-07-12 21:10:41 UTC (rev 9238)
@@ -47,7 +47,12 @@
 
 from console_logger    import ConsoleLogger
 
+## If there is no display, set the matplotlib backend to Agg
+if not os.environ.has_key("DISPLAY"):
+    import matplotlib
+    matplotlib.use('Agg')
 
+
 #####  ExperimentContext  ###################################################
 
 class ExperimentContext(HasTraits):
@@ -81,23 +86,26 @@
         self.expfunc(self.script_params, self)
 
 
-    def figure(self, title="Figure"):
+    def figure(self, title="Figure", **kwargs):
         """Return a new Matplotlib figure and add it to the notebook.
 
         Note that you must paint on this figure using the Matplotlib OO
-        API, not pylab.
+        API, not pylab.  Apart from 'title', the other **kwargs arguments
+        are passed to matplotlib figure constructor.
         """
         if self.gui:
             ## Late import to allow backend to be chosen
             from mpl_figure_editor import TraitedFigure
-            f = TraitedFigure()
+            f = TraitedFigure(title=title, **kwargs)
             f.title = title
             f.figure.__traited_figure = f
             return f.figure
         else:
             ## Late import to allow backend to be chosen
             import pylab
-            return pylab.figure()
+            f = pylab.figure(**kwargs)
+            f._title = title
+            return f
 
 
     def show(self, fig):
@@ -110,9 +118,24 @@
             self._all_tabs.append(fig.__traited_figure)
         else:
             ## Late import to allow backend to be chosen
-            import pylab
-            pylab.show()
+            from mpl_utilities import showOrSave
 
+            ## Determine a semi-intelligent name to save the
+            ## figure under, if we are indeed saving figures.
+            figname = getattr(fig, "_title", "figure").lower().replace(" ", "_")
+            figname = os.path.join(self.expdir, figname)
+
+            i = 1
+            while True:
+                if os.path.exists(figname+("_%d.png"%i)):
+                    i += 1
+                else:
+                    figname += "_%d.png" % i
+                    break
+                
+            showOrSave(filename = figname, use_environment=True)
+
+    
     @property
     def _display_expdir(self):
         """Shortened version of expdir suitable for display."""



From chapados at mail.berlios.de  Sat Jul 12 23:53:14 2008
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 12 Jul 2008 23:53:14 +0200
Subject: [Plearn-commits] r9239 - trunk/plearn/var
Message-ID: <200807122153.m6CLrERH024592@sheep.berlios.de>

Author: chapados
Date: 2008-07-12 23:53:13 +0200 (Sat, 12 Jul 2008)
New Revision: 9239

Modified:
   trunk/plearn/var/AffineTransformWeightPenalty.cc
Log:
Fixed bound check in L1 penalty

Modified: trunk/plearn/var/AffineTransformWeightPenalty.cc
===================================================================
--- trunk/plearn/var/AffineTransformWeightPenalty.cc	2008-07-12 21:10:41 UTC (rev 9238)
+++ trunk/plearn/var/AffineTransformWeightPenalty.cc	2008-07-12 21:53:13 UTC (rev 9239)
@@ -135,7 +135,7 @@
         if (!input->matGradient.isCompact())
             PLERROR("AffineTransformWeightPenalty::bprop, L1 penalty currently not handling non-compact weight matrix");
         int n=input->width();
-        if (!fast_exact_is_equal(weight_decay_, 0))
+        if (!fast_exact_is_equal(weight_decay_, 0) && l > 0)
         {
             real delta = weight_decay_ * gradientdata[0];
             real* w = input->matValue[1];
@@ -148,7 +148,7 @@
                     d_w[i] -= delta;
             }
         }
-        if(!fast_exact_is_equal(bias_decay_, 0))
+        if(!fast_exact_is_equal(bias_decay_, 0) && l >= 0)
         {
             real delta = bias_decay_ * gradientdata[0];
             real* d_biases = input->matGradient[0];



From nouiz at mail.berlios.de  Mon Jul 14 19:55:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 14 Jul 2008 19:55:52 +0200
Subject: [Plearn-commits] r9240 - trunk/plearn_learners/meta
Message-ID: <200807141755.m6EHtqLH004242@sheep.berlios.de>

Author: nouiz
Date: 2008-07-14 19:55:52 +0200 (Mon, 14 Jul 2008)
New Revision: 9240

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
added fct MultiClassAdaBoost::getOutputNamesoutput()


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-12 21:53:13 UTC (rev 9239)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-14 17:55:52 UTC (rev 9240)
@@ -220,13 +220,15 @@
 
 void MultiClassAdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
-    Vec tmp1(learner1.outputsize());
-    Vec tmp2(learner2.outputsize());
-    learner1.computeOutput(input,tmp1);
-    learner2.computeOutput(input,tmp2);
-    int ind1=int(round(tmp1[0]));
-    int ind2=int(round(tmp2[0]));
+    PLASSERT(output.size()==outputsize());
 
+    Vec output1(learner1.outputsize());
+    Vec output2(learner2.outputsize());
+    learner1.computeOutput(input,output1);
+    learner2.computeOutput(input,output2);
+    int ind1=int(round(output1[0]));
+    int ind2=int(round(output2[0]));
+
     int ind=-1;
     if(ind1==0 && ind2==0)
         ind=0;
@@ -237,8 +239,8 @@
     else
         ind=1;//TODOself.confusion_target;
     output[0]=ind;
-    output[1]=tmp1[0];
-    output[2]=tmp2[0];
+    output[1]=output1[0];
+    output[2]=output2[0];
 }
 void MultiClassAdaBoost::computeOutputAndCosts(const Vec& input,
                                                const Vec& target,
@@ -336,6 +338,15 @@
     PLASSERT(costs.size()==nTestCosts());
 }
 
+TVec<string> MultiClassAdaBoost::getOutputNames() const
+{
+    TVec<string> names(3);
+    names[0]="prediction";
+    names[1]="prediction_learner_1";
+    names[2]="prediction_learner_2";
+    return names;
+}
+
 TVec<string> MultiClassAdaBoost::getTestCostNames() const
 {
     // Return the names of the costs computed by computeCostsFromOutputs

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-07-12 21:53:13 UTC (rev 9239)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-07-14 17:55:52 UTC (rev 9240)
@@ -115,6 +115,8 @@
     virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
                                          const Vec& target, Vec& costs) const;
 
+    virtual TVec<string> getOutputNames() const;
+
     //! Returns the names of the costs computed by computeCostsFromOutpus (and
     //! thus the test method).
     // (PLEASE IMPLEMENT IN .cc)



From nouiz at mail.berlios.de  Mon Jul 14 19:56:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 14 Jul 2008 19:56:16 +0200
Subject: [Plearn-commits] r9241 - trunk/plearn_learners/generic
Message-ID: <200807141756.m6EHuGB3004273@sheep.berlios.de>

Author: nouiz
Date: 2008-07-14 19:56:16 +0200 (Mon, 14 Jul 2008)
New Revision: 9241

Modified:
   trunk/plearn_learners/generic/PLearner.h
Log:
added comment


Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2008-07-14 17:55:52 UTC (rev 9240)
+++ trunk/plearn_learners/generic/PLearner.h	2008-07-14 17:56:16 UTC (rev 9241)
@@ -620,6 +620,8 @@
     /**
      *  Returns a vector of length outputsize() containing the outputs' names.
      *  Default version returns ["out0", "out1", ...]
+     *  Don't forget name should not have space or it will cause trouble when
+     *  they are saved in the file {metadatadir}/fieldnames
      */
     virtual TVec<string> getOutputNames() const;
 



From saintmlx at mail.berlios.de  Mon Jul 14 21:33:12 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 14 Jul 2008 21:33:12 +0200
Subject: [Plearn-commits] r9242 - trunk/plearn/python
Message-ID: <200807141933.m6EJXCEf016125@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-14 21:33:11 +0200 (Mon, 14 Jul 2008)
New Revision: 9242

Modified:
   trunk/plearn/python/PythonExtension.cc
   trunk/plearn/python/PythonExtension.h
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
- allow conversion of Python unicode to C++ string using UTF-8 encoding
- const_cast string constants to 'char*' for Python API to make g++ 4.2 happy



Modified: trunk/plearn/python/PythonExtension.cc
===================================================================
--- trunk/plearn/python/PythonExtension.cc	2008-07-14 17:56:16 UTC (rev 9241)
+++ trunk/plearn/python/PythonExtension.cc	2008-07-14 19:33:11 UTC (rev 9242)
@@ -440,10 +440,10 @@
 
 // Init func for python module.
 // init module, then inject global funcs
-void initPythonExtensionModule(char* module_name)
+void initPythonExtensionModule(char const * module_name)
 {
     PythonObjectWrapper::initializePython();
-    PyObject* plext= Py_InitModule(module_name, NULL);
+    PyObject* plext= Py_InitModule(const_cast<char*>(module_name), NULL);
     setPythonModuleAndInject(plext);
 }
 

Modified: trunk/plearn/python/PythonExtension.h
===================================================================
--- trunk/plearn/python/PythonExtension.h	2008-07-14 17:56:16 UTC (rev 9241)
+++ trunk/plearn/python/PythonExtension.h	2008-07-14 19:33:11 UTC (rev 9242)
@@ -48,7 +48,7 @@
 void injectPLearnClasses(PyObject* env);
 
 // Init func for python module.
-void initPythonExtensionModule(char* module_name);
+void initPythonExtensionModule(char const * module_name);
 
 // inject funcs. and classes, set global module.
 void setPythonModuleAndInject(PyObject* module);

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-07-14 17:56:16 UTC (rev 9241)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-07-14 19:33:11 UTC (rev 9242)
@@ -149,8 +149,26 @@
 string ConvertFromPyObject<string>::convert(PyObject* pyobj,
                                             bool print_traceback)
 {
-    PLASSERT( pyobj );
-    if (! PyString_Check(pyobj))
+    PLASSERT(pyobj);
+
+    // if unicode, encode into a string (utf8) then return
+    if(PyUnicode_Check(pyobj))
+    {
+        PyObject* pystr= PyUnicode_AsUTF8String(pyobj);
+        if(!pystr)
+        {
+            if(PyErr_Occurred()) PyErr_Print();
+            Py_DECREF(pystr);
+            PLERROR("in ConvertFromPyObject<string>::convert : "
+                    "Unicode to string conversion failed.");
+        }
+        string str= PyString_AsString(pystr);
+        Py_DECREF(pystr);
+        return str;
+    }
+    
+    // otherwise, should already be a string
+    if(!PyString_Check(pyobj))
         PLPythonConversionError("ConvertFromPyObject<string>", pyobj,
                                 print_traceback);
     return PyString_AsString(pyobj);
@@ -183,13 +201,13 @@
     if(pyobj == Py_None)
         return 0;
 
-    if(!PyObject_HasAttrString(pyobj, "_cptr"))
+    if(!PyObject_HasAttrString(pyobj, const_cast<char*>("_cptr")))
     {
         PLERROR("in ConvertFromPyObject<Object*>::convert : "
                 "python object has no attribute '_cptr'");
         return 0;
     }
-    PyObject* cptr= PyObject_GetAttrString(pyobj, "_cptr");
+    PyObject* cptr= PyObject_GetAttrString(pyobj, const_cast<char*>("_cptr"));
 
     if (! PyCObject_Check(cptr))
         PLPythonConversionError("ConvertFromPyObject<Object*>", pyobj,
@@ -263,7 +281,7 @@
     PLASSERT(pyobj);
     if(pyobj == Py_None)
         return 0;
-    if(PyObject_HasAttrString(pyobj, "_cptr"))
+    if(PyObject_HasAttrString(pyobj, const_cast<char*>("_cptr")))
         return static_cast<VMatrix*>(
             ConvertFromPyObject<Object*>::convert(pyobj, print_traceback));
     Mat m;
@@ -316,28 +334,28 @@
 RealRange ConvertFromPyObject<RealRange>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT(pyobj);
-    PyObject* py_leftbracket= PyObject_GetAttrString(pyobj, "leftbracket");
+    PyObject* py_leftbracket= PyObject_GetAttrString(pyobj, const_cast<char*>("leftbracket"));
     if(!py_leftbracket)
         PLPythonConversionError("ConvertFromPyObject<RealRange>: "
                                 "not a RealRange (no 'leftbracket' attr.)",
                                 pyobj, print_traceback);
     string leftbracket= ConvertFromPyObject<string>::convert(py_leftbracket, print_traceback);
     Py_DECREF(py_leftbracket);
-    PyObject* py_low= PyObject_GetAttrString(pyobj, "low");
+    PyObject* py_low= PyObject_GetAttrString(pyobj, const_cast<char*>("low"));
     if(!py_low) 
         PLPythonConversionError("ConvertFromPyObject<RealRange>: "
                                 "not a RealRange (no 'low' attr.)",
                                 pyobj, print_traceback);
     real low= ConvertFromPyObject<real>::convert(py_low, print_traceback);
     Py_DECREF(py_low);
-    PyObject* py_high= PyObject_GetAttrString(pyobj, "high");
+    PyObject* py_high= PyObject_GetAttrString(pyobj, const_cast<char*>("high"));
     if(!py_high) 
         PLPythonConversionError("ConvertFromPyObject<RealRange>: "
                                 "not a RealRange (no 'high' attr.)",
                                 pyobj, print_traceback);
     real high= ConvertFromPyObject<real>::convert(py_high, print_traceback);
     Py_DECREF(py_high);
-    PyObject* py_rightbracket= PyObject_GetAttrString(pyobj, "rightbracket");
+    PyObject* py_rightbracket= PyObject_GetAttrString(pyobj, const_cast<char*>("rightbracket"));
     if(!py_rightbracket) 
         PLPythonConversionError("ConvertFromPyObject<RealRange>: "
                                 "not a RealRange (no 'rightbracket' attr.)",
@@ -350,14 +368,14 @@
 VMField ConvertFromPyObject<VMField>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT(pyobj);
-    PyObject* py_name = PyObject_GetAttrString(pyobj, "name");
+    PyObject* py_name = PyObject_GetAttrString(pyobj, const_cast<char*>("name"));
     if (!py_name)
         PLPythonConversionError("ConvertFromPyObject<VMField>: not a VMField (no 'name' attr.)",
                                 pyobj, print_traceback);
     string name = ConvertFromPyObject<string>::convert(py_name, print_traceback);
     Py_DECREF(py_name);
 
-    PyObject* py_fieldtype = PyObject_GetAttrString(pyobj, "fieldtype");
+    PyObject* py_fieldtype = PyObject_GetAttrString(pyobj, const_cast<char*>("fieldtype"));
     if(!py_fieldtype) 
         PLPythonConversionError("ConvertFromPyObject<VMField>: not a VMField (no 'fieldtype' attr.)",
                                 pyobj, print_traceback);



From saintmlx at mail.berlios.de  Mon Jul 14 21:55:37 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Mon, 14 Jul 2008 21:55:37 +0200
Subject: [Plearn-commits] r9243 - trunk/plearn/python
Message-ID: <200807141955.m6EJtbxT018332@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-14 21:55:37 +0200 (Mon, 14 Jul 2008)
New Revision: 9243

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
Log:
- convert PPath using string conversion (to support unicode)



Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2008-07-14 19:33:11 UTC (rev 9242)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2008-07-14 19:55:37 UTC (rev 9243)
@@ -177,11 +177,7 @@
 PPath ConvertFromPyObject<PPath>::convert(PyObject* pyobj,
                                           bool print_traceback)
 {
-    PLASSERT( pyobj );
-    if (! PyString_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<PPath>", pyobj,
-                                print_traceback);
-    return PPath(PyString_AsString(pyobj));
+    return PPath(ConvertFromPyObject<string>::convert(pyobj, print_traceback));
 }
 
 PPointable* ConvertFromPyObject<PPointable*>::convert(PyObject* pyobj,



From nouiz at mail.berlios.de  Tue Jul 15 17:26:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 17:26:19 +0200
Subject: [Plearn-commits] r9244 - trunk/plearn/vmat
Message-ID: <200807151526.m6FFQJeY031127@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 17:26:18 +0200 (Tue, 15 Jul 2008)
New Revision: 9244

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
bugfix, even if we don't want a warning, we should do the same operation


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-14 19:55:37 UTC (rev 9243)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-15 15:26:18 UTC (rev 9244)
@@ -2212,15 +2212,17 @@
         return;
     }
 
-    if(v < -1 && warn_if_cannot_compute)
-        PLWARNING("In VMatrix::compute_missing_size_value() - in class %s"
-                  " more then one of"
-                  " inputsize(%d), targetsize(%d), weightsize(%d) and"
-                  " extrasize(%d) is unknow so we cannot compute them with the"
-                  " width(%d)",
-                  classname().c_str(), inputsize_, targetsize_, weightsize_,
-                  extrasize_, width_);
-    else if(v==0 && warn_if_size_mismatch && width_ >= 0 &&
+    if(v < -1){
+        if(warn_if_cannot_compute)
+            PLWARNING("In VMatrix::compute_missing_size_value() - in class %s"
+                      " more then one of"
+                      " inputsize(%d), targetsize(%d), weightsize(%d) and"
+                      " extrasize(%d) is unknow so we cannot compute them with"
+                      " the width(%d)",
+                      classname().c_str(), inputsize_, targetsize_, weightsize_,
+                      extrasize_, width_);
+        return;
+    }else if(v==0 && warn_if_size_mismatch && width_ >= 0 &&
             width_ != inputsize_ + targetsize_ + weightsize_ + extrasize_)
         PLWARNING("In VMatrix::compute_missing_size_value() for class %s - "
                   "inputsize_(%d) + targetsize_(%d) + weightsize_(%d) + "



From nouiz at mail.berlios.de  Tue Jul 15 18:49:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 18:49:49 +0200
Subject: [Plearn-commits] r9245 - in trunk/plearn: base math
Message-ID: <200807151649.m6FGnnDV030532@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 18:49:43 +0200 (Tue, 15 Jul 2008)
New Revision: 9245

Modified:
   trunk/plearn/base/Storage.h
   trunk/plearn/math/TMat_decl.h
Log:
Added check that we don't want to create a Storage with a size bigger then an int can contain. The current version is limited to int size as this is the type of the fields length_


Modified: trunk/plearn/base/Storage.h
===================================================================
--- trunk/plearn/base/Storage.h	2008-07-15 15:26:18 UTC (rev 9244)
+++ trunk/plearn/base/Storage.h	2008-07-15 16:49:43 UTC (rev 9245)
@@ -53,6 +53,7 @@
 #include <plearn/sys/MemoryMap.h>
 #include "PP.h"
 #include <plearn/io/PStream.h>
+#include <limits>
 
 //! A define used to debug Storage::resize.
 //! Compile with this symbol defined to enable it.
@@ -97,10 +98,17 @@
         }
     }
 
-    inline Storage(int the_length, T* dataptr)
-        :length_(the_length), data(dataptr), 
+    inline Storage(long the_length, T* dataptr)
+        :length_(int(the_length)), data(dataptr), 
          dont_delete_data(true), fd(STORAGE_UNUSED_HANDLE)
     {
+        //we do the check outside a BOUNDCHECK as we normaly do our test with
+        //small dataset. Also, this is not a performance bottleneck and is a
+        //fraction of the time of the malloc.
+        if(the_length>std::numeric_limits<int>::max())
+            PLERROR("In Storage(%ld) - we ask to create a bigger Storage than "
+                    "is possible (limited to 2e31, int)",the_length);
+
     }
 
     inline int length() const
@@ -127,14 +135,21 @@
 	
 
     //!  data is initially filled with zeros
-    Storage(int the_length=0)
-        :length_(the_length), data(0), 
+    Storage(long the_length=0)
+        :length_((int)the_length), data(0), 
          dont_delete_data(false), fd((tFileHandle)STORAGE_UNUSED_HANDLE)
     {
+        //we do the check outside the BOUNDCHECK as we normaly do our test with
+        //small dataset. Also, this is not a performance bottleneck and is a
+        //fraction of the time of the malloc.
+        if(the_length>std::numeric_limits<int>::max())
+            PLERROR("In Storage(%ld) - we ask to create a bigger Storage than "
+                    "is possible (limited to 2e31, int)",the_length);
+
         int l = length();
 #ifdef BOUNDCHECK
         if(l<0)
-            PLERROR("new Storage called with a length() <0");
+            PLERROR("new Storage called with a length() <0; lenght = %d", l);
 #endif
         if (l>0) 
         {
@@ -231,8 +246,16 @@
   It is the job of the CALLER (Mat and Vec) to have an appropriate
   policy to minimize the number of calls to Storage::resize 
 */
-    inline void resize(int newlength)
+    inline void resize(long lnewlength)
     {
+        //we do the check outside a BOUNDCHECK as we normaly do our test with
+        //small dataset. Also, this is not a performance bottleneck and is a
+        //fraction of the time of the malloc.
+        if(lnewlength>std::numeric_limits<int>::max())
+            PLERROR("In Storage(%ld) - we ask to create a bigger Storage than "
+                    "is possible (limited to 2e31, int)",lnewlength);
+        int newlength=(int)lnewlength;
+
 #ifdef BOUNDCHECK
         if(newlength<0)
             PLERROR("Storage::resize called with a length() <0");
@@ -316,10 +339,19 @@
 
     inline void resizeMat(int new_length, int new_width, int extrarows, int extracols, int new_offset, int old_mod, int old_length, int old_width, int old_offset)
     {
-        int s = new_length*new_width;
-        int extrabytes = (new_length+extrarows)*(new_width+extracols) - s;
-        int newsize = new_offset+s+extrabytes;
+        long ls = new_length*new_width;
+        long lextrabytes = (new_length+extrarows)*(new_width+extracols) - ls;
+        long lnewsize = new_offset+ls+lextrabytes;
+
+        //we do the check outside a BOUNDCHECK as we normaly do our test with
+        //small dataset. Also, this is not a performance bottleneck and is a
+        //fraction of the time of the malloc.
+        if(lnewsize>std::numeric_limits<int>::max())
+            PLERROR("In Storage.resizeMat - we ask to create a bigger Storage "
+                    " %ld then is possible (limited to 2e31, int)",lnewsize);
+        int newsize=(int)lnewsize;
         int new_mod = new_width+extracols;
+
 #ifdef BOUNDCHECK
         if(newsize<0)
             PLERROR("Storage::resize called with a length() <0");

Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2008-07-15 15:26:18 UTC (rev 9244)
+++ trunk/plearn/math/TMat_decl.h	2008-07-15 16:49:43 UTC (rev 9245)
@@ -210,7 +210,8 @@
             length_ = new_length;
             width_  = new_width;
             mod_    = new_width;
-            storage = new Storage<T>(length()*mod() + extra);
+            long newsize=(long)length()*mod() + extra;
+            storage = new Storage<T>(newsize);
         }
         else
         {



From nouiz at mail.berlios.de  Tue Jul 15 18:53:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 18:53:06 +0200
Subject: [Plearn-commits] r9246 - trunk/commands/PLearnCommands
Message-ID: <200807151653.m6FGr60W001044@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 18:53:02 +0200 (Tue, 15 Jul 2008)
New Revision: 9246

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
Added a special catch for std::exception. That way we can print more debug info.


Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2008-07-15 16:49:43 UTC (rev 9245)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2008-07-15 16:53:02 UTC (rev 9246)
@@ -338,6 +338,11 @@
         cerr << "FATAL ERROR: " << e.message() << endl;
         EXIT_CODE = 1;
     }
+    catch (std::exception& e)
+    {
+        cerr << "FATAL ERROR thrown by STL : " << e.what() << endl;
+        EXIT_CODE = 2;
+    }
     catch (...) 
     {
         cerr << "FATAL ERROR: uncaught unknown exception "



From nouiz at mail.berlios.de  Tue Jul 15 19:46:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 19:46:30 +0200
Subject: [Plearn-commits] r9247 - trunk/python_modules/plearn/parallel
Message-ID: <200807151746.m6FHkUBU003305@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 19:46:29 +0200 (Tue, 15 Jul 2008)
New Revision: 9247

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
print more env variable when the prog is executed


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-07-15 16:53:02 UTC (rev 9246)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-07-15 17:46:29 UTC (rev 9247)
@@ -929,6 +929,7 @@
                     echo "PATH: $PATH" 1>&2
                     echo "PYTHONPATH: $PYTHONPATH" 1>&2
                     echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
+                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"
                     #which python 1>&2
                     #echo -n python version: 1>&2
                     #python -V 1>&2



From nouiz at mail.berlios.de  Tue Jul 15 19:50:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 19:50:54 +0200
Subject: [Plearn-commits] r9248 - trunk/plearn/math
Message-ID: <200807151750.m6FHosF2003914@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 19:50:54 +0200 (Tue, 15 Jul 2008)
New Revision: 9248

Modified:
   trunk/plearn/math/TMat_decl.h
Log:
more test for the creation of storage with size > MAX_INT


Modified: trunk/plearn/math/TMat_decl.h
===================================================================
--- trunk/plearn/math/TMat_decl.h	2008-07-15 17:46:29 UTC (rev 9247)
+++ trunk/plearn/math/TMat_decl.h	2008-07-15 17:50:54 UTC (rev 9248)
@@ -231,7 +231,7 @@
                 // otherwise a matrix reallocation will occur EVERY TIME
                 // appendRow is called, turning an amortized O(N) algorithm
                 // into an O(N^2) one.
-                int new_size = offset_+new_length*MAX(mod(),new_width);
+                long new_size = offset_+(long)new_length*MAX(mod(),new_width);
                 if(new_size > storage->length())
                     storage->resize(new_size + extra);
                 if(new_width > mod())



From nouiz at mail.berlios.de  Tue Jul 15 21:18:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 21:18:00 +0200
Subject: [Plearn-commits] r9249 - trunk/plearn/io
Message-ID: <200807151918.m6FJI0rm018217@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 21:17:59 +0200 (Tue, 15 Jul 2008)
New Revision: 9249

Modified:
   trunk/plearn/io/fileutils.cc
Log:
first try to make the fct mv() work under windows


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-07-15 17:50:54 UTC (rev 9248)
+++ trunk/plearn/io/fileutils.cc	2008-07-15 19:17:59 UTC (rev 9249)
@@ -390,7 +390,12 @@
 void mv(const PPath& source, const PPath& destination)
 {
     // TODO Cross-platform
-    string command = "\\mv '" + source.absolute() + "' '" + destination.absolute()+"'";
+#ifdef WIN32
+    string cmd="rename";
+#else
+    string cmd="mv";
+#endif
+    string command = "\\"+cmd+" '" + source.absolute() + "' '" + destination.absolute()+"'";
     system(command.c_str());
 }
 



From laulysta at mail.berlios.de  Tue Jul 15 21:22:17 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Tue, 15 Jul 2008 21:22:17 +0200
Subject: [Plearn-commits] r9250 - trunk/plearn_learners_experimental
Message-ID: <200807151922.m6FJMHe4018758@sheep.berlios.de>

Author: laulysta
Date: 2008-07-15 21:22:17 +0200 (Tue, 15 Jul 2008)
New Revision: 9250

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:
dynamic learning rate debuger 


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-15 19:17:59 UTC (rev 9249)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-15 19:22:17 UTC (rev 9250)
@@ -73,7 +73,7 @@
 DenoisingRecurrentNet::DenoisingRecurrentNet() :
     use_target_layers_masks( false ),
     end_of_sequence_symbol( -1000 ),
-    encoding("note_duration"),
+    encoding("note_octav_duration"),
     input_window_size(1),
     tied_input_reconstruction_weights( true ),
     input_noise_prob( 0.15 ),
@@ -111,6 +111,7 @@
                   "Value of the first input component for end-of-sequence "
                   "delimiter.\n");
 
+    // TO DO: input_layer is to be removed eventually because only its size is really used
     declareOption(ol, "input_layer", &DenoisingRecurrentNet::input_layer,
                   OptionBase::buildoption,
                   "The input layer of the model.\n");
@@ -180,8 +181,10 @@
                   &DenoisingRecurrentNet::input_window_size,
                   OptionBase::buildoption,
                   "How many time steps to present as input\n"
+                  "If it's 0, then all layers are essentially ignored, and instead an unconditional predictor is trained\n"
                   "This option is ignored when mode is raw_masked_supervised,"
-                  "since in this mode the full expanded and preprocessed input and target are given explicitly.");
+                  "since in this mode the full expanded and preprocessed input and target are given explicitly."
+        );
 
     declareOption(ol, "tied_input_reconstruction_weights", 
                   &DenoisingRecurrentNet::tied_input_reconstruction_weights,
@@ -230,6 +233,9 @@
                   OptionBase::buildoption,
                   "The learning rate used in the fine tuning phase\n");
 
+    declareOption(ol, "mean_encoded_vec", &DenoisingRecurrentNet::mean_encoded_vec,
+                  OptionBase::learntoption,
+                  "When training with trainUnconditionalPredictor (if input_window_size==0), this is simply used to store the the avg encoded frame");
 
  /*
     declareOption(ol, "", &DenoisingRecurrentNet::,
@@ -299,7 +305,6 @@
         // Parsing symbols in target
         int tar_layer = 0;
         int tar_layer_size = 0;
-        int lala = target_layers.length();
         target_symbol_sizes.resize(target_layers.length());
         for( tar_layer=0; tar_layer<target_layers.length(); tar_layer++ )
             target_symbol_sizes[tar_layer].resize(0);
@@ -510,8 +515,73 @@
     stage = 0;
 }
 
+void DenoisingRecurrentNet::trainUnconditionalPredictor()
+{
+    MODULE_LOG << "trainUnconditionalPredictor() called " << endl;
+
+    // reserve memory for sequences
+    seq.resize(5000,2); // contains the current sequence
+
+    // real weight = 0; // Unused
+    Vec train_costs( getTrainCostNames().length() );
+    train_costs.fill(-1);
+
+    if( !initTrain() )
+    {
+        MODULE_LOG << "train() aborted" << endl;
+        return;
+    }
+
+
+    if( stage==0 && nstages==1 )
+    {        
+        // clear stats of previous epoch
+        train_stats->forget();
+
+
+        int nvecs = 0;
+        int nseq = nSequences();        
+
+        ProgressBar* pb = 0;
+        if( report_progress)
+            pb = new ProgressBar( "Sequences ",nseq);
+        for(int i=0; i<nseq; i++)
+        {
+            getSequence(i, seq);
+            encodeSequenceAndPopulateLists(seq);
+            if(i==0)
+            {
+                mean_encoded_vec.resize(encoded_seq.width());
+                mean_encoded_vec.clear();
+            }
+            for(int t=0; t<encoded_seq.length(); t++)
+            {
+                mean_encoded_vec += encoded_seq(t);                
+                nvecs++;
+            }
+        }
+        mean_encoded_vec *= 1./nvecs;            
+        train_stats->update(train_costs);
+        train_stats->finalize();            
+
+        if( pb )
+        {
+            delete pb;
+            pb = 0;
+        }
+        ++stage;
+    }
+}
+
+
 void DenoisingRecurrentNet::train()
 {
+    if(input_window_size==0)
+    {
+        trainUnconditionalPredictor();
+        return;
+    }
+
     MODULE_LOG << "train() called " << endl;
 
     // reserve memory for sequences
@@ -671,6 +741,10 @@
     {
 
         input_list[t-input_window_size] = encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec();
+        //perr << "t-input_window_size = " << endl;
+        //perr << "subMat:" << endl << encoded_seq.subMatRows(t-input_window_size,input_window_size) << endl;
+        //perr << "toVec:" << endl << encoded_seq.subMatRows(t-input_window_size,input_window_size).toVec() << endl;
+        //perr << "input_list:" << endl << input_list[t-input_window_size] << endl;
         // target is copied so that when adding noise to input, it doesn't modify target 
         //targets(t-input_window_size) << encoded_seq(t);
         targets_list[0](t-input_window_size) << encoded_seq(t);
@@ -733,8 +807,50 @@
     nll_list.resize(l,ntargets);
 }
 
-// TODO: think properly about prepended stuff
 
+// must fill train_costs, train_n_items and     target_prediction_list[0](t)
+void DenoisingRecurrentNet::unconditionalFprop(Vec train_costs, Vec train_n_items) const
+{
+    int pred_size = mean_encoded_vec.length();
+    if(pred_size<=0)
+        PLERROR("mean_encoded_vec not properly initialized. Did you call trainUnconditionalPredictor prior to unconditionalFprop ?");
+
+    int l = input_list.length();
+    int tar = 0;
+    train_n_items[tar] += l;
+    target_prediction_list[tar].resize(l,pred_size);
+    for(int i=0; i<l; i++)
+    {        
+        Vec target_prediction_i = target_prediction_list[tar](i);
+        target_prediction_i << mean_encoded_vec;
+        Vec target_vec = targets_list[tar](i);
+
+        /*
+        target_layers[tar]->setExpectation(target_prediction_i);
+        nll_list(i,tar) = target_layers[tar]->fpropNLL(target_vec); 
+        */
+        double nllcost = 0;
+        for(int k=0; k<target_vec.length(); k++)
+            if(target_vec[k]!=0)
+                nllcost -= target_vec[k]*safelog(target_prediction_i[k]);
+        nll_list(i,tar) = nllcost;
+
+        if (isinf(nll_list(i,tar)))
+        {
+            PLWARNING("Row %d of sequence of length %d lead to inf cost",i,l);
+            perr << "Problem at positions (vec of length " << target_vec.length() << "): ";
+            for(int k=0; k<target_vec.length(); k++)
+                if(target_vec[k]!=0 && target_prediction_i[k]==0)
+                    perr << k << " ";
+            perr << endl;
+            // perr << "target_vec = " << target_vec << endl;
+            // perr << "target_prediction_i = " << target_prediction_i << endl;
+        }
+        else
+            train_costs[tar] += nll_list(i,tar);
+    }
+}
+
 // fprop accumulates costs in costs and counts in n_items
 void DenoisingRecurrentNet::recurrentFprop(Vec train_costs, Vec train_n_items) const
 {
@@ -823,51 +939,26 @@
         else
             hidden_gradient.resize(hidden_layer->size);
         hidden_gradient.clear();
-        if(use_target_layers_masks)
+        for( int tar=0; tar<target_layers.length(); tar++)
         {
-            for( int tar=0; tar<target_layers.length(); tar++)
+            if( !fast_exact_is_equal(target_layers_weights[tar],0) )
             {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(target_prediction_list[tar](i));
-                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
+                target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
+                target_layers[tar]->activation += target_layers[tar]->bias;
+                target_layers[tar]->setExpectation(target_prediction_list[tar](i));
+                target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
+                bias_gradient *= target_layers_weights[tar];
+                if(use_target_layers_masks)
                     bias_gradient *= masks_list[tar](i);
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                             hidden_gradient, bias_gradient,true);
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                             hidden_gradient, bias_gradient,true);
-                }
+                target_layers[tar]->update(bias_gradient);
+                if( hidden_layer2 )
+                    target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                         hidden_gradient, bias_gradient,true);
+                else
+                    target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
+                                                         hidden_gradient, bias_gradient,true);
             }
         }
-        else
-        {
-            for( int tar=0; tar<target_layers.length(); tar++)
-            {
-                if( !fast_exact_is_equal(target_layers_weights[tar],0) )
-                {
-                    target_layers[tar]->activation << target_prediction_act_no_bias_list[tar](i);
-                    target_layers[tar]->activation += target_layers[tar]->bias;
-                    target_layers[tar]->setExpectation(target_prediction_list[tar](i));
-                    target_layers[tar]->bpropNLL(targets_list[tar](i),nll_list(i,tar),bias_gradient);
-                    bias_gradient *= target_layers_weights[tar];
-                    target_layers[tar]->update(bias_gradient);
-                    if( hidden_layer2 )
-                        target_connections[tar]->bpropUpdate(hidden2_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                             hidden_gradient, bias_gradient,true); 
-                    else
-                        target_connections[tar]->bpropUpdate(hidden_list(i),target_prediction_act_no_bias_list[tar](i),
-                                                             hidden_gradient, bias_gradient,true); 
-                        
-                }
-            }
-        }
-
         if (hidden_layer2)
         {
             hidden_layer2->bpropUpdate(
@@ -890,16 +981,15 @@
                 
             dynamic_connections->bpropUpdate(
                 hidden_list(i-1),
-                hidden_act_no_bias_list(i), // Here, it should be cond_bias, but doesn't matter
+                hidden_act_no_bias_list(i), // Here, it should be dynamic_act_no_bias_contribution, but doesn't matter because a RBMMatrixConnection::bpropUpdate doesn't use its second argument
                 hidden_gradient, hidden_temporal_gradient);
                 
-            hidden_temporal_gradient << hidden_gradient;
-                
             input_connections->bpropUpdate(
                 input_list[i],
                 hidden_act_no_bias_list(i), 
                 visi_bias_gradient, hidden_temporal_gradient);// Here, it should be activations - cond_bias, but doesn't matter
                 
+            hidden_temporal_gradient << hidden_gradient;                
         }
         else
         {
@@ -925,6 +1015,55 @@
 
 
 /*
+  
+Frequences dans le trainset:
+
+**NOTES**
+0  DO            0.0872678308077029924            
+1  DO#           0.00716010857716887095                   
+2  RE            0.178895847137025221                   
+3  RE#           0.0037189817684399511                   
+4  MI            0.114241135358112283                   
+5  FA            0.00517237694231303512                   
+6  FA#           0.0806848056083954851                   
+7  SOL           0.194776326757432616                   
+8  SOL#          0.00301365763994271892                   
+9  LA            0.13988928548528437                   
+10 LA#           0.00369760831000064084                   
+11 SI            0.181482035608181741                   
+
+**OCTAVES**
+0  OCT1          0.362130506337230429                   
+1  OCT2          0.574048346762989659                   
+2  OCT3          0.0635219184816295107                   
+3  OCT4          0.000299228418150340866                
+
+**DUREES**
+0  1/8           0.00333425951653236984                   
+1  1/6           0.000170987667514480506                   
+2  1/4           0.0386432128582725951                   
+3  1/3           0.00716010857716887095                   
+4  2/4           0.569880522367324227                   
+5  2/3           0                               
+6  3/4           0.00220146621924893673                   
+7  4/4           0.305896937183405604                   
+8  5/4           4.27469168786201266e-05                   
+9  6/4           0.0222283967768824656                   
+10 8/4           0.0365058670143415878                   
+11 10/4          0.000876311796011712552                   
+12 12/4          0.0078440592472267933                   
+13 14/4          6.41203753179301933e-05                   
+14 16/4          0.00331288605809306001                   
+15 18/4          8.54938337572402532e-05                   
+16 20/4          0.000726697586936542119                   
+17 24/4          0.000619830294739991887                   
+18 28/4          0.000149614209075170433                   
+19 32/4          0.000256481501271720773         
+
+ */
+
+
+/*
 Format de donnees:
 
 matrice de 2 colonnes:
@@ -935,11 +1074,9 @@
       ou -1 (missing)
       ou -999 (fin de sequence)
 
-duree: 0 double-croche
-       1 
-..16   exprimee en 1/16 de mesure (resultat du quantize de Stan)
+duree: voir indices (colonne de gauche) et DUREES dans table de frequences ci-dessus
+       1 unite correspond a une noire.
 
-
  */
 
 void DenoisingRecurrentNet::encodeSequence(Mat sequence, Mat& encoded_seq) const
@@ -955,7 +1092,7 @@
     else if(encoding=="note_duration")
         encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 0);
     else if(encoding=="note_octav_duration")
-        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 5);    
+        encode_onehot_note_octav_duration(sequence, encoded_seq, prepend_zero_rows, false, 4);    
     else if(encoding=="raw_masked_supervised")
         PLERROR("raw_masked_supervised means already encoded! You shouldnt have landed here!!!");
     else if(encoding=="generic")
@@ -997,6 +1134,12 @@
 }
 
 
+int DenoisingRecurrentNet::getDurationBit(int duration)
+{
+    if(duration==5)  // map infrequent 5 to 4
+        duration=4;
+    return duration;
+}
 
 
 // encodings
@@ -1060,10 +1203,10 @@
             encoded_sequence(prepend_zero_rows+i,note_nbits+octavpos) = 1;
         }
 
-        int duration = int(sequence(i,1));
-        if(duration<0 || duration>=duration_nbits)
-            PLERROR("duration out of valid range");
-        encoded_sequence(prepend_zero_rows+i,note_nbits+octav_nbits+duration) = 1;
+        int duration_bit = getDurationBit(int(sequence(i,1)));
+        if(duration_bit<0 || duration_bit>=duration_nbits)
+            PLERROR("duration_bit out of valid range");
+        encoded_sequence(prepend_zero_rows+i,note_nbits+octav_nbits+duration_bit) = 1;
     }
 }
 
@@ -1270,8 +1413,12 @@
         seq.resize(seqlen, w);
         testset->getMat(start,0,seq);
         encodeSequenceAndPopulateLists(seq);
-        recurrentFprop(costs, n_items);
 
+        if(input_window_size==0)
+            unconditionalFprop(costs, n_items);
+        else
+            recurrentFprop(costs, n_items);
+
         if (testoutputs)
         {
             for(int t=0; t<seqlen; t++)

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-15 19:17:59 UTC (rev 9249)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-15 19:22:17 UTC (rev 9250)
@@ -141,6 +141,9 @@
     // Phase recurrent no noise (supervised fine tuning)
     double recurrent_lr;
 
+
+    // When training with trainUnconditionalPredictor, this is simply used to store the avg encoded frame
+    Vec mean_encoded_vec;
     
     //#####  Not Options  #####################################################
 
@@ -164,9 +167,9 @@
                                         int prepend_zero_rows, bool use_silence=false);    
 
     static int duration_to_number_of_timeframes(int duration);
+    static int getDurationBit(int duration);
 
 
-
     // input noise injection
     void inject_zero_forcing_noise(Mat sequence, double noise_prob);
 
@@ -379,6 +382,10 @@
 
     void resize_lists(int l) const;
 
+    void trainUnconditionalPredictor();
+    void unconditionalFprop(Vec train_costs, Vec train_n_items) const;
+
+
 private:
     //#####  Private Data Members  ############################################
 



From saintmlx at mail.berlios.de  Tue Jul 15 21:39:16 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 15 Jul 2008 21:39:16 +0200
Subject: [Plearn-commits] r9251 - trunk/python_modules/plearn/pybridge
Message-ID: <200807151939.m6FJdG2k020466@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-15 21:39:16 +0200 (Tue, 15 Jul 2008)
New Revision: 9251

Modified:
   trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
Log:
- bugfix in unpickling of wrapped PLearn objects



Modified: trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py
===================================================================
--- trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-07-15 19:22:17 UTC (rev 9250)
+++ trunk/python_modules/plearn/pybridge/wrapped_plearn_object.py	2008-07-15 19:39:16 UTC (rev 9251)
@@ -177,7 +177,7 @@
         for k in dict:
             if k != '_cptr':
                 self.__setattr__(k, dict[k])
-        for o in d[1]:
+        for o in d[2]:
             self.setOptionFromPython(o,d[2][o])
         self.build()
         return dict



From saintmlx at mail.berlios.de  Tue Jul 15 21:50:42 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 15 Jul 2008 21:50:42 +0200
Subject: [Plearn-commits] r9252 - trunk/python_modules/plearn/pytest
Message-ID: <200807151950.m6FJogj8021382@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-15 21:50:42 +0200 (Tue, 15 Jul 2008)
New Revision: 9252

Modified:
   trunk/python_modules/plearn/pytest/programs.py
Log:
- use subprocess.Popen instead of os.system+redirection



Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2008-07-15 19:39:16 UTC (rev 9251)
+++ trunk/python_modules/plearn/pytest/programs.py	2008-07-15 19:50:42 UTC (rev 9252)
@@ -1,4 +1,4 @@
-import logging, os, sets, sys
+import logging, os, sets, sys, subprocess
 from plearn.utilities import ppath
 from plearn.utilities import moresh
 from plearn.utilities import toolkit
@@ -267,28 +267,21 @@
                 raise Not_Implemented
             the_compiler = "python " + \
                 os.path.join(ppath.ppath('PLEARNDIR'), 'scripts', 'pymake')
-            redirection = ">"
         else:
             the_compiler = self.compiler
-            redirection = ">&"
 
         compile_options = ""
         if self.compile_options is not None:
             compile_options = self.compile_options
 
-        compile_cmd   = "%s %s %s -link-target %s %s %s" \
-                          % ( the_compiler, compile_options,
-                              self.getProgramPath(),
-                              self.getInternalExecPath(),
-                              redirection, log_fname )
+        compile_cmd= self.getCompileCommand(the_compiler, compile_options)
 
-        compile_cmd= self.getCompileCommand(the_compiler, compile_options, redirection, log_fname)
-
         logging.debug(compile_cmd)
-        if sys.platform == 'win32':
-            compile_exit_code = os.system(compile_cmd)
-        else:
-            compile_exit_code = os.WEXITSTATUS( os.system(compile_cmd) )
+        p= subprocess.Popen(compile_cmd, 
+                            shell= True,
+                            stdout= open(log_fname, 'w'),
+                            stderr= subprocess.STDOUT)
+        compile_exit_code= p.wait()
         logging.debug("compile_exit_code <- %d\n"%compile_exit_code)
 
         moresh.popd()
@@ -304,12 +297,11 @@
             
         return compile_exit_code==0
 
-    def getCompileCommand(self, the_compiler, compile_options, redirection, log_fname):
-        compile_cmd   = "%s %s %s -link-target %s %s %s" \
+    def getCompileCommand(self, the_compiler, compile_options):
+        compile_cmd   = "%s %s %s -link-target %s" \
                         % ( the_compiler, compile_options,
                             self.getProgramPath(),
-                            self.getInternalExecPath(),
-                            redirection, log_fname )
+                            self.getInternalExecPath())
         return compile_cmd
 
 
@@ -431,14 +423,13 @@
     def _signature(self):
         return self.getProgramPath()[:-2]+'so'
 
-    def getCompileCommand(self, the_compiler, compile_options, redirection, log_fname):
+    def getCompileCommand(self, the_compiler, compile_options):
         """
         WARNING: this replaces the shared object in-place
         """
-        compile_cmd   = "%s %s %s %s %s" \
+        compile_cmd   = "%s %s %s" \
                         % ( the_compiler, compile_options,
-                            self.getProgramPath(),
-                            redirection, log_fname )
+                            self.getProgramPath())
         return compile_cmd
 
         



From nouiz at mail.berlios.de  Tue Jul 15 22:16:00 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 22:16:00 +0200
Subject: [Plearn-commits] r9253 - trunk/plearn/io
Message-ID: <200807152016.m6FKG0uY023617@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 22:16:00 +0200 (Tue, 15 Jul 2008)
New Revision: 9253

Modified:
   trunk/plearn/io/fileutils.cc
   trunk/plearn/io/fileutils.h
Log:
implement mv in a cross-platform way with NSPR


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2008-07-15 19:50:42 UTC (rev 9252)
+++ trunk/plearn/io/fileutils.cc	2008-07-15 20:16:00 UTC (rev 9253)
@@ -387,16 +387,13 @@
 ////////
 // mv //
 ////////
-void mv(const PPath& source, const PPath& destination)
+PRStatus mv(const PPath& source, const PPath& destination, bool fail_on_error)
 {
-    // TODO Cross-platform
-#ifdef WIN32
-    string cmd="rename";
-#else
-    string cmd="mv";
-#endif
-    string command = "\\"+cmd+" '" + source.absolute() + "' '" + destination.absolute()+"'";
-    system(command.c_str());
+    PRStatus ret=PR_Rename(source.absolute().c_str(),destination.absolute().c_str());
+    if(ret!=PR_SUCCESS && fail_on_error)
+        PLERROR("In mv(%s,%s) - the move failed!",source.c_str(),destination.c_str());
+
+    return ret;
 }
 
 /////////////
@@ -404,6 +401,7 @@
 /////////////
 void mvforce(const PPath& source, const PPath& destination)
 {
+    // TODO Cross-platform, PR_Access, PR_Delete and PR_Rename
     string command = "\\mv -f '" + source.absolute() + "' '" + destination.absolute()+"'";
     system(command.c_str());
 }

Modified: trunk/plearn/io/fileutils.h
===================================================================
--- trunk/plearn/io/fileutils.h	2008-07-15 19:50:42 UTC (rev 9252)
+++ trunk/plearn/io/fileutils.h	2008-07-15 20:16:00 UTC (rev 9253)
@@ -132,7 +132,8 @@
 bool rm(const PPath& file);
 
 //! Calls system mv command to move the given source file to destination.
-void mv(const PPath& source, const PPath& dest);
+//! It fail if file exist. Use mvforce to force the overwrite existing file.
+PRStatus mv(const PPath& source, const PPath& dest, bool fail_on_error = true);
 
 //! Same as mv, but will not prompt before overwriting.
 void mvforce(const PPath& source, const PPath& dest);



From nouiz at mail.berlios.de  Tue Jul 15 22:17:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 15 Jul 2008 22:17:54 +0200
Subject: [Plearn-commits] r9254 - trunk/plearn/base
Message-ID: <200807152017.m6FKHsef023861@sheep.berlios.de>

Author: nouiz
Date: 2008-07-15 22:17:53 +0200 (Tue, 15 Jul 2008)
New Revision: 9254

Modified:
   trunk/plearn/base/Storage.h
Log:
test for size less then 0. In the case we ask for really big amount of memory that don't fit in a long...


Modified: trunk/plearn/base/Storage.h
===================================================================
--- trunk/plearn/base/Storage.h	2008-07-15 20:16:00 UTC (rev 9253)
+++ trunk/plearn/base/Storage.h	2008-07-15 20:17:53 UTC (rev 9254)
@@ -251,15 +251,18 @@
         //we do the check outside a BOUNDCHECK as we normaly do our test with
         //small dataset. Also, this is not a performance bottleneck and is a
         //fraction of the time of the malloc.
-        if(lnewlength>std::numeric_limits<int>::max())
-            PLERROR("In Storage(%ld) - we ask to create a bigger Storage than "
-                    "is possible (limited to 2e31, int)",lnewlength);
+        if(lnewlength>std::numeric_limits<int>::max() || lnewlength<std::numeric_limits<int>::min())
+            PLERROR("In Storage(%ld) - we ask to create a bigger/smaller"
+                    " Storage than is possible with an int",
+                    lnewlength);
+#ifdef BOUNDCHECK
+        if(lnewlength<0)
+            PLERROR("Storage::resize(%ld) called with a length() <0",
+                    lnewlength);
+#endif
+
         int newlength=(int)lnewlength;
 
-#ifdef BOUNDCHECK
-        if(newlength<0)
-            PLERROR("Storage::resize called with a length() <0");
-#endif
         if (newlength==length())
             return;
 #if defined(_MINGW_) || defined(WIN32)



From laulysta at mail.berlios.de  Wed Jul 16 02:03:30 2008
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 16 Jul 2008 02:03:30 +0200
Subject: [Plearn-commits] r9255 - trunk/plearn_learners_experimental
Message-ID: <200807160003.m6G03UWv010935@sheep.berlios.de>

Author: laulysta
Date: 2008-07-16 02:03:29 +0200 (Wed, 16 Jul 2008)
New Revision: 9255

Modified:
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
   trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
Log:

nimportequoi


Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-15 20:17:53 UTC (rev 9254)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.cc	2008-07-16 00:03:29 UTC (rev 9255)
@@ -83,6 +83,7 @@
     tied_hidden_reconstruction_weights( false ),
     noisy_recurrent_lr( 0.000001),
     dynamic_gradient_scale_factor( 1.0 ),
+    dynamic_input_reconstruction_lr(0.),
     recurrent_lr( 0.00001 )
 {
     random_gen = new PRandom();
@@ -228,6 +229,11 @@
                   OptionBase::buildoption,
                   "The scale factor of the learning rate used in the noisy recurrent phase for the dynamic hidden reconstruction\n");
 
+    declareOption(ol, "dynamic_input_reconstruction_lr", 
+                  &DenoisingRecurrentNet::dynamic_input_reconstruction_lr,
+                  OptionBase::buildoption,
+                  "The learning rate for the input reconstruction cost in the recurrent noisy phase\n");
+
     declareOption(ol, "recurrent_lr", 
                   &DenoisingRecurrentNet::recurrent_lr,
                   OptionBase::buildoption,
@@ -512,6 +518,8 @@
         target_connections[i]->forget();
     }
 
+    input_reconstruction_bias.clear();
+
     stage = 0;
 }
 
@@ -660,7 +668,7 @@
                 {
                     setLearningRate( noisy_recurrent_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate();
+                    recurrentUpdate(true);
                 }
 
                 // recurrent no noise phase
@@ -670,7 +678,7 @@
                         encoded_seq << clean_encoded_seq;                    
                     setLearningRate( recurrent_lr );
                     recurrentFprop(train_costs, train_n_items);
-                    recurrentUpdate();
+                    recurrentUpdate(false);
                 }
             }
 
@@ -705,8 +713,9 @@
 
 void DenoisingRecurrentNet::performGreedyDenoisingPhase()
 {
-    // TO DO!
-    PLERROR("performGreedyDenoisingPhase not yet implemented");
+    
+
+
 }
 
 
@@ -915,6 +924,79 @@
     }
 }
 
+
+void DenoisingRecurrentNet::applyMultipleSoftmaxToInputWindow(Vec input_reconstruction_activation, Vec input_reconstruction_prob)
+{
+    if(target_layers.length()!=1)
+        PLERROR("applyMultipleSoftmaxToInputWindow was thought to work with a single target layer which is a RBMMixedLayer combining differnet multinomial costs");
+
+    // int nelems = target_layers[0]->size();
+    int nelems = target_prediction_list[0].width();
+
+    if(input_reconstruction_activation.length() != input_window_size*nelems)
+        
+        PLERROR("Problem: input_reconstruction_activation.length() != input_window_size*nelems  (%d != %d * %d)",input_reconstruction_activation.length(),input_window_size,nelems);
+
+    for(int k=0; k<input_window_size; k++)
+    {
+        Vec activation_window = input_reconstruction_activation.subVec(k*nelems, nelems);
+        Vec prob_window = input_reconstruction_prob.subVec(k*nelems, nelems);
+        target_layers[0]->fprop(activation_window, prob_window);
+    }    
+}
+
+
+
+
+Mat DenoisingRecurrentNet::getInputConnectionsWeightMatrix()
+{
+    RBMMatrixConnection* conn = dynamic_cast<RBMMatrixConnection*>((RBMConnection*)input_connections);
+    if(conn==0)
+        PLERROR("Expecting input connection to be a RBMMatrixConnection. Je sais c'est sale, mais au point ou on est rendu..");
+    return conn->weights;
+}
+
+//! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
+//! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
+//! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
+void DenoisingRecurrentNet::fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
+                                              Vec clean_input, Vec hidden_gradient, double input_reconstruction_lr)
+{
+    // set appropriate sizes
+    int fullinputlength = clean_input.length();
+    if(input_reconstruction_bias.length()==0)
+    {
+        input_reconstruction_bias.resize(fullinputlength);
+        input_reconstruction_bias.clear();
+    }
+    input_reconstruction_activation.resize(fullinputlength);
+    input_reconstruction_prob.resize(fullinputlength);
+
+    // predict (denoised) input_reconstruction 
+    transposeProduct(input_reconstruction_activation, reconstruction_weights, hidden); 
+    input_reconstruction_activation += input_reconstruction_bias;
+    applyMultipleSoftmaxToInputWindow(input_reconstruction_activation, input_reconstruction_prob);
+
+    // gradient of -log softmax is just  output_of_softmax - onehot_target
+    // so let's accumulate this in hidden_gradient
+    Vec input_reconstruction_activation_grad = input_reconstruction_prob;
+    input_reconstruction_activation_grad -= clean_input;
+    input_reconstruction_activation_grad *= input_reconstruction_lr;
+
+    // update bias
+    input_reconstruction_bias -= input_reconstruction_activation_grad;
+
+    // update weight
+    // productTransposeAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad);
+    externalProductAcc(reconstruction_weights, hidden, input_reconstruction_activation_grad);
+
+    // accumulate in hidden_gradient
+    productAcc(hidden_gradient, reconstruction_weights, input_reconstruction_activation_grad);
+}
+
+
+
+
 /*
 input_list
 targets_list
@@ -928,7 +1010,7 @@
 nll_list
 */
 
-void DenoisingRecurrentNet::recurrentUpdate()
+void DenoisingRecurrentNet::recurrentUpdate(bool input_is_corrupted)
 {
     hidden_temporal_gradient.resize(hidden_layer->size);
     hidden_temporal_gradient.clear();
@@ -971,8 +1053,19 @@
                 hidden_gradient, bias_gradient);
         }
             
+        // Add contribution of input reconstruction cost in hidden_gradient
+        if(input_is_corrupted && dynamic_input_reconstruction_lr!=0)
+        {
+            Mat reconstruction_weights = getInputConnectionsWeightMatrix();
+            Vec clean_input = clean_encoded_seq.subMatRows(i, input_window_size).toVec();
+
+            fpropUpdateInputReconstructionFromHidden(hidden_list(i), reconstruction_weights, input_reconstruction_bias, input_reconstruction_prob, 
+                                                     clean_input, hidden_gradient, dynamic_input_reconstruction_lr);
+        }
+
         if(i!=0 && dynamic_connections )
         {   
+            // add contribution to gradient of next time step hidden layer
             hidden_gradient += hidden_temporal_gradient;
                 
             hidden_layer->bpropUpdate(

Modified: trunk/plearn_learners_experimental/DenoisingRecurrentNet.h
===================================================================
--- trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-15 20:17:53 UTC (rev 9254)
+++ trunk/plearn_learners_experimental/DenoisingRecurrentNet.h	2008-07-16 00:03:29 UTC (rev 9255)
@@ -135,8 +135,9 @@
 
     // Phase noisy recurrent (supervised): uses input_noise_prob
     // this phase *also* uses dynamic_gradient_scale_factor;
-    double noisy_recurrent_lr;
+    double noisy_recurrent_lr;    
     double dynamic_gradient_scale_factor;
+    double dynamic_input_reconstruction_lr;
     
     // Phase recurrent no noise (supervised fine tuning)
     double recurrent_lr;
@@ -144,6 +145,9 @@
 
     // When training with trainUnconditionalPredictor, this is simply used to store the avg encoded frame
     Vec mean_encoded_vec;
+
+    // learnt bias for input reconstruction
+    Vec input_reconstruction_bias;
     
     //#####  Not Options  #####################################################
 
@@ -257,7 +261,7 @@
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the recurrent tuning phase,
     //! after the visible units have been clamped
-    void recurrentUpdate();
+    void recurrentUpdate(bool input_is_corrupted);
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -351,6 +355,10 @@
     mutable Mat encoded_seq; // contains encoded version of current train or test sequence (possibly corrupted by noise)
     mutable Mat clean_encoded_seq; // copy of clean sequence contains encoded version of current train or test sequence
 
+    mutable Vec input_reconstruction_activation; // temporary Vec to hold input reconstruction activation (before softmax)
+    mutable Vec input_reconstruction_prob;       // temporary Vec to hold input reconstruction prob (after applying softmax)
+
+
 protected:
     //#####  Protected Member Functions  ######################################
 
@@ -366,6 +374,8 @@
 
     void performGreedyDenoisingPhase();
 
+    void applyMultipleSoftmaxToInputWindow(Vec input_reconstruction_activation, Vec input_reconstruction_prob);
+
     // note: the following functions are declared const because they have
     // to be called by test (which is const). Similarly, the members they 
     // manipulate are all declared mutable.
@@ -385,7 +395,16 @@
     void trainUnconditionalPredictor();
     void unconditionalFprop(Vec train_costs, Vec train_n_items) const;
 
+    Mat getInputConnectionsWeightMatrix();
 
+    //! Builds input_reconstruction_prob from hidden (using reconstruction_weights which is  nhidden x ninputs, and input_reconstruction_bias)
+    //! then backpropagates reconstruction cost (after comparison with clean_input) with learning rate input_reconstruction_lr
+    //! accumulates gradient in hidden_gradient, and updates reconstruction_weights and input_reconstruction_bias
+    void fpropUpdateInputReconstructionFromHidden(Vec hidden, Mat& reconstruction_weights, Vec& input_reconstruction_bias, Vec& input_reconstruction_prob, 
+                                                  Vec clean_input, Vec hidden_gradient, double input_reconstruction_lr);
+
+
+
 private:
     //#####  Private Data Members  ############################################
 



From nouiz at mail.berlios.de  Wed Jul 16 17:01:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jul 2008 17:01:15 +0200
Subject: [Plearn-commits] r9256 - trunk/plearn_learners/regressors
Message-ID: <200807161501.m6GF1FoH016012@sheep.berlios.de>

Author: nouiz
Date: 2008-07-16 17:01:15 +0200 (Wed, 16 Jul 2008)
New Revision: 9256

Modified:
   trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
Log:
removed a warning.


Modified: trunk/plearn_learners/regressors/RegressionTreeRegisters.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-07-16 00:03:29 UTC (rev 9255)
+++ trunk/plearn_learners/regressors/RegressionTreeRegisters.cc	2008-07-16 15:01:15 UTC (rev 9256)
@@ -124,7 +124,8 @@
 {   
     //check that we can put all the examples of the train_set
     //with respect to the size of RTR_type who limit the capacity
-    PLCHECK(the_train_set.length()<=std::numeric_limits<RTR_type>::max());
+    PLCHECK(the_train_set.length()>0 
+            && (unsigned)the_train_set.length()<=std::numeric_limits<RTR_type>::max());
 
     if(the_train_set==source && tsource)
         //we set the existing source file



From nouiz at mail.berlios.de  Wed Jul 16 18:00:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jul 2008 18:00:32 +0200
Subject: [Plearn-commits] r9257 - trunk/plearn/vmat
Message-ID: <200807161600.m6GG0Whk021690@sheep.berlios.de>

Author: nouiz
Date: 2008-07-16 18:00:31 +0200 (Wed, 16 Jul 2008)
New Revision: 9257

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
added doc


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-16 15:01:15 UTC (rev 9256)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-16 16:00:31 UTC (rev 9257)
@@ -855,6 +855,7 @@
                   "Currently supported types: \n"
                   "- skip       : Ignore the content of the field, won't be inserted in the resulting VMat\n"
                   "- auto       : If a numeric value, keep it as is, if not, look it up in the mapping (possibly inserting a new mapping if it's not there) \n"
+                  "- auto-num   : take any float value with the decimal separator as the dot. If theirs is a $ at the start or end it is removed. If their is comma they are removed.\n"
                   "- num        : numeric value, keep as is\n"
                   "- num-comma  : numeric value where thousands are separeted by comma\n"
                   "- char       : look it up in the mapping (possibly inserting a new mapping if it's not there)\n"



From nouiz at mail.berlios.de  Wed Jul 16 19:19:55 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 16 Jul 2008 19:19:55 +0200
Subject: [Plearn-commits] r9258 - trunk/plearn/vmat
Message-ID: <200807161719.m6GHJtVD020988@sheep.berlios.de>

Author: nouiz
Date: 2008-07-16 19:19:53 +0200 (Wed, 16 Jul 2008)
New Revision: 9258

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
   trunk/plearn/vmat/TextFilesVMatrix.h
Log:
added an option default_spec that give a default spec if none is set for a specific fieldname


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-16 16:00:31 UTC (rev 9257)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-16 17:19:53 UTC (rev 9258)
@@ -253,13 +253,6 @@
 
     if(reorder_fieldspec_from_headers)
     {
-        if(fnames_header.size()!=fieldspec.size())
-        {
-            PLWARNING("In TextFilesVMatrix::setColumnNamesAndWidth() - "
-                    "We read %d field names from the header but have %d"
-                    "fieldspec",fnames_header.size(),fieldspec.size());
-        }
-
         //check that all field names from the header have a spec
         TVec<string> not_used_fn;
         for(int i=0;i<fnames_header.size();i++)
@@ -269,8 +262,12 @@
             for(;j<fieldspec.size();j++)
                 if(fieldspec[j].first==name)
                     break;
-            if(j>=fieldspec.size())
-                not_used_fn.append(name);
+            if(j>=fieldspec.size()){
+                if(default_spec!=""){
+                    fieldspec.append(make_pair(name,default_spec));
+                }else
+                    not_used_fn.append(name);
+            }
         }
         //check that all fieldspec names are also in the header
         TVec<string> not_used_fs;
@@ -284,6 +281,14 @@
             if(j>=fnames_header.size())
                 not_used_fs.append(name);
         }
+        //check that we have the good number of fieldspec
+        if(fnames_header.size()!=fieldspec.size())
+        {
+            PLWARNING("In TextFilesVMatrix::setColumnNamesAndWidth() - "
+                    "We read %d field names from the header but have %d"
+                    "fieldspec",fnames_header.size(),fieldspec.size());
+        }
+
         if(not_used_fs.size()!=0)
             PLWARNING("TextFilesVMatrix::setColumnNamesAndWidth() - "
                       "Fieldspecs do not exist in source for field(s): %s\n"
@@ -343,7 +348,9 @@
         metadatadir = metadatapath;
         setMetaDataDir(metadatapath);
     }
-
+    if (!default_spec.empty() && !reorder_fieldspec_from_headers)
+        PLERROR("In TextFilesVMatrix::build_: when the option default_spec is used, reorder_fieldspec_from_headers must be true");
+    
     if(!force_mkdir(getMetaDataDir()))
         PLERROR("In TextFilesVMatrix::build_: could not create directory '%s'",
                 getMetaDataDir().absolute().c_str());
@@ -895,6 +902,12 @@
                   "The expansion is equivalent to the regex 'field_spec_name*'."
                   "The option reorder_fieldspec_from_headers must be true");
 
+    declareOption(ol, "default_spec", 
+                  &TextFilesVMatrix::default_spec,
+                  OptionBase::buildoption,
+                  "If their is no fieldspec for a fieldname, we will use this"
+                  "value. reorder_fieldspec_from_headers must be true.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }

Modified: trunk/plearn/vmat/TextFilesVMatrix.h
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.h	2008-07-16 16:00:31 UTC (rev 9257)
+++ trunk/plearn/vmat/TextFilesVMatrix.h	2008-07-16 17:19:53 UTC (rev 9258)
@@ -128,6 +128,10 @@
 
     bool reorder_fieldspec_from_headers;
     bool partial_match;
+    
+    //! If their is no fieldspec, we will use this value
+    //! reorder_fieldspec_from_headers must be true.
+    string default_spec;
 
     // ****************
     // * Constructors *



From nouiz at mail.berlios.de  Thu Jul 17 17:47:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 17 Jul 2008 17:47:24 +0200
Subject: [Plearn-commits] r9259 - trunk/python_modules/plearn/parallel
Message-ID: <200807171547.m6HFlOnG024642@sheep.berlios.de>

Author: nouiz
Date: 2008-07-17 17:47:24 +0200 (Thu, 17 Jul 2008)
New Revision: 9259

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
move output to stderr


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-07-16 17:19:53 UTC (rev 9258)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-07-17 15:47:24 UTC (rev 9259)
@@ -929,7 +929,7 @@
                     echo "PATH: $PATH" 1>&2
                     echo "PYTHONPATH: $PYTHONPATH" 1>&2
                     echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH" 1>&2
-                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"
+                    echo "OMP_NUM_THREADS: $OMP_NUM_THREADS" 1>&2
                     #which python 1>&2
                     #echo -n python version: 1>&2
                     #python -V 1>&2



From nouiz at mail.berlios.de  Thu Jul 17 22:32:28 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 17 Jul 2008 22:32:28 +0200
Subject: [Plearn-commits] r9260 - trunk/plearn/vmat
Message-ID: <200807172032.m6HKWSE0023632@sheep.berlios.de>

Author: nouiz
Date: 2008-07-17 22:32:27 +0200 (Thu, 17 Jul 2008)
New Revision: 9260

Modified:
   trunk/plearn/vmat/GaussianizeVMatrix.cc
Log:
foget ealier some data to be more memory friendly. If we are swapping, we won't put them back on the disk as we don't need them


Modified: trunk/plearn/vmat/GaussianizeVMatrix.cc
===================================================================
--- trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-07-17 15:47:24 UTC (rev 9259)
+++ trunk/plearn/vmat/GaussianizeVMatrix.cc	2008-07-17 20:32:27 UTC (rev 9260)
@@ -193,10 +193,14 @@
     for (int i = 0; i < candidates.length(); i++) {
         int j = candidates[i];
         StatsCollector& stat = stats[j];
-        if (fast_exact_is_equal(stat.stddev(), 0))
+        if (fast_exact_is_equal(stat.stddev(), 0)){
+            stats[j].forget();//to keep the total memory used lower faster
             continue;
-        if (!gaussianize_binary && stat.isbinary())
+        }
+        if (!gaussianize_binary && stat.isbinary()) {
+            stats[j].forget();//to keep the total memory used lower faster
             continue;
+        }
         if ((stat.max() - stat.min()) > threshold_ratio * stat.stddev()) {
             features_to_gaussianize.append(j);
             values.append(Vec());



From nouiz at mail.berlios.de  Thu Jul 17 22:33:06 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 17 Jul 2008 22:33:06 +0200
Subject: [Plearn-commits] r9261 - trunk/plearn/vmat
Message-ID: <200807172033.m6HKX6MB023881@sheep.berlios.de>

Author: nouiz
Date: 2008-07-17 22:33:06 +0200 (Thu, 17 Jul 2008)
New Revision: 9261

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
give more info when it catch an exception


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-17 20:32:27 UTC (rev 9260)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-17 20:33:06 UTC (rev 9261)
@@ -1419,7 +1419,14 @@
         // There is a lock file, and it is not older than 'max_lock_age'.
         string bywho;
         try{ bywho = loadFileAsString(lockfile); }
-        catch(...) {
+        catch(const PLearnError& e) {
+            PLERROR("In VMatrix::lockMetaDataDir - Catching exceptions is"
+                    " dangerous in PLearn (memory"
+                    " leaks may occur), thus I prefer to stop here. "
+                    " Comment this line if you don't care."
+                    " The error message is: %s",e.message().c_str());
+            bywho = "UNKNOWN (could not read .lock file)" 
+        } catch(...) {
             PLERROR("In VMatrix::lockMetaDataDir - Catching exceptions is dangerous in PLearn (memory "
                     "leaks may occur), thus I prefer to stop here. Comment this line if you don't care.");
             bywho = "UNKNOWN (could not read .lock file)";



From nouiz at mail.berlios.de  Thu Jul 17 23:00:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 17 Jul 2008 23:00:27 +0200
Subject: [Plearn-commits] r9262 - trunk/plearn/vmat
Message-ID: <200807172100.m6HL0RST002538@sheep.berlios.de>

Author: nouiz
Date: 2008-07-17 23:00:22 +0200 (Thu, 17 Jul 2008)
New Revision: 9262

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
fixed type


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-17 20:33:06 UTC (rev 9261)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-17 21:00:22 UTC (rev 9262)
@@ -1425,7 +1425,7 @@
                     " leaks may occur), thus I prefer to stop here. "
                     " Comment this line if you don't care."
                     " The error message is: %s",e.message().c_str());
-            bywho = "UNKNOWN (could not read .lock file)" 
+            bywho = "UNKNOWN (could not read .lock file)" ;
         } catch(...) {
             PLERROR("In VMatrix::lockMetaDataDir - Catching exceptions is dangerous in PLearn (memory "
                     "leaks may occur), thus I prefer to stop here. Comment this line if you don't care.");



From nouiz at mail.berlios.de  Fri Jul 18 15:53:15 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 15:53:15 +0200
Subject: [Plearn-commits] r9263 - trunk/plearn/vmat
Message-ID: <200807181353.m6IDrFCL013766@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 15:53:15 +0200 (Fri, 18 Jul 2008)
New Revision: 9263

Modified:
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
Log:
when their is not specified fiels and no train_set, we use the source as the train_set. 


Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-17 21:00:22 UTC (rev 9262)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-18 13:53:15 UTC (rev 9263)
@@ -151,8 +151,9 @@
 void MissingIndicatorVMatrix::build_()
 {
     if (!source) PLERROR("In MissingIndicatorVMatrix:: source vmat must be supplied");
-    if(!train_set && !fields)
-      PLERROR("In MissingIndicatorVMatrix:: train_set or fields must be supplied");
+    if(!train_set && !fields){
+      train_set = source;
+    }
     updateMtime(source);
     if(train_set)
       updateMtime(train_set);



From nouiz at mail.berlios.de  Fri Jul 18 17:07:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 17:07:08 +0200
Subject: [Plearn-commits] r9264 - trunk/plearn/vmat
Message-ID: <200807181507.m6IF78ob022861@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 17:07:07 +0200 (Fri, 18 Jul 2008)
New Revision: 9264

Modified:
   trunk/plearn/vmat/VMatrix.cc
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
In VMatrix::getPrecomputedStatsFromFile, don't renerate useless warning about non-uptodate file as we recompute it if needed. 
Also, catch error while we have the lock. We release it in case of error and we rethrow the error.


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-18 13:53:15 UTC (rev 9263)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-18 15:07:07 UTC (rev 9264)
@@ -1621,16 +1621,22 @@
     if (!metadatadir.isEmpty()) {
         lockMetaDataDir();
         statsfile =  metadatadir / filename;
-        uptodate = isUpToDate(statsfile, true, true);
+        uptodate = isUpToDate(statsfile);
     }
-    if (uptodate)
-        PLearn::load(statsfile, stats);
-    else
-    {
-        VMat vm = const_cast<VMatrix*>(this);
-        stats = PLearn::computeStats(vm, maxnvalues, progress_bar);
+    try{
+        if (uptodate)
+            PLearn::load(statsfile, stats);
+        else
+        {
+            VMat vm = const_cast<VMatrix*>(this);
+            stats = PLearn::computeStats(vm, maxnvalues, progress_bar);
+            if(!metadatadir.isEmpty())
+                PLearn::save(statsfile, stats);
+        }
+    }catch(const PLearnError& e){
         if(!metadatadir.isEmpty())
-            PLearn::save(statsfile, stats);
+            unlockMetaDataDir();
+        throw e;
     }
     if (!metadatadir.isEmpty())
         unlockMetaDataDir();

Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-07-18 13:53:15 UTC (rev 9263)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-07-18 15:07:07 UTC (rev 9264)
@@ -254,6 +254,8 @@
         if(is_equal(max_constant_threshold,1))
 //We don't need all the value, if (min==max && non_missing_value>0) it is constant value.
             maxnvalues = 0;
+        if(!the_train_source->hasMetaDataDir() && hasMetaDataDir())
+            the_train_source->setMetaDataDir(getMetaDataDir()+"/source");
         stats = the_train_source->
             getPrecomputedStatsFromFile("stats_variableDeletionVMatrix_"+
                                         tostring(maxnvalues)+".psave",



From nouiz at mail.berlios.de  Fri Jul 18 18:49:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 18:49:59 +0200
Subject: [Plearn-commits] r9265 - trunk/plearn/vmat
Message-ID: <200807181649.m6IGnx89028204@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 18:49:58 +0200 (Fri, 18 Jul 2008)
New Revision: 9265

Modified:
   trunk/plearn/vmat/VariableDeletionVMatrix.cc
Log:
made a PLASSERT an PLCHECK 


Modified: trunk/plearn/vmat/VariableDeletionVMatrix.cc
===================================================================
--- trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-07-18 15:07:07 UTC (rev 9264)
+++ trunk/plearn/vmat/VariableDeletionVMatrix.cc	2008-07-18 16:49:58 UTC (rev 9265)
@@ -260,7 +260,7 @@
             getPrecomputedStatsFromFile("stats_variableDeletionVMatrix_"+
                                         tostring(maxnvalues)+".psave",
                                         maxnvalues, true);
-        PLASSERT( stats.length() == source->width() );
+        PLCHECK( stats.length() == source->width() );
     }
 
     indices.resize(0);



From nouiz at mail.berlios.de  Fri Jul 18 19:22:49 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 19:22:49 +0200
Subject: [Plearn-commits] r9266 - trunk/plearn/vmat
Message-ID: <200807181722.m6IHMnWY004731@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 19:22:43 +0200 (Fri, 18 Jul 2008)
New Revision: 9266

Modified:
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
Log:
added a check


Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-18 16:49:58 UTC (rev 9265)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-18 17:22:43 UTC (rev 9266)
@@ -151,6 +151,9 @@
 void MissingIndicatorVMatrix::build_()
 {
     if (!source) PLERROR("In MissingIndicatorVMatrix:: source vmat must be supplied");
+    if(source->inputsize()>0)
+      PLERROR("In MissingIndicatorVMatrix::build_() source must have an inputsize > 0."
+	      " inputsize = %d", source->inputsize());
     if(!train_set && !fields){
       train_set = source;
     }



From nouiz at mail.berlios.de  Fri Jul 18 19:28:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 19:28:45 +0200
Subject: [Plearn-commits] r9267 - trunk/plearn/vmat
Message-ID: <200807181728.m6IHSjkq026658@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 19:28:44 +0200 (Fri, 18 Jul 2008)
New Revision: 9267

Modified:
   trunk/plearn/vmat/MissingIndicatorVMatrix.cc
Log:
corrected comparaison


Modified: trunk/plearn/vmat/MissingIndicatorVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-18 17:22:43 UTC (rev 9266)
+++ trunk/plearn/vmat/MissingIndicatorVMatrix.cc	2008-07-18 17:28:44 UTC (rev 9267)
@@ -151,8 +151,8 @@
 void MissingIndicatorVMatrix::build_()
 {
     if (!source) PLERROR("In MissingIndicatorVMatrix:: source vmat must be supplied");
-    if(source->inputsize()>0)
-      PLERROR("In MissingIndicatorVMatrix::build_() source must have an inputsize > 0."
+    if(source->inputsize()<=0)
+      PLERROR("In MissingIndicatorVMatrix::build_() source must have an inputsize <= 0."
 	      " inputsize = %d", source->inputsize());
     if(!train_set && !fields){
       train_set = source;



From nouiz at mail.berlios.de  Fri Jul 18 20:38:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 20:38:29 +0200
Subject: [Plearn-commits] r9268 - trunk/python_modules/plearn/parallel
Message-ID: <200807181838.m6IIcTFt006007@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 20:38:29 +0200 (Fri, 18 Jul 2008)
New Revision: 9268

Modified:
   trunk/python_modules/plearn/parallel/dbi.py
Log:
removed the bad dublicate creation of the LOGS dir


Modified: trunk/python_modules/plearn/parallel/dbi.py
===================================================================
--- trunk/python_modules/plearn/parallel/dbi.py	2008-07-18 17:28:44 UTC (rev 9267)
+++ trunk/python_modules/plearn/parallel/dbi.py	2008-07-18 18:38:29 UTC (rev 9268)
@@ -180,8 +180,6 @@
             self.__dict__[key] = args[key]
 
         # check if log directory exists, if not create it
-        if (not os.path.exists('LOGS')):
-            os.mkdir('LOGS')
         if (not os.path.exists(self.log_dir)):
 #            if self.dolog or self.file_redirect_stdout or self.file_redirect_stderr:
             os.mkdir(self.log_dir)



From tihocan at mail.berlios.de  Fri Jul 18 21:21:56 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 18 Jul 2008 21:21:56 +0200
Subject: [Plearn-commits] r9269 -
	trunk/plearn_learners/classifiers/EXPERIMENTAL
Message-ID: <200807181921.m6IJLuDO012957@sheep.berlios.de>

Author: tihocan
Date: 2008-07-18 21:21:55 +0200 (Fri, 18 Jul 2008)
New Revision: 9269

Added:
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h
Log:
New simple ensemble of logistic classifiers to have a quick easy-to-use benchmark

Added: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-18 18:38:29 UTC (rev 9268)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-18 19:21:55 UTC (rev 9269)
@@ -0,0 +1,351 @@
+// -*- C++ -*-
+
+// KFoldLogisticClassifier.cc
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file KFoldLogisticClassifier.cc */
+
+
+#include "KFoldLogisticClassifier.h"
+#include <plearn/opt/GradientOptimizer.h>
+#include <plearn/vmat/ExplicitSplitter.h>
+#include <plearn/vmat/KFoldSplitter.h>
+#include <plearn_learners/generic/NNet.h>
+#include <plearn_learners/hyper/EarlyStoppingOracle.h>
+#include <plearn_learners/hyper/HyperLearner.h>
+#include <plearn_learners/hyper/HyperOptimize.h>
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    KFoldLogisticClassifier,
+    "Average of multiple logistic classifiers from K-Fold split of the data.",
+    "The training set is split into 'kfold' folds, and we train one logistic\n"
+    "classifier on each fold (whose learning is controlled by early stopping\n"
+    "based on the validation NLL).\n"
+    "The output of this classifier is then the average of the outputs of the\n"
+    "underlying logistic classifiers."
+);
+
+/////////////////////////////
+// KFoldLogisticClassifier //
+/////////////////////////////
+KFoldLogisticClassifier::KFoldLogisticClassifier():
+    kfold(5),
+    learning_rate(1e-3),
+    max_degraded_steps(100),
+    max_epochs(1000),
+    step_size(1)
+{
+}
+
+////////////////////
+// declareOptions //
+////////////////////
+void KFoldLogisticClassifier::declareOptions(OptionList& ol)
+{
+    // Build options.
+
+    declareOption(ol, "kfold", &KFoldLogisticClassifier::kfold,
+                  OptionBase::buildoption,
+        "Number of splits of the data (and of classifiers being trained).");
+
+    declareOption(ol, "learning_rate", &KFoldLogisticClassifier::learning_rate,
+                  OptionBase::buildoption,
+        "Learning rate in the logistic classifiers.");
+
+    declareOption(ol, "max_degraded_steps",
+                  &KFoldLogisticClassifier::max_degraded_steps,
+                  OptionBase::buildoption,
+        "Maximum number of optimization steps performed after finding a\n"
+        "candidate for early stopping.");
+
+    declareOption(ol, "max_epochs",
+                  &KFoldLogisticClassifier::max_epochs,
+                  OptionBase::buildoption,
+        "Maximum number of epochs when training logistic classifiers\n");
+
+    declareOption(ol, "step_size",
+                  &KFoldLogisticClassifier::step_size,
+                  OptionBase::buildoption,
+        "Measure performance every 'step_size' epochs.");
+
+    // Learnt options.
+
+    declareOption(ol, "log_net", &KFoldLogisticClassifier::log_net,
+                  OptionBase::learntoption,
+        "Underlying logistic classifiers.");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+////////////
+// build_ //
+////////////
+void KFoldLogisticClassifier::build_()
+{
+}
+
+///////////
+// build //
+///////////
+void KFoldLogisticClassifier::build()
+{
+    inherited::build();
+    build_();
+}
+
+/////////////////////////////////
+// makeDeepCopyFromShallowCopy //
+/////////////////////////////////
+void KFoldLogisticClassifier::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("KFoldLogisticClassifier::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+////////////////
+// outputsize //
+////////////////
+int KFoldLogisticClassifier::outputsize() const
+{
+    if (log_net.isEmpty())
+        return -1;
+    else
+        return log_net[0]->outputsize();
+}
+
+////////////
+// forget //
+////////////
+void KFoldLogisticClassifier::forget()
+{
+    //! (Re-)initialize the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!)
+    /*!
+      A typical forget() method should do the following:
+      - call inherited::forget() to initialize its random number generator
+        with the 'seed' option
+      - initialize the learner's parameters, using this random generator
+      - stage = 0
+    */
+    inherited::forget();
+    log_net.resize(0);
+}
+
+///////////
+// train //
+///////////
+void KFoldLogisticClassifier::train()
+{
+    if (!initTrain())
+        return;
+
+    PLCHECK( stage == 0 );
+
+    // Find out the number of classes in the dataset.
+    TVec<bool> all_classes;
+    Vec input, target;
+    real weight;
+    PLCHECK(train_set->targetsize() == 1);
+    for (int i = 0; i < train_set->length(); i++) {
+        train_set->getExample(i, input, target, weight);
+        int t = int(round(target[0]));
+        if (t >= all_classes.length()) {
+            int n_to_add = t - all_classes.length() + 1;
+            for (int j = 0; j < n_to_add; j++)
+                all_classes.append(false);
+        }
+        all_classes[t] = true;
+    }
+    int n_classes = all_classes.length();
+    PLCHECK(n_classes >= 2);
+    PLCHECK(all_classes.find(false) == -1);
+
+    // Split the data.
+    PP<KFoldSplitter> splitter = new KFoldSplitter();
+    splitter->K = this->kfold;
+    splitter->build();
+    splitter->setDataSet(train_set);
+
+    // Create logistic regressors.
+    log_net.resize(0);
+    string cost_func;
+    for (int k = 0; k < kfold; k++) {
+        PP<GradientOptimizer> opt = new GradientOptimizer();
+        opt->start_learning_rate = this->learning_rate;
+        opt->build();
+        PP<NNet> nnet = new NNet();
+        nnet->optimizer = opt;
+        nnet->seed_ = this->seed_;
+        if (n_classes == 2) {
+            cost_func = "stable_cross_entropy";
+            nnet->output_transfer_func = "sigmoid";
+            nnet->noutputs = 1;
+        } else {
+            cost_func = "NLL";
+            nnet->output_transfer_func = "softmax";
+            nnet->noutputs = n_classes;
+        }
+        nnet->cost_funcs = TVec<string>(1, cost_func);
+        nnet->build();
+        log_net.append(get_pointer(nnet));
+    }
+
+    // Train logistic regressors.
+    for (int k = 0; k < log_net.length(); k++) {
+        // Initialize the hyper-learning framework for early stopping.
+        // Splitter.
+        PP<ExplicitSplitter> hsplitter = new ExplicitSplitter();
+        hsplitter->splitsets = TMat<VMat>(1, 2);
+        hsplitter->splitsets(0) << splitter->getSplit(k);
+        hsplitter->build();
+        // PTester.
+        PP<PTester> htester = new PTester();
+        htester->splitter = hsplitter;
+        string cost = "E[test.E[" + cost_func + "]]";
+        htester->setStatNames(TVec<string>(1, cost), false);
+        htester->build();
+        // Oracle.
+        PP<EarlyStoppingOracle> early_stop = new EarlyStoppingOracle();
+        early_stop->max_degraded_steps = this->max_degraded_steps;
+        early_stop->range = TVec<double>(3, 1);
+        early_stop->range[1] = this->max_epochs + 1;
+        early_stop->range[2] = this->step_size;
+        early_stop->build();
+        // Strategy.
+        PP<HyperOptimize> strategy = new HyperOptimize();
+        strategy->oracle = early_stop;
+        strategy->which_cost = "0";
+        strategy->build();
+        // HyperLearner.
+        PP<HyperLearner> hyper = new HyperLearner();
+        hyper->dont_restart_upon_change = TVec<string>(1, "nstages");
+        hyper->learner_ = log_net[k];
+        hyper->option_fields = hyper->dont_restart_upon_change;
+        hyper->save_final_learner = false;
+        hyper->strategy = TVec< PP<HyperCommand> >(1, get_pointer(strategy));
+        hyper->tester = htester;
+        hyper->build();
+        // Perform training.
+        hyper->train();
+        // Make sure we keep only the best model.
+        log_net[k] = hyper->getLearner();
+    }
+    this->stage = 1;
+}
+
+///////////////////
+// computeOutput //
+///////////////////
+void KFoldLogisticClassifier::computeOutput(const Vec& input, Vec& output) const
+{
+    PLCHECK(!log_net.isEmpty());
+    log_net[0]->computeOutput(input, output);
+    store_output.resize(output.length());
+    for (int k = 1; k < log_net.length(); k++) {
+        log_net[k]->computeOutput(input, store_output);
+        output += store_output;
+    }
+    output /= real(log_net.length());
+}
+
+/////////////////////////////
+// computeCostsFromOutputs //
+/////////////////////////////
+void KFoldLogisticClassifier::computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                           const Vec& target, Vec& costs) const
+{
+    int t = int(round(target[0]));
+    costs.resize(1);
+    if (output.length() == 1) {
+        // Binary classification.
+        if (t == 1)
+            costs[0] = - pl_log(output[0]);
+        else {
+            PLASSERT(t == 0);
+            costs[0] = - pl_log(1 - output[0]);
+        }
+    } else {
+        // More than two targets.
+        PLASSERT(is_equal(sum(output), 1));
+        costs[0] = - pl_log(output[t]);
+    }
+}
+
+//////////////////////
+// getTestCostNames //
+//////////////////////
+TVec<string> KFoldLogisticClassifier::getTestCostNames() const
+{
+    static TVec<string> costs;
+    if (costs.isEmpty())
+        costs.append("nll");
+    return costs;
+}
+
+///////////////////////
+// getTrainCostNames //
+///////////////////////
+TVec<string> KFoldLogisticClassifier::getTrainCostNames() const
+{
+    return TVec<string>();
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h	2008-07-18 18:38:29 UTC (rev 9268)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h	2008-07-18 19:21:55 UTC (rev 9269)
@@ -0,0 +1,195 @@
+// -*- C++ -*-
+
+// KFoldLogisticClassifier.h
+//
+// Copyright (C) 2008 Olivier Delalleau
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Olivier Delalleau
+
+/*! \file KFoldLogisticClassifier.h */
+
+
+#ifndef KFoldLogisticClassifier_INC
+#define KFoldLogisticClassifier_INC
+
+#include <plearn_learners/generic/PLearner.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class KFoldLogisticClassifier : public PLearner
+{
+    typedef PLearner inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    int kfold;
+    real learning_rate;
+    int max_degraded_steps;
+    int max_epochs;
+    int step_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    KFoldLogisticClassifier();
+
+
+    //#####  PLearner Member Functions  #######################################
+
+    //! Returns the size of this learner's output, (which typically
+    //! may depend on its inputsize(), targetsize() and set options).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual int outputsize() const;
+
+    //! (Re-)initializes the PLearner in its fresh state (that state may depend
+    //! on the 'seed' option) and sets 'stage' back to 0 (this is the stage of
+    //! a fresh learner!).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void forget();
+
+    //! The role of the train method is to bring the learner up to
+    //! stage==nstages, updating the train_stats collector with training costs
+    //! measured on-line in the process.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void train();
+
+    //! Computes the output from the input.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeOutput(const Vec& input, Vec& output) const;
+
+    //! Computes the costs from already computed output.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void computeCostsFromOutputs(const Vec& input, const Vec& output,
+                                         const Vec& target, Vec& costs) const;
+
+    //! Returns the names of the costs computed by computeCostsFromOutpus (and
+    //! thus the test method).
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTestCostNames() const;
+
+    //! Returns the names of the objective costs that the train method computes
+    //! and for which it updates the VecStatsCollector train_stats.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual TVec<std::string> getTrainCostNames() const;
+
+
+    // *** SUBCLASS WRITING: ***
+    // While in general not necessary, in case of particular needs
+    // (efficiency concerns for ex) you may also want to overload
+    // some of the following methods:
+    // virtual void computeOutputAndCosts(const Vec& input, const Vec& target,
+    //                                    Vec& output, Vec& costs) const;
+    // virtual void computeCostsOnly(const Vec& input, const Vec& target,
+    //                               Vec& costs) const;
+    // virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
+    //                   VMat testoutputs=0, VMat testcosts=0) const;
+    // virtual int nTestCosts() const;
+    // virtual int nTrainCosts() const;
+    // virtual void resetInternalState();
+    // virtual bool isStatefulLearner() const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(KFoldLogisticClassifier);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+
+    //! Used to store outputs.
+    mutable Vec store_output;
+
+    //#####  Protected Options  ###############################################
+
+    TVec< PP<PLearner> > log_net;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(KFoldLogisticClassifier);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Fri Jul 18 22:24:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 18 Jul 2008 22:24:24 +0200
Subject: [Plearn-commits] r9270 - trunk/plearn_learners/regressors
Message-ID: <200807182024.m6IKOOhD000479@sheep.berlios.de>

Author: nouiz
Date: 2008-07-18 22:24:23 +0200 (Fri, 18 Jul 2008)
New Revision: 9270

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTreeLeave.cc
   trunk/plearn_learners/regressors/RegressionTreeNode.cc
Log:
don't save the sorted train set. don't save old deprecated data.


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-07-18 19:21:55 UTC (rev 9269)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-07-18 20:24:23 UTC (rev 9270)
@@ -107,9 +107,10 @@
                   "When making a prediction, the tree will adjust the output value of each leave to the closest value provided in this vector.");
     declareOption(ol, "leave_template", &RegressionTree::leave_template, OptionBase::buildoption,
                   "The template for the leave objects to create.\n");
-    declareOption(ol, "sorted_train_set", &RegressionTree::sorted_train_set, OptionBase::buildoption, 
-                  "The train set sorted on all columns\n"
-                  "If it is not provided by a wrapping algorithm, it is created at stage 0.\n");
+    declareOption(ol, "sorted_train_set", &RegressionTree::sorted_train_set,
+                  OptionBase::buildoption | OptionBase::nosave, 
+                  "The train set sorted on all columns. If it is not provided by a\n"
+                  " wrapping algorithm, it is created at stage 0.\n");
       
     declareOption(ol, "root", &RegressionTree::root, OptionBase::learntoption,
                   "The root node of the tree being built\n");

Modified: trunk/plearn_learners/regressors/RegressionTreeLeave.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-07-18 19:21:55 UTC (rev 9269)
+++ trunk/plearn_learners/regressors/RegressionTreeLeave.cc	2008-07-18 20:24:23 UTC (rev 9270)
@@ -81,7 +81,8 @@
                   "The hyper parameter to balance the error and the confidence factor\n");
     declareStaticOption(ol, "verbosity", &RegressionTreeLeave::verbosity, OptionBase::buildoption,
                   "The desired level of verbosity\n");
-    declareOption(ol, "train_set", &RegressionTreeLeave::train_set, OptionBase::buildoption,
+    declareOption(ol, "train_set", &RegressionTreeLeave::train_set, 
+                  OptionBase::buildoption | OptionBase::nosave,
                   "The train set with the sorted row index matrix and the leave id vector\n");
     declareOption(ol, "length", &RegressionTreeLeave::length, OptionBase::learntoption,
                   "The number of rows in this leave\n");
@@ -96,9 +97,9 @@
     declareOption(ol, "loss_function_factor", &RegressionTreeLeave::loss_function_factor, OptionBase::learntoption,
                   "2 / pow(loss_function_weight, 2.0).\n");
 
-    declareOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::buildoption,
+    declareOption(ol, "output", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
-    declareOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::buildoption,
+    declareOption(ol, "error", &RegressionTreeLeave::dummy_vec, OptionBase::nosave,
                   "DEPRECATED");
 
     inherited::declareOptions(ol);

Modified: trunk/plearn_learners/regressors/RegressionTreeNode.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-07-18 19:21:55 UTC (rev 9269)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.cc	2008-07-18 20:24:23 UTC (rev 9270)
@@ -80,7 +80,8 @@
                   "The desired level of verbosity\n");
     declareOption(ol, "leave_template", &RegressionTreeNode::leave, OptionBase::buildoption,
                   "The template for the leave objects to create.\n");
-    declareOption(ol, "train_set", &RegressionTreeNode::train_set, OptionBase::buildoption,
+    declareOption(ol, "train_set", &RegressionTreeNode::train_set,
+                  OptionBase::buildoption | OptionBase::nosave,
                   "The matrix with the sorted train set\n");
     declareOption(ol, "leave", &RegressionTreeNode::leave, OptionBase::buildoption,
                   "The leave of all the  belonging rows when this node is a leave\n");
@@ -211,6 +212,7 @@
 
     leave->getOutputAndError(leave_output,leave_error);
 }
+
 #define BY_ROW
 //#define RCMP
 void RegressionTreeNode::lookForBestSplit()
@@ -300,6 +302,7 @@
         <<row_split_value[27]<<endl;
 #endif
 }
+
 tuple<real,real,int>RegressionTreeNode::bestSplitInRow(
     int col,
     TVec<RTR_type>& candidates,



From tihocan at mail.berlios.de  Sat Jul 19 22:59:14 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Sat, 19 Jul 2008 22:59:14 +0200
Subject: [Plearn-commits] r9271 -
	trunk/plearn_learners/classifiers/EXPERIMENTAL
Message-ID: <200807192059.m6JKxEvI012470@sheep.berlios.de>

Author: tihocan
Date: 2008-07-19 22:59:14 +0200 (Sat, 19 Jul 2008)
New Revision: 9271

Modified:
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
Log:
Fixed start number of stages

Modified: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-18 20:24:23 UTC (rev 9270)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-19 20:59:14 UTC (rev 9271)
@@ -251,9 +251,9 @@
         // Oracle.
         PP<EarlyStoppingOracle> early_stop = new EarlyStoppingOracle();
         early_stop->max_degraded_steps = this->max_degraded_steps;
-        early_stop->range = TVec<double>(3, 1);
+        early_stop->range = TVec<double>(3, this->step_size);
         early_stop->range[1] = this->max_epochs + 1;
-        early_stop->range[2] = this->step_size;
+        early_stop->option = "nstages";
         early_stop->build();
         // Strategy.
         PP<HyperOptimize> strategy = new HyperOptimize();



From tihocan at mail.berlios.de  Mon Jul 21 17:22:02 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Jul 2008 17:22:02 +0200
Subject: [Plearn-commits] r9272 - trunk/plearn/vmat
Message-ID: <200807211522.m6LFM2vS027963@sheep.berlios.de>

Author: tihocan
Date: 2008-07-21 17:22:01 +0200 (Mon, 21 Jul 2008)
New Revision: 9272

Modified:
   trunk/plearn/vmat/TextFilesVMatrix.cc
Log:
- Typo fixes ('their' -> 'there')
- Indent fixes


Modified: trunk/plearn/vmat/TextFilesVMatrix.cc
===================================================================
--- trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-19 20:59:14 UTC (rev 9271)
+++ trunk/plearn/vmat/TextFilesVMatrix.cc	2008-07-21 15:22:01 UTC (rev 9272)
@@ -477,7 +477,12 @@
             if(fieldspec[i].second=="char" && mapping[i].empty())//should add auto when it is char that are selected
                 PLWARNING("In TextFilesVMatrix::autoBuildMappings - mapping already existing but not for field %d (%s)",i,fieldspec[i].first.c_str());
 
-        PLWARNING("In TextFilesVMatrix::autoBuildMappings - The existing mapping is not complete! Their is %d fields with build mapping and their is %d fields that do not need mapping with a total of %d fields. Erase the mapping directory in the metadatadir to have it regenerated next time!",nb_already_exist,nb_type_no_mapping,mapping.length());
+        PLWARNING("In TextFilesVMatrix::autoBuildMappings - The existing "
+                "mapping is not complete! There are %d fields with build "
+                "mapping and there are %d fields that do not need mapping "
+                "in a total of %d fields. Erase the mapping directory in "
+                "the metadatadir to have it regenerated next time!",
+                nb_already_exist,nb_type_no_mapping,mapping.length());
     }//else already build
 }
 
@@ -862,7 +867,8 @@
                   "Currently supported types: \n"
                   "- skip       : Ignore the content of the field, won't be inserted in the resulting VMat\n"
                   "- auto       : If a numeric value, keep it as is, if not, look it up in the mapping (possibly inserting a new mapping if it's not there) \n"
-                  "- auto-num   : take any float value with the decimal separator as the dot. If theirs is a $ at the start or end it is removed. If their is comma they are removed.\n"
+                  "- auto-num   : take any float value with the decimal separator as the dot. If there is a $\n"
+                  "               at the start or end it is removed. If there are commas they are removed.\n"
                   "- num        : numeric value, keep as is\n"
                   "- num-comma  : numeric value where thousands are separeted by comma\n"
                   "- char       : look it up in the mapping (possibly inserting a new mapping if it's not there)\n"
@@ -905,7 +911,7 @@
     declareOption(ol, "default_spec", 
                   &TextFilesVMatrix::default_spec,
                   OptionBase::buildoption,
-                  "If their is no fieldspec for a fieldname, we will use this"
+                  "If there is no fieldspec for a fieldname, we will use this"
                   "value. reorder_fieldspec_from_headers must be true.");
 
     // Now call the parent class' declareOptions



From nouiz at mail.berlios.de  Mon Jul 21 17:32:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jul 2008 17:32:07 +0200
Subject: [Plearn-commits] r9273 - trunk/plearn_learners/hyper
Message-ID: <200807211532.m6LFW7oe029132@sheep.berlios.de>

Author: nouiz
Date: 2008-07-21 17:32:05 +0200 (Mon, 21 Jul 2008)
New Revision: 9273

Modified:
   trunk/plearn_learners/hyper/HyperOptimize.cc
Log:
added check for error


Modified: trunk/plearn_learners/hyper/HyperOptimize.cc
===================================================================
--- trunk/plearn_learners/hyper/HyperOptimize.cc	2008-07-21 15:22:01 UTC (rev 9272)
+++ trunk/plearn_learners/hyper/HyperOptimize.cc	2008-07-21 15:32:05 UTC (rev 9273)
@@ -256,6 +256,10 @@
         if(isfile(fname)){
             //we reload the old version if it exist
             resultsmat = new FileVMatrix(fname, true);
+            if(resultsmat.width()!=w)
+                PLERROR("In HyperOptimize::getResultsMat() - The existing "
+                        "results mat that we should reload don't have the "
+                        "width that we need. Did you added some statnames?");
             return;
         }else
             resultsmat = new FileVMatrix(fname,0,w);



From tihocan at mail.berlios.de  Mon Jul 21 17:32:41 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Jul 2008 17:32:41 +0200
Subject: [Plearn-commits] r9274 -
	trunk/plearn_learners/classifiers/EXPERIMENTAL
Message-ID: <200807211532.m6LFWfXE029240@sheep.berlios.de>

Author: tihocan
Date: 2008-07-21 17:32:41 +0200 (Mon, 21 Jul 2008)
New Revision: 9274

Modified:
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h
Log:
Using conjugate gradient now

Modified: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-21 15:32:05 UTC (rev 9273)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-21 15:32:41 UTC (rev 9274)
@@ -38,7 +38,7 @@
 
 
 #include "KFoldLogisticClassifier.h"
-#include <plearn/opt/GradientOptimizer.h>
+#include <plearn/opt/ConjGradientOptimizer.h>
 #include <plearn/vmat/ExplicitSplitter.h>
 #include <plearn/vmat/KFoldSplitter.h>
 #include <plearn_learners/generic/NNet.h>
@@ -64,9 +64,8 @@
 /////////////////////////////
 KFoldLogisticClassifier::KFoldLogisticClassifier():
     kfold(5),
-    learning_rate(1e-3),
-    max_degraded_steps(100),
-    max_epochs(1000),
+    max_degraded_steps(20),
+    max_epochs(500),
     step_size(1)
 {
 }
@@ -82,10 +81,6 @@
                   OptionBase::buildoption,
         "Number of splits of the data (and of classifiers being trained).");
 
-    declareOption(ol, "learning_rate", &KFoldLogisticClassifier::learning_rate,
-                  OptionBase::buildoption,
-        "Learning rate in the logistic classifiers.");
-
     declareOption(ol, "max_degraded_steps",
                   &KFoldLogisticClassifier::max_degraded_steps,
                   OptionBase::buildoption,
@@ -214,8 +209,7 @@
     log_net.resize(0);
     string cost_func;
     for (int k = 0; k < kfold; k++) {
-        PP<GradientOptimizer> opt = new GradientOptimizer();
-        opt->start_learning_rate = this->learning_rate;
+        PP<ConjGradientOptimizer> opt = new ConjGradientOptimizer();
         opt->build();
         PP<NNet> nnet = new NNet();
         nnet->optimizer = opt;
@@ -230,6 +224,7 @@
             nnet->noutputs = n_classes;
         }
         nnet->cost_funcs = TVec<string>(1, cost_func);
+        nnet->batch_size = 0;
         nnet->build();
         log_net.append(get_pointer(nnet));
     }

Modified: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h	2008-07-21 15:32:05 UTC (rev 9273)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.h	2008-07-21 15:32:41 UTC (rev 9274)
@@ -62,7 +62,6 @@
     //#####  Public Build Options  ############################################
 
     int kfold;
-    real learning_rate;
     int max_degraded_steps;
     int max_epochs;
     int step_size;



From nouiz at mail.berlios.de  Mon Jul 21 19:04:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jul 2008 19:04:34 +0200
Subject: [Plearn-commits] r9275 - trunk/plearn/vmat
Message-ID: <200807211704.m6LH4YG8011982@sheep.berlios.de>

Author: nouiz
Date: 2008-07-21 19:04:32 +0200 (Mon, 21 Jul 2008)
New Revision: 9275

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
Log:
in MeanMedianModeImputationVMatrix correctly handle the case where the train_set don't have a metadata dir. In this case we give it one if we have one or when we receive one.


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-21 15:32:41 UTC (rev 9274)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-21 17:04:32 UTC (rev 9275)
@@ -332,17 +332,27 @@
       PLWARNING("In MeanMedianModeImputationVMatrix::build_() In the source VMatrix their is %d field(s) that do not have instruction: '%s'.",
 		no_instruction.size(),tostring(no_instruction).c_str());
 
-    PPath train_metadata = train_set->getMetaDataDir();
-    PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
-    train_set->lockMetaDataDir();
-    if (!train_set->isUpToDate(mean_median_mode_file_name)
-	||!source->isUpToDate(mean_median_mode_file_name))
+}
+void MeanMedianModeImputationVMatrix::setMetaDataDir(const PPath& the_metadatadir)
+{
+  inherited::setMetaDataDir(the_metadatadir);
+  if(!train_set->hasMetaDataDir() && !hasMetaDataDir())
+    PLERROR("In MeanMedianModeImputationVMatrix::setMetaDataDir() - the "
+	    " train_set should have a metadata dir or we should have one!");
+  else if(!train_set->hasMetaDataDir())
+    train_set->setMetaDataDir(getMetaDataDir()/"train_set");
+  
+  PPath train_metadata = train_set->getMetaDataDir();
+  PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
+  train_set->lockMetaDataDir();
+  if (!train_set->isUpToDate(mean_median_mode_file_name)
+      ||!source->isUpToDate(mean_median_mode_file_name))
     {
-        computeMeanMedianModeVectors();
-        createMeanMedianModeFile(mean_median_mode_file_name);
+      computeMeanMedianModeVectors();
+      createMeanMedianModeFile(mean_median_mode_file_name);
     }
-    else loadMeanMedianModeFile(mean_median_mode_file_name);
-    train_set->unlockMetaDataDir();
+  else loadMeanMedianModeFile(mean_median_mode_file_name);
+  train_set->unlockMetaDataDir();
 }
 
 void MeanMedianModeImputationVMatrix::createMeanMedianModeFile(PPath file_name)

Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-07-21 15:32:41 UTC (rev 9274)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.h	2008-07-21 17:04:32 UTC (rev 9275)
@@ -106,6 +106,7 @@
   VMat                 mean_median_mode_file;
 
           void         build_();
+  virtual void setMetaDataDir(const PPath& the_metadatadir);
           void         createMeanMedianModeFile(PPath file_name); 
           void         loadMeanMedianModeFile(PPath file_name); 
           void         computeMeanMedianModeVectors();  



From nouiz at mail.berlios.de  Mon Jul 21 19:05:51 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 21 Jul 2008 19:05:51 +0200
Subject: [Plearn-commits] r9276 - trunk/plearn/vmat
Message-ID: <200807211705.m6LH5pfg013103@sheep.berlios.de>

Author: nouiz
Date: 2008-07-21 19:05:50 +0200 (Mon, 21 Jul 2008)
New Revision: 9276

Modified:
   trunk/plearn/vmat/SelectColumnsVMatrix.cc
   trunk/plearn/vmat/SelectColumnsVMatrix.h
Log:
locallized some functionnality in a function. 


Modified: trunk/plearn/vmat/SelectColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-07-21 17:04:32 UTC (rev 9275)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.cc	2008-07-21 17:05:50 UTC (rev 9276)
@@ -214,62 +214,7 @@
     if (source) {
         updateMtime(source);
         if (fields.isNotEmpty()) {
-            // Find out the indices from the fields.
-            indices.resize(0);
-            if (!fields_partial_match) {
-                TVec<string> missing_field;
-                for (int i = 0; i < fields.length(); i++) {
-                    string the_field = fields[i];
-                    int the_index = source->fieldIndex(the_field);  // string only
-                    if (!extend_with_missing && the_index == -1) {
-                        // The_field does not exist AS A STRING in the vmat
-                        // It may be of the form FIELD1-FIELDN (a range of fields).
-                        size_t pos = the_field.find('-');
-                        bool ok = false;
-                        if (pos != string::npos) {
-                            string field1 = the_field.substr(0, pos);
-                            string fieldn = the_field.substr(pos + 1);
-                            int the_index1 = source->getFieldIndex(field1);  // either string or number
-                            int the_indexn = source->getFieldIndex(fieldn);  // either string or number
-                            if (the_index1 >= 0 && the_indexn > the_index1) {
-                                // Indeed, this is a range.
-                                ok = true;
-                                for (int j = the_index1; j <= the_indexn; j++)
-                                    indices.append(j);
-                            }
-                        }
-                        // OR it may be a number by itself only
-                        else if (pl_islong(the_field) &&
-                                 (the_index = source->getFieldIndex(the_field)) != -1)
-                        {
-                            ok = true;
-                            indices.append(the_index);
-                        }
-                        if (!ok)
-                            PLERROR("In SelectColumnsVMatrix::build_ - Unknown field \"%s\" in source VMat;\n"
-                                    "    (you may want to use the 'extend_with_missing' option)", the_field.c_str());
-                    } else
-                        indices.append(the_index);
-                    if(extend_with_missing && the_index == -1)
-                        missing_field.append(the_field);
-                }
-                if(missing_field.size()>0){
-                    PLWARNING("In SelectColumnsVMatrix::build_() - We are"
-                              " adding %d columns to the source matrix with missing values."
-                              " The columns names are: %s",missing_field.size(),
-                              tostring(missing_field).c_str());
-                }
-            } else {
-                // We need to check whether or not we should add each field.
-                TVec<string> source_fields = source->fieldNames();
-                for (int i = 0; i < source_fields.length(); i++)
-                    for (int j = 0; j < fields.length(); j++)
-                        if (source_fields[i].find(fields[j]) != string::npos) {
-                            // We want to add this field.
-                            indices.append(i);
-                            break;
-                        }
-            }
+            getIndicesFromFields();
         }
         
         //we inverse the selection
@@ -363,6 +308,65 @@
     }
 }
 
+void SelectColumnsVMatrix::getIndicesFromFields(){
+            // Find out the indices from the fields.
+    indices.resize(0);
+    if (!fields_partial_match) {
+        TVec<string> missing_field;
+        for (int i = 0; i < fields.length(); i++) {
+            string the_field = fields[i];
+            int the_index = source->fieldIndex(the_field);  // string only
+            if (!extend_with_missing && the_index == -1) {
+                // The_field does not exist AS A STRING in the vmat
+                // It may be of the form FIELD1-FIELDN (a range of fields).
+                size_t pos = the_field.find('-');
+                bool ok = false;
+                if (pos != string::npos) {
+                    string field1 = the_field.substr(0, pos);
+                    string fieldn = the_field.substr(pos + 1);
+                    int the_index1 = source->getFieldIndex(field1);  // either string or number
+                    int the_indexn = source->getFieldIndex(fieldn);  // either string or number
+                    if (the_index1 >= 0 && the_indexn > the_index1) {
+                        // Indeed, this is a range.
+                        ok = true;
+                        for (int j = the_index1; j <= the_indexn; j++)
+                            indices.append(j);
+                    }
+                }
+                // OR it may be a number by itself only
+                else if (pl_islong(the_field) &&
+                         (the_index = source->getFieldIndex(the_field)) != -1)
+                {
+                    ok = true;
+                    indices.append(the_index);
+                }
+                if (!ok)
+                    PLERROR("In SelectColumnsVMatrix::build_ - Unknown field \"%s\" in source VMat;\n"
+                            "    (you may want to use the 'extend_with_missing' option)", the_field.c_str());
+            } else
+                indices.append(the_index);
+            if(extend_with_missing && the_index == -1)
+                missing_field.append(the_field);
+        }
+        if(missing_field.size()>0){
+            PLWARNING("In SelectColumnsVMatrix::build_() - We are"
+                      " adding %d columns to the source matrix with missing values."
+                      " The columns names are: %s",missing_field.size(),
+                      tostring(missing_field).c_str());
+        }
+    } else {
+        // We need to check whether or not we should add each field.
+        TVec<string> source_fields = source->fieldNames();
+        for (int i = 0; i < source_fields.length(); i++)
+            for (int j = 0; j < fields.length(); j++)
+                if (source_fields[i].find(fields[j]) != string::npos) {
+                    // We want to add this field.
+                    indices.append(i);
+                    break;
+                }
+    }
+}
+
 ////////////////////////////
 // getStringToRealMapping //
 ////////////////////////////

Modified: trunk/plearn/vmat/SelectColumnsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-07-21 17:04:32 UTC (rev 9275)
+++ trunk/plearn/vmat/SelectColumnsVMatrix.h	2008-07-21 17:05:50 UTC (rev 9276)
@@ -104,6 +104,9 @@
 
     static void declareOptions(OptionList &ol);
     void getNewRow(int i, const Vec& v) const { getSubRow(i, 0, v); }
+    //!fill this.indices from this.fields
+    //!We do the partial match if this.fields_partial_match is true.
+    void getIndicesFromFields();
 
 public:
 



From tihocan at mail.berlios.de  Mon Jul 21 20:16:28 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 21 Jul 2008 20:16:28 +0200
Subject: [Plearn-commits] r9277 -
	trunk/plearn_learners/classifiers/EXPERIMENTAL
Message-ID: <200807211816.m6LIGSRd012314@sheep.berlios.de>

Author: tihocan
Date: 2008-07-21 20:16:27 +0200 (Mon, 21 Jul 2008)
New Revision: 9277

Modified:
   trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
Log:
Now also compute classification error

Modified: trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc
===================================================================
--- trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-21 17:05:50 UTC (rev 9276)
+++ trunk/plearn_learners/classifiers/EXPERIMENTAL/KFoldLogisticClassifier.cc	2008-07-21 18:16:27 UTC (rev 9277)
@@ -294,19 +294,22 @@
                                            const Vec& target, Vec& costs) const
 {
     int t = int(round(target[0]));
-    costs.resize(1);
+    costs.resize(2);
     if (output.length() == 1) {
         // Binary classification.
+        PLASSERT(output[0] >= 0 && output[0] <= 1);
         if (t == 1)
             costs[0] = - pl_log(output[0]);
         else {
             PLASSERT(t == 0);
             costs[0] = - pl_log(1 - output[0]);
         }
+        costs[1] = (output[0] - 0.5) * (2 * target[0] - 1) >= 0 ? 0 : 1;
     } else {
         // More than two targets.
         PLASSERT(is_equal(sum(output), 1));
         costs[0] = - pl_log(output[t]);
+        costs[1] = argmax(output) == t ? 0 : 1;
     }
 }
 
@@ -316,8 +319,10 @@
 TVec<string> KFoldLogisticClassifier::getTestCostNames() const
 {
     static TVec<string> costs;
-    if (costs.isEmpty())
+    if (costs.isEmpty()) {
         costs.append("nll");
+        costs.append("class_error");
+    }
     return costs;
 }
 



From nouiz at mail.berlios.de  Tue Jul 22 20:43:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jul 2008 20:43:20 +0200
Subject: [Plearn-commits] r9278 - trunk/plearn/vmat
Message-ID: <200807221843.m6MIhKCj006804@sheep.berlios.de>

Author: nouiz
Date: 2008-07-22 20:43:19 +0200 (Tue, 22 Jul 2008)
New Revision: 9278

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
removed warning as we recompute the data if it is not uptodate. 


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-21 18:16:27 UTC (rev 9277)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-22 18:43:19 UTC (rev 9278)
@@ -345,8 +345,8 @@
   PPath train_metadata = train_set->getMetaDataDir();
   PPath mean_median_mode_file_name = train_metadata + "mean_median_mode_file.pmat";
   train_set->lockMetaDataDir();
-  if (!train_set->isUpToDate(mean_median_mode_file_name)
-      ||!source->isUpToDate(mean_median_mode_file_name))
+  if (!train_set->isUpToDate(mean_median_mode_file_name,false)
+      ||!source->isUpToDate(mean_median_mode_file_name,false))
     {
       computeMeanMedianModeVectors();
       createMeanMedianModeFile(mean_median_mode_file_name);



From nouiz at mail.berlios.de  Tue Jul 22 20:47:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 22 Jul 2008 20:47:24 +0200
Subject: [Plearn-commits] r9279 - trunk/plearn_learners/generic
Message-ID: <200807221847.m6MIlOY8008146@sheep.berlios.de>

Author: nouiz
Date: 2008-07-22 20:47:23 +0200 (Tue, 22 Jul 2008)
New Revision: 9279

Modified:
   trunk/plearn_learners/generic/AddCostToLearner.cc
Log:
added cost type1_err, type2_err, sensitivity, specificity, false_positive_rate, false_negative_rate


Modified: trunk/plearn_learners/generic/AddCostToLearner.cc
===================================================================
--- trunk/plearn_learners/generic/AddCostToLearner.cc	2008-07-22 18:43:19 UTC (rev 9278)
+++ trunk/plearn_learners/generic/AddCostToLearner.cc	2008-07-22 18:47:23 UTC (rev 9279)
@@ -145,6 +145,15 @@
         " - 'NLL': -log(o[t])\n"
         " - 'mse': the mean squared error (o - t)^2\n"
         " - 'squared_norm_reconstruction_error': | ||i||^2 - ||o||^2 |\n"
+        " - 'train_time': the time spend in the train() function\n"
+        " - 'type1_err': SUM[type1_err] will return the number of type 1 error(false positive).\n"
+        "                E[type1_err], will return  type1_err/NNONMISSING\n" 
+        "                you probably want false_positive_rate\n"
+        " - 'type2_err': idem as type1_err but for the type 2 error(false negative)\n" 
+        " - 'false_positive_rate': E[false_positive_rate] return nb of false pos/nb total of neg"
+        " - 'false_negative_rate': E[false_negative_rate] return nb of false neg/nb total of pos"
+        " - 'sensitivity': E[sensitivity] return nb true pos/nb total pos"
+        " - 'specificity': E[specificity] return nb true neg/nb total ng"
     );
 
     declareOption(ol, "force_output_to_target_interval", &AddCostToLearner::force_output_to_target_interval, OptionBase::buildoption,
@@ -187,8 +196,9 @@
         "classes.");
 
     declareOption(ol, "train_time",
-                  &AddCostToLearner::train_time, OptionBase::buildoption,
+                  &AddCostToLearner::train_time, OptionBase::learntoption,
                   "The time it took to train in second.");
+
     // Now call the parent class' declareOptions
     inherited::declareOptions(ol);
 }
@@ -213,6 +223,7 @@
     int n = costs.length();
     int min_verb = 2;
     bool display = (verbosity >= min_verb);
+    bool activ_profiler=false;
     int os = learner_->outputsize();
     if (os < 0) {
         // The sub-learner does not know its outputsize yet: we skip the build for
@@ -258,23 +269,45 @@
         } else if (c == "class_error") {
         } else if (c == "binary_class_error") {
         } else if (c == "train_time") {
+            activ_profiler=true;
         } else if (c == "linear_class_error") {
         } else if (c == "square_class_error") {
         } else if (c == "confusion_matrix") {
             if(n_classes<=0)
                 PLERROR("In AddCostToLearner::build_ there must be a positive number of class. n_classes ="+n_classes);
+            output_min = 0;
+            output_max = n_classes;
         } else if (c == "NLL") {
             // Output should be in [0,1].
             output_min = max(output_min, real(0));
             output_max = min(output_max, real(1));
+        } else if (c == "type1_err") {
+            output_min = 0;
+            output_max = 1;
+        } else if (c == "type2_err") {
+            output_min = 0;
+            output_max = 1;
+        } else if (c == "false_negative_rate") {
+            output_min = 0;
+            output_max = 1;
+        } else if (c == "false_positive_rate") {
+            output_min = 0;
+            output_max = 1;
+        } else if (c == "sensitivity") {
+            output_min = 0;
+            output_max = 1;
+        } else if (c == "specificity") {
+            output_min = 0;
+            output_max = 1;
         } else {
-            PLERROR("In AddCostToLearner::build_ - Invalid cost requested (make sure you are using the new costs syntax)");
+            PLERROR("In AddCostToLearner::build_ - Invalid cost requested %s (make sure you are using the new costs syntax)",c.c_str());
         }
     }
     if (n > 0 && display) {
         cout << endl;
     }
-    Profiler::activate();
+    if(activ_profiler)
+        Profiler::activate();
 }
 
 /////////////////////////////
@@ -596,6 +629,93 @@
             costs[ind_cost] = abs(pownorm(input, 2) - pownorm(sub_learner_output, 2));
         } else if (c == "train_time") {
             costs[ind_cost] = train_time;
+        } else if (c == "type1_err") {
+            //false positive error
+#ifdef BOUNDCHECK
+            PLASSERT(sub_learner_output.length()==1);
+#endif
+            real target=desired_target[0];
+            real out=sub_learner_output[0];
+            if( fast_is_equal(target,0) && fast_is_equal(out,1))
+                costs[ind_cost] = 1;
+            else
+                costs[ind_cost] = 0;
+        } else if (c == "type2_err") {
+            //false negative error
+#ifdef BOUNDCHECK
+            PLASSERT(sub_learner_output.length()==1);
+#endif
+            real target=desired_target[0];
+            real out=sub_learner_output[0];
+            if( fast_is_equal(target,1) && fast_is_equal(out,0))
+                costs[ind_cost] = 1;
+            else
+                costs[ind_cost] = 0;
+        } else if (c == "false_positive_rate") {
+            //=1 - the specificity
+            //nb of false pos/nb total of neg
+            //should use X[test1.E[false_positive_rate]] to have the real value
+#ifdef BOUNDCHECK
+            PLASSERT(sub_learner_output.length()==1);
+#endif
+            real target=desired_target[0];
+            real out=sub_learner_output[0];
+
+            if( fast_is_equal(target,0) && fast_is_equal(out,1))
+                costs[ind_cost] = 1;                
+            else if( fast_is_equal(target, 0))
+                costs[ind_cost] = 0;
+            else
+                costs[ind_cost] = MISSING_VALUE;
+        } else if (c == "false_negative_rate") {
+            //nb of false neg/nb total of pos
+            //should use X[test1.E[false_nagative_rate]] to have the real value
+#ifdef BOUNDCHECK
+            PLASSERT(sub_learner_output.length()==1);
+#endif
+            real target=desired_target[0];
+            real out=sub_learner_output[0];
+
+             if( fast_is_equal(target,1) && fast_is_equal(out,0))
+                costs[ind_cost] = 1;                
+            else if( fast_is_equal(target, 1))
+                costs[ind_cost] = 0;
+            else
+                costs[ind_cost] = MISSING_VALUE;
+        } else if (c == "sensitivity") {
+            //nb true pos/(nb true pos + nb false neg)
+            //equiv to=nb true pos/nb total pos
+            //should use X[test1.E[sensitivity]] to have the real value
+#ifdef BOUNDCHECK
+            PLASSERT(sub_learner_output.length()==1);
+#endif
+            real target=desired_target[0];
+            real out=sub_learner_output[0];
+                
+            if(fast_is_equal(target,1)){
+                if(fast_is_equal(out,1))
+                    costs[ind_cost] = 1;
+                else
+                    costs[ind_cost] = 0;
+            }else
+                costs[ind_cost] = MISSING_VALUE;
+        } else if (c == "specificity") {
+            //nb true neg/(nb true neg + nb false pos)
+            //equiv to=nb true neg/nb total ng
+            //should use X[test1.E[specificity]] to have the real value
+#ifdef BOUNDCHECK
+            PLASSERT(sub_learner_output.length()==1);
+#endif
+            real target=desired_target[0];
+            real out=sub_learner_output[0];
+             
+            if( fast_is_equal(target, 0)){
+                if(fast_is_equal(out, 0))
+                    costs[ind_cost] = 1;
+                else
+                costs[ind_cost] = 0;
+            } else
+                costs[ind_cost] = MISSING_VALUE;
         } else {
             PLERROR("In AddCostToLearner::computeCostsFromOutputs - Unknown cost");
         }



From nouiz at mail.berlios.de  Thu Jul 24 17:45:34 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 24 Jul 2008 17:45:34 +0200
Subject: [Plearn-commits] r9280 - trunk/plearn/vmat
Message-ID: <200807241545.m6OFjYUg001023@sheep.berlios.de>

Author: nouiz
Date: 2008-07-24 17:45:33 +0200 (Thu, 24 Jul 2008)
New Revision: 9280

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
if their is an error, we erase the file that we are saving in case it is incomplet


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-22 18:47:23 UTC (rev 9279)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-24 15:45:33 UTC (rev 9280)
@@ -1636,6 +1636,10 @@
     }catch(const PLearnError& e){
         if(!metadatadir.isEmpty())
             unlockMetaDataDir();
+        //we erase the file if we are creating it
+        // as it can be partilly saved.
+        if(!uptodate && isfile(statsfile))
+            rm(statsfile);
         throw e;
     }
     if (!metadatadir.isEmpty())



From nouiz at mail.berlios.de  Fri Jul 25 19:32:51 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Jul 2008 19:32:51 +0200
Subject: [Plearn-commits] r9281 - trunk/plearn/vmat
Message-ID: <200807251732.m6PHWpAr002653@sheep.berlios.de>

Author: nouiz
Date: 2008-07-25 19:32:50 +0200 (Fri, 25 Jul 2008)
New Revision: 9281

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
In MeanMedianModeImputationVMatrix:
-corrected the check for update file
-removed some useless computation
-Load the train_set matrix in memory for the computation of the data. As we travers the matrix in the columns direction. This give great speed up. Meaby we could transpose it too as a speed up.



Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-24 15:45:33 UTC (rev 9280)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-25 17:32:50 UTC (rev 9281)
@@ -41,6 +41,7 @@
 
 
 #include "MeanMedianModeImputationVMatrix.h"
+#include "MemoryVMatrix.h"
 
 namespace PLearn {
 using namespace std;
@@ -365,8 +366,7 @@
 
 void MeanMedianModeImputationVMatrix::loadMeanMedianModeFile(PPath file_name)
 {
-    isUpToDate(file_name,true,true);
-    source->isUpToDate(file_name,true,true);
+    train_set->isUpToDate(file_name,true,true);
 
     mean_median_mode_file = new FileVMatrix(file_name);
     mean_median_mode_file->getRow(0, variable_mean);
@@ -382,18 +382,17 @@
 void MeanMedianModeImputationVMatrix::computeMeanMedianModeVectors()
 {
     TVec<int> variable_present_count(width_);
-    TVec<int> variable_missing_count(width_);
     TVec<int> variable_mode_count(width_);
     variable_mean.clear();
     variable_median.clear();
     variable_mode.clear();
     variable_present_count.clear();
-    variable_missing_count.clear();
     variable_mode_count.clear();
     Vec variable_vec(train_set->length());
     cout << fixed << showpoint;
     ProgressBar* pb = 0;
     pb = new ProgressBar("Computing the mean, median and mode vectors", width_);
+    VMat train_set = new MemoryVMatrix(train_set);
     for (int train_col = 0; train_col < width_; train_col++)
     {
         real current_value = 0.0;
@@ -403,10 +402,7 @@
         for (int train_row = 0; train_row < train_set->length(); train_row++)
         {
             if (is_missing(variable_vec[train_row]))
-            {
-                variable_missing_count[train_col] += 1;
-                continue;
-            }
+	      continue;
             variable_mean[train_col] += variable_vec[train_row];
             variable_present_count[train_col] += 1;
             if (variable_vec[train_row] != current_value)



From larocheh at mail.berlios.de  Fri Jul 25 21:49:16 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 25 Jul 2008 21:49:16 +0200
Subject: [Plearn-commits] r9282 - trunk/plearn/vmat
Message-ID: <200807251949.m6PJnGgn023463@sheep.berlios.de>

Author: larocheh
Date: 2008-07-25 21:49:16 +0200 (Fri, 25 Jul 2008)
New Revision: 9282

Added:
   trunk/plearn/vmat/LIBSVMSparseVMatrix.cc
   trunk/plearn/vmat/LIBSVMSparseVMatrix.h
Log:
VMatrix that reads its data from a LIBSVM file. This data is used in "sparse mode", i.e. getExamples (which contains the input values) and getExtra (which contains the indices of non-zero inputs) must be called, not getRow()...


Added: trunk/plearn/vmat/LIBSVMSparseVMatrix.cc
===================================================================
--- trunk/plearn/vmat/LIBSVMSparseVMatrix.cc	2008-07-25 17:32:50 UTC (rev 9281)
+++ trunk/plearn/vmat/LIBSVMSparseVMatrix.cc	2008-07-25 19:49:16 UTC (rev 9282)
@@ -0,0 +1,212 @@
+// -*- C++ -*-
+
+// LIBSVMSparseVMatrix.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file LIBSVMSparseVMatrix.cc */
+
+
+#include "LIBSVMSparseVMatrix.h"
+#include "plearn/io/openFile.h"
+#include "plearn/io/fileutils.h"
+
+namespace PLearn {
+using namespace std;
+
+
+PLEARN_IMPLEMENT_OBJECT(
+    LIBSVMSparseVMatrix,
+    "VMatrix containing data from a libsvm format file",
+    ""
+    );
+
+LIBSVMSparseVMatrix::LIBSVMSparseVMatrix()
+{
+}
+
+void LIBSVMSparseVMatrix::getNewRow(int i, const Vec& v) const
+{
+    PLERROR("In LIBSVMSparseVMatrix::getNewRow(): not implemented yet."); 
+}
+
+void LIBSVMSparseVMatrix::declareOptions(OptionList& ol)
+{
+    declareOption(ol, "class_strings", &LIBSVMSparseVMatrix::class_strings,
+                  OptionBase::buildoption,
+                  "Strings associated to the different classes.\n");
+
+    declareOption(ol, "libsvm_file", &LIBSVMSparseVMatrix::libsvm_file,
+                  OptionBase::buildoption,
+                  "File name of libsvm data.\n");
+
+    //declareOption(ol, "libsvm_input_data", 
+    //              &LIBSVMSparseVMatrix::libsvm_input_data,
+    //              OptionBase::learntoption,
+    //              "Input data.\n");
+    //
+    //declareOption(ol, "libsvm_extra_data", 
+    //              &LIBSVMSparseVMatrix::libsvm_extra_data,
+    //              OptionBase::learntoption,
+    //              "Extra data.\n");
+    //
+    //declareOption(ol, "libsvm_target_data", 
+    //              &LIBSVMSparseVMatrix::libsvm_target_data,
+    //              OptionBase::learntoption,
+    //              "Target data.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void LIBSVMSparseVMatrix::build_()
+{
+
+    // Read data
+    PStream libsvm_stream = openFile(libsvm_file, PStream::raw_ascii);
+    updateMtime(libsvm_file);
+    libsvm_stream.skipBlanks();
+    int input_index = 0;
+    int largest_input_index = 0;
+    int target_index = 0;
+    int n_inputs = 0;
+    string line;
+    vector<string> tokens; 
+    length_ = 0;
+    width_ = -1;
+    libsvm_input_data.resize(0);
+    libsvm_extra_data.resize(0);
+    libsvm_target_data.resize(0);
+    while(!libsvm_stream.eof())
+    {
+        libsvm_stream.getline(line);
+        line = removeblanks(line);
+        libsvm_stream.skipBlanks();
+        tokens = split(line,": ");
+        
+        // Get target
+        target_index = class_strings.find(tokens[0]);
+        if( target_index < 0)
+            PLERROR("In LIBSVMSparseVMatrix::build_(): target %s unknown",
+                    tokens[0].c_str());
+        
+        if( (tokens.size()-1)%2 != 0 )
+            PLERROR("In LIBSVMSparseVMatrix::build_(): line %s has incompatible "
+                    "format", line.c_str()); 
+        libsvm_target_data.push_back(target_index);
+
+        n_inputs = (tokens.size()-1)/2;
+        Vec input_vec(n_inputs);
+        Vec extra_vec(n_inputs);
+        // Get inputs
+        for( int i=0; i<n_inputs; i++)
+        {
+            input_index = toint(tokens[2*i+1]);
+            extra_vec[i] = input_index;
+            if( input_index > largest_input_index )
+                largest_input_index = input_index;
+            input_vec[i] = toreal(tokens[2*i+2]);
+        }
+        libsvm_input_data.push_back(input_vec);
+        libsvm_extra_data.push_back(extra_vec);
+        length_++;
+    }
+
+    // Set sizes
+    inputsize_ = largest_input_index+1;
+    targetsize_ = 1;
+    weightsize_ = 0;
+    extrasize_ = largest_input_index+1;
+}
+ 
+
+// ### Nothing to add here, simply calls build_
+void LIBSVMSparseVMatrix::build()
+{
+    inherited::build();
+    build_();
+}
+
+void LIBSVMSparseVMatrix::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(class_strings, copies);
+    deepCopyField(libsvm_file, copies);
+    deepCopyField(libsvm_input_data, copies);
+    deepCopyField(libsvm_extra_data, copies);
+    deepCopyField(libsvm_target_data, copies);
+}
+
+void LIBSVMSparseVMatrix::getExample(int i, Vec& input, Vec& target, real& weight)
+{
+    if( i>= length_ || i < 0 )
+        PLERROR("In LIBSVMSparseVMatrix::getExample(): row index should "
+                "be between 0 and length_-1");
+    input.resize(libsvm_input_data[i].length());
+    input << libsvm_input_data[i];
+    target.resize(targetsize_);
+    target[0] = libsvm_target_data[i];
+    weight = 1;
+}
+
+void LIBSVMSparseVMatrix::getExamples(int i_start, int length, Mat& inputs, Mat& targets,
+                          Vec& weights, Mat* extras, bool allow_circular)
+{
+    PLERROR("In LIBSVMSparseVMatrix::getExamples(): not compatible with "
+            "sparse inputs");    
+}
+
+void LIBSVMSparseVMatrix::getExtra(int i, Vec& extra)
+{
+    if( i>= length_ || i < 0 )
+        PLERROR("In LIBSVMSparseVMatrix::getExample(): row index should "
+                "be between 0 and length_-1");
+    extra.resize(libsvm_extra_data[i].length());
+    extra << libsvm_extra_data[i];
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/vmat/LIBSVMSparseVMatrix.h
===================================================================
--- trunk/plearn/vmat/LIBSVMSparseVMatrix.h	2008-07-25 17:32:50 UTC (rev 9281)
+++ trunk/plearn/vmat/LIBSVMSparseVMatrix.h	2008-07-25 19:49:16 UTC (rev 9282)
@@ -0,0 +1,144 @@
+// -*- C++ -*-
+
+// LIBSVMSparseVMatrix.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file LIBSVMSparseVMatrix.h */
+
+
+#ifndef LIBSVMSparseVMatrix_INC
+#define LIBSVMSparseVMatrix_INC
+
+#include <plearn/vmat/RowBufferedVMatrix.h>
+
+namespace PLearn {
+
+/**
+ * VMatrix containing data from a libsvm format file
+ */
+class LIBSVMSparseVMatrix : public RowBufferedVMatrix
+{
+    typedef RowBufferedVMatrix inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Strings associated to the different classes
+    TVec<string> class_strings;
+
+    //! File name of libsvm data
+    PPath libsvm_file;
+
+    //! Input data
+    TVec< Vec > libsvm_input_data;
+
+    //! Index of non-zero inputs
+    TVec< Vec > libsvm_extra_data;
+
+    //! Target data
+    Vec libsvm_target_data;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    LIBSVMSparseVMatrix();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(LIBSVMSparseVMatrix);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+    virtual void getExample(int i, Vec& input, Vec& target, real& weight);
+
+    void getExamples(int i_start, int length, Mat& inputs, Mat& targets,
+                     Vec& weights, Mat* extra = NULL,
+                     bool allow_circular = false);
+
+    virtual void getExtra(int i, Vec& extra);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    // (PLEASE IMPLEMENT IN .cc)
+    static void declareOptions(OptionList& ol);
+
+    //! Fill the vector 'v' with the content of the i-th row.
+    //! 'v' is assumed to be the right size.
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void getNewRow(int i, const Vec& v) const;
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    // (PLEASE IMPLEMENT IN .cc)
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(LIBSVMSparseVMatrix);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From larocheh at mail.berlios.de  Fri Jul 25 21:52:56 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 25 Jul 2008 21:52:56 +0200
Subject: [Plearn-commits] r9283 - trunk/plearn/vmat
Message-ID: <200807251952.m6PJqu42024139@sheep.berlios.de>

Author: larocheh
Date: 2008-07-25 21:52:56 +0200 (Fri, 25 Jul 2008)
New Revision: 9283

Modified:
   trunk/plearn/vmat/LIBSVMSparseVMatrix.cc
Log:
Oups, forgot that indices in LIBSVM are from 1 to N and must be converted to 0 to N-1.


Modified: trunk/plearn/vmat/LIBSVMSparseVMatrix.cc
===================================================================
--- trunk/plearn/vmat/LIBSVMSparseVMatrix.cc	2008-07-25 19:49:16 UTC (rev 9282)
+++ trunk/plearn/vmat/LIBSVMSparseVMatrix.cc	2008-07-25 19:52:56 UTC (rev 9283)
@@ -97,7 +97,7 @@
     updateMtime(libsvm_file);
     libsvm_stream.skipBlanks();
     int input_index = 0;
-    int largest_input_index = 0;
+    int largest_input_index = -1;
     int target_index = 0;
     int n_inputs = 0;
     string line;
@@ -131,7 +131,7 @@
         // Get inputs
         for( int i=0; i<n_inputs; i++)
         {
-            input_index = toint(tokens[2*i+1]);
+            input_index = toint(tokens[2*i+1])-1;
             extra_vec[i] = input_index;
             if( input_index > largest_input_index )
                 largest_input_index = input_index;



From larocheh at mail.berlios.de  Fri Jul 25 22:09:37 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 25 Jul 2008 22:09:37 +0200
Subject: [Plearn-commits] r9284 - trunk/plearn_learners/online
Message-ID: <200807252009.m6PK9bZQ026026@sheep.berlios.de>

Author: larocheh
Date: 2008-07-25 22:09:37 +0200 (Fri, 25 Jul 2008)
New Revision: 9284

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
Log:
Removed a useless comment...


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-07-25 19:52:56 UTC (rev 9283)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2008-07-25 20:09:37 UTC (rev 9284)
@@ -461,7 +461,6 @@
 real RBMBinomialLayer::fpropNLL(const Vec& target)
 {
     PLASSERT( target.size() == input_size );
-    //I'm here!!!
     real ret = 0;
     real target_i, activation_i;
     if( use_signed_samples )



From larocheh at mail.berlios.de  Fri Jul 25 22:10:37 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 25 Jul 2008 22:10:37 +0200
Subject: [Plearn-commits] r9285 - trunk/plearn_learners/online
Message-ID: <200807252010.m6PKAbfY026306@sheep.berlios.de>

Author: larocheh
Date: 2008-07-25 22:10:36 +0200 (Fri, 25 Jul 2008)
New Revision: 9285

Modified:
   trunk/plearn_learners/online/RBMWoodsLayer.cc
   trunk/plearn_learners/online/RBMWoodsLayer.h
Log:
Implemented the freeEnergy functions.


Modified: trunk/plearn_learners/online/RBMWoodsLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-07-25 20:09:37 UTC (rev 9284)
+++ trunk/plearn_learners/online/RBMWoodsLayer.cc	2008-07-25 20:10:36 UTC (rev 9285)
@@ -1028,38 +1028,146 @@
 real RBMWoodsLayer::freeEnergyContribution(const Vec& unit_activations)
     const
 {
-    PLERROR( "RBMWoodsLayer::freeEnergyContribution(): not implemeted yet" );
     PLASSERT( unit_activations.size() == size );
+    int n_nodes_per_tree = size / n_trees;
+    tree_free_energies.resize(n_trees);
+    tree_energies.resize(n_trees * (n_nodes_per_tree+1) );
 
-    // result = -\sum_{i=0}^{size-1} softplus(a_i)
+    int offset=0;
+    int sub_tree_size = n_nodes_per_tree / 2;
+    int sub_root = sub_tree_size;
     real result = 0;
-    real* a = unit_activations.data();
-    for (int i=0; i<size; i++)
+    real tree_energy = 0;
+    real tree_free_energy = 0;
+    real leaf_activation = 0;
+    for( int t = 0; t<n_trees; t++ )
     {
-        if (use_fast_approximations)
-            result -= tabulated_softplus(a[i]);
-        else
-            result -= softplus(a[i]);
+        for( int n = 0; n < n_nodes_per_tree; n = n+2 ) // Looking only at leaves
+        {
+            // Computation energy of tree
+            tree_energy = 0;
+            sub_tree_size = n_nodes_per_tree / 2;
+            sub_root = sub_tree_size;
+            for( int d=0; d<tree_depth-1; d++ )
+            {
+                if( n < sub_root )
+                {
+                    tree_energy -= unit_activations[offset+sub_root];
+                    sub_tree_size /= 2;
+                    sub_root -= sub_tree_size + 1;
+                }
+                else
+                {
+                    if( use_signed_samples )
+                        tree_energy -= -unit_activations[offset+sub_root];
+                    sub_tree_size /= 2;
+                    sub_root += sub_tree_size+1;
+                }
+            }
+            
+            leaf_activation = unit_activations[offset+n];
+            // Add free energy of tree with activated leaf
+            if( n == 0)
+                tree_free_energy = -tree_energy + leaf_activation;
+            else
+                tree_free_energy = logadd( -tree_energy + leaf_activation, 
+                                           tree_free_energy );
+            tree_energies[offset+t+n] = tree_energy - leaf_activation;
+
+            // Add free_energy of tree with inactivated leaf
+            if( use_signed_samples )
+            {
+                tree_free_energy = logadd( -tree_energy - leaf_activation, 
+                                           tree_free_energy );
+                tree_energies[offset+t+n+1] = tree_energy + leaf_activation;
+            }
+            else
+            {
+                tree_free_energy = logadd( -tree_energy, tree_free_energy );
+                tree_energies[offset+t+n+1] = tree_energy;
+            }
+        }
+        tree_free_energies[t] = -tree_free_energy;
+        result -= tree_free_energy;
+        offset += n_nodes_per_tree;
     }
     return result;
 }
 
+void RBMWoodsLayer::freeEnergyContributionGradient( 
+    const Vec& unit_activations,
+    Vec& unit_activations_gradient,
+    real output_gradient, bool accumulate) const
+{
+    PLASSERT( unit_activations.size() == size );
+    unit_activations_gradient.resize( size );
+    if( !accumulate ) unit_activations_gradient.clear();
+    
+    // This method assumes freeEnergyContribution() has been called before,
+    // with the same unit_activations vector!!!
+    
+    int n_nodes_per_tree = size / n_trees;
+    int offset=0;
+    int sub_tree_size = n_nodes_per_tree / 2;
+    int sub_root = sub_tree_size;
+    real tree_energy = 0;
+    real tree_energy_gradient = 0;
+    real tree_energy_leaf_on_gradient = 0;
+    real tree_energy_leaf_off_gradient = 0;
+    for( int t = 0; t<n_trees; t++ )
+    {
+        for( int n = 0; n < n_nodes_per_tree; n = n+2 ) // Looking only at leaves
+        {
+            // Computation energy of tree
+            tree_energy = 0;
+            sub_tree_size = n_nodes_per_tree / 2;
+            sub_root = sub_tree_size;            
+            tree_energy_leaf_on_gradient = output_gradient * 
+                safeexp(-tree_energies[offset+t+n] + tree_free_energies[t]);
+            tree_energy_leaf_off_gradient = output_gradient * 
+                safeexp(-tree_energies[offset+t+n+1] + tree_free_energies[t]);
+            tree_energy_gradient = tree_energy_leaf_on_gradient + 
+                tree_energy_leaf_off_gradient;
+            for( int d=0; d<tree_depth-1; d++ )
+            {
+                if( n < sub_root )
+                {
+                    unit_activations_gradient[offset+sub_root] -= 
+                        tree_energy_gradient;
+                    sub_tree_size /= 2;
+                    sub_root -= sub_tree_size + 1;
+                }
+                else
+                {
+                    if( use_signed_samples )
+                    {
+                        unit_activations_gradient[offset+sub_root] += 
+                            tree_energy_gradient;
+                    }
+                    sub_tree_size /= 2;
+                    sub_root += sub_tree_size+1;
+                }
+            }
+            
+            unit_activations_gradient[offset+n] -= tree_energy_leaf_on_gradient;
+
+            if( use_signed_samples )
+                unit_activations_gradient[offset+n] += tree_energy_leaf_off_gradient;
+        }
+        offset += n_nodes_per_tree;
+    }
+}
+
 int RBMWoodsLayer::getConfigurationCount()
 {
-    PLERROR( "RBMWoodsLayer::getConfigurationCount(): not implemeted yet" );
-    return size < 31 ? 1<<size : INFINITE_CONFIGURATIONS;
+    PLWARNING( "RBMWoodsLayer::getConfigurationCount(): getConfiguration() not "
+               " implemented yet, so outputs INFINITE_CONFIGURATIONS");
+    return INFINITE_CONFIGURATIONS;
 }
 
 void RBMWoodsLayer::getConfiguration(int conf_index, Vec& output)
 {
     PLERROR( "RBMWoodsLayer::getConfigurationCount(): not implemeted yet" );
-    PLASSERT( output.length() == size );
-    PLASSERT( conf_index >= 0 && conf_index < getConfigurationCount() );
-
-    for ( int i = 0; i < size; ++i ) {
-        output[i] = conf_index & 1;
-        conf_index >>= 1;
-    }
 }
 
 } // end of namespace PLearn

Modified: trunk/plearn_learners/online/RBMWoodsLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMWoodsLayer.h	2008-07-25 20:09:37 UTC (rev 9284)
+++ trunk/plearn_learners/online/RBMWoodsLayer.h	2008-07-25 20:10:36 UTC (rev 9285)
@@ -131,6 +131,15 @@
     //! the OTHER layer of an RBM, from which unit_activations was computed.
     virtual real freeEnergyContribution(const Vec& unit_activations) const;
 
+
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec& unit_activations,
+                                                Vec& unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false) const;
     virtual int getConfigurationCount();
 
     virtual void getConfiguration(int conf_index, Vec& output);
@@ -167,6 +176,10 @@
     Vec on_free_energy_gradient;
     Vec off_free_energy_gradient;
 
+    // Temporary computations, for freeEnergyContribution() and its gradient variant
+    mutable Vec tree_free_energies;
+    mutable Vec tree_energies;
+
 protected:
     //#####  Protected Member Functions  ######################################
 



From nouiz at mail.berlios.de  Fri Jul 25 22:10:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Jul 2008 22:10:52 +0200
Subject: [Plearn-commits] r9286 - trunk/python_modules/plearn/learners
Message-ID: <200807252010.m6PKAqfl026330@sheep.berlios.de>

Author: nouiz
Date: 2008-07-25 22:10:46 +0200 (Fri, 25 Jul 2008)
New Revision: 9286

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
added a print_time fct...


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-25 20:10:36 UTC (rev 9285)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-25 20:10:46 UTC (rev 9286)
@@ -1,6 +1,7 @@
 from plearn.pyext import *
 from plearn.pyplearn.plargs import *
 import time,os.path
+from numpy import array
 
 class AdaBoostMultiClasses:
     def __init__(self,trainSet1,trainSet2,weakLearner,confusion_target=1):
@@ -26,6 +27,8 @@
         self.nstages = 0
         self.stage = 0
         self.train_time = 0
+        self.test_time = 0
+        self.test_sub_time = 0
         self.confusion_target=confusion_target
         
     def myAdaBoostLearner(self,sublearner,trainSet):
@@ -167,6 +170,7 @@
         return (output,costs)
 
     def test(self,testSet,test_stats,return_outputs,return_costs):
+        t1=time.time()
         testSet1=pl.ProcessingVMatrix(source=testSet,
                                prg = "[%0:%"+str(testSet.inputsize-1)+"] @CLASSE_REEL 1 0 ifelse :CLASSE_REEL")
         testSet2=pl.ProcessingVMatrix(source=testSet,
@@ -174,12 +178,14 @@
 
         stats1=pl.VecStatsCollector()
         stats2=pl.VecStatsCollector()
+        t2=time.time()
         (test_stats1, testoutputs1, testcosts1)=self.learner1.test(testSet1,
                                                                    stats1,
                                                                    True,True)
         (test_stats2, testoutputs2, testcosts2)=self.learner2.test(testSet2,
                                                                    stats2,
                                                                    True,True)
+        t3=time.time()
         outputs=[]
         costs=[]
         #calculate stats, outputs, costs
@@ -209,6 +215,10 @@
             test_stats.update(cost,1)
             if return_costs:
                 costs.append(cost)
+        t4=time.time()
+        self.test_time+=t4-t1
+        self.test_sub_time+=t3-t2
+
         return(test_stats,outputs,costs)
     
     def outputsize(self):
@@ -275,3 +285,6 @@
         self.learner1.setTrainStatsCollector(VecStatsCollector())
         self.learner2.setTrainStatsCollector(VecStatsCollector())
 
+
+    def print_time(self):
+        print "train time=%.2f test time=%.2f test sub time=%.2f"%(self.train_time,self.test_time,self.test_sub_time)



From nouiz at mail.berlios.de  Fri Jul 25 22:11:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Jul 2008 22:11:48 +0200
Subject: [Plearn-commits] r9287 - trunk/plearn_learners/meta
Message-ID: <200807252011.m6PKBm8g026533@sheep.berlios.de>

Author: nouiz
Date: 2008-07-25 22:11:47 +0200 (Fri, 25 Jul 2008)
New Revision: 9287

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
added profiling


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-07-25 20:10:46 UTC (rev 9286)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-07-25 20:11:47 UTC (rev 9287)
@@ -275,6 +275,7 @@
 
 void AdaBoost::train()
 {
+
     if (nstages < stage){        //!< Asking to revert to previous stage
         PLCHECK(nstages>0); // should use forget
         cout<<"In AdaBoost::train() - reverting from stage "<<stage
@@ -299,6 +300,11 @@
         PLERROR("In AdaBoost::train() -  we can't retrain a reverted learner...");
     }
     
+    if(found_zero_error_weak_learner) // Training is over...
+        return;
+
+    Profiler::pl_profile_start("AdaBoost::train");
+
     if(!train_set)
         PLERROR("In AdaBoost::train, you did not setTrainingSet");
     
@@ -316,8 +322,6 @@
     PLCHECK_MSG(train_set->inputsize()>0, "In AdaBoost::train, the inputsize"
                 " of the train_set must be know.");
 
-    if(found_zero_error_weak_learner) // Training is over...
-        return;
 
     static Vec input;
     static Vec output;
@@ -659,6 +663,8 @@
 
 
     }
+    Profiler::pl_profile_end("AdaBoost::train");
+
 }
 
 



From larocheh at mail.berlios.de  Fri Jul 25 22:20:58 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 25 Jul 2008 22:20:58 +0200
Subject: [Plearn-commits] r9288 - trunk/plearn_learners/online
Message-ID: <200807252020.m6PKKwLt027645@sheep.berlios.de>

Author: larocheh
Date: 2008-07-25 22:20:57 +0200 (Fri, 25 Jul 2008)
New Revision: 9288

Added:
   trunk/plearn_learners/online/RBMRateLayer.cc
   trunk/plearn_learners/online/RBMRateLayer.h
Log:
Generalization of binary neurons to binomial neurons (i.e. neurons taking value in 0 and N, N being an hyper-parameter).



Added: trunk/plearn_learners/online/RBMRateLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMRateLayer.cc	2008-07-25 20:11:47 UTC (rev 9287)
+++ trunk/plearn_learners/online/RBMRateLayer.cc	2008-07-25 20:20:57 UTC (rev 9288)
@@ -0,0 +1,347 @@
+// -*- C++ -*-
+
+// RBMRateLayer.cc
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMRateLayer.cc */
+
+
+
+#include "RBMRateLayer.h"
+#include <plearn/math/TMat_maths.h>
+#include "RBMConnection.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    RBMRateLayer,
+    "Layer in an RBM consisting in rate-coded units",
+    "");
+
+RBMRateLayer::RBMRateLayer( real the_learning_rate ) :
+    inherited( the_learning_rate ),
+    n_spikes( 10 )
+{
+}
+
+void RBMRateLayer::generateSample()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectation_is_up_to_date, "Expectation should be computed "
+            "before calling generateSample()");
+
+    real exp_i = 0;
+    for( int i=0; i<size; i++)
+    {
+        exp_i = expectation[i];
+        sample[i] = round(random_gen->gaussian_mu_sigma(
+                              exp_i,exp_i*(1-exp_i/n_spikes)) );
+    }
+}
+
+void RBMRateLayer::generateSamples()
+{
+    PLASSERT_MSG(random_gen,
+                 "random_gen should be initialized before generating samples");
+
+    PLCHECK_MSG(expectations_are_up_to_date, "Expectations should be computed "
+                        "before calling generateSamples()");
+
+    PLASSERT( samples.width() == size && samples.length() == batch_size );
+
+    real exp_i = 0;
+    for (int k = 0; k < batch_size; k++)
+    {
+        for( int i=0; i<size; i++)
+        {
+            exp_i = expectations(k,i);
+            samples(k,i) = round(random_gen->gaussian_mu_sigma(
+                                     exp_i,exp_i*(1-exp_i/n_spikes)) );
+        }
+    }
+}
+
+void RBMRateLayer::computeExpectation()
+{
+    if( expectation_is_up_to_date )
+        return;
+
+    if (use_fast_approximations)
+        for(int i=0; i<size; i++)
+            expectation[i] = n_spikes*fastsigmoid(activation[i]);
+    else
+        for(int i=0; i<size; i++)
+            expectation[i] = n_spikes*sigmoid(activation[i]);
+    expectation_is_up_to_date = true;
+}
+
+void RBMRateLayer::computeExpectations()
+{
+    if( expectations_are_up_to_date )
+        return;
+
+    PLASSERT( expectations.width() == size
+              && expectations.length() == batch_size );
+
+    if (use_fast_approximations)
+        for (int k = 0; k < batch_size; k++)
+            for(int i=0; i<size; i++)
+                expectations(k,i) =  n_spikes*fastsigmoid(activations(k,i));
+    else
+        for (int k = 0; k < batch_size; k++)
+            for(int i=0; i<size; i++)
+                expectations(k,i) = n_spikes*sigmoid(activations(k,i));
+    expectations_are_up_to_date = true;
+}
+
+
+void RBMRateLayer::fprop( const Vec& input, Vec& output ) const
+{
+    PLASSERT( input.size() == input_size );
+    output.resize( output_size );
+    if (use_fast_approximations)
+        for(int i=0; i<size; i++)
+            output[i] = n_spikes*fastsigmoid(input[i]+bias[i]);
+    else
+        for(int i=0; i<size; i++)
+            output[i] = n_spikes*sigmoid(input[i]+bias[i]);
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void RBMRateLayer::bpropUpdate(const Vec& input, const Vec& output,
+                                      Vec& input_gradient,
+                                      const Vec& output_gradient,
+                                      bool accumulate)
+{
+    PLASSERT( input.size() == size );
+    PLASSERT( output.size() == size );
+    PLASSERT( output_gradient.size() == size );
+
+    if( accumulate )
+    {
+        PLASSERT_MSG( input_gradient.size() == size,
+                      "Cannot resize input_gradient AND accumulate into it" );
+    }
+    else
+    {
+        input_gradient.resize( size );
+        input_gradient.clear();
+    }
+
+    if( momentum != 0. )
+        bias_inc.resize( size );
+    
+    for( int i=0 ; i<size ; i++ )
+    {
+        real output_i = output[i];
+        real in_grad_i;
+        in_grad_i = output_i * (1-output_i) * output_gradient[i] * n_spikes;
+        input_gradient[i] += in_grad_i;
+        
+        if( momentum == 0. )
+        {
+            // update the bias: bias -= learning_rate * input_gradient
+            bias[i] -= learning_rate * in_grad_i;
+        }
+        else
+        {
+            // The update rule becomes:
+            // bias_inc = momentum * bias_inc - learning_rate * input_gradient
+            // bias += bias_inc
+            bias_inc[i] = momentum * bias_inc[i] - learning_rate * in_grad_i;
+            bias[i] += bias_inc[i];
+        }
+    }
+    applyBiasDecay();
+}
+
+void RBMRateLayer::bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate)
+{
+    PLERROR("In RBMRateLayer::bpropUpdate(): mini-batch version of bpropUpdate is not "
+            "implemented yet");
+}
+
+//////////////
+// fpropNLL //
+//////////////
+real RBMRateLayer::fpropNLL(const Vec& target)
+{
+    PLERROR("In RBMRateLayer::fpropNLL(): not implemented");
+    PLASSERT( target.size() == input_size );
+    real ret = 0;
+    real target_i, activation_i;
+    if(use_fast_approximations){
+        for( int i=0 ; i<size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += tabulated_softplus(activation_i) - target_i * activation_i;
+            // nll = - target*log(sigmoid(act)) -(1-target)*log(1-sigmoid(act))
+            // but it is numerically unstable, so use instead the following identity:
+            //     = target*softplus(-act) +(1-target)*(act+softplus(-act))
+            //     = act + softplus(-act) - target*act
+            //     = softplus(act) - target*act
+        }
+    } else {
+        for( int i=0 ; i<size ; i++ )
+        {
+            target_i = target[i];
+            activation_i = activation[i];
+            ret += softplus(activation_i) - target_i * activation_i;
+        }
+    }
+
+    return ret;
+}
+
+void RBMRateLayer::bpropNLL(const Vec& target, real nll,
+                                   Vec& bias_gradient)
+{
+    PLERROR("In RBMRateLayer::bpropNLL(): not implemented");
+    computeExpectation();
+
+    PLASSERT( target.size() == input_size );
+    bias_gradient.resize( size );
+
+    // bias_gradient = expectation - target
+    substract(expectation, target, bias_gradient);
+}
+
+void RBMRateLayer::declareOptions(OptionList& ol)
+{
+
+    declareOption(ol, "n_spikes", &RBMRateLayer::n_spikes,
+                  OptionBase::buildoption,
+                  "Maximum number of spikes for each neuron.\n");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void RBMRateLayer::build_()
+{
+    if( n_spikes < 1 )
+        PLERROR("In RBMRateLayer::build_(): n_spikes should be positive");
+}
+
+void RBMRateLayer::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void RBMRateLayer::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+    //deepCopyField(tmp_softmax, copies);
+}
+
+real RBMRateLayer::energy(const Vec& unit_values) const
+{
+    return -dot(unit_values, bias);
+}
+
+real RBMRateLayer::freeEnergyContribution(const Vec& unit_activations)
+    const
+{
+    PLASSERT( unit_activations.size() == size );
+
+    // result = -\sum_{i=0}^{size-1} softplus(a_i)
+    real result = 0;
+    real* a = unit_activations.data();
+    for (int i=0; i<size; i++)
+    {
+        if (use_fast_approximations)
+            result -= n_spikes*tabulated_softplus(a[i]);
+        else
+            result -= n_spikes*softplus(a[i]);
+    }
+    return result;
+}
+
+void RBMRateLayer::freeEnergyContributionGradient(
+    const Vec& unit_activations,
+    Vec& unit_activations_gradient,
+    real output_gradient, bool accumulate) const
+{
+    PLASSERT( unit_activations.size() == size );
+    unit_activations_gradient.resize( size );
+    if( !accumulate ) unit_activations_gradient.clear();
+    real* a = unit_activations.data();
+    real* ga = unit_activations_gradient.data();
+    for (int i=0; i<size; i++)
+    {
+        if (use_fast_approximations)
+            ga[i] -= output_gradient * n_spikes *
+                fastsigmoid( a[i] );
+        else
+            ga[i] -= output_gradient * n_spikes *
+                sigmoid( a[i] );
+    }
+}
+
+int RBMRateLayer::getConfigurationCount()
+{
+    return INFINITE_CONFIGURATIONS;
+}
+
+void RBMRateLayer::getConfiguration(int conf_index, Vec& output)
+{
+    PLERROR("In RBMRateLayer::getConfiguration(): not implemented");
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/RBMRateLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMRateLayer.h	2008-07-25 20:11:47 UTC (rev 9287)
+++ trunk/plearn_learners/online/RBMRateLayer.h	2008-07-25 20:20:57 UTC (rev 9288)
@@ -0,0 +1,175 @@
+// -*- C++ -*-
+
+// RBMRateLayer.h
+//
+// Copyright (C) 2008 Hugo Larochelle
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Hugo Larochelle
+
+/*! \file RBMRateLayer.h */
+
+
+#ifndef RBMRateLayer_INC
+#define RBMRateLayer_INC
+
+#include "RBMLayer.h"
+
+namespace PLearn {
+using namespace std;
+
+/**
+ * Layer in an RBM consisting in rate-coded units
+ */
+class RBMRateLayer: public RBMLayer
+{
+    typedef RBMLayer inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Maximum number of spikes for each neuron
+    int n_spikes;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    RBMRateLayer( real the_learning_rate=0. );
+
+    //! generate a sample, and update the sample field
+    virtual void generateSample();
+
+    //! batch version
+    virtual void generateSamples();
+
+    //! compute the expectation
+    virtual void computeExpectation();
+
+    //! batch version
+    virtual void computeExpectations();
+
+    //! forward propagation
+    virtual void fprop( const Vec& input, Vec& output ) const;
+
+    //! back-propagates the output gradient to the input
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient, const Vec& output_gradient,
+                             bool accumulate=false);
+
+    //! back-propagates the output gradient to the input, in mini-batch mode
+    virtual void bpropUpdate(const Mat& inputs, const Mat& outputs,
+                             Mat& input_gradients,
+                             const Mat& output_gradients,
+                             bool accumulate=false);
+
+    //! Computes the negative log-likelihood of target given the
+    //! internal activations of the layer
+    virtual real fpropNLL(const Vec& target);
+
+    //! Computes the gradient of the negative log-likelihood of target
+    //! with respect to the layer's bias, given the internal activations
+    virtual void bpropNLL(const Vec& target, real nll, Vec& bias_gradient);
+
+    // Compute -bias' unit_values
+    virtual real energy(const Vec& unit_values) const;
+
+    //! Computes -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! This quantity is used for computing the free energy of a sample x in
+    //! the OTHER layer of an RBM, from which unit_activations was computed.
+    virtual real freeEnergyContribution(const Vec& unit_activations) const;
+
+    //! Computes gradient of the result of freeEnergyContribution
+    //! -log(\sum_{possible values of h} exp(h' unit_activations))
+    //! with respect to unit_activations. Optionally, a gradient
+    //! with respect to freeEnergyContribution can be given
+    virtual void freeEnergyContributionGradient(const Vec& unit_activations,
+                                                Vec& unit_activations_gradient,
+                                                real output_gradient = 1,
+                                                bool accumulate = false) 
+        const;
+
+
+    virtual int getConfigurationCount();
+
+    virtual void getConfiguration(int conf_index, Vec& output);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(RBMRateLayer);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Not Options  #####################################################
+    mutable Vec tmp_softmax;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(RBMRateLayer);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From nouiz at mail.berlios.de  Fri Jul 25 22:52:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Fri, 25 Jul 2008 22:52:47 +0200
Subject: [Plearn-commits] r9289 - trunk/plearn_learners/meta
Message-ID: <200807252052.m6PKqlOv031368@sheep.berlios.de>

Author: nouiz
Date: 2008-07-25 22:52:46 +0200 (Fri, 25 Jul 2008)
New Revision: 9289

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
changed two fields to be a PP. This allow for setting there value in python.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-25 20:20:57 UTC (rev 9288)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-25 20:52:46 UTC (rev 9289)
@@ -201,20 +201,20 @@
     }
     */
     PLWARNING("In MultiClassAdaBoost::train() - not implemented, should be already trained");
-    int stage1=learner1.stage;
-    int stage2=learner2.stage;
+    int stage1=learner1->stage;
+    int stage2=learner2->stage;
 
     if(stage1>0 && stage1<nb_stage_to_use)
         PLERROR("In  MultiClassAdaBoost::train() - asked to use more stage then already trained for learner1");
     if(stage2>0 && stage2<nb_stage_to_use)
         PLERROR("In  MultiClassAdaBoost::train() - asked to use more stage then already trained for learner1");
     if(nb_stage_to_use>0){
-        learner1.nstages=nb_stage_to_use;
-        learner1.train();
+        learner1->nstages=nb_stage_to_use;
+        learner1->train();
     }
     if(nb_stage_to_use>0){
-        learner2.nstages=nb_stage_to_use;
-        learner2.train();
+        learner2->nstages=nb_stage_to_use;
+        learner2->train();
     }
 }
 
@@ -222,10 +222,10 @@
 {
     PLASSERT(output.size()==outputsize());
 
-    Vec output1(learner1.outputsize());
-    Vec output2(learner2.outputsize());
-    learner1.computeOutput(input,output1);
-    learner2.computeOutput(input,output2);
+    Vec output1(learner1->outputsize());
+    Vec output2(learner2->outputsize());
+    learner1->computeOutput(input,output1);
+    learner2->computeOutput(input,output2);
     int ind1=int(round(output1[0]));
     int ind2=int(round(output2[0]));
 
@@ -250,15 +250,15 @@
 
     output.resize(outputsize());
 
-    Vec output1(learner1.outputsize());
-    Vec output2(learner2.outputsize());
-    Vec subcosts1(learner1.nTestCosts());
-    Vec subcosts2(learner1.nTestCosts());
+    Vec output1(learner1->outputsize());
+    Vec output2(learner2->outputsize());
+    Vec subcosts1(learner1->nTestCosts());
+    Vec subcosts2(learner1->nTestCosts());
     getSubLearnerTarget(target, sub_target_tmp);
 
-    learner1.computeOutputAndCosts(input, sub_target_tmp[0],
+    learner1->computeOutputAndCosts(input, sub_target_tmp[0],
                                    output1, subcosts1);
-    learner2.computeOutputAndCosts(input, sub_target_tmp[1],
+    learner2->computeOutputAndCosts(input, sub_target_tmp[1],
                                    output2, subcosts2);
 
     int ind1=int(round(output1[0]));
@@ -325,12 +325,12 @@
     PLASSERT(nTestCosts()==costs.size());
     if(forward_sub_learner_test_costs){
         costs.resize(7);
-        Vec subcosts1(learner1.nTestCosts());
-        Vec subcosts2(learner1.nTestCosts());
+        Vec subcosts1(learner1->nTestCosts());
+        Vec subcosts2(learner1->nTestCosts());
         getSubLearnerTarget(target, sub_target_tmp);
 
-        learner1.computeCostsOnly(input,sub_target_tmp[0],subcosts1);
-        learner2.computeCostsOnly(input,sub_target_tmp[1],subcosts2);
+        learner1->computeCostsOnly(input,sub_target_tmp[0],subcosts1);
+        learner2->computeCostsOnly(input,sub_target_tmp[1],subcosts2);
         subcosts1+=subcosts2;
         costs.append(subcosts1);
     }
@@ -362,7 +362,7 @@
     names.append("class1");
     names.append("class2");
     if(forward_sub_learner_test_costs){
-        TVec<string> subcosts=learner1.getTestCostNames();
+        TVec<string> subcosts=learner1->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
             subcosts[i]="sum_sublearner."+subcosts[i];
         }
@@ -405,6 +405,7 @@
         PLERROR("In MultiClassAdaBoost::getSubLearnerTarget - "
                   "We only support target 0/1/2. We got %f.", target[0]); 
 }
+
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-07-25 20:20:57 UTC (rev 9288)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-07-25 20:52:46 UTC (rev 9289)
@@ -75,8 +75,8 @@
     bool forward_sub_learner_test_costs;
 
     //! The learner1 and learner2 must be trained!
-    AdaBoost learner1;
-    AdaBoost learner2;
+    PP<AdaBoost> learner1;
+    PP<AdaBoost> learner2;
 
 public:
     //#####  Public Member Functions  #########################################



From larocheh at mail.berlios.de  Fri Jul 25 23:49:16 2008
From: larocheh at mail.berlios.de (larocheh at BerliOS)
Date: Fri, 25 Jul 2008 23:49:16 +0200
Subject: [Plearn-commits] r9290 - trunk/plearn_learners_experimental
Message-ID: <200807252149.m6PLnGdn005188@sheep.berlios.de>

Author: larocheh
Date: 2008-07-25 23:49:16 +0200 (Fri, 25 Jul 2008)
New Revision: 9290

Modified:
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
   trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
Log:
Progressing towards a journal paper...


Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-07-25 20:52:46 UTC (rev 9289)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.cc	2008-07-25 21:49:16 UTC (rev 9290)
@@ -74,6 +74,7 @@
     pseudolikelihood_context_size ( 0 ),
     pseudolikelihood_context_type( "uniform_random" ),
     k_most_correlated( -1 ),
+    generative_learning_weight( 0 ),
     nll_cost_index( -1 ),
     class_cost_index( -1 ),
     training_cpu_time_cost_index ( -1 ),
@@ -193,6 +194,12 @@
                   "Number of most correlated input elements over which to sample.\n"
                   );
 
+    declareOption(ol, "generative_learning_weight", 
+                  &PseudolikelihoodRBM::generative_learning_weight,
+                  OptionBase::buildoption,
+                  "Weight of generative learning.\n"
+                  );
+
     declareOption(ol, "input_layer", &PseudolikelihoodRBM::input_layer,
                   OptionBase::buildoption,
                   "The binomial input layer of the RBM.\n");
@@ -217,6 +224,15 @@
 //                  OptionBase::learntoption,
 //                  "Cumulative testing time since age=0, in seconds.\n");
 
+
+    declareOption(ol, "target_layer", &PseudolikelihoodRBM::target_layer,
+                  OptionBase::learntoption,
+                  "The target layer of the RBM.\n");
+
+    declareOption(ol, "target_connection", &PseudolikelihoodRBM::target_connection,
+                  OptionBase::learntoption,
+                  "The connection weights between the target and hidden layer.\n");
+
     declareOption(ol, "log_Z", &PseudolikelihoodRBM::log_Z,
                   OptionBase::learntoption,
                   "Normalisation constant (on log scale).\n");
@@ -254,12 +270,7 @@
 
     if( inputsize_ > 0 && targetsize_ >= 0)
     {
-        if( n_classes > 1 && targetsize_ != 1 )
-            PLERROR("In PseudolikelihoodRBM::build_(): can't use supervised "
-                "learning (n_classes > 1) if there is no target field "
-                "(targetsize() != 1)");
-        
-        if( compute_input_space_nll && n_classes > 1 )
+        if( compute_input_space_nll && targetsize() > 0 )
             PLERROR("In PseudolikelihoodRBM::build_(): compute_input_space_nll "
                     "is not compatible with n_classes > 1");
 
@@ -295,14 +306,14 @@
     cost_names.resize(0);
     
     int current_index = 0;
-    if( compute_input_space_nll || n_classes > 1 )
+    if( compute_input_space_nll || targetsize() > 0 )
     {
         cost_names.append("NLL");
         nll_cost_index = current_index;
         current_index++;
     }
     
-    if( n_classes > 1 )
+    if( targetsize() > 0 )
     {
         cost_names.append("class_error");
         class_cost_index = current_index;
@@ -338,6 +349,54 @@
         PLERROR("In PseudolikelihoodRBM::build_layers_and_connections(): "
                 "hidden_layer must be provided");
 
+    if( targetsize() == 1 )
+    {
+        if( n_classes <= 1 )
+            PLERROR("In PseudolikelihoodRBM::build_layers_and_connections(): "
+                    "n_classes should be > 1");
+        if( target_layer->size != n_classes )
+        {
+            target_layer = new RBMMultinomialLayer();
+            target_layer->size = n_classes;
+            target_layer->random_gen = random_gen;
+            target_layer->build();
+            target_layer->forget();
+        }
+        
+        if( target_connection->up_size != hidden_layer->size ||
+            target_connection->down_size != target_layer->size )
+        {
+            target_connection = new RBMMatrixConnection(); 
+            target_connection->up_size = hidden_layer->size;
+            target_connection->down_size = target_layer->size;
+            target_connection->random_gen = random_gen;
+            target_connection->build();
+            target_connection->forget();
+        }
+    }
+    else if ( targetsize() > 1 )
+    {
+        if( target_layer->size != targetsize() )
+        {
+            target_layer = new RBMBinomialLayer();
+            target_layer->size = targetsize();
+            target_layer->random_gen = random_gen;
+            target_layer->build();
+            target_layer->forget();
+        }
+        
+        if( target_connection->up_size != hidden_layer->size ||
+            target_connection->down_size != target_layer->size )
+        {
+            target_connection = new RBMMatrixConnection(); 
+            target_connection->up_size = hidden_layer->size;
+            target_connection->down_size = target_layer->size;
+            target_connection->random_gen = random_gen;
+            target_connection->build();
+            target_connection->forget();
+        }
+    }
+
     if( !connection )
         PLERROR("PseudolikelihoodRBM::build_layers_and_connections(): \n"
                 "connection must be provided");
@@ -374,11 +433,9 @@
 
     // CD option
     pos_hidden.resize( hidden_layer->size );
-    pers_cd_input.resize( n_gibbs_chains );
     pers_cd_hidden.resize( n_gibbs_chains );
     for( int i=0; i<n_gibbs_chains; i++ )
     {
-        pers_cd_input[i].resize( input_layer->size );
         pers_cd_hidden[i].resize( hidden_layer->size );
     }
     if( persistent_gibbs_chain_is_started.length() != n_gibbs_chains )
@@ -402,12 +459,14 @@
     if( inputsize_ >= 0 )
         PLASSERT( input_layer->size == inputsize() );
 
-    if( n_classes > 1 )
+    if( targetsize() > 0 )
     {
-        class_output.resize( n_classes );
-        before_class_output.resize( n_classes );
-        class_gradient.resize( n_classes );
-        target_one_hot.resize( n_classes );
+        class_output.resize( target_layer->size );
+        class_gradient.resize( target_layer->size );
+        target_one_hot.resize( target_layer->size );
+        
+        pos_target.resize( target_layer->size );
+        neg_target.resize( target_layer->size );
     }
 
     if( !input_layer->random_gen )
@@ -450,11 +509,12 @@
     deepCopyField(connection, copies);
     deepCopyField(cost_names, copies);
     deepCopyField(transpose_connection, copies);
+    deepCopyField(target_layer, copies);
+    deepCopyField(target_connection, copies);
 
     deepCopyField(target_one_hot, copies);
     deepCopyField(input_gradient, copies);
     deepCopyField(class_output, copies);
-    deepCopyField(before_class_output, copies);
     deepCopyField(class_gradient, copies);
     deepCopyField(hidden_activation_pos_i, copies);
     deepCopyField(hidden_activation_neg_i, copies);
@@ -474,15 +534,16 @@
     deepCopyField(gnums_act, copies);
     deepCopyField(conf, copies);
     deepCopyField(pos_input, copies);
+    deepCopyField(pos_target, copies);
     deepCopyField(pos_hidden, copies);
     deepCopyField(neg_input, copies);
+    deepCopyField(neg_target, copies);
     deepCopyField(neg_hidden, copies);
     deepCopyField(reconstruction_activation_gradient, copies);
     deepCopyField(hidden_layer_expectation_gradient, copies);
     deepCopyField(hidden_layer_activation_gradient, copies);
     deepCopyField(masked_autoencoder_input, copies);
     deepCopyField(autoencoder_input_indices, copies);
-    deepCopyField(pers_cd_input, copies);
     deepCopyField(pers_cd_hidden, copies);
     deepCopyField(persistent_gibbs_chain_is_started, copies);
 }
@@ -493,7 +554,7 @@
 ////////////////
 int PseudolikelihoodRBM::outputsize() const
 {
-    return n_classes > 1 ? n_classes : hidden_layer->size;
+    return targetsize() > 0 ? target_layer->size : hidden_layer->size;
 }
 
 ////////////
@@ -523,7 +584,7 @@
     MODULE_LOG << "train() called " << endl;
 
     MODULE_LOG << "stage = " << stage
-        << ", target nstages = " << nstages << endl;
+               << ", target nstages = " << nstages << endl;
 
     PLASSERT( train_set );
 
@@ -567,7 +628,7 @@
         if( pb )
             pb->update( stage - init_stage + 1 );
 
-        if( n_classes > 1 )
+        if( targetsize() == 1 )
         {
             target_one_hot.clear();
             if( !is_missing(target[0]) )
@@ -575,788 +636,1042 @@
                 target_index = (int)round( target[0] );
                 target_one_hot[ target_index ] = 1;
             }
-            PLERROR("In PseudolikelihoodRBM::train(): supervised learning "
-                    "not implemented yet.");
+        }
+//        else
+//        {
 
+        // Discriminative learning is the sum of all learning rates
+        lr = 0;
+
+        if( decrease_ct != 0 ) 
+            lr += learning_rate / (1.0 + stage * decrease_ct );
+        else 
+            lr += learning_rate;
+
+        if( cd_decrease_ct != 0 ) 
+            lr += cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+        else 
+            lr += cd_learning_rate;
+        
+        if( denoising_decrease_ct != 0 ) 
+            lr += denoising_learning_rate / (1.0 + stage * denoising_decrease_ct );
+        else 
+            lr += denoising_learning_rate;
+
+        setLearningRate(lr);
+
+        if( targetsize() == 1 )
+        {
+            // Multi-class classification
+
+            connection->setAsDownInput( input );
+            hidden_layer->getAllActivations( 
+                (RBMMatrixConnection*) connection );
+
+            Vec target_act = target_layer->activation;
+            Vec hidden_act = hidden_layer->activation;
+            for( int i=0 ; i<target_layer->size ; i++ )
+            {
+                target_act[i] = target_layer->bias[i];
+                // LATERAL CONNECTIONS CODE HERE!!
+                real *w = &(target_connection->weights(0,i));
+                // step from one row to the next in weights matrix
+                int m = target_connection->weights.mod();                
+                
+                for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+                {
+                    // *w = weights(j,i)
+                    hidden_activation_pos_i[j] = hidden_act[j] + *w;
+                }
+                target_act[i] -= hidden_layer->freeEnergyContribution(
+                    hidden_activation_pos_i);
+            }
+            
+            target_layer->expectation_is_up_to_date = false;
+            target_layer->computeExpectation();
+            real nll = target_layer->fpropNLL(target_one_hot);
+            train_costs[nll_cost_index] = nll;
+            train_costs[class_cost_index] = 
+                argmax(target_layer->expectation) == target_index;
+            target_layer->bpropNLL(target_one_hot,nll,class_gradient);
+
+            hidden_activation_gradient.clear();
+            for( int i=0 ; i<target_layer->size ; i++ )
+            {
+                real *w = &(target_connection->weights(0,i));
+                // step from one row to the next in weights matrix
+                int m = target_connection->weights.mod();                
+                
+                for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+                {
+                    // *w = weights(j,i)
+                    hidden_activation_pos_i[j] = hidden_act[j] + *w;
+                }
+                hidden_layer->freeEnergyContributionGradient(
+                    hidden_activation_pos_i,
+                    hidden_activation_pos_i_gradient,
+                    -class_gradient[i],
+                    false
+                    );
+                hidden_activation_gradient += hidden_activation_pos_i_gradient;
+
+                // Update target connections
+                for( int j=0 ; j<hidden_layer->size ; j++, w+=m )
+                    *w -= learning_rate * hidden_activation_pos_i_gradient[j];
+            }
+
+            externalProduct( connection_gradient, hidden_activation_gradient,
+                             input );
+
+            // Update target bias            
+            multiplyScaledAdd(class_gradient, 1.0, -lr,
+                              target_layer->bias);
+
+            // Hidden bias update
+            multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                              hidden_layer->bias);
+            // Connection weights update
+            multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                               connection->weights );
+            // Input bias update
+            multiplyScaledAdd(input_gradient, 1.0, -lr,
+                              input_layer->bias);            
+        }
+        if( targetsize() > 1 )
+        {
+            // Multi-task binary classification
+            PLERROR("NNNNNNNNNNOOOOOOOOOOOOOOOOOOOOOO!!!!!!!!!!!!!!");
+        }
+
+        if( !fast_is_equal(learning_rate, 0.) )
+        {
             if( decrease_ct != 0 )
                 lr = learning_rate / (1.0 + stage * decrease_ct );
             else
                 lr = learning_rate;
 
             setLearningRate(lr);
-        }
-        else
-        {
-            if( !fast_is_equal(learning_rate, 0.) )
+
+            if( pseudolikelihood_context_size == 0 )
             {
-                if( decrease_ct != 0 )
-                    lr = learning_rate / (1.0 + stage * decrease_ct );
-                else
-                    lr = learning_rate;
+                // Compute input_probs
+                //
+                //a = W x + c
+                //  for i in 1...d
+                //      num_pos = b_i
+                //      num_neg = 0
+                //      for j in 1...h
+                //          num_pos += softplus( a_j - W_ji x_i + W_ji)
+                //          num_neg += softplus( a_j - W_ji x_i)
+                //      p_i = exp(num_pos) / (exp(num_pos) + exp(num_neg))
 
-                setLearningRate(lr);
-
-                if( pseudolikelihood_context_size == 0 )
+                if( targetsize() <= 0 )
                 {
-                    // Compute input_probs
-                    //
-                    //a = W x + c
-                    //  for i in 1...d
-                    //      num_pos = b_i
-                    //      num_neg = 0
-                    //      for j in 1...h
-                    //          num_pos += softplus( a_j - W_ji x_i + W_ji)
-                    //          num_neg += softplus( a_j - W_ji x_i)
-                    //      p_i = exp(num_pos) / (exp(num_pos) + exp(num_neg))
-
+                    // This was not computed previously
                     connection->setAsDownInput( input );
                     hidden_layer->getAllActivations( 
                         (RBMMatrixConnection*) connection );
+                }
+                else
+                {
+                    if( targetsize() == 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target_one_hot );
+                    else if( targetsize() > 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target );
+                }
 
-                    real num_pos_act;
-                    real num_neg_act;
-                    real num_pos;
-                    real num_neg;
-                    real* a = hidden_layer->activation.data();
-                    real* a_pos_i = hidden_activation_pos_i.data();
-                    real* a_neg_i = hidden_activation_neg_i.data();
-                    real* w, *gw;
-                    int m = connection->weights.mod();
-                    real input_i, input_probs_i;
-                    real pseudolikelihood = 0;
-                    real* ga_pos_i = hidden_activation_pos_i_gradient.data();
-                    real* ga_neg_i = hidden_activation_neg_i_gradient.data();
-                    hidden_activation_gradient.clear();
-                    connection_gradient.clear();
-                    for( int i=0; i<input_layer->size ; i++ )
+                real num_pos_act;
+                real num_neg_act;
+                real num_pos;
+                real num_neg;
+                real* a = hidden_layer->activation.data();
+                real* a_pos_i = hidden_activation_pos_i.data();
+                real* a_neg_i = hidden_activation_neg_i.data();
+                real* w, *gw;
+                int m = connection->weights.mod();
+                real input_i, input_probs_i;
+                real pseudolikelihood = 0;
+                real* ga_pos_i = hidden_activation_pos_i_gradient.data();
+                real* ga_neg_i = hidden_activation_neg_i_gradient.data();
+                hidden_activation_gradient.clear();
+                connection_gradient.clear();
+                for( int i=0; i<input_layer->size ; i++ )
+                {
+                    num_pos_act = input_layer->bias[i];
+                    // LATERAL CONNECTIONS CODE HERE!
+                    num_neg_act = 0;
+                    w = &(connection->weights(0,i));
+                    input_i = input[i];
+                    for( int j=0; j<hidden_layer->size; j++,w+=m )
                     {
-                        num_pos_act = input_layer->bias[i];
-                        num_neg_act = 0;
-                        w = &(connection->weights(0,i));
-                        input_i = input[i];
-                        for( int j=0; j<hidden_layer->size; j++,w+=m )
-                        {
-                            a_pos_i[j] = a[j] - *w * ( input_i - 1 );
-                            a_neg_i[j] = a[j] - *w * input_i;
-                        }
-                        num_pos_act -= hidden_layer->freeEnergyContribution(
-                            hidden_activation_pos_i);
-                        num_neg_act -= hidden_layer->freeEnergyContribution(
-                            hidden_activation_neg_i);
-                        //num_pos = safeexp(num_pos_act);
-                        //num_neg = safeexp(num_neg_act);
-                        //input_probs_i = num_pos / (num_pos + num_neg);
-                        if( input_layer->use_fast_approximations )
-                            input_probs_i = fastsigmoid(
-                                num_pos_act - num_neg_act);
-                        else
-                        {
-                            num_pos = safeexp(num_pos_act);
-                            num_neg = safeexp(num_neg_act);
-                            input_probs_i = num_pos / (num_pos + num_neg);
-                        }
+                        a_pos_i[j] = a[j] - *w * ( input_i - 1 );
+                        a_neg_i[j] = a[j] - *w * input_i;
+                    }
+                    num_pos_act -= hidden_layer->freeEnergyContribution(
+                        hidden_activation_pos_i);
+                    num_neg_act -= hidden_layer->freeEnergyContribution(
+                        hidden_activation_neg_i);
+                    //num_pos = safeexp(num_pos_act);
+                    //num_neg = safeexp(num_neg_act);
+                    //input_probs_i = num_pos / (num_pos + num_neg);
+                    if( input_layer->use_fast_approximations )
+                        input_probs_i = fastsigmoid(
+                            num_pos_act - num_neg_act);
+                    else
+                    {
+                        num_pos = safeexp(num_pos_act);
+                        num_neg = safeexp(num_neg_act);
+                        input_probs_i = num_pos / (num_pos + num_neg);
+                    }
 
-                        // Compute input_prob gradient
-                        if( input_layer->use_fast_approximations )
-                            pseudolikelihood += tabulated_softplus( 
-                                num_pos_act - num_neg_act ) 
-                                - input_i * (num_pos_act - num_neg_act);
-                        else
-                            pseudolikelihood += softplus( 
-                                num_pos_act - num_neg_act ) 
-                                - input_i * (num_pos_act - num_neg_act);
-                        input_gradient[i] = input_probs_i - input_i;
+                    // Compute input_prob gradient
+                    if( input_layer->use_fast_approximations )
+                        pseudolikelihood += tabulated_softplus( 
+                            num_pos_act - num_neg_act ) 
+                            - input_i * (num_pos_act - num_neg_act);
+                    else
+                        pseudolikelihood += softplus( 
+                            num_pos_act - num_neg_act ) 
+                            - input_i * (num_pos_act - num_neg_act);
+                    input_gradient[i] = input_probs_i - input_i;
 
-                        hidden_layer->freeEnergyContributionGradient(
-                            hidden_activation_pos_i,
-                            hidden_activation_pos_i_gradient,
-                            -input_gradient[i],
-                            false);
-                        hidden_activation_gradient += hidden_activation_pos_i_gradient;
+                    hidden_layer->freeEnergyContributionGradient(
+                        hidden_activation_pos_i,
+                        hidden_activation_pos_i_gradient,
+                        -input_gradient[i],
+                        false);
+                    hidden_activation_gradient += hidden_activation_pos_i_gradient;
 
-                        hidden_layer->freeEnergyContributionGradient(
-                            hidden_activation_neg_i,
-                            hidden_activation_neg_i_gradient,
-                            input_gradient[i],
-                            false);
-                        hidden_activation_gradient += hidden_activation_neg_i_gradient;
+                    hidden_layer->freeEnergyContributionGradient(
+                        hidden_activation_neg_i,
+                        hidden_activation_neg_i_gradient,
+                        input_gradient[i],
+                        false);
+                    hidden_activation_gradient += hidden_activation_neg_i_gradient;
 
-                        gw = &(connection_gradient(0,i));
-                        for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                        {
-                            *gw -= ga_pos_i[j] * ( input_i - 1 );
-                            *gw -= ga_neg_i[j] * input_i;
-                        }
+                    gw = &(connection_gradient(0,i));
+                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                    {
+                        *gw -= ga_pos_i[j] * ( input_i - 1 );
+                        *gw -= ga_neg_i[j] * input_i;
                     }
+                }
 
-                    externalProductAcc( connection_gradient, hidden_activation_gradient,
-                                        input );
+                externalProductAcc( connection_gradient, hidden_activation_gradient,
+                                    input );
 
-                    // Hidden bias update
-                    multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
-                                      hidden_layer->bias);
-                    // Connection weights update
-                    multiplyScaledAdd( connection_gradient, 1.0, -lr,
-                                       connection->weights );
-                    // Input bias update
-                    multiplyScaledAdd(input_gradient, 1.0, -lr,
-                                      input_layer->bias);
+                if( targetsize() > 0 )
+                    lr *= generative_learning_weight;
 
-                    // N.B.: train costs contains pseudolikelihood
-                    //       or pseudoNLL, not NLL
+                // Hidden bias update
+                multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                                  hidden_layer->bias);
+                // Connection weights update
+                multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                                   connection->weights );
+                // Input bias update
+                multiplyScaledAdd(input_gradient, 1.0, -lr,
+                                  input_layer->bias);
+
+                if( targetsize() == 1 )
+                    externalProductScaleAcc( target_connection->weights, 
+                                             hidden_activation_gradient,
+                                             target_one_hot,
+                                             lr );
+                if( targetsize() > 1 )
+                    externalProductScaleAcc( target_connection->weights, 
+                                             hidden_activation_gradient,
+                                             target,
+                                             lr );
+
+                // N.B.: train costs contains pseudolikelihood
+                //       or pseudoNLL, not NLL
+                if( compute_input_space_nll )
                     train_costs[nll_cost_index] = pseudolikelihood;
 
 //                    cout << "input_gradient: " << input_gradient << endl;
 //                    cout << "hidden_activation_gradient" << hidden_activation_gradient << endl;
 
-                }
-                else
+            }
+            else
+            {
+                if( ( pseudolikelihood_context_type == "most_correlated" ||
+                      pseudolikelihood_context_type == "most_correlated_uniform_random" )
+                    && correlations_per_i.length() == 0 )
                 {
-                    if( ( pseudolikelihood_context_type == "most_correlated" ||
-                          pseudolikelihood_context_type == "most_correlated_uniform_random" )
-                        && correlations_per_i.length() == 0 )
+                    Vec corr_input(inputsize());
+                    Vec corr_target(targetsize());
+                    real corr_weight;
+                    Vec mean(inputsize());
+                    mean.clear();
+                    for(int t=0; t<train_set->length(); t++)
                     {
-                        Vec corr_input(inputsize());
-                        Vec corr_target(targetsize());
-                        real corr_weight;
-                        Vec mean(inputsize());
-                        mean.clear();
-                        for(int t=0; t<train_set->length(); t++)
-                        {
-                            train_set->getExample(t,corr_input,corr_target,
-                                                  corr_weight);
-                            mean += corr_input;
-                        }
-                        mean /= train_set->length();
+                        train_set->getExample(t,corr_input,corr_target,
+                                              corr_weight);
+                        mean += corr_input;
+                    }
+                    mean /= train_set->length();
                         
-                        correlations_per_i.resize(inputsize(),inputsize());
-                        correlations_per_i.clear();
-                        Mat cov(inputsize(), inputsize());
-                        cov.clear();
-                        for(int t=0; t<train_set->length(); t++)
+                    correlations_per_i.resize(inputsize(),inputsize());
+                    correlations_per_i.clear();
+                    Mat cov(inputsize(), inputsize());
+                    cov.clear();
+                    for(int t=0; t<train_set->length(); t++)
+                    {
+                        train_set->getExample(t,corr_input,corr_target,
+                                              corr_weight);
+                        corr_input -= mean;
+                        externalProductAcc(cov,
+                                           corr_input,corr_input);
+                    }
+                    //correlations_per_i /= train_set->length();
+
+                    for( int i=0; i<inputsize(); i++ )
+                        for( int j=0; j<inputsize(); j++)
                         {
-                            train_set->getExample(t,corr_input,corr_target,
-                                                  corr_weight);
-                            corr_input -= mean;
-                            externalProductAcc(cov,
-                                               corr_input,corr_input);
+                            correlations_per_i(i,j) = 
+                                abs(cov(i,j)) 
+                                / sqrt(cov(i,i)*cov(j,j));
                         }
-                        //correlations_per_i /= train_set->length();
 
-                        for( int i=0; i<inputsize(); i++ )
-                            for( int j=0; j<inputsize(); j++)
-                            {
-                                correlations_per_i(i,j) = 
-                                    abs(cov(i,j)) 
-                                    / sqrt(cov(i,i)*cov(j,j));
-                            }
-
-                        if( pseudolikelihood_context_type == "most_correlated")
-                        {
-                            if( pseudolikelihood_context_size <= 0 )
-                                PLERROR("In PseudolikelihoodRBM::train(): "
+                    if( pseudolikelihood_context_type == "most_correlated")
+                    {
+                        if( pseudolikelihood_context_size <= 0 )
+                            PLERROR("In PseudolikelihoodRBM::train(): "
                                     "pseudolikelihood_context_size should be > 0 "
                                     "for \"most_correlated\" context type");
-                            real current_min;
-                            int current_min_position;
-                            real* corr;
-                            int* context;
-                            Vec context_corr(pseudolikelihood_context_size);
-                            context_indices_per_i.resize(
-                                inputsize(),
-                                pseudolikelihood_context_size);
+                        real current_min;
+                        int current_min_position;
+                        real* corr;
+                        int* context;
+                        Vec context_corr(pseudolikelihood_context_size);
+                        context_indices_per_i.resize(
+                            inputsize(),
+                            pseudolikelihood_context_size);
 
-                            // HUGO: this is quite inefficient for big 
-                            // pseudolikelihood_context_sizes, should use a heap
-                            for( int i=0; i<inputsize(); i++ )
+                        // HUGO: this is quite inefficient for big 
+                        // pseudolikelihood_context_sizes, should use a heap
+                        for( int i=0; i<inputsize(); i++ )
+                        {
+                            current_min = REAL_MAX;
+                            current_min_position = -1;
+                            corr = correlations_per_i[i];
+                            context = context_indices_per_i[i];
+                            for( int j=0; j<inputsize(); j++ )
                             {
-                                current_min = REAL_MAX;
-                                current_min_position = -1;
-                                corr = correlations_per_i[i];
-                                context = context_indices_per_i[i];
-                                for( int j=0; j<inputsize(); j++ )
+                                if( i == j )
+                                    continue;
+
+                                // Filling first pseudolikelihood_context_size elements
+                                if( j - (j>i?1:0) < pseudolikelihood_context_size )
                                 {
-                                    if( i == j )
-                                        continue;
-
-                                    // Filling first pseudolikelihood_context_size elements
-                                    if( j - (j>i?1:0) < pseudolikelihood_context_size )
+                                    context[j - (j>i?1:0)] = j;
+                                    context_corr[j - (j>i?1:0)] = corr[j];
+                                    if( current_min > corr[j] )
                                     {
-                                        context[j - (j>i?1:0)] = j;
-                                        context_corr[j - (j>i?1:0)] = corr[j];
-                                        if( current_min > corr[j] )
-                                        {
-                                            current_min = corr[j];
-                                            current_min_position = j - (j>i?1:0);
-                                        }
-                                        continue;
+                                        current_min = corr[j];
+                                        current_min_position = j - (j>i?1:0);
                                     }
+                                    continue;
+                                }
 
-                                    if( corr[j] > current_min )
-                                    {
-                                        context[current_min_position] = j;
-                                        context_corr[current_min_position] = corr[j];
-                                        current_min = 
-                                            min( context_corr, 
-                                                 current_min_position );
-                                    }
+                                if( corr[j] > current_min )
+                                {
+                                    context[current_min_position] = j;
+                                    context_corr[current_min_position] = corr[j];
+                                    current_min = 
+                                        min( context_corr, 
+                                             current_min_position );
                                 }
                             }
                         }
+                    }
                         
-                        if( pseudolikelihood_context_type == 
-                            "most_correlated_uniform_random" )
-                        {
-                            if( k_most_correlated < 
-                                pseudolikelihood_context_size )
-                                PLERROR("In PseudolikelihoodRBM::train(): "
-                                        "k_most_correlated should be "
-                                        ">= pseudolikelihood_context_size");
+                    if( pseudolikelihood_context_type == 
+                        "most_correlated_uniform_random" )
+                    {
+                        if( k_most_correlated < 
+                            pseudolikelihood_context_size )
+                            PLERROR("In PseudolikelihoodRBM::train(): "
+                                    "k_most_correlated should be "
+                                    ">= pseudolikelihood_context_size");
 
-                            if( k_most_correlated > inputsize() - 1 )
-                                PLERROR("In PseudolikelihoodRBM::train(): "
-                                        "k_most_correlated should be "
-                                        "< inputsize()");
+                        if( k_most_correlated > inputsize() - 1 )
+                            PLERROR("In PseudolikelihoodRBM::train(): "
+                                    "k_most_correlated should be "
+                                    "< inputsize()");
 
-                            real current_min;
-                            int current_min_position;
-                            real* corr;
-                            int* context;
-                            Vec context_corr( k_most_correlated );
-                            context_most_correlated.resize( inputsize() );
+                        real current_min;
+                        int current_min_position;
+                        real* corr;
+                        int* context;
+                        Vec context_corr( k_most_correlated );
+                        context_most_correlated.resize( inputsize() );
 
-                            // HUGO: this is quite inefficient for big 
-                            // pseudolikelihood_context_sizes, should use a heap
-                            for( int i=0; i<inputsize(); i++ )
+                        // HUGO: this is quite inefficient for big 
+                        // pseudolikelihood_context_sizes, should use a heap
+                        for( int i=0; i<inputsize(); i++ )
+                        {
+                            context_most_correlated[i].resize( 
+                                k_most_correlated );
+                            current_min = REAL_MAX;
+                            current_min_position = -1;
+                            corr = correlations_per_i[i];
+                            context = context_most_correlated[i].data();
+                            for( int j=0; j<inputsize(); j++ )
                             {
-                                context_most_correlated[i].resize( 
-                                    k_most_correlated );
-                                current_min = REAL_MAX;
-                                current_min_position = -1;
-                                corr = correlations_per_i[i];
-                                context = context_most_correlated[i].data();
-                                for( int j=0; j<inputsize(); j++ )
+                                if( i == j )
+                                    continue;
+
+                                // Filling first k_most_correlated elements
+                                if( j - (j>i?1:0) <  k_most_correlated )
                                 {
-                                    if( i == j )
-                                        continue;
-
-                                    // Filling first k_most_correlated elements
-                                    if( j - (j>i?1:0) <  k_most_correlated )
+                                    context[j - (j>i?1:0)] = j;
+                                    context_corr[j - (j>i?1:0)] = corr[j];
+                                    if( current_min > corr[j] )
                                     {
-                                        context[j - (j>i?1:0)] = j;
-                                        context_corr[j - (j>i?1:0)] = corr[j];
-                                        if( current_min > corr[j] )
-                                        {
-                                            current_min = corr[j];
-                                            current_min_position = j - (j>i?1:0);
-                                        }
-                                        continue;
+                                        current_min = corr[j];
+                                        current_min_position = j - (j>i?1:0);
                                     }
+                                    continue;
+                                }
 
-                                    if( corr[j] > current_min )
-                                    {
-                                        context[current_min_position] = j;
-                                        context_corr[current_min_position] = corr[j];
-                                        current_min = 
-                                            min( context_corr, 
-                                                 current_min_position );
-                                    }
+                                if( corr[j] > current_min )
+                                {
+                                    context[current_min_position] = j;
+                                    context_corr[current_min_position] = corr[j];
+                                    current_min = 
+                                        min( context_corr, 
+                                             current_min_position );
                                 }
                             }
-                        }                        
-                    }
+                        }
+                    }                        
+                }
 
-                    if( pseudolikelihood_context_type == "uniform_random" ||
-                        pseudolikelihood_context_type == "most_correlated_uniform_random" )
+                if( pseudolikelihood_context_type == "uniform_random" ||
+                    pseudolikelihood_context_type == "most_correlated_uniform_random" )
+                {
+                    // Generate contexts
+                    if( pseudolikelihood_context_type == "uniform_random" )
+                        for( int i=0; i<context_indices.length(); i++)
+                            context_indices[i] = i;
+                    int tmp,k;
+                    int* c;
+                    int n;
+                    if( pseudolikelihood_context_type == "uniform_random" )
                     {
-                        // Generate contexts
-                        if( pseudolikelihood_context_type == "uniform_random" )
-                            for( int i=0; i<context_indices.length(); i++)
-                                context_indices[i] = i;
-                        int tmp,k;
-                        int* c;
-                        int n;
-                        if( pseudolikelihood_context_type == "uniform_random" )
+                        c = context_indices.data();
+                        n = input_layer->size-1;
+                    }
+                    int* ci;
+                    for( int i=0; i<context_indices_per_i.length(); i++)
+                    {
+                        if( pseudolikelihood_context_type == 
+                            "most_correlated_uniform_random" )
                         {
-                            c = context_indices.data();
-                            n = input_layer->size-1;
+                            c = context_most_correlated[i].data();
+                            n = context_most_correlated[i].length();
                         }
-                        int* ci;
-                        for( int i=0; i<context_indices_per_i.length(); i++)
+
+                        ci = context_indices_per_i[i];
+                        for (int j = 0; j < context_indices_per_i.width(); j++) 
                         {
-                            if( pseudolikelihood_context_type == 
-                                "most_correlated_uniform_random" )
-                            {
-                                c = context_most_correlated[i].data();
-                                n = context_most_correlated[i].length();
-                            }
+                            k = j + 
+                                random_gen->uniform_multinomial_sample(n - j);
+                                
+                            tmp = c[j];
+                            c[j] = c[k];
+                            c[k] = tmp;
 
-                            ci = context_indices_per_i[i];
-                            for (int j = 0; j < context_indices_per_i.width(); j++) 
+                            if( pseudolikelihood_context_type 
+                                == "uniform_random" )
                             {
-                                k = j + 
-                                    random_gen->uniform_multinomial_sample(n - j);
-                                
-                                tmp = c[j];
-                                c[j] = c[k];
-                                c[k] = tmp;
-
-                                if( pseudolikelihood_context_type 
-                                    == "uniform_random" )
-                                {
-                                    if( c[j] >= i )
-                                        ci[j] = c[j]+1;
-                                    else
-                                        ci[j] = c[j];
-                                }
-
-                                if( pseudolikelihood_context_type == 
-                                    "most_correlated_uniform_random" )
+                                if( c[j] >= i )
+                                    ci[j] = c[j]+1;
+                                else
                                     ci[j] = c[j];
                             }
+
+                            if( pseudolikelihood_context_type == 
+                                "most_correlated_uniform_random" )
+                                ci[j] = c[j];
                         }
                     }
+                }
 
+                if( targetsize() <= 0 )
+                {
+                    // This was not computed previously
                     connection->setAsDownInput( input );
                     hidden_layer->getAllActivations( 
-                        (RBMMatrixConnection *) connection );
+                        (RBMMatrixConnection*) connection );
+                }
+                else
+                {
+                    if( targetsize() == 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target_one_hot );
+                    else if( targetsize() > 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target );
+                            
+                }
 
-                    int n_conf = ipow(2, pseudolikelihood_context_size);
-                    //nums_act.resize( 2 * n_conf );
-                    //gnums_act.resize( 2 * n_conf );
-                    //context_probs.resize( 2 * n_conf );
-                    //hidden_activations_context.resize( 2*n_conf, hidden_layer->size );
-                    //hidden_activations_context_k_gradient.resize( hidden_layer->size );
-                    real* nums_data;
-                    real* gnums_data;
-                    real* cp_data;
-                    real* a = hidden_layer->activation.data();
-                    real* w, *gw, *gi, *ac, *bi, *gac;
-                    int* context_i;
-                    int m;
-                    int conf_index;
-                    real input_i, input_j,  log_Zi;
-                    real pseudolikelihood = 0;
+                int n_conf = ipow(2, pseudolikelihood_context_size);
+                //nums_act.resize( 2 * n_conf );
+                //gnums_act.resize( 2 * n_conf );
+                //context_probs.resize( 2 * n_conf );
+                //hidden_activations_context.resize( 2*n_conf, hidden_layer->size );
+                //hidden_activations_context_k_gradient.resize( hidden_layer->size );
+                real* nums_data;
+                real* gnums_data;
+                real* cp_data;
+                real* a = hidden_layer->activation.data();
+                real* w, *gw, *gi, *ac, *bi, *gac;
+                int* context_i;
+                int m;
+                int conf_index;
+                real input_i, input_j,  log_Zi;
+                real pseudolikelihood = 0;
 
-                    input_gradient.clear();
-                    hidden_activation_gradient.clear();
-                    connection_gradient.clear();
-                    gi = input_gradient.data();
-                    bi = input_layer->bias.data();
-                    for( int i=0; i<input_layer->size ; i++ )
+                input_gradient.clear();
+                hidden_activation_gradient.clear();
+                connection_gradient.clear();
+                gi = input_gradient.data();
+                bi = input_layer->bias.data();
+                for( int i=0; i<input_layer->size ; i++ )
+                {
+                    nums_data = nums_act.data();
+                    cp_data = context_probs.data();
+                    input_i = input[i];
+
+                    m = connection->weights.mod();
+                    // input_i = 1
+                    for( int k=0; k<n_conf; k++)
                     {
-                        nums_data = nums_act.data();
-                        cp_data = context_probs.data();
-                        input_i = input[i];
+                        *nums_data = bi[i];
+                        *cp_data = input_i;
+                        conf_index = k;
+                        ac = hidden_activations_context[k];
 
-                        m = connection->weights.mod();
-                        // input_i = 1
-                        for( int k=0; k<n_conf; k++)
+                        w = &(connection->weights(0,i));
+                        for( int j=0; j<hidden_layer->size; j++,w+=m )
+                            ac[j] = a[j] - *w * ( input_i - 1 );
+
+                        context_i = context_indices_per_i[i];
+                        for( int l=0; l<pseudolikelihood_context_size; l++ )
                         {
-                            *nums_data = bi[i];
-                            *cp_data = input_i;
-                            conf_index = k;
-                            ac = hidden_activations_context[k];
-
-                            w = &(connection->weights(0,i));
-                            for( int j=0; j<hidden_layer->size; j++,w+=m )
-                                ac[j] = a[j] - *w * ( input_i - 1 );
-
-                            context_i = context_indices_per_i[i];
-                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                            input_j = input[*context_i];
+                            w = &(connection->weights(0,*context_i));
+                            if( conf_index & 1)
                             {
-                                input_j = input[*context_i];
-                                w = &(connection->weights(0,*context_i));
-                                if( conf_index & 1)
-                                {
-                                    *cp_data *= input_j;
-                                    *nums_data += bi[*context_i];
-                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
-                                        ac[j] -=  *w * ( input_j - 1 );
-                                }
-                                else
-                                {
-                                    *cp_data *= (1-input_j);
-                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
-                                        ac[j] -=  *w * input_j;
-                                }
+                                *cp_data *= input_j;
+                                *nums_data += bi[*context_i];
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                    ac[j] -=  *w * ( input_j - 1 );
+                            }
+                            else
+                            {
+                                *cp_data *= (1-input_j);
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                    ac[j] -=  *w * input_j;
+                            }
 
-                                conf_index >>= 1;
-                                context_i++;
-                            }
-                            *nums_data -= hidden_layer->freeEnergyContribution(
-                                hidden_activations_context(k));
-                            nums_data++;
-                            cp_data++;
+                            conf_index >>= 1;
+                            context_i++;
                         }
+                        *nums_data -= hidden_layer->freeEnergyContribution(
+                            hidden_activations_context(k));
+                        nums_data++;
+                        cp_data++;
+                    }
 
-                        // input_i = 0
-                        for( int k=0; k<n_conf; k++)
-                        {
-                            *nums_data = 0;
-                            *cp_data = (1-input_i);
-                            conf_index = k;
-                            ac = hidden_activations_context[n_conf + k];
+                    // input_i = 0
+                    for( int k=0; k<n_conf; k++)
+                    {
+                        *nums_data = 0;
+                        *cp_data = (1-input_i);
+                        conf_index = k;
+                        ac = hidden_activations_context[n_conf + k];
                         
-                            w = &(connection->weights(0,i));
-                            for( int j=0; j<hidden_layer->size; j++,w+=m )
-                                ac[j] = a[j] - *w * input_i;
+                        w = &(connection->weights(0,i));
+                        for( int j=0; j<hidden_layer->size; j++,w+=m )
+                            ac[j] = a[j] - *w * input_i;
 
-                            context_i = context_indices_per_i[i];
-                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                        context_i = context_indices_per_i[i];
+                        for( int l=0; l<pseudolikelihood_context_size; l++ )
+                        {
+                            w = &(connection->weights(0,*context_i));
+                            input_j = input[*context_i];
+                            if( conf_index & 1)
                             {
-                                w = &(connection->weights(0,*context_i));
-                                input_j = input[*context_i];
-                                if( conf_index & 1)
-                                {
-                                    *cp_data *= input_j;
-                                    *nums_data += bi[*context_i];
-                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
-                                        ac[j] -=  *w * ( input_j - 1 );
-                                }
-                                else
-                                {
-                                    *cp_data *= (1-input_j);
-                                    for( int j=0; j<hidden_layer->size; j++,w+=m )
-                                        ac[j] -=  *w * input_j;
-                                }
+                                *cp_data *= input_j;
+                                *nums_data += bi[*context_i];
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                    ac[j] -=  *w * ( input_j - 1 );
+                            }
+                            else
+                            {
+                                *cp_data *= (1-input_j);
+                                for( int j=0; j<hidden_layer->size; j++,w+=m )
+                                    ac[j] -=  *w * input_j;
+                            }
 
-                                conf_index >>= 1;
-                                context_i++;
-                            }
-                            *nums_data -= hidden_layer->freeEnergyContribution(
-                                hidden_activations_context(n_conf + k));
-                            nums_data++;
-                            cp_data++;
+                            conf_index >>= 1;
+                            context_i++;
                         }
+                        *nums_data -= hidden_layer->freeEnergyContribution(
+                            hidden_activations_context(n_conf + k));
+                        nums_data++;
+                        cp_data++;
+                    }
                     
 
-                        // Gradient computation
-                        //exp( nums_act, nums);
-                        //Zi = sum(nums);
-                        //log_Zi = pl_log(Zi);
-                        log_Zi = logadd(nums_act);
+                    // Gradient computation
+                    //exp( nums_act, nums);
+                    //Zi = sum(nums);
+                    //log_Zi = pl_log(Zi);
+                    log_Zi = logadd(nums_act);
 
-                        nums_data = nums_act.data();
-                        gnums_data = gnums_act.data();
-                        cp_data = context_probs.data();
+                    nums_data = nums_act.data();
+                    gnums_data = gnums_act.data();
+                    cp_data = context_probs.data();
 
-                        // Compute input_prob gradient
+                    // Compute input_prob gradient
 
-                        m = connection_gradient.mod();
-                        // input_i = 1                    
-                        for( int k=0; k<n_conf; k++)
-                        {
-                            pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
-                            *gnums_data = (safeexp(*nums_data - log_Zi) - *cp_data);
-                            gi[i] += *gnums_data;
+                    m = connection_gradient.mod();
+                    // input_i = 1                    
+                    for( int k=0; k<n_conf; k++)
+                    {
+                        pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
+                        *gnums_data = (safeexp(*nums_data - log_Zi) - *cp_data);
+                        gi[i] += *gnums_data;
                         
-                            hidden_layer->freeEnergyContributionGradient(
-                                hidden_activations_context(k),
-                                hidden_activations_context_k_gradient,
-                                -*gnums_data,
-                                false);
-                            hidden_activation_gradient += 
-                                hidden_activations_context_k_gradient;
+                        hidden_layer->freeEnergyContributionGradient(
+                            hidden_activations_context(k),
+                            hidden_activations_context_k_gradient,
+                            -*gnums_data,
+                            false);
+                        hidden_activation_gradient += 
+                            hidden_activations_context_k_gradient;
                         
-                            gac = hidden_activations_context_k_gradient.data();
-                            gw = &(connection_gradient(0,i));
-                            for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                                *gw -= gac[j] * ( input_i - 1 );
+                        gac = hidden_activations_context_k_gradient.data();
+                        gw = &(connection_gradient(0,i));
+                        for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                            *gw -= gac[j] * ( input_i - 1 );
 
-                            context_i = context_indices_per_i[i];
-                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                        context_i = context_indices_per_i[i];
+                        for( int l=0; l<pseudolikelihood_context_size; l++ )
+                        {
+                            gw = &(connection_gradient(0,*context_i));
+                            input_j = input[*context_i];
+                            if( conf_index & 1)
                             {
-                                gw = &(connection_gradient(0,*context_i));
-                                input_j = input[*context_i];
-                                if( conf_index & 1)
-                                {
-                                    gi[*context_i] += *gnums_data;
-                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                                        *gw -= gac[j] * ( input_j - 1 );
-                                }
-                                else
-                                {
-                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                                        *gw -= gac[j] * input_j;
-                                }
-                                conf_index >>= 1;
-                                context_i++;
+                                gi[*context_i] += *gnums_data;
+                                for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                                    *gw -= gac[j] * ( input_j - 1 );
                             }
-
-                            nums_data++;
-                            gnums_data++;
-                            cp_data++;
+                            else
+                            {
+                                for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                                    *gw -= gac[j] * input_j;
+                            }
+                            conf_index >>= 1;
+                            context_i++;
                         }
 
-                        // input_i = 0
-                        for( int k=0; k<n_conf; k++)
-                        {
-                            pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
-                            *gnums_data = (safeexp(*nums_data - log_Zi) - *cp_data);
+                        nums_data++;
+                        gnums_data++;
+                        cp_data++;
+                    }
+
+                    // input_i = 0
+                    for( int k=0; k<n_conf; k++)
+                    {
+                        pseudolikelihood -= *cp_data * (*nums_data - log_Zi);
+                        *gnums_data = (safeexp(*nums_data - log_Zi) - *cp_data);
                         
-                            hidden_layer->freeEnergyContributionGradient(
-                                hidden_activations_context(n_conf + k),
-                                hidden_activations_context_k_gradient,
-                                -*gnums_data,
-                                false);
-                            hidden_activation_gradient += 
-                                hidden_activations_context_k_gradient;
+                        hidden_layer->freeEnergyContributionGradient(
+                            hidden_activations_context(n_conf + k),
+                            hidden_activations_context_k_gradient,
+                            -*gnums_data,
+                            false);
+                        hidden_activation_gradient += 
+                            hidden_activations_context_k_gradient;
                         
-                            gac = hidden_activations_context_k_gradient.data();
-                            gw = &(connection_gradient(0,i));
-                            for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                                *gw -= gac[j] *input_i;
+                        gac = hidden_activations_context_k_gradient.data();
+                        gw = &(connection_gradient(0,i));
+                        for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                            *gw -= gac[j] *input_i;
 
-                            context_i = context_indices_per_i[i];
-                            for( int l=0; l<pseudolikelihood_context_size; l++ )
+                        context_i = context_indices_per_i[i];
+                        for( int l=0; l<pseudolikelihood_context_size; l++ )
+                        {
+                            gw = &(connection_gradient(0,*context_i));
+                            input_j = input[*context_i];
+                            if( conf_index & 1)
                             {
-                                gw = &(connection_gradient(0,*context_i));
-                                input_j = input[*context_i];
-                                if( conf_index & 1)
-                                {
-                                    gi[*context_i] += *gnums_data;
-                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                                        *gw -= gac[j] * ( input_j - 1 );
-                                }
-                                else
-                                {
-                                    for( int j=0; j<hidden_layer->size; j++,gw+=m )
-                                        *gw -= gac[j] * input_j;
-                                }
-
-                                conf_index >>= 1;
-                                context_i++;
+                                gi[*context_i] += *gnums_data;
+                                for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                                    *gw -= gac[j] * ( input_j - 1 );
                             }
+                            else
+                            {
+                                for( int j=0; j<hidden_layer->size; j++,gw+=m )
+                                    *gw -= gac[j] * input_j;
+                            }
 
-                            nums_data++;
-                            gnums_data++;
-                            cp_data++;
+                            conf_index >>= 1;
+                            context_i++;
                         }
+
+                        nums_data++;
+                        gnums_data++;
+                        cp_data++;
                     }
+                }
 
 //                    cout << "input_gradient: " << input_gradient << endl;
 //                    cout << "hidden_activation_gradient" << hidden_activation_gradient << endl;
 
-                    externalProductAcc( connection_gradient, hidden_activation_gradient,
-                                        input );
+                externalProductAcc( connection_gradient, hidden_activation_gradient,
+                                    input );
+                if( targetsize() > 0 )
+                    lr *= generative_learning_weight;
 
-                    // Hidden bias update
-                    multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
-                                      hidden_layer->bias);
-                    // Connection weights update
-                    multiplyScaledAdd( connection_gradient, 1.0, -lr,
-                                       connection->weights );
-                    // Input bias update
-                    multiplyScaledAdd(input_gradient, 1.0, -lr,
-                                      input_layer->bias);
+                // Hidden bias update
+                multiplyScaledAdd(hidden_activation_gradient, 1.0, -lr,
+                                  hidden_layer->bias);
+                // Connection weights update
+                multiplyScaledAdd( connection_gradient, 1.0, -lr,
+                                   connection->weights );
+                // Input bias update
+                multiplyScaledAdd(input_gradient, 1.0, -lr,
+                                  input_layer->bias);
 
-                    // N.B.: train costs contains pseudolikelihood
-                    //       or pseudoNLL, not NLL
+                if( targetsize() == 1 )
+                    externalProductScaleAcc( target_connection->weights, 
+                                             hidden_activation_gradient,
+                                             target_one_hot,
+                                             lr );
+                if( targetsize() > 1 )
+                    externalProductScaleAcc( target_connection->weights, 
+                                             hidden_activation_gradient,
+                                             target,
+                                             lr );
+
+                // N.B.: train costs contains pseudolikelihood
+                //       or pseudoNLL, not NLL
+                if( compute_input_space_nll )
                     train_costs[nll_cost_index] = pseudolikelihood;
-                }
             }
+        }
             
-            // CD learning
-            if( !fast_is_equal(cd_learning_rate, 0.) )
+        // CD learning
+        if( !fast_is_equal(cd_learning_rate, 0.) )
+        {
+
+            if( !fast_is_equal(persistent_cd_weight, 1.) )
             {
+                if( cd_decrease_ct != 0 )
+                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                else
+                    lr = cd_learning_rate;
 
-                if( !fast_is_equal(persistent_cd_weight, 1.) )
+                if( targetsize() > 0 )
+                    lr *= generative_learning_weight;
+                    
+                lr *= (1-persistent_cd_weight);
+
+                setLearningRate(lr);
+
+                // Positive phase
+                pos_input = input;
+                if( targetsize() > 0 )
+                    pos_target = target_one_hot;
+                connection->setAsDownInput( input );
+                hidden_layer->getAllActivations( 
+                    (RBMMatrixConnection*) connection );
+                if( targetsize() == 1 )
+                    productAcc( hidden_layer->activation,
+                                target_connection->weights,
+                                target_one_hot );
+                else if( targetsize() > 1 )
+                    productAcc( hidden_layer->activation,
+                                target_connection->weights,
+                                target );
+                        
+                hidden_layer->computeExpectation();
+                //pos_hidden.resize( hidden_layer->size );
+                pos_hidden << hidden_layer->expectation;
+                    
+                // Negative phase
+                for(int i=0; i<cd_n_gibbs; i++)
                 {
-                    if( cd_decrease_ct != 0 )
-                        lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                    if( use_mean_field_cd )
+                    {
+                        connection->setAsUpInput( hidden_layer->expectation );
+                    }
                     else
-                        lr = cd_learning_rate;
-                    
-                    lr *= (1-persistent_cd_weight);
+                    {
+                        hidden_layer->generateSample();
+                        connection->setAsUpInput( hidden_layer->sample );
+                    }
+                    input_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    input_layer->computeExpectation();
+                    // LATERAL CONNECTIONS CODE HERE!
 
-                    setLearningRate(lr);
+                    if( use_mean_field_cd )
+                    {
+                        connection->setAsDownInput( input_layer->expectation );
+                    }
+                    else
+                    {
+                        input_layer->generateSample();
+                        connection->setAsDownInput( input_layer->sample );
+                    }
 
-                    // Positive phase
-                    pos_input = input;
-                    connection->setAsDownInput( input );
                     hidden_layer->getAllActivations( 
                         (RBMMatrixConnection*) connection );
-                    hidden_layer->computeExpectation();
-                    //pos_hidden.resize( hidden_layer->size );
-                    pos_hidden << hidden_layer->expectation;
-                    
-                    // Negative phase
-                    for(int i=0; i<cd_n_gibbs; i++)
+
+                    if( targetsize() > 0 )
                     {
                         if( use_mean_field_cd )
-                        {
-                            connection->setAsUpInput( hidden_layer->expectation );
-                        }
+                            target_connection->setAsUpInput( 
+                                hidden_layer->expectation );
                         else
-                        {
-                            hidden_layer->generateSample();
-                            connection->setAsUpInput( hidden_layer->sample );
-                        }
-                        input_layer->getAllActivations( 
-                            (RBMMatrixConnection*) connection );
-                        input_layer->computeExpectation();
+                            target_connection->setAsUpInput( 
+                                hidden_layer->sample );
+                        target_layer->getAllActivations( 
+                            (RBMMatrixConnection*) target_connection );
+                        target_layer->computeExpectation();
                         if( use_mean_field_cd )
-                        {
-                            connection->setAsDownInput( input_layer->expectation );
-                        }
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target_layer->expectation );
                         else
                         {
-                            input_layer->generateSample();
-                            connection->setAsDownInput( input_layer->sample );
-                        }
-                        hidden_layer->getAllActivations( 
-                            (RBMMatrixConnection*) connection );
-                        hidden_layer->computeExpectation();
+                            target_layer->generateSample();
+                            productAcc( hidden_layer->activation,
+                                        target_connection->weights,
+                                        target_layer->sample );
+                        }   
                     }
+                        
+                    hidden_layer->computeExpectation();
+                }
                     
+                if( use_mean_field_cd )
+                    neg_input = input_layer->expectation;
+                else
+                    neg_input = input_layer->sample;
+
+                neg_hidden = hidden_layer->expectation;
+                    
+                input_layer->update(pos_input,neg_input);
+                hidden_layer->update(pos_hidden,neg_hidden);
+                connection->update(pos_input,pos_hidden,
+                                   neg_input,neg_hidden);
+                if( targetsize() > 0 )
+                {
                     if( use_mean_field_cd )
-                        neg_input = input_layer->expectation;
+                        neg_target = target_layer->expectation;
                     else
-                        neg_input = input_layer->sample;
-                    neg_hidden = hidden_layer->expectation;
-                    
-                    input_layer->update(pos_input,neg_input);
-                    hidden_layer->update(pos_hidden,neg_hidden);
-                    connection->update(pos_input,pos_hidden,
-                                       neg_input,neg_hidden);
+                        neg_target = target_layer->sample;
+                    target_layer->update(pos_target,neg_target);
+                    target_connection->update(pos_target,pos_hidden,
+                                              neg_target,neg_hidden);
                 }
+            }
 
-                if( !fast_is_equal(persistent_cd_weight, 0.) )
-                {
-                    if( use_mean_field_cd )
-                        PLERROR("In PseudolikelihoodRBM::train(): Persistent "
+            if( !fast_is_equal(persistent_cd_weight, 0.) )
+            {
+                if( use_mean_field_cd )
+                    PLERROR("In PseudolikelihoodRBM::train(): Persistent "
                             "Contrastive Divergence was not implemented for "
                             "MF-CD");
 
-                    if( cd_decrease_ct != 0 )
-                        lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
-                    else
-                        lr = cd_learning_rate;
+                if( cd_decrease_ct != 0 )
+                    lr = cd_learning_rate / (1.0 + stage * cd_decrease_ct );
+                else
+                    lr = cd_learning_rate;
                     
-                    lr *= persistent_cd_weight;
+                if( targetsize() > 0 )
+                    lr *= generative_learning_weight;
 
-                    setLearningRate(lr);
+                lr *= persistent_cd_weight;
 
-                    int chain_i = stage % n_gibbs_chains;
+                setLearningRate(lr);
 
-                    if( !persistent_gibbs_chain_is_started[chain_i] )
-                    {  
-                        // Start gibbs chain
-                        connection->setAsDownInput( input );
-                        hidden_layer->getAllActivations( 
-                            (RBMMatrixConnection*) connection );
-                        hidden_layer->computeExpectation();
-                        hidden_layer->generateSample();
-                        pers_cd_hidden[chain_i] << hidden_layer->sample;
-                        persistent_gibbs_chain_is_started[chain_i] = true;
-                    }
+                int chain_i = stage % n_gibbs_chains;
 
-                    if( fast_is_equal(persistent_cd_weight, 1.) )
-                    {
-                        // Hidden positive sample was not computed previously
-                        connection->setAsDownInput( input );
-                        hidden_layer->getAllActivations( 
-                            (RBMMatrixConnection*) connection );
-                        hidden_layer->computeExpectation();
-                        pos_hidden << hidden_layer->expectation;
-                    }
+                if( !persistent_gibbs_chain_is_started[chain_i] )
+                {  
+                    // Start gibbs chain
+                    connection->setAsDownInput( input );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    if( targetsize() == 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target_one_hot );
+                    else if( targetsize() > 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target );
+                        
+                    hidden_layer->computeExpectation();
+                    hidden_layer->generateSample();
+                    pers_cd_hidden[chain_i] << hidden_layer->sample;
+                    persistent_gibbs_chain_is_started[chain_i] = true;
+                }
 
-                    hidden_layer->sample << pers_cd_hidden[chain_i];
-                    // Prolonged Gibbs chain
-                    for(int i=0; i<cd_n_gibbs; i++)
+                if( fast_is_equal(persistent_cd_weight, 1.) )
+                {
+                    // Hidden positive sample was not computed previously
+                    connection->setAsDownInput( input );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    if( targetsize() == 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target_one_hot );
+                    else if( targetsize() > 1 )
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target );
+                            
+                    hidden_layer->computeExpectation();
+                    pos_hidden << hidden_layer->expectation;
+                }
+
+                hidden_layer->sample << pers_cd_hidden[chain_i];
+                // Prolonged Gibbs chain
+                for(int i=0; i<cd_n_gibbs; i++)
+                {
+                    connection->setAsUpInput( hidden_layer->sample );
+                    input_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    input_layer->computeExpectation();
+                    // LATERAL CONNECTIONS CODE HERE!
+                    input_layer->generateSample();
+                    connection->setAsDownInput( input_layer->sample );
+                    hidden_layer->getAllActivations( 
+                        (RBMMatrixConnection*) connection );
+                    if( targetsize() > 0 )
                     {
-                        connection->setAsUpInput( hidden_layer->sample );
-                        input_layer->getAllActivations( 
-                            (RBMMatrixConnection*) connection );
-                        input_layer->computeExpectation();
-                        input_layer->generateSample();
-                        connection->setAsDownInput( input_layer->sample );
-                        hidden_layer->getAllActivations( 
-                            (RBMMatrixConnection*) connection );
-                        hidden_layer->computeExpectation();
-                        hidden_layer->generateSample();
+                        target_connection->setAsUpInput( hidden_layer->sample );
+                        target_layer->getAllActivations( 
+                            (RBMMatrixConnection*) target_connection );
+                        target_layer->computeExpectation();
+                        target_layer->generateSample();
+                        productAcc( hidden_layer->activation,
+                                    target_connection->weights,
+                                    target_layer->sample );
                     }
+                    hidden_layer->computeExpectation();
+                    hidden_layer->generateSample();
+                }
 
-                    pers_cd_input[chain_i] << input_layer->sample;
-                    pers_cd_hidden[chain_i] << hidden_layer->sample;
+                pers_cd_hidden[chain_i] << hidden_layer->sample;
 
-                    input_layer->update(input, pers_cd_input[chain_i]);
-                    hidden_layer->update(pos_hidden,hidden_layer->expectation);
-                    connection->update(input,pos_hidden,
-                                       pers_cd_input[chain_i],
-                                       hidden_layer->expectation);
+                input_layer->update(input, input_layer->sample);
+                hidden_layer->update(pos_hidden,hidden_layer->expectation);
+                connection->update(input,pos_hidden,
+                                   input_layer->sample,
+                                   hidden_layer->expectation);
+                if( targetsize() > 0 )
+                {
+                    target_layer->update(target_one_hot, target_layer->sample);
+                    target_connection->update(target_one_hot,pos_hidden,
+                                              target_layer->sample,
+                                              hidden_layer->expectation);
                 }
             }
+        }
         
-            if( !fast_is_equal(denoising_learning_rate, 0.) )
-            {
-                if( denoising_decrease_ct != 0 )
-                    lr = denoising_learning_rate / 
-                        (1.0 + stage * denoising_decrease_ct );
-                else
-                    lr = denoising_learning_rate;
+        if( !fast_is_equal(denoising_learning_rate, 0.) )
+        {
+            if( denoising_decrease_ct != 0 )
+                lr = denoising_learning_rate / 
+                    (1.0 + stage * denoising_decrease_ct );
+            else
+                lr = denoising_learning_rate;
 
-                setLearningRate(lr);
-                // I'm here
-                if( fraction_of_masked_inputs > 0 )
-                    random_gen->shuffleElements(autoencoder_input_indices);
+            if( targetsize() > 0 )
+                lr *= generative_learning_weight;
+
+            setLearningRate(lr);
+            if( targetsize() > 0 )
+                PLERROR("In PseudolikelihoodRBM::train(): denoising "
+                        "autoencoder training is not implemented for "
+                        "targetsize() > 0"); 
+            if( fraction_of_masked_inputs > 0 )
+                random_gen->shuffleElements(autoencoder_input_indices);
                 
-                masked_autoencoder_input << input;
-                if( fraction_of_masked_inputs > 0 )
-                {
-                    for( int j=0 ; 
-                         j < round(fraction_of_masked_inputs*input_layer->size) ; 
-                         j++)
-                        masked_autoencoder_input[ autoencoder_input_indices[j] ] = 0; 
-                }
+            masked_autoencoder_input << input;
+            if( fraction_of_masked_inputs > 0 )
+            {
+                for( int j=0 ; 
+                     j < round(fraction_of_masked_inputs*input_layer->size) ; 
+                     j++)
+                    masked_autoencoder_input[ autoencoder_input_indices[j] ] = 0; 
+            }
 
-                // Somehow, doesn't compile without the fancy casts...
-                ((RBMMatrixConnection *)connection)->RBMConnection::fprop( masked_autoencoder_input, 
-                                   hidden_layer->activation );
+            // Somehow, doesn't compile without the fancy casts...
+            ((RBMMatrixConnection *)connection)->RBMConnection::fprop( masked_autoencoder_input, 
+                                                                       hidden_layer->activation );
 
-                hidden_layer->fprop( hidden_layer->activation,
-                                     hidden_layer->expectation );
+            hidden_layer->fprop( hidden_layer->activation,
+                                 hidden_layer->expectation );
                 
-                transpose_connection->fprop( hidden_layer->expectation,
-                                             input_layer->activation );
-                input_layer->fprop( input_layer->activation,
-                                    input_layer->expectation );
-                input_layer->setExpectation( input_layer->expectation );
+            transpose_connection->fprop( hidden_layer->expectation,
+                                         input_layer->activation );
+            input_layer->fprop( input_layer->activation,
+                                input_layer->expectation );
+            input_layer->setExpectation( input_layer->expectation );
 
-                real cost = input_layer->fpropNLL(input);
+            real cost = input_layer->fpropNLL(input);
                 
-                input_layer->bpropNLL(input, cost, 
-                                      reconstruction_activation_gradient);
-                if( only_reconstruct_masked_inputs && 
-                    fraction_of_masked_inputs > 0 )
-                {
-                    for( int j=(int)round(fraction_of_masked_inputs*input_layer->size) ; 
-                         j < input_layer->size ; 
-                         j++)
-                        reconstruction_activation_gradient[ 
-                            autoencoder_input_indices[j] ] = 0; 
-                }
-                input_layer->update( reconstruction_activation_gradient );
+            input_layer->bpropNLL(input, cost, 
+                                  reconstruction_activation_gradient);
+            if( only_reconstruct_masked_inputs && 
+                fraction_of_masked_inputs > 0 )
+            {
+                for( int j=(int)round(fraction_of_masked_inputs*input_layer->size) ; 
+                     j < input_layer->size ; 
+                     j++)
+                    reconstruction_activation_gradient[ 
+                        autoencoder_input_indices[j] ] = 0; 
+            }
+            input_layer->update( reconstruction_activation_gradient );
 
-                transpose_connection->bpropUpdate( 
-                    hidden_layer->expectation,
-                    input_layer->activation,
-                    hidden_layer_expectation_gradient,
-                    reconstruction_activation_gradient );
+            transpose_connection->bpropUpdate( 
+                hidden_layer->expectation,
+                input_layer->activation,
+                hidden_layer_expectation_gradient,
+                reconstruction_activation_gradient );
 
-                hidden_layer->bpropUpdate( hidden_layer->activation,
-                                           hidden_layer->expectation,
-                                           hidden_layer_activation_gradient,
-                                           hidden_layer_expectation_gradient );
+            hidden_layer->bpropUpdate( hidden_layer->activation,
+                                       hidden_layer->expectation,
+                                       hidden_layer_activation_gradient,
+                                       hidden_layer_expectation_gradient );
                 
-                connection->bpropUpdate( masked_autoencoder_input, 
-                                         hidden_layer->activation,
-                                         reconstruction_activation_gradient, // is not used afterwards...
-                                         hidden_layer_activation_gradient );
-            }
+            connection->bpropUpdate( masked_autoencoder_input, 
+                                     hidden_layer->activation,
+                                     reconstruction_activation_gradient, // is not used afterwards...
+                                     hidden_layer_activation_gradient );
+        }
 
-        }
+//        }
         train_stats->update( train_costs );
         
     }
@@ -1472,8 +1787,10 @@
     input_layer->setLearningRate( the_learning_rate );
     hidden_layer->setLearningRate( the_learning_rate );
     connection->setLearningRate( the_learning_rate );
-    //target_layer->setLearningRate( the_learning_rate );
-    //last_to_target->setLearningRate( the_learning_rate );
+    if( target_layer )
+        target_layer->setLearningRate( the_learning_rate );
+    if( target_connection )
+        target_connection->setLearningRate( the_learning_rate );
 }
 
 void PseudolikelihoodRBM::compute_Z() const

Modified: trunk/plearn_learners_experimental/PseudolikelihoodRBM.h
===================================================================
--- trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-07-25 20:52:46 UTC (rev 9289)
+++ trunk/plearn_learners_experimental/PseudolikelihoodRBM.h	2008-07-25 21:49:16 UTC (rev 9290)
@@ -131,6 +131,9 @@
     //! Number of most correlated input elements over which to sample
     int k_most_correlated;
 
+    //! Weight of generative learning
+    real generative_learning_weight;
+
     //! The binomial input layer of the RBM
     PP<RBMBinomialLayer> input_layer;
 
@@ -152,6 +155,12 @@
 
     PP<RBMMatrixTransposeConnection> transpose_connection;
 
+    //! The target layer of the RBM
+    PP<RBMLayer> target_layer;
+
+    //! The connection weights between the target and hidden layer
+    PP<RBMMatrixConnection> target_connection;
+
 public:
     //#####  Public Member Functions  #########################################
 
@@ -230,29 +239,12 @@
 
     //#####  Not Options  #####################################################
 
-    ////! Matrix connection weights between the hidden layer and the target layer
-    ////! (pointer to classification_module->last_to_target)
-    //PP<RBMMatrixConnection> last_to_target;
-    //
-    ////! Connection weights between the hidden layer and the target layer
-    ////! (pointer to classification_module->last_to_target)
-    //PP<RBMConnection> last_to_target_connection;
-    //
-    ////! Connection weights between the hidden layer and the visible layer
-    ////! (pointer to classification_module->joint_connection)
-    //PP<RBMConnection> joint_connection;
-    //
-    ////! Part of the RBM visible layer corresponding to the target
-    ////! (pointer to classification_module->target_layer)
-    //PP<RBMLayer> target_layer;
-
     //! Temporary variables for Contrastive Divergence
     mutable Vec target_one_hot;
 
     //! Temporary variables for RBM computations
     mutable Vec input_gradient;
     mutable Vec class_output;
-    mutable Vec before_class_output;
     mutable Vec class_gradient;
     mutable Vec hidden_activation_pos_i;
     mutable Vec hidden_activation_neg_i;
@@ -272,15 +264,16 @@
     mutable Vec gnums_act;
     mutable Vec conf;
     mutable Vec pos_input;
+    mutable Vec pos_target;
     mutable Vec pos_hidden;
     mutable Vec neg_input;
+    mutable Vec neg_target;
     mutable Vec neg_hidden;
     mutable Vec reconstruction_activation_gradient;
     mutable Vec hidden_layer_expectation_gradient;
     mutable Vec hidden_layer_activation_gradient;
     mutable Vec masked_autoencoder_input;
     mutable TVec<int> autoencoder_input_indices;
-    mutable TVec<Vec> pers_cd_input;
     mutable TVec<Vec> pers_cd_hidden;
 
     //! Keeps the index of the NLL cost in train_costs



From nouiz at mail.berlios.de  Mon Jul 28 15:49:16 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jul 2008 15:49:16 +0200
Subject: [Plearn-commits] r9291 - trunk/python_modules/plearn/learners
Message-ID: <200807281349.m6SDnGnn022609@sheep.berlios.de>

Author: nouiz
Date: 2008-07-28 15:49:16 +0200 (Mon, 28 Jul 2008)
New Revision: 9291

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
create a sub learner MultiClassAdaboost that is in C. Forward all compute*, and the test function to it. This way we are 2 time faster in some of my test.



Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-25 21:49:16 UTC (rev 9290)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-28 13:49:16 UTC (rev 9291)
@@ -2,7 +2,10 @@
 from plearn.pyplearn.plargs import *
 import time,os.path
 from numpy import array
-
+##
+## This class will be removed in the futur when MultiClassAdaboost in C is complete.
+## The C version is much faster for all compute* and the test function.
+##
 class AdaBoostMultiClasses:
     def __init__(self,trainSet1,trainSet2,weakLearner,confusion_target=1):
 #        """
@@ -23,7 +26,15 @@
             self.learner2 = self.myAdaBoostLearner(weakLearner(2),trainSet2)
             self.learner2.setExperimentDirectory(plargs.expdirr+"/learner2")
             self.learner2.setTrainingSet(trainSet2,True)
+        self.multi_class_adaboost = pl.MultiClassAdaBoost(forward_sub_learner_test_costs=True,
+                                                          report_progress=1,
+                                                          verbosity=1,
+                                                          nb_stage_to_use=-1
+                                                          )
+        self.multi_class_adaboost.learner1=self.learner1
+        self.multi_class_adaboost.learner2=self.learner2
 
+        self.multi_class_adaboost
         self.nstages = 0
         self.stage = 0
         self.train_time = 0
@@ -65,6 +76,7 @@
 
         
     def getTestCostNames(self):
+        return self.multi_class_adaboost.getTestCostNames()
         costnames = ["class_error","linear_class_error","square_class_error"]
         for i in range(3):
             for j in range(3):
@@ -80,6 +92,7 @@
         return costnames
     
     def getTrainCostNames(self):
+        return self.multi_class_adaboost.getTrainCostNames()
         costnames = ["sublearner1."+x for x in self.learner1.getTrainCostNames()]
         costnames += ["sublearner2."+x for x in self.learner2.getTrainCostNames()]                                
         return costnames
@@ -108,6 +121,7 @@
         
         return a tuple: (predicted result, output of sub learner1,output of sub learner2)
         """
+        return self.multi_class_adaboost.computeOutput(input)
         out1=self.learner1.computeOutput(input)[0]
         out2=self.learner2.computeOutput(input)[0]
         ind1=int(round(out1))
@@ -123,6 +137,8 @@
         return (ind,out1,out2)
     
     def computeCostsFromOutput(self,input,output,target_,costs=[],forward_sub_learner_costs=True):
+
+        return self.multi_class_adaboost.computeCostsFromOutput(input,output,target_)
         target=int(target_)
         del costs[:]
         class_error=int(output[0] != target)
@@ -165,12 +181,19 @@
         return costs
 
     def computeOutputAndCosts(self,input,target):
+        return self.multi_class_adaboost.computeOutputAndCosts(input,target)
         output=self.computeOutput(input)
         costs=self.computeCostsFromOutput(input,output,target)
         return (output,costs)
 
     def test(self,testSet,test_stats,return_outputs,return_costs):
         t1=time.time()
+        (test_stats2,outputs,costs)=self.multi_class_adaboost.test(testSet, test_stats, return_outputs, return_costs)
+        t2=time.time()
+        self.test_sub_time+=t2-t1
+
+        return(test_stats,outputs,costs)
+
         testSet1=pl.ProcessingVMatrix(source=testSet,
                                prg = "[%0:%"+str(testSet.inputsize-1)+"] @CLASSE_REEL 1 0 ifelse :CLASSE_REEL")
         testSet2=pl.ProcessingVMatrix(source=testSet,
@@ -284,7 +307,8 @@
 
         self.learner1.setTrainStatsCollector(VecStatsCollector())
         self.learner2.setTrainStatsCollector(VecStatsCollector())
+        self.multi_class_adaboost.learner1=self.learner1
+        self.multi_class_adaboost.learner2=self.learner2
 
-
     def print_time(self):
         print "train time=%.2f test time=%.2f test sub time=%.2f"%(self.train_time,self.test_time,self.test_sub_time)



From nouiz at mail.berlios.de  Mon Jul 28 19:58:21 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jul 2008 19:58:21 +0200
Subject: [Plearn-commits] r9292 - trunk/plearn_learners/meta
Message-ID: <200807281758.m6SHwLNV015224@sheep.berlios.de>

Author: nouiz
Date: 2008-07-28 19:58:20 +0200 (Mon, 28 Jul 2008)
New Revision: 9292

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
added a check


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-07-28 13:49:16 UTC (rev 9291)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-07-28 17:58:20 UTC (rev 9292)
@@ -362,7 +362,9 @@
         sum_voting_weights = 0;
         voting_weights.resize(0,nstages);
 
-    }
+    } else
+        PLCHECK_MSG(example_weights.length()==n,"In AdaBoost::train - the train"
+                    " set should not change between each train without a forget!");
 
     VMat unweighted_data = train_set.subMatColumns(0, inputsize()+1);
     learners_error.resize(nstages);



From nouiz at mail.berlios.de  Mon Jul 28 21:48:35 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jul 2008 21:48:35 +0200
Subject: [Plearn-commits] r9293 - trunk/python_modules/plearn/vmat
Message-ID: <200807281948.m6SJmZ0G029007@sheep.berlios.de>

Author: nouiz
Date: 2008-07-28 21:48:33 +0200 (Mon, 28 Jul 2008)
New Revision: 9293

Modified:
   trunk/python_modules/plearn/vmat/PMat.py
Log:
more explicit error


Modified: trunk/python_modules/plearn/vmat/PMat.py
===================================================================
--- trunk/python_modules/plearn/vmat/PMat.py	2008-07-28 17:58:20 UTC (rev 9292)
+++ trunk/python_modules/plearn/vmat/PMat.py	2008-07-28 19:48:33 UTC (rev 9293)
@@ -393,7 +393,7 @@
 
     def appendRow(self,row):
         if len(row)!=self.width:
-            raise TypeError('length of row ('+str(len(row))+ ') differs from matrix width ('+str(self.width)+')')
+            raise TypeError('length of row ('+str(len(row))+ ') differs from matrix width ('+str(self.width)+') for file "'+self.fname+'"')
         if self.swap_bytes: # must make a copy and swap bytes
             ar = numpy.numarray.numarray(row,type=self.elemtype)
             ar.byteswap(True)



From nouiz at mail.berlios.de  Mon Jul 28 22:33:19 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jul 2008 22:33:19 +0200
Subject: [Plearn-commits] r9294 - trunk/python_modules/plearn/learners
Message-ID: <200807282033.m6SKXJ1h000822@sheep.berlios.de>

Author: nouiz
Date: 2008-07-28 22:33:18 +0200 (Mon, 28 Jul 2008)
New Revision: 9294

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
somes bugfix to allow reloading the old learner now that we don't save the sorted_train_set in RegressionTree.


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-28 19:48:33 UTC (rev 9293)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-28 20:33:18 UTC (rev 9294)
@@ -18,6 +18,11 @@
         self.trainSet1=trainSet1
         self.trainSet2=trainSet2
 
+        self.multi_class_adaboost = pl.MultiClassAdaBoost(forward_sub_learner_test_costs=True,
+                                                          report_progress=1,
+                                                          verbosity=1,
+                                                          nb_stage_to_use=-1
+                                                          )
         if weakLearner:
             self.learner1 = self.myAdaBoostLearner(weakLearner(0),trainSet1)
             self.learner1.setExperimentDirectory(plargs.expdirr+"/learner1")
@@ -26,15 +31,9 @@
             self.learner2 = self.myAdaBoostLearner(weakLearner(2),trainSet2)
             self.learner2.setExperimentDirectory(plargs.expdirr+"/learner2")
             self.learner2.setTrainingSet(trainSet2,True)
-        self.multi_class_adaboost = pl.MultiClassAdaBoost(forward_sub_learner_test_costs=True,
-                                                          report_progress=1,
-                                                          verbosity=1,
-                                                          nb_stage_to_use=-1
-                                                          )
-        self.multi_class_adaboost.learner1=self.learner1
-        self.multi_class_adaboost.learner2=self.learner2
+            self.multi_class_adaboost.learner1=self.learner1
+            self.multi_class_adaboost.learner2=self.learner2
 
-        self.multi_class_adaboost
         self.nstages = 0
         self.stage = 0
         self.train_time = 0
@@ -65,7 +64,7 @@
         self.learner1.train()
         self.learner2.nstages = self.nstages
         self.learner2.train()
-        self.stage=self.learner1.stage
+        self.stage=max(self.learner1.stage,self.learner2.stage)
         t2=time.time()
         self.train_time+=t2-t1
         self.train_stats=VecStatsCollector()
@@ -309,6 +308,9 @@
         self.learner2.setTrainStatsCollector(VecStatsCollector())
         self.multi_class_adaboost.learner1=self.learner1
         self.multi_class_adaboost.learner2=self.learner2
+        for (learner,trainSet) in [(self.learner1,trainSet1), (self.learner2,trainSet2)]:
+            for weak in learner.weak_learners:
+                weak.setTrainingSet(trainSet,False)
 
     def print_time(self):
         print "train time=%.2f test time=%.2f test sub time=%.2f"%(self.train_time,self.test_time,self.test_sub_time)



From nouiz at mail.berlios.de  Mon Jul 28 23:00:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jul 2008 23:00:32 +0200
Subject: [Plearn-commits] r9295 - trunk/plearn_learners/hyper
Message-ID: <200807282100.m6SL0Wnb003344@sheep.berlios.de>

Author: nouiz
Date: 2008-07-28 23:00:32 +0200 (Mon, 28 Jul 2008)
New Revision: 9295

Modified:
   trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
Log:
added some check


Modified: trunk/plearn_learners/hyper/EarlyStoppingOracle.cc
===================================================================
--- trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2008-07-28 20:33:18 UTC (rev 9294)
+++ trunk/plearn_learners/hyper/EarlyStoppingOracle.cc	2008-07-28 21:00:32 UTC (rev 9295)
@@ -133,8 +133,12 @@
     {
         option_values.resize(0);
         real step = 1;
-        if(range.length()==3)
+        if(range.length()==3){
             step = range[2];
+            if(step<0 || is_equal(step,0))
+                PLERROR("IN EarlyStoppingOracle::build_() - the"
+                        " step(=%f) should be greater the 0!",step);
+        }
         for(real x = range[0]; x<range[1]; x+=step)
             option_values.append(tostring(x));
         if(fast_exact_is_equal(range[0], range[1]))



From nouiz at mail.berlios.de  Mon Jul 28 23:04:37 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Mon, 28 Jul 2008 23:04:37 +0200
Subject: [Plearn-commits] r9296 - trunk/plearn_learners/regressors
Message-ID: <200807282104.m6SL4bx7003622@sheep.berlios.de>

Author: nouiz
Date: 2008-07-28 23:04:37 +0200 (Mon, 28 Jul 2008)
New Revision: 9296

Modified:
   trunk/plearn_learners/regressors/RegressionTreeNode.h
Log:
Added fct RegressionTreeNode::haveChildrenNode() 


Modified: trunk/plearn_learners/regressors/RegressionTreeNode.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-07-28 21:00:32 UTC (rev 9295)
+++ trunk/plearn_learners/regressors/RegressionTreeNode.h	2008-07-28 21:04:37 UTC (rev 9296)
@@ -114,7 +114,7 @@
     {
         computeOutputAndNodes(inputv,outputv);
     }
-
+    bool         haveChildrenNode(){return left_node;}
     
 private:
     void         build_();



From nouiz at mail.berlios.de  Tue Jul 29 15:59:02 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Jul 2008 15:59:02 +0200
Subject: [Plearn-commits] r9297 - trunk/plearn/vmat
Message-ID: <200807291359.m6TDx2Zw008124@sheep.berlios.de>

Author: nouiz
Date: 2008-07-29 15:59:01 +0200 (Tue, 29 Jul 2008)
New Revision: 9297

Modified:
   trunk/plearn/vmat/ConcatColumnsVMatrix.cc
Log:
don't check the length if it is -1. If we use a ConstantVMatrix, this can happen.


Modified: trunk/plearn/vmat/ConcatColumnsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2008-07-28 21:04:37 UTC (rev 9296)
+++ trunk/plearn/vmat/ConcatColumnsVMatrix.cc	2008-07-29 13:59:01 UTC (rev 9297)
@@ -103,7 +103,7 @@
     for(int i=0; i<sources.size(); i++)
     {
         updateMtime(sources[i]);
-        if(sources[i]->length()!=length_)
+        if(sources[i]->length()!=length_ && sources[i]->length()!=-1)
             PLERROR("ConcatColumnsVMatrix: Problem concatenating two VMat with"
                     " different lengths");
         if(sources[i]->width() == -1)



From nouiz at mail.berlios.de  Tue Jul 29 16:15:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Tue, 29 Jul 2008 16:15:47 +0200
Subject: [Plearn-commits] r9298 - trunk/plearn_learners/meta
Message-ID: <200807291415.m6TEFlb4009182@sheep.berlios.de>

Author: nouiz
Date: 2008-07-29 16:15:46 +0200 (Tue, 29 Jul 2008)
New Revision: 9298

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
we take the test cost names from the weak_learner_template instead of the first learner. In some case we want the test cost names when their is no weak learner done. In other case, when we reload old learner, the trainned weak learner don't have anymoret their train set. The RegressionTree need the train set to give the test cost names.


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-07-29 13:59:01 UTC (rev 9297)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-07-29 14:15:46 UTC (rev 9298)
@@ -743,7 +743,7 @@
     TVec<string> costs=getTrainCostNames();
 
     if(forward_sub_learner_test_costs){
-        TVec<string> subcosts=weak_learners[0]->getTestCostNames();
+        TVec<string> subcosts=weak_learner_template->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
             subcosts[i]="weighted_weak_learner."+subcosts[i];
         }



From saintmlx at mail.berlios.de  Tue Jul 29 22:30:39 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 29 Jul 2008 22:30:39 +0200
Subject: [Plearn-commits] r9299 - trunk/plearn/vmat
Message-ID: <200807292030.m6TKUdea026089@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-29 22:30:36 +0200 (Tue, 29 Jul 2008)
New Revision: 9299

Modified:
   trunk/plearn/vmat/SelectRowsVMatrix.cc
   trunk/plearn/vmat/SelectRowsVMatrix.h
   trunk/plearn/vmat/VMatrix.cc
Log:
- add support for extrasize



Modified: trunk/plearn/vmat/SelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-07-29 14:15:46 UTC (rev 9298)
+++ trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-07-29 20:30:36 UTC (rev 9299)
@@ -55,6 +55,7 @@
     : obtained_inputsize_from_source(false),
       obtained_targetsize_from_source(false),
       obtained_weightsize_from_source(false),
+      obtained_extrasize_from_source(false),
       rows_to_remove(false)
 {}
 
@@ -62,6 +63,7 @@
     : obtained_inputsize_from_source(false),
       obtained_targetsize_from_source(false),
       obtained_weightsize_from_source(false),
+      obtained_extrasize_from_source(false),
       indices(the_indices),
       rows_to_remove(the_rows_to_remove)
 {
@@ -74,6 +76,7 @@
     : obtained_inputsize_from_source(false),
       obtained_targetsize_from_source(false),
       obtained_weightsize_from_source(false),
+      obtained_extrasize_from_source(false),
       rows_to_remove(the_rows_to_remove)
 {
     source = the_source;
@@ -129,6 +132,9 @@
     declareOption(ol, "obtained_weightsize_from_source", &SelectRowsVMatrix::obtained_weightsize_from_source, OptionBase::learntoption,
                   "Set to 1 if the weightsize was obtained from the source VMat.");
 
+    declareOption(ol, "obtained_extrasize_from_source", &SelectRowsVMatrix::obtained_extrasize_from_source, OptionBase::learntoption,
+                  "Set to 1 if the weightsize was obtained from the source VMat.");
+
     inherited::declareOptions(ol);
 
     // Hide unused options.
@@ -220,6 +226,11 @@
             obtained_weightsize_from_source = true;
         } else if (obtained_weightsize_from_source && weightsize_ != source->weightsize())
             PLERROR(error_msg.c_str());
+        if(extrasize_ == 0) {
+            extrasize_ = source->extrasize();
+            obtained_extrasize_from_source = true;
+        } else if (obtained_extrasize_from_source && extrasize_ != source->extrasize())
+            PLERROR(error_msg.c_str());
         fieldinfos = source->fieldinfos;
     } else {
         // Restore the original undefined sizes if the current one had been obtained

Modified: trunk/plearn/vmat/SelectRowsVMatrix.h
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.h	2008-07-29 14:15:46 UTC (rev 9298)
+++ trunk/plearn/vmat/SelectRowsVMatrix.h	2008-07-29 20:30:36 UTC (rev 9299)
@@ -65,6 +65,7 @@
     bool obtained_inputsize_from_source;
     bool obtained_targetsize_from_source;
     bool obtained_weightsize_from_source;
+    bool obtained_extrasize_from_source;
 
     TVec<int> selected_indices;
 

Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-29 14:15:46 UTC (rev 9298)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-29 20:30:36 UTC (rev 9299)
@@ -160,12 +160,10 @@
         "DO NOT play with this if you don't know the implementation!\n"
         "This add a dependency mtime to the gived value.");
 
-
     declareOption(
         ol, "fieldinfos", &VMatrix::fieldinfos, OptionBase::buildoption,
         "Field infos.\n");
 
-
     inherited::declareOptions(ol);
 }
 



From saintmlx at mail.berlios.de  Tue Jul 29 23:03:31 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Tue, 29 Jul 2008 23:03:31 +0200
Subject: [Plearn-commits] r9300 - trunk/plearn/vmat
Message-ID: <200807292103.m6TL3VRS030179@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-29 23:03:28 +0200 (Tue, 29 Jul 2008)
New Revision: 9300

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
- declare saveAMAT and saveDMAT as remote



Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-29 20:30:36 UTC (rev 9299)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-29 21:03:28 UTC (rev 9300)
@@ -288,11 +288,24 @@
          RetDoc ("dot product")));         
 
     declareMethod(
+        rmm, "saveAMAT", &VMatrix::saveAMAT,
+        (BodyDoc("Saves this matrix as an .amat file."),
+         ArgDoc ("amatfile", "Path of the file to create."),
+         ArgDoc ("verbose", "output details?"),
+         ArgDoc ("no_header", "save data only"),
+         ArgDoc ("save_strings", "save string instead of real values")));
+
+    declareMethod(
         rmm, "savePMAT", &VMatrix::savePMAT,
         (BodyDoc("Saves this matrix as a .pmat file."),
          ArgDoc ("pmatfile", "Path of the file to create.")));
 
     declareMethod(
+        rmm, "saveDMAT", &VMatrix::saveDMAT,
+        (BodyDoc("Saves this matrix as a .dmat directory."),
+         ArgDoc ("dmatdir", "Path of the dir to create.")));
+
+    declareMethod(
         rmm, "subMat", &VMatrix::subMat,
         (BodyDoc("Return a sub-matrix from a VMatrix\n"),
          ArgDoc ("i", "start row"),
@@ -2007,7 +2020,7 @@
     ProgressBar pb(cout, "Saving to pmat", nsamples);
 
     for(int i=0; i<nsamples; i++)
-    {
+    { 
         getRow(i,tmpvec);
         m.putRow(i,tmpvec);
         pb(i);



From saintmlx at mail.berlios.de  Wed Jul 30 00:04:15 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Wed, 30 Jul 2008 00:04:15 +0200
Subject: [Plearn-commits] r9301 - trunk/plearn/base
Message-ID: <200807292204.m6TM4FcI002753@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-30 00:04:15 +0200 (Wed, 30 Jul 2008)
New Revision: 9301

Modified:
   trunk/plearn/base/plerror.h
Log:
- we need meaningful error messages... please don't re-undo this change



Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2008-07-29 21:03:28 UTC (rev 9300)
+++ trunk/plearn/base/plerror.h	2008-07-29 22:04:15 UTC (rev 9301)
@@ -63,8 +63,7 @@
 extern ostream* error_stream;
 #endif
 
-//#define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
-#define PLERROR   errormsg //Use if the compiler don't like variadic macros
+#define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
 #define PLWARNING warningmsg
 #define PLDEPRECATED deprecationmsg
 



From nouiz at mail.berlios.de  Wed Jul 30 15:16:44 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 15:16:44 +0200
Subject: [Plearn-commits] r9302 - trunk/plearn/vmat
Message-ID: <200807301316.m6UDGiqD019209@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 15:16:43 +0200 (Wed, 30 Jul 2008)
New Revision: 9302

Modified:
   trunk/plearn/vmat/SelectRowsVMatrix.cc
Log:
corrected comment


Modified: trunk/plearn/vmat/SelectRowsVMatrix.cc
===================================================================
--- trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-07-29 22:04:15 UTC (rev 9301)
+++ trunk/plearn/vmat/SelectRowsVMatrix.cc	2008-07-30 13:16:43 UTC (rev 9302)
@@ -133,7 +133,7 @@
                   "Set to 1 if the weightsize was obtained from the source VMat.");
 
     declareOption(ol, "obtained_extrasize_from_source", &SelectRowsVMatrix::obtained_extrasize_from_source, OptionBase::learntoption,
-                  "Set to 1 if the weightsize was obtained from the source VMat.");
+                  "Set to 1 if the extrasize was obtained from the source VMat.");
 
     inherited::declareOptions(ol);
 



From nouiz at mail.berlios.de  Wed Jul 30 15:44:24 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 15:44:24 +0200
Subject: [Plearn-commits] r9303 - trunk/plearn_learners/regressors
Message-ID: <200807301344.m6UDiOnx022043@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 15:44:24 +0200 (Wed, 30 Jul 2008)
New Revision: 9303

Modified:
   trunk/plearn_learners/regressors/RegressionTree.cc
   trunk/plearn_learners/regressors/RegressionTree.h
Log:
-added profiling
-small bug fix in the buil_()(always init some var event if their is no test_set)
-small uptimization in train() (use computeOutputAndCosts())


Modified: trunk/plearn_learners/regressors/RegressionTree.cc
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.cc	2008-07-30 13:16:43 UTC (rev 9302)
+++ trunk/plearn_learners/regressors/RegressionTree.cc	2008-07-30 13:44:24 UTC (rev 9303)
@@ -193,10 +193,12 @@
         if (weightsize != 1 && weightsize != 0)
             PLERROR("RegressionTree: expected weightsize to be 1 or 0, got %d",
                     weightsize);
-        nodes = new TVec<PP<RegressionTreeNode> >();
-        tmp_vec.resize(2);
     }
 
+    tmp_vec.resize(2);
+    nodes = new TVec<PP<RegressionTreeNode> >();
+    tmp_computeCostsFromOutput.resize(outputsize());
+    
     if (loss_function_weight != 0.0)
     {
         l2_loss_function_factor = 2.0 / pow(loss_function_weight, 2);
@@ -211,6 +213,8 @@
 
 void RegressionTree::train()
 {
+    Profiler::pl_profile_start("RegressionTree::train");
+
     if (stage == 0) initialiseTree();
     PP<ProgressBar> pb;
     if (report_progress)
@@ -231,7 +235,10 @@
     pb = NULL;
     verbose("split_cols: "+tostring(split_cols),2);
     verbose("split_values: "+tostring(split_values),2);
-    if (compute_train_stats < 1) return;
+    if (compute_train_stats < 1){
+        Profiler::pl_profile_end("RegressionTree::train");
+        return;
+    }
     if (report_progress)
     {
         pb = new ProgressBar("RegressionTree : computing the statistics: ", length);
@@ -248,12 +255,13 @@
          train_sample_index++)
     {  
         sorted_train_set->getExample(train_sample_index, sample_input, sample_target, sample_weight);
-        computeOutput(sample_input, sample_output);
-        computeCostsFromOutputs(sample_input, sample_output, sample_target, sample_costs); 
+        computeOutputAndCosts(sample_input,sample_target,sample_output,sample_costs);
         train_stats->update(sample_costs);
         if (report_progress) pb->update(train_sample_index);
     }
     train_stats->finalize();
+
+    Profiler::pl_profile_end("RegressionTree::train");
 }
 
 void RegressionTree::verbose(string the_msg, int the_level)
@@ -415,42 +423,50 @@
     outputv[0] = closest_value;
 }
 
-void RegressionTree::computeOutputAndCosts(const Vec& inputv,
-                                           const Vec& targetv,
-                                           Vec& outputv, Vec& costsv) const
+void RegressionTree::computeOutputAndCosts(const Vec& input,
+                                           const Vec& target,
+                                           Vec& output, Vec& costs) const
 {
-    PLASSERT(costsv.size()==nTestCosts());
+    PLASSERT(costs.size()==nTestCosts());
     PLASSERT(nodes);
     nodes->resize(0);
 
-    computeOutputAndNodes(inputv, tmp_vec, nodes);
+    computeOutputAndNodes(input, tmp_vec, nodes);
     if(!output_confidence_target)
-        outputv[0]=tmp_vec[0];
+        output[0]=tmp_vec[0];
     else
-        outputv<<tmp_vec;
+        output<<tmp_vec;
 
-    costsv.clear();
-    costsv[0] = pow((outputv[0] - targetv[0]), 2);
-    
-    costsv[1] = tmp_vec[1];
-    costsv[2] = 1.0 - (l2_loss_function_factor * costsv[0]);
-    costsv[3] = 1.0 - (l1_loss_function_factor * abs(outputv[0] - targetv[0]));
-    costsv[4] = !fast_is_equal(targetv[0],outputv[0]);
-    for(int i=0;i<nodes->length();i++)
-        costsv[5+(*nodes)[i]->getSplitCol()]++;
+    computeCostsFromOutputsAndNodes(input, tmp_vec, target, *nodes, costs);
 }
 
+void RegressionTree::computeCostsFromOutputsAndNodes(const Vec& input,
+                                                     const Vec& output, 
+                                                     const Vec& target,
+                                                     const TVec<PP<RegressionTreeNode> >& nodes,
+                                                     Vec& costs) const
+{
+    costs.clear();
+    costs[0] = pow((output[0] - target[0]), 2);
+    if(output.length()>1) costs[1] = output[1];
+    else costs[1] = MISSING_VALUE;
+    costs[2] = 1.0 - (l2_loss_function_factor * costs[0]);
+    costs[3] = 1.0 - (l1_loss_function_factor * abs(output[0] - target[0]));
+    costs[4] = !fast_is_equal(target[0],output[0]);
+
+    for(int i=0;i<nodes.length();i++)
+        costs[5+nodes[i]->getSplitCol()]++;
+}
+
 void RegressionTree::computeCostsFromOutputs(const Vec& input,
                                              const Vec& output, 
                                              const Vec& target,
                                              Vec& costs) const
 {
-    Vec tmp(output.size());
-    computeOutputAndCosts(input, target, tmp, costs); 
-    PLASSERT(output==tmp);
+    computeOutputAndCosts(input, target, tmp_computeCostsFromOutput, costs); 
+    PLASSERT(output==tmp_computeCostsFromOutput);
 }
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/plearn_learners/regressors/RegressionTree.h
===================================================================
--- trunk/plearn_learners/regressors/RegressionTree.h	2008-07-30 13:16:43 UTC (rev 9302)
+++ trunk/plearn_learners/regressors/RegressionTree.h	2008-07-30 13:44:24 UTC (rev 9303)
@@ -93,6 +93,8 @@
     TVec<PP<RegressionTreeNode> > *nodes;
 
     mutable Vec tmp_vec;
+    mutable Vec tmp_computeCostsFromOutput;
+
 public:
     RegressionTree();
     virtual              ~RegressionTree();
@@ -114,7 +116,11 @@
     virtual void         computeOutputAndNodes(const Vec& input, Vec& output,
                                                TVec<PP<RegressionTreeNode> >* nodes=0) const;
     virtual void         computeCostsFromOutputs(const Vec& input, const Vec& output, const Vec& target, Vec& costs) const;
-  
+    virtual void         computeCostsFromOutputsAndNodes(const Vec& input,
+                                                         const Vec& output, 
+                                                         const Vec& target,
+                                                         const TVec<PP<RegressionTreeNode> >& nodes,
+                                                         Vec& costs) const;
 private:
     void                   build_();
     void                   initialiseTree();



From nouiz at mail.berlios.de  Wed Jul 30 17:10:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 17:10:32 +0200
Subject: [Plearn-commits] r9304 - trunk/plearn/misc
Message-ID: <200807301510.m6UFAWQb030001@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 17:10:30 +0200 (Wed, 30 Jul 2008)
New Revision: 9304

Modified:
   trunk/plearn/misc/vmatmain.cc
Log:
print the string value when their is a string mapping when converting to a csv file.


Modified: trunk/plearn/misc/vmatmain.cc
===================================================================
--- trunk/plearn/misc/vmatmain.cc	2008-07-30 13:44:24 UTC (rev 9303)
+++ trunk/plearn/misc/vmatmain.cc	2008-07-30 15:10:30 UTC (rev 9304)
@@ -97,7 +97,7 @@
     PP<ProgressBar> pb;
     if (verbose)
         pb = new ProgressBar(cout, "Saving to CSV", source.length());
-  
+
     // Next, output each line.  Perform missing-value checks if required.
     for (int i=0, n=source.length() ; i<n ; ++i) {
         if (pb)
@@ -105,28 +105,33 @@
         Vec currow = source(i);
         if (! skip_missings || ! currow.hasMissing()) {
             for (int j=0, m=currow.size() ; j<m ; ++j) {
+                string strval="";
                 if (convert_date && j==0)
                     // Date conversion: add 19000000 to convert from CYYMMDD to
                     // YYYYMMDD, and always output without trailing . if not
                     // necessary
                     sprintf(buffer, "%8f", currow[j] + 19000000.0);
-                else
+                else if((strval=source->getValString(j,currow[j]))!=""){
+                    if(strval.length()>1000-1)
+                        PLERROR("a value is too big!");
+                    strncpy(buffer,strval.c_str(),1000);
+                }else{
                     // Normal processing
                     sprintf(buffer, "%#.*f", precision, currow[j]);
 
-                // strip all trailing zeros and final period
-                // there is always a period since sprintf includes # modifier
-                char* period = buffer;
-                while (*period && *period != '.')
-                    period++;
-                for (char* last = period + strlen(period) - 1 ;
-                     last >= period && (*last == '0' || *last == '.') ; --last) {
-                    bool should_break = *last == '.';
-                    *last = '\0';
-                    if (should_break)
-                        break;
+                    // strip all trailing zeros and final period
+                    // there is always a period since sprintf includes # modifier
+                    char* period = buffer;
+                    while (*period && *period != '.')
+                        period++;
+                    for (char* last = period + strlen(period) - 1 ;
+                         last >= period && (*last == '0' || *last == '.') ; --last) {
+                        bool should_break = *last == '.';
+                        *last = '\0';
+                        if (should_break)
+                            break;
+                    }
                 }
-        
                 destination << buffer;
                 if (j < m-1)
                     destination << delimiter;



From nouiz at mail.berlios.de  Wed Jul 30 17:14:54 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 17:14:54 +0200
Subject: [Plearn-commits] r9305 - trunk/plearn/base
Message-ID: <200807301514.m6UFEsp3030253@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 17:14:53 +0200 (Wed, 30 Jul 2008)
New Revision: 9305

Modified:
   trunk/plearn/base/plerror.h
Log:
re put a version of PLERROR that work with ISO c90 in comment. So people will find what to do more easily


Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2008-07-30 15:10:30 UTC (rev 9304)
+++ trunk/plearn/base/plerror.h	2008-07-30 15:14:53 UTC (rev 9305)
@@ -63,6 +63,7 @@
 extern ostream* error_stream;
 #endif
 
+//#define PLERROR   errormsg //Use if the compiler don't like variadic macros
 #define PLERROR(...)   errormsg2(__FILE__,__LINE__,__VA_ARGS__)
 #define PLWARNING warningmsg
 #define PLDEPRECATED deprecationmsg



From nouiz at mail.berlios.de  Wed Jul 30 17:51:59 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 17:51:59 +0200
Subject: [Plearn-commits] r9306 - trunk
Message-ID: <200807301551.m6UFpxb9000557@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 17:51:59 +0200 (Wed, 30 Jul 2008)
New Revision: 9306

Modified:
   trunk/pymake.config.model
Log:
remove warning about variadic macro in gcc. We use the -pedantic option that check the conformance with iso c90. variadic macro are added in iso c99.


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-07-30 15:14:53 UTC (rev 9305)
+++ trunk/pymake.config.model	2008-07-30 15:51:59 UTC (rev 9306)
@@ -526,7 +526,10 @@
 else:
     gcc_opt_options = ''
 
-pedantic_mode = ' -pedantic '
+#-pedantic check for conformity with ISO C
+#plerror.h use variadic macro that is introduced in ISO C99
+#So we add -Wno-variadic-macros to allow this exception to ISO C90
+pedantic_mode = ' -pedantic -Wno-variadic-macros '
 if platform=='darwin':
     pedantic_mode = ' '
     



From nouiz at mail.berlios.de  Wed Jul 30 17:53:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 17:53:29 +0200
Subject: [Plearn-commits] r9307 - trunk
Message-ID: <200807301553.m6UFrTP9000627@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 17:53:27 +0200 (Wed, 30 Jul 2008)
New Revision: 9307

Modified:
   trunk/pymake.config.model
Log:
added option that are commented to disable the lock in the vmatrix


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-07-30 15:51:59 UTC (rev 9306)
+++ trunk/pymake.config.model	2008-07-30 15:53:27 UTC (rev 9307)
@@ -271,8 +271,10 @@
   [ 'logging=dbg', 'logging=mand', 'logging=imp', 'logging=normal',
     'logging=extreme', 'logging=dbg-profile' ],
 
-  [ 'numpy', 'numarray' ]
+  [ 'numpy', 'numarray' ],
 
+#  [ '', 'nolock' ]
+
 ]
 
 ### Using Python code snippets in C++ code
@@ -721,7 +723,12 @@
 
 cpp_variables += ['BOUNDCHECK', 'NDEBUG', 'Py_DEBUG']
 
+pymakeOption( name = '',description = 'dummy option to hide optional options' )
+pymakeOption( name = 'nolock',
+              description = 'USE WITH CARE: disable vmatrix lock',
+              cpp_definitions = ['DISABLE_VMATRIX_LOCK'])
 
+cpp_variables += ['DISABLE_VMATRIX_LOCK']
 #####  Floating-Point Number Representation  ################################
 
 pymakeOption( name = 'float',



From nouiz at mail.berlios.de  Wed Jul 30 17:57:29 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 17:57:29 +0200
Subject: [Plearn-commits] r9308 - trunk/plearn_learners/meta
Message-ID: <200807301557.m6UFvT5K000834@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 17:57:29 +0200 (Wed, 30 Jul 2008)
New Revision: 9308

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
try to be more friendly with RegressionTree.getTestCostNames() that need a train_set


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-07-30 15:53:27 UTC (rev 9307)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-07-30 15:57:29 UTC (rev 9308)
@@ -743,7 +743,15 @@
     TVec<string> costs=getTrainCostNames();
 
     if(forward_sub_learner_test_costs){
-        TVec<string> subcosts=weak_learner_template->getTestCostNames();
+        TVec<string> subcosts;
+        //We try to find a weak_learner with a train set
+        //as a RegressionTree need it to generate the test costs names
+        if(weak_learner_template->getTrainingSet())
+            subcosts=weak_learner_template->getTestCostNames();
+        else if(weak_learners.length()>0)
+            subcosts=weak_learners[0]->getTestCostNames();
+        else
+            subcosts=weak_learner_template->getTestCostNames();
         for(int i=0;i<subcosts.length();i++){
             subcosts[i]="weighted_weak_learner."+subcosts[i];
         }



From nouiz at mail.berlios.de  Wed Jul 30 17:58:23 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 17:58:23 +0200
Subject: [Plearn-commits] r9309 - trunk/plearn/vmat
Message-ID: <200807301558.m6UFwNIx000875@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 17:58:23 +0200 (Wed, 30 Jul 2008)
New Revision: 9309

Modified:
   trunk/plearn/vmat/VMatrix.cc
Log:
added a definition DISABLE_VMATRIX_LOCK, that when it exist disable the lock in VMatrix


Modified: trunk/plearn/vmat/VMatrix.cc
===================================================================
--- trunk/plearn/vmat/VMatrix.cc	2008-07-30 15:57:29 UTC (rev 9308)
+++ trunk/plearn/vmat/VMatrix.cc	2008-07-30 15:58:23 UTC (rev 9309)
@@ -1415,6 +1415,7 @@
 /////////////////////
 void VMatrix::lockMetaDataDir(time_t max_lock_age, bool verbose) const
 {
+#ifndef DISABLE_VMATRIX_LOCK
     if(!hasMetaDataDir())
         PLERROR("In VMatrix::lockMetaDataDir() subclass %s -"
                 " metadatadir was not set", classname().c_str());
@@ -1452,6 +1453,7 @@
     string lock_content = "host " + hostname() + ", pid " + tostring(getPid()) + ", user " + getUser();
     lockf_ << lock_content;
     lockf_.flush();
+#endif
 }
 
 ///////////////////////
@@ -1459,11 +1461,13 @@
 ///////////////////////
 void VMatrix::unlockMetaDataDir() const
 {
+#ifndef DISABLE_VMATRIX_LOCK
     if(!lockf_)
         PLERROR("In VMatrix::unlockMetaDataDir() was called while no lock is held by this object");
     lockf_ = PStream();   // Release the lock.
     PPath lockfile = metadatadir / ".lock";
     rm(lockfile); // Remove the file.
+#endif
 }
 
 ////////////////////



From nouiz at mail.berlios.de  Wed Jul 30 19:42:09 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 19:42:09 +0200
Subject: [Plearn-commits] r9310 - trunk/plearn/vmat
Message-ID: <200807301742.m6UHg9mA031442@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 19:42:08 +0200 (Wed, 30 Jul 2008)
New Revision: 9310

Modified:
   trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
Log:
renamed variable to don't hide a class variable


Modified: trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc
===================================================================
--- trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-30 15:58:23 UTC (rev 9309)
+++ trunk/plearn/vmat/MeanMedianModeImputationVMatrix.cc	2008-07-30 17:42:08 UTC (rev 9310)
@@ -392,14 +392,14 @@
     cout << fixed << showpoint;
     ProgressBar* pb = 0;
     pb = new ProgressBar("Computing the mean, median and mode vectors", width_);
-    VMat train_set = new MemoryVMatrix(train_set);
+    VMat train_set_mem = new MemoryVMatrix(train_set);
     for (int train_col = 0; train_col < width_; train_col++)
     {
         real current_value = 0.0;
         int current_value_count = 0;
-        train_set->getColumn(train_col, variable_vec);
-        sortColumn(variable_vec, 0, train_set->length());
-        for (int train_row = 0; train_row < train_set->length(); train_row++)
+        train_set_mem->getColumn(train_col, variable_vec);
+        sortColumn(variable_vec, 0, train_set_mem->length());
+        for (int train_row = 0; train_row < train_set_mem->length(); train_row++)
         {
             if (is_missing(variable_vec[train_row]))
 	      continue;



From tihocan at mail.berlios.de  Wed Jul 30 19:54:57 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Jul 2008 19:54:57 +0200
Subject: [Plearn-commits] r9311 - trunk/plearn_learners/online
Message-ID: <200807301754.m6UHsv6V000474@sheep.berlios.de>

Author: tihocan
Date: 2008-07-30 19:54:56 +0200 (Wed, 30 Jul 2008)
New Revision: 9311

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed crash when computing median of empty vector

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-07-30 17:42:08 UTC (rev 9310)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-07-30 17:54:56 UTC (rev 9311)
@@ -1699,8 +1699,12 @@
                     (*bound_cd_nll)(i, t) =
                         visible_layer->getConfigurationCount() *
                         ipow(bound_coeff, t + 1);
-                if (ratio_cd_leftout_is_output)
-                    (*ratio_cd_leftout)(i, t) = median(all_ratios);
+                if (ratio_cd_leftout_is_output) {
+                    if (all_ratios.isEmpty())
+                        (*ratio_cd_leftout)(i, t) = MISSING_VALUE;
+                    else
+                        (*ratio_cd_leftout)(i, t) = median(all_ratios);
+                }
                 if (abs_cd_is_output) {
                     (*abs_cd)(i, t) = mean_abs_updates;
                     (*abs_cd)(i, t + n_steps_compare) = mean_abs_stoch_updates;



From tihocan at mail.berlios.de  Wed Jul 30 19:57:31 2008
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Wed, 30 Jul 2008 19:57:31 +0200
Subject: [Plearn-commits] r9312 - trunk/plearn_learners/online
Message-ID: <200807301757.m6UHvV8e000569@sheep.berlios.de>

Author: tihocan
Date: 2008-07-30 19:57:31 +0200 (Wed, 30 Jul 2008)
New Revision: 9312

Modified:
   trunk/plearn_learners/online/RBMModule.cc
Log:
Fixed workaround to bug in code meant to generate more than 1 sample: for now we are only allowed to sample 1 sample at a time, until this bug is properly fixed

Modified: trunk/plearn_learners/online/RBMModule.cc
===================================================================
--- trunk/plearn_learners/online/RBMModule.cc	2008-07-30 17:54:56 UTC (rev 9311)
+++ trunk/plearn_learners/online/RBMModule.cc	2008-07-30 17:57:31 UTC (rev 9312)
@@ -1273,6 +1273,10 @@
                           n_samples == hidden_sample->length() );
                 n_samples = hidden_sample->length();
             }
+            // The code above to generate more than 1 sample is buggy. Olivier (D)
+            // is supposed to look into it. In the meantime, we just only
+            // generate 1 sample.
+            n_samples = 1;
             PLCHECK( n_samples > 0 );
 
             // the visible_layer->expectations contain the "state" from which we



From nouiz at mail.berlios.de  Wed Jul 30 20:37:47 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 20:37:47 +0200
Subject: [Plearn-commits] r9313 - trunk/plearn_learners/meta
Message-ID: <200807301837.m6UIbljT007250@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 20:37:46 +0200 (Wed, 30 Jul 2008)
New Revision: 9313

Modified:
   trunk/plearn_learners/meta/AdaBoost.cc
Log:
small optimization


Modified: trunk/plearn_learners/meta/AdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/AdaBoost.cc	2008-07-30 17:57:31 UTC (rev 9312)
+++ trunk/plearn_learners/meta/AdaBoost.cc	2008-07-30 18:37:46 UTC (rev 9313)
@@ -229,6 +229,14 @@
 {
     if(conf_rated_adaboost && pseudo_loss_adaboost)
         PLERROR("In Adaboost:build_(): conf_rated_adaboost and pseudo_loss_adaboost cannot both be true, a choice must be made");
+
+    
+    int n;
+    if(weak_learners.size()>0)
+        n=weak_learners[0]->outputsize();
+    else
+        n=weak_learner_template->outputsize();
+    weak_learner_output.resize(n);
 }
 
 // ### Nothing to add here, simply calls build_
@@ -673,20 +681,22 @@
 void AdaBoost::computeOutput(const Vec& input, Vec& output) const
 {
     PLASSERT(weak_learners.size()>0);
-    
+    PLASSERT(weak_learner_output.size()==weak_learners[0]->outputsize());
+    PLASSERT(output.size()==1);
     real sum_out=0;
-    weak_learner_output.resize(weak_learners[0]->outputsize());
-    for (int i=0;i<weak_learners.size();i++)
-    {
-        weak_learners[i]->computeOutput(input,weak_learner_output);
-        if(!pseudo_loss_adaboost && !conf_rated_adaboost)
+    if(!pseudo_loss_adaboost && !conf_rated_adaboost)
+        for (int i=0;i<weak_learners.size();i++){
+            weak_learners[i]->computeOutput(input,weak_learner_output);
             sum_out += (weak_learner_output[0] < output_threshold ? 0 : 1) 
                 *voting_weights[i];
-        else
+        }
+    else
+        for (int i=0;i<weak_learners.size();i++){
+            weak_learners[i]->computeOutput(input,weak_learner_output);
             sum_out += weak_learner_output[0]*voting_weights[i];
-    }
+        }
+
     output[0] = sum_out/sum_voting_weights;
-    output.resize(1);
 }
 
 void AdaBoost::computeCostsFromOutputs(const Vec& input, const Vec& output, 



From nouiz at mail.berlios.de  Wed Jul 30 20:39:27 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 20:39:27 +0200
Subject: [Plearn-commits] r9314 - trunk/plearn_learners/meta
Message-ID: <200807301839.m6UIdRJg007413@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 20:39:26 +0200 (Wed, 30 Jul 2008)
New Revision: 9314

Modified:
   trunk/plearn_learners/meta/MultiClassAdaBoost.cc
   trunk/plearn_learners/meta/MultiClassAdaBoost.h
Log:
optimization by reusing some tmp vector.


Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.cc
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-30 18:37:46 UTC (rev 9313)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.cc	2008-07-30 18:39:26 UTC (rev 9314)
@@ -115,6 +115,10 @@
     sub_target_tmp.resize(2);
     for(int i=0;i<sub_target_tmp.size();i++)
         sub_target_tmp[i].resize(1);
+
+    output1.resize(learner1->outputsize());
+    output2.resize(learner2->outputsize());
+
 }
 
 // ### Nothing to add here, simply calls build_
@@ -222,10 +226,8 @@
 {
     PLASSERT(output.size()==outputsize());
 
-    Vec output1(learner1->outputsize());
-    Vec output2(learner2->outputsize());
-    learner1->computeOutput(input,output1);
-    learner2->computeOutput(input,output2);
+    learner1->computeOutput(input, output1);
+    learner2->computeOutput(input, output2);
     int ind1=int(round(output1[0]));
     int ind2=int(round(output2[0]));
 
@@ -247,13 +249,14 @@
                                                Vec& output, Vec& costs) const
 {
     PLASSERT(costs.size()==nTestCosts());
-
+    PLASSERT_MSG(output.length()!=outputsize(),
+                 "In MultiClassAdaBoost::computeOutputAndCosts -"
+                 " output don't have the good length!");
     output.resize(outputsize());
 
-    Vec output1(learner1->outputsize());
-    Vec output2(learner2->outputsize());
-    Vec subcosts1(learner1->nTestCosts());
-    Vec subcosts2(learner1->nTestCosts());
+    subcosts1.resize(learner1->nTestCosts());
+    subcosts2.resize(learner1->nTestCosts());
+
     getSubLearnerTarget(target, sub_target_tmp);
 
     learner1->computeOutputAndCosts(input, sub_target_tmp[0],
@@ -322,11 +325,11 @@
 
     costs[4]=costs[5]=costs[6]=0;
     costs[out+4]=1;
-    PLASSERT(nTestCosts()==costs.size());
+
     if(forward_sub_learner_test_costs){
         costs.resize(7);
-        Vec subcosts1(learner1->nTestCosts());
-        Vec subcosts2(learner1->nTestCosts());
+        subcosts1.resize(learner1->nTestCosts());
+        subcosts2.resize(learner1->nTestCosts());
         getSubLearnerTarget(target, sub_target_tmp);
 
         learner1->computeCostsOnly(input,sub_target_tmp[0],subcosts1);

Modified: trunk/plearn_learners/meta/MultiClassAdaBoost.h
===================================================================
--- trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-07-30 18:37:46 UTC (rev 9313)
+++ trunk/plearn_learners/meta/MultiClassAdaBoost.h	2008-07-30 18:39:26 UTC (rev 9314)
@@ -59,8 +59,10 @@
 {
     typedef PLearner inherited;
 
-    Vec tmp_costs;
-    Vec tmp_target;
+    mutable Vec output1;
+    mutable Vec output2;
+    mutable Vec subcosts1;
+    mutable Vec subcosts2;
 
 public:
     //#####  Public Build Options  ############################################



From nouiz at mail.berlios.de  Wed Jul 30 21:08:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 21:08:17 +0200
Subject: [Plearn-commits] r9315 - trunk
Message-ID: <200807301908.m6UJ8HRj010674@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 21:08:15 +0200 (Wed, 30 Jul 2008)
New Revision: 9315

Modified:
   trunk/pymake.config.model
Log:
correctly handle the -nolock option without forcing all people to recompile everything


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-07-30 18:39:26 UTC (rev 9314)
+++ trunk/pymake.config.model	2008-07-30 19:08:15 UTC (rev 9315)
@@ -273,7 +273,7 @@
 
   [ 'numpy', 'numarray' ],
 
-#  [ '', 'nolock' ]
+  [ '', 'nolock' ]
 
 ]
 
@@ -723,7 +723,9 @@
 
 cpp_variables += ['BOUNDCHECK', 'NDEBUG', 'Py_DEBUG']
 
-pymakeOption( name = '',description = 'dummy option to hide optional options' )
+pymakeOption( name = '',
+              description = 'dummy option to hide unusual options',
+              in_output_dirname = False)
 pymakeOption( name = 'nolock',
               description = 'USE WITH CARE: disable vmatrix lock',
               cpp_definitions = ['DISABLE_VMATRIX_LOCK'])



From nouiz at mail.berlios.de  Wed Jul 30 21:31:52 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 21:31:52 +0200
Subject: [Plearn-commits] r9316 - trunk
Message-ID: <200807301931.m6UJVqPn012836@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 21:31:51 +0200 (Wed, 30 Jul 2008)
New Revision: 9316

Modified:
   trunk/pymake.config.model
Log:
moved stuff to their own section at the good position in the file


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-07-30 19:08:15 UTC (rev 9315)
+++ trunk/pymake.config.model	2008-07-30 19:31:51 UTC (rev 9316)
@@ -723,14 +723,6 @@
 
 cpp_variables += ['BOUNDCHECK', 'NDEBUG', 'Py_DEBUG']
 
-pymakeOption( name = '',
-              description = 'dummy option to hide unusual options',
-              in_output_dirname = False)
-pymakeOption( name = 'nolock',
-              description = 'USE WITH CARE: disable vmatrix lock',
-              cpp_definitions = ['DISABLE_VMATRIX_LOCK'])
-
-cpp_variables += ['DISABLE_VMATRIX_LOCK']
 #####  Floating-Point Number Representation  ################################
 
 pymakeOption( name = 'float',
@@ -916,6 +908,18 @@
 
 cpp_variables += ['PL_USE_NUMARRAY', 'PL_USE_NUMPY']
 
+
+#####  Hidden option   ###################################################
+pymakeOption( name = '',
+              description = 'dummy option to hide unusual options',
+              in_output_dirname = False)
+pymakeOption( name = 'nolock',
+              description = 'USE WITH CARE: disable vmatrix lock',
+              cpp_definitions = ['DISABLE_VMATRIX_LOCK'])
+
+cpp_variables += ['DISABLE_VMATRIX_LOCK']
+
+
 #####  Network Setup  #######################################################
 
 # nprocesses_on_localhost = 1



From nouiz at mail.berlios.de  Wed Jul 30 21:35:38 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Wed, 30 Jul 2008 21:35:38 +0200
Subject: [Plearn-commits] r9317 - trunk/python_modules/plearn/learners
Message-ID: <200807301935.m6UJZcs7013172@sheep.berlios.de>

Author: nouiz
Date: 2008-07-30 21:35:38 +0200 (Wed, 30 Jul 2008)
New Revision: 9317

Modified:
   trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
Log:
better print


Modified: trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py
===================================================================
--- trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-30 19:31:51 UTC (rev 9316)
+++ trunk/python_modules/plearn/learners/AdaBoostMultiClasses.py	2008-07-30 19:35:38 UTC (rev 9317)
@@ -313,4 +313,4 @@
                 weak.setTrainingSet(trainSet,False)
 
     def print_time(self):
-        print "train time=%.2f test time=%.2f test sub time=%.2f"%(self.train_time,self.test_time,self.test_sub_time)
+        print "AdaBoostMultiClass: train time=%.2f test time=%.2f test sub time=%.2f"%(self.train_time,self.test_time,self.test_sub_time)



From saintmlx at mail.berlios.de  Thu Jul 31 00:14:22 2008
From: saintmlx at mail.berlios.de (saintmlx at BerliOS)
Date: Thu, 31 Jul 2008 00:14:22 +0200
Subject: [Plearn-commits] r9318 - trunk/plearn/base
Message-ID: <200807302214.m6UMEMdB000347@sheep.berlios.de>

Author: saintmlx
Date: 2008-07-31 00:14:22 +0200 (Thu, 31 Jul 2008)
New Revision: 9318

Modified:
   trunk/plearn/base/plerror.cc
   trunk/plearn/base/plerror.h
Log:
- fixed errormsg2 (va args)



Modified: trunk/plearn/base/plerror.cc
===================================================================
--- trunk/plearn/base/plerror.cc	2008-07-30 19:35:38 UTC (rev 9317)
+++ trunk/plearn/base/plerror.cc	2008-07-30 22:14:22 UTC (rev 9318)
@@ -65,13 +65,22 @@
     snprintf(message, ERROR_MSG_SIZE, "In file: \"%s\" at line %d\n", filename, linenumber);
     PLASSERT(ERROR_MSG_SIZE>=strlen(message)+strlen(msg));
     strcat(message,msg);
-    errormsg(message, args);
+    verrormsg(message, args);
 
+    va_end(args);
+
 }
+
 void errormsg(const char* msg, ...)
 {
     va_list args;
     va_start(args,msg);
+    verrormsg(msg, args);
+    va_end(args);
+}
+
+void verrormsg(const char* msg, va_list args)
+{
     char message[ERROR_MSG_SIZE];
 
 #if !defined(ULTRIX) && !defined(_MINGW_) && !defined(WIN32)
@@ -80,8 +89,6 @@
     vsprintf(message,msg,args);
 #endif
 
-    va_end(args);
-
 #ifndef USE_EXCEPTIONS
 #if USING_MPI
     *error_stream <<" ERROR from rank=" << PLMPI::rank << ": " <<message<<endl;

Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2008-07-30 19:35:38 UTC (rev 9317)
+++ trunk/plearn/base/plerror.h	2008-07-30 22:14:22 UTC (rev 9318)
@@ -71,9 +71,13 @@
 void errormsg2(const char* filename, const int linenumber, const char* msg, ...)
     __attribute__((noreturn))
     __attribute__((format(printf, 3, 4)));
+//errormsg: version that takes a variable number of args
 void errormsg(const char* msg, ...)
     __attribute__((noreturn))
     __attribute__((format(printf, 1, 2)));
+//verrormsg: internal errormsg that takes a single va_list
+void verrormsg(const char* msg, va_list args)
+    __attribute__((noreturn));
 void warningmsg(const char* msg, ...)
     __attribute__((format(printf, 1, 2)));
 void deprecationmsg(const char* msg, ...)



From nouiz at mail.berlios.de  Thu Jul 31 16:43:04 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 16:43:04 +0200
Subject: [Plearn-commits] r9319 - trunk
Message-ID: <200807311443.m6VEh4Pm030227@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 16:43:04 +0200 (Thu, 31 Jul 2008)
New Revision: 9319

Modified:
   trunk/pymake.config.model
Log:
on cygwin with gcc 3.3 -Wno-variadic-macros is not defined


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-07-30 22:14:22 UTC (rev 9318)
+++ trunk/pymake.config.model	2008-07-31 14:43:04 UTC (rev 9319)
@@ -531,7 +531,10 @@
 #-pedantic check for conformity with ISO C
 #plerror.h use variadic macro that is introduced in ISO C99
 #So we add -Wno-variadic-macros to allow this exception to ISO C90
-pedantic_mode = ' -pedantic -Wno-variadic-macros '
+if platform != 'cygwin':
+    pedantic_mode = ' -pedantic -Wno-variadic-macros '
+else:
+    pedantic_mode = ' -pedantic '
 if platform=='darwin':
     pedantic_mode = ' '
     



From nouiz at mail.berlios.de  Thu Jul 31 16:44:17 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 16:44:17 +0200
Subject: [Plearn-commits] r9320 - trunk
Message-ID: <200807311444.m6VEiHtR030383@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 16:44:16 +0200 (Thu, 31 Jul 2008)
New Revision: 9320

Modified:
   trunk/pymake.config.model
Log:
added option -Wno-uninitialized to remove useless warning


Modified: trunk/pymake.config.model
===================================================================
--- trunk/pymake.config.model	2008-07-31 14:43:04 UTC (rev 9319)
+++ trunk/pymake.config.model	2008-07-31 14:44:16 UTC (rev 9320)
@@ -273,7 +273,8 @@
 
   [ 'numpy', 'numarray' ],
 
-  [ '', 'nolock' ]
+  [ '', 'nolock'],
+  [ '', 'Wno-uninitialized' ],
 
 ]
 
@@ -535,6 +536,7 @@
     pedantic_mode = ' -pedantic -Wno-variadic-macros '
 else:
     pedantic_mode = ' -pedantic '
+
 if platform=='darwin':
     pedantic_mode = ' '
     
@@ -916,10 +918,18 @@
 pymakeOption( name = '',
               description = 'dummy option to hide unusual options',
               in_output_dirname = False)
+              
+#On cygwin with gcc 3.3 they generate useless such warning
+pymakeOption( name = 'Wno-uninitialized',
+	      description = 'to remove warning about uninitialized variables',
+	      compileroptions = '-Wno-uninitialized',
+              in_output_dirname = False)
+              
 pymakeOption( name = 'nolock',
               description = 'USE WITH CARE: disable vmatrix lock',
               cpp_definitions = ['DISABLE_VMATRIX_LOCK'])
 
+
 cpp_variables += ['DISABLE_VMATRIX_LOCK']
 
 



From nouiz at mail.berlios.de  Thu Jul 31 16:52:50 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 16:52:50 +0200
Subject: [Plearn-commits] r9321 - trunk/plearn/base
Message-ID: <200807311452.m6VEqowq030866@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 16:52:50 +0200 (Thu, 31 Jul 2008)
New Revision: 9321

Modified:
   trunk/plearn/base/plerror.h
Log:
added missing include that made the build fail on cygwin with gcc 3.3


Modified: trunk/plearn/base/plerror.h
===================================================================
--- trunk/plearn/base/plerror.h	2008-07-31 14:44:16 UTC (rev 9320)
+++ trunk/plearn/base/plerror.h	2008-07-31 14:52:50 UTC (rev 9321)
@@ -50,6 +50,7 @@
 
 #include <string>
 #include "plexceptions.h"
+#include <cstdarg>
 
 #ifndef __GNUC__
 // Suppress __attribute__(()) GCC extension on other compilers.



From nouiz at mail.berlios.de  Thu Jul 31 17:05:45 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 17:05:45 +0200
Subject: [Plearn-commits] r9322 - trunk/commands
Message-ID: <200807311505.m6VF5jHj031786@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 17:05:44 +0200 (Thu, 31 Jul 2008)
New Revision: 9322

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
remove some include on windows


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-07-31 14:52:50 UTC (rev 9321)
+++ trunk/commands/plearn_desjardins.cc	2008-07-31 15:05:44 UTC (rev 9322)
@@ -65,7 +65,9 @@
 /************
  * PLearner *
  ************/
+#ifndef WIN32
 #include <plearn_learners/generic/AddCostToLearner.h>
+#endif
 #include <plearn_learners/regressors/RegressionTree.h>
 
 /************



From nouiz at mail.berlios.de  Thu Jul 31 17:11:20 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 17:11:20 +0200
Subject: [Plearn-commits] r9323 - trunk/commands
Message-ID: <200807311511.m6VFBKvM032247@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 17:11:20 +0200 (Thu, 31 Jul 2008)
New Revision: 9323

Modified:
   trunk/commands/plearn_desjardins.cc
Log:
reordered


Modified: trunk/commands/plearn_desjardins.cc
===================================================================
--- trunk/commands/plearn_desjardins.cc	2008-07-31 15:05:44 UTC (rev 9322)
+++ trunk/commands/plearn_desjardins.cc	2008-07-31 15:11:20 UTC (rev 9323)
@@ -69,6 +69,7 @@
 #include <plearn_learners/generic/AddCostToLearner.h>
 #endif
 #include <plearn_learners/regressors/RegressionTree.h>
+#include <plearn_learners/meta/MultiClassAdaBoost.h>
 
 /************
  * Splitter *
@@ -94,8 +95,6 @@
 #include <plearn/vmat/TransposeVMatrix.h>
 #include <plearn/vmat/VariableDeletionVMatrix.h>
 #include <plearn/vmat/MeanMedianModeImputationVMatrix.h>
-
-#include <plearn_learners/meta/MultiClassAdaBoost.h>
 #include <plearn/vmat/MissingIndicatorVMatrix.h>
 
 



From nouiz at mail.berlios.de  Thu Jul 31 18:43:48 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 18:43:48 +0200
Subject: [Plearn-commits] r9324 - trunk/plearn/sys
Message-ID: <200807311643.m6VGhmmg019351@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 18:43:45 +0200 (Thu, 31 Jul 2008)
New Revision: 9324

Modified:
   trunk/plearn/sys/Profiler.cc
   trunk/plearn/sys/Profiler.h
Log:
added fct Profiler::pl_profile_disactivate()


Modified: trunk/plearn/sys/Profiler.cc
===================================================================
--- trunk/plearn/sys/Profiler.cc	2008-07-31 15:11:20 UTC (rev 9323)
+++ trunk/plearn/sys/Profiler.cc	2008-07-31 16:43:45 UTC (rev 9324)
@@ -127,6 +127,9 @@
 // call Profiler::activate if PL_PROFILE is set
 void Profiler::pl_profile_activate(){
     Profiler::activate();}
+// call Profiler::disactivate if PL_PROFILE is set
+void Profiler::pl_profile_disactivate(){
+    Profiler::disactivate();}
 // call Profiler::report if PL_PROFILE is set
 void Profiler::pl_profile_report(ostream& out){
     Profiler::report(out);}

Modified: trunk/plearn/sys/Profiler.h
===================================================================
--- trunk/plearn/sys/Profiler.h	2008-07-31 15:11:20 UTC (rev 9323)
+++ trunk/plearn/sys/Profiler.h	2008-07-31 16:43:45 UTC (rev 9324)
@@ -175,6 +175,13 @@
     static inline void pl_profile_activate() {}
 #endif
 
+    //!  call disactivate() if if PL_PROFILE is set
+#if defined(PROFILE) && defined(PL_PROFILE)
+    static void pl_profile_disactivate();
+#else
+    static inline void pl_profile_disactivate() {}
+#endif
+
     //!  call report() if if PL_PROFILE is set
 #if defined(PROFILE) && defined(PL_PROFILE)
     static void pl_profile_report(ostream& out);



From nouiz at mail.berlios.de  Thu Jul 31 18:48:14 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 18:48:14 +0200
Subject: [Plearn-commits] r9325 - trunk/commands/PLearnCommands
Message-ID: <200807311648.m6VGmE1k024981@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 18:48:12 +0200 (Thu, 31 Jul 2008)
New Revision: 9325

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
Added a new --profile option that print some profiling information


Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2008-07-31 16:43:45 UTC (rev 9324)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2008-07-31 16:48:12 UTC (rev 9325)
@@ -66,6 +66,8 @@
         ""
         "Global parameter:\n"
         "                 --no-version: do not print the version \n"
+        "                 --profile: print some profiling information\n"
+        "                     Must have been compiled with '-logging=dbg-profile'\n"
         "                 --verbosity LEVEL: The level of log to print.\n"
         "                     Must have been compiled with the same level of more\n"
         "                     Available level:\n"

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 16:43:45 UTC (rev 9324)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 16:48:12 UTC (rev 9325)
@@ -156,6 +156,11 @@
 
     // Note that the findpos function (stringutils.h) returns -1 if the
     // option is not found.
+    int profile_pos       = findpos( command_line, "--profile" );
+    if(profile_pos != -1)
+        Profiler::activate();
+    // Note that the findpos function (stringutils.h) returns -1 if the
+    // option is not found.
     int no_version_pos       = findpos( command_line, "--no-version" );
 
     // If we don't want no progress bars
@@ -243,6 +248,7 @@
     for ( int c=0; c < argc; c++ )
         // Neglecting to copy options
         if ( c != no_version_pos             &&
+             c != profile_pos                &&
              c != no_progress_bars           &&
              c != verbosity_pos              &&
              c != verbosity_value_pos        &&
@@ -295,11 +301,6 @@
 int plearn_main( int argc, char** argv,
                  int major_version, int minor_version, int fixlevel )
 {
-#ifdef PL_PROFILE    
-    Profiler::activate();
-    Profiler::start("Prog");
-#endif
-
     setVersion(major_version, minor_version, fixlevel);
 
     // Establish the terminate handler that's called in situations of
@@ -320,6 +321,7 @@
 
         vector<string> command_line = stringvector(argc-1, argv+1);
         string command = global_options(command_line);
+        Profiler::pl_profile_start("Prog");
 
         if ( command == "" )
         {
@@ -350,12 +352,12 @@
         EXIT_CODE = 2;
     }
 
-#ifdef PL_PROFILE
-    Profiler::end("Prog");
-    Profiler::disactivate();
-    Profiler::report(cerr);
-    Profiler::reportwall(cerr);
-#endif
+    Profiler::pl_profile_end("Prog");
+    if(Profiler::isActive()){
+        Profiler::pl_profile_disactivate();
+        Profiler::pl_profile_report(cerr);
+        Profiler::pl_profile_reportwall(cerr);
+    }
     return EXIT_CODE;
 }
 



From nouiz at mail.berlios.de  Thu Jul 31 18:57:30 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 18:57:30 +0200
Subject: [Plearn-commits] r9326 - trunk/commands/PLearnCommands
Message-ID: <200807311657.m6VGvUcr003303@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 18:57:29 +0200 (Thu, 31 Jul 2008)
New Revision: 9326

Modified:
   trunk/commands/PLearnCommands/HelpCommand.cc
Log:
Added help for global options that didn't had one.


Modified: trunk/commands/PLearnCommands/HelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HelpCommand.cc	2008-07-31 16:48:12 UTC (rev 9325)
+++ trunk/commands/PLearnCommands/HelpCommand.cc	2008-07-31 16:57:29 UTC (rev 9326)
@@ -65,6 +65,7 @@
         "To get help on datasets:                            " + prgname() + " help datasets \n\n" 
         ""
         "Global parameter:\n"
+        "                 --no-progress: don't print progress bar\n"
         "                 --no-version: do not print the version \n"
         "                 --profile: print some profiling information\n"
         "                     Must have been compiled with '-logging=dbg-profile'\n"
@@ -76,6 +77,13 @@
         "                             VLEVEL_NORMAL  // Normal (Default)\n"
         "                             VLEVEL_DBG     // Debug Info\n"
         "                             VLEVEL_EXTREME // Extreme Verbosity\n"
+        "                 --enable-logging module1,module2,module3,...:\n"
+        "                     Option to enable logging for the specified modules,\n"
+        "                     specified a comma-separated list of modules (without spaces).\n"
+        "                     Special keywords __ALL__ and __NONE__ can be specified to log\n"
+        "                     for all modules or no modules respectively."
+        "                 --servers\n"
+        "                 --global-calendars\n"
          << endl;
 }
 



From chrish at mail.berlios.de  Thu Jul 31 19:52:37 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 31 Jul 2008 19:52:37 +0200
Subject: [Plearn-commits] r9327 - trunk/commands/PLearnCommands
Message-ID: <200807311752.m6VHqb2j000279@sheep.berlios.de>

Author: chrish
Date: 2008-07-31 19:52:37 +0200 (Thu, 31 Jul 2008)
New Revision: 9327

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
Fix profiler-related breakage.

Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 16:57:29 UTC (rev 9326)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 17:52:37 UTC (rev 9327)
@@ -50,6 +50,7 @@
 #include <plearn/misc/Calendar.h>
 #include <plearn/misc/PLearnService.h>
 #include <plearn/vmat/VMat.h>
+#include <plearn/sys/Profiler.h>
 
 // From C++ stdlib
 #include <exception>
@@ -58,10 +59,6 @@
 #include <plearn/sys/PLMPI.h>
 #endif
 
-//#define PL_PROFILE
-#ifdef PL_PROFILE
-#include <plearn/sys/Profiler.h>
-#endif
 
 namespace PLearn {
 using namespace std;



From nouiz at mail.berlios.de  Thu Jul 31 20:48:32 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 20:48:32 +0200
Subject: [Plearn-commits] r9328 - in trunk/plearn/base/test/.pytest:
	PL_assert PL_assert/expected_results PL_check/expected_results
Message-ID: <200807311848.m6VImW6a005276@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 20:48:31 +0200 (Thu, 31 Jul 2008)
New Revision: 9328

Modified:
   trunk/plearn/base/test/.pytest/PL_assert/
   trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
   trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
Log:
corrected test following the change in plerror.h



Property changes on: trunk/plearn/base/test/.pytest/PL_assert
___________________________________________________________________
Name: svn:ignore
   - .plearn
run_results
assertions__compiler_pymake__options_None.log

   + .plearn
run_results
assertions__compiler_pymake__options_None.log
PSAVEDIFF


Modified: trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-07-31 17:52:37 UTC (rev 9327)
+++ trunk/plearn/base/test/.pytest/PL_assert/expected_results/RUN.log	2008-07-31 18:48:31 UTC (rev 9328)
@@ -1,4 +1,5 @@
-FATAL ERROR: Assertion failed: 3+8 == 123+46
+FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/base/plerror.cc" at line 173
+Assertion failed: 3+8 == 123+46
 Function: int main()
     File: assertions.cc
     Line: 18

Modified: trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log
===================================================================
--- trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-07-31 17:52:37 UTC (rev 9327)
+++ trunk/plearn/base/test/.pytest/PL_check/expected_results/RUN.log	2008-07-31 18:48:31 UTC (rev 9328)
@@ -1,4 +1,5 @@
-FATAL ERROR: Check failed: ein == stein
+FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/base/plerror.cc" at line 187
+Check failed: ein == stein
 Function: void PLearn::PLCheckTest::perform()
     File: PLCheckTest.cc
     Line: 116



From nouiz at mail.berlios.de  Thu Jul 31 20:55:08 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 20:55:08 +0200
Subject: [Plearn-commits] r9329 -
	trunk/plearn/io/test/.pytest/test_ppath/expected_results
Message-ID: <200807311855.m6VIt8Ps006319@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 20:55:08 +0200 (Thu, 31 Jul 2008)
New Revision: 9329

Modified:
   trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log
Log:
corrected test following change in plerror.h


Modified: trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log
===================================================================
--- trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log	2008-07-31 18:48:31 UTC (rev 9328)
+++ trunk/plearn/io/test/.pytest/test_ppath/expected_results/RUN.log	2008-07-31 18:55:08 UTC (rev 9329)
@@ -32,9 +32,11 @@
 
 #####  Methods up and dirname  ############################################
 
-PPath('PL_ROOT:').up()                                      FATAL ERROR: In PPath::up - Cannot go up on directory 'PL_ROOT:'
+PPath('PL_ROOT:').up()                                      FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/io/PPath.cc" at line 931
+In PPath::up - Cannot go up on directory 'PL_ROOT:'
 
-PPath('').up()                                              FATAL ERROR: In PPath::up - Cannot go up on directory ''
+PPath('').up()                                              FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/io/PPath.cc" at line 931
+In PPath::up - Cannot go up on directory ''
 
 PPath('PL_ROOT:foo').up() == 'PL_ROOT:'                     True
 
@@ -76,7 +78,8 @@
 
 PPath('/foo/bar').addProtocol()                             file:/foo/bar
 
-PPath('foo/bar').addProtocol()                              FATAL ERROR: In PPath::addProtocol - A protocol can only be added to an absolute path, and 'foo/bar' is relative
+PPath('foo/bar').addProtocol()                              FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/io/PPath.cc" at line 851
+In PPath::addProtocol - A protocol can only be added to an absolute path, and 'foo/bar' is relative
 
 PPath('file:/foo/bar').removeProtocol()                     PL_ROOT:foo/bar
 
@@ -130,9 +133,11 @@
 
 foo./bar                                                    foo./bar
 
-PL_ROOT:..                                                  FATAL ERROR: In PPath::resolveDots - 'PL_ROOT:..' is invalid
+PL_ROOT:..                                                  FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/io/PPath.cc" at line 609
+In PPath::resolveDots - 'PL_ROOT:..' is invalid
 
-PL_ROOT:../foo                                              FATAL ERROR: In PPath::resolveDots - 'PL_ROOT:../foo' is invalid
+PL_ROOT:../foo                                              FATAL ERROR: In file: "/home/fringant2/lisa/bastienf/PLearn/plearn/io/PPath.cc" at line 609
+In PPath::resolveDots - 'PL_ROOT:../foo' is invalid
 
 ../foo                                                      ../foo
 



From chrish at mail.berlios.de  Thu Jul 31 21:46:02 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 31 Jul 2008 21:46:02 +0200
Subject: [Plearn-commits] r9330 - trunk/plearn/db
Message-ID: <200807311946.m6VJk2FL009977@sheep.berlios.de>

Author: chrish
Date: 2008-07-31 21:46:01 +0200 (Thu, 31 Jul 2008)
New Revision: 9330

Modified:
   trunk/plearn/db/getDataSet.cc
Log:
De-abspath-ize getDataSet.cc.

Modified: trunk/plearn/db/getDataSet.cc
===================================================================
--- trunk/plearn/db/getDataSet.cc	2008-07-31 18:55:08 UTC (rev 9329)
+++ trunk/plearn/db/getDataSet.cc	2008-07-31 19:46:01 UTC (rev 9330)
@@ -79,7 +79,7 @@
         dataset_abs = dataset_path;
     else
         // There may be parameters that need parsing.
-        parseBaseAndParameters(dataset_path.absolute(), dataset_abs, params);
+        parseBaseAndParameters(dataset_path, dataset_abs, params);
     PPath dataset(dataset_abs);
     bool use_params = false;
 
@@ -114,13 +114,13 @@
                 // Old XML-like format.
                 PLDEPRECATED("In getDataSet - File %s is using the old XML-like VMat format, " 
                              "you should switch to a PLearn script (ideally a .pymat file).",
-                             dataset.absolute().c_str());
+                             dataset.c_str());
                 vm = new VVMatrix(dataset);
             } else {
                 vm = dynamic_cast<VMatrix*>(newObject(code));
                 if (vm.isNull())
                     PLERROR("In getDataSet - Object described in %s is not a VMatrix subclass",
-                            dataset.absolute().c_str());
+                            dataset.c_str());
             }
             vm->updateMtime(date);
         } else if (ext == "pymat" || ext == "py") {
@@ -137,7 +137,7 @@
             vm = dynamic_cast<VMatrix*>(newObject(code));
             if (vm.isNull())
                 PLERROR("In getDataSet - Object described in %s is not a VMatrix subclass",
-                        dataset.absolute().c_str());
+                        dataset.c_str());
             //Their is two case:
             //1) params.size()>0, The mtime should be now
             //2) params.size()==0 the mtime should be the file mtime
@@ -168,7 +168,7 @@
             PLERROR("In getDataSet - Unknown extension for VMat file: %s", ext.c_str());
         if (!use_params && !params.empty())
             PLWARNING("In getDataSet - Ignoring parameters when reading file %s",
-                      dataset.absolute().c_str());
+                      dataset.c_str());
         // Set default metadata directory if not already set.
         if (!vm->hasMetaDataDir())
             vm->setMetaDataDir(dataset.dirname() / (dataset.basename() + ".metadata"));



From nouiz at mail.berlios.de  Thu Jul 31 22:08:26 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 22:08:26 +0200
Subject: [Plearn-commits] r9331 - trunk/commands/PLearnCommands
Message-ID: <200807312008.m6VK8Qh4011477@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 22:08:26 +0200 (Thu, 31 Jul 2008)
New Revision: 9331

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
hopefully correctly bugfixed the new profile option in all case. Pytest had test failing when PYMAKE_OPTIONS=-logging=dbg-profile.


Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 19:46:01 UTC (rev 9330)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 20:08:26 UTC (rev 9331)
@@ -50,8 +50,9 @@
 #include <plearn/misc/Calendar.h>
 #include <plearn/misc/PLearnService.h>
 #include <plearn/vmat/VMat.h>
+#ifdef PL_PROFILE
 #include <plearn/sys/Profiler.h>
-
+#endif
 // From C++ stdlib
 #include <exception>
 
@@ -155,7 +156,7 @@
     // option is not found.
     int profile_pos       = findpos( command_line, "--profile" );
     if(profile_pos != -1)
-        Profiler::activate();
+        Profiler::pl_profile_activate();
     // Note that the findpos function (stringutils.h) returns -1 if the
     // option is not found.
     int no_version_pos       = findpos( command_line, "--no-version" );
@@ -303,7 +304,8 @@
     // Establish the terminate handler that's called in situations of
     // double-fault.
     set_terminate(plearn_terminate_handler);
-    
+
+    vector<string> command_line;
     int EXIT_CODE = 0;
     try {
 
@@ -316,9 +318,8 @@
         // set program name
         prgname(argv[0]);
 
-        vector<string> command_line = stringvector(argc-1, argv+1);
+        command_line = stringvector(argc-1, argv+1);
         string command = global_options(command_line);
-        Profiler::pl_profile_start("Prog");
 
         if ( command == "" )
         {
@@ -326,6 +327,8 @@
             return 0;
         }
 
+        Profiler::pl_profile_start("Prog");
+
         PLearnCommandRegistry::run(command, command_line);
 #if USING_MPI
         PLMPI::finalize();
@@ -349,8 +352,8 @@
         EXIT_CODE = 2;
     }
 
-    Profiler::pl_profile_end("Prog");
-    if(Profiler::isActive()){
+    if(findpos( command_line, "--profile" )!=-1){
+        Profiler::pl_profile_end("Prog");
         Profiler::pl_profile_disactivate();
         Profiler::pl_profile_report(cerr);
         Profiler::pl_profile_reportwall(cerr);



From nouiz at mail.berlios.de  Thu Jul 31 22:18:07 2008
From: nouiz at mail.berlios.de (nouiz at BerliOS)
Date: Thu, 31 Jul 2008 22:18:07 +0200
Subject: [Plearn-commits] r9332 - trunk/commands/PLearnCommands
Message-ID: <200807312018.m6VKI7BD012087@sheep.berlios.de>

Author: nouiz
Date: 2008-07-31 22:18:07 +0200 (Thu, 31 Jul 2008)
New Revision: 9332

Modified:
   trunk/commands/PLearnCommands/plearn_main.cc
Log:
..


Modified: trunk/commands/PLearnCommands/plearn_main.cc
===================================================================
--- trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 20:08:26 UTC (rev 9331)
+++ trunk/commands/PLearnCommands/plearn_main.cc	2008-07-31 20:18:07 UTC (rev 9332)
@@ -50,9 +50,8 @@
 #include <plearn/misc/Calendar.h>
 #include <plearn/misc/PLearnService.h>
 #include <plearn/vmat/VMat.h>
-#ifdef PL_PROFILE
 #include <plearn/sys/Profiler.h>
-#endif
+
 // From C++ stdlib
 #include <exception>
 



From chrish at mail.berlios.de  Thu Jul 31 22:52:49 2008
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 31 Jul 2008 22:52:49 +0200
Subject: [Plearn-commits] r9333 - trunk/scripts
Message-ID: <200807312052.m6VKqnLp014740@sheep.berlios.de>

Author: chrish
Date: 2008-07-31 22:52:49 +0200 (Thu, 31 Jul 2008)
New Revision: 9333

Modified:
   trunk/scripts/pyplearn_driver.py
Log:
Cosmetic cleanups to pyplearn_driver.py

Modified: trunk/scripts/pyplearn_driver.py
===================================================================
--- trunk/scripts/pyplearn_driver.py	2008-07-31 20:18:07 UTC (rev 9332)
+++ trunk/scripts/pyplearn_driver.py	2008-07-31 20:52:49 UTC (rev 9333)
@@ -1,10 +1,9 @@
 #!/usr/bin/env python
-__cvs_id__ = "$Id$"
 
-import sys
-import os
-from plearn.pyplearn             import *
+import sys, os
+from plearn.pyplearn import *
 from plearn.pyplearn.plearn_repr import *
+from plearn.utilities import options_dialog
 
 # Add the absolute directory portion of the current script to the path
 sys.path = [os.path.dirname(os.path.abspath(sys.argv[1]))] + sys.path
@@ -13,18 +12,15 @@
 lines = pyplearn_file.read()
 pyplearn_file.close()
 
-from plearn.utilities import options_dialog
-orig_verb, orig_logs, gui_namespaces, use_gui= options_dialog.getGuiInfo(sys.argv)
+orig_verb, orig_logs, gui_namespaces, use_gui = options_dialog.getGuiInfo(sys.argv)
 
-gui_code= """
+if use_gui:
+    lines += """
 from plearn.utilities import options_dialog
 runit, verb, logs= options_dialog.optionsDialog(%s,plargs.expdir,%d,%s,%s)
-"""%(repr(sys.argv[1]),orig_verb,repr(orig_logs),repr(gui_namespaces))
-
-if use_gui:
-    lines+= gui_code
+""" % (repr(sys.argv[1]), orig_verb, repr(orig_logs), repr(gui_namespaces))
 else:
-    lines+= "\nrunit, verb, logs= True, %d, %s\n"%(orig_verb,repr(orig_logs))
+    lines += "\nrunit, verb, logs= True, %d, %s\n" % (orig_verb, repr(orig_logs))
 
 if len(sys.argv) == 3 and sys.argv[2] == '--help':
     # Simply print the docstring of the pyplearn script



