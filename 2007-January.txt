From laulysta at mail.berlios.de  Wed Jan  3 03:44:09 2007
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Wed, 3 Jan 2007 03:44:09 +0100
Subject: [Plearn-commits] r6551 - trunk/plearn_learners_experimental
Message-ID: <200701030244.l032i9kL002815@sheep.berlios.de>

Author: laulysta
Date: 2007-01-03 03:44:08 +0100 (Wed, 03 Jan 2007)
New Revision: 6551

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
Log:
projet


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2006-12-29 20:07:39 UTC (rev 6550)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2007-01-03 02:44:08 UTC (rev 6551)
@@ -163,6 +163,7 @@
         pos_down_values.resize(visible_size);
         pos_up_values.resize(hidden_layer->size);
         hidden_layer_target.resize(hidden_layer->size);
+        hidden_layer_sample.resize(hidden_layer->size);
 
         visible_layer->size = visible_size;
 
@@ -209,7 +210,7 @@
 
 int DynamicallyLinkedRBMsModel::outputsize() const
 {
-    int out_size = 0;
+    int out_size = 1; // Not really...
     return out_size;
 }
 
@@ -316,7 +317,12 @@
             for(int sample=0 ; sample<train_set->length() ; sample++ )
             {
                 train_set->getExample(sample, input, target, weight);
-            
+
+                //cout << "lalalalala" << input << endl;
+                //exit(0);
+                if(train_set->getString(sample,0) == "<oov>")
+                    continue;
+
                 clamp_visible_units(input);
                 
                 mean_cost += rbm_update();
@@ -357,7 +363,7 @@
 
         previous_hidden_layer.resize(hidden_layer->size);
         dynamic_connections->setLearningRate( dynamic_learning_rate );
-
+        real mean_cost = 0;
         while(stage < end_stage)
         {
             for(int sample=0 ; sample<train_set->length() ; sample++ )
@@ -373,14 +379,25 @@
                     previous_hidden_layer.clear();
 
                 train_set->getExample(sample, input, target, weight);
+
+                if(train_set->getString(sample,0) == "<oov>")
+                {
+                    hidden_layer_sample.clear();
+                    continue;
+                }
             
                 clamp_visible_units(input);
                 
-                dynamic_connections_update();
+                mean_cost += dynamic_connections_update();
                                 
             }
             if( pb )
                 pb->update( stage + 1 - init_stage);
+
+            if(verbosity>0)
+                cout << "mean cost at stage " << stage << 
+                    " = " << mean_cost/train_set->length() << endl;
+            mean_cost = 0;
             stage++;
         }    
         if( pb )
@@ -400,6 +417,7 @@
         MODULE_LOG << "Training the whole model" << endl;
 
         int init_stage = stage;
+        //int end_stage = max(0,nstages-(rbm_nstages + dynamic_nstages));
         int end_stage = nstages;
 
         MODULE_LOG << "  stage = " << stage << endl;
@@ -415,6 +433,7 @@
         visible_layer->setLearningRate( fine_tuning_learning_rate );
         connections->setLearningRate( fine_tuning_learning_rate );
 
+        real mean_cost = 0;
         while(stage < end_stage)
         {
             for(int sample=0 ; sample<train_set->length() ; sample++ )
@@ -430,14 +449,25 @@
                     previous_hidden_layer.clear();
 
                 train_set->getExample(sample, input, target, weight);
-            
+
+                if(train_set->getString(sample,0) == "<oov>")
+                {
+                    hidden_layer_sample.clear();
+                    continue;
+                }
+                
                 clamp_visible_units(input);
                 
-                fine_tuning_update();                                
+                mean_cost += fine_tuning_update();                                
             }
 
             if( pb )
                 pb->update( stage + 1 - init_stage);
+
+            if(verbosity>0)
+                cout << "mean cost at stage " << stage << 
+                    " = " << mean_cost/train_set->length() << endl;
+            mean_cost = 0;
             stage++;
         }    
         if( pb )
@@ -452,7 +482,7 @@
 }
 
 
-void DynamicallyLinkedRBMsModel::clamp_visible_units(const Vec& input)
+void DynamicallyLinkedRBMsModel::clamp_visible_units(const Vec& input) const
 {
     int it = 0;
     int ss = -1;
@@ -460,8 +490,11 @@
     {
         ss = symbol_sizes[i];
         // If input is a real ...
-        if(ss < 0)            
+        if(ss < 0) 
+        {
+            //cout << "yoyoyoyoyo" << endl;
             visible_layer->expectation[it++] = input[i];
+        }
         else // ... or a symbol
         {
             // Convert to one-hot vector
@@ -513,13 +546,15 @@
 
     hidden_layer->computeExpectation();
 
+    hidden_layer->generateSample();
+
     //############ CD update #########################
 
     visible_layer->update( 
         pos_down_values, visible_layer->sample ); // ... of visible_layer bias ...
 
     hidden_layer->update( 
-        pos_up_values, hidden_layer->sample );// ... of hidden_layer bias ...
+        pos_up_values, hidden_layer->expectation );// ... of hidden_layer bias ...
     
     connections->update( 
         pos_down_values, pos_up_values, visible_layer->sample
@@ -537,7 +572,7 @@
    
 }
 
-void DynamicallyLinkedRBMsModel::dynamic_connections_update()
+real DynamicallyLinkedRBMsModel::dynamic_connections_update()
 {
     // Obtain target hidden_layer h_t
     connections->setAsDownInput(visible_layer->expectation);
@@ -562,13 +597,128 @@
     dynamic_connections->bpropUpdate(previous_hidden_layer,
                                      hidden_layer->activation,
                                      input_gradient, bias_gradient);
+
+    return nll;
 }
 
-void DynamicallyLinkedRBMsModel::fine_tuning_update()
+real DynamicallyLinkedRBMsModel::fine_tuning_update()
 {
+
+ // Obtain target hidden_layer h_t
+//    connections->setAsDownInput(visible_layer->expectation);
+//    hidden_layer->getAllActivations(connections);
+//    hidden_layer->computeExpectation();
+//    hidden_layer_target << hidden_layer->expectation;
+//    hidden_layer->generateSample();
+//    hidden_layer_sample << hidden_layer->sample;
+    
     // Use "previous_hidden_layer" field and "dynamic_connections" module 
     // to set bias of "hidden_layer"
 
+
+    // cond_bias.clear();
+
+     
+
+    cond_bias.resize(hidden_layer->size);
+   
+    dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+
+//     MODULE_LOG << cond_bias->data[0] << endl;
+//    cout << (&(real *)(cond_bias->data))[0] << endl;
+   
+  
+
+    hidden_layer->getAllBias(cond_bias);
+    // hidden_layer->expectation_is_up_to_date = false;
+    //hidden_layer->computeExpectation();
+
+
+    // cout <<  hidden_layer->bias << endl;
+
+
+
+ //###### Positive phase #####################
+
+    //up phase
+    connections->setAsDownInput( visible_layer->expectation );
+    hidden_layer->getAllActivations( connections );
+    hidden_layer->computeExpectation();
+
+
+    //save the stats for the positive phase
+    pos_down_values << visible_layer->expectation;
+    pos_up_values << hidden_layer->expectation;
+
+
+
+    //down phase
+    hidden_layer->generateSample();
+    hidden_layer_sample << hidden_layer->sample;
+    connections->setAsUpInput( hidden_layer->sample );
+
+    visible_layer->getAllActivations( connections );
+
+    visible_layer->computeExpectation();
+    
+    visible_layer->generateSample();
+
+
+
+
+    //############ Negative phase  ##################
+
+    connections->setAsDownInput( visible_layer->sample );
+    
+    hidden_layer->getAllActivations( connections );
+
+    hidden_layer->computeExpectation();
+
+    hidden_layer->generateSample();
+
+    
+
+
+    //cout <<  hidden_layer->bias << endl;
+     //############ CD update #########################
+
+    visible_layer->update( 
+        pos_down_values, visible_layer->sample ); // ... of visible_layer bias ...
+
+    
+
+
+// cout << pos_up_values << endl;
+// cout << "blabla" << endl;
+// cout << hidden_layer->sample << endl;
+   
+    hidden_layer->bpropCD(pos_up_values, hidden_layer->expectation, bias_gradient);
+
+
+    dynamic_connections->bpropUpdate(previous_hidden_layer,
+                                     cond_bias,
+                                     input_gradient, bias_gradient);
+
+   
+    // hidden_layer->update( 
+    //    pos_up_values, hidden_layer->sample );// ... of hidden_layer bias ...
+    
+    connections->update( 
+        pos_down_values, pos_up_values, visible_layer->sample
+        , hidden_layer->sample ); // ... of connections between layers.
+
+    //cout << ((PLearn::PP<PLearn::GradNNetLayerModule>)dynamic_connections)->weights << endl;
+
+
+
+
+
+
+
+
+    // Use "previous_hidden_layer" field and "dynamic_connections" module 
+    // to set bias of "hidden_layer"
+
     // Positive phase
 
     // Negative phase
@@ -588,6 +738,31 @@
     //      using usual CD update. Conditional bias was updated
     //      before at (*)
 
+
+
+
+
+    // Compute reconstruction error
+
+    //cond_bias.clear();
+    //dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+    //hidden_layer->getAllBias(cond_bias);
+
+    
+
+
+
+    connections->setAsUpInput( pos_up_values );
+
+    visible_layer->getAllActivations( connections );
+
+    visible_layer->computeExpectation();
+    
+    return visible_layer->fpropNLL(pos_down_values);
+
+    
+
+
 }
 
 void DynamicallyLinkedRBMsModel::computeOutput(const Vec& input, Vec& output) const
@@ -606,9 +781,9 @@
 }
 
 void DynamicallyLinkedRBMsModel::test(VMat testset, PP<VecStatsCollector> test_stats,
-                  VMat testoutputs, VMat testcosts) const
+                  VMat testoutputs, VMat testcosts)const
 {
-    PLERROR("DynamicallyLinkedRBMsModel::test(): TODO!");
+   
 
     int len = testset.length();
     Vec input;
@@ -628,17 +803,99 @@
         test_stats->update(costs);
     }
 
-    output.resize(1);
-    costs.resize(getTestCostNames().length());
 
-
+    int begin = 0;
+    int nb_oov = 0;
     for (int i = 0; i < len; i++)
     {
         testset.getExample(i, input, target, weight);
       
-        // fill output (not necessary for now) and costs
-        // Depends on learning phase
-      
+       
+
+
+        if(testset->getString(i,0) == "<oov>")
+        {
+            begin = 0;
+            nb_oov++;
+            continue;
+        }
+        
+
+
+
+
+        clamp_visible_units(input);
+
+
+
+
+        if(begin > 0)
+        {
+
+            //h*_{t-1}
+            //////////////////////////////////
+            dynamic_connections->fprop(previous_hidden_layer, cond_bias);
+           
+            hidden_layer->getAllBias(cond_bias); 
+
+
+            //up phase
+            connections->setAsDownInput( previous_input );
+            hidden_layer->getAllActivations( connections );
+            hidden_layer->computeExpectation();
+            //////////////////////////////////
+
+            previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour
+
+            //h*_{t}
+            ////////////
+            dynamic_connections->fprop( hidden_layer->expectation ,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            hidden_layer->expectation_is_up_to_date = false;
+            hidden_layer->computeExpectation();//h_{t}
+            ///////////
+
+            previous_input << visible_layer->expectation;//v_{t-1}
+            
+        }
+        else
+        {
+            previous_hidden_layer.clear();//h_{t-1}
+            dynamic_connections->fprop(previous_hidden_layer,hidden_layer->activation);//conection entre h_{t-1} et h_{t}
+            hidden_layer->expectation_is_up_to_date = false;
+            hidden_layer->computeExpectation();//h_{t}
+
+            previous_input.resize(visible_layer->size);
+            previous_input << visible_layer->expectation;
+            begin++;
+        }
+
+
+       
+
+
+
+
+
+
+     
+
+
+
+        connections->setAsUpInput( hidden_layer->expectation );
+
+        visible_layer->getAllActivations( connections );
+
+        visible_layer->computeExpectation();
+
+       
+
+        costs[0] = visible_layer->fpropNLL(previous_input) / inputsize() ;
+
+
+
+
+        // costs[0] = 0; //nll/nb_de_temps_par_mesure
+
         if (testoutputs)
             testoutputs->putOrAppendRow(i, output);
 
@@ -651,6 +908,11 @@
         if (report_progress)
             pb->update(i);
     }
+
+    //costs[0] = costs[0]/(len - nb_oov) ;
+
+    //cout << "Probabilite moyenne : " << costs[0] << endl;
+
 }
 
 

Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2006-12-29 20:07:39 UTC (rev 6550)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.h	2007-01-03 02:44:08 UTC (rev 6551)
@@ -46,6 +46,8 @@
 #include <plearn_learners/online/RBMLayer.h>
 #include <plearn_learners/online/RBMConnection.h>
 
+#include <plearn_learners/online/GradNNetLayerModule.h>
+
 namespace PLearn {
 
 /**
@@ -139,7 +141,7 @@
     virtual TVec<std::string> getTrainCostNames() const;
 
     //! Clamps the visible units based on an input vector
-    void clamp_visible_units(const Vec& input);
+    void clamp_visible_units(const Vec& input) const;
 
     //! Updates the RBM parameters in the rbm training phase,
     //! after the visible units have been clamped.
@@ -149,12 +151,15 @@
 
     //! Updates the dynamic connections in the dynamic training
     //! phase, after the visible units have been clamped
-    void dynamic_connections_update();
+    //! Outputs the negative log-likelihood of the hidden representation
+    //! of training example t given a sample from the hidden representation
+    //! of training example t-1.
+    real dynamic_connections_update();
 
     //! Updates both the RBM parameters and the 
     //! dynamic connections in the fine tuning phase,
     //! after the visible units have been clamped
-    void fine_tuning_update();
+    real fine_tuning_update();
 
     virtual void test(VMat testset, PP<VecStatsCollector> test_stats,
                       VMat testoutputs=0, VMat testcosts=0) const;
@@ -199,8 +204,11 @@
 
     //! Stores input gradient of dynamic connections
     mutable Vec input_gradient;
-
+    
     //! Stores previous hidden layer value
+    mutable Vec previous_input;
+    
+    //! Stores previous hidden layer value
     mutable Vec previous_hidden_layer;
 
     //! Stores a sample from the hidden layer



From chapados at mail.berlios.de  Wed Jan  3 22:25:32 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Wed, 3 Jan 2007 22:25:32 +0100
Subject: [Plearn-commits] r6552 - trunk/plearn/python
Message-ID: <200701032125.l03LPWD6017749@sheep.berlios.de>

Author: chapados
Date: 2007-01-03 22:25:31 +0100 (Wed, 03 Jan 2007)
New Revision: 6552

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Added conversion from Python to VMat

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-01-03 02:44:08 UTC (rev 6551)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-01-03 21:25:31 UTC (rev 6552)
@@ -165,7 +165,14 @@
     return m;
 }
 
+VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj)
+{
+    Mat m;
+    ConvertFromPyObject<Mat>::convert(pyobj, m);
+    return m;
+}
 
+
 //#####  Constructors+Destructors  ############################################
 
 // Copy constructor: increment refcount if controlling ownership.

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-01-03 02:44:08 UTC (rev 6551)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-01-03 21:25:31 UTC (rev 6552)
@@ -184,6 +184,13 @@
     static void convert(PyObject* pyobj, Mat& result);
 };
 
+template <>
+struct ConvertFromPyObject<VMat>
+{
+    // Return new MemoryVMatrix
+    static VMat convert(PyObject*);
+};
+
 template <class T>
 struct ConvertFromPyObject< TVec<T> >
 {



From chapados at mail.berlios.de  Thu Jan  4 01:22:09 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 4 Jan 2007 01:22:09 +0100
Subject: [Plearn-commits] r6553 - trunk/plearn_learners/regressors
Message-ID: <200701040022.l040M9NM018814@sheep.berlios.de>

Author: chapados
Date: 2007-01-04 01:22:08 +0100 (Thu, 04 Jan 2007)
New Revision: 6553

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Minor message change

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-03 21:25:31 UTC (rev 6552)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-04 00:22:08 UTC (rev 6553)
@@ -509,7 +509,7 @@
     m_optimizer->setToOptimize(hyperparam_vars, (Variable*)nll);
     m_optimizer->build();
     PP<ProgressBar> pb(
-        report_progress? new ProgressBar("Hyperoptimizing GaussianProcessRegressor "
+        report_progress? new ProgressBar("Training GaussianProcessRegressor "
                                          "from stage " + tostring(stage) + " to stage " +
                                          tostring(nstages), nstages-stage)
         : 0);



From chapados at mail.berlios.de  Thu Jan  4 21:31:25 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 4 Jan 2007 21:31:25 +0100
Subject: [Plearn-commits] r6554 - trunk/plearn/python
Message-ID: <200701042031.l04KVPb4019221@sheep.berlios.de>

Author: chapados
Date: 2007-01-04 21:31:24 +0100 (Thu, 04 Jan 2007)
New Revision: 6554

Modified:
   trunk/plearn/python/PythonObjectWrapper.cc
   trunk/plearn/python/PythonObjectWrapper.h
Log:
Added conversion from Python to C++ that does not print a traceback on the event of failure; useful for catching such errors

Modified: trunk/plearn/python/PythonObjectWrapper.cc
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.cc	2007-01-04 00:22:08 UTC (rev 6553)
+++ trunk/plearn/python/PythonObjectWrapper.cc	2007-01-04 20:31:24 UTC (rev 6554)
@@ -55,10 +55,12 @@
 
 // Error-reporting
 void PLPythonConversionError(const char* function_name,
-                             PyObject* pyobj)
+                             PyObject* pyobj, bool print_traceback)
 {
-    fprintf(stderr,"For python object: ");
-    PyObject_Print(pyobj, stderr, Py_PRINT_RAW);
+    if (print_traceback) {
+        fprintf(stderr,"For python object: ");
+        PyObject_Print(pyobj, stderr, Py_PRINT_RAW);
+    }
     PLERROR("Cannot convert Python object using %s", function_name);
 }
 
@@ -78,52 +80,54 @@
 
 //#####  ConvertFromPyObject  #################################################
 
-bool ConvertFromPyObject<bool>::convert(PyObject* pyobj)
+bool ConvertFromPyObject<bool>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     return PyObject_IsTrue(pyobj) != 0;
 }
 
-int ConvertFromPyObject<int>::convert(PyObject* pyobj)
+int ConvertFromPyObject<int>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyInt_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<int>", pyobj);
+        PLPythonConversionError("ConvertFromPyObject<int>", pyobj, print_traceback);
     return int(PyInt_AS_LONG(pyobj));
 }
 
-long ConvertFromPyObject<long>::convert(PyObject* pyobj)
+long ConvertFromPyObject<long>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyLong_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<long>", pyobj);
+        PLPythonConversionError("ConvertFromPyObject<long>", pyobj, print_traceback);
     return PyLong_AsLong(pyobj);
 }
 
-double ConvertFromPyObject<double>::convert(PyObject* pyobj)
+double ConvertFromPyObject<double>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyFloat_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<double>", pyobj);
+        PLPythonConversionError("ConvertFromPyObject<double>", pyobj,
+                                print_traceback);
     return PyFloat_AS_DOUBLE(pyobj);
 }
 
-string ConvertFromPyObject<string>::convert(PyObject* pyobj)
+string ConvertFromPyObject<string>::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyString_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject<string>", pyobj);
+        PLPythonConversionError("ConvertFromPyObject<string>", pyobj,
+                                print_traceback);
     return PyString_AsString(pyobj);
 }
 
-void ConvertFromPyObject<Vec>::convert(PyObject* pyobj, Vec& v)
+void ConvertFromPyObject<Vec>::convert(PyObject* pyobj, Vec& v, bool print_traceback)
 {
     // NA_InputArray possibly creates a well-behaved temporary (i.e. not
     // discontinuous is memory)
     PLASSERT( pyobj );
     PyArrayObject* pyarr = NA_InputArray(pyobj, tFloat64, NUM_C_ARRAY);
     if (! pyarr)
-        PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj);
+        PLPythonConversionError("ConvertFromPyObject<Vec>", pyobj, print_traceback);
     if (pyarr->nd != 1)
         PLERROR("ConvertFromPyObject<Vec>: Dimensionality of the returned array "
                 "should be 1; got %d", pyarr->nd);
@@ -133,21 +137,21 @@
     Py_XDECREF(pyarr);
 }
 
-Vec ConvertFromPyObject<Vec>::convert(PyObject* pyobj)
+Vec ConvertFromPyObject<Vec>::convert(PyObject* pyobj, bool print_traceback)
 {
     Vec v;
-    convert(pyobj, v);
+    convert(pyobj, v, print_traceback);
     return v;
 }
 
-void ConvertFromPyObject<Mat>::convert(PyObject* pyobj, Mat& m)
+void ConvertFromPyObject<Mat>::convert(PyObject* pyobj, Mat& m, bool print_traceback)
 {
     // NA_InputArray possibly creates a well-behaved temporary (i.e. not
     // discontinuous is memory)
     PLASSERT( pyobj );
     PyArrayObject* pyarr = NA_InputArray(pyobj, tFloat64, NUM_C_ARRAY);
     if (! pyarr)
-        PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj);
+        PLPythonConversionError("ConvertFromPyObject<Mat>", pyobj, print_traceback);
     if (pyarr->nd != 2)
         PLERROR("ConvertFromPyObject<Mat>: Dimensionality of the returned array "
                 "should be 2; got %d", pyarr->nd);
@@ -158,17 +162,17 @@
     Py_XDECREF(pyarr);
 }
 
-Mat ConvertFromPyObject<Mat>::convert(PyObject* pyobj)
+Mat ConvertFromPyObject<Mat>::convert(PyObject* pyobj, bool print_traceback)
 {
     Mat m;
-    convert(pyobj, m);
+    convert(pyobj, m, print_traceback);
     return m;
 }
 
-VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj)
+VMat ConvertFromPyObject<VMat>::convert(PyObject* pyobj, bool print_traceback)
 {
     Mat m;
-    ConvertFromPyObject<Mat>::convert(pyobj, m);
+    ConvertFromPyObject<Mat>::convert(pyobj, m, print_traceback);
     return m;
 }
 

Modified: trunk/plearn/python/PythonObjectWrapper.h
===================================================================
--- trunk/plearn/python/PythonObjectWrapper.h	2007-01-04 00:22:08 UTC (rev 6553)
+++ trunk/plearn/python/PythonObjectWrapper.h	2007-01-04 20:31:24 UTC (rev 6554)
@@ -63,8 +63,10 @@
 
 class PythonObjectWrapper;                   // Forward-declare
 
-//! Used for error reporting
-void PLPythonConversionError(const char* function_name, PyObject* pyobj);
+//! Used for error reporting.  If 'print_traceback' is true, a full
+//! Python traceback is printed to stderr.  Otherwise, raise PLERROR.
+void PLPythonConversionError(const char* function_name, PyObject* pyobj,
+                             bool print_traceback);
 
 
 //#####  PythonGlobalInterpreterLock  #########################################
@@ -137,82 +139,84 @@
 template <>
 struct ConvertFromPyObject<bool>
 {
-    static bool convert(PyObject*);
+    static bool convert(PyObject*, bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<int>
 {
-    static int convert(PyObject*);
+    static int convert(PyObject*, bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<long>
 {
-    static long convert(PyObject*);
+    static long convert(PyObject*, bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<double>
 {
-    static double convert(PyObject*);
+    static double convert(PyObject*, bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<string>
 {
-    static string convert(PyObject*);
+    static string convert(PyObject*, bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<Vec>
 {
     // Return fresh storage
-    static Vec convert(PyObject*);
+    static Vec convert(PyObject*, bool print_traceback);
 
     // Convert into pre-allocated Vec; resize it if necessary
-    static void convert(PyObject* pyobj, Vec& result);
+    static void convert(PyObject* pyobj, Vec& result,
+                        bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<Mat>
 {
     // Return fresh storage
-    static Mat convert(PyObject*);
+    static Mat convert(PyObject*, bool print_traceback=true);
 
     // Convert into pre-allocated Vec; resize it if necessary
-    static void convert(PyObject* pyobj, Mat& result);
+    static void convert(PyObject* pyobj, Mat& result,
+                        bool print_traceback);
 };
 
 template <>
 struct ConvertFromPyObject<VMat>
 {
     // Return new MemoryVMatrix
-    static VMat convert(PyObject*);
+    static VMat convert(PyObject*, bool print_traceback);
 };
 
 template <class T>
 struct ConvertFromPyObject< TVec<T> >
 {
-    static TVec<T> convert(PyObject*);
+    static TVec<T> convert(PyObject*, bool print_traceback);
 };
 
 template <class T>
 struct ConvertFromPyObject< std::vector<T> >
 {
-    static std::vector<T> convert(PyObject*);
+    static std::vector<T> convert(PyObject*, bool print_traceback);
 };
 
 template <class T, class U>
 struct ConvertFromPyObject< std::map<T,U> >
 {
-    static std::map<T,U> convert(PyObject*);
+    static std::map<T,U> convert(PyObject*, bool print_traceback);
 };
 
 template <class T, class U>
 struct ConvertFromPyObject< std::pair<T,U> >
 {
-    static std::pair<T,U> convert(PyObject*);
+    static std::pair<T,U> convert(PyObject*, bool print_traceback);
 };
 
 
@@ -321,15 +325,26 @@
 
     //#####  Conversion Back to C++  ##########################################
 
-    // General version that relies on ConvertFromPyOBject
+    //! General version that relies on ConvertFromPyOBject
     template <class T>
     T as() const
     {
-        return ConvertFromPyObject<T>::convert(m_object);
+        return ConvertFromPyObject<T>::convert(m_object, true);
     }
+
+    /**
+     *  Conversion that does not print any traceback to stderr if there is a
+     *  conversion error.  This should be used in the rare cases where multiple
+     *  return types are expected from Python and one wants to attempt several
+     *  (e.g. from more specific to more general).
+     */
+    template <class T>
+    T asNoTraceback() const
+    {
+        return ConvertFromPyObject<T>::convert(m_object, false);
+    }
     
 
-
     //#####  Low-Level PyObject Creation  #####################################
 
     /**
@@ -451,7 +466,7 @@
 //#####  ConvertFromPyObject Implementations  #################################
 
 template <class T>
-TVec<T> ConvertFromPyObject< TVec<T> >::convert(PyObject* pyobj)
+TVec<T> ConvertFromPyObject< TVec<T> >::convert(PyObject* pyobj, bool print_traceback)
 {
     PLASSERT( pyobj );
     
@@ -462,7 +477,7 @@
         TVec<T> v(size);
         for (int i=0 ; i<size ; ++i) {
             PyObject* elem_i = PyTuple_GET_ITEM(pyobj, i);
-            v[i] = ConvertFromPyObject<T>::convert(elem_i);
+            v[i] = ConvertFromPyObject<T>::convert(elem_i, print_traceback);
         }
         return v;
     }
@@ -472,61 +487,67 @@
         TVec<T> v(size);
         for (int i=0 ; i<size ; ++i) {
             PyObject* elem_i = PyList_GET_ITEM(pyobj, i);
-            v[i] = ConvertFromPyObject<T>::convert(elem_i);
+            v[i] = ConvertFromPyObject<T>::convert(elem_i, print_traceback);
         }
         return v;
     }
     else
-        PLPythonConversionError("ConvertFromPyObject< TVec<T> >", pyobj);
+        PLPythonConversionError("ConvertFromPyObject< TVec<T> >", pyobj,
+                                print_traceback);
 
     return TVec<T>();                        // Shut up compiler
 }
 
 template <class T>
-std::vector<T> ConvertFromPyObject< std::vector<T> >::convert(PyObject* pyobj)
+std::vector<T> ConvertFromPyObject< std::vector<T> >::convert(PyObject* pyobj,
+                                                              bool print_traceback)
 {
     PLASSERT( pyobj );
     
     // Simple but inefficient implementation: create temporary TVec and copy
     // into a vector
-    TVec<T> v = ConvertFromPyObject< TVec<T> >::convert(pyobj);
+    TVec<T> v = ConvertFromPyObject< TVec<T> >::convert(pyobj, print_traceback);
     return std::vector<T>(v.begin(), v.end());
 }
 
 template <class T, class U>
-std::map<T,U> ConvertFromPyObject< std::map<T,U> >::convert(PyObject* pyobj)
+std::map<T,U> ConvertFromPyObject< std::map<T,U> >::convert(PyObject* pyobj,
+                                                            bool print_traceback)
 {
     PLASSERT( pyobj );
     if (! PyDict_Check(pyobj))
-        PLPythonConversionError("ConvertFromPyObject< std::map<T,U> >", pyobj);
+        PLPythonConversionError("ConvertFromPyObject< std::map<T,U> >", pyobj,
+                                print_traceback);
     
     PyObject *key, *value;
     int pos = 0;
     std::map<T,U> result;
 
     while (PyDict_Next(pyobj, &pos, &key, &value)) {
-        T the_key = ConvertFromPyObject<T>::convert(key);
-        U the_val = ConvertFromPyObject<U>::convert(value);
+        T the_key = ConvertFromPyObject<T>::convert(key, print_traceback);
+        U the_val = ConvertFromPyObject<U>::convert(value, print_traceback);
         result.insert(make_pair(the_key,the_val));
     }
     return result;
 }
 
 template <class T, class U>
-std::pair<T,U> ConvertFromPyObject< std::pair<T,U> >::convert(PyObject* pyobj)
+std::pair<T,U> ConvertFromPyObject< std::pair<T,U> >::convert(PyObject* pyobj,
+                                                              bool print_traceback)
 {
     PLASSERT( pyobj );
     // Here, we support both Python Tuples and Lists
     if (! PyTuple_Check(pyobj) && PyTuple_GET_SIZE(pyobj) != 2)
-        PLPythonConversionError("ConvertFromPyObject< std::pair<T,U> >", pyobj);
+        PLPythonConversionError("ConvertFromPyObject< std::pair<T,U> >", pyobj,
+                                print_traceback);
 
     std::pair<T,U> p;
 
     PyObject* first = PyTuple_GET_ITEM(pyobj, 0);
-    p.first = ConvertFromPyObject<T>::convert(first);
+    p.first = ConvertFromPyObject<T>::convert(first, print_traceback);
 
     PyObject* second = PyTuple_GET_ITEM(pyobj, 1);
-    p.second = ConvertFromPyObject<T>::convert(second);
+    p.second = ConvertFromPyObject<T>::convert(second, print_traceback);
 
     return p;
 }



From dorionc at mail.berlios.de  Thu Jan  4 22:37:11 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 4 Jan 2007 22:37:11 +0100
Subject: [Plearn-commits] r6555 - trunk/python_modules/plearn/pytest
Message-ID: <200701042137.l04LbBON029143@sheep.berlios.de>

Author: dorionc
Date: 2007-01-04 22:37:11 +0100 (Thu, 04 Jan 2007)
New Revision: 6555

Modified:
   trunk/python_modules/plearn/pytest/modes.py
   trunk/python_modules/plearn/pytest/programs.py
Log:
The pymake-compile option had been made unusable by passage to PyPLearnSingleton. Fixed.

Modified: trunk/python_modules/plearn/pytest/modes.py
===================================================================
--- trunk/python_modules/plearn/pytest/modes.py	2007-01-04 20:31:24 UTC (rev 6554)
+++ trunk/python_modules/plearn/pytest/modes.py	2007-01-04 21:37:11 UTC (rev 6555)
@@ -704,11 +704,13 @@
     #  Instance methods
     #    
     def __init__( self, targets, options ):
+        logging.debug("--pymake-compile (=%s) option forwarded to Program."%options.pymake_compile)
+        Program.pymake_opt_override = options.pymake_compile
+
+        # The option had to be forwarded to Program *before* calling the
+        # inherited ctor since Program instances are instanciated there...
         super(RoutineBasedMode, self).__init__(targets, options)
 
-        logging.debug("--pymake-compile (=%s) option forwarded to Program."%options.pymake_compile)
-        Program.pymake_opt_override = options.pymake_compile
-        
         test_instances = Test._instances_map.items()        
         self.dispatch_tests(test_instances)
 

Modified: trunk/python_modules/plearn/pytest/programs.py
===================================================================
--- trunk/python_modules/plearn/pytest/programs.py	2007-01-04 20:31:24 UTC (rev 6554)
+++ trunk/python_modules/plearn/pytest/programs.py	2007-01-04 21:37:11 UTC (rev 6555)
@@ -139,12 +139,14 @@
         core.PyTestObject.__init__(self, **overrides)
         assert self.name is not None
         assert self.compiler is not None or self.compile_options is None
+        logging.debug("Creating program using compiler %s with compile_options '%s'."%(self.compiler, self.compile_options))
 
         # Methods are called even if messages are not logged
         logging.debug("\nProgram: "+self.getProgramPath())
 
         #SING: internal_exec_path = self.getInternalExecPath(overrides.pop('_signature'))
-        internal_exec_path = self.getInternalExecPath(self._signature(**overrides))
+        #CLS:  internal_exec_path = self.getInternalExecPath(self._signature(**overrides))
+        internal_exec_path = self.getInternalExecPath(self.__signature())
         logging.debug("Internal Exec Path: %s"%internal_exec_path)
         if self.isCompilable():
             if self.compiler is None:
@@ -153,6 +155,17 @@
             self.__log_file_path = internal_exec_path+'.log'
         logging.debug("Program instance: %s\n"%repr(self))
 
+    def __signature(self):
+        if self.compiler is None:
+            signature = self.name
+        else:
+            if self.compile_options == "":
+                self.compile_options = None                
+            signature = "%s__compiler_%s__options_%s"%(
+                self.name, self.compiler, self.compile_options)
+        signature = signature.replace('-', '') # Compile options...
+        return signature.replace(' ', '_')
+
     def _optionFormat(self, option_pair, indent_level, inner_repr):
         optname, val = option_pair
         if val is None:



From tihocan at mail.berlios.de  Fri Jan  5 20:04:29 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Fri, 5 Jan 2007 20:04:29 +0100
Subject: [Plearn-commits] r6556 - trunk/plearn/math
Message-ID: <200701051904.l05J4TGk018342@sheep.berlios.de>

Author: tihocan
Date: 2007-01-05 20:04:28 +0100 (Fri, 05 Jan 2007)
New Revision: 6556

Modified:
   trunk/plearn/math/VecStatsCollector.cc
Log:
Renamed a variable to prevent confusion (and get rid of Intel Compiler remark)

Modified: trunk/plearn/math/VecStatsCollector.cc
===================================================================
--- trunk/plearn/math/VecStatsCollector.cc	2007-01-04 21:37:11 UTC (rev 6555)
+++ trunk/plearn/math/VecStatsCollector.cc	2007-01-05 19:04:28 UTC (rev 6556)
@@ -279,9 +279,9 @@
     {
         tuple<Vec, real> outdated = m_observation_window->update(x, weight);
         Vec& obs = get<0>(outdated);
-        real weight = get<1>(outdated);
+        real w = get<1>(outdated);
         if ( obs.isNotEmpty() )
-            remove_observation(obs, weight);
+            remove_observation(obs, w);
     }
 }
 



From chapados at mail.berlios.de  Sat Jan  6 01:10:04 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sat, 6 Jan 2007 01:10:04 +0100
Subject: [Plearn-commits] r6557 - trunk/plearn/ker
Message-ID: <200701060010.l060A4s5007890@sheep.berlios.de>

Author: chapados
Date: 2007-01-06 01:10:04 +0100 (Sat, 06 Jan 2007)
New Revision: 6557

Added:
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn/ker/SummationKernel.h
Log:
Summation kernel: add the values of several sub-kernels

Added: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-01-05 19:04:28 UTC (rev 6556)
+++ trunk/plearn/ker/SummationKernel.cc	2007-01-06 00:10:04 UTC (rev 6557)
@@ -0,0 +1,165 @@
+// -*- C++ -*-
+
+// SummationKernel.cc
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file SummationKernel.cc */
+
+
+#include "SummationKernel.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    SummationKernel,
+    "Kernel computing the sum of other kernels",
+    "This kernel computes the summation of several subkernel objects.  It can\n"
+    "also chop up parts of its input vector and send it to each kernel (so that\n"
+    "each kernel can operate on a subset of the variables).\n"
+    );
+
+
+//#####  Constructor  #########################################################
+
+SummationKernel::SummationKernel()
+{ }
+
+
+//#####  declareOptions  ######################################################
+
+void SummationKernel::declareOptions(OptionList& ol)
+{
+    declareOption(
+        ol, "terms", &SummationKernel::m_terms, OptionBase::buildoption,
+        "Individual kernels to add to produce the final result.  The\n"
+        "hyperparameters of kernel i can be accesed under the option names\n"
+        "'terms[i].hyperparam' for, e.g. GaussianProcessRegressor.\n");
+
+    declareOption(
+        ol, "input_indexes", &SummationKernel::m_input_indexes,
+        OptionBase::buildoption,
+        "Optionally, one can specify which of individual input variables should\n"
+        "be routed to each kernel.  The format is as a vector of vectors: for\n"
+        "each kernel in 'terms', one must list the INDEXES in the original input\n"
+        "vector(zero-based) that should be passed to that kernel.  If a list of\n"
+        "indexes is empty for a given kernel, it means that the COMPLETE input\n"
+        "vector should be passed to the kernel.\n");
+    
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+
+//#####  build  ###############################################################
+
+void SummationKernel::build()
+{
+    // ### Nothing to add here, simply calls build_
+    inherited::build();
+    build_();
+}
+
+
+//#####  build_  ##############################################################
+
+void SummationKernel::build_()
+{
+    // Preallocate buffers for kernel evaluation
+    const int N = m_input_indexes.size();
+    m_input_buf1.resize(N);
+    m_input_buf2.resize(N);
+    for (int i=0 ; i<N ; ++i) {
+        const int M = m_input_indexes[i].size();
+        m_input_buf1[i].resize(M);
+        m_input_buf2[i].resize(M);
+    }
+
+    for (int i=0, n=m_terms.size() ; i<n ; ++i)
+        if (! m_terms[i])
+            PLERROR("SummationKernel::build_: kernel for term[%d] is not specified",i);
+
+    if (m_input_indexes.size() > 0 && m_terms.size() != m_input_indexes.size())
+        PLERROR("SummationKernel::build_: if 'input_indexes' is specified "
+                "it must have the same size (%d) as 'terms'; found %d elements",
+                m_terms.size(), m_input_indexes.size());
+}
+
+
+//#####  evaluate  ############################################################
+
+real SummationKernel::evaluate(const Vec& x1, const Vec& x2) const
+{
+    real kernel_value = 0.0;
+    bool split_inputs = m_input_indexes.size() > 0;
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
+        if (split_inputs && m_input_indexes[i].size() > 0) {
+            selectElements(x1, m_input_indexes[i], m_input_buf1[i]);
+            selectElements(x2, m_input_indexes[i], m_input_buf2[i]);
+            kernel_value += m_terms[i]->evaluate(m_input_buf1[i],
+                                                 m_input_buf2[i]);
+        }
+        else
+            kernel_value += m_terms[i]->evaluate(x1,x2);
+    }
+    return kernel_value;
+}
+
+
+//#####  makeDeepCopyFromShallowCopy  #########################################
+
+void SummationKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(m_terms,          copies);
+    deepCopyField(m_input_indexes,  copies);
+    deepCopyField(m_input_buf1,     copies);
+    deepCopyField(m_input_buf2,     copies);
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn/ker/SummationKernel.h
===================================================================
--- trunk/plearn/ker/SummationKernel.h	2007-01-05 19:04:28 UTC (rev 6556)
+++ trunk/plearn/ker/SummationKernel.h	2007-01-06 00:10:04 UTC (rev 6557)
@@ -0,0 +1,134 @@
+// -*- C++ -*-
+
+// SummationKernel.h
+//
+// Copyright (C) 2007 Nicolas Chapados
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Nicolas Chapados
+
+/*! \file SummationKernel.h */
+
+
+#ifndef SummationKernel_INC
+#define SummationKernel_INC
+
+#include <plearn/ker/Kernel.h>
+
+namespace PLearn {
+
+/**
+ *  Kernel computing the sum of other kernels
+ *
+ *  This kernel computes the summation of several subkernel objects.  It can
+ *  also chop up parts of its input vector and send it to each kernel (so that
+ *  each kernel can operate on a subset of the variables).
+ */
+class SummationKernel : public Kernel
+{
+    typedef Kernel inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    /**
+     *  Individual kernels to add to produce the final result.  The
+     *  hyperparameters of kernel i can be accesed under the option names
+     *  'terms[i].hyperparam' for, e.g. GaussianProcessRegressor.
+     */
+    TVec<Ker> m_terms;
+
+    /**
+     *  Optionally, one can specify which of individual input variables should
+     *  be routed to each kernel.  The format is as a vector of vectors: for
+     *  each kernel in 'terms', one must list the INDEXES in the original input
+     *  vector(zero-based) that should be passed to that kernel.  If a list of
+     *  indexes is empty for a given kernel, it means that the COMPLETE input
+     *  vector should be passed to the kernel.
+     */
+    TVec< TVec<int> > m_input_indexes;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    SummationKernel();
+
+
+    //#####  Kernel Member Functions  #########################################
+
+    //! Compute K(x1,x2).
+    virtual real evaluate(const Vec& x1, const Vec& x2) const;
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(SummationKernel);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //! Input buffers for kernel evaluation in cases where subsetting is needed
+    TVec<Vec> m_input_buf1;
+    TVec<Vec> m_input_buf2;
+
+protected:
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //! This does the actual building.
+    void build_();
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SummationKernel);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From dorionc at mail.berlios.de  Sat Jan  6 22:39:16 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Sat, 6 Jan 2007 22:39:16 +0100
Subject: [Plearn-commits] r6558 - in tags: .
	POTS-0.1/python_modules/plearn/math
	POTS-0.1/python_modules/plearn/report
Message-ID: <200701062139.l06LdGhU015814@sheep.berlios.de>

Author: dorionc
Date: 2007-01-06 22:39:15 +0100 (Sat, 06 Jan 2007)
New Revision: 6558

Added:
   tags/POTS-0.1/
Modified:
   tags/POTS-0.1/python_modules/plearn/math/__init__.py
   tags/POTS-0.1/python_modules/plearn/math/arrays.py
   tags/POTS-0.1/python_modules/plearn/math/statistical_tools.py
   tags/POTS-0.1/python_modules/plearn/report/formatter.py
Log:
Tag pour release POTS 0.1

Copied: tags/POTS-0.1 (from rev 6555, trunk)

Modified: tags/POTS-0.1/python_modules/plearn/math/__init__.py
===================================================================
--- trunk/python_modules/plearn/math/__init__.py	2007-01-04 21:37:11 UTC (rev 6555)
+++ tags/POTS-0.1/python_modules/plearn/math/__init__.py	2007-01-06 21:39:15 UTC (rev 6558)
@@ -31,11 +31,11 @@
 
 # Author: Christian Dorion
 
-# Most of the content of this __init__ file has been moved to 'arrays.py'. For
-# backward comptibility, the functions defined there are forwarded
-# here. However, the 'arrays' module requires numarray... this 'try'
-# statement should make sure other stuff defined here still work even when
-# numarray is not available.
+# Most of the content of this __init__ file has been moved to
+# 'arrays.py'. For backward comptibility, the functions defined there are
+# forwarded here. However, the 'arrays' module requires numarray... this
+# 'try' statement should make sure other stuff defined here still work even
+# when numarray is not available.
 try:
     from arrays import *
 except ImportError:

Modified: tags/POTS-0.1/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-01-04 21:37:11 UTC (rev 6555)
+++ tags/POTS-0.1/python_modules/plearn/math/arrays.py	2007-01-06 21:39:15 UTC (rev 6558)
@@ -113,8 +113,15 @@
     function accepts as many matrices as one wants, processing multiplication
     'from left to right'.
     """
+    #mmult_shapes(*args)
     return reduce(matrixmultiply, args)
 
+def mmult_shapes(*args):
+    """For debugging purposes; print shapes."""
+    for a in args:
+        print shape(a),
+    print
+
 def rrange(start, stop, step, include_stop=False):
     """Behaves like the builtin range() function but with real arguments.
 
@@ -137,6 +144,15 @@
     m = asarray(m)
     return zeros(shape(m))-ufunc.less(m,0)+ufunc.greater(m,0)
 
+def to_diagonal(a):
+    """Returns a diagonal matrix with elements in 'a' on the diagonal."""
+    assert len(a.shape)==1
+    n = len(a)
+    A = zeros(shape=(n,n), type=a.type())
+    for i in range(n):
+        A[i,i] = a[i]
+    return A
+
 def fast_softmax( x ):
     s = 0
     res = []
@@ -148,10 +164,9 @@
     return [ r/s for r in res ]
 
 def hasNaN(f):
-    has = sum(isNaN(f))
-    while not isinstance(has, int):
-        has = sum(has)
-    return has
+    f = ravel(f)
+    f = choose(isNaN(f), (0, 1))
+    return sum(f)
     
 def isNaN(f):
     """Return 1 where f contains NaN values, 0 elsewhere."""
@@ -192,6 +207,7 @@
     
     a = [1.0, float('NaN'), 3.0, float('NaN')]
     print a
+    print hasNaN(a)
     print replace_nans(a)
 
     print 

Modified: tags/POTS-0.1/python_modules/plearn/math/statistical_tools.py
===================================================================
--- trunk/python_modules/plearn/math/statistical_tools.py	2007-01-04 21:37:11 UTC (rev 6555)
+++ tags/POTS-0.1/python_modules/plearn/math/statistical_tools.py	2007-01-06 21:39:15 UTC (rev 6558)
@@ -333,7 +333,7 @@
         prediction = mmult(_X_, aug_beta)
         epsilon[ycol][where(isNotNaN(Ycol))] = Y - prediction
 
-    # Is sigma really supposed to be NxN?
+    # The estimate of the covariance matrix of the errors
     sigma = mmult(epsilon, transpose(epsilon)) / T    
 
     # if hccm is not None:
@@ -354,99 +354,10 @@
         
     return alpha, beta, epsilon, sigma
 
-#####  Very First Sketch...  ################################################
 
-class LinearGMM(object):
-    """A GMM implementation assuming a linear model and a predetermined 'X'.
+#####  Builtin Tests  #######################################################
 
-    NOTE:
-    This thing is a *very first* sketch toward a more general GMM class...
-    
-    Notation:
-    
-    X (T x k), \beta \in \RSet[k], y,u \in \RSet[T]
-       y = X \beta + u
-         where E[ u u\Tr ] = \Omega (T x T)
-    
-    Instruments W (T x l), T > l, l >= k, 
-       E[ u_t | W_t ] = 0;  E[ u_t u_s | W_t W_s ] = \omega_{ts}
-    
-    The above implies that:
-       E[ W_t\Tr ( y_t - X_t \beta ) ] = 0 \forall t
-    
-    Suppose J (l x k) full rank is s.t.
-       J\Tr W\Tr (y- X\beta) = 0
-    => Orthogonality conditions!!
-    => J = (W\Tr \Omega_0 W)\inv W\Tr X
-     where the 0 subscript denotes the TRUE value of \Omega
-    
-    GMM criterion:
-    
-     Q(\beta, y)
-      = (y - X \beta)\Tr W (W\Tr \Omega_0 W)\inv W\Tr (y - X\beta)
-      = T^{-0.5} (y - X \beta)\Tr W (T\inv W\Tr \Omega_0 W)\inv W\Tr (y - X\beta) T^{-0.5}
-    
-     => Q(\beta_0, y_0) \simASY \Chi^2(l)
-    
-    \betahat_{GMM}
-        = \BETA(\Omega_0)
-        = (X\Tr W (W\Tr \Omega_0 W)\inv W\Tr X)\inv X\Tr W (W\Tr \Omega_0 W)\inv W\Tr y
-    
-    Inefficient
-      \betahat_{iGMM} = (X\Tr W \Lambda W\Tr X)\inv X\Tr W \Lambda W\Tr y
-    
-    IV with W:  \uhat
-      => Let \Omegahat = Diag(\uhat)
-      => \betahat_{FGMM} = \BETA(\Omegahat)
-    
-    \Varhat( \betahat_{FGMM} ) = (X\Tr W (W\Tr \Omegahat W)\inv W\Tr X)\inv
-    """
-
-    def __init__(self, y, X, W):
-        self.y = y
-        self.X = X
-        self.W = W
-
-        T = len(y)
-        self.T = T
-        assert X.shape[0] == T
-        assert W.shape[0] == T
-        assert W.shape[1] >= X.shape[1]
-
-        # Performing IV with W as instruments
-        Xt       = transpose(X)
-        Wt       = transpose(W)
-        WtW      = mmult(Wt, W)
-        WtW_inv  = inverse(WtW)
-        ProjW    = mmult(W, WtW_inv, Wt) 
-        XtProjW  = mmult(Xt, ProjW)
-
-        self.beta_iv = mmult(inverse(mmult(XtProjW, X)), XtProjW, y)
-
-        # The IV residuals
-        self.uhat_iv = y - mmult(X, self.beta_iv)
-        
-        # Consistent estimator of the covariance matrix of the error terms
-        Omegahat = zeros((T, T), type=Float64)
-        for t in range(T):
-            Omegahat[t,t] = self.uhat_iv[t]**2
-
-        # Feasible GMM
-        XtW         = mmult(Xt, W)
-        WtX         = transpose(XtW)
-        Lambda      = inverse( mmult(Wt, Omegahat, W) )
-        XtWLambda   = mmult(XtW, Lambda)
-        XtWLambdaWt = mmult(XtWLambda, Wt)
-
-        # This is the (feasible) GMM beta!
-        self.beta = mmult(inverse(mmult(XtWLambdaWt, X)), XtWLambdaWt, y)
-
-        # And these are the residuals
-        self.uhat = y - mmult(X, self.beta)
-
-
-
-def test_ols_regression(T, K, alpha=0.0, scale=10.0, plot=False):
+def _test_ols_regression(T, K, alpha=0.0, scale=10.0, plot=False):
     u = normal(0, 1, (T,))
     beta = range(1, K+1)
     
@@ -491,35 +402,29 @@
 
     return X, Y, olsR, ols, scipy
 
+class StatsHolder(dict):
+    def __init__(self, **kwargs):
+        dict.__init__(self, kwargs)
+
+    __getattr__ = dict.__getitem__
+    __setattr__ = dict.__setitem__
+    __delattr__ = dict.__delitem__
+
 if __name__ == "__main__":        
     from numarray.random_array import seed, random, normal
     seed(02, 25)
     
     # Setting the problem
     print "T=10, K=1, (a, b)=(50, 1)"
-    test_ols_regression(10, 1, 50.0, plot=True)
+    _test_ols_regression(10, 1, 50.0, plot=True)
 
     print
     print "T=100, K=1, (a, b)=(25, 1)"
-    test_ols_regression(100, 1, 25.0, plot=True)
+    _test_ols_regression(100, 1, 25.0, plot=True)
 
     print
     print "T=100, K=3 (a, b)=(25, 1)"
     try:
-        test_ols_regression(100, 3, 25.0)
+        _test_ols_regression(100, 3, 25.0)
     except:
         print "SciPy FAILS!"
-
-    # # Performing OLS
-    # Xt = transpose(X)
-    # XtX = mmult(Xt, X)
-    # XtY = mmult(Xt, y)
-    # beta_ols = mmult( inverse(XtX), XtY)
-    # print "Distance beta_0 and beta_ols:", matrix_distance(beta, beta_ols)
-    # 
-    # gmm = LinearGMM(y, X, X)
-    # print "Distance beta_ols and beta_IV:", matrix_distance(beta_ols, gmm.beta_iv)
-    # print "Distance beta_ols and beta_GMM:", matrix_distance(beta_ols, gmm.beta)
-    # 
-    # print "GMM residuals:"
-    # print gmm.uhat

Modified: tags/POTS-0.1/python_modules/plearn/report/formatter.py
===================================================================
--- trunk/python_modules/plearn/report/formatter.py	2007-01-04 21:37:11 UTC (rev 6555)
+++ tags/POTS-0.1/python_modules/plearn/report/formatter.py	2007-01-06 21:39:15 UTC (rev 6558)
@@ -63,7 +63,7 @@
 def latexTable(table, headers=[], 
                align="", super_headers=[],
                padding=0.5, vpadding=0.0, caption="", label="",
-               fontsize="", landscape=False, writer=DEFAULT_WRITER):
+               fontsize="", landscape=False, targetpos="", writer=DEFAULT_WRITER):
     lwriter = lambda line : writer("%s\n"%line)
     if align:
         assert len(align)==len(table[0]), \
@@ -86,7 +86,7 @@
     if landscape:
         lwriter(r"\begin{landscape}")
         lwriter(r"\addtocounter{@inlandscapetable}{1}")
-    lwriter("\\begin{table}")
+    lwriter("\\begin{table}%s"%targetpos)
     
     lwriter("\\begin{center}")
     if fontsize:
@@ -133,6 +133,10 @@
             handling_multicol.append(elem)
     writer('&'.join(handling_multicol) + r"\\" + "\n")
 
+def vpaddingLine(vpadding, length):
+    vpad = r"\raisebox{%.3fcm}{\rule{0pt}{%.3fcm}}"%(-0.5*vpadding, vpadding)
+    return [vpad]+[""]*(length-1)
+
 def strictlyUpperTriangularTable(table, headers=[], format="%s"):
     """Returns a table of strings and modified headers suitable for latex/twikiTable.
 



From chapados at mail.berlios.de  Mon Jan  8 02:05:04 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Mon, 8 Jan 2007 02:05:04 +0100
Subject: [Plearn-commits] r6559 - trunk/plearn_learners/regressors
Message-ID: <200701080105.l081543F010308@sheep.berlios.de>

Author: chapados
Date: 2007-01-08 02:05:03 +0100 (Mon, 08 Jan 2007)
New Revision: 6559

Modified:
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Minor cosmetic change in log output

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-06 21:39:15 UTC (rev 6558)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-08 01:05:03 UTC (rev 6559)
@@ -525,6 +525,7 @@
         early_stopping = m_optimizer->optimizeN(*statscol);
         statscol->finalize();
     }
+    pb = 0;                                  // Finish progress bar right now
 
     // Some logging about the final values
     logVarray(hyperparam_vars, "Hyperparameter final values:");
@@ -539,7 +540,7 @@
 {
     string entry = title + '\n';
     for (int i=0, n=varr.size() ; i<n ; ++i) {
-        entry += right(varr[i]->getName(), 25) + ": " + tostring(varr[i]->value[0]);
+        entry += right(varr[i]->getName(), 35) + ": " + tostring(varr[i]->value[0]);
         if (i < n-1)
             entry += '\n';
     }



From tihocan at mail.berlios.de  Mon Jan  8 19:07:02 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 8 Jan 2007 19:07:02 +0100
Subject: [Plearn-commits] r6560 - trunk/commands/PLearnCommands
Message-ID: <200701081807.l08I726m003189@sheep.berlios.de>

Author: tihocan
Date: 2007-01-08 19:07:01 +0100 (Mon, 08 Jan 2007)
New Revision: 6560

Modified:
   trunk/commands/PLearnCommands/VMatCommand.cc
Log:
Hack to transform a 'vmat view' command into the new (less intuitive) 'vmat_view' syntax

Modified: trunk/commands/PLearnCommands/VMatCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/VMatCommand.cc	2007-01-08 01:05:03 UTC (rev 6559)
+++ trunk/commands/PLearnCommands/VMatCommand.cc	2007-01-08 18:07:01 UTC (rev 6560)
@@ -136,6 +136,15 @@
         for(int k=0; k<bbox.length(); k++)
             cout << bbox[k].first << " : " << bbox[k].second << endl;
     }
+    else if (command == "view")
+    {
+        // The 'view' command has been moved to VMatViewCommand (to avoid
+        // a forced dependency on the curses library).
+        vector<string> new_args(args.size() - 1);
+        for (size_t i = 1; i < args.size(); i++)
+            new_args[i - 1] = args[i];
+        PLearnCommandRegistry::run("vmat_view", new_args);
+    }
     else
     {
         // Dirty hack to plug into old vmatmain code



From chrish at mail.berlios.de  Wed Jan 10 18:06:14 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 10 Jan 2007 18:06:14 +0100
Subject: [Plearn-commits] r6561 - trunk/plearn_learners/classifiers
Message-ID: <200701101706.l0AH6Exx021941@sheep.berlios.de>

Author: chrish
Date: 2007-01-10 18:06:13 +0100 (Wed, 10 Jan 2007)
New Revision: 6561

Modified:
   trunk/plearn_learners/classifiers/FeatureSetNaiveBayesClassifier.cc
   trunk/plearn_learners/classifiers/FeatureSetNaiveBayesClassifier.h
Log:
Remove executable bit from FeatureSetNaiveBayesClassifier files.


Property changes on: trunk/plearn_learners/classifiers/FeatureSetNaiveBayesClassifier.cc
___________________________________________________________________
Name: svn:executable
   - *


Property changes on: trunk/plearn_learners/classifiers/FeatureSetNaiveBayesClassifier.h
___________________________________________________________________
Name: svn:executable
   - *



From manzagop at mail.berlios.de  Wed Jan 10 20:55:11 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Wed, 10 Jan 2007 20:55:11 +0100
Subject: [Plearn-commits] r6562 - trunk/plearn_learners/online
Message-ID: <200701101955.l0AJtB3u011185@sheep.berlios.de>

Author: manzagop
Date: 2007-01-10 20:55:11 +0100 (Wed, 10 Jan 2007)
New Revision: 6562

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
   trunk/plearn_learners/online/DeepBeliefNet.h
Log:
Corrected a bug: during the fine tuning step, when there was both a 
final cost and a classification cost, the gradient from the final cost
was not being propagated to the layers[n-2] and lower: it was overwritten
by the classification module's gradient.



Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-01-10 17:06:13 UTC (rev 6561)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-01-10 19:55:11 UTC (rev 6562)
@@ -835,6 +835,9 @@
             expectation_gradients[ n_layers-2 ],
             activation_gradients[ n_layers-1 ] );
     }
+    else  {
+        expectation_gradients[ n_layers-2 ]->clear();
+    }
 
     if( use_classification_cost )
     {
@@ -859,8 +862,10 @@
 
         classification_module->bpropUpdate( layers[ n_layers-2 ]->expectation,
                                             class_output,
-                                            expectation_gradients[n_layers-2],
+                                            class_input_gradient,
                                             class_gradient );
+
+        expectation_gradients[n_layers-2] += class_input_gradient;
     }
 
     for( int i=n_layers-2 ; i>0 ; i-- )

Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-01-10 17:06:13 UTC (rev 6561)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-01-10 19:55:11 UTC (rev 6562)
@@ -247,6 +247,8 @@
 
     mutable Vec class_gradient;
 
+    mutable Vec class_input_gradient;
+
     //! Stores the gradient of the cost at the input of final_cost
     mutable Vec final_cost_gradient;
 



From chrish at mail.berlios.de  Thu Jan 11 19:23:55 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 11 Jan 2007 19:23:55 +0100
Subject: [Plearn-commits] r6563 - trunk/plearn/vmat
Message-ID: <200701111823.l0BINtvv016661@sheep.berlios.de>

Author: chrish
Date: 2007-01-11 19:23:54 +0100 (Thu, 11 Jan 2007)
New Revision: 6563

Modified:
   trunk/plearn/vmat/ProcessingVMatrix.cc
   trunk/plearn/vmat/VMatLanguage.cc
Log:
* Document valid range for VPL indices when using %<number> notation.
* Supply some actual documentation for ProcessingVMatrix.


Modified: trunk/plearn/vmat/ProcessingVMatrix.cc
===================================================================
--- trunk/plearn/vmat/ProcessingVMatrix.cc	2007-01-10 19:55:11 UTC (rev 6562)
+++ trunk/plearn/vmat/ProcessingVMatrix.cc	2007-01-11 18:23:54 UTC (rev 6563)
@@ -49,18 +49,29 @@
 
 ProcessingVMatrix::ProcessingVMatrix()
     :inherited()
-    /* ### Initialise all fields to their default value */
 {
-    // ...
-
-    // ### You may or may not want to call build_() to finish building the object
-    // build_();
 }
 
 PLEARN_IMPLEMENT_OBJECT(
     ProcessingVMatrix,
     "VMatrix whose rows are processed using a VPL script",
-    "See class VMatLanguage for help on VPL syntax."
+    "This VMatrix class processes each for of a source VMatrix using VPL scripts.\n"
+    "The result of this VMatrix can be defined in one of two ways. The first way is to\n"
+    "use a single program (in option 'prg') to process the whole row\n"
+    "and set the inputsize, targetsize, weightsize and extrasize variables to defines\n"
+    "which part of the output of the VPL program is treated as input, target, etc.\n"
+    "of the VMatrix \n"
+    "\n"
+    "The second way is to have separate programs for each of the input, target, etc.\n"
+    "parts of the VMatrix (using options 'input_prg', 'target_prg', 'weight_prg' and\n"
+    "extra_prg), and then the respective inputsize, etc. will be inferred automatically\n"
+    "from the length of the output of each respective VPL program.\n"
+    "\n"
+    "Note that in both cases, an empty program will produce a vector of length 0 as output\n"
+    "If you want for example the output target vector of this VMatrix to be a copy of its\n"
+    "input target vector, this must be done explicitly using a short VPL program.\n"
+    "\n"
+    "See class VMatLanguage for a description of the VPL syntax."
     );
 
 void ProcessingVMatrix::getNewRow(int i, const Vec& v) const

Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-01-10 19:55:11 UTC (rev 6562)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-01-11 18:23:54 UTC (rev 6563)
@@ -81,7 +81,8 @@
                         "fields myfield1 up to myfield10.\n"
                         "\n"
                         "There are two notations to refer to a field value: the @ symbol followed\n"
-                        "by the fieldname, or % followed by the field number.\n"
+                        "by the fieldname, or % followed by the field number (with valid indices.\n"
+                        "going from 0 to num_fields-1).\n"
                         "\n"
                         "To batch-copy fields, use the following syntax: [field1:fieldn] (fields\n"
                         "can be in @ or % notation, with the keyword 'END' denoting the last field).\n"



From chrish at mail.berlios.de  Thu Jan 11 20:17:43 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 11 Jan 2007 20:17:43 +0100
Subject: [Plearn-commits] r6564 - trunk/plearn/vmat
Message-ID: <200701111917.l0BJHhm5024226@sheep.berlios.de>

Author: chrish
Date: 2007-01-11 20:17:42 +0100 (Thu, 11 Jan 2007)
New Revision: 6564

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
Say that VPL [%0:%10] for field copies does a copy up to and including 10.


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-01-11 18:23:54 UTC (rev 6563)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-01-11 19:17:42 UTC (rev 6564)
@@ -84,7 +84,8 @@
                         "by the fieldname, or % followed by the field number (with valid indices.\n"
                         "going from 0 to num_fields-1).\n"
                         "\n"
-                        "To batch-copy fields, use the following syntax: [field1:fieldn] (fields\n"
+                        "To batch-copy fields, use the following syntax: [field1:fieldn], which copies\n"
+                        "all the fields from field1 up to and including fieldn (fields\n"
                         "can be in @ or % notation, with the keyword 'END' denoting the last field).\n"
                         "The fields can also be transformed with a VPL program using the syntax:\n"
                         "[field1:fieldn:vpl_code], where vpl_code can be any VPL code, for example\n"



From chapados at mail.berlios.de  Thu Jan 11 21:01:57 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 11 Jan 2007 21:01:57 +0100
Subject: [Plearn-commits] r6565 - trunk/plearn/ker
Message-ID: <200701112001.l0BK1vlh031474@sheep.berlios.de>

Author: chapados
Date: 2007-01-11 21:01:57 +0100 (Thu, 11 Jan 2007)
New Revision: 6565

Modified:
   trunk/plearn/ker/SummationKernel.cc
Log:
Properly set is_symmetric as a function of subkernels

Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-01-11 19:17:42 UTC (rev 6564)
+++ trunk/plearn/ker/SummationKernel.cc	2007-01-11 20:01:57 UTC (rev 6565)
@@ -106,9 +106,13 @@
         m_input_buf2[i].resize(M);
     }
 
-    for (int i=0, n=m_terms.size() ; i<n ; ++i)
+    // Kernel is symmetric only if all terms are
+    is_symmetric = true;
+    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
         if (! m_terms[i])
             PLERROR("SummationKernel::build_: kernel for term[%d] is not specified",i);
+        is_symmetric = is_symmetric && m_terms[i]->is_symmetric
+    }
 
     if (m_input_indexes.size() > 0 && m_terms.size() != m_input_indexes.size())
         PLERROR("SummationKernel::build_: if 'input_indexes' is specified "



From chapados at mail.berlios.de  Thu Jan 11 21:02:35 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 11 Jan 2007 21:02:35 +0100
Subject: [Plearn-commits] r6566 - in trunk/plearn_learners: generic
	regressors
Message-ID: <200701112002.l0BK2Z5E031577@sheep.berlios.de>

Author: chapados
Date: 2007-01-11 21:02:34 +0100 (Thu, 11 Jan 2007)
New Revision: 6566

Modified:
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/generic/PLearner.h
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.h
Log:
Added function computeOutputCovMat (remote-callable as well)

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-01-11 20:01:57 UTC (rev 6565)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-01-11 20:02:34 UTC (rev 6566)
@@ -46,6 +46,7 @@
 #include <plearn/base/stringutils.h>
 #include <plearn/io/fileutils.h>
 #include <plearn/io/pl_log.h>
+#include <plearn/math/pl_erf.h>
 #include <plearn/vmat/FileVMatrix.h>
 #include <plearn/misc/PLearnService.h>
 #include <plearn/misc/RemotePLearnServer.h>
@@ -356,6 +357,32 @@
                  "know how to compute them.")));
 
     declareMethod(
+        rmm, "computeOutputCovMat", &PLearner::remote_computeOutputCovMat,
+        (BodyDoc("Version of computeOutput that is capable of returning an output matrix\n"
+                 "given an input matrix (set of output vectors), as well as the complete\n"
+                 "covariance matrix between the outputs.\n"
+                 "\n"
+                 "A separate covariance matrix is returned for each output dimension, but\n"
+                 "these matrices are allowed to share the same storage.  This would be\n"
+                 "the case in situations where the output covariance really depends only\n"
+                 "on the location of the training inputs, as in, e.g.,\n"
+                 "GaussianProcessRegressor.\n"
+                 "\n"
+                 "The default implementation is to repeatedly call computeOutput,\n"
+                 "followed by computeConfidenceFromOutput (sampled with probability\n"
+                 "Erf[1/(2*Sqrt(2))], to extract 1*stddev given by subtraction of the two\n"
+                 "intervals, then squaring the stddev to obtain the variance), thereby\n"
+                 "filling a diagonal output covariance matrix.  If\n"
+                 "computeConfidenceFromOutput returns 'false' (confidence intervals not\n"
+                 "supported), the returned covariance matrix is filled with\n"
+                 "MISSING_VALUE.\n"),
+         ArgDoc ("inputs", "Matrix containing the set of test points"),
+         RetDoc ("Two quantities are returned:\n"
+                 "- The matrix containing the expected output (as rows) for each input row.\n"
+                 "- A vector of covariance matrices between the outputs (one covariance\n"
+                 "  matrix per output dimension).\n")));
+    
+    declareMethod(
         rmm, "batchComputeOutputAndConfidencePMat",
         &PLearner::remote_batchComputeOutputAndConfidence,
         (BodyDoc("Repeatedly calls computeOutput and computeConfidenceFromOutput with the\n"
@@ -553,6 +580,52 @@
     return false;
 }
 
+void PLearner::computeOutputCovMat(const Mat& inputs, Mat& outputs,
+                                   TVec<Mat>& covariance_matrices) const
+{
+    PLASSERT( inputs.width() == inputsize() && outputsize() > 0 );
+    const int N = inputs.length();
+    const int M = outputsize();
+    outputs.resize(N, M);
+    covariance_matrices.resize(M);
+
+    bool has_confidence  = true;
+    bool init_covariance = 0;
+    Vec cur_input, cur_output;
+    TVec< pair<real,real> > intervals;
+    for (int i=0 ; i<N ; ++i) {
+        cur_input  = inputs(i);
+        cur_output = outputs(i);
+        computeOutput(cur_input, cur_output);
+        if (has_confidence) {
+            static const real probability = pl_erf(1. / (2*sqrt(2)));
+            has_confidence = computeConfidenceFromOutput(cur_input, cur_output,
+                                                         probability, intervals);
+            if (has_confidence) {
+                // Create the covariance matrices only once; filled with zeros
+                if (! init_covariance) {
+                    for (int j=0 ; j<M ; ++j)
+                        covariance_matrices[j] = Mat(N, N, 0.0);
+                    init_covariance = true;
+                }
+                
+                // Compute the variance for each output j, and set it on
+                // element i,i of the j-th covariance matrix
+                for (int j=0 ; j<M ; ++j) {
+                    float stddev = intervals[j].second - intervals[j].first;
+                    float var = stddev*stddev;
+                    covariance_matrices[j](i,i) = var;
+                }
+            }
+        }
+    }
+
+    // If confidence intervals are not supported, fill the covariance matrices
+    // with missing values
+    for (int j=0 ; j<M ; ++j)
+        covariance_matrices[j] = Mat(N, N, MISSING_VALUE);
+}
+
 void PLearner::batchComputeOutputAndConfidence(VMat inputs, real probability, VMat outputs_and_confidence) const
 {
     Vec input(inputsize());
@@ -901,6 +974,16 @@
         return TVec< pair<real,real> >();
 }
 
+//! Version of computeOutputCovMat that's called by RMI
+tuple<Mat, TVec<Mat> >
+PLearner::remote_computeOutputCovMat(const Mat& inputs) const
+{
+    Mat outputs;
+    TVec<Mat> covmat;
+    computeOutputCovMat(inputs, outputs, covmat);
+    return make_tuple(outputs, covmat);
+}
+
 //! Version of batchComputeOutputAndConfidence that's called by RMI
 void PLearner::remote_batchComputeOutputAndConfidence(VMat inputs, real probability,
                                                       string pmat_fname) const

Modified: trunk/plearn_learners/generic/PLearner.h
===================================================================
--- trunk/plearn_learners/generic/PLearner.h	2007-01-11 20:01:57 UTC (rev 6565)
+++ trunk/plearn_learners/generic/PLearner.h	2007-01-11 20:02:34 UTC (rev 6566)
@@ -464,6 +464,29 @@
                                      TVec< pair<real,real> >& intervals) const;
 
     /**
+     *  Version of computeOutput that is capable of returning an output matrix
+     *  given an input matrix (set of output vectors), as well as the complete
+     *  covariance matrix between the outputs
+     *
+     *  A separate covariance matrix is returned for each output dimension, but
+     *  these matrices are allowed to share the same storage.  This would be
+     *  the case in situations where the output covariance really depends only
+     *  on the location of the training inputs, as in, e.g.,
+     *  GaussianProcessRegressor.
+     *
+     *  The default implementation is to repeatedly call computeOutput,
+     *  followed by computeConfidenceFromOutput (sampled with probability
+     *  Erf[1/(2*Sqrt(2))], to extract 1*stddev given by subtraction of the two
+     *  intervals, then squaring the stddev to obtain the variance), thereby
+     *  filling a diagonal output covariance matrix.  If
+     *  computeConfidenceFromOutput returns 'false' (confidence intervals not
+     *  supported), the returned covariance matrix is filled with
+     *  MISSING_VALUE.
+     */
+    virtual void computeOutputCovMat(const Mat& inputs, Mat& outputs,
+                                     TVec<Mat>& covariance_matrices) const;
+    
+    /**
      *  Repeatedly calls computeOutput and computeConfidenceFromOutput with the
      *  rows of inputs.  Writes outputs_and_confidence rows (as a series of
      *  triples (output, low, high), one for each output)
@@ -598,6 +621,7 @@
     TVec< pair<real,real> > remote_computeConfidenceFromOutput(const Vec& input,
                                                                const Vec& output,
                                                                real probability) const;
+    tuple<Mat, TVec<Mat> > remote_computeOutputCovMat(const Mat& inputs) const;
     void remote_batchComputeOutputAndConfidence(VMat inputs, real probability,
                                                 string pmat_fname) const;
     

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-11 20:01:57 UTC (rev 6565)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-11 20:02:34 UTC (rev 6566)
@@ -106,7 +106,8 @@
 GaussianProcessRegressor::GaussianProcessRegressor() 
     : m_weight_decay(0.0),
       m_include_bias(true),
-      m_compute_confidence(false)
+      m_compute_confidence(false),
+      m_confidence_epsilon(1e-8)
 { }
 
 
@@ -139,9 +140,17 @@
         OptionBase::buildoption,
         "Whether to perform the additional train-time computations required\n"
         "to compute confidence intervals.  This includes computing a separate\n"
-        "inverse of the Gram matrix.\n");
+        "inverse of the Gram matrix.  Specification of this option is necessary\n"
+        "for calling both computeConfidenceFromOutput and computeOutputCovMat.\n");
 
     declareOption(
+        ol, "confidence_epsilon", &GaussianProcessRegressor::m_confidence_epsilon,
+        OptionBase::buildoption,
+        "Small regularization to be added post-hoc to the computed output\n"
+        "covariance matrix and confidence intervals; this is mostly used as a\n"
+        "disaster prevention device, to avoid negative predictive variance\n");
+    
+    declareOption(
         ol, "hyperparameters", &GaussianProcessRegressor::m_hyperparameters,
         OptionBase::buildoption,
         "List of hyperparameters to optimize.  They must be specified in the\n"
@@ -209,6 +218,10 @@
     if (! m_kernel)
         PLERROR("GaussianProcessRegressor::build_: 'kernel' option must be specified");
 
+    if (! m_kernel->is_symmetric)
+        PLERROR("GaussianProcessRegressor::build_: the kernel (%s) must be symmetric",
+                m_kernel->classname().c_str());
+    
     // If we are reloading the model, set the training inputs into the kernel
     if (m_training_inputs)
         m_kernel->setDataForKernelMatrix(m_training_inputs);
@@ -220,6 +233,9 @@
         PLERROR("GaussianProcessRegressor::build_: 'hyperparameters' are specified "
                 "but no 'optimizer'; an optimizer is required in order to carry out "
                 "hyperparameter optimization");
+
+    if (m_confidence_epsilon < 0)
+        PLERROR("GaussianProcessRegressor::build_: 'confidence_epsilon' must be non-negative");
 }
 
 // ### Nothing to add here, simply calls build_
@@ -234,16 +250,19 @@
 {
     inherited::makeDeepCopyFromShallowCopy(copies);
 
-    deepCopyField(m_kernel,               copies);
-    deepCopyField(m_hyperparameters,      copies);
-    deepCopyField(m_optimizer,            copies);
-    deepCopyField(m_alpha,                copies);
-    deepCopyField(m_gram_inverse,         copies);
-    deepCopyField(m_target_mean,          copies);
-    deepCopyField(m_training_inputs,      copies);
-    deepCopyField(m_kernel_evaluations,   copies);
-    deepCopyField(m_gram_inverse_product, copies);
-    deepCopyField(m_intervals,            copies);
+    deepCopyField(m_kernel,                     copies);
+    deepCopyField(m_hyperparameters,            copies);
+    deepCopyField(m_optimizer,                  copies);
+    deepCopyField(m_alpha,                      copies);
+    deepCopyField(m_gram_inverse,               copies);
+    deepCopyField(m_target_mean,                copies);
+    deepCopyField(m_training_inputs,            copies);
+    deepCopyField(m_kernel_evaluations,         copies);
+    deepCopyField(m_gram_inverse_product,       copies);
+    deepCopyField(m_intervals,                  copies);
+    deepCopyField(m_gram_traintest_inputs,      copies);
+    deepCopyField(m_gram_inv_traintest_product, copies);
+    deepCopyField(m_sigma_reductor,             copies);
 }
 
 
@@ -346,15 +365,23 @@
 void GaussianProcessRegressor::computeOutput(const Vec& input, Vec& output) const
 {
     PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
-    PLASSERT( output.size() == m_alpha.width() );
-    PLASSERT( input.size() == m_training_inputs.width() );
+    PLASSERT( m_alpha.width()  == output.size() );
+    PLASSERT( m_alpha.length() == m_training_inputs.length() );
+    PLASSERT( input.size()     == m_training_inputs.width()  );
 
     m_kernel_evaluations.resize(m_alpha.length());
-    m_kernel->evaluate_all_x_i(input, m_kernel_evaluations);
+    computeOutputAux(input, output, m_kernel_evaluations);
+}
 
+
+void GaussianProcessRegressor::computeOutputAux(
+    const Vec& input, Vec& output, Vec& kernel_evaluations) const
+{
+    m_kernel->evaluate_all_x_i(input, kernel_evaluations);
+
     // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
     product(Mat(1, output.size(), output),
-            Mat(1, m_kernel_evaluations.size(), m_kernel_evaluations),
+            Mat(1, kernel_evaluations.size(), kernel_evaluations),
             m_alpha);
 
     if (m_include_bias)
@@ -375,7 +402,7 @@
     // the confidence bounds.  If impossible, simply set missing-value for the
     // NLL cost.
     if (m_compute_confidence) {
-        static const float PROBABILITY = 0.3829;    // 0.5 stddev
+        static const float PROBABILITY = pl_erf(1. / (2*sqrt(2)));  // 0.5 stddev
         bool confavail = computeConfidenceFromOutput(input, output, PROBABILITY,
                                                      m_intervals);
         assert( confavail && m_intervals.size() == output.size() &&
@@ -418,7 +445,7 @@
     m_gram_inverse_product.resize(m_kernel_evaluations.size());
     product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
     real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(0., base_sigma_sq - sigma_reductor));
+    real sigma = sqrt(max(0., base_sigma_sq - sigma_reductor + m_confidence_epsilon));
 
     // two-tailed
     const real multiplier = gauss_01_quantile((1+probability)/2);
@@ -431,6 +458,80 @@
 }
 
 
+//#####  computeOutputCovMat  #################################################
+
+void GaussianProcessRegressor::computeOutputCovMat(
+    const Mat& inputs, Mat& outputs, TVec<Mat>& covariance_matrices) const
+{
+    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
+    PLASSERT( m_alpha.width()  == outputsize() );
+    PLASSERT( m_alpha.length() == m_training_inputs.length() );
+    PLASSERT( inputs.width()   == m_training_inputs.width()  );
+    PLASSERT( inputs.width()   == inputsize() );
+    const int N = inputs.length();
+    const int M = outputsize();
+    const int T = m_training_inputs.length();
+    outputs.resize(N, M);
+    covariance_matrices.resize(M);
+
+    // Preallocate space for the covariance matrix, and since all outputs share
+    // the same matrix, copy it into the remaining elements of
+    // covariance_matrices
+    Mat& covmat = covariance_matrices[0];
+    covmat.resize(N, N);
+    for (int j=1 ; j<M ; ++j)
+        covariance_matrices[j] = covmat;
+
+    // Start by computing the matrix of kernel evaluations between the train
+    // and test outputs, and compute the output
+    m_gram_traintest_inputs.resize(N, T);
+    for (int i=0 ; i<N ; ++i) {
+        Vec cur_traintest_kereval = m_gram_traintest_inputs(i);
+        Vec cur_output = outputs(i);
+        computeOutputAux(inputs(i), cur_output, cur_traintest_kereval);
+    }
+
+    // Next compute the kernel evaluations between the test inputs; more or
+    // less lifted from Kernel.cc ==> must see with Olivier how to better
+    // factor this code
+    Mat& K = covmat;
+    K.resize(N,N);
+    const int mod = K.mod();
+    real Kij;
+    real* Ki;
+    real* Kji;
+    for (int i=0 ; i<N ; ++i) {
+        Ki  = K[i];
+        Kji = &K[0][i];
+        const Vec& cur_input_i = inputs(i);
+        for (int j=0 ; j<=i ; ++j, Kji += mod) {
+            Kij = m_kernel->evaluate(cur_input_i, inputs(j));
+            *Ki++ = Kij;
+            if (j<i)
+                *Kji = Kij;    // Assume symmetry, checked at build
+        }
+    }
+
+    // The predictive covariance matrix is (c.f. Rasmussen and Williams):
+    //
+    //    cov(f*) = K(X*,X*) - K(X*,X) [K(X,X) + sigma*I]^-1 K(X,X*)
+    //
+    // where X are the training inputs, and X* are the test inputs.
+    m_gram_inv_traintest_product.resize(T,N);
+    m_sigma_reductor.resize(N,N);
+    productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
+                     m_gram_traintest_inputs);
+    product(m_sigma_reductor, m_gram_traintest_inputs,
+            m_gram_inv_traintest_product);
+    covmat -= m_sigma_reductor;
+
+    // As a preventive measure, never output negative variance, even though
+    // this does not garantee the non-negative-definiteness of the matrix
+    for (int i=0 ; i<N ; ++i)
+        covmat(i,i) = max(0.0, covmat(i,i) + m_confidence_epsilon);
+}
+
+
 //#####  get*CostNames  #######################################################
 
 TVec<string> GaussianProcessRegressor::getTestCostNames() const

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.h
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-01-11 20:01:57 UTC (rev 6565)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.h	2007-01-11 20:02:34 UTC (rev 6566)
@@ -129,11 +129,19 @@
     /**
      *  Whether to perform the additional train-time computations required
      *  to compute confidence intervals.  This includes computing a separate
-     *  inverse of the Gram matrix.
+     *  inverse of the Gram matrix.  Specification of this option is necessary
+     *  for calling both computeConfidenceFromOutput and computeOutputCovMat.
      */
     bool m_compute_confidence;
 
     /**
+     *  Small regularization to be added post-hoc to the computed output
+     *  covariance matrix and confidence intervals; this is mostly used as a
+     *  disaster prevention device, to avoid negative predictive variance
+     */
+    real m_confidence_epsilon;
+    
+    /**
      *  List of hyperparameters to optimize.  They must be specified in the
      *  form "option-name":initial-value, where 'option-name' is the name of an
      *  option to set within the Kernel object (the array-index form
@@ -204,6 +212,10 @@
                                      real probability,
                                      TVec< pair<real,real> >& intervals) const;
 
+    /// Compute the posterior mean and covariance matrix of a set of inputs
+    virtual void computeOutputCovMat(const Mat& inputs, Mat& outputs,
+                                     TVec<Mat>& covariance_matrices) const;
+    
     /// Returns the names of the costs computed by computeCostsFromOutputs (and
     /// thus the test method). 
     virtual TVec<std::string> getTestCostNames() const;
@@ -228,6 +240,12 @@
     /// Declares the class options.
     static void declareOptions(OptionList& ol);
 
+    /// Utility internal function for computeOutput, which accepts the
+    /// destination for kernel evaluations in argument, and performs no error
+    /// checking nor vector resizes
+    void computeOutputAux(const Vec& input, Vec& output,
+                          Vec& kernel_evaluations) const;
+    
     /// Optimize the hyperparameters if any.  Return a Variable on which
     /// train() carries out a final fprop for obtaining the final trained
     /// learner parameters.
@@ -273,6 +291,16 @@
     //! Buffer to hold confidence intervals when computing costs from outputs
     mutable TVec< pair<real,real> > m_intervals;
 
+    //! Buffer to hold the Gram matrix of train inputs with test inputs.
+    //! Element i,j contains K(test(i), train(j)).
+    mutable Mat m_gram_traintest_inputs;
+
+    //! Buffer to hold the product of the gram inverse with gram_traintest_inputs
+    mutable Mat m_gram_inv_traintest_product;
+
+    //! Buffer to hold the sigma reductor for m_gram_inverse_product
+    mutable Mat m_sigma_reductor;
+    
 private: 
     /// This does the actual building. 
     void build_();



From chapados at mail.berlios.de  Thu Jan 11 21:03:21 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 11 Jan 2007 21:03:21 +0100
Subject: [Plearn-commits] r6567 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor
Message-ID: <200701112003.l0BK3LFX031711@sheep.berlios.de>

Author: chapados
Date: 2007-01-11 21:03:20 +0100 (Thu, 11 Jan 2007)
New Revision: 6567

Added:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
Log:
Beginning of test-case for GaussianProcessRegressor

Added: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR	2007-01-11 20:02:34 UTC (rev 6566)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/INPUTS_GPR	2007-01-11 20:03:20 UTC (rev 6567)
@@ -0,0 +1,8 @@
+!L 1 learner_hyperopt.plearn
+!M 1 setTrainingSet 2 MemoryVMatrix(data = 5 2 [5 10 6 11 5.5 11 10 15 20 3], inputsize=1, targetsize=1, weightsize=0) 0
+!M 1 train 0
+!M 1 getObject 0
+!M 1 computeOutput 1 [ 10 ]
+!M 1 computeOutput 1 [ 12 ]
+!M 1 computeOutputCovMat 1 4 1 [8 9 10 12]
+!Q

Added: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2007-01-11 20:02:34 UTC (rev 6566)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/learner_hyperopt.plearn	2007-01-11 20:03:20 UTC (rev 6567)
@@ -0,0 +1,20 @@
+GaussianProcessRegressor(
+    kernel                  = RationalQuadraticARDKernel(log_signal_sigma = 0.0,
+                                                         log_noise_sigma  = 0.0,
+                                                         log_alpha        = 0.0,
+                                                         log_global_sigma = 0.0,
+                                                         log_input_sigma  = [ 0.0 ]),
+    weight_decay            = 0,
+    include_bias            = 1,
+    compute_confidence      = 1,
+    ARD_hyperprefix_initval = ("log_input_sigma", 0.0),
+    hyperparameters         = [ ("log_signal_sigma", 0.0) ,
+                                ("log_noise_sigma",  0.0) ,
+                                ("log_alpha",        0.0) ],
+    optimizer               = ConjGradientOptimizer(nstages     = 1,
+                                                    sigma       = 0.1,
+                                                    rho         = 0.05,
+                                                    slope_ratio = 10,
+                                                    verbosity   = 1),
+    nstages                 = 10
+    )

Added: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-01-11 20:02:34 UTC (rev 6566)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-01-11 20:03:20 UTC (rev 6567)
@@ -0,0 +1,108 @@
+"""Pytest config file.
+
+Test is a class regrouping the elements that define a test for PyTest.
+    
+    For each Test instance you declare in a config file, a test will be ran
+    by PyTest.
+    
+      @ivar(name):
+    The name of the Test must uniquely determine the
+    test. Among others, it will be used to identify the test's results
+    (.PyTest/name/*_results/) and to report test informations.
+      @type(name):
+    String
+    
+      @ivar(description):
+    The description must provide other users an
+    insight of what exactly is the Test testing. You are encouraged
+    to used triple quoted strings for indented multi-lines
+    descriptions.
+      @type(description):
+    String
+    
+      @ivar(category):
+    The category to which this test belongs. By default, a
+    test is considered a 'General' test.
+    
+    It is not desirable to let an extensive and lengthy test as 'General',
+    while one shall refrain abusive use of categories since it is likely
+    that only 'General' tests will be ran before most commits...
+    
+      @type(category):
+    string
+    
+      @ivar(program):
+    The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner:
+    
+    1) Look for a local program named PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named 
+PRGNAME
+    3) Call 'which PRGNAME'
+    4) Fail
+    
+    Compilable program should provide the keyword argument 'compiler'
+    mapping to a string interpreted as the compiler name (e.g.
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
+    Program
+    
+      @ivar(arguments):
+    The command line arguments to be passed to the program
+    for the test to proceed.
+      @type(arguments):
+    String
+    
+      @ivar(resources):
+    A list of resources that are used by your program
+    either in the command line or directly in the code (plearn or pyplearn
+    files, databases, ...). The elements of the list must be string
+    representations of the path, absolute or relative, to the resource.
+      @type(resources):
+    List of Strings
+    
+      @ivar(precision):
+    The precision (absolute and relative) used when comparing
+    floating numbers in the test output (default = 1e-6)
+      @type(precision):
+    float
+    
+      @ivar(pfileprg):
+    The program to be used for comparing files of psave &
+    vmat formats. It can be either:
+      - "__program__": maps to this test's program if its compilable;
+    maps to 'plearn_tests' otherwise (default);
+      - "__plearn__": always maps to 'plearn_tests' (for when the program
+    under test is not a version of PLearn);
+      - A Program (see 'program' option) instance
+      - None: if you are sure no files are to be compared.
+    
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
+      @ivar(disabled):
+    If true, the test will not be ran.
+      @type(disabled):
+    bool
+    
+"""
+Test(
+    name = "PL_GaussianProcessRegressor_Hyperopt",
+    description = "Exercises the hyperparameter optimization functions of GaussianProcessRegressor",
+    category = "General",
+    program = Program(
+        name = "plearn_tests",
+        compiler = "pymake"
+        ),
+    arguments = "server < INPUTS_GPR | perl -pe 's/(\d\.\d\d\d\d\d\d)\d+/$1/g;'",
+    resources = [ "INPUTS_GPR" ],
+    precision = 1e-06,
+    pfileprg = "__program__",
+    disabled = True
+    )



From chapados at mail.berlios.de  Thu Jan 11 21:48:22 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Thu, 11 Jan 2007 21:48:22 +0100
Subject: [Plearn-commits] r6568 - in
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor:
	. .pytest .pytest/PL_GaussianProcessRegressor_Hyperopt
	.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results
Message-ID: <200701112048.l0BKmMBr004776@sheep.berlios.de>

Author: chapados
Date: 2007-01-11 21:48:22 +0100 (Thu, 11 Jan 2007)
New Revision: 6568

Added:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
Log:
Test results for GaussianProcessRegressor with hyperparameter optimization


Property changes on: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest
___________________________________________________________________
Name: svn:ignore
   + *.compilation_log



Property changes on: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-01-11 20:03:20 UTC (rev 6567)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/.pytest/PL_GaussianProcessRegressor_Hyperopt/expected_results/RUN.log	2007-01-11 20:48:22 UTC (rev 6568)
@@ -0,0 +1,102 @@
+Type !? to get some help.
+!R 0 
+!R 0 
+!R 0 
+!R 1 GaussianProcessRegressor(
+kernel = *1 ->RationalQuadraticARDKernel(
+log_alpha = 1.045475 ;
+log_signal_sigma = 1.788859 ;
+log_global_sigma = 0 ;
+log_input_sigma = 1 [ 1.605025 ] ;
+log_noise_sigma = -1.084960 ;
+is_symmetric = 1 ;
+report_progress = 0 ;
+specify_dataset = *0 ;
+cache_gram_matrix = 0 ;
+data_inputsize = 1 ;
+n_examples = 5  )
+;
+weight_decay = 0 ;
+include_bias = 1 ;
+compute_confidence = 1 ;
+confidence_epsilon = 1.000000e-08 ;
+hyperparameters = 3 [ ("log_signal_sigma" , "0.0" )("log_noise_sigma" , "0.0" )("log_alpha" , "0.0" )] ;
+ARD_hyperprefix_initval = ("log_input_sigma" , "0.0" );
+optimizer = *2 ->ConjGradientOptimizer(
+verbosity = 1 ;
+expected_red = 1 ;
+no_negative_gamma = 1 ;
+sigma = 0.100000 ;
+rho = 0.050000 ;
+constrain_limit = 0.100000 ;
+max_extrapolate = 3 ;
+max_eval_per_line_search = 20 ;
+slope_ratio = 10 ;
+nstages = 1  )
+;
+alpha = 5  1  [ 
+-1.448018 	
+-1.758700 	
+2.977907 	
+0.366193 	
+-0.252937 	
+]
+;
+gram_inverse = 5  5  [ 
+2.375045 	0.318202 	-2.795014 	0.183693 	-0.015760 	
+0.318202 	2.916507 	-3.005313 	-0.300764 	0.023724 	
+-2.795014 	-3.005313 	5.750029 	0.045140 	-0.001069 	
+0.183693 	-0.300764 	0.045140 	0.105748 	-0.013296 	
+-0.015760 	0.023724 	-0.001069 	-0.013296 	0.029873 	
+]
+;
+target_mean = 1 [ 10 ] ;
+training_inputs = *3 ->MemoryVMatrix(
+data = 5  1  [ 
+5 	
+6 	
+5.5 	
+10 	
+20 	
+]
+;
+source = *0 ;
+fieldnames = []
+;
+writable = 0 ;
+length = 5 ;
+width = 1 ;
+inputsize = 1 ;
+targetsize = 0 ;
+weightsize = 0 ;
+extrasize = 0 ;
+metadatadir = ""  )
+;
+seed = 1827 ;
+stage = 10 ;
+n_examples = 5 ;
+inputsize = 1 ;
+targetsize = 1 ;
+weightsize = 0 ;
+forget_when_training_set_changes = 0 ;
+nstages = 10 ;
+report_progress = 1 ;
+verbosity = 1 ;
+nservers = 0 ;
+save_trainingset_prefix = ""  )
+
+!R 1 1 [ 15.000000 ] 
+!R 1 1 [ 14.402740 ] 
+!R 2 4  1  [ 
+13.500058 	
+14.425999 	
+15.000000 	
+14.402740 	
+]
+1 [ 4  4  [ 
+0.495755 	0.296446 	-1.492139e-13 	-0.618033 	
+0.296446 	0.372151 	-1.207922e-13 	-0.472278 	
+-6.394884e-14 	-2.060573e-13 	9.999907e-09 	-1.065814e-13 	
+-0.618033 	-0.472278 	-5.684341e-14 	2.538947 	
+]
+] 

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-01-11 20:03:20 UTC (rev 6567)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-01-11 20:48:22 UTC (rev 6568)
@@ -101,8 +101,8 @@
         compiler = "pymake"
         ),
     arguments = "server < INPUTS_GPR | perl -pe 's/(\d\.\d\d\d\d\d\d)\d+/$1/g;'",
-    resources = [ "INPUTS_GPR" ],
+    resources = [ "INPUTS_GPR", "learner_hyperopt.plearn" ],
     precision = 1e-06,
     pfileprg = "__program__",
-    disabled = True
+    disabled = False
     )



From ducharme at mail.berlios.de  Fri Jan 12 15:54:10 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 12 Jan 2007 15:54:10 +0100
Subject: [Plearn-commits] r6569 - in trunk: plearn/ker
	plearn_learners/generic plearn_learners/regressors
Message-ID: <200701121454.l0CEsA9K004231@sheep.berlios.de>

Author: ducharme
Date: 2007-01-12 15:54:09 +0100 (Fri, 12 Jan 2007)
New Revision: 6569

Modified:
   trunk/plearn/ker/SummationKernel.cc
   trunk/plearn_learners/generic/PLearner.cc
   trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Correction de quelques erreurs de typo.


Modified: trunk/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ trunk/plearn/ker/SummationKernel.cc	2007-01-12 14:54:09 UTC (rev 6569)
@@ -111,7 +111,7 @@
     for (int i=0, n=m_terms.size() ; i<n ; ++i) {
         if (! m_terms[i])
             PLERROR("SummationKernel::build_: kernel for term[%d] is not specified",i);
-        is_symmetric = is_symmetric && m_terms[i]->is_symmetric
+        is_symmetric = is_symmetric && m_terms[i]->is_symmetric;
     }
 
     if (m_input_indexes.size() > 0 && m_terms.size() != m_input_indexes.size())

Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-01-12 14:54:09 UTC (rev 6569)
@@ -598,7 +598,7 @@
         cur_output = outputs(i);
         computeOutput(cur_input, cur_output);
         if (has_confidence) {
-            static const real probability = pl_erf(1. / (2*sqrt(2)));
+            static const real probability = pl_erf(1. / (2*sqrt(2.0)));
             has_confidence = computeConfidenceFromOutput(cur_input, cur_output,
                                                          probability, intervals);
             if (has_confidence) {

Modified: trunk/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-12 14:54:09 UTC (rev 6569)
@@ -402,7 +402,7 @@
     // the confidence bounds.  If impossible, simply set missing-value for the
     // NLL cost.
     if (m_compute_confidence) {
-        static const float PROBABILITY = pl_erf(1. / (2*sqrt(2)));  // 0.5 stddev
+        static const float PROBABILITY = pl_erf(1. / (2*sqrt(2.0)));  // 0.5 stddev
         bool confavail = computeConfidenceFromOutput(input, output, PROBABILITY,
                                                      m_intervals);
         assert( confavail && m_intervals.size() == output.size() &&



From ducharme at mail.berlios.de  Fri Jan 12 16:44:09 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 12 Jan 2007 16:44:09 +0100
Subject: [Plearn-commits] r6570 - in tags: . OPAL-3.0.3/plearn/ker
	OPAL-3.0.3/plearn_learners/generic
	OPAL-3.0.3/plearn_learners/regressors
Message-ID: <200701121544.l0CFi98C010145@sheep.berlios.de>

Author: ducharme
Date: 2007-01-12 16:44:07 +0100 (Fri, 12 Jan 2007)
New Revision: 6570

Added:
   tags/OPAL-3.0.3/
   tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc
   tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc
   tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc
Removed:
   tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc
   tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc
   tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Tag pour release OPAL 3.0.3

Copied: tags/OPAL-3.0.3 (from rev 6568, trunk)

Deleted: tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc	2007-01-12 15:44:07 UTC (rev 6570)
@@ -1,169 +0,0 @@
-// -*- C++ -*-
-
-// SummationKernel.cc
-//
-// Copyright (C) 2007 Nicolas Chapados
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Nicolas Chapados
-
-/*! \file SummationKernel.cc */
-
-
-#include "SummationKernel.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    SummationKernel,
-    "Kernel computing the sum of other kernels",
-    "This kernel computes the summation of several subkernel objects.  It can\n"
-    "also chop up parts of its input vector and send it to each kernel (so that\n"
-    "each kernel can operate on a subset of the variables).\n"
-    );
-
-
-//#####  Constructor  #########################################################
-
-SummationKernel::SummationKernel()
-{ }
-
-
-//#####  declareOptions  ######################################################
-
-void SummationKernel::declareOptions(OptionList& ol)
-{
-    declareOption(
-        ol, "terms", &SummationKernel::m_terms, OptionBase::buildoption,
-        "Individual kernels to add to produce the final result.  The\n"
-        "hyperparameters of kernel i can be accesed under the option names\n"
-        "'terms[i].hyperparam' for, e.g. GaussianProcessRegressor.\n");
-
-    declareOption(
-        ol, "input_indexes", &SummationKernel::m_input_indexes,
-        OptionBase::buildoption,
-        "Optionally, one can specify which of individual input variables should\n"
-        "be routed to each kernel.  The format is as a vector of vectors: for\n"
-        "each kernel in 'terms', one must list the INDEXES in the original input\n"
-        "vector(zero-based) that should be passed to that kernel.  If a list of\n"
-        "indexes is empty for a given kernel, it means that the COMPLETE input\n"
-        "vector should be passed to the kernel.\n");
-    
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-
-//#####  build  ###############################################################
-
-void SummationKernel::build()
-{
-    // ### Nothing to add here, simply calls build_
-    inherited::build();
-    build_();
-}
-
-
-//#####  build_  ##############################################################
-
-void SummationKernel::build_()
-{
-    // Preallocate buffers for kernel evaluation
-    const int N = m_input_indexes.size();
-    m_input_buf1.resize(N);
-    m_input_buf2.resize(N);
-    for (int i=0 ; i<N ; ++i) {
-        const int M = m_input_indexes[i].size();
-        m_input_buf1[i].resize(M);
-        m_input_buf2[i].resize(M);
-    }
-
-    // Kernel is symmetric only if all terms are
-    is_symmetric = true;
-    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
-        if (! m_terms[i])
-            PLERROR("SummationKernel::build_: kernel for term[%d] is not specified",i);
-        is_symmetric = is_symmetric && m_terms[i]->is_symmetric
-    }
-
-    if (m_input_indexes.size() > 0 && m_terms.size() != m_input_indexes.size())
-        PLERROR("SummationKernel::build_: if 'input_indexes' is specified "
-                "it must have the same size (%d) as 'terms'; found %d elements",
-                m_terms.size(), m_input_indexes.size());
-}
-
-
-//#####  evaluate  ############################################################
-
-real SummationKernel::evaluate(const Vec& x1, const Vec& x2) const
-{
-    real kernel_value = 0.0;
-    bool split_inputs = m_input_indexes.size() > 0;
-    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
-        if (split_inputs && m_input_indexes[i].size() > 0) {
-            selectElements(x1, m_input_indexes[i], m_input_buf1[i]);
-            selectElements(x2, m_input_indexes[i], m_input_buf2[i]);
-            kernel_value += m_terms[i]->evaluate(m_input_buf1[i],
-                                                 m_input_buf2[i]);
-        }
-        else
-            kernel_value += m_terms[i]->evaluate(x1,x2);
-    }
-    return kernel_value;
-}
-
-
-//#####  makeDeepCopyFromShallowCopy  #########################################
-
-void SummationKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_terms,          copies);
-    deepCopyField(m_input_indexes,  copies);
-    deepCopyField(m_input_buf1,     copies);
-    deepCopyField(m_input_buf2,     copies);
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3/plearn/ker/SummationKernel.cc (from rev 6569, trunk/plearn/ker/SummationKernel.cc)

Deleted: tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc	2007-01-12 15:44:07 UTC (rev 6570)
@@ -1,1016 +0,0 @@
-// -*- C++ -*-
-
-// PLearner.cc
-//
-// Copyright (C) 1998-2002 Pascal Vincent
-// Copyright (C) 1999-2002 Yoshua Bengio, Nicolas Chapados, Charles Dugas, Rejean Ducharme, Universite de Montreal
-// Copyright (C) 2001,2002 Francis Pieraut, Jean-Sebastien Senecal
-// Copyright (C) 2002 Frederic Morin, Xavier Saint-Mleux, Julien Keable
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-
- 
-
-/* *******************************************************      
- * $Id$
- ******************************************************* */
-
-#include "PLearner.h"
-#include <plearn/base/stringutils.h>
-#include <plearn/io/fileutils.h>
-#include <plearn/io/pl_log.h>
-#include <plearn/math/pl_erf.h>
-#include <plearn/vmat/FileVMatrix.h>
-#include <plearn/misc/PLearnService.h>
-#include <plearn/misc/RemotePLearnServer.h>
-#include <plearn/vmat/PLearnerOutputVMatrix.h>
-
-namespace PLearn {
-using namespace std;
-
-PLearner::PLearner()
-    : n_train_costs_(-1),
-      n_test_costs_(-1),
-      seed_(1827),                           //!< R.I.P. L.v.B.
-      stage(0),
-      nstages(1),
-      report_progress(true),
-      verbosity(1),
-      nservers(0),
-      save_trainingset_prefix(""),
-      inputsize_(-1),
-      targetsize_(-1),
-      weightsize_(-1),
-      n_examples(-1),
-      forget_when_training_set_changes(false)  
-{}
-
-PLEARN_IMPLEMENT_ABSTRACT_OBJECT(
-    PLearner,
-    "The base class for all PLearn learning algorithms",
-    "PLearner provides a base class for all learning algorithms within PLearn.\n"
-    "It presents an abstraction of learning that centers around a \"train-test\"\n"
-    "paradigm:\n"
-    "\n"
-    "- Phase 1: TRAINING.  In this phase, one must first establish an experiment\n"
-    "  directory (usually done by an enclosing PTester) to store any temporary\n"
-    "  files that the learner might seek to create.  Then, one sets a training\n"
-    "  set VMat (also done by the enclosing PTester), which contains the set of\n"
-    "  input-target pairs that the learner should attempt to represent.  Finally\n"
-    "  one calls the train() virtual member function to carry out the actual\n"
-    "  action of training the model.\n"
-    "\n"
-    "- Phase 2: TESTING.  In this phase (to be done after training), one\n"
-    "  repeatedly calls functions from the computeOutput() family to evaluate\n"
-    "  the trained model on new input vectors.\n"
-    "\n"
-    "Note that the PTester class is the usual \"driver\" for a PLearner (and\n"
-    "automatically calls the above functions in the appropriate order), in the\n"
-    "usual scenario wherein one wants to evaluate the generalization performance\n"
-    "on a dataset.\n"
-    );
-
-void PLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(tmp_output, copies);
-    // TODO What's wrong with this?
-    deepCopyField(train_set, copies);
-    deepCopyField(validation_set, copies);
-    deepCopyField(train_stats, copies);
-    deepCopyField(random_gen, copies);
-}
-
-void PLearner::declareOptions(OptionList& ol)
-{
-    declareOption(
-        ol, "expdir", &PLearner::expdir, OptionBase::buildoption | OptionBase::nosave, 
-        "Path of the directory associated with this learner, in which\n"
-        "it should save any file it wishes to create. \n"
-        "The directory will be created if it does not already exist.\n"
-        "If expdir is the empty string (the default), then the learner \n"
-        "should not create *any* file. Note that, anyway, most file creation and \n"
-        "reporting are handled at the level of the PTester class rather than \n"
-        "at the learner's. \n");
-
-    declareOption(
-        ol, "seed", &PLearner::seed_, OptionBase::buildoption, 
-        "The initial seed for the random number generator used in this\n"
-        "learner, for instance for parameter initialization.\n"
-        "If -1 is provided, then a 'random' seed is chosen based on time\n"
-        "of day, ensuring that different experiments run differently.\n"
-        "If 0 is provided, no (re)initialization of the random number\n"
-        "generator is performed.\n"
-        "With a given positive seed, build() and forget() should always\n"
-        "initialize the parameters to the same values.");
-
-    declareOption(
-        ol, "stage", &PLearner::stage, OptionBase::learntoption, 
-        "The current training stage, since last fresh initialization (forget()): \n"
-        "0 means untrained, n often means after n epochs or optimization steps, etc...\n"
-        "The true meaning is learner-dependant."
-        "You should never modify this option directly!"
-        "It is the role of forget() to bring it back to 0,\n"
-        "and the role of train() to bring it up to 'nstages'...");
-
-    declareOption(
-        ol, "n_examples", &PLearner::n_examples, OptionBase::learntoption, 
-        "The number of samples in the training set.\n"
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "inputsize", &PLearner::inputsize_, OptionBase::learntoption, 
-        "The number of input columns in the data sets."
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "targetsize", &PLearner::targetsize_, OptionBase::learntoption, 
-        "The number of target columns in the data sets."
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "weightsize", &PLearner::weightsize_, OptionBase::learntoption, 
-        "The number of cost weight columns in the data sets."
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "forget_when_training_set_changes",
-        &PLearner::forget_when_training_set_changes, OptionBase::buildoption, 
-        "Whether or not to call the forget() method (re-initialize model \n"
-        "as before training) in setTrainingSet when the\n"
-        "training set changes (e.g. of dimension).");
-
-    declareOption(
-        ol, "nstages", &PLearner::nstages, OptionBase::buildoption, 
-        "The stage until which train() should train this learner and return.\n"
-        "The meaning of 'stage' is learner-dependent, but for learners whose \n"
-        "training is incremental (such as involving incremental optimization), \n"
-        "it is typically synonym with the number of 'epochs', i.e. the number \n"
-        "of passages of the optimization process through the whole training set, \n"
-        "since the last fresh initialisation.");
-
-    declareOption(
-        ol, "report_progress", &PLearner::report_progress, OptionBase::buildoption, 
-        "should progress in learning and testing be reported in a ProgressBar.\n");
-
-    declareOption(
-        ol, "verbosity", &PLearner::verbosity, OptionBase::buildoption, 
-        "Level of verbosity. If 0 should not write anything on perr. \n"
-        "If >0 may write some info on the steps performed along the way.\n"
-        "The level of details written should depend on this value.");
-
-    declareOption(
-        ol, "nservers", &PLearner::nservers, OptionBase::buildoption, 
-        "Max number of computation servers to use in parallel with the main process.\n"
-        "If <=0 no parallelization will occur at this level.\n");
-
-    declareOption(
-        ol, "save_trainingset_prefix", &PLearner::save_trainingset_prefix,
-        OptionBase::buildoption,
-        "Whether the training set should be saved upon a call to\n"
-        "setTrainingSet().  The saved file is put in the learner's expdir\n"
-        "(assuming there is one) and has the form \"<prefix>_trainset_XXX.pmat\"\n"
-        "The prefix is what this option specifies.  'XXX' is a unique\n"
-        "serial number that is globally incremented with each saved\n"
-        "setTrainingSet.  This option is useful when manipulating very\n"
-        "complex nested learner structures, and you want to ensure that\n"
-        "the inner learner is getting the correct results.  (Default="",\n"
-        "i.e. don't save anything.)\n");
-  
-    inherited::declareOptions(ol);
-}
-
-void PLearner::declareMethods(RemoteMethodMap& rmm)
-{
-    // Insert a backpointer to remote methods; note that this
-    // different than for declareOptions()
-    rmm.inherited(inherited::_getRemoteMethodMap_());
-
-    declareMethod(
-        rmm, "setTrainingSet", &PLearner::setTrainingSet,
-        (BodyDoc("Declares the training set.  Then calls build() and forget() if\n"
-                 "necessary.\n"),
-         ArgDoc ("training_set", "The training set VMatrix to set; should have\n"
-                 "its inputsize, targetsize and weightsize fields set properly.\n"),
-         ArgDoc ("call_forget", "Whether the forget() function should be called\n"
-                 "upon setting the training set\n")));
-
-    declareMethod(
-        rmm, "setExperimentDirectory", &PLearner::setExperimentDirectory,
-        (BodyDoc("The experiment directory is the directory in which files related to\n"
-                 "this model are to be saved.  If it is an empty string, it is understood\n"
-                 "to mean that the user doesn't want any file created by this learner.\n"),
-         ArgDoc ("expdir", "Experiment directory to set")));
-
-    declareMethod(
-        rmm, "getExperimentDirectory", &PLearner::getExperimentDirectory,
-        (BodyDoc("This returns the currently set experiment directory\n"
-                 "(see setExperimentDirectory)\n"),
-         RetDoc ("Current experiment directory")));
-
-    declareMethod(
-        rmm, "forget", &PLearner::forget,
-        (BodyDoc("(Re-)initializes the PLearner in its fresh state (that state may depend\n"
-                 "on the 'seed' option) and sets 'stage' back to 0 (this is the stage of\n"
-                 "a fresh learner!)\n"
-                 "\n"
-                 "A typical forget() method should do the following:\n"
-                 "\n"
-                 "- call inherited::forget() to initialize the random number generator\n"
-                 "  with the 'seed' option\n"
-                 "\n"
-                 "- initialize the learner's parameters, using this random generator\n"
-                 "\n"
-                 "- stage = 0;\n"
-                 "\n"
-                 "This method is typically called by the build_() method, after it has\n"
-                 "finished setting up the parameters, and if it deemed useful to set or\n"
-                 "reset the learner in its fresh state.  (remember build may be called\n"
-                 "after modifying options that do not necessarily require the learner to\n"
-                 "restart from a fresh state...)  forget is also called by the\n"
-                 "setTrainingSet method, after calling build(), so it will generally be\n"
-                 "called TWICE during setTrainingSet!\n")));
-
-    declareMethod(
-        rmm, "train", &PLearner::train,
-        (BodyDoc("The role of the train method is to bring the learner up to\n"
-                 "stage==nstages, updating the stats with training costs measured on-line\n"
-                 "in the process.\n")));
-
-    declareMethod(
-        rmm, "resetInternalState", &PLearner::resetInternalState,
-        (BodyDoc("If the learner is a stateful one (inherits from StatefulLearner),\n"
-                 "this resets the internal state to its initial value; by default,\n"
-                 "this function does nothing.")));
-
-    declareMethod(
-        rmm, "computeOutput", &PLearner::remote_computeOutput,
-        (BodyDoc("On a trained learner, this computes the output from the input"),
-         ArgDoc ("input", "Input vector (should have width inputsize)"),
-         RetDoc ("Computed output (will have width outputsize)")));
-
-    declareMethod(
-        rmm, "use", &PLearner::remote_use,
-        (BodyDoc("Compute the output of a trained learner on every row of an\n"
-                 "input VMatrix.  The outputs are stored in a .pmat matrix\n"
-                 "under the specified filename."),
-         ArgDoc ("input_vmat", "VMatrix containing the inputs"),
-         ArgDoc ("output_pmat_fname", "Name of the .pmat to store the computed outputs")));
-
-    declareMethod(
-        rmm, "use2", &PLearner::remote_use2,
-        (BodyDoc("Compute the output of a trained learner on every row of an\n"
-                 "input VMatrix.  The outputs are returned as a matrix.\n"),
-         ArgDoc ("input_vmat", "VMatrix containing the inputs"),
-         RetDoc ("Matrix holding the computed outputs")));
-
-    declareMethod(
-        rmm, "computeInputOutputMat", &PLearner::computeInputOutputMat,
-        (BodyDoc("Returns a matrix which is a (horizontal) concatenation\n"
-                 "and the computed outputs.\n"),
-         ArgDoc ("inputs", "VMatrix containing the inputs"),
-         RetDoc ("Matrix holding the inputs+computed_outputs")));
-
-    declareMethod(
-        rmm, "computeInputOutputConfMat", &PLearner::computeInputOutputConfMat,
-        (BodyDoc("Return a Mat that is the contatenation of inputs, outputs, lower\n"
-                 "confidence bound, and upper confidence bound.  If confidence intervals\n"
-                 "cannot be computed for the learner, they are filled with MISSING_VALUE.\n"),
-         ArgDoc ("inputs", "VMatrix containing the inputs"),
-         ArgDoc ("probability", "Level at which the confidence intervals should be computed, "
-                                "e.g. 0.95."),
-         RetDoc ("Matrix holding the inputs+outputs+confidence-low+confidence-high")));
-
-    declareMethod(
-        rmm, "computeOutputAndCosts", &PLearner::remote_computeOutputAndCosts,
-        (BodyDoc("Compute both the output from the input, and the costs associated\n"
-                 "with the desired target.  The computed costs\n"
-                 "are returned in the order given by getTestCostNames()\n"),
-         ArgDoc ("input",  "Input vector (should have width inputsize)"),
-         ArgDoc ("target", "Target vector (for cost computation)"),
-         RetDoc ("- Vec containing output \n"
-                 "- Vec containing cost")));
-
-    declareMethod(
-        rmm, "computeCostsFromOutputs", &PLearner::remote_computeCostsFromOutputs,
-        (BodyDoc("Compute the costs from already-computed output.  The computed costs\n"
-                 "are returned in the order given by getTestCostNames()"),
-         ArgDoc ("input",  "Input vector (should have width inputsize)"),
-         ArgDoc ("output", "Output vector computed by previous call to computeOutput()"),
-         ArgDoc ("target", "Target vector"),
-         RetDoc ("The computed costs vector")));
-
-    declareMethod(
-        rmm, "computeCostsOnly", &PLearner::remote_computeCostsOnly,
-        (BodyDoc("Compute the costs only, without the outputs; for some learners, this\n"
-                 "may be more efficient than calling computeOutputAndCosts() if the\n"
-                 "outputs are not needed.  (The default implementation simply calls\n"
-                 "computeOutputAndCosts() and discards the output.)\n"),
-         ArgDoc ("input",  "Input vector (should have width inputsize)"),
-         ArgDoc ("target", "Target vector"),
-         RetDoc ("The computed costs vector")));
-
-    declareMethod(
-        rmm, "computeConfidenceFromOutput", &PLearner::remote_computeConfidenceFromOutput,
-        (BodyDoc("Compute a confidence intervals for the output, given the input and the\n"
-                 "pre-computed output (resulting from computeOutput or similar).  The\n"
-                 "probability level of the confidence interval must be specified.\n"
-                 "(e.g. 0.95).  Result is stored in a TVec of pairs low:high for each\n"
-                 "output variable (this is a \"box\" interval; it does not account for\n"
-                 "correlations among the output variables).\n"),
-         ArgDoc ("input",       "Input vector (should have width inputsize)"),
-         ArgDoc ("output",      "Output vector computed by previous call to computeOutput()"),
-         ArgDoc ("probability", "Level at which the confidence interval must be computed,\n"
-                                "e.g. 0.95\n"),
-         RetDoc ("Vector of pairs low:high giving, respectively, the lower-bound confidence\n"
-                 "and upper-bound confidence for each dimension of the output vector.  If this\n"
-                 "vector is empty, then confidence intervals could not be computed for the\n"
-                 "given learner.  Note that this is the PLearner default (not to compute\n"
-                 "any confidence intervals), but some learners such as LinearRegressor\n"
-                 "know how to compute them.")));
-
-    declareMethod(
-        rmm, "computeOutputCovMat", &PLearner::remote_computeOutputCovMat,
-        (BodyDoc("Version of computeOutput that is capable of returning an output matrix\n"
-                 "given an input matrix (set of output vectors), as well as the complete\n"
-                 "covariance matrix between the outputs.\n"
-                 "\n"
-                 "A separate covariance matrix is returned for each output dimension, but\n"
-                 "these matrices are allowed to share the same storage.  This would be\n"
-                 "the case in situations where the output covariance really depends only\n"
-                 "on the location of the training inputs, as in, e.g.,\n"
-                 "GaussianProcessRegressor.\n"
-                 "\n"
-                 "The default implementation is to repeatedly call computeOutput,\n"
-                 "followed by computeConfidenceFromOutput (sampled with probability\n"
-                 "Erf[1/(2*Sqrt(2))], to extract 1*stddev given by subtraction of the two\n"
-                 "intervals, then squaring the stddev to obtain the variance), thereby\n"
-                 "filling a diagonal output covariance matrix.  If\n"
-                 "computeConfidenceFromOutput returns 'false' (confidence intervals not\n"
-                 "supported), the returned covariance matrix is filled with\n"
-                 "MISSING_VALUE.\n"),
-         ArgDoc ("inputs", "Matrix containing the set of test points"),
-         RetDoc ("Two quantities are returned:\n"
-                 "- The matrix containing the expected output (as rows) for each input row.\n"
-                 "- A vector of covariance matrices between the outputs (one covariance\n"
-                 "  matrix per output dimension).\n")));
-    
-    declareMethod(
-        rmm, "batchComputeOutputAndConfidencePMat",
-        &PLearner::remote_batchComputeOutputAndConfidence,
-        (BodyDoc("Repeatedly calls computeOutput and computeConfidenceFromOutput with the\n"
-                 "rows of inputs.  Writes outputs_and_confidence rows (as a series of\n"
-                 "triples (output, low, high), one for each output).  The results are\n"
-                 "stored in a .pmat whose filename is passed as argument.\n"),
-         ArgDoc ("input_vmat",  "VMatrix containing the input rows"),
-         ArgDoc ("probability", "Level at which the confidence interval must be computed,\n"
-                                "e.g. 0.95\n"),
-         ArgDoc ("result_pmat_filename", "Filename where to store the results")));
-
-    declareMethod(
-        rmm, "getTestCostNames", &PLearner::getTestCostNames,
-        (BodyDoc("Return the name of the costs computed by computeCostsFromOutputs()\n"
-                 "and computeOutputAndCosts()"),
-         RetDoc ("List of test cost names")));
-
-    declareMethod(
-        rmm, "getTrainCostNames", &PLearner::getTrainCostNames,
-        (BodyDoc("Return the names of the objective costs that the train\n"
-                 "method computes and for which it updates the VecStatsCollector\n"
-                 "train_stats."),
-         RetDoc ("List of train cost names")));
-}
-
-////////////////////////////
-// setExperimentDirectory //
-////////////////////////////
-void PLearner::setExperimentDirectory(const PPath& the_expdir) 
-{ 
-    if(the_expdir=="")
-        expdir = "";
-    else
-    {
-        if(!force_mkdir(the_expdir))
-            PLERROR("In PLearner::setExperimentDirectory Could not create experiment directory %s",
-                    the_expdir.absolute().c_str());
-        expdir = the_expdir / "";
-    }
-}
-
-void PLearner::setTrainingSet(VMat training_set, bool call_forget)
-{ 
-    // YB: je ne suis pas sur qu'il soit necessaire de faire un build si la
-    // LONGUEUR du train_set a change?  les methodes non-parametriques qui
-    // utilisent la longueur devrait faire leur "resize" dans train, pas dans
-    // build.
-    bool training_set_has_changed = !train_set || !(train_set->looksTheSameAs(training_set));
-    train_set = training_set;
-    if (training_set_has_changed)
-    {
-        inputsize_ = train_set->inputsize();
-        targetsize_ = train_set->targetsize();
-        weightsize_ = train_set->weightsize();
-        if (forget_when_training_set_changes)
-            call_forget=true;
-    }
-    n_examples = train_set->length();
-    if (training_set_has_changed || call_forget)
-        build(); // MODIF FAITE PAR YOSHUA: sinon apres un setTrainingSet le build n'est pas complete dans un NNet train_set = training_set;
-    if (call_forget)
-        forget();
-
-    // Save the new training set if desired
-    if (save_trainingset_prefix != "" && expdir != "") {
-        static int trainingset_serial = 1;
-        PPath fname = expdir / (save_trainingset_prefix + "_trainset_" +
-                                tostring(trainingset_serial++) + ".pmat");
-        train_set->savePMAT(fname);
-    }
-}
-
-void PLearner::setValidationSet(VMat validset)
-{ validation_set = validset; }
-
-
-void PLearner::setTrainStatsCollector(PP<VecStatsCollector> statscol)
-{ train_stats = statscol; }
-
-
-int PLearner::inputsize() const
-{ 
-    if (inputsize_<0)
-        PLERROR("Must specify a training set before calling PLearner::inputsize()"); 
-    return inputsize_; 
-}
-
-int PLearner::targetsize() const 
-{ 
-    if(targetsize_ == -1) 
-        PLERROR("In PLearner::targetsize - 'targetsize_' is -1, either no training set has beeen specified or its sizes were not set properly");
-    return targetsize_; 
-}
-
-int PLearner::weightsize() const 
-{ 
-    if(weightsize_ == -1) 
-        PLERROR("In PLearner::weightsize - 'weightsize_' is -1, either no training set has beeen specified or its sizes were not set properly");
-    return weightsize_; 
-}
-
-////////////
-// build_ //
-////////////
-void PLearner::build_()
-{
-    if(expdir!="")
-    {
-        if(!force_mkdir(expdir))
-            PLWARNING("In PLearner Could not create experiment directory %s",expdir.c_str());
-        else
-            expdir = expdir.absolute() / "";
-    }
-    if (random_gen && seed_ != 0)
-        random_gen->manual_seed(seed_);
-}
-
-///////////
-// build //
-///////////
-void PLearner::build()
-{
-    inherited::build();
-    build_();
-}
-
-////////////
-// forget //
-////////////
-void PLearner::forget()
-{
-    if (random_gen && seed_ != 0)
-        random_gen->manual_seed(seed_);
-}
-
-int PLearner::nTestCosts() const 
-{ 
-    if(n_test_costs_<0)
-        n_test_costs_ = getTestCostNames().size(); 
-    return n_test_costs_;
-}
-
-int PLearner::nTrainCosts() const 
-{ 
-    if(n_train_costs_<0)
-        n_train_costs_ = getTrainCostNames().size();
-    return n_train_costs_; 
-}
-
-int PLearner::getTestCostIndex(const string& costname) const
-{
-    TVec<string> costnames = getTestCostNames();
-    for(int i=0; i<costnames.length(); i++)
-        if(costnames[i]==costname)
-            return i;
-    PLERROR("In PLearner::getTestCostIndex, No test cost named %s in this learner.\n"
-            "Available test costs are: %s", costname.c_str(),
-            tostring(costnames).c_str());
-    return -1;
-}
-
-int PLearner::getTrainCostIndex(const string& costname) const
-{
-    TVec<string> costnames = getTrainCostNames();
-    for(int i=0; i<costnames.length(); i++)
-        if(costnames[i]==costname)
-            return i;
-    PLERROR("In PLearner::getTrainCostIndex, No train cost named %s in this learner.\n"
-            "Available train costs are: %s", costname.c_str(), tostring(costnames).c_str());
-    return -1;
-}
-                                
-void PLearner::computeOutputAndCosts(const Vec& input, const Vec& target, 
-                                     Vec& output, Vec& costs) const
-{
-    computeOutput(input, output);
-    computeCostsFromOutputs(input, output, target, costs);
-}
-
-void PLearner::computeCostsOnly(const Vec& input, const Vec& target,  
-                                Vec& costs) const
-{
-    tmp_output.resize(outputsize());
-    computeOutputAndCosts(input, target, tmp_output, costs);
-}
-
-bool PLearner::computeConfidenceFromOutput(
-    const Vec& input, const Vec& output,
-    real probability,
-    TVec< pair<real,real> >& intervals) const
-{
-    // Default version does not know how to compute confidence intervals
-    intervals.resize(output.size());
-    intervals.fill(std::make_pair(MISSING_VALUE,MISSING_VALUE));  
-    return false;
-}
-
-void PLearner::computeOutputCovMat(const Mat& inputs, Mat& outputs,
-                                   TVec<Mat>& covariance_matrices) const
-{
-    PLASSERT( inputs.width() == inputsize() && outputsize() > 0 );
-    const int N = inputs.length();
-    const int M = outputsize();
-    outputs.resize(N, M);
-    covariance_matrices.resize(M);
-
-    bool has_confidence  = true;
-    bool init_covariance = 0;
-    Vec cur_input, cur_output;
-    TVec< pair<real,real> > intervals;
-    for (int i=0 ; i<N ; ++i) {
-        cur_input  = inputs(i);
-        cur_output = outputs(i);
-        computeOutput(cur_input, cur_output);
-        if (has_confidence) {
-            static const real probability = pl_erf(1. / (2*sqrt(2)));
-            has_confidence = computeConfidenceFromOutput(cur_input, cur_output,
-                                                         probability, intervals);
-            if (has_confidence) {
-                // Create the covariance matrices only once; filled with zeros
-                if (! init_covariance) {
-                    for (int j=0 ; j<M ; ++j)
-                        covariance_matrices[j] = Mat(N, N, 0.0);
-                    init_covariance = true;
-                }
-                
-                // Compute the variance for each output j, and set it on
-                // element i,i of the j-th covariance matrix
-                for (int j=0 ; j<M ; ++j) {
-                    float stddev = intervals[j].second - intervals[j].first;
-                    float var = stddev*stddev;
-                    covariance_matrices[j](i,i) = var;
-                }
-            }
-        }
-    }
-
-    // If confidence intervals are not supported, fill the covariance matrices
-    // with missing values
-    for (int j=0 ; j<M ; ++j)
-        covariance_matrices[j] = Mat(N, N, MISSING_VALUE);
-}
-
-void PLearner::batchComputeOutputAndConfidence(VMat inputs, real probability, VMat outputs_and_confidence) const
-{
-    Vec input(inputsize());
-    Vec output(outputsize());
-    int outsize = outputsize();
-    Vec output_and_confidence(3*outsize);
-    TVec< pair<real,real> > intervals;
-    int l = inputs.length();
-    for(int i=0; i<l; i++)
-    {
-        inputs->getRow(i,input);
-        computeOutput(input,output);
-        computeConfidenceFromOutput(input,output,probability,intervals);
-        for(int j=0; j<outsize; j++)
-        {
-            output_and_confidence[3*j] = output[j];
-            output_and_confidence[3*j+1] = intervals[j].first;
-            output_and_confidence[3*j+2] = intervals[j].second;
-        }
-        outputs_and_confidence->putOrAppendRow(i,output_and_confidence);
-    }
-}
-
-/////////
-// use //
-/////////
-void PLearner::use(VMat testset, VMat outputs) const
-{
-    int l = testset.length();
-    int w = testset.width();
-
-    TVec< PP<RemotePLearnServer> > servers;
-    if(nservers>0)
-        servers = PLearnService::instance().reserveServers(nservers);
-
-    if(servers.length()==0) 
-    { // sequential code      
-        Vec input;
-        Vec target;
-        real weight;
-        Vec output(outputsize());
-
-        PP<ProgressBar> pb;
-        if(report_progress)
-            pb = new ProgressBar("Using learner",l);
-
-        for(int i=0; i<l; i++)
-        {
-            testset.getExample(i, input, target, weight);
-            computeOutput(input, output);
-            outputs->putOrAppendRow(i,output);
-            if(pb)
-                pb->update(i);
-        }
-    }
-    else // parallel code
-    {
-        int n = servers.length(); // number of allocated servers
-        DBG_LOG << "PLearner::use parallel code using " << n << " servers" << endl;
-        for(int k=0; k<n; k++)  // send this object with objid 0
-            servers[k]->newObject(0, *this);
-        int chunksize = l/n;
-        if(chunksize*n<l)
-            ++chunksize;
-        if(chunksize*w>1000000) // max 1 Mega elements
-            chunksize = max(1,1000000/w);
-        Mat chunk(chunksize,w);
-        int send_i=0;
-        Mat outmat;
-        int receive_i = 0;
-        while(send_i<l)
-        {
-            for(int k=0; k<n && send_i<l; k++)
-            {
-                int actualchunksize = chunksize;
-                if(send_i+actualchunksize>l)
-                    actualchunksize = l-send_i;
-                chunk.resize(actualchunksize,w);
-                testset->getMat(send_i, 0, chunk);
-                VMat inputs(chunk);
-                inputs->copySizesFrom(testset);
-                DBG_LOG << "PLearner::use calling use2 remote method with chunk starting at " 
-                        << send_i << " of length " << actualchunksize << ":" << inputs << endl;
-                servers[k]->callMethod(0,"use2",inputs);
-                send_i += actualchunksize;
-            }
-            for(int k=0; k<n && receive_i<l; k++)
-            {
-                outmat.resize(0,0);
-                servers[k]->getResults(outmat);
-                for(int ii=0; ii<outmat.length(); ii++)
-                    outputs->putOrAppendRow(receive_i++,outmat(ii));
-            }
-        }
-        if(send_i!=l || receive_i!=l)
-            PLERROR("In PLearn::use parallel execution failed to complete successfully.");
-    }
-}
-
-VMat PLearner::processDataSet(VMat dataset) const
-{
-    // PLearnerOutputVMatrix does exactly this.
-    return new PLearnerOutputVMatrix(dataset, this);
-}
-
-
-TVec<string> PLearner::getOutputNames() const
-{
-    int n = outputsize();
-    TVec<string> outnames(n);
-    for(int k=0; k<n; k++)
-        outnames[k] = "out" + tostring(k);
-    return outnames;
-}
-
-////////////////
-// useOnTrain //
-////////////////
-void PLearner::useOnTrain(Mat& outputs) const {
-    VMat train_output(outputs);
-    use(train_set, train_output);
-}
-
-//////////
-// test //
-//////////
-void PLearner::test(VMat testset, PP<VecStatsCollector> test_stats, 
-                    VMat testoutputs, VMat testcosts) const
-{
-    int len = testset.length();
-    Vec input;
-    Vec target;
-    real weight;
-
-    Vec output(outputsize());
-    Vec costs(nTestCosts());
-
-    PP<ProgressBar> pb;
-    if (report_progress) 
-        pb = new ProgressBar("Testing learner", len);
-
-    if (len == 0) {
-        // Empty test set: we give -1 cost arbitrarily.
-        costs.fill(-1);
-        test_stats->update(costs);
-    }
-
-    for (int i = 0; i < len; i++)
-    {
-        testset.getExample(i, input, target, weight);
-      
-        // Always call computeOutputAndCosts, since this is better
-        // behaved with stateful learners
-        computeOutputAndCosts(input,target,output,costs);
-      
-        if (testoutputs)
-            testoutputs->putOrAppendRow(i, output);
-
-        if (testcosts)
-            testcosts->putOrAppendRow(i, costs);
-
-        if (test_stats)
-            test_stats->update(costs, weight);
-
-        if (report_progress)
-            pb->update(i);
-    }
-
-}
-
-///////////////
-// initTrain //
-///////////////
-bool PLearner::initTrain()
-{
-    string warn_msg = "In PLearner::trainingCheck (called by '" +
-        this->classname() + "') - ";
-    
-    // Check 'nstages' is valid.
-    if (nstages < 0) {
-        PLWARNING((warn_msg + "Option nstages (set to " + tostring(nstages)
-                    + ") must be non-negative").c_str());
-        return false;
-    }
-
-    // Check we actually need to train.
-    if (stage == nstages) {
-        if (verbosity >= 1)
-            PLWARNING((warn_msg + "The learner is already trained").c_str());
-        return false;
-    }
-
-    if (stage > nstages) {
-        if (verbosity >= 1) {
-            string msg = warn_msg + "Learner was already trained up to stage "
-                + tostring(stage) + ", but asked to train up to nstages="
-                + tostring(nstages) + ": it will be reverted to stage 0 and "
-                                      "trained again";
-            PLWARNING(msg.c_str());
-        }
-        forget();
-    }
-
-    // Check there is a training set.
-    if (!train_set) {
-        if (verbosity >= 1)
-            PLWARNING((warn_msg + "No training set specified").c_str());
-        return false;
-    }
-
-    // Initialize train_stats if needed.
-    if (!train_stats)
-        train_stats = new VecStatsCollector();
-
-    // Everything is fine.
-    return true;
-}
-
-////////////////////////
-// resetInternalState //
-////////////////////////
-void PLearner::resetInternalState()
-{ }
-
-bool PLearner::isStatefulLearner() const
-{
-    return false;
-}
-
-
-//#####  computeInputOutputMat  ###############################################
-
-Mat PLearner::computeInputOutputMat(VMat inputs) const
-{
-    int l = inputs.length();
-    int nin = inputsize();
-    int nout = outputsize();
-    Mat m(l, nin+nout);
-    for(int i=0; i<l; i++)
-    {
-        Vec v = m(i);
-        Vec invec = v.subVec(0,nin);
-        Vec outvec = v.subVec(nin,nout);
-        inputs->getRow(i, invec);
-        computeOutput(invec, outvec);
-    }
-    return m;
-}
-
-
-//#####  computeInputOutputConfMat  ###########################################
-
-Mat PLearner::computeInputOutputConfMat(VMat inputs, real probability) const
-{
-    int l = inputs.length();
-    int nin = inputsize();
-    int nout = outputsize();
-    Mat m(l, nin+3*nout);
-    TVec< pair<real,real> > intervals;
-    for(int i=0; i<l; i++)
-    {
-        Vec v = m(i);
-        Vec invec   = v.subVec(0,nin);
-        Vec outvec  = v.subVec(nin,nout);
-        Vec lowconf = v.subVec(nin+nout, nout);
-        Vec hiconf  = v.subVec(nin+2*nout, nout);
-        inputs->getRow(i, invec);
-        computeOutput(invec, outvec);
-        bool conf_avail = computeConfidenceFromOutput(invec, outvec, probability,
-                                                      intervals);
-        if (conf_avail) {
-            for (int i=0, n=intervals.size() ; i<n ; ++i) {
-                lowconf[i] = intervals[i].first;
-                hiconf[i]  = intervals[i].second;
-            }
-        }
-        else {
-            lowconf << MISSING_VALUE;
-            hiconf  << MISSING_VALUE;
-        }
-    }
-    return m;
-}
-
-
-//! Version of computeOutput that returns a result by value
-Vec PLearner::remote_computeOutput(const Vec& input) const
-{
-    tmp_output.resize(outputsize());
-    computeOutput(input, tmp_output);
-    return tmp_output;
-}
-
-//! Version of use that's called by RMI
-void PLearner::remote_use(VMat inputs, string output_fname) const
-{
-    VMat outputs = new FileVMatrix(output_fname, inputs.length(), outputsize());
-    use(inputs,outputs);
-}
-
-//! Version of use2 that's called by RMI
-Mat PLearner::remote_use2(VMat inputs) const
-{
-    Mat outputs(inputs.length(), outputsize());
-    use(inputs,outputs);
-    return outputs;
-}
-    
-//! Version of computeOutputAndCosts that's called by RMI
-
-tuple<Vec,Vec> PLearner::remote_computeOutputAndCosts(const Vec& input, const Vec& target) const
-{
-    tmp_output.resize(outputsize());
-    Vec costs(nTestCosts());
-    computeOutputAndCosts(input,target,tmp_output,costs);
-    return make_tuple(tmp_output, costs);
-}
-
-//! Version of computeCostsFromOutputs that's called by RMI
-Vec PLearner::remote_computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                             const Vec& target) const
-{
-    Vec costs(nTestCosts());
-    computeCostsFromOutputs(input,output,target,costs);
-    return costs;
-}
-
-//! Version of computeCostsOnly that's called by RMI
-Vec PLearner::remote_computeCostsOnly(const Vec& input, const Vec& target) const
-{
-    Vec costs(nTestCosts());
-    computeCostsOnly(input,target,costs);
-    return costs;
-}
-
-//! Version of computeConfidenceFromOutput that's called by RMI
-TVec< pair<real,real> >
-PLearner::remote_computeConfidenceFromOutput(const Vec& input, const Vec& output,
-                                             real probability) const
-{
-    TVec< pair<real,real> > intervals(output.length());
-    bool ok = computeConfidenceFromOutput(input, output, probability, intervals);
-    if (ok)
-        return intervals;
-    else
-        return TVec< pair<real,real> >();
-}
-
-//! Version of computeOutputCovMat that's called by RMI
-tuple<Mat, TVec<Mat> >
-PLearner::remote_computeOutputCovMat(const Mat& inputs) const
-{
-    Mat outputs;
-    TVec<Mat> covmat;
-    computeOutputCovMat(inputs, outputs, covmat);
-    return make_tuple(outputs, covmat);
-}
-
-//! Version of batchComputeOutputAndConfidence that's called by RMI
-void PLearner::remote_batchComputeOutputAndConfidence(VMat inputs, real probability,
-                                                      string pmat_fname) const
-{
-    TVec<string> fieldnames;
-    for(int j=0; j<outputsize(); j++)
-    {
-        fieldnames.append("output_"+tostring(j));
-        fieldnames.append("low_"+tostring(j));
-        fieldnames.append("high_"+tostring(j));
-    }
-    VMat out_and_conf = new FileVMatrix(pmat_fname,inputs.length(),fieldnames);
-    batchComputeOutputAndConfidence(inputs, probability, out_and_conf);
-}
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3/plearn_learners/generic/PLearner.cc (from rev 6569, trunk/plearn_learners/generic/PLearner.cc)

Deleted: tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-12 15:44:07 UTC (rev 6570)
@@ -1,664 +0,0 @@
-// -*- C++ -*-
-
-// GaussianProcessRegressor.cc
-//
-// Copyright (C) 2006 Nicolas Chapados 
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
-   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $ 
-   ******************************************************* */
-
-// Authors: Nicolas Chapados
-
-/*! \file GaussianProcessRegressor.cc */
-
-#define PL_LOG_MODULE_NAME "GaussianProcessRegressor"
-
-// From PLearn
-#include <plearn/base/stringutils.h>
-#include <plearn/io/pl_log.h>
-#include <plearn/vmat/ExtendedVMatrix.h>
-#include <plearn/math/pl_erf.h>
-#include <plearn/var/GaussianProcessNLLVariable.h>
-#include <plearn/var/ObjectOptionVariable.h>
-#include <plearn/opt/Optimizer.h>
-
-#include "GaussianProcessRegressor.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    GaussianProcessRegressor,
-    "Kernelized version of linear ridge regression.",
-    "Given a kernel K(x,y) = phi(x)'phi(y), where phi(x) is the projection of a\n"
-    "vector x into feature space, this class implements a version of the ridge\n"
-    "estimator, giving the prediction at x as\n"
-    "\n"
-    "    f(x) = k(x)'(M + lambda I)^-1 y,\n"
-    "\n"
-    "where x is the test vector where to estimate the response, k(x) is the\n"
-    "vector of kernel evaluations between the test vector and the elements of\n"
-    "the training set, namely\n"
-    "\n"
-    "    k(x) = (K(x,x1), K(x,x2), ..., K(x,xN))',\n"
-    "\n"
-    "M is the Gram Matrix on the elements of the training set, i.e. the matrix\n"
-    "where the element (i,j) is equal to K(xi, xj), lambda is the VARIANCE of\n"
-    "the observation noise (and can be interpreted as a weight decay\n"
-    "coefficient), and y is the vector of training-set targets.\n"
-    "\n"
-    "The uncertainty in a prediction can be computed by calling\n"
-    "computeConfidenceFromOutput.  Furthermore, if desired, this learner allows\n"
-    "optimization of the kernel hyperparameters by direct optimization of the\n"
-    "marginal likelihood w.r.t. the hyperparameters.  This mechanism relies on a\n"
-    "user-provided Optimizer (see the 'optimizer' option) and does not rely on\n"
-    "the PLearn HyperLearner system.\n"
-    "\n"
-    "GaussianProcessRegressor produces the following train costs:\n"
-    "\n"
-    "- \"nmll\" : the negative marginal log-likelihood on the training set.\n"
-    "- \"mse\"  : the mean-squared error on the training set (by convention,\n"
-    "           divided by two)\n"
-    "\n"
-    "and the following test costs:\n"
-    "\n"
-    "- \"nll\" : the negative log-likelihood of the test example under the\n"
-    "          predictive distribution.  Available only if the option\n"
-    "          'compute_confidence' is true.\n"
-    "- \"mse\" : the squared error of the test example with respect to the\n"
-    "          predictive mean (by convention, divided by two).\n"
-    "\n"
-    "The disadvantage of this learner is that its training time is O(N^3) in the\n"
-    "number of training examples (due to the matrix inversion).  When saving the\n"
-    "learner, the training set inputs must be saved, along with an additional\n"
-    "matrix of length number-of-training-examples, and width number-of-targets.\n"
-    );
-
-GaussianProcessRegressor::GaussianProcessRegressor() 
-    : m_weight_decay(0.0),
-      m_include_bias(true),
-      m_compute_confidence(false),
-      m_confidence_epsilon(1e-8)
-{ }
-
-
-void GaussianProcessRegressor::declareOptions(OptionList& ol)
-{
-    //#####  Build Options  ###################################################
-    
-    declareOption(
-        ol, "kernel", &GaussianProcessRegressor::m_kernel,
-        OptionBase::buildoption,
-        "Kernel to use for the computation.  This must be a similarity kernel\n"
-        "(i.e. closer vectors give higher kernel evaluations).");
-
-    declareOption(
-        ol, "weight_decay", &GaussianProcessRegressor::m_weight_decay,
-        OptionBase::buildoption,
-        "Weight decay coefficient (default = 0)");
-
-    declareOption(
-        ol, "include_bias", &GaussianProcessRegressor::m_include_bias,
-        OptionBase::buildoption,
-        "Whether to include a bias term in the regression (true by default)\n"
-        "The effect of this option is NOT to prepend a column of 1 to the inputs\n"
-        "(which has often no effect for GP regression), but to estimate a\n"
-        "separate mean of the targets, perform the GP regression on the\n"
-        "zero-mean targets, and add it back when computing the outputs.\n");
-
-    declareOption(
-        ol, "compute_confidence", &GaussianProcessRegressor::m_compute_confidence,
-        OptionBase::buildoption,
-        "Whether to perform the additional train-time computations required\n"
-        "to compute confidence intervals.  This includes computing a separate\n"
-        "inverse of the Gram matrix.  Specification of this option is necessary\n"
-        "for calling both computeConfidenceFromOutput and computeOutputCovMat.\n");
-
-    declareOption(
-        ol, "confidence_epsilon", &GaussianProcessRegressor::m_confidence_epsilon,
-        OptionBase::buildoption,
-        "Small regularization to be added post-hoc to the computed output\n"
-        "covariance matrix and confidence intervals; this is mostly used as a\n"
-        "disaster prevention device, to avoid negative predictive variance\n");
-    
-    declareOption(
-        ol, "hyperparameters", &GaussianProcessRegressor::m_hyperparameters,
-        OptionBase::buildoption,
-        "List of hyperparameters to optimize.  They must be specified in the\n"
-        "form \"option-name\":initial-value, where 'option-name' is the name\n"
-        "of an option to set within the Kernel object (the array-index form\n"
-        "'option[i]' is supported), and 'initial-value' is the\n"
-        "(PLearn-serialization string representation) for starting point for the\n"
-        "optimization.  Currently, the hyperparameters are constrained to be\n"
-        "scalars.\n");
-
-    declareOption(
-        ol, "ARD_hyperprefix_initval",
-        &GaussianProcessRegressor::m_ARD_hyperprefix_initval,
-        OptionBase::buildoption,
-        "If the kernel support automatic relevance determination (ARD; e.g.\n"
-        "SquaredExponentialARDKernel), the list of hyperparameters corresponding\n"
-        "to each input can be created automatically by giving an option prefix\n"
-        "and an initial value.  The ARD options are created to have the form\n"
-        "\n"
-        "   'prefix[0]', 'prefix[1]', 'prefix[N-1]'\n"
-        "\n"
-        "where N is the number of inputs.  This option is useful when the\n"
-        "dataset inputsize is not (easily) known ahead of time. \n");
-    
-    declareOption(
-        ol, "optimizer", &GaussianProcessRegressor::m_optimizer,
-        OptionBase::buildoption,
-        "Specification of the optimizer to use for train-time hyperparameter\n"
-        "optimization.  A ConjGradientOptimizer should be an adequate choice.\n");
-
-
-    //#####  Learnt Options  ##################################################
-
-    declareOption(
-        ol, "alpha", &GaussianProcessRegressor::m_alpha,
-        OptionBase::learntoption,
-        "Vector of learned parameters, determined from the equation\n"
-        "    (M + lambda I)^-1 y");
-
-    declareOption(
-        ol, "gram_inverse", &GaussianProcessRegressor::m_gram_inverse,
-        OptionBase::learntoption,
-        "Inverse of the Gram matrix, used to compute confidence intervals (must\n"
-        "be saved since the confidence intervals are obtained from the equation\n"
-        "\n"
-        "  sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)\n");
-
-    declareOption(
-        ol, "target_mean", &GaussianProcessRegressor::m_target_mean,
-        OptionBase::learntoption,
-        "Mean of the targets, if the option 'include_bias' is true");
-    
-    declareOption(
-        ol, "training_inputs", &GaussianProcessRegressor::m_training_inputs,
-        OptionBase::learntoption,
-        "Saved version of the training set, which must be kept along for\n"
-        "carrying out kernel evaluations with the test point");
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-void GaussianProcessRegressor::build_()
-{
-    if (! m_kernel)
-        PLERROR("GaussianProcessRegressor::build_: 'kernel' option must be specified");
-
-    if (! m_kernel->is_symmetric)
-        PLERROR("GaussianProcessRegressor::build_: the kernel (%s) must be symmetric",
-                m_kernel->classname().c_str());
-    
-    // If we are reloading the model, set the training inputs into the kernel
-    if (m_training_inputs)
-        m_kernel->setDataForKernelMatrix(m_training_inputs);
-
-    // If we specified hyperparameters without an optimizer, complain.
-    // (It is mildly legal to specify an optimizer without hyperparameters;
-    // this does nothing).
-    if (m_hyperparameters.size() > 0 && ! m_optimizer)
-        PLERROR("GaussianProcessRegressor::build_: 'hyperparameters' are specified "
-                "but no 'optimizer'; an optimizer is required in order to carry out "
-                "hyperparameter optimization");
-
-    if (m_confidence_epsilon < 0)
-        PLERROR("GaussianProcessRegressor::build_: 'confidence_epsilon' must be non-negative");
-}
-
-// ### Nothing to add here, simply calls build_
-void GaussianProcessRegressor::build()
-{
-    inherited::build();
-    build_();
-}
-
-
-void GaussianProcessRegressor::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_kernel,                     copies);
-    deepCopyField(m_hyperparameters,            copies);
-    deepCopyField(m_optimizer,                  copies);
-    deepCopyField(m_alpha,                      copies);
-    deepCopyField(m_gram_inverse,               copies);
-    deepCopyField(m_target_mean,                copies);
-    deepCopyField(m_training_inputs,            copies);
-    deepCopyField(m_kernel_evaluations,         copies);
-    deepCopyField(m_gram_inverse_product,       copies);
-    deepCopyField(m_intervals,                  copies);
-    deepCopyField(m_gram_traintest_inputs,      copies);
-    deepCopyField(m_gram_inv_traintest_product, copies);
-    deepCopyField(m_sigma_reductor,             copies);
-}
-
-
-//#####  setTrainingSet  ######################################################
-
-void GaussianProcessRegressor::setTrainingSet(VMat training_set, bool call_forget)
-{
-    PLASSERT( training_set );
-    int inputsize = training_set->inputsize() ;
-    if (inputsize < 0)
-        PLERROR("GaussianProcessRegressor::setTrainingSet: the training set inputsize "
-                "must be specified (current value = %d)", inputsize);
-
-    // Convert to a real matrix in order to make saving it saner
-    m_training_inputs = training_set.subMatColumns(0, inputsize).toMat();
-    inherited::setTrainingSet(training_set, call_forget);
-}
-
-
-//#####  outputsize  ##########################################################
-
-int GaussianProcessRegressor::outputsize() const
-{
-    return targetsize();
-}
-
-
-//#####  forget  ##############################################################
-
-void GaussianProcessRegressor::forget()
-{
-    inherited::forget();
-    if (m_optimizer)
-        m_optimizer->reset();
-    m_alpha.resize(0,0);
-    m_target_mean.resize(0);
-    m_gram_inverse.resize(0,0);
-    stage = 0;
-}
-    
-
-//#####  train  ###############################################################
-
-void GaussianProcessRegressor::train()
-{
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    PLASSERT( m_kernel );
-    if (! train_set || ! m_training_inputs)
-        PLERROR("GaussianProcessRegressor::train: the training set must be specified");
-    int trainlength = train_set->length();
-    int inputsize   = train_set->inputsize() ;
-    int targetsize  = train_set->targetsize();
-    int weightsize  = train_set->weightsize();
-    if (inputsize  < 0 || targetsize < 0 || weightsize < 0)
-        PLERROR("GaussianProcessRegressor::train: inconsistent inputsize/targetsize/weightsize "
-                "(%d/%d/%d) in training set", inputsize, targetsize, weightsize);
-    if (weightsize > 0)
-        PLERROR("GaussianProcessRegressor::train: observations weights are not currently supported");
-
-    // Subtract the mean if we require it
-    Mat targets(trainlength, targetsize);
-    train_set.subMatColumns(inputsize, targetsize)->getMat(0,0,targets);
-    if (m_include_bias) {
-        m_target_mean.resize(targets.width());
-        columnMean(targets, m_target_mean);
-        targets -= m_target_mean;
-    }
-
-    // Optimize hyperparameters
-    PP<GaussianProcessNLLVariable> nll = hyperOptimize(m_training_inputs, targets);
-    PLASSERT( nll );
-    
-    // Compute parameters
-    nll->fprop();
-    m_alpha = nll->alpha();
-    m_gram_inverse = nll->gramInverse();
-
-    // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
-    Mat residuals(m_alpha.length(), m_alpha.width());
-    product(residuals, nll->gram(), m_alpha);
-    residuals -= targets;
-    real mse = dot(residuals, residuals) / (2 * trainlength);
-    
-    // And accumulate some statistics
-    Vec costs(2);
-    costs[0] = nll->value[0];
-    costs[1] = mse;
-    getTrainStatsCollector()->update(costs);
-    MODULE_LOG << "Train NLL: " << costs[0] << endl;
-}
-
-
-//#####  computeOutput  #######################################################
-
-void GaussianProcessRegressor::computeOutput(const Vec& input, Vec& output) const
-{
-    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
-    PLASSERT( m_alpha.width()  == output.size() );
-    PLASSERT( m_alpha.length() == m_training_inputs.length() );
-    PLASSERT( input.size()     == m_training_inputs.width()  );
-
-    m_kernel_evaluations.resize(m_alpha.length());
-    computeOutputAux(input, output, m_kernel_evaluations);
-}
-
-
-void GaussianProcessRegressor::computeOutputAux(
-    const Vec& input, Vec& output, Vec& kernel_evaluations) const
-{
-    m_kernel->evaluate_all_x_i(input, kernel_evaluations);
-
-    // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
-    product(Mat(1, output.size(), output),
-            Mat(1, kernel_evaluations.size(), kernel_evaluations),
-            m_alpha);
-
-    if (m_include_bias)
-        output += m_target_mean;
-}
-
-
-//#####  computeCostsFromOutputs  #############################################
-
-void GaussianProcessRegressor::computeCostsFromOutputs(const Vec& input, const Vec& output, 
-                                                       const Vec& target, Vec& costs) const
-{
-    costs.resize(2);
-
-    // NLL cost is the NLL of the target under the predictive distribution
-    // (centered at predictive mean, with variance obtainable from the
-    // confidence bounds).  HOWEVER, to obain it, we have to be able to compute
-    // the confidence bounds.  If impossible, simply set missing-value for the
-    // NLL cost.
-    if (m_compute_confidence) {
-        static const float PROBABILITY = pl_erf(1. / (2*sqrt(2)));  // 0.5 stddev
-        bool confavail = computeConfidenceFromOutput(input, output, PROBABILITY,
-                                                     m_intervals);
-        assert( confavail && m_intervals.size() == output.size() &&
-                output.size() == target.size() );
-        static const real LN_2PI_OVER_2 = pl_log(2*M_PI) / 2.0;
-        real nll = 0;
-        for (int i=0, n=output.size() ; i<n ; ++i) {
-            real sigma = m_intervals[i].second - m_intervals[i].first;
-            sigma = max(sigma, 1e-15);        // Very minor regularization
-            real diff = target[i] - output[i];
-            nll += diff*diff / (2.*sigma*sigma) + pl_log(sigma) + LN_2PI_OVER_2;
-        }
-        costs[0] = nll;
-    }
-    else
-        costs[0] = MISSING_VALUE;
-    
-    real squared_loss = 0.5*powdistance(output,target);
-    costs[1] = squared_loss;
-}     
-
-
-//#####  computeConfidenceFromOutput  #########################################
-
-bool GaussianProcessRegressor::computeConfidenceFromOutput(
-    const Vec& input, const Vec& output, real probability,
-    TVec< pair<real,real> >& intervals) const
-{
-    if (! m_compute_confidence) {
-        PLWARNING("GaussianProcessRegressor::computeConfidenceFromOutput: the option\n"
-                  "'compute_confidence' must be true in order to compute valid\n"
-                  "condidence intervals");
-        return false;
-    }
-
-    // BIG-BIG assumption: assume that computeOutput has just been called and
-    // that m_kernel_evaluations contains the right stuff.
-    PLASSERT( m_kernel && m_gram_inverse.isNotNull() );
-    real base_sigma_sq = m_kernel(input, input);
-    m_gram_inverse_product.resize(m_kernel_evaluations.size());
-    product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
-    real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(0., base_sigma_sq - sigma_reductor + m_confidence_epsilon));
-
-    // two-tailed
-    const real multiplier = gauss_01_quantile((1+probability)/2);
-    real half_width = multiplier * sigma;
-    intervals.resize(output.size());
-    for (int i=0, n=output.size() ; i<n ; ++i)
-        intervals[i] = std::make_pair(output[i] - half_width,
-                                      output[i] + half_width);
-    return true;
-}
-
-
-//#####  computeOutputCovMat  #################################################
-
-void GaussianProcessRegressor::computeOutputCovMat(
-    const Mat& inputs, Mat& outputs, TVec<Mat>& covariance_matrices) const
-{
-    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
-    PLASSERT( m_alpha.width()  == outputsize() );
-    PLASSERT( m_alpha.length() == m_training_inputs.length() );
-    PLASSERT( inputs.width()   == m_training_inputs.width()  );
-    PLASSERT( inputs.width()   == inputsize() );
-    const int N = inputs.length();
-    const int M = outputsize();
-    const int T = m_training_inputs.length();
-    outputs.resize(N, M);
-    covariance_matrices.resize(M);
-
-    // Preallocate space for the covariance matrix, and since all outputs share
-    // the same matrix, copy it into the remaining elements of
-    // covariance_matrices
-    Mat& covmat = covariance_matrices[0];
-    covmat.resize(N, N);
-    for (int j=1 ; j<M ; ++j)
-        covariance_matrices[j] = covmat;
-
-    // Start by computing the matrix of kernel evaluations between the train
-    // and test outputs, and compute the output
-    m_gram_traintest_inputs.resize(N, T);
-    for (int i=0 ; i<N ; ++i) {
-        Vec cur_traintest_kereval = m_gram_traintest_inputs(i);
-        Vec cur_output = outputs(i);
-        computeOutputAux(inputs(i), cur_output, cur_traintest_kereval);
-    }
-
-    // Next compute the kernel evaluations between the test inputs; more or
-    // less lifted from Kernel.cc ==> must see with Olivier how to better
-    // factor this code
-    Mat& K = covmat;
-    K.resize(N,N);
-    const int mod = K.mod();
-    real Kij;
-    real* Ki;
-    real* Kji;
-    for (int i=0 ; i<N ; ++i) {
-        Ki  = K[i];
-        Kji = &K[0][i];
-        const Vec& cur_input_i = inputs(i);
-        for (int j=0 ; j<=i ; ++j, Kji += mod) {
-            Kij = m_kernel->evaluate(cur_input_i, inputs(j));
-            *Ki++ = Kij;
-            if (j<i)
-                *Kji = Kij;    // Assume symmetry, checked at build
-        }
-    }
-
-    // The predictive covariance matrix is (c.f. Rasmussen and Williams):
-    //
-    //    cov(f*) = K(X*,X*) - K(X*,X) [K(X,X) + sigma*I]^-1 K(X,X*)
-    //
-    // where X are the training inputs, and X* are the test inputs.
-    m_gram_inv_traintest_product.resize(T,N);
-    m_sigma_reductor.resize(N,N);
-    productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
-                     m_gram_traintest_inputs);
-    product(m_sigma_reductor, m_gram_traintest_inputs,
-            m_gram_inv_traintest_product);
-    covmat -= m_sigma_reductor;
-
-    // As a preventive measure, never output negative variance, even though
-    // this does not garantee the non-negative-definiteness of the matrix
-    for (int i=0 ; i<N ; ++i)
-        covmat(i,i) = max(0.0, covmat(i,i) + m_confidence_epsilon);
-}
-
-
-//#####  get*CostNames  #######################################################
-
-TVec<string> GaussianProcessRegressor::getTestCostNames() const
-{
-    TVec<string> c(2);
-    c[0] = "nll";
-    c[1] = "mse";
-    return c;
-}
-
-
-TVec<string> GaussianProcessRegressor::getTrainCostNames() const
-{
-    TVec<string> c(2);
-    c[0] = "nmll";
-    c[1] = "mse";
-    return c;
-}
-
-
-//#####  hyperOptimize  #######################################################
-
-PP<GaussianProcessNLLVariable>
-GaussianProcessRegressor::hyperOptimize(const Mat& inputs, const Mat& targets)
-{
-    // If there are no hyperparameters or optimizer, just create a simple
-    // variable and return it right away.
-    if (! m_optimizer || (m_hyperparameters.size() == 0 &&
-                          m_ARD_hyperprefix_initval.first.empty()) )
-    {
-        return new GaussianProcessNLLVariable(
-            m_kernel, m_weight_decay, inputs, targets,
-            TVec<string>(), VarArray(), m_compute_confidence);
-    }
-
-    // Otherwise create Vars that wrap each hyperparameter
-    const int numhyper  = m_hyperparameters.size();
-    const int numinputs = ( ! m_ARD_hyperprefix_initval.first.empty() ?
-                            inputsize() : 0 );
-    VarArray     hyperparam_vars (numhyper + numinputs);
-    TVec<string> hyperparam_names(numhyper + numinputs);
-    int i;
-    for (i=0 ; i<numhyper ; ++i) {
-        hyperparam_names[i] = m_hyperparameters[i].first;
-        hyperparam_vars [i] = new ObjectOptionVariable(
-            (Kernel*)m_kernel, m_hyperparameters[i].first, m_hyperparameters[i].second);
-        hyperparam_vars[i]->setName(m_hyperparameters[i].first);
-    }
-
-    // If specified, create the Vars for automatic relevance determination
-    string& ARD_name = m_ARD_hyperprefix_initval.first;
-    string& ARD_init = m_ARD_hyperprefix_initval.second;
-    if (! ARD_name.empty()) {
-        // Small hack to ensure the ARD vector in the kernel has proper size
-        Vec init(numinputs, lexical_cast<double>(ARD_init));
-        m_kernel->changeOption(ARD_name, tostring(init, PStream::plearn_ascii));
-        
-        for (int j=0 ; j<numinputs ; ++j, ++i) {
-            hyperparam_names[i] = ARD_name + '[' + tostring(j) + ']';
-            hyperparam_vars [i] = new ObjectOptionVariable(
-                (Kernel*)m_kernel, hyperparam_names[i], ARD_init);
-            hyperparam_vars [i]->setName(hyperparam_names[i]);
-        }
-    }
-
-    // Create the cost-function variable
-    PP<GaussianProcessNLLVariable> nll = new GaussianProcessNLLVariable(
-        m_kernel, m_weight_decay, inputs, targets, hyperparam_names,
-        hyperparam_vars, true);
-    nll->setName("GaussianProcessNLLVariable");
-
-    // Some logging about the initial values
-    logVarray(hyperparam_vars, "Hyperparameter initial values:");
-    
-    // And optimize for nstages
-    m_optimizer->setToOptimize(hyperparam_vars, (Variable*)nll);
-    m_optimizer->build();
-    PP<ProgressBar> pb(
-        report_progress? new ProgressBar("Training GaussianProcessRegressor "
-                                         "from stage " + tostring(stage) + " to stage " +
-                                         tostring(nstages), nstages-stage)
-        : 0);
-    bool early_stopping = false;
-    PP<VecStatsCollector> statscol = new VecStatsCollector;
-    for (const int initial_stage = stage ; !early_stopping && stage < nstages
-             ; ++stage)
-    {
-        if (pb)
-            pb->update(stage - initial_stage);
-
-        statscol->forget();
-        early_stopping = m_optimizer->optimizeN(*statscol);
-        statscol->finalize();
-    }
-    pb = 0;                                  // Finish progress bar right now
-
-    // Some logging about the final values
-    logVarray(hyperparam_vars, "Hyperparameter final values:");
-    return nll;
-}
-
-
-//#####  logVarray  ###########################################################
-
-void GaussianProcessRegressor::logVarray(const VarArray& varr,
-                                         const string& title)
-{
-    string entry = title + '\n';
-    for (int i=0, n=varr.size() ; i<n ; ++i) {
-        entry += right(varr[i]->getName(), 35) + ": " + tostring(varr[i]->value[0]);
-        if (i < n-1)
-            entry += '\n';
-    }
-    MODULE_LOG << entry << endl; 
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3/plearn_learners/regressors/GaussianProcessRegressor.cc (from rev 6569, trunk/plearn_learners/regressors/GaussianProcessRegressor.cc)



From ducharme at mail.berlios.de  Fri Jan 12 20:39:22 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Fri, 12 Jan 2007 20:39:22 +0100
Subject: [Plearn-commits] r6571 - in tags: . OPAL-3.0.3.1/plearn/ker
	OPAL-3.0.3.1/plearn_learners/generic
	OPAL-3.0.3.1/plearn_learners/regressors
Message-ID: <200701121939.l0CJdMIM016707@sheep.berlios.de>

Author: ducharme
Date: 2007-01-12 20:39:21 +0100 (Fri, 12 Jan 2007)
New Revision: 6571

Added:
   tags/OPAL-3.0.3.1/
   tags/OPAL-3.0.3.1/plearn/ker/SummationKernel.cc
   tags/OPAL-3.0.3.1/plearn_learners/generic/PLearner.cc
   tags/OPAL-3.0.3.1/plearn_learners/regressors/GaussianProcessRegressor.cc
Removed:
   tags/OPAL-3.0.3.1/plearn/ker/SummationKernel.cc
   tags/OPAL-3.0.3.1/plearn_learners/generic/PLearner.cc
   tags/OPAL-3.0.3.1/plearn_learners/regressors/GaussianProcessRegressor.cc
Log:
Tag pour release OPAL 3.0.3.1

Copied: tags/OPAL-3.0.3.1 (from rev 6568, trunk)

Deleted: tags/OPAL-3.0.3.1/plearn/ker/SummationKernel.cc
===================================================================
--- trunk/plearn/ker/SummationKernel.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3.1/plearn/ker/SummationKernel.cc	2007-01-12 19:39:21 UTC (rev 6571)
@@ -1,169 +0,0 @@
-// -*- C++ -*-
-
-// SummationKernel.cc
-//
-// Copyright (C) 2007 Nicolas Chapados
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Nicolas Chapados
-
-/*! \file SummationKernel.cc */
-
-
-#include "SummationKernel.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    SummationKernel,
-    "Kernel computing the sum of other kernels",
-    "This kernel computes the summation of several subkernel objects.  It can\n"
-    "also chop up parts of its input vector and send it to each kernel (so that\n"
-    "each kernel can operate on a subset of the variables).\n"
-    );
-
-
-//#####  Constructor  #########################################################
-
-SummationKernel::SummationKernel()
-{ }
-
-
-//#####  declareOptions  ######################################################
-
-void SummationKernel::declareOptions(OptionList& ol)
-{
-    declareOption(
-        ol, "terms", &SummationKernel::m_terms, OptionBase::buildoption,
-        "Individual kernels to add to produce the final result.  The\n"
-        "hyperparameters of kernel i can be accesed under the option names\n"
-        "'terms[i].hyperparam' for, e.g. GaussianProcessRegressor.\n");
-
-    declareOption(
-        ol, "input_indexes", &SummationKernel::m_input_indexes,
-        OptionBase::buildoption,
-        "Optionally, one can specify which of individual input variables should\n"
-        "be routed to each kernel.  The format is as a vector of vectors: for\n"
-        "each kernel in 'terms', one must list the INDEXES in the original input\n"
-        "vector(zero-based) that should be passed to that kernel.  If a list of\n"
-        "indexes is empty for a given kernel, it means that the COMPLETE input\n"
-        "vector should be passed to the kernel.\n");
-    
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-
-//#####  build  ###############################################################
-
-void SummationKernel::build()
-{
-    // ### Nothing to add here, simply calls build_
-    inherited::build();
-    build_();
-}
-
-
-//#####  build_  ##############################################################
-
-void SummationKernel::build_()
-{
-    // Preallocate buffers for kernel evaluation
-    const int N = m_input_indexes.size();
-    m_input_buf1.resize(N);
-    m_input_buf2.resize(N);
-    for (int i=0 ; i<N ; ++i) {
-        const int M = m_input_indexes[i].size();
-        m_input_buf1[i].resize(M);
-        m_input_buf2[i].resize(M);
-    }
-
-    // Kernel is symmetric only if all terms are
-    is_symmetric = true;
-    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
-        if (! m_terms[i])
-            PLERROR("SummationKernel::build_: kernel for term[%d] is not specified",i);
-        is_symmetric = is_symmetric && m_terms[i]->is_symmetric
-    }
-
-    if (m_input_indexes.size() > 0 && m_terms.size() != m_input_indexes.size())
-        PLERROR("SummationKernel::build_: if 'input_indexes' is specified "
-                "it must have the same size (%d) as 'terms'; found %d elements",
-                m_terms.size(), m_input_indexes.size());
-}
-
-
-//#####  evaluate  ############################################################
-
-real SummationKernel::evaluate(const Vec& x1, const Vec& x2) const
-{
-    real kernel_value = 0.0;
-    bool split_inputs = m_input_indexes.size() > 0;
-    for (int i=0, n=m_terms.size() ; i<n ; ++i) {
-        if (split_inputs && m_input_indexes[i].size() > 0) {
-            selectElements(x1, m_input_indexes[i], m_input_buf1[i]);
-            selectElements(x2, m_input_indexes[i], m_input_buf2[i]);
-            kernel_value += m_terms[i]->evaluate(m_input_buf1[i],
-                                                 m_input_buf2[i]);
-        }
-        else
-            kernel_value += m_terms[i]->evaluate(x1,x2);
-    }
-    return kernel_value;
-}
-
-
-//#####  makeDeepCopyFromShallowCopy  #########################################
-
-void SummationKernel::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_terms,          copies);
-    deepCopyField(m_input_indexes,  copies);
-    deepCopyField(m_input_buf1,     copies);
-    deepCopyField(m_input_buf2,     copies);
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3.1/plearn/ker/SummationKernel.cc (from rev 6569, trunk/plearn/ker/SummationKernel.cc)

Deleted: tags/OPAL-3.0.3.1/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3.1/plearn_learners/generic/PLearner.cc	2007-01-12 19:39:21 UTC (rev 6571)
@@ -1,1016 +0,0 @@
-// -*- C++ -*-
-
-// PLearner.cc
-//
-// Copyright (C) 1998-2002 Pascal Vincent
-// Copyright (C) 1999-2002 Yoshua Bengio, Nicolas Chapados, Charles Dugas, Rejean Ducharme, Universite de Montreal
-// Copyright (C) 2001,2002 Francis Pieraut, Jean-Sebastien Senecal
-// Copyright (C) 2002 Frederic Morin, Xavier Saint-Mleux, Julien Keable
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-
- 
-
-/* *******************************************************      
- * $Id$
- ******************************************************* */
-
-#include "PLearner.h"
-#include <plearn/base/stringutils.h>
-#include <plearn/io/fileutils.h>
-#include <plearn/io/pl_log.h>
-#include <plearn/math/pl_erf.h>
-#include <plearn/vmat/FileVMatrix.h>
-#include <plearn/misc/PLearnService.h>
-#include <plearn/misc/RemotePLearnServer.h>
-#include <plearn/vmat/PLearnerOutputVMatrix.h>
-
-namespace PLearn {
-using namespace std;
-
-PLearner::PLearner()
-    : n_train_costs_(-1),
-      n_test_costs_(-1),
-      seed_(1827),                           //!< R.I.P. L.v.B.
-      stage(0),
-      nstages(1),
-      report_progress(true),
-      verbosity(1),
-      nservers(0),
-      save_trainingset_prefix(""),
-      inputsize_(-1),
-      targetsize_(-1),
-      weightsize_(-1),
-      n_examples(-1),
-      forget_when_training_set_changes(false)  
-{}
-
-PLEARN_IMPLEMENT_ABSTRACT_OBJECT(
-    PLearner,
-    "The base class for all PLearn learning algorithms",
-    "PLearner provides a base class for all learning algorithms within PLearn.\n"
-    "It presents an abstraction of learning that centers around a \"train-test\"\n"
-    "paradigm:\n"
-    "\n"
-    "- Phase 1: TRAINING.  In this phase, one must first establish an experiment\n"
-    "  directory (usually done by an enclosing PTester) to store any temporary\n"
-    "  files that the learner might seek to create.  Then, one sets a training\n"
-    "  set VMat (also done by the enclosing PTester), which contains the set of\n"
-    "  input-target pairs that the learner should attempt to represent.  Finally\n"
-    "  one calls the train() virtual member function to carry out the actual\n"
-    "  action of training the model.\n"
-    "\n"
-    "- Phase 2: TESTING.  In this phase (to be done after training), one\n"
-    "  repeatedly calls functions from the computeOutput() family to evaluate\n"
-    "  the trained model on new input vectors.\n"
-    "\n"
-    "Note that the PTester class is the usual \"driver\" for a PLearner (and\n"
-    "automatically calls the above functions in the appropriate order), in the\n"
-    "usual scenario wherein one wants to evaluate the generalization performance\n"
-    "on a dataset.\n"
-    );
-
-void PLearner::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-    deepCopyField(tmp_output, copies);
-    // TODO What's wrong with this?
-    deepCopyField(train_set, copies);
-    deepCopyField(validation_set, copies);
-    deepCopyField(train_stats, copies);
-    deepCopyField(random_gen, copies);
-}
-
-void PLearner::declareOptions(OptionList& ol)
-{
-    declareOption(
-        ol, "expdir", &PLearner::expdir, OptionBase::buildoption | OptionBase::nosave, 
-        "Path of the directory associated with this learner, in which\n"
-        "it should save any file it wishes to create. \n"
-        "The directory will be created if it does not already exist.\n"
-        "If expdir is the empty string (the default), then the learner \n"
-        "should not create *any* file. Note that, anyway, most file creation and \n"
-        "reporting are handled at the level of the PTester class rather than \n"
-        "at the learner's. \n");
-
-    declareOption(
-        ol, "seed", &PLearner::seed_, OptionBase::buildoption, 
-        "The initial seed for the random number generator used in this\n"
-        "learner, for instance for parameter initialization.\n"
-        "If -1 is provided, then a 'random' seed is chosen based on time\n"
-        "of day, ensuring that different experiments run differently.\n"
-        "If 0 is provided, no (re)initialization of the random number\n"
-        "generator is performed.\n"
-        "With a given positive seed, build() and forget() should always\n"
-        "initialize the parameters to the same values.");
-
-    declareOption(
-        ol, "stage", &PLearner::stage, OptionBase::learntoption, 
-        "The current training stage, since last fresh initialization (forget()): \n"
-        "0 means untrained, n often means after n epochs or optimization steps, etc...\n"
-        "The true meaning is learner-dependant."
-        "You should never modify this option directly!"
-        "It is the role of forget() to bring it back to 0,\n"
-        "and the role of train() to bring it up to 'nstages'...");
-
-    declareOption(
-        ol, "n_examples", &PLearner::n_examples, OptionBase::learntoption, 
-        "The number of samples in the training set.\n"
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "inputsize", &PLearner::inputsize_, OptionBase::learntoption, 
-        "The number of input columns in the data sets."
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "targetsize", &PLearner::targetsize_, OptionBase::learntoption, 
-        "The number of target columns in the data sets."
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "weightsize", &PLearner::weightsize_, OptionBase::learntoption, 
-        "The number of cost weight columns in the data sets."
-        "Obtained from training set with setTrainingSet.");
-
-    declareOption(
-        ol, "forget_when_training_set_changes",
-        &PLearner::forget_when_training_set_changes, OptionBase::buildoption, 
-        "Whether or not to call the forget() method (re-initialize model \n"
-        "as before training) in setTrainingSet when the\n"
-        "training set changes (e.g. of dimension).");
-
-    declareOption(
-        ol, "nstages", &PLearner::nstages, OptionBase::buildoption, 
-        "The stage until which train() should train this learner and return.\n"
-        "The meaning of 'stage' is learner-dependent, but for learners whose \n"
-        "training is incremental (such as involving incremental optimization), \n"
-        "it is typically synonym with the number of 'epochs', i.e. the number \n"
-        "of passages of the optimization process through the whole training set, \n"
-        "since the last fresh initialisation.");
-
-    declareOption(
-        ol, "report_progress", &PLearner::report_progress, OptionBase::buildoption, 
-        "should progress in learning and testing be reported in a ProgressBar.\n");
-
-    declareOption(
-        ol, "verbosity", &PLearner::verbosity, OptionBase::buildoption, 
-        "Level of verbosity. If 0 should not write anything on perr. \n"
-        "If >0 may write some info on the steps performed along the way.\n"
-        "The level of details written should depend on this value.");
-
-    declareOption(
-        ol, "nservers", &PLearner::nservers, OptionBase::buildoption, 
-        "Max number of computation servers to use in parallel with the main process.\n"
-        "If <=0 no parallelization will occur at this level.\n");
-
-    declareOption(
-        ol, "save_trainingset_prefix", &PLearner::save_trainingset_prefix,
-        OptionBase::buildoption,
-        "Whether the training set should be saved upon a call to\n"
-        "setTrainingSet().  The saved file is put in the learner's expdir\n"
-        "(assuming there is one) and has the form \"<prefix>_trainset_XXX.pmat\"\n"
-        "The prefix is what this option specifies.  'XXX' is a unique\n"
-        "serial number that is globally incremented with each saved\n"
-        "setTrainingSet.  This option is useful when manipulating very\n"
-        "complex nested learner structures, and you want to ensure that\n"
-        "the inner learner is getting the correct results.  (Default="",\n"
-        "i.e. don't save anything.)\n");
-  
-    inherited::declareOptions(ol);
-}
-
-void PLearner::declareMethods(RemoteMethodMap& rmm)
-{
-    // Insert a backpointer to remote methods; note that this
-    // different than for declareOptions()
-    rmm.inherited(inherited::_getRemoteMethodMap_());
-
-    declareMethod(
-        rmm, "setTrainingSet", &PLearner::setTrainingSet,
-        (BodyDoc("Declares the training set.  Then calls build() and forget() if\n"
-                 "necessary.\n"),
-         ArgDoc ("training_set", "The training set VMatrix to set; should have\n"
-                 "its inputsize, targetsize and weightsize fields set properly.\n"),
-         ArgDoc ("call_forget", "Whether the forget() function should be called\n"
-                 "upon setting the training set\n")));
-
-    declareMethod(
-        rmm, "setExperimentDirectory", &PLearner::setExperimentDirectory,
-        (BodyDoc("The experiment directory is the directory in which files related to\n"
-                 "this model are to be saved.  If it is an empty string, it is understood\n"
-                 "to mean that the user doesn't want any file created by this learner.\n"),
-         ArgDoc ("expdir", "Experiment directory to set")));
-
-    declareMethod(
-        rmm, "getExperimentDirectory", &PLearner::getExperimentDirectory,
-        (BodyDoc("This returns the currently set experiment directory\n"
-                 "(see setExperimentDirectory)\n"),
-         RetDoc ("Current experiment directory")));
-
-    declareMethod(
-        rmm, "forget", &PLearner::forget,
-        (BodyDoc("(Re-)initializes the PLearner in its fresh state (that state may depend\n"
-                 "on the 'seed' option) and sets 'stage' back to 0 (this is the stage of\n"
-                 "a fresh learner!)\n"
-                 "\n"
-                 "A typical forget() method should do the following:\n"
-                 "\n"
-                 "- call inherited::forget() to initialize the random number generator\n"
-                 "  with the 'seed' option\n"
-                 "\n"
-                 "- initialize the learner's parameters, using this random generator\n"
-                 "\n"
-                 "- stage = 0;\n"
-                 "\n"
-                 "This method is typically called by the build_() method, after it has\n"
-                 "finished setting up the parameters, and if it deemed useful to set or\n"
-                 "reset the learner in its fresh state.  (remember build may be called\n"
-                 "after modifying options that do not necessarily require the learner to\n"
-                 "restart from a fresh state...)  forget is also called by the\n"
-                 "setTrainingSet method, after calling build(), so it will generally be\n"
-                 "called TWICE during setTrainingSet!\n")));
-
-    declareMethod(
-        rmm, "train", &PLearner::train,
-        (BodyDoc("The role of the train method is to bring the learner up to\n"
-                 "stage==nstages, updating the stats with training costs measured on-line\n"
-                 "in the process.\n")));
-
-    declareMethod(
-        rmm, "resetInternalState", &PLearner::resetInternalState,
-        (BodyDoc("If the learner is a stateful one (inherits from StatefulLearner),\n"
-                 "this resets the internal state to its initial value; by default,\n"
-                 "this function does nothing.")));
-
-    declareMethod(
-        rmm, "computeOutput", &PLearner::remote_computeOutput,
-        (BodyDoc("On a trained learner, this computes the output from the input"),
-         ArgDoc ("input", "Input vector (should have width inputsize)"),
-         RetDoc ("Computed output (will have width outputsize)")));
-
-    declareMethod(
-        rmm, "use", &PLearner::remote_use,
-        (BodyDoc("Compute the output of a trained learner on every row of an\n"
-                 "input VMatrix.  The outputs are stored in a .pmat matrix\n"
-                 "under the specified filename."),
-         ArgDoc ("input_vmat", "VMatrix containing the inputs"),
-         ArgDoc ("output_pmat_fname", "Name of the .pmat to store the computed outputs")));
-
-    declareMethod(
-        rmm, "use2", &PLearner::remote_use2,
-        (BodyDoc("Compute the output of a trained learner on every row of an\n"
-                 "input VMatrix.  The outputs are returned as a matrix.\n"),
-         ArgDoc ("input_vmat", "VMatrix containing the inputs"),
-         RetDoc ("Matrix holding the computed outputs")));
-
-    declareMethod(
-        rmm, "computeInputOutputMat", &PLearner::computeInputOutputMat,
-        (BodyDoc("Returns a matrix which is a (horizontal) concatenation\n"
-                 "and the computed outputs.\n"),
-         ArgDoc ("inputs", "VMatrix containing the inputs"),
-         RetDoc ("Matrix holding the inputs+computed_outputs")));
-
-    declareMethod(
-        rmm, "computeInputOutputConfMat", &PLearner::computeInputOutputConfMat,
-        (BodyDoc("Return a Mat that is the contatenation of inputs, outputs, lower\n"
-                 "confidence bound, and upper confidence bound.  If confidence intervals\n"
-                 "cannot be computed for the learner, they are filled with MISSING_VALUE.\n"),
-         ArgDoc ("inputs", "VMatrix containing the inputs"),
-         ArgDoc ("probability", "Level at which the confidence intervals should be computed, "
-                                "e.g. 0.95."),
-         RetDoc ("Matrix holding the inputs+outputs+confidence-low+confidence-high")));
-
-    declareMethod(
-        rmm, "computeOutputAndCosts", &PLearner::remote_computeOutputAndCosts,
-        (BodyDoc("Compute both the output from the input, and the costs associated\n"
-                 "with the desired target.  The computed costs\n"
-                 "are returned in the order given by getTestCostNames()\n"),
-         ArgDoc ("input",  "Input vector (should have width inputsize)"),
-         ArgDoc ("target", "Target vector (for cost computation)"),
-         RetDoc ("- Vec containing output \n"
-                 "- Vec containing cost")));
-
-    declareMethod(
-        rmm, "computeCostsFromOutputs", &PLearner::remote_computeCostsFromOutputs,
-        (BodyDoc("Compute the costs from already-computed output.  The computed costs\n"
-                 "are returned in the order given by getTestCostNames()"),
-         ArgDoc ("input",  "Input vector (should have width inputsize)"),
-         ArgDoc ("output", "Output vector computed by previous call to computeOutput()"),
-         ArgDoc ("target", "Target vector"),
-         RetDoc ("The computed costs vector")));
-
-    declareMethod(
-        rmm, "computeCostsOnly", &PLearner::remote_computeCostsOnly,
-        (BodyDoc("Compute the costs only, without the outputs; for some learners, this\n"
-                 "may be more efficient than calling computeOutputAndCosts() if the\n"
-                 "outputs are not needed.  (The default implementation simply calls\n"
-                 "computeOutputAndCosts() and discards the output.)\n"),
-         ArgDoc ("input",  "Input vector (should have width inputsize)"),
-         ArgDoc ("target", "Target vector"),
-         RetDoc ("The computed costs vector")));
-
-    declareMethod(
-        rmm, "computeConfidenceFromOutput", &PLearner::remote_computeConfidenceFromOutput,
-        (BodyDoc("Compute a confidence intervals for the output, given the input and the\n"
-                 "pre-computed output (resulting from computeOutput or similar).  The\n"
-                 "probability level of the confidence interval must be specified.\n"
-                 "(e.g. 0.95).  Result is stored in a TVec of pairs low:high for each\n"
-                 "output variable (this is a \"box\" interval; it does not account for\n"
-                 "correlations among the output variables).\n"),
-         ArgDoc ("input",       "Input vector (should have width inputsize)"),
-         ArgDoc ("output",      "Output vector computed by previous call to computeOutput()"),
-         ArgDoc ("probability", "Level at which the confidence interval must be computed,\n"
-                                "e.g. 0.95\n"),
-         RetDoc ("Vector of pairs low:high giving, respectively, the lower-bound confidence\n"
-                 "and upper-bound confidence for each dimension of the output vector.  If this\n"
-                 "vector is empty, then confidence intervals could not be computed for the\n"
-                 "given learner.  Note that this is the PLearner default (not to compute\n"
-                 "any confidence intervals), but some learners such as LinearRegressor\n"
-                 "know how to compute them.")));
-
-    declareMethod(
-        rmm, "computeOutputCovMat", &PLearner::remote_computeOutputCovMat,
-        (BodyDoc("Version of computeOutput that is capable of returning an output matrix\n"
-                 "given an input matrix (set of output vectors), as well as the complete\n"
-                 "covariance matrix between the outputs.\n"
-                 "\n"
-                 "A separate covariance matrix is returned for each output dimension, but\n"
-                 "these matrices are allowed to share the same storage.  This would be\n"
-                 "the case in situations where the output covariance really depends only\n"
-                 "on the location of the training inputs, as in, e.g.,\n"
-                 "GaussianProcessRegressor.\n"
-                 "\n"
-                 "The default implementation is to repeatedly call computeOutput,\n"
-                 "followed by computeConfidenceFromOutput (sampled with probability\n"
-                 "Erf[1/(2*Sqrt(2))], to extract 1*stddev given by subtraction of the two\n"
-                 "intervals, then squaring the stddev to obtain the variance), thereby\n"
-                 "filling a diagonal output covariance matrix.  If\n"
-                 "computeConfidenceFromOutput returns 'false' (confidence intervals not\n"
-                 "supported), the returned covariance matrix is filled with\n"
-                 "MISSING_VALUE.\n"),
-         ArgDoc ("inputs", "Matrix containing the set of test points"),
-         RetDoc ("Two quantities are returned:\n"
-                 "- The matrix containing the expected output (as rows) for each input row.\n"
-                 "- A vector of covariance matrices between the outputs (one covariance\n"
-                 "  matrix per output dimension).\n")));
-    
-    declareMethod(
-        rmm, "batchComputeOutputAndConfidencePMat",
-        &PLearner::remote_batchComputeOutputAndConfidence,
-        (BodyDoc("Repeatedly calls computeOutput and computeConfidenceFromOutput with the\n"
-                 "rows of inputs.  Writes outputs_and_confidence rows (as a series of\n"
-                 "triples (output, low, high), one for each output).  The results are\n"
-                 "stored in a .pmat whose filename is passed as argument.\n"),
-         ArgDoc ("input_vmat",  "VMatrix containing the input rows"),
-         ArgDoc ("probability", "Level at which the confidence interval must be computed,\n"
-                                "e.g. 0.95\n"),
-         ArgDoc ("result_pmat_filename", "Filename where to store the results")));
-
-    declareMethod(
-        rmm, "getTestCostNames", &PLearner::getTestCostNames,
-        (BodyDoc("Return the name of the costs computed by computeCostsFromOutputs()\n"
-                 "and computeOutputAndCosts()"),
-         RetDoc ("List of test cost names")));
-
-    declareMethod(
-        rmm, "getTrainCostNames", &PLearner::getTrainCostNames,
-        (BodyDoc("Return the names of the objective costs that the train\n"
-                 "method computes and for which it updates the VecStatsCollector\n"
-                 "train_stats."),
-         RetDoc ("List of train cost names")));
-}
-
-////////////////////////////
-// setExperimentDirectory //
-////////////////////////////
-void PLearner::setExperimentDirectory(const PPath& the_expdir) 
-{ 
-    if(the_expdir=="")
-        expdir = "";
-    else
-    {
-        if(!force_mkdir(the_expdir))
-            PLERROR("In PLearner::setExperimentDirectory Could not create experiment directory %s",
-                    the_expdir.absolute().c_str());
-        expdir = the_expdir / "";
-    }
-}
-
-void PLearner::setTrainingSet(VMat training_set, bool call_forget)
-{ 
-    // YB: je ne suis pas sur qu'il soit necessaire de faire un build si la
-    // LONGUEUR du train_set a change?  les methodes non-parametriques qui
-    // utilisent la longueur devrait faire leur "resize" dans train, pas dans
-    // build.
-    bool training_set_has_changed = !train_set || !(train_set->looksTheSameAs(training_set));
-    train_set = training_set;
-    if (training_set_has_changed)
-    {
-        inputsize_ = train_set->inputsize();
-        targetsize_ = train_set->targetsize();
-        weightsize_ = train_set->weightsize();
-        if (forget_when_training_set_changes)
-            call_forget=true;
-    }
-    n_examples = train_set->length();
-    if (training_set_has_changed || call_forget)
-        build(); // MODIF FAITE PAR YOSHUA: sinon apres un setTrainingSet le build n'est pas complete dans un NNet train_set = training_set;
-    if (call_forget)
-        forget();
-
-    // Save the new training set if desired
-    if (save_trainingset_prefix != "" && expdir != "") {
-        static int trainingset_serial = 1;
-        PPath fname = expdir / (save_trainingset_prefix + "_trainset_" +
-                                tostring(trainingset_serial++) + ".pmat");
-        train_set->savePMAT(fname);
-    }
-}
-
-void PLearner::setValidationSet(VMat validset)
-{ validation_set = validset; }
-
-
-void PLearner::setTrainStatsCollector(PP<VecStatsCollector> statscol)
-{ train_stats = statscol; }
-
-
-int PLearner::inputsize() const
-{ 
-    if (inputsize_<0)
-        PLERROR("Must specify a training set before calling PLearner::inputsize()"); 
-    return inputsize_; 
-}
-
-int PLearner::targetsize() const 
-{ 
-    if(targetsize_ == -1) 
-        PLERROR("In PLearner::targetsize - 'targetsize_' is -1, either no training set has beeen specified or its sizes were not set properly");
-    return targetsize_; 
-}
-
-int PLearner::weightsize() const 
-{ 
-    if(weightsize_ == -1) 
-        PLERROR("In PLearner::weightsize - 'weightsize_' is -1, either no training set has beeen specified or its sizes were not set properly");
-    return weightsize_; 
-}
-
-////////////
-// build_ //
-////////////
-void PLearner::build_()
-{
-    if(expdir!="")
-    {
-        if(!force_mkdir(expdir))
-            PLWARNING("In PLearner Could not create experiment directory %s",expdir.c_str());
-        else
-            expdir = expdir.absolute() / "";
-    }
-    if (random_gen && seed_ != 0)
-        random_gen->manual_seed(seed_);
-}
-
-///////////
-// build //
-///////////
-void PLearner::build()
-{
-    inherited::build();
-    build_();
-}
-
-////////////
-// forget //
-////////////
-void PLearner::forget()
-{
-    if (random_gen && seed_ != 0)
-        random_gen->manual_seed(seed_);
-}
-
-int PLearner::nTestCosts() const 
-{ 
-    if(n_test_costs_<0)
-        n_test_costs_ = getTestCostNames().size(); 
-    return n_test_costs_;
-}
-
-int PLearner::nTrainCosts() const 
-{ 
-    if(n_train_costs_<0)
-        n_train_costs_ = getTrainCostNames().size();
-    return n_train_costs_; 
-}
-
-int PLearner::getTestCostIndex(const string& costname) const
-{
-    TVec<string> costnames = getTestCostNames();
-    for(int i=0; i<costnames.length(); i++)
-        if(costnames[i]==costname)
-            return i;
-    PLERROR("In PLearner::getTestCostIndex, No test cost named %s in this learner.\n"
-            "Available test costs are: %s", costname.c_str(),
-            tostring(costnames).c_str());
-    return -1;
-}
-
-int PLearner::getTrainCostIndex(const string& costname) const
-{
-    TVec<string> costnames = getTrainCostNames();
-    for(int i=0; i<costnames.length(); i++)
-        if(costnames[i]==costname)
-            return i;
-    PLERROR("In PLearner::getTrainCostIndex, No train cost named %s in this learner.\n"
-            "Available train costs are: %s", costname.c_str(), tostring(costnames).c_str());
-    return -1;
-}
-                                
-void PLearner::computeOutputAndCosts(const Vec& input, const Vec& target, 
-                                     Vec& output, Vec& costs) const
-{
-    computeOutput(input, output);
-    computeCostsFromOutputs(input, output, target, costs);
-}
-
-void PLearner::computeCostsOnly(const Vec& input, const Vec& target,  
-                                Vec& costs) const
-{
-    tmp_output.resize(outputsize());
-    computeOutputAndCosts(input, target, tmp_output, costs);
-}
-
-bool PLearner::computeConfidenceFromOutput(
-    const Vec& input, const Vec& output,
-    real probability,
-    TVec< pair<real,real> >& intervals) const
-{
-    // Default version does not know how to compute confidence intervals
-    intervals.resize(output.size());
-    intervals.fill(std::make_pair(MISSING_VALUE,MISSING_VALUE));  
-    return false;
-}
-
-void PLearner::computeOutputCovMat(const Mat& inputs, Mat& outputs,
-                                   TVec<Mat>& covariance_matrices) const
-{
-    PLASSERT( inputs.width() == inputsize() && outputsize() > 0 );
-    const int N = inputs.length();
-    const int M = outputsize();
-    outputs.resize(N, M);
-    covariance_matrices.resize(M);
-
-    bool has_confidence  = true;
-    bool init_covariance = 0;
-    Vec cur_input, cur_output;
-    TVec< pair<real,real> > intervals;
-    for (int i=0 ; i<N ; ++i) {
-        cur_input  = inputs(i);
-        cur_output = outputs(i);
-        computeOutput(cur_input, cur_output);
-        if (has_confidence) {
-            static const real probability = pl_erf(1. / (2*sqrt(2)));
-            has_confidence = computeConfidenceFromOutput(cur_input, cur_output,
-                                                         probability, intervals);
-            if (has_confidence) {
-                // Create the covariance matrices only once; filled with zeros
-                if (! init_covariance) {
-                    for (int j=0 ; j<M ; ++j)
-                        covariance_matrices[j] = Mat(N, N, 0.0);
-                    init_covariance = true;
-                }
-                
-                // Compute the variance for each output j, and set it on
-                // element i,i of the j-th covariance matrix
-                for (int j=0 ; j<M ; ++j) {
-                    float stddev = intervals[j].second - intervals[j].first;
-                    float var = stddev*stddev;
-                    covariance_matrices[j](i,i) = var;
-                }
-            }
-        }
-    }
-
-    // If confidence intervals are not supported, fill the covariance matrices
-    // with missing values
-    for (int j=0 ; j<M ; ++j)
-        covariance_matrices[j] = Mat(N, N, MISSING_VALUE);
-}
-
-void PLearner::batchComputeOutputAndConfidence(VMat inputs, real probability, VMat outputs_and_confidence) const
-{
-    Vec input(inputsize());
-    Vec output(outputsize());
-    int outsize = outputsize();
-    Vec output_and_confidence(3*outsize);
-    TVec< pair<real,real> > intervals;
-    int l = inputs.length();
-    for(int i=0; i<l; i++)
-    {
-        inputs->getRow(i,input);
-        computeOutput(input,output);
-        computeConfidenceFromOutput(input,output,probability,intervals);
-        for(int j=0; j<outsize; j++)
-        {
-            output_and_confidence[3*j] = output[j];
-            output_and_confidence[3*j+1] = intervals[j].first;
-            output_and_confidence[3*j+2] = intervals[j].second;
-        }
-        outputs_and_confidence->putOrAppendRow(i,output_and_confidence);
-    }
-}
-
-/////////
-// use //
-/////////
-void PLearner::use(VMat testset, VMat outputs) const
-{
-    int l = testset.length();
-    int w = testset.width();
-
-    TVec< PP<RemotePLearnServer> > servers;
-    if(nservers>0)
-        servers = PLearnService::instance().reserveServers(nservers);
-
-    if(servers.length()==0) 
-    { // sequential code      
-        Vec input;
-        Vec target;
-        real weight;
-        Vec output(outputsize());
-
-        PP<ProgressBar> pb;
-        if(report_progress)
-            pb = new ProgressBar("Using learner",l);
-
-        for(int i=0; i<l; i++)
-        {
-            testset.getExample(i, input, target, weight);
-            computeOutput(input, output);
-            outputs->putOrAppendRow(i,output);
-            if(pb)
-                pb->update(i);
-        }
-    }
-    else // parallel code
-    {
-        int n = servers.length(); // number of allocated servers
-        DBG_LOG << "PLearner::use parallel code using " << n << " servers" << endl;
-        for(int k=0; k<n; k++)  // send this object with objid 0
-            servers[k]->newObject(0, *this);
-        int chunksize = l/n;
-        if(chunksize*n<l)
-            ++chunksize;
-        if(chunksize*w>1000000) // max 1 Mega elements
-            chunksize = max(1,1000000/w);
-        Mat chunk(chunksize,w);
-        int send_i=0;
-        Mat outmat;
-        int receive_i = 0;
-        while(send_i<l)
-        {
-            for(int k=0; k<n && send_i<l; k++)
-            {
-                int actualchunksize = chunksize;
-                if(send_i+actualchunksize>l)
-                    actualchunksize = l-send_i;
-                chunk.resize(actualchunksize,w);
-                testset->getMat(send_i, 0, chunk);
-                VMat inputs(chunk);
-                inputs->copySizesFrom(testset);
-                DBG_LOG << "PLearner::use calling use2 remote method with chunk starting at " 
-                        << send_i << " of length " << actualchunksize << ":" << inputs << endl;
-                servers[k]->callMethod(0,"use2",inputs);
-                send_i += actualchunksize;
-            }
-            for(int k=0; k<n && receive_i<l; k++)
-            {
-                outmat.resize(0,0);
-                servers[k]->getResults(outmat);
-                for(int ii=0; ii<outmat.length(); ii++)
-                    outputs->putOrAppendRow(receive_i++,outmat(ii));
-            }
-        }
-        if(send_i!=l || receive_i!=l)
-            PLERROR("In PLearn::use parallel execution failed to complete successfully.");
-    }
-}
-
-VMat PLearner::processDataSet(VMat dataset) const
-{
-    // PLearnerOutputVMatrix does exactly this.
-    return new PLearnerOutputVMatrix(dataset, this);
-}
-
-
-TVec<string> PLearner::getOutputNames() const
-{
-    int n = outputsize();
-    TVec<string> outnames(n);
-    for(int k=0; k<n; k++)
-        outnames[k] = "out" + tostring(k);
-    return outnames;
-}
-
-////////////////
-// useOnTrain //
-////////////////
-void PLearner::useOnTrain(Mat& outputs) const {
-    VMat train_output(outputs);
-    use(train_set, train_output);
-}
-
-//////////
-// test //
-//////////
-void PLearner::test(VMat testset, PP<VecStatsCollector> test_stats, 
-                    VMat testoutputs, VMat testcosts) const
-{
-    int len = testset.length();
-    Vec input;
-    Vec target;
-    real weight;
-
-    Vec output(outputsize());
-    Vec costs(nTestCosts());
-
-    PP<ProgressBar> pb;
-    if (report_progress) 
-        pb = new ProgressBar("Testing learner", len);
-
-    if (len == 0) {
-        // Empty test set: we give -1 cost arbitrarily.
-        costs.fill(-1);
-        test_stats->update(costs);
-    }
-
-    for (int i = 0; i < len; i++)
-    {
-        testset.getExample(i, input, target, weight);
-      
-        // Always call computeOutputAndCosts, since this is better
-        // behaved with stateful learners
-        computeOutputAndCosts(input,target,output,costs);
-      
-        if (testoutputs)
-            testoutputs->putOrAppendRow(i, output);
-
-        if (testcosts)
-            testcosts->putOrAppendRow(i, costs);
-
-        if (test_stats)
-            test_stats->update(costs, weight);
-
-        if (report_progress)
-            pb->update(i);
-    }
-
-}
-
-///////////////
-// initTrain //
-///////////////
-bool PLearner::initTrain()
-{
-    string warn_msg = "In PLearner::trainingCheck (called by '" +
-        this->classname() + "') - ";
-    
-    // Check 'nstages' is valid.
-    if (nstages < 0) {
-        PLWARNING((warn_msg + "Option nstages (set to " + tostring(nstages)
-                    + ") must be non-negative").c_str());
-        return false;
-    }
-
-    // Check we actually need to train.
-    if (stage == nstages) {
-        if (verbosity >= 1)
-            PLWARNING((warn_msg + "The learner is already trained").c_str());
-        return false;
-    }
-
-    if (stage > nstages) {
-        if (verbosity >= 1) {
-            string msg = warn_msg + "Learner was already trained up to stage "
-                + tostring(stage) + ", but asked to train up to nstages="
-                + tostring(nstages) + ": it will be reverted to stage 0 and "
-                                      "trained again";
-            PLWARNING(msg.c_str());
-        }
-        forget();
-    }
-
-    // Check there is a training set.
-    if (!train_set) {
-        if (verbosity >= 1)
-            PLWARNING((warn_msg + "No training set specified").c_str());
-        return false;
-    }
-
-    // Initialize train_stats if needed.
-    if (!train_stats)
-        train_stats = new VecStatsCollector();
-
-    // Everything is fine.
-    return true;
-}
-
-////////////////////////
-// resetInternalState //
-////////////////////////
-void PLearner::resetInternalState()
-{ }
-
-bool PLearner::isStatefulLearner() const
-{
-    return false;
-}
-
-
-//#####  computeInputOutputMat  ###############################################
-
-Mat PLearner::computeInputOutputMat(VMat inputs) const
-{
-    int l = inputs.length();
-    int nin = inputsize();
-    int nout = outputsize();
-    Mat m(l, nin+nout);
-    for(int i=0; i<l; i++)
-    {
-        Vec v = m(i);
-        Vec invec = v.subVec(0,nin);
-        Vec outvec = v.subVec(nin,nout);
-        inputs->getRow(i, invec);
-        computeOutput(invec, outvec);
-    }
-    return m;
-}
-
-
-//#####  computeInputOutputConfMat  ###########################################
-
-Mat PLearner::computeInputOutputConfMat(VMat inputs, real probability) const
-{
-    int l = inputs.length();
-    int nin = inputsize();
-    int nout = outputsize();
-    Mat m(l, nin+3*nout);
-    TVec< pair<real,real> > intervals;
-    for(int i=0; i<l; i++)
-    {
-        Vec v = m(i);
-        Vec invec   = v.subVec(0,nin);
-        Vec outvec  = v.subVec(nin,nout);
-        Vec lowconf = v.subVec(nin+nout, nout);
-        Vec hiconf  = v.subVec(nin+2*nout, nout);
-        inputs->getRow(i, invec);
-        computeOutput(invec, outvec);
-        bool conf_avail = computeConfidenceFromOutput(invec, outvec, probability,
-                                                      intervals);
-        if (conf_avail) {
-            for (int i=0, n=intervals.size() ; i<n ; ++i) {
-                lowconf[i] = intervals[i].first;
-                hiconf[i]  = intervals[i].second;
-            }
-        }
-        else {
-            lowconf << MISSING_VALUE;
-            hiconf  << MISSING_VALUE;
-        }
-    }
-    return m;
-}
-
-
-//! Version of computeOutput that returns a result by value
-Vec PLearner::remote_computeOutput(const Vec& input) const
-{
-    tmp_output.resize(outputsize());
-    computeOutput(input, tmp_output);
-    return tmp_output;
-}
-
-//! Version of use that's called by RMI
-void PLearner::remote_use(VMat inputs, string output_fname) const
-{
-    VMat outputs = new FileVMatrix(output_fname, inputs.length(), outputsize());
-    use(inputs,outputs);
-}
-
-//! Version of use2 that's called by RMI
-Mat PLearner::remote_use2(VMat inputs) const
-{
-    Mat outputs(inputs.length(), outputsize());
-    use(inputs,outputs);
-    return outputs;
-}
-    
-//! Version of computeOutputAndCosts that's called by RMI
-
-tuple<Vec,Vec> PLearner::remote_computeOutputAndCosts(const Vec& input, const Vec& target) const
-{
-    tmp_output.resize(outputsize());
-    Vec costs(nTestCosts());
-    computeOutputAndCosts(input,target,tmp_output,costs);
-    return make_tuple(tmp_output, costs);
-}
-
-//! Version of computeCostsFromOutputs that's called by RMI
-Vec PLearner::remote_computeCostsFromOutputs(const Vec& input, const Vec& output,
-                                             const Vec& target) const
-{
-    Vec costs(nTestCosts());
-    computeCostsFromOutputs(input,output,target,costs);
-    return costs;
-}
-
-//! Version of computeCostsOnly that's called by RMI
-Vec PLearner::remote_computeCostsOnly(const Vec& input, const Vec& target) const
-{
-    Vec costs(nTestCosts());
-    computeCostsOnly(input,target,costs);
-    return costs;
-}
-
-//! Version of computeConfidenceFromOutput that's called by RMI
-TVec< pair<real,real> >
-PLearner::remote_computeConfidenceFromOutput(const Vec& input, const Vec& output,
-                                             real probability) const
-{
-    TVec< pair<real,real> > intervals(output.length());
-    bool ok = computeConfidenceFromOutput(input, output, probability, intervals);
-    if (ok)
-        return intervals;
-    else
-        return TVec< pair<real,real> >();
-}
-
-//! Version of computeOutputCovMat that's called by RMI
-tuple<Mat, TVec<Mat> >
-PLearner::remote_computeOutputCovMat(const Mat& inputs) const
-{
-    Mat outputs;
-    TVec<Mat> covmat;
-    computeOutputCovMat(inputs, outputs, covmat);
-    return make_tuple(outputs, covmat);
-}
-
-//! Version of batchComputeOutputAndConfidence that's called by RMI
-void PLearner::remote_batchComputeOutputAndConfidence(VMat inputs, real probability,
-                                                      string pmat_fname) const
-{
-    TVec<string> fieldnames;
-    for(int j=0; j<outputsize(); j++)
-    {
-        fieldnames.append("output_"+tostring(j));
-        fieldnames.append("low_"+tostring(j));
-        fieldnames.append("high_"+tostring(j));
-    }
-    VMat out_and_conf = new FileVMatrix(pmat_fname,inputs.length(),fieldnames);
-    batchComputeOutputAndConfidence(inputs, probability, out_and_conf);
-}
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3.1/plearn_learners/generic/PLearner.cc (from rev 6569, trunk/plearn_learners/generic/PLearner.cc)

Deleted: tags/OPAL-3.0.3.1/plearn_learners/regressors/GaussianProcessRegressor.cc
===================================================================
--- trunk/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-11 20:48:22 UTC (rev 6568)
+++ tags/OPAL-3.0.3.1/plearn_learners/regressors/GaussianProcessRegressor.cc	2007-01-12 19:39:21 UTC (rev 6571)
@@ -1,664 +0,0 @@
-// -*- C++ -*-
-
-// GaussianProcessRegressor.cc
-//
-// Copyright (C) 2006 Nicolas Chapados 
-// 
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-// 
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-// 
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-// 
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-// 
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-// 
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************      
-   * $Id: .pyskeleton_header 544 2003-09-01 00:05:31Z plearner $ 
-   ******************************************************* */
-
-// Authors: Nicolas Chapados
-
-/*! \file GaussianProcessRegressor.cc */
-
-#define PL_LOG_MODULE_NAME "GaussianProcessRegressor"
-
-// From PLearn
-#include <plearn/base/stringutils.h>
-#include <plearn/io/pl_log.h>
-#include <plearn/vmat/ExtendedVMatrix.h>
-#include <plearn/math/pl_erf.h>
-#include <plearn/var/GaussianProcessNLLVariable.h>
-#include <plearn/var/ObjectOptionVariable.h>
-#include <plearn/opt/Optimizer.h>
-
-#include "GaussianProcessRegressor.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    GaussianProcessRegressor,
-    "Kernelized version of linear ridge regression.",
-    "Given a kernel K(x,y) = phi(x)'phi(y), where phi(x) is the projection of a\n"
-    "vector x into feature space, this class implements a version of the ridge\n"
-    "estimator, giving the prediction at x as\n"
-    "\n"
-    "    f(x) = k(x)'(M + lambda I)^-1 y,\n"
-    "\n"
-    "where x is the test vector where to estimate the response, k(x) is the\n"
-    "vector of kernel evaluations between the test vector and the elements of\n"
-    "the training set, namely\n"
-    "\n"
-    "    k(x) = (K(x,x1), K(x,x2), ..., K(x,xN))',\n"
-    "\n"
-    "M is the Gram Matrix on the elements of the training set, i.e. the matrix\n"
-    "where the element (i,j) is equal to K(xi, xj), lambda is the VARIANCE of\n"
-    "the observation noise (and can be interpreted as a weight decay\n"
-    "coefficient), and y is the vector of training-set targets.\n"
-    "\n"
-    "The uncertainty in a prediction can be computed by calling\n"
-    "computeConfidenceFromOutput.  Furthermore, if desired, this learner allows\n"
-    "optimization of the kernel hyperparameters by direct optimization of the\n"
-    "marginal likelihood w.r.t. the hyperparameters.  This mechanism relies on a\n"
-    "user-provided Optimizer (see the 'optimizer' option) and does not rely on\n"
-    "the PLearn HyperLearner system.\n"
-    "\n"
-    "GaussianProcessRegressor produces the following train costs:\n"
-    "\n"
-    "- \"nmll\" : the negative marginal log-likelihood on the training set.\n"
-    "- \"mse\"  : the mean-squared error on the training set (by convention,\n"
-    "           divided by two)\n"
-    "\n"
-    "and the following test costs:\n"
-    "\n"
-    "- \"nll\" : the negative log-likelihood of the test example under the\n"
-    "          predictive distribution.  Available only if the option\n"
-    "          'compute_confidence' is true.\n"
-    "- \"mse\" : the squared error of the test example with respect to the\n"
-    "          predictive mean (by convention, divided by two).\n"
-    "\n"
-    "The disadvantage of this learner is that its training time is O(N^3) in the\n"
-    "number of training examples (due to the matrix inversion).  When saving the\n"
-    "learner, the training set inputs must be saved, along with an additional\n"
-    "matrix of length number-of-training-examples, and width number-of-targets.\n"
-    );
-
-GaussianProcessRegressor::GaussianProcessRegressor() 
-    : m_weight_decay(0.0),
-      m_include_bias(true),
-      m_compute_confidence(false),
-      m_confidence_epsilon(1e-8)
-{ }
-
-
-void GaussianProcessRegressor::declareOptions(OptionList& ol)
-{
-    //#####  Build Options  ###################################################
-    
-    declareOption(
-        ol, "kernel", &GaussianProcessRegressor::m_kernel,
-        OptionBase::buildoption,
-        "Kernel to use for the computation.  This must be a similarity kernel\n"
-        "(i.e. closer vectors give higher kernel evaluations).");
-
-    declareOption(
-        ol, "weight_decay", &GaussianProcessRegressor::m_weight_decay,
-        OptionBase::buildoption,
-        "Weight decay coefficient (default = 0)");
-
-    declareOption(
-        ol, "include_bias", &GaussianProcessRegressor::m_include_bias,
-        OptionBase::buildoption,
-        "Whether to include a bias term in the regression (true by default)\n"
-        "The effect of this option is NOT to prepend a column of 1 to the inputs\n"
-        "(which has often no effect for GP regression), but to estimate a\n"
-        "separate mean of the targets, perform the GP regression on the\n"
-        "zero-mean targets, and add it back when computing the outputs.\n");
-
-    declareOption(
-        ol, "compute_confidence", &GaussianProcessRegressor::m_compute_confidence,
-        OptionBase::buildoption,
-        "Whether to perform the additional train-time computations required\n"
-        "to compute confidence intervals.  This includes computing a separate\n"
-        "inverse of the Gram matrix.  Specification of this option is necessary\n"
-        "for calling both computeConfidenceFromOutput and computeOutputCovMat.\n");
-
-    declareOption(
-        ol, "confidence_epsilon", &GaussianProcessRegressor::m_confidence_epsilon,
-        OptionBase::buildoption,
-        "Small regularization to be added post-hoc to the computed output\n"
-        "covariance matrix and confidence intervals; this is mostly used as a\n"
-        "disaster prevention device, to avoid negative predictive variance\n");
-    
-    declareOption(
-        ol, "hyperparameters", &GaussianProcessRegressor::m_hyperparameters,
-        OptionBase::buildoption,
-        "List of hyperparameters to optimize.  They must be specified in the\n"
-        "form \"option-name\":initial-value, where 'option-name' is the name\n"
-        "of an option to set within the Kernel object (the array-index form\n"
-        "'option[i]' is supported), and 'initial-value' is the\n"
-        "(PLearn-serialization string representation) for starting point for the\n"
-        "optimization.  Currently, the hyperparameters are constrained to be\n"
-        "scalars.\n");
-
-    declareOption(
-        ol, "ARD_hyperprefix_initval",
-        &GaussianProcessRegressor::m_ARD_hyperprefix_initval,
-        OptionBase::buildoption,
-        "If the kernel support automatic relevance determination (ARD; e.g.\n"
-        "SquaredExponentialARDKernel), the list of hyperparameters corresponding\n"
-        "to each input can be created automatically by giving an option prefix\n"
-        "and an initial value.  The ARD options are created to have the form\n"
-        "\n"
-        "   'prefix[0]', 'prefix[1]', 'prefix[N-1]'\n"
-        "\n"
-        "where N is the number of inputs.  This option is useful when the\n"
-        "dataset inputsize is not (easily) known ahead of time. \n");
-    
-    declareOption(
-        ol, "optimizer", &GaussianProcessRegressor::m_optimizer,
-        OptionBase::buildoption,
-        "Specification of the optimizer to use for train-time hyperparameter\n"
-        "optimization.  A ConjGradientOptimizer should be an adequate choice.\n");
-
-
-    //#####  Learnt Options  ##################################################
-
-    declareOption(
-        ol, "alpha", &GaussianProcessRegressor::m_alpha,
-        OptionBase::learntoption,
-        "Vector of learned parameters, determined from the equation\n"
-        "    (M + lambda I)^-1 y");
-
-    declareOption(
-        ol, "gram_inverse", &GaussianProcessRegressor::m_gram_inverse,
-        OptionBase::learntoption,
-        "Inverse of the Gram matrix, used to compute confidence intervals (must\n"
-        "be saved since the confidence intervals are obtained from the equation\n"
-        "\n"
-        "  sigma^2 = k(x,x) - k(x)'(M + lambda I)^-1 k(x)\n");
-
-    declareOption(
-        ol, "target_mean", &GaussianProcessRegressor::m_target_mean,
-        OptionBase::learntoption,
-        "Mean of the targets, if the option 'include_bias' is true");
-    
-    declareOption(
-        ol, "training_inputs", &GaussianProcessRegressor::m_training_inputs,
-        OptionBase::learntoption,
-        "Saved version of the training set, which must be kept along for\n"
-        "carrying out kernel evaluations with the test point");
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-void GaussianProcessRegressor::build_()
-{
-    if (! m_kernel)
-        PLERROR("GaussianProcessRegressor::build_: 'kernel' option must be specified");
-
-    if (! m_kernel->is_symmetric)
-        PLERROR("GaussianProcessRegressor::build_: the kernel (%s) must be symmetric",
-                m_kernel->classname().c_str());
-    
-    // If we are reloading the model, set the training inputs into the kernel
-    if (m_training_inputs)
-        m_kernel->setDataForKernelMatrix(m_training_inputs);
-
-    // If we specified hyperparameters without an optimizer, complain.
-    // (It is mildly legal to specify an optimizer without hyperparameters;
-    // this does nothing).
-    if (m_hyperparameters.size() > 0 && ! m_optimizer)
-        PLERROR("GaussianProcessRegressor::build_: 'hyperparameters' are specified "
-                "but no 'optimizer'; an optimizer is required in order to carry out "
-                "hyperparameter optimization");
-
-    if (m_confidence_epsilon < 0)
-        PLERROR("GaussianProcessRegressor::build_: 'confidence_epsilon' must be non-negative");
-}
-
-// ### Nothing to add here, simply calls build_
-void GaussianProcessRegressor::build()
-{
-    inherited::build();
-    build_();
-}
-
-
-void GaussianProcessRegressor::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(m_kernel,                     copies);
-    deepCopyField(m_hyperparameters,            copies);
-    deepCopyField(m_optimizer,                  copies);
-    deepCopyField(m_alpha,                      copies);
-    deepCopyField(m_gram_inverse,               copies);
-    deepCopyField(m_target_mean,                copies);
-    deepCopyField(m_training_inputs,            copies);
-    deepCopyField(m_kernel_evaluations,         copies);
-    deepCopyField(m_gram_inverse_product,       copies);
-    deepCopyField(m_intervals,                  copies);
-    deepCopyField(m_gram_traintest_inputs,      copies);
-    deepCopyField(m_gram_inv_traintest_product, copies);
-    deepCopyField(m_sigma_reductor,             copies);
-}
-
-
-//#####  setTrainingSet  ######################################################
-
-void GaussianProcessRegressor::setTrainingSet(VMat training_set, bool call_forget)
-{
-    PLASSERT( training_set );
-    int inputsize = training_set->inputsize() ;
-    if (inputsize < 0)
-        PLERROR("GaussianProcessRegressor::setTrainingSet: the training set inputsize "
-                "must be specified (current value = %d)", inputsize);
-
-    // Convert to a real matrix in order to make saving it saner
-    m_training_inputs = training_set.subMatColumns(0, inputsize).toMat();
-    inherited::setTrainingSet(training_set, call_forget);
-}
-
-
-//#####  outputsize  ##########################################################
-
-int GaussianProcessRegressor::outputsize() const
-{
-    return targetsize();
-}
-
-
-//#####  forget  ##############################################################
-
-void GaussianProcessRegressor::forget()
-{
-    inherited::forget();
-    if (m_optimizer)
-        m_optimizer->reset();
-    m_alpha.resize(0,0);
-    m_target_mean.resize(0);
-    m_gram_inverse.resize(0,0);
-    stage = 0;
-}
-    
-
-//#####  train  ###############################################################
-
-void GaussianProcessRegressor::train()
-{
-    // This generic PLearner method does a number of standard stuff useful for
-    // (almost) any learner, and return 'false' if no training should take
-    // place. See PLearner.h for more details.
-    if (!initTrain())
-        return;
-
-    PLASSERT( m_kernel );
-    if (! train_set || ! m_training_inputs)
-        PLERROR("GaussianProcessRegressor::train: the training set must be specified");
-    int trainlength = train_set->length();
-    int inputsize   = train_set->inputsize() ;
-    int targetsize  = train_set->targetsize();
-    int weightsize  = train_set->weightsize();
-    if (inputsize  < 0 || targetsize < 0 || weightsize < 0)
-        PLERROR("GaussianProcessRegressor::train: inconsistent inputsize/targetsize/weightsize "
-                "(%d/%d/%d) in training set", inputsize, targetsize, weightsize);
-    if (weightsize > 0)
-        PLERROR("GaussianProcessRegressor::train: observations weights are not currently supported");
-
-    // Subtract the mean if we require it
-    Mat targets(trainlength, targetsize);
-    train_set.subMatColumns(inputsize, targetsize)->getMat(0,0,targets);
-    if (m_include_bias) {
-        m_target_mean.resize(targets.width());
-        columnMean(targets, m_target_mean);
-        targets -= m_target_mean;
-    }
-
-    // Optimize hyperparameters
-    PP<GaussianProcessNLLVariable> nll = hyperOptimize(m_training_inputs, targets);
-    PLASSERT( nll );
-    
-    // Compute parameters
-    nll->fprop();
-    m_alpha = nll->alpha();
-    m_gram_inverse = nll->gramInverse();
-
-    // Compute train MSE, as 1/(2N) * dot(z,z), with z=K*alpha - y
-    Mat residuals(m_alpha.length(), m_alpha.width());
-    product(residuals, nll->gram(), m_alpha);
-    residuals -= targets;
-    real mse = dot(residuals, residuals) / (2 * trainlength);
-    
-    // And accumulate some statistics
-    Vec costs(2);
-    costs[0] = nll->value[0];
-    costs[1] = mse;
-    getTrainStatsCollector()->update(costs);
-    MODULE_LOG << "Train NLL: " << costs[0] << endl;
-}
-
-
-//#####  computeOutput  #######################################################
-
-void GaussianProcessRegressor::computeOutput(const Vec& input, Vec& output) const
-{
-    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
-    PLASSERT( m_alpha.width()  == output.size() );
-    PLASSERT( m_alpha.length() == m_training_inputs.length() );
-    PLASSERT( input.size()     == m_training_inputs.width()  );
-
-    m_kernel_evaluations.resize(m_alpha.length());
-    computeOutputAux(input, output, m_kernel_evaluations);
-}
-
-
-void GaussianProcessRegressor::computeOutputAux(
-    const Vec& input, Vec& output, Vec& kernel_evaluations) const
-{
-    m_kernel->evaluate_all_x_i(input, kernel_evaluations);
-
-    // Finally compute k(x,x_i) * (M + \lambda I)^-1 y
-    product(Mat(1, output.size(), output),
-            Mat(1, kernel_evaluations.size(), kernel_evaluations),
-            m_alpha);
-
-    if (m_include_bias)
-        output += m_target_mean;
-}
-
-
-//#####  computeCostsFromOutputs  #############################################
-
-void GaussianProcessRegressor::computeCostsFromOutputs(const Vec& input, const Vec& output, 
-                                                       const Vec& target, Vec& costs) const
-{
-    costs.resize(2);
-
-    // NLL cost is the NLL of the target under the predictive distribution
-    // (centered at predictive mean, with variance obtainable from the
-    // confidence bounds).  HOWEVER, to obain it, we have to be able to compute
-    // the confidence bounds.  If impossible, simply set missing-value for the
-    // NLL cost.
-    if (m_compute_confidence) {
-        static const float PROBABILITY = pl_erf(1. / (2*sqrt(2)));  // 0.5 stddev
-        bool confavail = computeConfidenceFromOutput(input, output, PROBABILITY,
-                                                     m_intervals);
-        assert( confavail && m_intervals.size() == output.size() &&
-                output.size() == target.size() );
-        static const real LN_2PI_OVER_2 = pl_log(2*M_PI) / 2.0;
-        real nll = 0;
-        for (int i=0, n=output.size() ; i<n ; ++i) {
-            real sigma = m_intervals[i].second - m_intervals[i].first;
-            sigma = max(sigma, 1e-15);        // Very minor regularization
-            real diff = target[i] - output[i];
-            nll += diff*diff / (2.*sigma*sigma) + pl_log(sigma) + LN_2PI_OVER_2;
-        }
-        costs[0] = nll;
-    }
-    else
-        costs[0] = MISSING_VALUE;
-    
-    real squared_loss = 0.5*powdistance(output,target);
-    costs[1] = squared_loss;
-}     
-
-
-//#####  computeConfidenceFromOutput  #########################################
-
-bool GaussianProcessRegressor::computeConfidenceFromOutput(
-    const Vec& input, const Vec& output, real probability,
-    TVec< pair<real,real> >& intervals) const
-{
-    if (! m_compute_confidence) {
-        PLWARNING("GaussianProcessRegressor::computeConfidenceFromOutput: the option\n"
-                  "'compute_confidence' must be true in order to compute valid\n"
-                  "condidence intervals");
-        return false;
-    }
-
-    // BIG-BIG assumption: assume that computeOutput has just been called and
-    // that m_kernel_evaluations contains the right stuff.
-    PLASSERT( m_kernel && m_gram_inverse.isNotNull() );
-    real base_sigma_sq = m_kernel(input, input);
-    m_gram_inverse_product.resize(m_kernel_evaluations.size());
-    product(m_gram_inverse_product, m_gram_inverse, m_kernel_evaluations);
-    real sigma_reductor = dot(m_gram_inverse_product, m_kernel_evaluations);
-    real sigma = sqrt(max(0., base_sigma_sq - sigma_reductor + m_confidence_epsilon));
-
-    // two-tailed
-    const real multiplier = gauss_01_quantile((1+probability)/2);
-    real half_width = multiplier * sigma;
-    intervals.resize(output.size());
-    for (int i=0, n=output.size() ; i<n ; ++i)
-        intervals[i] = std::make_pair(output[i] - half_width,
-                                      output[i] + half_width);
-    return true;
-}
-
-
-//#####  computeOutputCovMat  #################################################
-
-void GaussianProcessRegressor::computeOutputCovMat(
-    const Mat& inputs, Mat& outputs, TVec<Mat>& covariance_matrices) const
-{
-    PLASSERT( m_kernel && m_alpha.isNotNull() && m_training_inputs );
-    PLASSERT( m_alpha.width()  == outputsize() );
-    PLASSERT( m_alpha.length() == m_training_inputs.length() );
-    PLASSERT( inputs.width()   == m_training_inputs.width()  );
-    PLASSERT( inputs.width()   == inputsize() );
-    const int N = inputs.length();
-    const int M = outputsize();
-    const int T = m_training_inputs.length();
-    outputs.resize(N, M);
-    covariance_matrices.resize(M);
-
-    // Preallocate space for the covariance matrix, and since all outputs share
-    // the same matrix, copy it into the remaining elements of
-    // covariance_matrices
-    Mat& covmat = covariance_matrices[0];
-    covmat.resize(N, N);
-    for (int j=1 ; j<M ; ++j)
-        covariance_matrices[j] = covmat;
-
-    // Start by computing the matrix of kernel evaluations between the train
-    // and test outputs, and compute the output
-    m_gram_traintest_inputs.resize(N, T);
-    for (int i=0 ; i<N ; ++i) {
-        Vec cur_traintest_kereval = m_gram_traintest_inputs(i);
-        Vec cur_output = outputs(i);
-        computeOutputAux(inputs(i), cur_output, cur_traintest_kereval);
-    }
-
-    // Next compute the kernel evaluations between the test inputs; more or
-    // less lifted from Kernel.cc ==> must see with Olivier how to better
-    // factor this code
-    Mat& K = covmat;
-    K.resize(N,N);
-    const int mod = K.mod();
-    real Kij;
-    real* Ki;
-    real* Kji;
-    for (int i=0 ; i<N ; ++i) {
-        Ki  = K[i];
-        Kji = &K[0][i];
-        const Vec& cur_input_i = inputs(i);
-        for (int j=0 ; j<=i ; ++j, Kji += mod) {
-            Kij = m_kernel->evaluate(cur_input_i, inputs(j));
-            *Ki++ = Kij;
-            if (j<i)
-                *Kji = Kij;    // Assume symmetry, checked at build
-        }
-    }
-
-    // The predictive covariance matrix is (c.f. Rasmussen and Williams):
-    //
-    //    cov(f*) = K(X*,X*) - K(X*,X) [K(X,X) + sigma*I]^-1 K(X,X*)
-    //
-    // where X are the training inputs, and X* are the test inputs.
-    m_gram_inv_traintest_product.resize(T,N);
-    m_sigma_reductor.resize(N,N);
-    productTranspose(m_gram_inv_traintest_product, m_gram_inverse,
-                     m_gram_traintest_inputs);
-    product(m_sigma_reductor, m_gram_traintest_inputs,
-            m_gram_inv_traintest_product);
-    covmat -= m_sigma_reductor;
-
-    // As a preventive measure, never output negative variance, even though
-    // this does not garantee the non-negative-definiteness of the matrix
-    for (int i=0 ; i<N ; ++i)
-        covmat(i,i) = max(0.0, covmat(i,i) + m_confidence_epsilon);
-}
-
-
-//#####  get*CostNames  #######################################################
-
-TVec<string> GaussianProcessRegressor::getTestCostNames() const
-{
-    TVec<string> c(2);
-    c[0] = "nll";
-    c[1] = "mse";
-    return c;
-}
-
-
-TVec<string> GaussianProcessRegressor::getTrainCostNames() const
-{
-    TVec<string> c(2);
-    c[0] = "nmll";
-    c[1] = "mse";
-    return c;
-}
-
-
-//#####  hyperOptimize  #######################################################
-
-PP<GaussianProcessNLLVariable>
-GaussianProcessRegressor::hyperOptimize(const Mat& inputs, const Mat& targets)
-{
-    // If there are no hyperparameters or optimizer, just create a simple
-    // variable and return it right away.
-    if (! m_optimizer || (m_hyperparameters.size() == 0 &&
-                          m_ARD_hyperprefix_initval.first.empty()) )
-    {
-        return new GaussianProcessNLLVariable(
-            m_kernel, m_weight_decay, inputs, targets,
-            TVec<string>(), VarArray(), m_compute_confidence);
-    }
-
-    // Otherwise create Vars that wrap each hyperparameter
-    const int numhyper  = m_hyperparameters.size();
-    const int numinputs = ( ! m_ARD_hyperprefix_initval.first.empty() ?
-                            inputsize() : 0 );
-    VarArray     hyperparam_vars (numhyper + numinputs);
-    TVec<string> hyperparam_names(numhyper + numinputs);
-    int i;
-    for (i=0 ; i<numhyper ; ++i) {
-        hyperparam_names[i] = m_hyperparameters[i].first;
-        hyperparam_vars [i] = new ObjectOptionVariable(
-            (Kernel*)m_kernel, m_hyperparameters[i].first, m_hyperparameters[i].second);
-        hyperparam_vars[i]->setName(m_hyperparameters[i].first);
-    }
-
-    // If specified, create the Vars for automatic relevance determination
-    string& ARD_name = m_ARD_hyperprefix_initval.first;
-    string& ARD_init = m_ARD_hyperprefix_initval.second;
-    if (! ARD_name.empty()) {
-        // Small hack to ensure the ARD vector in the kernel has proper size
-        Vec init(numinputs, lexical_cast<double>(ARD_init));
-        m_kernel->changeOption(ARD_name, tostring(init, PStream::plearn_ascii));
-        
-        for (int j=0 ; j<numinputs ; ++j, ++i) {
-            hyperparam_names[i] = ARD_name + '[' + tostring(j) + ']';
-            hyperparam_vars [i] = new ObjectOptionVariable(
-                (Kernel*)m_kernel, hyperparam_names[i], ARD_init);
-            hyperparam_vars [i]->setName(hyperparam_names[i]);
-        }
-    }
-
-    // Create the cost-function variable
-    PP<GaussianProcessNLLVariable> nll = new GaussianProcessNLLVariable(
-        m_kernel, m_weight_decay, inputs, targets, hyperparam_names,
-        hyperparam_vars, true);
-    nll->setName("GaussianProcessNLLVariable");
-
-    // Some logging about the initial values
-    logVarray(hyperparam_vars, "Hyperparameter initial values:");
-    
-    // And optimize for nstages
-    m_optimizer->setToOptimize(hyperparam_vars, (Variable*)nll);
-    m_optimizer->build();
-    PP<ProgressBar> pb(
-        report_progress? new ProgressBar("Training GaussianProcessRegressor "
-                                         "from stage " + tostring(stage) + " to stage " +
-                                         tostring(nstages), nstages-stage)
-        : 0);
-    bool early_stopping = false;
-    PP<VecStatsCollector> statscol = new VecStatsCollector;
-    for (const int initial_stage = stage ; !early_stopping && stage < nstages
-             ; ++stage)
-    {
-        if (pb)
-            pb->update(stage - initial_stage);
-
-        statscol->forget();
-        early_stopping = m_optimizer->optimizeN(*statscol);
-        statscol->finalize();
-    }
-    pb = 0;                                  // Finish progress bar right now
-
-    // Some logging about the final values
-    logVarray(hyperparam_vars, "Hyperparameter final values:");
-    return nll;
-}
-
-
-//#####  logVarray  ###########################################################
-
-void GaussianProcessRegressor::logVarray(const VarArray& varr,
-                                         const string& title)
-{
-    string entry = title + '\n';
-    for (int i=0, n=varr.size() ; i<n ; ++i) {
-        entry += right(varr[i]->getName(), 35) + ": " + tostring(varr[i]->value[0]);
-        if (i < n-1)
-            entry += '\n';
-    }
-    MODULE_LOG << entry << endl; 
-}
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Copied: tags/OPAL-3.0.3.1/plearn_learners/regressors/GaussianProcessRegressor.cc (from rev 6569, trunk/plearn_learners/regressors/GaussianProcessRegressor.cc)



From chapados at mail.berlios.de  Sun Jan 14 21:01:58 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Sun, 14 Jan 2007 21:01:58 +0100
Subject: [Plearn-commits] r6572 - in trunk: plearn/misc
	python_modules/plearn/parallel
Message-ID: <200701142001.l0EK1wfO001734@sheep.berlios.de>

Author: chapados
Date: 2007-01-14 21:01:57 +0100 (Sun, 14 Jan 2007)
New Revision: 6572

Modified:
   trunk/plearn/misc/viewVMat.cc
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Resolved minor conflicts

Modified: trunk/plearn/misc/viewVMat.cc
===================================================================
--- trunk/plearn/misc/viewVMat.cc	2007-01-12 19:39:21 UTC (rev 6571)
+++ trunk/plearn/misc/viewVMat.cc	2007-01-14 20:01:57 UTC (rev 6572)
@@ -264,8 +264,8 @@
                         // "e-6"...
                         //s = tostring(val);
                         // TODO: find a better solution
-                        char tmp[15];
-                        sprintf(tmp, "%14g", val);
+                        char tmp[1000];
+                        sprintf(tmp, "%14.12g", val);
                         s = tmp;
                     }
                     else {

Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-01-12 19:39:21 UTC (rev 6571)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-01-14 20:01:57 UTC (rev 6572)
@@ -48,11 +48,11 @@
                                      'kamado',
                                      'kamado',
                                      'kamado',
-                                     #'loki',
-                                     #'odin',
-                                     #'midgard',
-                                     #'valhalla',
-                                     #'vili'
+                                     # 'loki',
+                                     # 'odin',
+                                     # 'midgard',
+                                     # 'valhalla',
+                                     # 'vili'
                                      ],
 
                      'iro.umontreal.ca' : [ 'lhmm',    'lknn',    'lmfa',      'lmlp',
@@ -62,8 +62,8 @@
                      }
 
 # To override the default of 2
-MAX_LOADAVG = { 'inari'  : 6 ,
-                'kamado' : 6 }
+MAX_LOADAVG = { 'inari'  : 4 ,
+                'kamado' : 4 }
 
 BUFSIZE     = 4096
 SLEEP_TIME  = 15
@@ -367,6 +367,9 @@
 
     delay                    = PLOption(False)
 
+    ## Time interval (seconds) to wait before launching a new task
+    launch_delay_seconds     = PLOption(1)
+
     allow_unexpected_options = lambda self : False
     def __init__( self, oracles=None,
                   _predicate=inexistence_predicate, **overrides ):
@@ -474,8 +477,9 @@
 
                 if expdir is None:
                     expdir = generateExpdir()
-                    time.sleep(1) ## Making sure the next expdir will be generated at
-                                  ## another 'time', i.e. on another second
+                    ## Making sure the next expdir will be generated at
+                    ## another 'time', i.e. on another second
+                    time.sleep(self.launch_delay_seconds) 
                     arguments.append( "expdir=%s" % expdir )
 
             # # Module function defined above



From manzagop at mail.berlios.de  Mon Jan 15 17:06:33 2007
From: manzagop at mail.berlios.de (manzagop at BerliOS)
Date: Mon, 15 Jan 2007 17:06:33 +0100
Subject: [Plearn-commits] r6573 - trunk/plearn_learners/online
Message-ID: <200701151606.l0FG6XXi026738@sheep.berlios.de>

Author: manzagop
Date: 2007-01-15 17:06:33 +0100 (Mon, 15 Jan 2007)
New Revision: 6573

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.cc
Log:
DeepCopy() - some variables were not being deep copied. Added these.


Modified: trunk/plearn_learners/online/DeepBeliefNet.cc
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.cc	2007-01-14 20:01:57 UTC (rev 6572)
+++ trunk/plearn_learners/online/DeepBeliefNet.cc	2007-01-15 16:06:33 UTC (rev 6573)
@@ -379,7 +379,15 @@
     deepCopyField(joint_layer, copies);
     deepCopyField(activation_gradients, copies);
     deepCopyField(expectation_gradients, copies);
+    deepCopyField(final_cost_input, copies);
+    deepCopyField(final_cost_value, copies);
+    deepCopyField(final_cost_output, copies);
+    deepCopyField(class_output, copies);
+    deepCopyField(class_gradient, copies);
     deepCopyField(final_cost_gradient, copies);
+    deepCopyField(pos_down_values, copies);
+    deepCopyField(pos_up_values, copies);
+
 }
 
 



From tihocan at mail.berlios.de  Mon Jan 15 17:30:20 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Mon, 15 Jan 2007 17:30:20 +0100
Subject: [Plearn-commits] r6574 - trunk/python_modules/plearn/pymake
Message-ID: <200701151630.l0FGUKgJ028723@sheep.berlios.de>

Author: tihocan
Date: 2007-01-15 17:30:19 +0100 (Mon, 15 Jan 2007)
New Revision: 6574

Modified:
   trunk/python_modules/plearn/pymake/pymake.py
Log:
Fixed bug when using the undocumented -link-target option, where the link thus needs to point to the specified link target. This should fix the test suite compilation issues under Windows.

Modified: trunk/python_modules/plearn/pymake/pymake.py
===================================================================
--- trunk/python_modules/plearn/pymake/pymake.py	2007-01-15 16:06:33 UTC (rev 6573)
+++ trunk/python_modules/plearn/pymake/pymake.py	2007-01-15 16:30:19 UTC (rev 6574)
@@ -844,7 +844,7 @@
             info = file_info(cctarget)
             if info.hasmain or create_dll or create_so:
                 if not force_link and not force_recompilation and info.corresponding_output_is_up_to_date() and not create_dll and not create_so:
-                    info.make_symbolic_link(linkname) # remake the correct symbolic link
+                    info.make_symbolic_link(linkname, None, info.corresponding_output) # remake the correct symbolic link
                     print 'Target',info.filebase,'is up to date.'
                 else:
                     executables_to_link[info] = 1
@@ -1761,14 +1761,17 @@
         if not os.path.isdir(odir):  # make sure the OBJS dir exists, otherwise the command will fail
             os.makedirs(odir)
 
-    def make_symbolic_link(self, linkname, symlink_source_basename = None):
+    def make_symbolic_link(self, linkname, symlink_source_basename = None, symlink_to = None):
         """Recreates the symbolic link in filedir pointing to the
         executable in subdirectory objsdir.
 
         If the second argument, which is the user defined path and name for
         the symbolic link, is specified then the symbolic link will have
         this path and name, and the object files will be placed in the OBJS
-        subdirectory of the directory where the source file is."""
+        subdirectory of the directory where the source file is.
+        
+        If the third argument is given, it overrides the target of the link,
+        i.e. the location of the executable object."""
 
         # In the following, we create the link 'symlink_from' -> 'symlink_to'.
 
@@ -1784,9 +1787,10 @@
         else:
             symlink_to_base = self.filebase
 
-        symlink_to = join(objsdir, symlink_to_base)
-        if linkname != '':
-            symlink_to = join(self.filedir, symlink_to)
+        if not symlink_to:
+            symlink_to = join(objsdir, symlink_to_base)
+            if linkname != '':
+                symlink_to = join(self.filedir, symlink_to)
 
         # User-provided path for the link itself?
         if linkname == '':



From laulysta at mail.berlios.de  Mon Jan 15 19:45:18 2007
From: laulysta at mail.berlios.de (laulysta at BerliOS)
Date: Mon, 15 Jan 2007 19:45:18 +0100
Subject: [Plearn-commits] r6575 - trunk/plearn_learners_experimental
Message-ID: <200701151845.l0FIjI8n019014@sheep.berlios.de>

Author: laulysta
Date: 2007-01-15 19:45:17 +0100 (Mon, 15 Jan 2007)
New Revision: 6575

Modified:
   trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
Log:
This is the Hinton model from human motion paper(nips)


Modified: trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc
===================================================================
--- trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2007-01-15 16:30:19 UTC (rev 6574)
+++ trunk/plearn_learners_experimental/DynamicallyLinkedRBMsModel.cc	2007-01-15 18:45:17 UTC (rev 6575)
@@ -586,6 +586,7 @@
     // to set bias of "hidden_layer"
 
     dynamic_connections->fprop(previous_hidden_layer,hidden_layer->activation);
+    // hidden_layer->activation *= alpha;
     hidden_layer->expectation_is_up_to_date = false;
     hidden_layer->computeExpectation();
 
@@ -594,6 +595,11 @@
     hidden_layer->bpropNLL(hidden_layer_target, nll, bias_gradient);
 
     // bpropUpdate through dynamic_connections
+    // real delta = 0;
+    // for(int i=0; i<hidden_layer->size; i++)
+    //    delta -= dynamic_learning_rate * bias_gradient[i] * hidden_layer->activation[i]/alpha;
+    // bias_gradient *= alpha;
+    // alpha += delta;
     dynamic_connections->bpropUpdate(previous_hidden_layer,
                                      hidden_layer->activation,
                                      input_gradient, bias_gradient);
@@ -836,7 +842,7 @@
             //////////////////////////////////
             dynamic_connections->fprop(previous_hidden_layer, cond_bias);
            
-            hidden_layer->getAllBias(cond_bias); 
+            hidden_layer->getAllBias(cond_bias); //**************************
 
 
             //up phase
@@ -845,7 +851,7 @@
             hidden_layer->computeExpectation();
             //////////////////////////////////
 
-            previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour
+            previous_hidden_layer << hidden_layer->expectation;//h_{t-2} au prochain tour//******************************
 
             //h*_{t}
             ////////////



From chrish at mail.berlios.de  Mon Jan 15 22:54:01 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 15 Jan 2007 22:54:01 +0100
Subject: [Plearn-commits] r6576 - trunk/plearn/vmat
Message-ID: <200701152154.l0FLs1aP006634@sheep.berlios.de>

Author: chrish
Date: 2007-01-15 22:54:00 +0100 (Mon, 15 Jan 2007)
New Revision: 6576

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
* USE QUOTES in VMatLanguages error messages! Essential to understanding error
  messages when the %s string printed can either contain spaces or be the empty
  string!
* Fix bug in parsing of [%0:%6] VPL code that triggered code path for optional
  VPL transformation code when clearly none was defined.


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-01-15 18:45:17 UTC (rev 6575)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-01-15 21:54:00 UTC (rev 6576)
@@ -350,9 +350,9 @@
                 in.get(); // Read the ']' character.
                 token += end_of_token + ']';
             }
-	    vector<string> parts=split(token.substr(1),":]");
+	    vector<string> parts = split(token.substr(1, token.size()-2), ":");
 
-            // fieldcopy macro type is [start:end]
+            // fieldcopy macro type is [start:end] or [start:end:vpl_code]
             // fields can be refered to as %number or @name
 	    if (parts.size() == 2 || parts.size() == 3)
             {
@@ -392,9 +392,9 @@
                 if(a>b)
                     PLERROR("In copyfield macro, you have specified a start field that is after the end field. Eg : [%10:%5]");
                 if(a==-1)
-                    PLERROR("In copyfield macro, unknown field :%s",astr.c_str());
+                    PLERROR("In copyfield macro, unknown field : '%s'",astr.c_str());
                 if(b==-1)
-                    PLERROR("In copyfield macro, unknown field :%s",astr.c_str());
+                    PLERROR("In copyfield macro, unknown field : '%s'",astr.c_str());
 
                 for(int i=a;i<=b;i++)
                 {
@@ -428,7 +428,7 @@
                             "are only %d fields available", a, srcfieldnames.length());
                 fieldnames.push_back(srcfieldnames[a]);
             }
-            else PLERROR("Strange fieldcopy format. e.g : [%0:%5]. Found parts %s",join(parts," ").c_str());
+            else PLERROR("Strange fieldcopy format. e.g : [%0:%5]. Found parts '%s'",join(parts," ").c_str());
         }
 
         // did we find a comment?
@@ -497,13 +497,13 @@
             {
                 pos=defines.find(string("@")+colname);
                 if(pos==defines.end())
-                    PLERROR("unknown field : %s",colname.c_str());
+                    PLERROR("unknown field : '%s'",colname.c_str());
                 colname=pos->second.substr(1);
             }
             int colnum=toint(colname);
             real r=vmsource->getStringVal(colnum,str);
             if(is_missing(r))
-                PLERROR("%s : %s is not a known string for this field",token.c_str(),str.c_str());
+                PLERROR("String '%s' is not a known string for the field '%s'", str.c_str(), token.c_str());
             processed_sourcecode+=tostring(r)+" ";
         }
         else processed_sourcecode+=token + " ";
@@ -559,7 +559,7 @@
                 pos=opcodes.find(token);
                 if(pos!=opcodes.end())
                     program.append(pos->second);
-                else PLERROR("Undefined keyword : %s",token.c_str());
+                else PLERROR("Undefined keyword : '%s'",token.c_str());
             }
         }
         car=peekAfterSkipBlanks(processed_sourcecode);
@@ -612,7 +612,7 @@
         {
             fname = string("@") + fname;
             if(defines.find(fname) != defines.end())
-                PLERROR("fieldname %s is duplicate in processed matrix", fname.c_str());
+                PLERROR("fieldname '%s' is duplicate in processed matrix", fname.c_str());
             defines[fname]=string("%")+tostring(i);
         }
     }
@@ -704,7 +704,7 @@
                     if(defines.find(parts[2])!=defines.end())
                         b=toint(defines[parts[2]]);
                     else
-                        PLERROR("found a undefined non-numeric boundary in multifield declaration : '%s'",parts[2].c_str());
+                        PLERROR("Found an undefined non-numeric boundary in multifield declaration : '%s'",parts[2].c_str());
                 }
 
                 for(int i=a;i<=b;i++)
@@ -875,6 +875,14 @@
             pstack.push(*((float*)pptr++));
             break;
         case 1: // getfieldval
+            // Question: why is the next PLERROR line commented? Is if made
+            // obsolete by another bound check earlier in the code? Or is it
+            // temporarily disabled? If the former, please *delete* the line,
+            // together with this comment. If the latter, please reenable the
+            // line or replace this comment by one explaining what needs to be
+            // done before the line can be reenabled. (Bound checking in
+            // general is a good idea...)
+            //
             //if(*pptr > fieldvalues.width()) PLERROR("Tried to acces an out of bound field in VPL code");
             pstack.push(pfieldvalues[*pptr++]);
             break;
@@ -1185,7 +1193,7 @@
             {
                 JTime next = cal->calendarTimeOnOrAfter(date);
                 if(next<0)
-                    PLERROR("VMatLanguage :: attempting 'nextincal' for date %s on "
+                    PLERROR("VMatLanguage :: attempting 'nextincal' for date '%s' on "
                             "calendar '%s' but no next-date found",
                             d.info().c_str(), cal_name.c_str());
                 else
@@ -1205,7 +1213,7 @@
             {
                 JTime next = cal->calendarTimeOnOrBefore(date);
                 if(next<0)
-                    PLERROR("VMatLanguage :: attempting 'previncal' for date %s on "
+                    PLERROR("VMatLanguage :: attempting 'previncal' for date '%s' on "
                             "calendar '%s' but no previous-date found",
                             d.info().c_str(), cal_name.c_str());
                 else
@@ -1292,7 +1300,8 @@
             break;
         }        
         default:
-            PLERROR("BUG IN PreproInterpretor::run while running program: invalid opcode: %d", op);
+            PLASSERT_MSG(false, "BUG IN VMatLanguage::run while running program: unexpected opcode: " +
+                         tostring(op));
         }
     }
     // copy to result vec.



From chrish at mail.berlios.de  Mon Jan 15 23:17:59 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 15 Jan 2007 23:17:59 +0100
Subject: [Plearn-commits] r6577 - trunk/plearn_learners/generic
Message-ID: <200701152217.l0FMHxZm008946@sheep.berlios.de>

Author: chrish
Date: 2007-01-15 23:17:59 +0100 (Mon, 15 Jan 2007)
New Revision: 6577

Modified:
   trunk/plearn_learners/generic/VPLProcessor.cc
Log:
Fix problem with VPLProcessor not copying targets, etc. when corresponding 
target_prg, etc. is empty.


Modified: trunk/plearn_learners/generic/VPLProcessor.cc
===================================================================
--- trunk/plearn_learners/generic/VPLProcessor.cc	2007-01-15 21:54:00 UTC (rev 6576)
+++ trunk/plearn_learners/generic/VPLProcessor.cc	2007-01-15 22:17:59 UTC (rev 6577)
@@ -40,6 +40,7 @@
 
 /*! \file VPLProcessor.cc */
 
+#include <sstream>
 
 #include "VPLProcessor.h"
 #include <plearn/vmat/ProcessingVMatrix.h>
@@ -66,17 +67,6 @@
 
 void VPLProcessor::declareOptions(OptionList& ol)
 {
-    // ### Declare all of this object's options here
-    // ### For the "flags" of each option, you should typically specify  
-    // ### one of OptionBase::buildoption, OptionBase::learntoption or 
-    // ### OptionBase::tuningoption. Another possible flag to be combined with
-    // ### is OptionBase::nosave
-
-    // ### ex:
-    // declareOption(ol, "myoption", &VPLProcessor::myoption, OptionBase::buildoption,
-    //               "Help text describing this option");
-    // ...
-
     declareOption(ol, "filtering_prg", &VPLProcessor::filtering_prg, OptionBase::buildoption,
                   "Optional program string in VPL language to apply as filtering on the training VMat.\n"
                   "This program is to produce a single value interpreted as a boolean: only the rows for which\n"
@@ -139,7 +129,6 @@
 
 }
 
-// ### Nothing to add here, simply calls build_
 void VPLProcessor::build()
 {
     inherited::build();
@@ -193,6 +182,9 @@
 
 }
 
+/// @todo Check if this method works according to the documentation. For
+/// example, the next to last line (VMatr processed_trainset = new ...) has 
+/// no impact.
 void VPLProcessor::setTrainingSet(VMat training_set, bool call_forget)
 {
     orig_fieldnames = training_set->fieldNames();
@@ -212,13 +204,48 @@
 
 VMat VPLProcessor::processDataSet(VMat dataset) const
 {
-    VMat filtered_dataset= dataset;
+    VMat filtered_dataset = dataset;
     PPath filtered_dataset_metadatadir = getExperimentDirectory() / "filtered_dataset.metadata";
     if(!filtering_prg.empty())
         filtered_dataset = new FilteredVMatrix(dataset, filtering_prg, filtered_dataset_metadatadir, verbosity>1,
                                                use_filtering_prg_for_repeat, repeat_id_field_name, repeat_count_field_name);
 
-    return new ProcessingVMatrix(filtered_dataset, input_prg, target_prg, weight_prg, extra_prg);
+    // Since ProcessingVMatrix produces 0 length vectors when given an empty
+    // program (which is not the behavior that VPLProcessors is documented as
+    // implementing), we need to replace each program that is an empty string
+    // by a small VPL snippet that copies all the fields for the input or
+    // target, etc.
+
+    // First compute the start of each section (input, target, etc.) in the
+    // columns of the dataset.
+    const int start_of_targets = dataset->inputsize();
+    const int start_of_weights = start_of_targets + dataset->targetsize();
+    const int start_of_extras = start_of_weights + dataset->weightsize();
+
+    // Now compute each processing_*_prg program.
+    string processing_input_prg = input_prg;
+    if (processing_input_prg.empty() && dataset->inputsize() > 0) {
+        processing_input_prg = "[%0:%" + tostring(start_of_targets-1) + "]";
+    }
+    
+    string processing_target_prg = target_prg;
+    if (processing_target_prg.empty() && dataset->targetsize() > 0) {
+        processing_target_prg = "[%" + tostring(start_of_targets) + ":%" + tostring(start_of_weights-1) + "]";
+    }
+
+    string processing_weight_prg = weight_prg;
+    if (processing_weight_prg.empty() && dataset->weightsize() > 0) {
+        processing_weight_prg = "[%" + tostring(start_of_weights) + ":%" + tostring(start_of_extras-1) + "]";
+    }
+
+    string processing_extras_prg = extra_prg;
+    if (processing_extras_prg.empty() && dataset->extrasize() > 0) {
+        processing_extras_prg = "[%" + tostring(start_of_extras) + ":END]";
+    }
+    
+    return new ProcessingVMatrix(filtered_dataset, processing_input_prg,
+                                 processing_target_prg, processing_weight_prg,
+                                 processing_extras_prg);
 }
 
 void VPLProcessor::computeOutput(const Vec& input, Vec& output) const



From lamblin at mail.berlios.de  Tue Jan 16 02:24:09 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 16 Jan 2007 02:24:09 +0100
Subject: [Plearn-commits] r6578 - trunk/plearn_learners/online
Message-ID: <200701160124.l0G1O9H9000369@sheep.berlios.de>

Author: lamblin
Date: 2007-01-16 02:24:07 +0100 (Tue, 16 Jan 2007)
New Revision: 6578

Modified:
   trunk/plearn_learners/online/OnlineLearningModule.cc
   trunk/plearn_learners/online/OnlineLearningModule.h
Log:
Change prototype of setLearningRate


Modified: trunk/plearn_learners/online/OnlineLearningModule.cc
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.cc	2007-01-15 22:17:59 UTC (rev 6577)
+++ trunk/plearn_learners/online/OnlineLearningModule.cc	2007-01-16 01:24:07 UTC (rev 6578)
@@ -105,7 +105,15 @@
             "'bpropUpdate'.\n");
 }
 
+void OnlineLearningModule::setLearningRate( real dynamic_learning_rate )
+{
+    PLERROR("OnlineLearningModule does not have a learning rate that can be\n"
+            "changed from outside.\n"
+            "If your derived class has one, please implement setLearningrate()"
+            " in it.\n");
+}
 
+
 void OnlineLearningModule::build()
 {
     inherited::build();
@@ -117,14 +125,6 @@
     inherited::makeDeepCopyFromShallowCopy(copies);
 }
 
-void OnlineLearningModule::setLearningRate( real& dynamic_learning_rate )
-{
- PLERROR("In OnlineLearningModule.cc: method 'setLearningRate' not"
-            "implemented.\n"
-            "Please implement it in your derived class, or use"
-            "'setLearningRate'.\n");
-}
-
 void OnlineLearningModule::declareOptions(OptionList& ol)
 {
     declareOption(ol, "input_size", &OnlineLearningModule::input_size,

Modified: trunk/plearn_learners/online/OnlineLearningModule.h
===================================================================
--- trunk/plearn_learners/online/OnlineLearningModule.h	2007-01-15 22:17:59 UTC (rev 6577)
+++ trunk/plearn_learners/online/OnlineLearningModule.h	2007-01-16 01:24:07 UTC (rev 6578)
@@ -157,6 +157,10 @@
     // in case bpropUpdate does not do anything, make it known
     virtual bool bpropDoesNothing() { return false; }
 
+    // Allows to change the learning rate, if the derived class has one and
+    // allows to do so.
+    virtual void setLearningRate(real dynamic_learning_rate);
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.
@@ -170,8 +174,6 @@
     //! Transforms a shallow copy into a deep copy
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 
-    virtual void setLearningRate(real& dynamic_learning_rate);
-
 protected:
     //#####  Protected Member Functions  ######################################
 



From lamblin at mail.berlios.de  Tue Jan 16 03:24:29 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 16 Jan 2007 03:24:29 +0100
Subject: [Plearn-commits] r6579 - trunk/plearn_learners/generic
Message-ID: <200701160224.l0G2OTEY003369@sheep.berlios.de>

Author: lamblin
Date: 2007-01-16 03:24:28 +0100 (Tue, 16 Jan 2007)
New Revision: 6579

Modified:
   trunk/plearn_learners/generic/PLearner.cc
Log:
Get rid of icc warning


Modified: trunk/plearn_learners/generic/PLearner.cc
===================================================================
--- trunk/plearn_learners/generic/PLearner.cc	2007-01-16 01:24:07 UTC (rev 6578)
+++ trunk/plearn_learners/generic/PLearner.cc	2007-01-16 02:24:28 UTC (rev 6579)
@@ -894,12 +894,12 @@
         Vec hiconf  = v.subVec(nin+2*nout, nout);
         inputs->getRow(i, invec);
         computeOutput(invec, outvec);
-        bool conf_avail = computeConfidenceFromOutput(invec, outvec, probability,
-                                                      intervals);
+        bool conf_avail = computeConfidenceFromOutput(invec, outvec,
+                                                      probability, intervals);
         if (conf_avail) {
-            for (int i=0, n=intervals.size() ; i<n ; ++i) {
-                lowconf[i] = intervals[i].first;
-                hiconf[i]  = intervals[i].second;
+            for (int j=0, n=intervals.size() ; j<n ; ++j) {
+                lowconf[j] = intervals[j].first;
+                hiconf[j]  = intervals[j].second;
             }
         }
         else {



From lamblin at mail.berlios.de  Tue Jan 16 03:33:54 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 16 Jan 2007 03:33:54 +0100
Subject: [Plearn-commits] r6580 - trunk/commands/PLearnCommands
Message-ID: <200701160233.l0G2XsgI004068@sheep.berlios.de>

Author: lamblin
Date: 2007-01-16 03:33:53 +0100 (Tue, 16 Jan 2007)
New Revision: 6580

Modified:
   trunk/commands/PLearnCommands/HTMLHelpCommand.cc
Log:
Fix another icc warning


Modified: trunk/commands/PLearnCommands/HTMLHelpCommand.cc
===================================================================
--- trunk/commands/PLearnCommands/HTMLHelpCommand.cc	2007-01-16 02:24:28 UTC (rev 6579)
+++ trunk/commands/PLearnCommands/HTMLHelpCommand.cc	2007-01-16 02:33:53 UTC (rev 6580)
@@ -390,7 +390,7 @@
             defclass = (*olIt)->optionHolderClassName(obj);
         }
         out << string("  <tr class=\"") + (index++ % 2 == 0? "even" : "odd")
-			                            + "\">" << endl
+                                        + "\">" << endl
             << "    <td>"
             << "<div class=\"opttype\">" << highlight_known_classes(quote(opttype))
             << "</div>" << endl
@@ -435,7 +435,7 @@
             if( (*entry.isa_method)(o) ) {
                 string helpurl = string("class_") + it->first + ".html";
                 out << string("  <tr class=\"") +
-					   (index++%2 == 0? "even" : "odd") + "\">" << endl
+                       (index++%2 == 0? "even" : "odd") + "\">" << endl
                     << "    <td><a href=\"" << helpurl << "\">"
                     << quote(it->first) << "</a></td><td>" << quote(e.one_line_descr)
                     << "    </td>"
@@ -459,10 +459,10 @@
     index = 0;
     while (rmm) {
         RemoteMethodMap::MethodMap::const_iterator
-            it = rmm->begin(), end = rmm->end();
-        for ( ; it != end ; ++it ) {
-            const string& method_name = it->first.first;
-            const RemoteTrampoline* t = it->second;
+            rmmIt = rmm->begin(), end = rmm->end();
+        for ( ; rmmIt != end ; ++rmmIt ) {
+            const string& method_name = rmmIt->first.first;
+            const RemoteTrampoline* t = rmmIt->second;
             PLASSERT( t );
             const RemoteMethodDoc& doc = t->documentation();
 



From lamblin at mail.berlios.de  Tue Jan 16 03:49:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Tue, 16 Jan 2007 03:49:11 +0100
Subject: [Plearn-commits] r6581 - trunk/plearn/vmat
Message-ID: <200701160249.l0G2nBRB004853@sheep.berlios.de>

Author: lamblin
Date: 2007-01-16 03:49:09 +0100 (Tue, 16 Jan 2007)
New Revision: 6581

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
Another icc warning


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-01-16 02:33:53 UTC (rev 6580)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-01-16 02:49:09 UTC (rev 6581)
@@ -1271,15 +1271,15 @@
                     vars[i][j] = pstack.pop();
             }
 
-            Vec result(result_size, 1);
+            Vec res(result_size, 1);
             int step_size = result_size;
 
-            // Accumulate variable product into "result" variable.
+            // Accumulate variable product into "res" variable.
             for (int var_index = 0; var_index < num_vars; var_index++) {
                 step_size /= vars[var_index].size();
                 for (int var_data_index = 0; var_data_index < vars[var_index].size(); var_data_index++) {
                     for (int dest_index = 0; dest_index < result_size; dest_index += step_size) {
-                        result[dest_index] *= vars[var_index][var_data_index];
+                        res[dest_index] *= vars[var_index][var_data_index];
                     }
                 }
             }
@@ -1287,7 +1287,7 @@
 
             // Put the result onto the stack
             for (int i = 0; i < result_size; i++)
-                pstack.push(result[i]);
+                pstack.push(res[i]);
             break;
         }
         case 65: // thermometer



From chrish at mail.berlios.de  Tue Jan 16 20:45:04 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 16 Jan 2007 20:45:04 +0100
Subject: [Plearn-commits] r6582 - trunk/plearn_learners/generic
Message-ID: <200701161945.l0GJj4a4014288@sheep.berlios.de>

Author: chrish
Date: 2007-01-16 20:45:03 +0100 (Tue, 16 Jan 2007)
New Revision: 6582

Modified:
   trunk/plearn_learners/generic/NNet.cc
Log:
Actually use early stopping information from optimizer in NNet.


Modified: trunk/plearn_learners/generic/NNet.cc
===================================================================
--- trunk/plearn_learners/generic/NNet.cc	2007-01-16 02:49:09 UTC (rev 6581)
+++ trunk/plearn_learners/generic/NNet.cc	2007-01-16 19:45:03 UTC (rev 6582)
@@ -1052,7 +1052,7 @@
         optimizer->nstages = optstage_per_lstage;
         train_stats->forget();
         optimizer->early_stop = false;
-        optimizer->optimizeN(*train_stats);
+        early_stop = optimizer->optimizeN(*train_stats);
         // optimizer->verifyGradient(1e-6); // Uncomment if you want to check your new Var.
         train_stats->finalize();
         if(verbosity>2)



From chrish at mail.berlios.de  Tue Jan 16 21:03:19 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Tue, 16 Jan 2007 21:03:19 +0100
Subject: [Plearn-commits] r6583 - tags
Message-ID: <200701162003.l0GK3Joq016598@sheep.berlios.de>

Author: chrish
Date: 2007-01-16 21:03:19 +0100 (Tue, 16 Jan 2007)
New Revision: 6583

Added:
   tags/remote-0.9a/
Log:
Tag pour release remote 0.9a

Copied: tags/remote-0.9a (from rev 6582, trunk)



From lamblin at mail.berlios.de  Wed Jan 17 00:12:38 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 17 Jan 2007 00:12:38 +0100
Subject: [Plearn-commits] r6584 - trunk/plearn_learners/online
Message-ID: <200701162312.l0GNCcfY002768@sheep.berlios.de>

Author: lamblin
Date: 2007-01-17 00:12:37 +0100 (Wed, 17 Jan 2007)
New Revision: 6584

Modified:
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMGaussianLayer.cc
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMTruncExpLayer.cc
Log:
Get rid of an obsolete option


Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
@@ -59,7 +59,6 @@
     inherited( the_learning_rate )
 {
     size = the_size;
-    units_types = string( the_size, 'l' );
     activation.resize( the_size );
     sample.resize( the_size );
     expectation.resize( the_size );
@@ -225,10 +224,6 @@
 
 void RBMBinomialLayer::build_()
 {
-    if( size < 0 )
-        size = int(units_types.size());
-    if( size != (int) units_types.size() )
-        units_types = string( size, 'l' );
 }
 
 void RBMBinomialLayer::build()

Modified: trunk/plearn_learners/online/RBMGaussianLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMGaussianLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
@@ -67,7 +67,6 @@
     sigma_is_up_to_date( false )
 {
     size = the_size;
-    units_types = string( the_size, 'q' );
     activation.resize( the_size );
     sample.resize( the_size );
     expectation.resize( the_size );
@@ -225,11 +224,6 @@
 
 void RBMGaussianLayer::build_()
 {
-    if( size < 0 )
-        size = int(units_types.size());
-    if( size != (int) units_types.size() )
-        units_types = string( size, 'q' );
-
     sigma.resize( size );
     sigma_is_up_to_date = false;
 

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
@@ -87,12 +87,8 @@
 void RBMLayer::declareOptions(OptionList& ol)
 {
     declareOption(ol, "units_types", &RBMLayer::units_types,
-                  OptionBase::learntoption,
-                  "Each character of this string describes the type of an"
-                  " up unit:\n"
-                  "  - 'l' if the energy function of this unit is linear\n"
-                  "    (binomial or multinomial unit),\n"
-                  "  - 'q' if it is quadratic (for a gaussian unit).\n");
+                  OptionBase::nosave,
+                  "Obsolete option.");
 
     declareOption(ol, "size", &RBMLayer::size,
                   OptionBase::buildoption,

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-01-16 23:12:37 UTC (rev 6584)
@@ -70,10 +70,7 @@
     //! Number of units
     int size;
 
-    //! Each character of this string describes the type of an up unit:
-    //!   - 'l' if the energy function of this unit is linear (binomial or
-    //!     multinomial unit),
-    //!   - 'q' if it is quadratic (for a gaussian unit)
+    //! Obsolete option, still here for script compatibility
     string units_types;
 
     //#####  Learnt Options  ##################################################
@@ -177,12 +174,6 @@
     //! forgets everything
     virtual void forget();
 
-    //! return units_types
-    inline string getUnitsTypes()
-    {
-        return units_types;
-    }
-
     //! Set the internal bias values to rbm_bias
     virtual void getAllBias(const Vec& rbm_bias);
 

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
@@ -336,7 +336,6 @@
 
 void RBMMixedLayer::build_()
 {
-    units_types = "";
     size = 0;
     activation.resize( 0 );
     sample.resize( 0 );
@@ -353,7 +352,6 @@
         init_positions[i] = size;
 
         PP<RBMLayer> cur_layer = sub_layers[i];
-        units_types += cur_layer->units_types;
         size += cur_layer->size;
 
         activation = merge( activation, cur_layer->activation );

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
@@ -60,7 +60,6 @@
     inherited( the_learning_rate )
 {
     size = the_size;
-    units_types = string( the_size, 'l' );
     activation.resize( the_size );
     sample.resize( the_size );
     expectation.resize( the_size );
@@ -231,10 +230,6 @@
 
 void RBMMultinomialLayer::build_()
 {
-    if( size < 0 )
-        size = int(units_types.size());
-    if( size != (int) units_types.size() )
-        units_types = string( size, 'l' );
 }
 
 void RBMMultinomialLayer::build()

Modified: trunk/plearn_learners/online/RBMTruncExpLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-01-16 20:03:19 UTC (rev 6583)
+++ trunk/plearn_learners/online/RBMTruncExpLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
@@ -59,7 +59,6 @@
     inherited( the_learning_rate )
 {
     size = the_size;
-    units_types = string( the_size, 'l' );
     activation.resize( the_size );
     sample.resize( the_size );
     expectation.resize( the_size );
@@ -187,10 +186,6 @@
 
 void RBMTruncExpLayer::build_()
 {
-    if( size < 0 )
-        size = int(units_types.size());
-    if( size != (int) units_types.size() )
-        units_types = string( size, 'l' );
 }
 
 void RBMTruncExpLayer::build()



From lamblin at mail.berlios.de  Wed Jan 17 05:14:24 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 17 Jan 2007 05:14:24 +0100
Subject: [Plearn-commits] r6585 - trunk/plearn_learners/online
Message-ID: <200701170414.l0H4EOb6002423@sheep.berlios.de>

Author: lamblin
Date: 2007-01-17 05:14:23 +0100 (Wed, 17 Jan 2007)
New Revision: 6585

Modified:
   trunk/plearn_learners/online/DeepBeliefNet.h
   trunk/plearn_learners/online/GradNNetLayerModule.cc
   trunk/plearn_learners/online/RBMBinomialLayer.cc
   trunk/plearn_learners/online/RBMBinomialLayer.h
   trunk/plearn_learners/online/RBMLayer.cc
   trunk/plearn_learners/online/RBMLayer.h
   trunk/plearn_learners/online/RBMMixedLayer.cc
   trunk/plearn_learners/online/RBMMixedLayer.h
   trunk/plearn_learners/online/RBMMultinomialLayer.cc
   trunk/plearn_learners/online/RBMMultinomialLayer.h
Log:
Remove trailing spaces (s/ +$//)


Modified: trunk/plearn_learners/online/DeepBeliefNet.h
===================================================================
--- trunk/plearn_learners/online/DeepBeliefNet.h	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/DeepBeliefNet.h	2007-01-17 04:14:23 UTC (rev 6585)
@@ -263,16 +263,16 @@
     mutable Vec pos_up_values;
 
     //! Keeps the index of the NLL cost in train_costs
-    int nll_cost_index; 
-    
-    //! Keeps the index of the CLASS cost in train_costs    
-    int class_cost_index; 
-    
+    int nll_cost_index;
+
+    //! Keeps the index of the CLASS cost in train_costs
+    int class_cost_index;
+
     //! Keeps the index of the final cost in train_costs
-    int final_cost_index; 
-    
+    int final_cost_index;
+
     //! Keeps the index of the reconstruction cost in train_costs
-    int recons_cost_index; 
+    int recons_cost_index;
 
 protected:
     //#####  Protected Member Functions  ######################################

Modified: trunk/plearn_learners/online/GradNNetLayerModule.cc
===================================================================
--- trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/GradNNetLayerModule.cc	2007-01-17 04:14:23 UTC (rev 6585)
@@ -154,7 +154,7 @@
 // Simply updates and propagates back gradient
 // PA - the original version of this function propagated the error to the inputs,
 // then called the above bpropUpdate() - this proved inefficient as the weight
-// matrix had to be iterated over twice. 
+// matrix had to be iterated over twice.
 // However, since we're not using blas anymore, the speedup is only when
 // compiled in optimized mode (ie debug is much slower).
 void GradNNetLayerModule::bpropUpdate(const Vec& input, const Vec& output,

Modified: trunk/plearn_learners/online/RBMBinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMBinomialLayer.cc	2007-01-17 04:14:23 UTC (rev 6585)
@@ -157,7 +157,7 @@
     }
 }
 
-void RBMBinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+void RBMBinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                            const Vec& output,
                            Vec& input_gradient, Vec& rbm_bias_gradient,
                            const Vec& output_gradient)
@@ -192,7 +192,7 @@
         expectation_i = expectation[i];
         if(!fast_exact_is_equal(target_i,0.0))
             ret -= target_i * pl_log(expectation_i);
-        if(!fast_exact_is_equal(target_i,1.0)) 
+        if(!fast_exact_is_equal(target_i,1.0))
             ret -= (1-target_i) * pl_log(1-expectation_i);
     }
     return ret;

Modified: trunk/plearn_learners/online/RBMBinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMBinomialLayer.h	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMBinomialLayer.h	2007-01-17 04:14:23 UTC (rev 6585)
@@ -90,7 +90,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! forward propagation with provided bias
-    virtual void fprop( const Vec& input, const Vec& rbm_bias, 
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
                         Vec& output ) const;
 
     //! back-propagates the output gradient to the input
@@ -98,15 +98,15 @@
                              Vec& input_gradient, const Vec& output_gradient);
 
     //! back-propagates the output gradient to the input and the bias
-    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
                              Vec& input_gradient, Vec& rbm_bias_gradient,
                              const Vec& output_gradient) ;
 
-    //! Computes the negative log-likelihood of target given the 
+    //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
-    
+
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec bias_gradient);

Modified: trunk/plearn_learners/online/RBMLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMLayer.cc	2007-01-17 04:14:23 UTC (rev 6585)
@@ -205,7 +205,7 @@
     PLERROR("In RBMLayer::fprop(): not implemented");
 }
 
-void RBMLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+void RBMLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                            const Vec& output,
                            Vec& input_gradient, Vec& rbm_bias_gradient,
                            const Vec& output_gradient)
@@ -316,7 +316,7 @@
         bg[i] = bps[i]/pos_count - bns[i]/neg_count;
 }
 
-void RBMLayer::bpropCD(const Vec& pos_values, const Vec& neg_values, 
+void RBMLayer::bpropCD(const Vec& pos_values, const Vec& neg_values,
                        Vec& bias_gradient)
 {
     // grad = bias_pos_stats/pos_count - bias_neg_stats/neg_count

Modified: trunk/plearn_learners/online/RBMLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMLayer.h	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMLayer.h	2007-01-17 04:14:23 UTC (rev 6585)
@@ -128,9 +128,9 @@
     //! the expectation
     virtual void fprop( const Vec& input, Vec& output ) const;
 
-    //! computes the expectation given the conditional input 
+    //! computes the expectation given the conditional input
     //! and the given bias
-    virtual void fprop( const Vec& input, const Vec& rbm_bias, 
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
                         Vec& output ) const;
 
     //! back-propagates the output gradient to the input,
@@ -140,15 +140,15 @@
                              const Vec& output_gradient) = 0 ;
 
     //! back-propagates the output gradient to the input and the bias
-    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
                              Vec& input_gradient, Vec& rbm_bias_gradient,
                              const Vec& output_gradient) ;
 
-    //! Computes the negative log-likelihood of target given the 
+    //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
-    
+
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec bias_gradient);
@@ -184,7 +184,7 @@
     //! Computes the contrastive divergence bias with respect to the bias
     //! (or activations, which is equivalent), given the positive and
     //! negative phase values.
-    virtual void bpropCD(const Vec& pos_values, const Vec& neg_values, 
+    virtual void bpropCD(const Vec& pos_values, const Vec& neg_values,
                     Vec& bias_gradient);
 
     //#####  PLearn::Object Protocol  #########################################

Modified: trunk/plearn_learners/online/RBMMixedLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMMixedLayer.cc	2007-01-17 04:14:23 UTC (rev 6585)
@@ -128,7 +128,7 @@
     }
 }
 
-void RBMMixedLayer::fprop( const Vec& input, const Vec& rbm_bias, 
+void RBMMixedLayer::fprop( const Vec& input, const Vec& rbm_bias,
                            Vec& output ) const
 {
     PLASSERT( input.size() == input_size );
@@ -172,7 +172,7 @@
     }
 }
 
-void RBMMixedLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+void RBMMixedLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                                 const Vec& output,
                                 Vec& input_gradient, Vec& rbm_bias_gradient,
                                 const Vec& output_gradient)

Modified: trunk/plearn_learners/online/RBMMixedLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMixedLayer.h	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMMixedLayer.h	2007-01-17 04:14:23 UTC (rev 6585)
@@ -96,7 +96,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! forward propagation with provided bias
-    virtual void fprop( const Vec& input, const Vec& rbm_bias, 
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
                         Vec& output ) const;
 
     //! back-propagates the output gradient to the input
@@ -104,15 +104,15 @@
                              Vec& input_gradient, const Vec& output_gradient);
 
     //! back-propagates the output gradient to the input and the bias
-    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
                              Vec& input_gradient, Vec& rbm_bias_gradient,
                              const Vec& output_gradient) ;
 
-    //! Computes the negative log-likelihood of target given the 
+    //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
-    
+
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec bias_gradient);

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.cc
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.cc	2007-01-17 04:14:23 UTC (rev 6585)
@@ -159,7 +159,7 @@
     }
 }
 
-void RBMMultinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+void RBMMultinomialLayer::bpropUpdate(const Vec& input, const Vec& rbm_bias,
                                       const Vec& output,
                                       Vec& input_gradient, Vec& rbm_bias_gradient,
                                       const Vec& output_gradient)
@@ -190,7 +190,7 @@
     PLASSERT( target.size() == input_size );
 
     real ret = 0;
-    
+
     real target_i, expectation_i;
     for( int i=0 ; i<size ; i++ )
     {
@@ -214,7 +214,7 @@
     real* tar = target.data();
     real* biasg = bias_gradient.data();
     for( int i=0 ; i<size ; i++ )
-        biasg[i] = tar[i] - sum_tar * exp[i];    
+        biasg[i] = tar[i] - sum_tar * exp[i];
 }
 
 void RBMMultinomialLayer::declareOptions(OptionList& ol)

Modified: trunk/plearn_learners/online/RBMMultinomialLayer.h
===================================================================
--- trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-01-16 23:12:37 UTC (rev 6584)
+++ trunk/plearn_learners/online/RBMMultinomialLayer.h	2007-01-17 04:14:23 UTC (rev 6585)
@@ -90,7 +90,7 @@
     virtual void fprop( const Vec& input, Vec& output ) const;
 
     //! forward propagation with provided bias
-    virtual void fprop( const Vec& input, const Vec& rbm_bias, 
+    virtual void fprop( const Vec& input, const Vec& rbm_bias,
                         Vec& output ) const;
 
     //! back-propagates the output gradient to the input
@@ -98,15 +98,15 @@
                              Vec& input_gradient, const Vec& output_gradient);
 
     //! back-propagates the output gradient to the input and the bias
-    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias, 
+    virtual void bpropUpdate(const Vec& input, const Vec& rbm_bias,
                              const Vec& output,
                              Vec& input_gradient, Vec& rbm_bias_gradient,
                              const Vec& output_gradient) ;
 
-    //! Computes the negative log-likelihood of target given the 
+    //! Computes the negative log-likelihood of target given the
     //! internal activations of the layer
     virtual real fpropNLL(const Vec& target);
-    
+
     //! Computes the gradient of the negative log-likelihood of target
     //! with respect to the layer's bias, given the internal activations
     virtual void bpropNLL(const Vec& target, real nll, Vec bias_gradient);



From dorionc at mail.berlios.de  Wed Jan 17 23:23:26 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Wed, 17 Jan 2007 23:23:26 +0100
Subject: [Plearn-commits] r6586 - in trunk/python_modules/plearn/math: .
	stats stats/.pytest stats/.pytest/PL_cvx_numpy_matrix_conversions
	stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results
Message-ID: <200701172223.l0HMNQcB030684@sheep.berlios.de>

Author: dorionc
Date: 2007-01-17 23:23:25 +0100 (Wed, 17 Jan 2007)
New Revision: 6586

Added:
   trunk/python_modules/plearn/math/stats/
   trunk/python_modules/plearn/math/stats/.pytest/
   trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions/
   trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results/
   trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results/RUN.log
   trunk/python_modules/plearn/math/stats/__init__.py
   trunk/python_modules/plearn/math/stats/cvx_utils.py
   trunk/python_modules/plearn/math/stats/pytest.config
Log:
New (unstable) stats package. Will be announced when stable.



Property changes on: trunk/python_modules/plearn/math/stats/.pytest
___________________________________________________________________
Name: svn:ignore
   + *.compilation_log



Property changes on: trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results/RUN.log
===================================================================
--- trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results/RUN.log	2007-01-17 04:14:23 UTC (rev 6585)
+++ trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numpy_matrix_conversions/expected_results/RUN.log	2007-01-17 22:23:25 UTC (rev 6586)
@@ -0,0 +1,56 @@
+-------------------------------------------------------------------------------- 
+Test Case 1
+--------------------------------------------------------------------------------
+NumPy matrix shaped (10,)
+[ 1  2  3  4  5  6  7  8  9 10]
+CVX matrix shaped (1, 10)
+   1.0000e+00   2.0000e+00   3.0000e+00   4.0000e+00   5.0000e+00   6.0000e+00   7.0000e+00   8.0000e+00   9.0000e+00   1.0000e+01
+
+
+-------------------------------------------------------------------------------- 
+Test Case 2
+--------------------------------------------------------------------------------
+NumPy matrix shaped (1, 10)
+[[ 1  2  3  4  5  6  7  8  9 10]]
+CVX matrix shaped (1, 10)
+   1.0000e+00   2.0000e+00   3.0000e+00   4.0000e+00   5.0000e+00   6.0000e+00   7.0000e+00   8.0000e+00   9.0000e+00   1.0000e+01
+
+
+-------------------------------------------------------------------------------- 
+Test Case 3
+--------------------------------------------------------------------------------
+NumPy matrix shaped (10, 1)
+[[ 1]
+ [ 2]
+ [ 3]
+ [ 4]
+ [ 5]
+ [ 6]
+ [ 7]
+ [ 8]
+ [ 9]
+ [10]]
+CVX matrix shaped (10, 1)
+   1.0000e+00
+   2.0000e+00
+   3.0000e+00
+   4.0000e+00
+   5.0000e+00
+   6.0000e+00
+   7.0000e+00
+   8.0000e+00
+   9.0000e+00
+   1.0000e+01
+
+
+-------------------------------------------------------------------------------- 
+Test Case 4
+--------------------------------------------------------------------------------
+NumPy matrix shaped (2, 10)
+[[ 1  2  3  4  5  6  7  8  9 10]
+ [11 12 13 14 15 16 17 18 19 20]]
+CVX matrix shaped (2, 10)
+   1.0000e+00   2.0000e+00   3.0000e+00   4.0000e+00   5.0000e+00   6.0000e+00   7.0000e+00   8.0000e+00   9.0000e+00   1.0000e+01
+   1.1000e+01   1.2000e+01   1.3000e+01   1.4000e+01   1.5000e+01   1.6000e+01   1.7000e+01   1.8000e+01   1.9000e+01   2.0000e+01
+
+

Added: trunk/python_modules/plearn/math/stats/__init__.py
===================================================================
--- trunk/python_modules/plearn/math/stats/__init__.py	2007-01-17 04:14:23 UTC (rev 6585)
+++ trunk/python_modules/plearn/math/stats/__init__.py	2007-01-17 22:23:25 UTC (rev 6586)
@@ -0,0 +1,5 @@
+"""UNDER DEVELOPMENT!
+
+This package is meant to bundle statistical tools relying on RPy whenever
+possible and on CVXOPT whenever needed (MLE, GARCH...).
+"""

Added: trunk/python_modules/plearn/math/stats/cvx_utils.py
===================================================================
--- trunk/python_modules/plearn/math/stats/cvx_utils.py	2007-01-17 04:14:23 UTC (rev 6585)
+++ trunk/python_modules/plearn/math/stats/cvx_utils.py	2007-01-17 22:23:25 UTC (rev 6586)
@@ -0,0 +1,74 @@
+"""Provides functions to use CVX transparently where NumPy is the standard."""
+import numpy
+from cvxopt.base import matrix, spmatrix, log
+
+def numpy_to_cvx(*args):
+    """Returns CVX matrices from NumPy matrices
+
+    Note that NumPy one-dimensional arrays are transformed as row vectors
+    in CVX. See/Run test_matrix_conversions() for details.
+    """
+    matrices = []
+    for a in args:
+        a    = numpy.asmatrix(a)
+        m, n = a.shape
+        a    = numpy.ravel(a.transpose())
+        matrices.append( matrix(a, (m,n), 'd') )
+
+    if len(matrices)==1:
+        return matrices[0]
+    return matrices
+
+def cvx_to_numpy(*args):
+    """Returns NumPy matrices from CVX matrices
+
+    Note that CVX vectors are returns as one-dimensional NumPy
+    matrices. See/Run test_matrix_conversions() for details.
+    """
+    matrices = []
+    for a in args:
+        matrices.append( numpy.array(a) )
+
+    if len(matrices)==1:
+        return matrices[0]
+    return matrices
+
+
+#####  Builtin Tests  #######################################################
+
+def test_matrix_conversions():
+    _1_10_ = numpy.array( range(1,11) ) 
+    test_cases = [
+        _1_10_,
+        numpy.array([ _1_10_ ]),
+        numpy.matrix(_1_10_).transpose(),        
+        numpy.array([ _1_10_, range(11, 21) ])
+        ]
+
+    for t, a in enumerate(test_cases):
+        print "-"*80, '\nTest Case %d\n'%(t+1), "-"*80        
+        print "NumPy matrix shaped", a.shape
+        print a
+        
+        a_cvx = numpy_to_cvx(a)
+        print "CVX matrix shaped", a_cvx.size
+        print a_cvx
+
+        a = numpy.asmatrix(a)
+        a_numpy = cvx_to_numpy(a_cvx)
+        m, n = a.shape
+        assert (m,n)==a_cvx.size, \
+               "(m,n) = (%d,%d) != %s = a_cvx.size"%(m, n, a_cvx.size)
+        assert (m,n)==a_numpy.shape, \
+               "(m,n) = (%d,%d) != %s = a_numpy.size"%(m, n, a_numpy.shape)
+
+        for i in range(m):
+            for j in range(n):
+                assert a[i,j] == a_cvx[i,j], "a[%d,%d] = %f != %f = a_cvx[%d,%d]"%(
+                    i, j, a[i,j], a_cvx[i,j], i, j )
+                assert a[i,j] == a_numpy[i,j], "a[%d,%d] = %f != %f = a_numpy[%d,%d]"%(
+                    i, j, a[i,j], a_numpy[i,j], i, j )
+        print
+
+if __name__ == "__main__":
+    test_matrix_conversions()

Added: trunk/python_modules/plearn/math/stats/pytest.config
===================================================================
--- trunk/python_modules/plearn/math/stats/pytest.config	2007-01-17 04:14:23 UTC (rev 6585)
+++ trunk/python_modules/plearn/math/stats/pytest.config	2007-01-17 22:23:25 UTC (rev 6586)
@@ -0,0 +1,105 @@
+"""Pytest config file.
+
+Test is a class regrouping the elements that define a test for PyTest.
+    
+    For each Test instance you declare in a config file, a test will be ran
+    by PyTest.
+    
+      @ivar(name):
+    The name of the Test must uniquely determine the
+    test. Among others, it will be used to identify the test's results
+    (.PyTest/name/*_results/) and to report test informations.
+      @type(name):
+    String
+    
+      @ivar(description):
+    The description must provide other users an
+    insight of what exactly is the Test testing. You are encouraged
+    to used triple quoted strings for indented multi-lines
+    descriptions.
+      @type(description):
+    String
+    
+      @ivar(category):
+    The category to which this test belongs. By default, a
+    test is considered a 'General' test.
+    
+    It is not desirable to let an extensive and lengthy test as 'General',
+    while one shall refrain abusive use of categories since it is likely
+    that only 'General' tests will be ran before most commits...
+    
+      @type(category):
+    string
+    
+      @ivar(program):
+    The program to be run by the Test. The program's name
+    PRGNAME is used to lookup for the program in the following manner:
+    
+    1) Look for a local program named PRGNAME
+    2) Look for a plearn-like command (plearn, plearn_tests, ...) named 
+PRGNAME
+    3) Call 'which PRGNAME'
+    4) Fail
+    
+    Compilable program should provide the keyword argument 'compiler'
+    mapping to a string interpreted as the compiler name (e.g.
+    "compiler = 'pymake'"). If no compiler is provided while the program is
+    believed to be compilable, 'pymake' will be assigned by
+    default. Arguments to be forwarded to the compiler can be provided as a
+    string through the 'compile_options' keyword argument. @type program:
+    Program
+    
+      @ivar(arguments):
+    The command line arguments to be passed to the program
+    for the test to proceed.
+      @type(arguments):
+    String
+    
+      @ivar(resources):
+    A list of resources that are used by your program
+    either in the command line or directly in the code (plearn or pyplearn
+    files, databases, ...). The elements of the list must be string
+    representations of the path, absolute or relative, to the resource.
+      @type(resources):
+    List of Strings
+    
+      @ivar(precision):
+    The precision (absolute and relative) used when comparing
+    floating numbers in the test output (default = 1e-6)
+      @type(precision):
+    float
+    
+      @ivar(pfileprg):
+    The program to be used for comparing files of psave &
+    vmat formats. It can be either:
+      - "__program__": maps to this test's program if its compilable;
+    maps to 'plearn_tests' otherwise (default);
+      - "__plearn__": always maps to 'plearn_tests' (for when the program
+    under test is not a version of PLearn);
+      - A Program (see 'program' option) instance
+      - None: if you are sure no files are to be compared.
+    
+      @ivar(ignored_files_re):
+    Default behaviour of a test is to compare all
+    files created by running the test. In some case, one may prefer some of
+    these files to be ignored.
+      @type(ignored_files_re):
+    list of regular expressions
+    
+      @ivar(disabled):
+    If true, the test will not be ran.
+      @type(disabled):
+    bool
+    
+"""
+Test(
+    name = "PL_cvx_numpy_matrix_conversions",
+    description = "",
+    category = "General",
+    program = Program(name = "python"),
+    arguments = "-c 'from plearn.math.stats.cvx_utils import *; test_matrix_conversions()'",
+    resources = [ ],
+    precision = 1e-06,
+    pfileprg = None,
+    disabled = False
+    )



From dorionc at mail.berlios.de  Wed Jan 17 23:24:53 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Wed, 17 Jan 2007 23:24:53 +0100
Subject: [Plearn-commits] r6587 - trunk/python_modules/plearn/math
Message-ID: <200701172224.l0HMOrdj030961@sheep.berlios.de>

Author: dorionc
Date: 2007-01-17 23:24:53 +0100 (Wed, 17 Jan 2007)
New Revision: 6587

Modified:
   trunk/python_modules/plearn/math/__init__.py
   trunk/python_modules/plearn/math/arrays.py
   trunk/python_modules/plearn/math/statistical_tools.py
Log:
Minor modifications


Modified: trunk/python_modules/plearn/math/__init__.py
===================================================================
--- trunk/python_modules/plearn/math/__init__.py	2007-01-17 22:23:25 UTC (rev 6586)
+++ trunk/python_modules/plearn/math/__init__.py	2007-01-17 22:24:53 UTC (rev 6587)
@@ -31,11 +31,11 @@
 
 # Author: Christian Dorion
 
-# Most of the content of this __init__ file has been moved to 'arrays.py'. For
-# backward comptibility, the functions defined there are forwarded
-# here. However, the 'arrays' module requires numarray... this 'try'
-# statement should make sure other stuff defined here still work even when
-# numarray is not available.
+# Most of the content of this __init__ file has been moved to
+# 'arrays.py'. For backward comptibility, the functions defined there are
+# forwarded here. However, the 'arrays' module requires numarray... this
+# 'try' statement should make sure other stuff defined here still work even
+# when numarray is not available.
 try:
     from arrays import *
 except ImportError:

Modified: trunk/python_modules/plearn/math/arrays.py
===================================================================
--- trunk/python_modules/plearn/math/arrays.py	2007-01-17 22:23:25 UTC (rev 6586)
+++ trunk/python_modules/plearn/math/arrays.py	2007-01-17 22:24:53 UTC (rev 6587)
@@ -105,6 +105,14 @@
 def matrix_distance(m1, m2):
     return maximum.reduce( fabs(ravel(m1-m2)) )
 
+def matrix2vector(mat):
+    shp = shape(mat)
+    assert len(shp)==2
+    length = shp[0]*shp[1]
+
+    mat.setshape((length,))
+    return shp
+
 def mmult(*args):
     """Shorthand for matrix multiplication
 
@@ -113,8 +121,15 @@
     function accepts as many matrices as one wants, processing multiplication
     'from left to right'.
     """
+    #mmult_shapes(*args)
     return reduce(matrixmultiply, args)
 
+def mmult_shapes(*args):
+    """For debugging purposes; print shapes."""
+    for a in args:
+        print shape(a),
+    print
+
 def rrange(start, stop, step, include_stop=False):
     """Behaves like the builtin range() function but with real arguments.
 
@@ -137,6 +152,35 @@
     m = asarray(m)
     return zeros(shape(m))-ufunc.less(m,0)+ufunc.greater(m,0)
 
+def symmetric(m, precision=1e-6, rel_precision=1e-6):
+    return (asymmetry(m, precision, rel_precision) is None)
+    
+def asymmetry(m, precision=1e-6, rel_precision=1e-6):
+    n, nn = shape(m)
+    if n != nn:
+        return n, nn
+    for i in range(n):
+        for j in range(i+1, n):
+            if fabs(m[i,j]-m[j,i]) > max(precision, rel_precision*m[i,j]):
+                return i, j
+    return None
+
+def assert_symmetric(m, callback=None, precision=1e-6, rel_precision=1e-6):
+    asym = asymmetry(m, precision, rel_precision)
+    if asym is not None:
+        i, j = asym
+        if callback: callback()
+        raise AssertionError("Matrix is not symmetric (%d, %d):\n%s"%(i,j,m))
+
+def to_diagonal(a):
+    """Returns a diagonal matrix with elements in 'a' on the diagonal."""
+    assert len(a.shape)==1
+    n = len(a)
+    A = zeros(shape=(n,n), type=a.type())
+    for i in range(n):
+        A[i,i] = a[i]
+    return A
+
 def fast_softmax( x ):
     s = 0
     res = []
@@ -148,10 +192,9 @@
     return [ r/s for r in res ]
 
 def hasNaN(f):
-    has = sum(isNaN(f))
-    while not isinstance(has, int):
-        has = sum(has)
-    return has
+    f = ravel(f)
+    f = choose(isNaN(f), (0, 1))
+    return sum(f)
     
 def isNaN(f):
     """Return 1 where f contains NaN values, 0 elsewhere."""
@@ -192,6 +235,7 @@
     
     a = [1.0, float('NaN'), 3.0, float('NaN')]
     print a
+    print hasNaN(a)
     print replace_nans(a)
 
     print 
@@ -199,3 +243,15 @@
     print kronecker(array([1, 0]), identity(3))
     print
     print kronecker(array([[1, 2],[3, 4]]), identity(3))
+
+    print
+    print "matrix2vector"
+    a = array([[1,2,3], [4,5,6]])
+    print a
+    a_shape = matrix2vector(a)
+    print a
+    a.setshape(a_shape)
+    print a
+    
+    
+    

Modified: trunk/python_modules/plearn/math/statistical_tools.py
===================================================================
--- trunk/python_modules/plearn/math/statistical_tools.py	2007-01-17 22:23:25 UTC (rev 6586)
+++ trunk/python_modules/plearn/math/statistical_tools.py	2007-01-17 22:24:53 UTC (rev 6587)
@@ -333,7 +333,7 @@
         prediction = mmult(_X_, aug_beta)
         epsilon[ycol][where(isNotNaN(Ycol))] = Y - prediction
 
-    # Is sigma really supposed to be NxN?
+    # The estimate of the covariance matrix of the errors
     sigma = mmult(epsilon, transpose(epsilon)) / T    
 
     # if hccm is not None:
@@ -354,99 +354,10 @@
         
     return alpha, beta, epsilon, sigma
 
-#####  Very First Sketch...  ################################################
 
-class LinearGMM(object):
-    """A GMM implementation assuming a linear model and a predetermined 'X'.
+#####  Builtin Tests  #######################################################
 
-    NOTE:
-    This thing is a *very first* sketch toward a more general GMM class...
-    
-    Notation:
-    
-    X (T x k), \beta \in \RSet[k], y,u \in \RSet[T]
-       y = X \beta + u
-         where E[ u u\Tr ] = \Omega (T x T)
-    
-    Instruments W (T x l), T > l, l >= k, 
-       E[ u_t | W_t ] = 0;  E[ u_t u_s | W_t W_s ] = \omega_{ts}
-    
-    The above implies that:
-       E[ W_t\Tr ( y_t - X_t \beta ) ] = 0 \forall t
-    
-    Suppose J (l x k) full rank is s.t.
-       J\Tr W\Tr (y- X\beta) = 0
-    => Orthogonality conditions!!
-    => J = (W\Tr \Omega_0 W)\inv W\Tr X
-     where the 0 subscript denotes the TRUE value of \Omega
-    
-    GMM criterion:
-    
-     Q(\beta, y)
-      = (y - X \beta)\Tr W (W\Tr \Omega_0 W)\inv W\Tr (y - X\beta)
-      = T^{-0.5} (y - X \beta)\Tr W (T\inv W\Tr \Omega_0 W)\inv W\Tr (y - X\beta) T^{-0.5}
-    
-     => Q(\beta_0, y_0) \simASY \Chi^2(l)
-    
-    \betahat_{GMM}
-        = \BETA(\Omega_0)
-        = (X\Tr W (W\Tr \Omega_0 W)\inv W\Tr X)\inv X\Tr W (W\Tr \Omega_0 W)\inv W\Tr y
-    
-    Inefficient
-      \betahat_{iGMM} = (X\Tr W \Lambda W\Tr X)\inv X\Tr W \Lambda W\Tr y
-    
-    IV with W:  \uhat
-      => Let \Omegahat = Diag(\uhat)
-      => \betahat_{FGMM} = \BETA(\Omegahat)
-    
-    \Varhat( \betahat_{FGMM} ) = (X\Tr W (W\Tr \Omegahat W)\inv W\Tr X)\inv
-    """
-
-    def __init__(self, y, X, W):
-        self.y = y
-        self.X = X
-        self.W = W
-
-        T = len(y)
-        self.T = T
-        assert X.shape[0] == T
-        assert W.shape[0] == T
-        assert W.shape[1] >= X.shape[1]
-
-        # Performing IV with W as instruments
-        Xt       = transpose(X)
-        Wt       = transpose(W)
-        WtW      = mmult(Wt, W)
-        WtW_inv  = inverse(WtW)
-        ProjW    = mmult(W, WtW_inv, Wt) 
-        XtProjW  = mmult(Xt, ProjW)
-
-        self.beta_iv = mmult(inverse(mmult(XtProjW, X)), XtProjW, y)
-
-        # The IV residuals
-        self.uhat_iv = y - mmult(X, self.beta_iv)
-        
-        # Consistent estimator of the covariance matrix of the error terms
-        Omegahat = zeros((T, T), type=Float64)
-        for t in range(T):
-            Omegahat[t,t] = self.uhat_iv[t]**2
-
-        # Feasible GMM
-        XtW         = mmult(Xt, W)
-        WtX         = transpose(XtW)
-        Lambda      = inverse( mmult(Wt, Omegahat, W) )
-        XtWLambda   = mmult(XtW, Lambda)
-        XtWLambdaWt = mmult(XtWLambda, Wt)
-
-        # This is the (feasible) GMM beta!
-        self.beta = mmult(inverse(mmult(XtWLambdaWt, X)), XtWLambdaWt, y)
-
-        # And these are the residuals
-        self.uhat = y - mmult(X, self.beta)
-
-
-
-def test_ols_regression(T, K, alpha=0.0, scale=10.0, plot=False):
+def _test_ols_regression(T, K, alpha=0.0, scale=10.0, plot=False):
     u = normal(0, 1, (T,))
     beta = range(1, K+1)
     
@@ -491,35 +402,29 @@
 
     return X, Y, olsR, ols, scipy
 
+class StatsHolder(dict):
+    def __init__(self, **kwargs):
+        dict.__init__(self, kwargs)
+
+    __getattr__ = dict.__getitem__
+    __setattr__ = dict.__setitem__
+    __delattr__ = dict.__delitem__
+
 if __name__ == "__main__":        
     from numarray.random_array import seed, random, normal
     seed(02, 25)
     
     # Setting the problem
     print "T=10, K=1, (a, b)=(50, 1)"
-    test_ols_regression(10, 1, 50.0, plot=True)
+    _test_ols_regression(10, 1, 50.0, plot=True)
 
     print
     print "T=100, K=1, (a, b)=(25, 1)"
-    test_ols_regression(100, 1, 25.0, plot=True)
+    _test_ols_regression(100, 1, 25.0, plot=True)
 
     print
     print "T=100, K=3 (a, b)=(25, 1)"
     try:
-        test_ols_regression(100, 3, 25.0)
+        _test_ols_regression(100, 3, 25.0)
     except:
         print "SciPy FAILS!"
-
-    # # Performing OLS
-    # Xt = transpose(X)
-    # XtX = mmult(Xt, X)
-    # XtY = mmult(Xt, y)
-    # beta_ols = mmult( inverse(XtX), XtY)
-    # print "Distance beta_0 and beta_ols:", matrix_distance(beta, beta_ols)
-    # 
-    # gmm = LinearGMM(y, X, X)
-    # print "Distance beta_ols and beta_IV:", matrix_distance(beta_ols, gmm.beta_iv)
-    # print "Distance beta_ols and beta_GMM:", matrix_distance(beta_ols, gmm.beta)
-    # 
-    # print "GMM residuals:"
-    # print gmm.uhat



From dorionc at mail.berlios.de  Thu Jan 18 18:12:12 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 18 Jan 2007 18:12:12 +0100
Subject: [Plearn-commits] r6588 - trunk/python_modules/plearn/parallel
Message-ID: <200701181712.l0IHCCHY032012@sheep.berlios.de>

Author: dorionc
Date: 2007-01-18 18:12:12 +0100 (Thu, 18 Jan 2007)
New Revision: 6588

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Major robustification of the protocol yielding free machines!


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-01-17 22:24:53 UTC (rev 6587)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-01-18 17:12:12 UTC (rev 6588)
@@ -1,6 +1,7 @@
 import inspect, logging, operator, os, select, signal, sys, time
 
 from popen2 import Popen4
+from datetime import datetime, timedelta
 
 from plearn.utilities.ppath         import get_domain_name
 from plearn.utilities.moresh        import *
@@ -38,21 +39,17 @@
                      'iro.umontreal.ca': 'ClusterTask'
                      }
 
-# Used only for clusters of type 'ssh'.
-SSH_MACHINES_MAP = { 'apstat.com': [ # 'embla',
-                                     'inari',
-                                     'inari',
-                                     'inari',
-                                     'inari',
+# Used only for clusters of type 'ssh'. Do not enter the same machine more
+# than once: use the MAX_LOADAVG map to allow for higher maximum load
+# average than the default of 2.
+SSH_MACHINES_MAP = { 'apstat.com': [ 'embla',
+                                     'inari', 
                                      'kamado',
-                                     'kamado',
-                                     'kamado',
-                                     'kamado',
-                                     # 'loki',
-                                     # 'odin',
-                                     # 'midgard',
-                                     # 'valhalla',
-                                     # 'vili'
+                                     'loki',
+                                     'odin',
+                                     'midgard',
+                                     'valhalla',
+                                     'vili'
                                      ],
 
                      'iro.umontreal.ca' : [ 'lhmm',    'lknn',    'lmfa',      'lmlp',
@@ -61,29 +58,23 @@
                                             ]                         
                      }
 
-# To override the default of 2
+# To override the default of 1
 MAX_LOADAVG = { 'inari'  : 4 ,
                 'kamado' : 4 }
 
-BUFSIZE     = 4096
-SLEEP_TIME  = 15
-LOGDIR      = None  # May be set by set_logdir()
-DOMAIN_NAME = get_domain_name()
+# Do not perform a new query for the loadavg until recently launched
+# processes are likely to have started. 
+LOADAVG_DELAY = timedelta(seconds=15)
+BUFSIZE       = 4096
+SLEEP_TIME    = 15
+LOGDIR        = None  # May be set by set_logdir()
+DOMAIN_NAME   = get_domain_name()
 
 #######  To be assigned to a subclass of TaskType when these will be declared
 Task = None
 
 #######  Module Functions  ####################################################
 
-################################################################################
-# KNOWN ISSUE: The current way of managing machines is slightly hackish
-# with regard to load average. Here we have to multiply instances of the
-# name of a machine by ist MAX_LOADAVG due the remove in getLaunchCommand()
-#
-#           self.host = self.listAvailableMachines().next()
-#           self._machines.remove( self.host )
-# 
-################################################################################
 #DBG: import traceback
 #DBG: _waitpid = os.waitpid
 #DBG: def my_waitpid(*args):
@@ -94,9 +85,7 @@
 
 def get_ssh_machines():
     from random import shuffle
-    machines = []
-    for m in SSH_MACHINES_MAP[ DOMAIN_NAME ]:
-        machines.extend( [m]*MAX_LOADAVG.get(m,2) )        
+    machines = list( SSH_MACHINES_MAP[DOMAIN_NAME] ) # copy
     shuffle( machines )
     return machines
 
@@ -266,42 +255,83 @@
 
 class SshTask( TaskType ):
 
+    Xopt = '-x'
     _machines = get_ssh_machines()
+    _loadavg  = {}
+    _available_machines = None
     
-    def listAvailableMachines( cls ):
+    def getLoadAvg(cls, machine):
+        print "\nQuery to", machine
+        if machine in cls._loadavg:
+            # For typical PLearn/FinLearn tasks, the process begins by
+            # loading the script, creating the expdir, etc., which delays
+            # the impact of the process on the load average...
+            t, loadavg = cls._loadavg[machine]
+            cur_t  = datetime(*time.localtime()[:6])
+            print "Saved %f at %s (now %s)"%(loadavg, t, cur_t)
+            if cur_t < t+LOADAVG_DELAY:
+                return loadavg
+
+        # Query for the load average
+        print "NEW QUERY!"
+        p = os.popen('ssh -x %s cat /proc/loadavg' % machine)
+        line = p.readline()
+        return float(line.split()[0]) # Take the last minute average
+    getLoadAvg = classmethod(getLoadAvg)
+    
+    def listAvailableMachines(cls):
         for m in cls._machines:
-            max_loadavg = MAX_LOADAVG.get(m, 2)
-            
-            p = os.popen('ssh -x %s cat /proc/loadavg' % m)
-            line = p.readline()
-            loadavg = float(line.split()[1])
-
+            loadavg = cls.getLoadAvg(m)
+            max_loadavg = MAX_LOADAVG.get(m, 1.0)
+            print "Load %f / %f"%(loadavg, max_loadavg)
             if loadavg < max_loadavg:
+                # Register the load average *plus* one, taking in account
+                # the process we are about to launch
+                cls._loadavg[m] = datetime(*time.localtime()[:6]), loadavg+1
+                print "At %s Saving %f"%cls._loadavg[m]
+                print
                 yield m
     listAvailableMachines = classmethod(listAvailableMachines)
 
+    def nextAvailableMachine(cls):
+        # If a StopIteration exception is encountered on an already began
+        # loop, we simply have queried each machine once and shall start
+        # over. If such an exception is raise on a new loop, then no
+        # machines are currently available and we raise an
+        # EmptyTaskListError so as to wait a little while before querying
+        # again...
+        new_loop = False
+        if cls._available_machines is None:
+            cls._available_machines = cls.listAvailableMachines()            
+            new_loop = True
+
+        try:
+            return cls._available_machines.next()
+        except StopIteration:
+            cls._available_machines = None
+            if new_loop:
+                raise EmptyMachineListError
+            else:
+                return cls.nextAvailableMachine()
+    nextAvailableMachine = classmethod(nextAvailableMachine)
+
     #
     # Instance methods
     #
 
-    def getLaunchCommand( self, Xopt='-x' ):
+    def getLaunchCommand(self):
         # Get the first available machine
-        try:
-            self.host = self.listAvailableMachines().next()
-            self._machines.remove( self.host )
-        except StopIteration:
-            raise EmptyMachineListError
-
+        self.host = self.nextAvailableMachine()
         actual_command = ' '.join(['cd', os.getcwd(), ';', 'nice'] + self.argv)
         actual_command = actual_command.replace("'", "\'")
         actual_command = actual_command.replace('"', '\"')
-        return "ssh %s %s '%s'"%(self.host, Xopt, actual_command)
+        return "ssh %s %s '%s'"%(self.host, self.Xopt, actual_command)
 
     def getLogFileBaseName(self):
         return "ssh-%s-pid=%d"%(self.host, self.process.pid)
 
     def free(self):
-        self._machines.append( self.host )
+        #KNOWN ISSUE: self._machines.append( self.host )
         TaskType.free(self)
 
 class ClusterTask( TaskType ):



From dorionc at mail.berlios.de  Thu Jan 18 18:24:46 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Thu, 18 Jan 2007 18:24:46 +0100
Subject: [Plearn-commits] r6589 - trunk/python_modules/plearn/parallel
Message-ID: <200701181724.l0IHOkTr000949@sheep.berlios.de>

Author: dorionc
Date: 2007-01-18 18:24:46 +0100 (Thu, 18 Jan 2007)
New Revision: 6589

Modified:
   trunk/python_modules/plearn/parallel/dispatch.py
Log:
Removed forgotten prints...


Modified: trunk/python_modules/plearn/parallel/dispatch.py
===================================================================
--- trunk/python_modules/plearn/parallel/dispatch.py	2007-01-18 17:12:12 UTC (rev 6588)
+++ trunk/python_modules/plearn/parallel/dispatch.py	2007-01-18 17:24:46 UTC (rev 6589)
@@ -261,19 +261,19 @@
     _available_machines = None
     
     def getLoadAvg(cls, machine):
-        print "\nQuery to", machine
+        #print "\nQuery to", machine
         if machine in cls._loadavg:
             # For typical PLearn/FinLearn tasks, the process begins by
             # loading the script, creating the expdir, etc., which delays
             # the impact of the process on the load average...
             t, loadavg = cls._loadavg[machine]
             cur_t  = datetime(*time.localtime()[:6])
-            print "Saved %f at %s (now %s)"%(loadavg, t, cur_t)
+            #print "Saved %f at %s (now %s)"%(loadavg, t, cur_t)
             if cur_t < t+LOADAVG_DELAY:
                 return loadavg
 
         # Query for the load average
-        print "NEW QUERY!"
+        #print "NEW QUERY!"
         p = os.popen('ssh -x %s cat /proc/loadavg' % machine)
         line = p.readline()
         return float(line.split()[0]) # Take the last minute average
@@ -283,13 +283,13 @@
         for m in cls._machines:
             loadavg = cls.getLoadAvg(m)
             max_loadavg = MAX_LOADAVG.get(m, 1.0)
-            print "Load %f / %f"%(loadavg, max_loadavg)
+            #print "Load %f / %f"%(loadavg, max_loadavg)
             if loadavg < max_loadavg:
                 # Register the load average *plus* one, taking in account
                 # the process we are about to launch
                 cls._loadavg[m] = datetime(*time.localtime()[:6]), loadavg+1
-                print "At %s Saving %f"%cls._loadavg[m]
-                print
+                #print "At %s Saving %f"%cls._loadavg[m]
+                #print
                 yield m
     listAvailableMachines = classmethod(listAvailableMachines)
 



From lamblin at mail.berlios.de  Sat Jan 20 01:05:14 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Sat, 20 Jan 2007 01:05:14 +0100
Subject: [Plearn-commits] r6590 - trunk/doc
Message-ID: <200701200005.l0K05E3C013740@sheep.berlios.de>

Author: lamblin
Date: 2007-01-20 01:05:13 +0100 (Sat, 20 Jan 2007)
New Revision: 6590

Added:
   trunk/doc/machine_learning.tex
Modified:
   trunk/doc/
   trunk/doc/Makefile
   trunk/doc/index.html
   trunk/doc/programmers_guide.tex
Log:
New document, "Machine Learning with PLearn"



Property changes on: trunk/doc
___________________________________________________________________
Name: svn:ignore
   - programmers_guide
programmers_guide.aux
programmers_guide.bbl
programmers_guide.blg
programmers_guide.dvi
programmers_guide.log
programmers_guide.ps
programmers_guide.pdf
programmers_guide.toc
tools_guide
tools_guide.aux
tools_guide.dvi
tools_guide.log
tools_guide.pdf
tools_guide.ps
tools_guide.toc
tutorial.aux
users_guide
users_guide.aux
users_guide.bbl
users_guide.blg
users_guide.dvi
users_guide.log
users_guide.out
users_guide.pdf
users_guide.ps
users_guide.toc
python_modules_html

   + installation_guide
faq
machine_learning
programmers_guide
python_modules_html
tools_guide
users_guide
*.aux
*.bbl
*.blg
*.dvi
*.log
*.out
*.ps
*.pdf
*.toc


Modified: trunk/doc/Makefile
===================================================================
--- trunk/doc/Makefile	2007-01-18 17:24:46 UTC (rev 6589)
+++ trunk/doc/Makefile	2007-01-20 00:05:13 UTC (rev 6590)
@@ -13,7 +13,7 @@
 publish_guides:
 	scp -r *.html *_guide faq *.ps *.pdf *.txt *.jpg plearner at shell.berlios.de:/home/groups/plearn/htdocs/
 
-guides.all: installation_guide.all users_guide.all programmers_guide.all tools_guide.all faq.all
+guides.all: installation_guide.all users_guide.all programmers_guide.all tools_guide.all faq.all machine_learning.all
 
 autodoc_plearn_curses: 
 	cd html_autodoc; plearn_curses htmlhelp html_config.plearn
@@ -90,8 +90,24 @@
 	pdflatex $(PDFTEX_OPTIONS) programmers_guide
 
 
+machine_learning.all: machine_learning machine_learning.dvi machine_learning.ps machine_learning.pdf
 
+machine_learning: machine_learning.tex
+	latex2html -split 3 -show_section_numbers -local_icons machine_learning.tex
 
+machine_learning.dvi: machine_learning.tex
+	latex $(TEX_OPTIONS) machine_learning
+	latex $(TEX_OPTIONS) machine_learning
+	latex $(TEX_OPTIONS) machine_learning
+
+machine_learning.ps: machine_learning.dvi
+	dvips -Pcmps -t letter machine_learning -o
+
+machine_learning.pdf: machine_learning.dvi
+	pdflatex $(PDFTEX_OPTIONS) machine_learning
+
+
+
 tools_guide.all: tools_guide tools_guide.dvi tools_guide.ps tools_guide.pdf
 
 tools_guide: tools_guide.tex

Modified: trunk/doc/index.html
===================================================================
--- trunk/doc/index.html	2007-01-18 17:24:46 UTC (rev 6589)
+++ trunk/doc/index.html	2007-01-20 00:05:13 UTC (rev 6590)
@@ -51,6 +51,10 @@
     <a href="programmers_guide.pdf">[pdf]</a>
     <a href="programmers_guide.ps">[ps]</a>
   </li>
+  <li> <a href="machine_learning/index.html">Machine Learning with PLearn</a>
+    <a href="machine_learning.pdf">[pdf]</a>
+    <a href="machine_learning.ps">[ps]</a>
+  </li>
   <li> <a href="tools_guide/index.html">PLearn Tools Guide</a>
     <a href="tools_guide.pdf">[pdf]</a>
     <a href="tools_guide.ps">[ps]</a>

Added: trunk/doc/machine_learning.tex
===================================================================
--- trunk/doc/machine_learning.tex	2007-01-18 17:24:46 UTC (rev 6589)
+++ trunk/doc/machine_learning.tex	2007-01-20 00:05:13 UTC (rev 6590)
@@ -0,0 +1,565 @@
+%% -*- mode:latex; tex-open-quote:"\\og{}"; tex-close-quote:"\\fg{}" -*-
+%%
+%%  Copyright (c) 2007 by Pascal Lamblin
+
+%%
+%%  $Id$
+
+\documentclass[11pt]{book}
+\usepackage{t1enc}              % new font encoding  (hyphenate words w/accents)
+\usepackage{ae}                 % use virtual fonts for getting good PDF
+\usepackage{url}                % support URLs properly
+\usepackage{hyperref}
+
+\addtolength{\textheight}{3.5cm}
+\addtolength{\topmargin}{-1.8cm}
+\addtolength{\textwidth}{2.5cm}
+\addtolength{\oddsidemargin}{-1cm}
+\addtolength{\evensidemargin}{-1.5cm}
+
+
+%%%%%%%%% Definitions %%%%%%%%%%%%
+\newcommand{\PLearn}{{\bf \it PLearn}}
+
+\parskip=2mm
+\parindent=0mm
+
+\title{\Huge Machine Learning with PLearn\\
+\Large How some standard (and non-standard) ML algorithms are
+implemented in PLearn, and how to play with them}
+
+\begin{document}
+
+%%%%%%%% Title Page %%%%%%%%%%
+\pagenumbering{roman}
+\thispagestyle{empty}
+
+\maketitle
+
+\pagebreak
+
+\vspace*{10cm}
+
+
+
+Copyright \copyright\ 2007 Pascal Lamblin \\
+
+Permission is granted to copy and distribute this document in any medium,
+with or without modification, provided that the following conditions are
+met:
+
+\begin{enumerate}
+\item Modified versions must give fair credit to all authors.
+\item Modified versions may not be written with the aim to discredit, misrepresent, or otherwise taint the
+    reputation of any of the above authors.
+\item Modified versions must retain the above copyright notice, and append to
+    it the names of the authors of the modifications, together with the years the
+    modifications were written.
+\item Modified versions must retain this list of conditions unaltered,
+    and may not impose any further restrictions.
+\end{enumerate}
+
+
+\pagebreak
+
+%%%%%%%%% Table of contents %%%%%%%%%%%%
+\addcontentsline{toc}{chapter}{\numberline{}Table of contents}
+\tableofcontents
+
+\cleardoublepage\pagebreak
+\pagenumbering{arabic}
+
+%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
+
+\chapter*{Introduction}
+
+The purpose of this document is to document the way some particular
+learning algorithms (like Deep Belief Networks) are implemented using
+PLearn's base classes. It is not to detail how those base classes are
+working.
+
+You should read {\it PLearn programmer's guide} first (or at least have
+it reachable), you will need it for information about PLearn's generic
+classes, especially {\tt Object} and {\tt PLearner}, but also {\tt Var}
+and {\tt OnlineLearningModule}, and for the general coding philosophy.
+
+
+\chapter{A Var-based PLearner: NNet}
+
+\chapter{Boltzmann Machines and Deep Belief Networks}
+
+The equations can be seen on
+\url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Neurones/DBNEquations}
+if you have an account on LISA's TWiki.
+
+All the code files are located in {\tt
+\$PLEARNDIR/plearn\_learners/online}.
+
+\section{Architecture}
+
+\subsection{Restricted Boltzmann Machines}
+
+A Restricted Boltzmann Machine (RBM) is composed of two different layers
+of units, with weighted connection between them.
+
+The layers are modelled by the {\tt RBMLayer} class, while the
+connections are represented by {\tt RBMConnection}. Different
+sub-classes implement the multiple types of layers and connections.
+{\tt RBMLayer} and {\tt RBMConnection} both inherit from {\tt
+OnlineLearningModule}.
+
+An RBM can therefore be considered as a structure containing two
+instances of {\tt RBMLayer} and one of {\tt RBMConnection}, but there is
+no class modelling an RBM for the moment.
+
+\subsection{Deep Belief Networks}
+
+A Deep Belief Network (DBN) is a learning algorithm, therefore contained
+in a {\tt PLearner}, namely {\tt DeepBeliefNet}.
+
+It is composed of stacked RBMs. The units of a layer are shared between
+two RBMs, hence the need of dissociating layers and connections. A {\tt
+DeepBeliefNet} containing $n$ unit layers (including input and output
+layers) will typically contain $n$ instances of {\tt RBMLayer} and $n-1$
+instances of {\tt RBMConnection}.
+
+The training is usually done one layer at a time, each layer being
+trained as an RBM. See part \ref{DeepBeliefNet} for the detailed
+explanation.
+
+There are no functions for sampling from the learned probability
+distribution yet, they might be added at some point in time.
+
+\section{Code Components}
+
+Both classes inherit from {\tt OnlineLearningModule}, so they have
+deterministic {\tt fprop(\ldots)} and {\tt bpropUpdate(\ldots)} functions,
+that can be chained.
+
+\subsection{\tt RBMLayer}
+
+This class models a set of (usually independant) units, some of their
+intrinsic parameters, and their current state.
+
+{\tt RBMLayer} stores:
+\begin{itemize}
+    \item {\tt size}: number of units
+
+    \item {\tt bias}: vector of the units' biases
+
+    \item {\tt activation}: the value of the weighting sum of the
+    inputs, plus the bias
+
+    \item {\tt expectation}: the expected value of each unit's
+    distribution
+
+    \item {\tt sample}: a sample from the distribution
+
+    \item some flags to know what is up-to-date
+
+    \item {\tt bias\_pos\_stats} and {\tt bias\_neg\_stats}: accumulate
+    positive phase and negative phase contributions to the CD gradient
+    wrt the {\tt bias}
+
+    \item {\tt pos\_count} and {\tt neg\_count}: keep track of the
+    number of accumulated contributions
+
+    \item {\tt learning\_rate} and {\tt momentum}: control the update
+    (more hyper-parameters might be added)
+\end{itemize}
+
+The methods are:
+\begin{itemize}
+
+    \item {\tt getUnitActivation( int i, RBMConnection rbmc )}:
+    get the result of the linear transformation from {\tt rbmc},
+    and add the corresponding bias for unit {\tt i}. It calls {\tt
+    rbmc->computeProduct}.
+
+    \item {\tt getAllActivations( RBMConnection rbmc )}: same as above,
+    but for all units in the layer
+
+    \item {\tt computeExpectation()}: compute the value of {\tt
+    expectation}, given {\tt activation} (with a caching system, to
+    avoid computing twice if {\tt activation} didn't change)
+
+    \item {\tt generateSample()}: generates a sample, given the value of
+    {\tt activation}, and places it in {\tt sample}
+
+    \item {\tt accumulatePosStats( Vec pos\_values )}: accumulate
+    statistics from the positive phase
+
+\begin{verbatim}
+bias_pos_stats += pos_values;
+pos_count++;
+\end{verbatim}
+
+    \item {\tt accumulateNegStats( Vec neg\_values )}: idem with the
+    negative phase
+
+\begin{verbatim}
+bias_neg_stats += neg_values;
+neg_count++;
+\end{verbatim}
+
+    \item {\tt update()}: update the bias (and other parameters if some)
+    from accumulated statistics
+
+\begin{verbatim}
+bias -= learning_rate * (bias_pos_stats/pos_count - bias_neg_stats/neg_count)
+
+# reset
+bias_pos_stats.clear();
+bias_neg_stats.clear();
+pos_count = 0;
+neg_count = 0;
+\end{verbatim}
+\end{itemize}
+
+And from the {\tt OnlineLearningModule} interface:
+\begin{itemize}
+
+    \item {\tt fprop( Vec input, Vec output )}: {\tt input} represents
+    the output of the {\tt RBMConnection}, and {\tt output} the
+    expectation (mean-field approximation) of the layer. For an {\tt
+    RBMBinomialLayer}:
+
+\begin{verbatim}
+output = sigmoid( -(input + bias) );
+\end{verbatim}
+
+    \item {\tt bpropUpdate( Vec input, Vec output, Vec input\_grad, Vec
+    output\_grad )}: backpropagate a gradient through the layer, and
+    update the parameters ({\tt bias},\ldots) accordingly, given the
+    {\tt learning\_rate}, {\tt momentum}, etc.
+
+\end{itemize}
+
+Different types of units (binomial, Gaussian, even groups of units
+representing a multinomial distribution, etc.), so this class has
+several derived sub-classes, which may store more information (like a
+quadratic parameter, and the standard deviation for a Gaussian unit) and
+use them in the {\tt accumulate\{Pos,Neg\}Stats(\ldots)} and {\tt update()}
+methods.
+
+List of known sub-classes:
+\begin{itemize}
+
+    \item {\tt RBMBinomialLayer}: stores binomial (0 or 1) units (the
+    simplest implementation)
+
+    \item {\tt RBMMultinomialLayer}: stores a group of 0/1 units, so
+    that exactly one of them is 1 at any time
+
+    \item {\tt RBMGaussianLayer}: stores real-valued units with Gaussian
+    distributions
+
+    \item {\tt RBMTruncExpLayer}: stores real-valued units in a [0, 1]
+    range, with a truncated exponential distribution
+
+    \item {\tt RBMMixedLayer}: concatenation of several {\tt RBMLayer}
+
+\end{itemize}
+
+\subsection{RBMParameters}
+
+This class represents a linear transformation (not affine! the bias is
+in the {\tt RBMLayer}), used to compute one layer's activation given the
+other layer's value.
+
+{\tt RBMConnection} stores (and has to update):
+\begin{itemize}
+
+    \item {\tt up\_size} and {\tt down\_size}: the number of units in
+    the layers above and below (respectively)
+
+    \item {\tt input\_vec}: a pointer to its current input vector
+    (sample or expectation), and a flag to know if it is up or down
+
+    \item Something that contains the weights of the connections (can be
+    a matrix, a set of convolution filters\ldots), let's call it {\tt
+    weights}
+
+    \item {\tt weights\_pos\_stats}, {\tt weight\_neg\_stats}:
+    statistics accumulated during positive and negative (respectively)
+    phases
+
+    \item {\tt pos\_count} and {\tt neg\_count}
+
+    \item {\tt learning\_rate} and {\tt momentum}
+
+\end{itemize}
+
+The different sub-classes will store differently the parameters allowing
+to compute the linear transformation, and the statistics used to update
+those parameters (usually named {\tt [paramname]\_pos\_stats} and {\tt
+[paramname]\_neg\_stats}).
+
+The methods are:
+\begin{itemize}
+
+    \item {\tt setAsUpInput( Vec input )}: set the input vector, and
+    flag to 'up'
+
+    \item {\tt setAsDownInput( Vec input )}: same, but 'down'
+
+    \item {\tt computeProduct( int start, int length, Vec activations,
+    bool accumulate )}: compute the output activation of {\tt length}
+    units, starting from {\tt start}. These units belong to the {\em
+    above} layer if the {\tt input} was {\em down}, and to the layer
+    {\em below} if the {\tt input} was {\em up}. The output is put in
+    {\tt activations} (or added if {\tt accumulate}, not shown in the
+    code below).
+
+\begin{verbatim}
+if( up ):
+    for i=start to start+length:
+        activations[i-start] += sum_j weights(i,j) input_vec[j]
+else:
+    for j=start to start+length:
+        activations[j-start] += sum_i weights(i,j) input_vec[i]
+\end{verbatim}
+
+    \item {\tt accumulatePosStats( Vec down\_values, Vec up\_values )}:
+    in the basic case of an {\tt RBMMatrixConnection}
+
+\begin{verbatim}
+weights_pos_stats += up_values * down_values';
+pos_count++;
+\end{verbatim}
+
+    \item {\tt accumulateNegStats( Vec down\_values, Vec up\_values )}:
+    in the basic case of an {\tt RBMMatrixConnection}
+
+\begin{verbatim}
+weights_neg_stats += up_values * down_values';
+neg_count++;
+\end{verbatim}
+
+    \item {\tt update()}: update from accumulated statistics
+
+\begin{verbatim}
+weights -= learning_rate * (weights_pos_stats/pos_count - weight_neg_stats/neg_count);
+
+# reset
+weights_pos_stats.clear();
+weights_neg_stats.clear();
+pos_count = 0;
+neg_count = 0;
+\end{verbatim}
+
+    \item {\tt fprop( input, output )}: performs the linear
+    transformation on {\tt input}, and put the result in {\tt output};
+    typically
+
+\begin{verbatim}
+output = weights * input;
+\end{verbatim}
+
+\end{itemize}
+
+And from the {\tt OnlineLearningModule} interface:
+\begin{itemize}
+
+    \item {\tt bpropUpdate( input, output, input\_grad, output\_grad
+    )}: backpropagates the output gradient, and update the parameters
+    (weights, \ldots) accordingly, given the {\tt learning\_rate}, {\tt
+    momentum}, etc.
+
+\end{itemize}
+
+\begin{verbatim}
+input_grad = weights' * output_grad;
+weights -= learning_rate * output_grad * input';
+\end{verbatim}
+
+List of known subclasses, and their parameters:
+\begin{itemize}
+
+    \item {\tt RBMMatrixConnection}: {\tt Mat weights} (simple matrix
+    multiplication)
+
+    \item {\tt RBMConv2DConnection}: {\tt Mat kernel}, along with {\tt
+    int down\_image\_length, down\_image\_width, up\_image\_length,
+    up\_image\_width, kernel\_step1, kernel\_step2, kernel\_length,
+    kernel\_width} (2 dimensional convolution filters)
+
+    \item {\tt RBMMixedConnection}: {\tt TMat<RBMConnection>
+    sub\_connections} (block-matrix containing other {\tt
+    RBMConnection}, which specify a part of the global linear
+    transformation)
+
+\end{itemize}
+
+\section{Code Samples}
+
+\subsection{Propagation in an RBM}
+
+In the simple case of a Restricted Boltzmann Machine, we have two
+instances of {\tt RBMLayer} ({\tt input} and {\tt hidden}) and one of
+{\tt RBMConnection} ({\tt rbmc}) linking both of them.
+
+Getting in {\tt hidden\_exp} the expected value of the hidden layer,
+given one input sample {\tt input\_sample}, is easy:
+
+\begin{verbatim}
+input.sample << input_sample;
+rbmc.setAsDownInput( input.sample );
+hidden.getAllActivations( rbmc );
+hidden.computeExpectation();
+hidden_exp << hidden.expectation;
+\end{verbatim}
+
+If we want a sample {\tt hidden\_sample} instead, it is:
+
+\begin{verbatim}
+input.sample << input_sample;
+rbmc.setAsDownInput( input.sample );
+hidden.getAllActivations( rbmc );
+hidden.generateSample();
+hidden_sample << hidden.sample;
+\end{verbatim}
+
+\subsection{Step of Contrastive Divergence in an RBM}
+
+One step of contrastive divergence learning (with only one example, {\tt
+input\_sample}) in the same RBM would be:
+
+\begin{verbatim}
+// positive phase
+input.sample << input_sample;
+rbmc.setAsDownInput( input.sample );
+hidden.getAllActivations( rbmc );
+hidden.computeExpectation();
+hidden.generateSample();
+input.accumulatePosStats( input.sample );
+rbmc.accumulatePosStats( input.sample, hidden.expectation );
+hidden.accumulatePosStats( hidden.expectation );
+
+// down propagation
+rbmc.setAsUpInput( hidden.sample );
+input.getAllActivations( rbmc );
+input.generateSample();
+
+// negative phase
+rbmc.setAsDownInput( input.sample );
+hidden.getAllActivations( rbmc );
+hidden.computeExpectation();
+input.accumulateNegStats( input.sample );
+rbmc.accumulateNegStats( input.sample, hidden.expectation );
+hidden.accumulateNegStats( hidden.expectation );
+
+// update
+input.update();
+rbmc.update();
+hidden.update();
+\end{verbatim}
+
+{\bf Note}: it was empirically shown that the convergence is better if we
+use {\tt hidden.expectation} instead of {\tt hidden.sample} in the
+statistics.
+
+Or {\tt update(\ldots, \ldots)}
+
+\subsection{Learning in a DBN}
+
+Instead of having only one RBM, let's consider three sequential layers
+({\tt input}, {\tt hidden}, {\tt output}) and two connections:
+
+\begin{itemize}
+    \item {\tt rbmc\_ih} between {\tt input} and {\tt hidden};
+    \item {\tt rbmp\_ho} between {\tt hidden} and {\tt output}.
+\end{itemize}
+They form a (small) DBN.
+
+We first train the first RBM formed by ({\tt input}, {\tt rbmc\_ih},
+{\tt hidden}) as shown previously, ignoring the other elements. Then,
+we freeze the parameters of {\tt input} and {\tt rbmc\_ih}, and train
+the second RBM, formed by ({\tt hidden}, {\tt rbmc\_ho}, {\tt output})
+taking the output of the first one as inputs.
+
+One step of this second phase (with only one example, {\tt
+input\_sample}) will look like:
+
+\begin{verbatim}
+// propagation to hidden
+input.sample << input_sample;
+rbmc_ih.setAsDownInput( input.sample );
+hidden.getAllActivations( rbmc_ih );
+hidden.computeExpectation(); // we use mean-field approximation
+
+// positive phase
+rbmc_ho.setAsDownInput( hidden.expectation );
+output.getAllActivations( rbmc_ho );
+output.computeExpectation();
+output.generateSample();
+hidden.accumulatePosStats( hidden.expectation );
+rbmc_ho.accumulatePosStats( hidden.expectation, output.expectation );
+output.accumulatePosStats( output.expectation );
+
+// down propagation
+rbmc_ho.setAsUpInput( output.sample );
+hidden.getAllActivations( rbmc_ho );
+hidden.generateSample();
+
+// negative phase
+rbmc_ho.setAsDownInput( hidden.sample );
+output.getUnitActivations( rbmc_ho );
+output.computeExpectation();
+hidden.accumulateNegStats( hidden.sample );
+rbmc_ho.accumulateNegStats( hidden.sample, output.expectation );
+output.accumulateNegStats( output.expectation );
+
+// update
+hidden.update();
+rbmc_ho.update();
+output.update();
+\end{verbatim}
+
+\section{The {\tt DeepBeliefNet} Class}
+\label{DeepBeliefNet}
+
+
+To be continued\ldots
+
+
+
+
+
+\chapter*{License}
+
+This document is covered by the license appearing after the title page.
+
+The PLearn software library and tools described in this document are
+distributed under the following BSD-type license:
+
+\begin{verbatim}
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions are met:
+
+1. Redistributions of source code must retain the above copyright
+notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright
+notice, this list of conditions and the following disclaimer in the
+documentation and/or other materials provided with the distribution.
+
+3. The name of the authors may not be used to endorse or promote
+products derived from this software without specific prior written
+permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+\end{verbatim}
+
+
+\end{document}
+

Modified: trunk/doc/programmers_guide.tex
===================================================================
--- trunk/doc/programmers_guide.tex	2007-01-18 17:24:46 UTC (rev 6589)
+++ trunk/doc/programmers_guide.tex	2007-01-20 00:05:13 UTC (rev 6590)
@@ -1953,7 +1953,15 @@
 }
 \end{verbatim}
 
+%----------------------------------------------------------------------
+\section{Online Learning}
+%----------------------------------------------------------------------
 
+\subsection{OnlineLearningModule}
+
+They can be found in {\tt \$\{PLEARNDIR\}/plearn\_learners/online}.
+
+
 \chapter{Advanced}
 
 \section{RandomVar}



From dorionc at mail.berlios.de  Mon Jan 22 04:50:16 2007
From: dorionc at mail.berlios.de (dorionc at BerliOS)
Date: Mon, 22 Jan 2007 04:50:16 +0100
Subject: [Plearn-commits] r6591 - in trunk/python_modules/plearn/math/stats:
	. .pytest .pytest/PL_cvx_numarray_matrix_conversions
	.pytest/PL_cvx_numarray_matrix_conversions/expected_results
Message-ID: <200701220350.l0M3oGwq031702@sheep.berlios.de>

Author: dorionc
Date: 2007-01-22 04:50:16 +0100 (Mon, 22 Jan 2007)
New Revision: 6591

Added:
   trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions/
   trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions/expected_results/
   trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions/expected_results/RUN.log
Modified:
   trunk/python_modules/plearn/math/stats/__init__.py
   trunk/python_modules/plearn/math/stats/cvx_utils.py
   trunk/python_modules/plearn/math/stats/pytest.config
Log:
Added conversions and tests for NumArray



Property changes on: trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions
___________________________________________________________________
Name: svn:ignore
   + .plearn
run_results


Added: trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions/expected_results/RUN.log
===================================================================
--- trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions/expected_results/RUN.log	2007-01-20 00:05:13 UTC (rev 6590)
+++ trunk/python_modules/plearn/math/stats/.pytest/PL_cvx_numarray_matrix_conversions/expected_results/RUN.log	2007-01-22 03:50:16 UTC (rev 6591)
@@ -0,0 +1,56 @@
+-------------------------------------------------------------------------------- 
+Test Case 1
+--------------------------------------------------------------------------------
+NumArray matrix shaped (10,)
+[ 1  2  3  4  5  6  7  8  9 10]
+CVX matrix shaped (1, 10)
+   1.0000e+00   2.0000e+00   3.0000e+00   4.0000e+00   5.0000e+00   6.0000e+00   7.0000e+00   8.0000e+00   9.0000e+00   1.0000e+01
+
+
+-------------------------------------------------------------------------------- 
+Test Case 2
+--------------------------------------------------------------------------------
+NumArray matrix shaped (1, 10)
+[[ 1  2  3  4  5  6  7  8  9 10]]
+CVX matrix shaped (1, 10)
+   1.0000e+00   2.0000e+00   3.0000e+00   4.0000e+00   5.0000e+00   6.0000e+00   7.0000e+00   8.0000e+00   9.0000e+00   1.0000e+01
+
+
+-------------------------------------------------------------------------------- 
+Test Case 3
+--------------------------------------------------------------------------------
+NumArray matrix shaped (10, 1)
+[[ 1]
+ [ 2]
+ [ 3]
+ [ 4]
+ [ 5]
+ [ 6]
+ [ 7]
+ [ 8]
+ [ 9]
+ [10]]
+CVX matrix shaped (10, 1)
+   1.0000e+00
+   2.0000e+00
+   3.0000e+00
+   4.0000e+00
+   5.0000e+00
+   6.0000e+00
+   7.0000e+00
+   8.0000e+00
+   9.0000e+00
+   1.0000e+01
+
+
+-------------------------------------------------------------------------------- 
+Test Case 4
+--------------------------------------------------------------------------------
+NumArray matrix shaped (2, 10)
+[[ 1  2  3  4  5  6  7  8  9 10]
+ [11 12 13 14 15 16 17 18 19 20]]
+CVX matrix shaped (2, 10)
+   1.0000e+00   2.0000e+00   3.0000e+00   4.0000e+00   5.0000e+00   6.0000e+00   7.0000e+00   8.0000e+00   9.0000e+00   1.0000e+01
+   1.1000e+01   1.2000e+01   1.3000e+01   1.4000e+01   1.5000e+01   1.6000e+01   1.7000e+01   1.8000e+01   1.9000e+01   2.0000e+01
+
+

Modified: trunk/python_modules/plearn/math/stats/__init__.py
===================================================================
--- trunk/python_modules/plearn/math/stats/__init__.py	2007-01-20 00:05:13 UTC (rev 6590)
+++ trunk/python_modules/plearn/math/stats/__init__.py	2007-01-22 03:50:16 UTC (rev 6591)
@@ -1,3 +1,36 @@
+# stats/__init__.py
+# Copyright (C) 2007 Christian Dorion
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Christian Dorion
 """UNDER DEVELOPMENT!
 
 This package is meant to bundle statistical tools relying on RPy whenever

Modified: trunk/python_modules/plearn/math/stats/cvx_utils.py
===================================================================
--- trunk/python_modules/plearn/math/stats/cvx_utils.py	2007-01-20 00:05:13 UTC (rev 6590)
+++ trunk/python_modules/plearn/math/stats/cvx_utils.py	2007-01-22 03:50:16 UTC (rev 6591)
@@ -1,19 +1,97 @@
-"""Provides functions to use CVX transparently where NumPy is the standard."""
+# cvx_utils.py
+# Copyright (C) 2007 Christian Dorion
+#
+#  Redistribution and use in source and binary forms, with or without
+#  modification, are permitted provided that the following conditions are met:
+#
+#   1. Redistributions of source code must retain the above copyright
+#      notice, this list of conditions and the following disclaimer.
+#
+#   2. Redistributions in binary form must reproduce the above copyright
+#      notice, this list of conditions and the following disclaimer in the
+#      documentation and/or other materials provided with the distribution.
+#
+#   3. The name of the authors may not be used to endorse or promote
+#      products derived from this software without specific prior written
+#      permission.
+#
+#  THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+#  IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+#  OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+#  NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+#  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+#  TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+#
+#  This file is part of the PLearn library. For more information on the PLearn
+#  library, go to the PLearn Web site at www.plearn.org
+
+
+# Author: Christian Dorion
+"""Provides functions to use CVX transparently where NumPy or NumArray is the standard."""
 import numpy
-from cvxopt.base import matrix, spmatrix, log
+import numarray 
+from cvxopt import base as cvx
 
+
+#####  NumArray vs CVX  #####################################################
+
+def numarray_asmatrix(a):
+    if len(a.shape) == 1:
+        return numarray.array([ a ]) # One row matrix
+    elif len(a.shape) == 2:
+        return a
+    raise ValueError, a
+
+def numarray_to_cvx(*args):
+    """Returns CVX matrices from NumArray matrices
+
+    Note that NumArray one-dimensional arrays are transformed as row vectors
+    in CVX. See/Run test_numarray_cvx_conversions() for details.
+    """
+    matrices = []
+    for a in args:
+        a    = numarray_asmatrix(a)
+        m, n = a.shape
+        a    = numarray.ravel( numarray.transpose(a) )
+        matrices.append( cvx.matrix(a, (m,n), 'd') )
+
+    if len(matrices)==1:
+        return matrices[0]
+    return matrices
+
+def cvx_to_numarray(*args):
+    """Returns NumArray matrices from CVX matrices
+
+    Note that CVX vectors are returns as one-dimensional NumArray
+    matrices. See/Run test_numarray_cvx_conversions() for details.
+    """
+    matrices = []
+    for a in args:
+        matrices.append( numarray.array(a) )
+
+    if len(matrices)==1:
+        return matrices[0]
+    return matrices
+
+
+#####  NumPy vs CVX  ########################################################
+
 def numpy_to_cvx(*args):
     """Returns CVX matrices from NumPy matrices
 
     Note that NumPy one-dimensional arrays are transformed as row vectors
-    in CVX. See/Run test_matrix_conversions() for details.
+    in CVX. See/Run test_numpy_cvx_conversions() for details.
     """
     matrices = []
     for a in args:
         a    = numpy.asmatrix(a)
         m, n = a.shape
         a    = numpy.ravel(a.transpose())
-        matrices.append( matrix(a, (m,n), 'd') )
+        matrices.append( cvx.matrix(a, (m,n), 'd') )
 
     if len(matrices)==1:
         return matrices[0]
@@ -23,7 +101,7 @@
     """Returns NumPy matrices from CVX matrices
 
     Note that CVX vectors are returns as one-dimensional NumPy
-    matrices. See/Run test_matrix_conversions() for details.
+    matrices. See/Run test_numpy_cvx_conversions() for details.
     """
     matrices = []
     for a in args:
@@ -34,9 +112,121 @@
     return matrices
 
 
+#####  Numerical Derivatives  ###############################################
+
+def numerical_gradient(x_k, func, epsilon, *args):
+    n_params, one = x_k.size
+    assert one==1
+    
+    ei = cvx.matrix(0.0, x_k.size)
+    grad = cvx.matrix(0.0, x_k.size)
+    for i in range(n_params):
+        ei[i]   = epsilon
+        grad[i] = ( func(x_k+ei, *args) - func(x_k-ei, *args) ) / (2*epsilon)
+        ei[i]   = 0.0
+    return grad.T
+
+def numerical_hessian(x_k, f_k, func, epsilon, *args):
+    n_params, one = x_k.size
+    assert one==1
+    H = cvx.matrix(0.0, (n_params,n_params))
+    epsilon2 = epsilon**2
+    
+    ei = cvx.matrix(0.0, x_k.size)
+    esame = cvx.matrix(0.0, x_k.size)
+    ecross = cvx.matrix(0.0, x_k.size)
+    for i in range(n_params):
+        esame[i], ecross[i] = +epsilon, +epsilon
+        for j in range(i):            
+            esame[j], ecross[j] = +epsilon, -epsilon
+
+            lhs = func(x_k+esame, *args) - func(x_k+ecross, *args)
+            rhs = func(x_k-ecross, *args) - func(x_k-esame, *args)            
+            H[i,j] = (lhs - rhs) / (4*epsilon2)
+            H[j,i] = H[i,j]
+
+            esame[j], ecross[j] = 0.0, 0.0
+        esame[i], ecross[i] = 0.0, 0.0
+                
+        ei[i] = epsilon        
+        H[i,i] = (func(x_k+ei, *args) + func(x_k-ei, *args) - 2*f_k) / epsilon2        
+        ei[i] = 0.0        
+
+    return H
+
+def use_numerical_derivatives_nc(objective, starting_values, epsilon):
+    """NC stands for 'no nonlinear constraints'"""
+    def _using_numerical_derivatives(params=None, z=None):
+        # Access pattern for starting values
+        if params is None:
+            return 0, starting_values
+
+        # If objective returns None, this function shall return
+        OBJ = objective(params)
+        if OBJ is None: return None
+        
+        # Compute numerical gradient
+        OBJ_D = numerical_gradient(params, objective, epsilon)
+        if z is None:
+            return cvx.matrix(OBJ, (1,1)), OBJ_D
+
+        # Compute the numerical Hessian
+        assert z.size==(1,1)
+        OBJ_H = z[0,0]*numerical_hessian(params, OBJ, objective, epsilon)
+        return cvx.matrix(OBJ, (1,1)), OBJ_D, OBJ_H
+
+    def using_numerical_derivatives(params=None, z=None):
+        out = _using_numerical_derivatives(params, z)
+        print "OUT",
+        for o in out:
+            print o
+        if len(out)==3:
+            from cvxopt.lapack import syev
+            W = cvx.matrix(0.0, (6,1), 'd')
+            syev(out[-1], W)
+            print "EIGEN", W
+        return out
+        
+    return using_numerical_derivatives
+
+
 #####  Builtin Tests  #######################################################
 
-def test_matrix_conversions():
+def test_numarray_cvx_conversions():
+    _1_10_ = numarray.array( range(1,11) )
+    test_cases = [
+        _1_10_,
+        numarray.array([ _1_10_ ]),
+        numarray.transpose( numarray.array([ _1_10_ ]) ),        
+        numarray.array([ _1_10_, range(11, 21) ])
+        ]
+
+    for t, a in enumerate(test_cases):
+        print "-"*80, '\nTest Case %d\n'%(t+1), "-"*80        
+        print "NumArray matrix shaped", a.shape
+        print a
+        
+        a_cvx = numarray_to_cvx(a)
+        print "CVX matrix shaped", a_cvx.size
+        print a_cvx
+
+        a = numarray_asmatrix(a)
+        a_numarray = cvx_to_numarray(a_cvx)
+        m, n = a.shape
+        assert (m,n)==a_cvx.size, \
+               "(m,n) = (%d,%d) != %s = a_cvx.size"%(m, n, a_cvx.size)
+        assert (m,n)==a_numarray.shape, \
+               "(m,n) = (%d,%d) != %s = a_numarray.size"%(m, n, a_numarray.shape)
+
+        for i in range(m):
+            for j in range(n):
+                assert a[i,j] == a_cvx[i,j], "a[%d,%d] = %f != %f = a_cvx[%d,%d]"%(
+                    i, j, a[i,j], a_cvx[i,j], i, j )
+                assert a[i,j] == a_numarray[i,j], "a[%d,%d] = %f != %f = a_numarray[%d,%d]"%(
+                    i, j, a[i,j], a_numarray[i,j], i, j )
+        print
+
+def test_numpy_cvx_conversions():
     _1_10_ = numpy.array( range(1,11) ) 
     test_cases = [
         _1_10_,
@@ -71,4 +261,4 @@
         print
 
 if __name__ == "__main__":
-    test_matrix_conversions()
+    test_numarray_cvx_conversions()

Modified: trunk/python_modules/plearn/math/stats/pytest.config
===================================================================
--- trunk/python_modules/plearn/math/stats/pytest.config	2007-01-20 00:05:13 UTC (rev 6590)
+++ trunk/python_modules/plearn/math/stats/pytest.config	2007-01-22 03:50:16 UTC (rev 6591)
@@ -93,11 +93,23 @@
     
 """
 Test(
+    name = "PL_cvx_numarray_matrix_conversions",
+    description = "",
+    category = "General",
+    program = Program(name = "python"),
+    arguments = "-c 'from plearn.math.stats.cvx_utils import *; test_numarray_cvx_conversions()'",
+    resources = [ ],
+    precision = 1e-06,
+    pfileprg = None,
+    disabled = False
+    )
+
+Test(
     name = "PL_cvx_numpy_matrix_conversions",
     description = "",
     category = "General",
     program = Program(name = "python"),
-    arguments = "-c 'from plearn.math.stats.cvx_utils import *; test_matrix_conversions()'",
+    arguments = "-c 'from plearn.math.stats.cvx_utils import *; test_numpy_cvx_conversions()'",
     resources = [ ],
     precision = 1e-06,
     pfileprg = None,



From chrish at mail.berlios.de  Mon Jan 22 22:11:07 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 22 Jan 2007 22:11:07 +0100
Subject: [Plearn-commits] r6592 - trunk/plearn_learners/testers
Message-ID: <200701222111.l0MLB7v4005026@sheep.berlios.de>

Author: chrish
Date: 2007-01-22 22:11:06 +0100 (Mon, 22 Jan 2007)
New Revision: 6592

Modified:
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
* Add should_test option to PTester, so we can use the PTester to do just
  the training phase of the experiment, and do the testing later.
* Remove some dead, commented-out code.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-01-22 03:50:16 UTC (rev 6591)
+++ trunk/plearn_learners/testers/PTester.cc	2007-01-22 21:11:06 UTC (rev 6592)
@@ -90,6 +90,7 @@
        call_forget_in_run(true),
        save_test_confidence(false),
        train(true),
+       should_test(true),
        enforce_clean_expdir(true)
 {}
 
@@ -213,6 +214,13 @@
         "to load an already trained learner in the 'learner' field)");
 
     declareOption(
+        ol, "should_test", &PTester::should_test, OptionBase::buildoption,
+        "Whether to carry out the test at all. This can be used, for instance,\n"
+        "to train only (without testing) and save the learners, and test later. \n"
+        "Any test statistics that are required to be computed if 'should_test'\n"
+        "is false yield MISSING_VALUE.\n");
+    
+    declareOption(
         ol, "template_stats_collector", &PTester::template_stats_collector, OptionBase::buildoption,
         "If provided, this instance of a subclass of VecStatsCollector will be used as a template\n"
         "to build all the stats collector used during training and testing of the learner");
@@ -286,52 +294,7 @@
          RetDoc ("Current expdir.")));
 }
 
-// The following is no longer necessary with the declareMethod mechanism
-// (to be deleted)
 
-/**
- * void PTester::call(const string& methodname, int nargs, PStream& io)
- * {
- *     if(methodname=="perform")
- *     {
- *         if(nargs!=1) PLERROR("PTester remote method perform takes 1 argument");
- *         bool call_forget;
- *         io >> call_forget;
- *         Vec result = perform(call_forget);
- *         prepareToSendResults(io, 1);
- *         io << result;
- *         io.flush();
- *     }
- *     else if(methodname=="getStatNames")
- *     {
- *         if(nargs!=0) PLERROR("PTester remote method getStatNames takes 0 argument");
- *         TVec<string> result = getStatNames();
- *         prepareToSendResults(io, 1);
- *         io << result;
- *         io.flush();
- *     }
- *     else if(methodname=="setExperimentDirectory")
- *     {
- *         if(nargs!=1) PLERROR("PTester remote method setExperimentDirectory takes 1 argument");
- *         PPath the_expdir;
- *         io >> the_expdir;
- *         setExperimentDirectory(the_expdir);
- *         prepareToSendResults(io, 0);
- *         io.flush();
- *     }
- *     else if(methodname=="getExperimentDirectory")
- *     {
- *         if(nargs!=0) PLERROR("PTester remote method getExperimentDirectory takes 0 arguments");
- *         PPath result = getExperimentDirectory();
- *         prepareToSendResults(io, 1);
- *         io << result;
- *         io.flush();
- *     }
- *     else
- *         inherited::call(methodname, nargs, io);
- * }
- */
-
 void PTester::build_()
 {
 
@@ -598,72 +561,76 @@
             // perf_eval_costs[setnum][perf_evaluator_name][costname] will contain value
             // of the given cost returned by the given perf_evaluator on the given setnum
             TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
-            for(int setnum=1; setnum<dsets.length(); setnum++)
-            {
-                VMat testset = dsets[setnum];
-                PP<VecStatsCollector> test_stats = stcol[setnum];
-                string setname = "test"+tostring(setnum);
-                if(is_splitdir && save_data_sets)
-                    PLearn::save(splitdir/(setname+"_set.psave"),testset);
-                VMat test_outputs;
-                VMat test_costs;
-                VMat test_confidence;
-                if (is_splitdir)
-                    force_mkdir(splitdir); // TODO Why is this done so late?
 
-                if(is_splitdir && save_test_outputs)
-                    test_outputs = new FileVMatrix(splitdir/(setname+"_outputs.pmat"),0,learner->getOutputNames());
+            // Perform the test if required
+            if (should_test) {
+                for(int setnum=1; setnum<dsets.length(); setnum++)
+                {
+                    VMat testset = dsets[setnum];
+                    PP<VecStatsCollector> test_stats = stcol[setnum];
+                    string setname = "test"+tostring(setnum);
+                    if(is_splitdir && save_data_sets)
+                        PLearn::save(splitdir/(setname+"_set.psave"),testset);
+                    VMat test_outputs;
+                    VMat test_costs;
+                    VMat test_confidence;
+                    if (is_splitdir)
+                        force_mkdir(splitdir); // TODO Why is this done so late?
+
+                    if(is_splitdir && save_test_outputs)
+                        test_outputs = new FileVMatrix(splitdir/(setname+"_outputs.pmat"),0,learner->getOutputNames());
                     //test_outputs = new FileVMatrix(splitdir/(setname+"_outputs.pmat"),0,outputsize);
-                else if(!perf_evaluators.empty()) // we don't want to save test outputs to disk, but we need them for pef_evaluators
-                { // So let's store them in a MemoryVMatrix
-                    Mat data(testset.length(),outputsize);
-                    data.resize(0,outputsize);
-                    test_outputs = new MemoryVMatrix(data);
-                    test_outputs->declareFieldNames(learner->getOutputNames());
-                }
+                    else if(!perf_evaluators.empty()) // we don't want to save test outputs to disk, but we need them for pef_evaluators
+                    { // So let's store them in a MemoryVMatrix
+                        Mat data(testset.length(),outputsize);
+                        data.resize(0,outputsize);
+                        test_outputs = new MemoryVMatrix(data);
+                        test_outputs->declareFieldNames(learner->getOutputNames());
+                    }
 
-                if(is_splitdir && save_test_costs)
-                    test_costs = new FileVMatrix(splitdir/(setname+"_costs.pmat"),0,learner->getTestCostNames());
+                    if(is_splitdir && save_test_costs)
+                        test_costs = new FileVMatrix(splitdir/(setname+"_costs.pmat"),0,learner->getTestCostNames());
                     //test_costs = new FileVMatrix(splitdir/(setname+"_costs.pmat"),0,testcostsize);
-                if(is_splitdir && save_test_confidence)
-                    test_confidence = new FileVMatrix(splitdir/(setname+"_confidence.pmat"),
-                                                      0,2*outputsize);
+                    if(is_splitdir && save_test_confidence)
+                        test_confidence = new FileVMatrix(splitdir/(setname+"_confidence.pmat"),
+                                                          0,2*outputsize);
 
-                bool reset_stats = (acc.find(setnum) == -1);
+                    bool reset_stats = (acc.find(setnum) == -1);
 
-                //perr << "reset_stats= " << reset_stats << endl;
+                    //perr << "reset_stats= " << reset_stats << endl;
 
-                if (reset_stats)
-                    test_stats->forget();
-                if (testset->length()==0)
-                    PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",setname.c_str());
+                    if (reset_stats)
+                        test_stats->forget();
+                    if (testset->length()==0)
+                        PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",setname.c_str());
 
-                // Before each test set, reset the internal state of the learner
-                learner->resetInternalState();
+                    // Before each test set, reset the internal state of the learner
+                    learner->resetInternalState();
 
-                learner->test(testset, test_stats, test_outputs, test_costs);
-                if (reset_stats)
-                    test_stats->finalize();
-                if(is_splitdir && save_stat_collectors)
-                    PLearn::save(splitdir/(setname+"_stats.psave"),test_stats);
+                    learner->test(testset, test_stats, test_outputs, test_costs);
+                    if (reset_stats)
+                        test_stats->finalize();
+                    if(is_splitdir && save_stat_collectors)
+                        PLearn::save(splitdir/(setname+"_stats.psave"),test_stats);
 
-                perf_evaluators_t::iterator it = perf_evaluators.begin();
-                perf_evaluators_t::iterator itend = perf_evaluators.end();
-                while(it!=itend)
-                {
-                    PPath perf_eval_dir;
-                    if(is_splitdir)
-                        perf_eval_dir = splitdir/setname/("perfeval_"+it->first);
-                    Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
-                    TVec<string> perf_costnames = it->second->getCostNames();
-                    if(perf_costvals.length()!=perf_costnames.length())
-                        PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
-                    map<string, real>& costmap = perf_eval_costs[setnum][it->first];
-                    for(int costi = 0; costi<perf_costnames.length(); costi++)
-                        costmap[perf_costnames[costi]] = perf_costvals[costi];
-                    ++it;
+                    perf_evaluators_t::iterator it = perf_evaluators.begin();
+                    perf_evaluators_t::iterator itend = perf_evaluators.end();
+                    while(it!=itend)
+                    {
+                        PPath perf_eval_dir;
+                        if(is_splitdir)
+                            perf_eval_dir = splitdir/setname/("perfeval_"+it->first);
+                        Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
+                        TVec<string> perf_costnames = it->second->getCostNames();
+                        if(perf_costvals.length()!=perf_costnames.length())
+                            PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
+                        map<string, real>& costmap = perf_eval_costs[setnum][it->first];
+                        for(int costi = 0; costi<perf_costnames.length(); costi++)
+                            costmap[perf_costnames[costi]] = perf_costvals[costi];
+                        ++it;
+                    }
+                    computeConfidence(testset, test_confidence);
                 }
-                computeConfidence(testset, test_confidence);
             }
 
             Vec splitres(1+nstats);
@@ -671,11 +638,15 @@
 
             for(int k=0; k<nstats; k++)
             {
+                // If we ask for a test-set that's beyond what's currently
+                // available, OR we are asking for test-statistics in
+                // train-only mode, then the statistic is MISSING_VALUE.
                 StatSpec& sp = statspecs[k];
-                if (sp.setnum>=stcol.length())
+                if (sp.setnum>=stcol.length() ||
+                    (! should_test && sp.setnum > 0))
+                {
                     splitres[k+1] = MISSING_VALUE;
-//            PLERROR("PTester::perform, trying to access a test set (test%d) beyond the last one (test%d)",
-//                    sp.setnum, stcol.length()-1);
+                }
                 else
                 {
                     if (acc.find(sp.setnum) == -1)

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2007-01-22 03:50:16 UTC (rev 6591)
+++ trunk/plearn_learners/testers/PTester.h	2007-01-22 21:11:06 UTC (rev 6592)
@@ -102,9 +102,15 @@
     /// intervals are saved in a file SETNAME_confidence.pmat (default=false)
     bool save_test_confidence;
   
-    /// whether or not to train or just test
+    /// whether or not to train or just test (see 'should_test', below)
     bool train;
 
+    /// Whether to carry out the test at all. This can be used, for instance,
+    /// to train only (without testing) and save the learners, and test later. 
+    /// Any test statistics that are required to be computed if 'should_test'
+    /// is false yield MISSING_VALUE.
+    bool should_test;
+
     /**
      *  If this option is true, the PTester ensures that the expdir does not
      *  already exist when the experiment is started, and gives a PLerror



From chrish at mail.berlios.de  Mon Jan 22 22:13:12 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 22 Jan 2007 22:13:12 +0100
Subject: [Plearn-commits] r6593 - trunk/plearn/vmat
Message-ID: <200701222113.l0MLDCQH005239@sheep.berlios.de>

Author: chrish
Date: 2007-01-22 22:13:11 +0100 (Mon, 22 Jan 2007)
New Revision: 6593

Modified:
   trunk/plearn/vmat/VMatLanguage.cc
Log:
Add copy-single-field-but-dont-fail-if-it-doesnt-exist field copy operator.


Modified: trunk/plearn/vmat/VMatLanguage.cc
===================================================================
--- trunk/plearn/vmat/VMatLanguage.cc	2007-01-22 21:11:06 UTC (rev 6592)
+++ trunk/plearn/vmat/VMatLanguage.cc	2007-01-22 21:13:11 UTC (rev 6593)
@@ -89,7 +89,10 @@
                         "can be in @ or % notation, with the keyword 'END' denoting the last field).\n"
                         "The fields can also be transformed with a VPL program using the syntax:\n"
                         "[field1:fieldn:vpl_code], where vpl_code can be any VPL code, for example\n"
-                        "for a 0.5 thresholding: 0.5 < 0 1 ifelse.\n"
+                        "for a 0.5 thresholding: 0.5 < 0 1 ifelse. To copy a single field, use [field].\n"
+                        "There is also a special feature available only for single field copies: if you\n"
+                        "use the syntax [field?], then VPL will not produce an error if the field cannot\n"
+                        "be found.\n"
                         "\n"
                         "Here's a real-life example of a VPL program:\n"
                         "\n"
@@ -410,6 +413,13 @@
             else if(parts.size()==1)
                 // fieldcopy macro type is [field]
             {
+                bool ignore_if_missing = false;
+                if (parts[0][parts[0].size()-1] == '?') {
+                    ignore_if_missing = true;
+                    // Remove ending '?' character.
+                    parts[0] = parts[0].substr(0, parts[0].size()-1);
+                }
+                
                 string astr=parts[0].substr(1);
                 int a=-1;
                 if(parts[0][0]=='@')
@@ -420,13 +430,17 @@
                 else if(parts[0][0]=='%')
                     a=toint(parts[0].substr(1));
                 else PLERROR("fieldcopy macro syntax is : [start:end] EG: [@year:%6]. 'end' must be after 'start'.. OR [field] to copy a single field");
-                if(a==-1)
-                    PLERROR("In copyfield macro, unknown field :%s",astr.c_str());
-                processed_sourcecode+=string("%")+tostring(a)+ " ";
-                if (a >= srcfieldnames.length())
-                    PLERROR("In VMatLanguage::preprocess - Asked field number %d, but there "
-                            "are only %d fields available", a, srcfieldnames.length());
-                fieldnames.push_back(srcfieldnames[a]);
+                if (a == -1) {
+                    if (!ignore_if_missing)
+                        PLERROR("In copyfield macro, unknown field :%s",astr.c_str());
+                }
+                else {
+                    processed_sourcecode+=string("%")+tostring(a)+ " ";
+                    if (a >= srcfieldnames.length())
+                        PLERROR("In VMatLanguage::preprocess - Asked field number %d, but there "
+                                "are only %d fields available", a, srcfieldnames.length());
+                    fieldnames.push_back(srcfieldnames[a]);
+                }
             }
             else PLERROR("Strange fieldcopy format. e.g : [%0:%5]. Found parts '%s'",join(parts," ").c_str());
         }



From chrish at mail.berlios.de  Mon Jan 22 23:40:12 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Mon, 22 Jan 2007 23:40:12 +0100
Subject: [Plearn-commits] r6594 - trunk/plearn/io
Message-ID: <200701222240.l0MMeCKm017896@sheep.berlios.de>

Author: chrish
Date: 2007-01-22 23:40:11 +0100 (Mon, 22 Jan 2007)
New Revision: 6594

Modified:
   trunk/plearn/io/fileutils.cc
Log:
Don't force permissions to 0775 in force_mkdir. Unless you're dealing with
sensitive information like passwords, PGP keys, etc. (which is not in
PLearn's job description) setting the permissions on newly created
directories is the job of the user's umask settings. Don't override it!


Modified: trunk/plearn/io/fileutils.cc
===================================================================
--- trunk/plearn/io/fileutils.cc	2007-01-22 21:13:11 UTC (rev 6593)
+++ trunk/plearn/io/fileutils.cc	2007-01-22 22:40:11 UTC (rev 6594)
@@ -240,7 +240,7 @@
     }
     for (int i = int(paths.size()) - 1; i >= 0; i--)
         if (!isdir(paths[i]) &&
-            PR_MkDir(paths[i].absolute().c_str(), 0775) != PR_SUCCESS)
+            PR_MkDir(paths[i].absolute().c_str(), 0777) != PR_SUCCESS)
             return false;
     return true;
 }



From tihocan at mail.berlios.de  Tue Jan 23 16:27:17 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 23 Jan 2007 16:27:17 +0100
Subject: [Plearn-commits] r6595 -
	trunk/plearn_learners/regressors/test/GaussianProcessRegressor
Message-ID: <200701231527.l0NFRH9X018299@sheep.berlios.de>

Author: tihocan
Date: 2007-01-23 16:27:16 +0100 (Tue, 23 Jan 2007)
New Revision: 6595

Modified:
   trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
Log:
The perl command to truncate numbers to 6 figures is not needed, since pldiff is smart enough

Modified: trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config
===================================================================
--- trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-01-22 22:40:11 UTC (rev 6594)
+++ trunk/plearn_learners/regressors/test/GaussianProcessRegressor/pytest.config	2007-01-23 15:27:16 UTC (rev 6595)
@@ -100,7 +100,7 @@
         name = "plearn_tests",
         compiler = "pymake"
         ),
-    arguments = "server < INPUTS_GPR | perl -pe 's/(\d\.\d\d\d\d\d\d)\d+/$1/g;'",
+    arguments = "server < INPUTS_GPR",
     resources = [ "INPUTS_GPR", "learner_hyperopt.plearn" ],
     precision = 1e-06,
     pfileprg = "__program__",



From tihocan at mail.berlios.de  Tue Jan 23 21:43:55 2007
From: tihocan at mail.berlios.de (tihocan at BerliOS)
Date: Tue, 23 Jan 2007 21:43:55 +0100
Subject: [Plearn-commits] r6596 - trunk/plearn_learners/testers
Message-ID: <200701232043.l0NKhtjw022219@sheep.berlios.de>

Author: tihocan
Date: 2007-01-23 21:43:54 +0100 (Tue, 23 Jan 2007)
New Revision: 6596

Modified:
   trunk/plearn_learners/testers/PTester.cc
   trunk/plearn_learners/testers/PTester.h
Log:
Renamed option 'train' into 'should_train', which is more understandable, and also coherent with the new 'should_test' option

Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-01-23 15:27:16 UTC (rev 6595)
+++ trunk/plearn_learners/testers/PTester.cc	2007-01-23 20:43:54 UTC (rev 6596)
@@ -89,7 +89,7 @@
        save_test_outputs(false),
        call_forget_in_run(true),
        save_test_confidence(false),
-       train(true),
+       should_train(true),
        should_test(true),
        enforce_clean_expdir(true)
 {}
@@ -209,11 +209,17 @@
         "If true, each learner to be trained will have its experiment directory set to Split#k/LearnerExpdir/");
 
     declareOption(
-        ol, "train", &PTester::train, OptionBase::buildoption,
+        ol, "should_train", &PTester::should_train, OptionBase::buildoption,
         "If true, the learners are trained, otherwise only tested (in that case it is advised\n"
         "to load an already trained learner in the 'learner' field)");
 
     declareOption(
+        ol, "train", &PTester::should_train,
+        OptionBase::learntoption | OptionBase::nosave,
+        "DEPRECATED - This option has been renamed to 'should_train' in\n"
+        "order to make it coherent with the 'should_test' option.");
+
+    declareOption(
         ol, "should_test", &PTester::should_test, OptionBase::buildoption,
         "Whether to carry out the test at all. This can be used, for instance,\n"
         "to train only (without testing) and save the learners, and test later. \n"
@@ -527,7 +533,7 @@
             if(is_splitdir && save_data_sets)
                 PLearn::save(splitdir/"training_set.psave",trainset);
 
-            if(train && provide_learner_expdir)
+            if(should_train && provide_learner_expdir)
             {
                 if(is_splitdir)
                     learner->setExperimentDirectory( splitdir/"LearnerExpdir/" );
@@ -535,14 +541,14 @@
                     learner->setExperimentDirectory("");
             }
 
-            learner->setTrainingSet(trainset, call_forget && train);
+            learner->setTrainingSet(trainset, call_forget && should_train);
             if(dsets.size()>1)
                 learner->setValidationSet(dsets[1]);
 
             int outputsize = learner->outputsize();
 
 
-            if (train)
+            if (should_train)
             {
                 if(is_splitdir && save_initial_learners)
                     PLearn::save(splitdir/"initial_learner.psave",learner);

Modified: trunk/plearn_learners/testers/PTester.h
===================================================================
--- trunk/plearn_learners/testers/PTester.h	2007-01-23 15:27:16 UTC (rev 6595)
+++ trunk/plearn_learners/testers/PTester.h	2007-01-23 20:43:54 UTC (rev 6596)
@@ -102,8 +102,8 @@
     /// intervals are saved in a file SETNAME_confidence.pmat (default=false)
     bool save_test_confidence;
   
-    /// whether or not to train or just test (see 'should_test', below)
-    bool train;
+    /// Whether or not to train or just test (see 'should_test', below).
+    bool should_train;
 
     /// Whether to carry out the test at all. This can be used, for instance,
     /// to train only (without testing) and save the learners, and test later. 



From lamblin at mail.berlios.de  Wed Jan 24 02:02:34 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 02:02:34 +0100
Subject: [Plearn-commits] r6597 - trunk/scripts/Skeletons
Message-ID: <200701240102.l0O12Ya4018247@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 02:02:34 +0100 (Wed, 24 Jan 2007)
New Revision: 6597

Modified:
   trunk/scripts/Skeletons/OnlineLearningModule.cc
   trunk/scripts/Skeletons/OnlineLearningModule.h
Log:
Reformat OnlineLearningModule's skeleton documentation


Modified: trunk/scripts/Skeletons/OnlineLearningModule.cc
===================================================================
--- trunk/scripts/Skeletons/OnlineLearningModule.cc	2007-01-23 20:43:54 UTC (rev 6596)
+++ trunk/scripts/Skeletons/OnlineLearningModule.cc	2007-01-24 01:02:34 UTC (rev 6597)
@@ -75,96 +75,77 @@
     PLERROR("DERIVEDCLASS::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
 }
 
-//! given the input, compute the output (possibly resize it  appropriately)
+///////////
+// fprop //
+///////////
 void DERIVEDCLASS::fprop(const Vec& input, Vec& output) const
 {
 }
 
+/////////////////
+// bpropUpdate //
+/////////////////
 /* THIS METHOD IS OPTIONAL
-//! Adapt based on the output gradient: this method should only
-//! be called just after a corresponding fprop; it should be
-//! called with the same arguments as fprop for the first two arguments
-//! (and output should not have been modified since then).
-//! Since sub-classes are supposed to learn ONLINE, the object
-//! is 'ready-to-be-used' just after any bpropUpdate.
-//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-//! JUST CALLS
-//!     bpropUpdate(input, output, input_gradient, output_gradient)
-//! AND IGNORES INPUT GRADIENT.
 void DERIVEDCLASS::bpropUpdate(const Vec& input, const Vec& output,
+                               Vec& input_gradient,
                                const Vec& output_gradient)
 {
 }
 */
 
 /* THIS METHOD IS OPTIONAL
-//! this version allows to obtain the input gradient as well
-//! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
 void DERIVEDCLASS::bpropUpdate(const Vec& input, const Vec& output,
-                               Vec& input_gradient,
                                const Vec& output_gradient)
 {
 }
 */
 
-//! reset the parameters to the state they would be BEFORE starting training.
-//! Note that this method is necessarily called from build().
-void DERIVEDCLASS::forget()
+//////////////////
+// bbpropUpdate //
+//////////////////
+/* THIS METHOD IS OPTIONAL
+void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& output,
+                                Vec& input_gradient,
+                                const Vec& output_gradient,
+                                Vec& input_diag_hessian,
+                                const Vec& output_diag_hessian)
 {
 }
+*/
 
 /* THIS METHOD IS OPTIONAL
-//! reset the parameters to the state they would be BEFORE starting training.
-//! Note that this method is necessarily called from build().
-//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT DO
-//! ANYTHING.
-void DERIVEDCLASS::finalize()
+void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& output,
+                                const Vec& output_gradient,
+                                const Vec& output_diag_hessian)
 {
 }
 */
 
-/* THIS METHOD IS OPTIONAL
-//! in case bpropUpdate does not do anything, make it known
-//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false;
-bool DERIVEDCLASS::bpropDoesNothing()
+////////////
+// forget //
+////////////
+void DERIVEDCLASS::forget()
 {
 }
-*/
 
+//////////////
+// finalize //
+//////////////
 /* THIS METHOD IS OPTIONAL
-//! Similar to bpropUpdate, but adapt based also on the estimation
-//! of the diagonal of the Hessian matrix, and propagates this
-//! back. If these methods are defined, you can use them INSTEAD of
-//! bpropUpdate(...)
-//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-//! JUST CALLS
-//!     bbpropUpdate(input, output, input_gradient, output_gradient,
-//!                  in_hess, out_hess)
-//! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
-void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& output,
-                                const Vec& output_gradient,
-                                const Vec& output_diag_hessian)
+void DERIVEDCLASS::finalize()
 {
 }
 */
 
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
 /* THIS METHOD IS OPTIONAL
-//! Similar to bpropUpdate, but adapt based also on the estimation
-//! of the diagonal of the Hessian matrix, and propagates this
-//! back. If these methods are defined, you can use them INSTEAD of
-//! bpropUpdate(...)
-//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-//! RAISES A PLERROR.
-void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& output,
-                                Vec& input_gradient,
-                                const Vec& output_gradient,
-                                Vec& input_diag_hessian,
-                                const Vec& output_diag_hessian)
+bool DERIVEDCLASS::bpropDoesNothing()
 {
 }
 */
 
-
 } // end of namespace PLearn
 
 

Modified: trunk/scripts/Skeletons/OnlineLearningModule.h
===================================================================
--- trunk/scripts/Skeletons/OnlineLearningModule.h	2007-01-23 20:43:54 UTC (rev 6596)
+++ trunk/scripts/Skeletons/OnlineLearningModule.h	2007-01-24 01:02:34 UTC (rev 6597)
@@ -38,62 +38,77 @@
     //! given the input, compute the output (possibly resize it  appropriately)
     virtual void fprop(const Vec& input, Vec& output) const;
 
-    //! Adapt based on the output gradient: this method should only
-    //! be called just after a corresponding fprop; it should be
-    //! called with the same arguments as fprop for the first two arguments
-    //! (and output should not have been modified since then).
+    /* Optional
+       THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
     //! Since sub-classes are supposed to learn ONLINE, the object
     //! is 'ready-to-be-used' just after any bpropUpdate.
-    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-    //! JUST CALLS
-    //!     bpropUpdate(input, output, input_gradient, output_gradient)
-    //! AND IGNORES INPUT GRADIENT.
-    // virtual void bpropUpdate(const Vec& input, const Vec& output,
-    //                          const Vec& output_gradient);
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient);
+    */
 
-    //! this version allows to obtain the input gradient as well
-    //! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
-    // virtual void bpropUpdate(const Vec& input, const Vec& output,
-    //                          Vec& input_gradient,
-    //                          const Vec& output_gradient);
+    /* Optional
+       A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, output, input_gradient, output_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+    */
 
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
     //! Similar to bpropUpdate, but adapt based also on the estimation
     //! of the diagonal of the Hessian matrix, and propagates this
     //! back. If these methods are defined, you can use them INSTEAD of
     //! bpropUpdate(...)
-    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
-    //! WHICH JUST CALLS
-    //!     bbpropUpdate(input, output, input_gradient, output_gradient,
-    //!                  out_hess, in_hess)
-    //! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
-    // virtual void bbpropUpdate(const Vec& input, const Vec& output,
-    //                           const Vec& output_gradient,
-    //                           const Vec& output_diag_hessian);
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian);
+    */
 
-    //! this version allows to obtain the input gradient and diag_hessian
-    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-    //! RAISES A PLERROR.
-    // virtual void bbpropUpdate(const Vec& input, const Vec& output,
-    //                           Vec& input_gradient,
-    //                           const Vec& output_gradient,
-    //                           Vec& input_diag_hessian,
-    //                           const Vec& output_diag_hessian);
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, output, input_gradient, output_gradient,
+                         out_hess, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+    */
 
-    //! reset the parameters to the state they would be BEFORE starting
+
+    //! Reset the parameters to the state they would be BEFORE starting
     //! training.  Note that this method is necessarily called from
     //! build().
     virtual void forget();
 
 
-    //! optionally perform some processing after training, or after a
-    //! series of fprop/bpropUpdate calls to prepare the model for truly
-    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
-    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
-    // virtual void finalize();
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
 
-    //! in case bpropUpdate does not do anything, make it known
-    //! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false;
-    // virtual bool bpropDoesNothing();
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
 
     //#####  PLearn::Object Protocol  #########################################
 



From lamblin at mail.berlios.de  Wed Jan 24 03:06:09 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 03:06:09 +0100
Subject: [Plearn-commits] r6598 - trunk/scripts/Skeletons
Message-ID: <200701240206.l0O269Yu024243@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 03:06:03 +0100 (Wed, 24 Jan 2007)
New Revision: 6598

Added:
   trunk/scripts/Skeletons/CostModule.cc
   trunk/scripts/Skeletons/CostModule.h
Log:
New skeleton for CostModule (subclass of OnlineLearningModule)


Added: trunk/scripts/Skeletons/CostModule.cc
===================================================================
--- trunk/scripts/Skeletons/CostModule.cc	2007-01-24 01:02:34 UTC (rev 6597)
+++ trunk/scripts/Skeletons/CostModule.cc	2007-01-24 02:06:03 UTC (rev 6598)
@@ -0,0 +1,169 @@
+
+#include "DERIVEDCLASS.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    DERIVEDCLASS,
+    "ONE LINE DESCRIPTION",
+    "MULTI-LINE \nHELP");
+
+DERIVEDCLASS::DERIVEDCLASS() :
+/* ### Initialize all fields to their default value here */
+{
+    // ...
+
+    // ### You may (or not) want to call build_() to finish building the object
+    // ### (doing so assumes the parent classes' build_() have been called too
+    // ### in the parent classes' constructors, something that you must ensure)
+}
+
+void DERIVEDCLASS::declareOptions(OptionList& ol)
+{
+    // ### Declare all of this object's options here.
+    // ### For the "flags" of each option, you should typically specify
+    // ### one of OptionBase::buildoption, OptionBase::learntoption or
+    // ### OptionBase::tuningoption. If you don't provide one of these three,
+    // ### this option will be ignored when loading values from a script.
+    // ### You can also combine flags, for example with OptionBase::nosave:
+    // ### (OptionBase::buildoption | OptionBase::nosave)
+
+    // ### ex:
+    // declareOption(ol, "myoption", &DERIVEDCLASS::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+    // ...
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void DERIVEDCLASS::build_()
+{
+    // ### This method should do the real building of the object,
+    // ### according to set 'options', in *any* situation.
+    // ### Typical situations include:
+    // ###  - Initial building of an object from a few user-specified options
+    // ###  - Building of a "reloaded" object: i.e. from the complete set of
+    // ###    all serialised options.
+    // ###  - Updating or "re-building" of an object after a few "tuning"
+    // ###    options have been modified.
+    // ### You should assume that the parent class' build_() has already been
+    // ### called.
+}
+
+// ### Nothing to add here, simply calls build_
+void DERIVEDCLASS::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void DERIVEDCLASS::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    // ### Call deepCopyField on all "pointer-like" fields
+    // ### that you wish to be deepCopied rather than
+    // ### shallow-copied.
+    // ### ex:
+    // deepCopyField(trainvec, copies);
+
+    // ### Remove this line when you have fully implemented this method.
+    PLERROR("DERIVEDCLASS::makeDeepCopyFromShallowCopy not fully (correctly) implemented yet!");
+}
+
+///////////
+// fprop //
+///////////
+void DERIVEDCLASS::fprop(const Vec& input, const Vec& target, Vec& cost) const
+{
+}
+
+/* OPTIONAL
+void DERIVEDCLASS::fprop(const Vec& input, const Vec& target, real& cost) const
+{
+}
+*/
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void DERIVEDCLASS::bpropUpdate(const Vec& input, const Vec& target, real cost,
+                               Vec& input_gradient)
+{
+}
+
+/* THIS METHOD IS OPTIONAL
+void DERIVEDCLASS::bpropUpdate(const Vec& input, const Vec& target, real cost)
+{
+}
+*/
+
+/////////////////
+// bpropUpdate //
+/////////////////
+/* THIS METHOD IS OPTIONAL
+void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& target, real cost,
+                                Vec& input_gradient, Vec& input_diag_hessian)
+{
+}
+*/
+
+/* THIS METHOD IS OPTIONAL
+void DERIVEDCLASS::bbpropUpdate(const Vec& input, const Vec& target, real cost)
+{
+}
+*/
+
+///////////
+// reset //
+///////////
+void DERIVEDCLASS::forget()
+{
+}
+
+//////////
+// name //
+//////////
+TVec<string> DERIVEDCLASS::name()
+{
+    // ### Usually, the name of the class without the trailing "CostModule"
+    return TVec<string>(1, "DERIVEDCLASS");
+}
+
+//////////////
+// finalize //
+//////////////
+/* THIS METHOD IS OPTIONAL
+void DERIVEDCLASS::finalize()
+{
+}
+*/
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+/* THIS METHOD IS OPTIONAL
+bool DERIVEDCLASS::bpropDoesNothing()
+{
+}
+*/
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/scripts/Skeletons/CostModule.h
===================================================================
--- trunk/scripts/Skeletons/CostModule.h	2007-01-24 01:02:34 UTC (rev 6597)
+++ trunk/scripts/Skeletons/CostModule.h	2007-01-24 02:06:03 UTC (rev 6598)
@@ -0,0 +1,159 @@
+#ifndef DERIVEDCLASS_INC
+#define DERIVEDCLASS_INC
+
+#include <plearn_learners/online/CostModule.h>
+
+namespace PLearn {
+
+/**
+ * The first sentence should be a BRIEF DESCRIPTION of what the class does.
+ * Place the rest of the class programmer documentation here.  Doxygen supports
+ * Javadoc-style comments.  See http://www.doxygen.org/manual.html
+ *
+ * @todo Write class to-do's here if there are any.
+ *
+ * @deprecated Write deprecated stuff here if there is any.  Indicate what else
+ * should be used instead.
+ */
+class DERIVEDCLASS : public CostModule
+{
+    typedef CostModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    DERIVEDCLASS();
+
+    // Your other public member functions go here
+
+    //! Given the input and the target, compute a vector of costs
+    //! (possibly resize it appropriately)
+    virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
+
+    /* Optional, if you want to optimize it for performance reasons.
+       Default implementation computes the other costs, then ignore them.
+    //! Given the input and the target, compute only the first cost
+    //! (of which we will compute the gradient)
+    virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
+    */
+
+    //! Adapt based on the cost, and compute input gradient to backpropagate.
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
+                             Vec& input_gradient);
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, target, cost, input_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! Adapt based on the the cost.
+    virtual void bpropUpdate(const Vec& input, const Vec& target,
+                             real cost);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       RAISES A PLERROR.
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this back.
+    //! If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
+                              Vec& input_gradient, Vec& input_diag_hessian);
+    */
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, target, cost, input_gradient, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& target,
+                              real cost);
+    */
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT
+       DO ANYTHING.
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+    */
+
+    /* Optional
+       THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+    */
+
+    //! Indicates the name of the computed costs
+    virtual TVec<string> name();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(DERIVEDCLASS);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(DERIVEDCLASS);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Jan 24 03:42:28 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 03:42:28 +0100
Subject: [Plearn-commits] r6599 - trunk/scripts/Skeletons
Message-ID: <200701240242.l0O2gSF0025841@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 03:42:27 +0100 (Wed, 24 Jan 2007)
New Revision: 6599

Modified:
   trunk/scripts/Skeletons/SequentialLearner.h
Log:


Modified: trunk/scripts/Skeletons/SequentialLearner.h
===================================================================
--- trunk/scripts/Skeletons/SequentialLearner.h	2007-01-24 02:06:03 UTC (rev 6598)
+++ trunk/scripts/Skeletons/SequentialLearner.h	2007-01-24 02:42:27 UTC (rev 6599)
@@ -51,9 +51,12 @@
     virtual TVec<std::string> getTrainCostNames() const;
     virtual TVec<std::string> getTestCostNames() const;
 
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT_METHODS
+    PLEARN_DECLARE_OBJECT(DERIVEDCLASS);
+
     //!  Does the necessary operations to transform a shallow copy (this)
     //!  into a deep copy by deep-copying all the members that need to be.
-    PLEARN_DECLARE_OBJECT(DERIVEDCLASS);
     virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
 };
 



From lamblin at mail.berlios.de  Wed Jan 24 03:52:56 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 03:52:56 +0100
Subject: [Plearn-commits] r6600 - trunk/scripts/Skeletons
Message-ID: <200701240252.l0O2quIp026192@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 03:52:54 +0100 (Wed, 24 Jan 2007)
New Revision: 6600

Modified:
   trunk/scripts/Skeletons/CostModule.cc
   trunk/scripts/Skeletons/CostModule.h
   trunk/scripts/Skeletons/OnlineLearningModule.cc
   trunk/scripts/Skeletons/OnlineLearningModule.h
Log:
Add "setLearningRate" function to skeleton, since it was added in the "real"
class


Modified: trunk/scripts/Skeletons/CostModule.cc
===================================================================
--- trunk/scripts/Skeletons/CostModule.cc	2007-01-24 02:42:27 UTC (rev 6599)
+++ trunk/scripts/Skeletons/CostModule.cc	2007-01-24 02:52:54 UTC (rev 6600)
@@ -152,7 +152,16 @@
 }
 */
 
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+void DERIVEDCLASS::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
 
+
 } // end of namespace PLearn
 
 

Modified: trunk/scripts/Skeletons/CostModule.h
===================================================================
--- trunk/scripts/Skeletons/CostModule.h	2007-01-24 02:42:27 UTC (rev 6599)
+++ trunk/scripts/Skeletons/CostModule.h	2007-01-24 02:52:54 UTC (rev 6600)
@@ -102,6 +102,12 @@
     virtual bool bpropDoesNothing();
     */
 
+    /* Optional
+       Default implementation does nothing
+    //! If this class has a learning rate (or something close to it), set it
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
     //! Indicates the name of the computed costs
     virtual TVec<string> name();
 

Modified: trunk/scripts/Skeletons/OnlineLearningModule.cc
===================================================================
--- trunk/scripts/Skeletons/OnlineLearningModule.cc	2007-01-24 02:42:27 UTC (rev 6599)
+++ trunk/scripts/Skeletons/OnlineLearningModule.cc	2007-01-24 02:52:54 UTC (rev 6600)
@@ -146,6 +146,15 @@
 }
 */
 
+/////////////////////
+// setLearningRate //
+/////////////////////
+/* OPTIONAL
+void DERIVEDCLASS::setLearningRate(real dynamic_learning_rate)
+{
+}
+*/
+
 } // end of namespace PLearn
 
 

Modified: trunk/scripts/Skeletons/OnlineLearningModule.h
===================================================================
--- trunk/scripts/Skeletons/OnlineLearningModule.h	2007-01-24 02:42:27 UTC (rev 6599)
+++ trunk/scripts/Skeletons/OnlineLearningModule.h	2007-01-24 02:52:54 UTC (rev 6600)
@@ -110,6 +110,12 @@
     virtual bool bpropDoesNothing();
     */
 
+    /* Optional
+       Default implementation does nothing
+    //! If this class has a learning rate (or something close to it), set it
+    virtual void setLearningRate(real dynamic_learning_rate);
+    */
+
     //#####  PLearn::Object Protocol  #########################################
 
     // Declares other standard object methods.



From lamblin at mail.berlios.de  Wed Jan 24 05:01:00 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 05:01:00 +0100
Subject: [Plearn-commits] r6601 - in trunk: commands plearn_learners/online
	scripts/Skeletons
Message-ID: <200701240401.l0O410tP031179@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 05:00:59 +0100 (Wed, 24 Jan 2007)
New Revision: 6601

Added:
   trunk/plearn_learners/online/ProcessInputCostModule.cc
   trunk/plearn_learners/online/ProcessInputCostModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
   trunk/scripts/Skeletons/CostModule.cc
Log:
Add new ProcessInputCostModule class


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-01-24 02:52:54 UTC (rev 6600)
+++ trunk/commands/plearn_noblas_inc.h	2007-01-24 04:00:59 UTC (rev 6601)
@@ -190,9 +190,11 @@
 #include <plearn_learners/online/CostModule.h>
 #include <plearn_learners/online/DeepBeliefNet.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn_learners/online/ModulesLearner.h>
 #include <plearn_learners/online/NLLCostModule.h>
 #include <plearn_learners/online/NLLErrModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
+#include <plearn_learners/online/ProcessInputCostModule.h>
 #include <plearn_learners/online/RBMBinomialLayer.h>
 #include <plearn_learners/online/RBMClassificationModule.h>
 #include <plearn_learners/online/RBMConnection.h>
@@ -207,7 +209,6 @@
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SquaredErrModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
-#include <plearn_learners/online/ModulesLearner.h>
 #include <plearn_learners/online/StackedModulesModule.h>
 #include <plearn_learners/online/Subsampling2DModule.h>
 #include <plearn_learners/online/Supersampling2DModule.h>

Added: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-01-24 02:52:54 UTC (rev 6600)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-01-24 04:00:59 UTC (rev 6601)
@@ -0,0 +1,239 @@
+// -*- C++ -*-
+
+// ProcessInputCostModule.cc
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ProcessInputCostModule.cc */
+
+
+
+#include "ProcessInputCostModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ProcessInputCostModule,
+    "Processes the input through an embedded OnlineLearningModule",
+    "This Module embeds an OnlineLearningModule, processing_module, and a\n"
+    "CostModule, cost_module. The input goes through processing_module,\n"
+    "the output of which is used as input by the CostModule.\n"
+    "If you want the input to go through several processing steps, you can\n"
+    "use a ModuleStackModule as processing_module.\n"
+    );
+
+ProcessInputCostModule::ProcessInputCostModule()
+{
+}
+
+void ProcessInputCostModule::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "myoption", &ProcessInputCostModule::myoption,
+    //               OptionBase::buildoption,
+    //               "Help text describing this option");
+
+    declareOption(ol, "processing_module",
+                  &ProcessInputCostModule::processing_module,
+                  OptionBase::buildoption,
+                  "Module that processes the input");
+
+    declareOption(ol, "cost_module",
+                  &ProcessInputCostModule::cost_module,
+                  OptionBase::buildoption,
+                  "Module that outputs the cost");
+
+    declareOption(ol, "processed_size",
+                  &ProcessInputCostModule::processed_size,
+                  OptionBase::learntoption,
+                  "Size of processing_module's output");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void ProcessInputCostModule::build_()
+{
+    if( processing_module )
+    {
+        input_size = processing_module->input_size;
+        processed_size = processing_module->output_size;
+    }
+
+    if( cost_module )
+    {
+        output_size = cost_module->output_size;
+        target_size = cost_module->target_size;
+    }
+
+    if( processing_module && cost_module )
+        PLASSERT( processed_size == cost_module->input_size );
+}
+
+void ProcessInputCostModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void ProcessInputCostModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(processing_module, copies);
+    deepCopyField(cost_module, copies);
+}
+
+///////////
+// fprop //
+///////////
+void ProcessInputCostModule::fprop(const Vec& input, const Vec& target,
+                                   Vec& cost) const
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+
+    processing_module->fprop( input, processed_value );
+    cost_module->fprop( processed_value, target, cost );
+}
+
+void ProcessInputCostModule::fprop(const Vec& input, const Vec& target,
+                                   real& cost) const
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+
+    processing_module->fprop( input, processed_value );
+    cost_module->fprop( processed_value, target, cost );
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void ProcessInputCostModule::bpropUpdate(const Vec& input, const Vec& target,
+                                         real cost, Vec& input_gradient)
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+
+    cost_module->bpropUpdate( processed_value, target, cost,
+                              processed_gradient );
+    processing_module->bpropUpdate( input, processed_value,
+                                    input_gradient, processed_gradient );
+}
+
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void ProcessInputCostModule::bbpropUpdate(const Vec& input, const Vec& target,
+                                          real cost, Vec& input_gradient,
+                                          Vec& input_diag_hessian)
+{
+    PLASSERT( processing_module );
+    PLASSERT( cost_module );
+
+    cost_module->bbpropUpdate( processed_value, target, cost,
+                               processed_gradient, processed_diag_hessian );
+    processing_module->bbpropUpdate( input, processed_value,
+                                     input_gradient, processed_gradient,
+                                     input_diag_hessian,
+                                     processed_diag_hessian );
+}
+
+
+////////////
+// forget //
+////////////
+void ProcessInputCostModule::forget()
+{
+    processing_module->forget();
+    cost_module->forget();
+
+    processed_value.clear();
+    processed_gradient.clear();
+    processed_diag_hessian.clear();
+}
+
+//////////
+// name //
+//////////
+TVec<string> ProcessInputCostModule::name()
+{
+    // ### Usually, the name of the class without the trailing "CostModule"
+    return cost_module->name();
+}
+
+//////////////
+// finalize //
+//////////////
+void ProcessInputCostModule::finalize()
+{
+    processing_module->finalize();
+    cost_module->finalize();
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+bool ProcessInputCostModule::bpropDoesNothing()
+{
+    return processing_module->bpropDoesNothing()
+        && cost_module->bpropDoesNothing();
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+void ProcessInputCostModule::setLearningRate(real dynamic_learning_rate)
+{
+    processing_module->setLearningRate( dynamic_learning_rate );
+    cost_module->setLearningRate( dynamic_learning_rate );
+}
+
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ProcessInputCostModule.h
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.h	2007-01-24 02:52:54 UTC (rev 6600)
+++ trunk/plearn_learners/online/ProcessInputCostModule.h	2007-01-24 04:00:59 UTC (rev 6601)
@@ -0,0 +1,191 @@
+// -*- C++ -*-
+
+// ProcessInputCostModule.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ProcessInputCostModule.h */
+
+
+#ifndef ProcessInputCostModule_INC
+#define ProcessInputCostModule_INC
+
+#include <plearn_learners/online/CostModule.h>
+
+namespace PLearn {
+
+/**
+ * Processes the input through an embedded OnlineLearningModule.
+ * This Module embeds an OnlineLearningModule, processing_module, and a
+ * CostModule, cost_module. The input goes through processing_module,
+ * the output of which is used as input by the CostModule.
+ * If you want the input to go through several processing steps, you can use a
+ * ModuleStackModule as processing_module.
+ *
+ * @todo: code ModuleStackModule
+ */
+class ProcessInputCostModule : public CostModule
+{
+    typedef CostModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! Module that processes the input
+    PP<OnlineLearningModule> processing_module;
+
+    //! CostModule that outputs this cost
+    PP<CostModule> cost_module;
+
+    //! Size of processing_module's output
+    int processed_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ProcessInputCostModule();
+
+    //! Given the input and the target, compute a vector of costs
+    //! (possibly resize it appropriately)
+    virtual void fprop(const Vec& input, const Vec& target, Vec& cost) const;
+
+    //! Given the input and the target, compute only the first cost
+    //! (of which we will compute the gradient)
+    virtual void fprop(const Vec& input, const Vec& target, real& cost) const;
+
+    //! Adapt based on the cost, and compute input gradient to backpropagate.
+    virtual void bpropUpdate(const Vec& input, const Vec& target, real cost,
+                             Vec& input_gradient);
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+       JUST CALLS
+            bpropUpdate(input, target, cost, input_gradient)
+       AND IGNORES INPUT GRADIENT.
+    //! Adapt based on the the cost.
+    virtual void bpropUpdate(const Vec& input, const Vec& target,
+                             real cost);
+    */
+
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this back.
+    //! If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& target, real cost,
+                              Vec& input_gradient, Vec& input_diag_hessian);
+
+    /* Optional
+       N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+       WHICH JUST CALLS
+            bbpropUpdate(input, target, cost, input_gradient, in_hess)
+       AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& target,
+                              real cost);
+    */
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+
+    //! If this class has a learning rate (or something close to it), set it
+    virtual void setLearningRate(real dynamic_learning_rate);
+
+    //! Indicates the name of the computed costs
+    virtual TVec<string> name();
+
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(ProcessInputCostModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+    //#####  Not Options  #####################################################
+    mutable Vec processed_value;
+    mutable Vec processed_gradient;
+    mutable Vec processed_diag_hessian;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ProcessInputCostModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/scripts/Skeletons/CostModule.cc
===================================================================
--- trunk/scripts/Skeletons/CostModule.cc	2007-01-24 02:52:54 UTC (rev 6600)
+++ trunk/scripts/Skeletons/CostModule.cc	2007-01-24 04:00:59 UTC (rev 6601)
@@ -118,9 +118,9 @@
 }
 */
 
-///////////
-// reset //
-///////////
+////////////
+// forget //
+////////////
 void DERIVEDCLASS::forget()
 {
 }



From lamblin at mail.berlios.de  Wed Jan 24 05:03:31 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 05:03:31 +0100
Subject: [Plearn-commits] r6602 - in trunk/python_modules/plearn: io
	math/stats report
Message-ID: <200701240403.l0O43V1f031319@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 05:03:31 +0100 (Wed, 24 Jan 2007)
New Revision: 6602

Modified:
   trunk/python_modules/plearn/io/
   trunk/python_modules/plearn/math/stats/
   trunk/python_modules/plearn/report/
Log:
Ignore *.pyc



Property changes on: trunk/python_modules/plearn/io
___________________________________________________________________
Name: svn:ignore
   + *.pyc



Property changes on: trunk/python_modules/plearn/math/stats
___________________________________________________________________
Name: svn:ignore
   + *.pyc



Property changes on: trunk/python_modules/plearn/report
___________________________________________________________________
Name: svn:ignore
   + *.pyc




From lamblin at mail.berlios.de  Wed Jan 24 05:43:21 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 05:43:21 +0100
Subject: [Plearn-commits] r6603 - trunk/plearn_learners/online
Message-ID: <200701240443.l0O4hLv2032701@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 05:43:21 +0100 (Wed, 24 Jan 2007)
New Revision: 6603

Modified:
   trunk/plearn_learners/online/ProcessInputCostModule.cc
Log:
Some more PLASSERT's


Modified: trunk/plearn_learners/online/ProcessInputCostModule.cc
===================================================================
--- trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-01-24 04:03:31 UTC (rev 6602)
+++ trunk/plearn_learners/online/ProcessInputCostModule.cc	2007-01-24 04:43:21 UTC (rev 6603)
@@ -53,7 +53,8 @@
     "use a ModuleStackModule as processing_module.\n"
     );
 
-ProcessInputCostModule::ProcessInputCostModule()
+ProcessInputCostModule::ProcessInputCostModule() :
+    processed_size( -1 )
 {
 }
 
@@ -123,6 +124,8 @@
 {
     PLASSERT( processing_module );
     PLASSERT( cost_module );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
 
     processing_module->fprop( input, processed_value );
     cost_module->fprop( processed_value, target, cost );
@@ -133,6 +136,8 @@
 {
     PLASSERT( processing_module );
     PLASSERT( cost_module );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
 
     processing_module->fprop( input, processed_value );
     cost_module->fprop( processed_value, target, cost );
@@ -146,6 +151,8 @@
 {
     PLASSERT( processing_module );
     PLASSERT( cost_module );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
 
     cost_module->bpropUpdate( processed_value, target, cost,
                               processed_gradient );
@@ -163,6 +170,8 @@
 {
     PLASSERT( processing_module );
     PLASSERT( cost_module );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( target.size() == target_size );
 
     cost_module->bbpropUpdate( processed_value, target, cost,
                                processed_gradient, processed_diag_hessian );



From lamblin at mail.berlios.de  Wed Jan 24 06:13:13 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 06:13:13 +0100
Subject: [Plearn-commits] r6604 - in trunk: commands plearn_learners/online
Message-ID: <200701240513.l0O5DDOF000458@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 06:13:12 +0100 (Wed, 24 Jan 2007)
New Revision: 6604

Added:
   trunk/plearn_learners/online/ModuleStackModule.cc
   trunk/plearn_learners/online/ModuleStackModule.h
Modified:
   trunk/commands/plearn_noblas_inc.h
Log:
Add new ModuleStackModule class, that wraps layered Modules in a single one.


Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-01-24 04:43:21 UTC (rev 6603)
+++ trunk/commands/plearn_noblas_inc.h	2007-01-24 05:13:12 UTC (rev 6604)
@@ -191,6 +191,7 @@
 #include <plearn_learners/online/DeepBeliefNet.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
 #include <plearn_learners/online/ModulesLearner.h>
+#include <plearn_learners/online/ModuleStackModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
 #include <plearn_learners/online/NLLErrModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>

Added: trunk/plearn_learners/online/ModuleStackModule.cc
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.cc	2007-01-24 04:43:21 UTC (rev 6603)
+++ trunk/plearn_learners/online/ModuleStackModule.cc	2007-01-24 05:13:12 UTC (rev 6604)
@@ -0,0 +1,281 @@
+// -*- C++ -*-
+
+// ModuleStackModule.cc
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ModuleStackModule.cc */
+
+
+
+#include "ModuleStackModule.h"
+
+namespace PLearn {
+using namespace std;
+
+PLEARN_IMPLEMENT_OBJECT(
+    ModuleStackModule,
+    "Wraps a stack of layered OnlineLearningModule into a single one",
+    "The OnlineLearningModule's are disposed like superposed layers:\n"
+    "outputs of module i are the inputs of module (i+1), the last layer is\n"
+    "the output layer.\n"
+    );
+
+ModuleStackModule::ModuleStackModule() :
+    n_modules(0)
+{
+}
+
+void ModuleStackModule::declareOptions(OptionList& ol)
+{
+    // declareOption(ol, "", &ModuleStackModule::,
+    //               OptionBase::buildoption,
+    //               "");
+
+    declareOption(ol, "modules", &ModuleStackModule::modules,
+                  OptionBase::buildoption,
+                  "The underlying modules");
+
+    declareOption(ol, "n_modules", &ModuleStackModule::n_modules,
+                  OptionBase::learntoption,
+                  "The number of modules");
+
+    // Now call the parent class' declareOptions
+    inherited::declareOptions(ol);
+}
+
+void ModuleStackModule::build_()
+{
+    // TODO: Do something with the random generator?
+
+    n_modules = modules.length();
+
+    if( n_modules > 0 )
+    {
+        values.resize( n_modules-1 );
+        gradients.resize( n_modules-1 );
+        diag_hessians.resize( n_modules-1 );
+
+        input_size = modules[0]->input_size;
+        output_size = modules[n_modules-1]->output_size;
+    }
+}
+
+// ### Nothing to add here, simply calls build_
+void ModuleStackModule::build()
+{
+    inherited::build();
+    build_();
+}
+
+
+void ModuleStackModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
+{
+    inherited::makeDeepCopyFromShallowCopy(copies);
+
+    deepCopyField(modules, copies);
+}
+
+///////////
+// fprop //
+///////////
+void ModuleStackModule::fprop(const Vec& input, Vec& output) const
+{
+    PLASSERT( n_modules > 0 );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( output.size() == output_size );
+
+    modules[0]->fprop( input, values[0] );
+    for( int i=1 ; i<n_modules-1 ; i++ )
+        modules[i]->fprop( values[i-1], values[i] );
+    modules[n_modules-1]->fprop( values[n_modules-2], output );
+}
+
+/////////////////
+// bpropUpdate //
+/////////////////
+void ModuleStackModule::bpropUpdate(const Vec& input, const Vec& output,
+                                    Vec& input_gradient,
+                                    const Vec& output_gradient)
+{
+    PLASSERT( n_modules > 0 );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( output.size() == output_size );
+    PLASSERT( output_gradient.size() == output_size );
+
+    // bpropUpdate should be called just after the corresponding fprop,
+    // so values should be up-to-date.
+    modules[n_modules-1]->bpropUpdate( values[n_modules-2], output,
+                                       gradients[n_modules-2], output_gradient );
+
+    for( int i=n_modules-2 ; i>0 ; i-- )
+        modules[i]->bpropUpdate( values[i-1], values[i],
+                                 gradients[i-1], gradients[i] );
+
+    modules[0]->bpropUpdate( input, values[0], input_gradient, gradients[0] );
+}
+
+void ModuleStackModule::bpropUpdate(const Vec& input, const Vec& output,
+                                    const Vec& output_gradient)
+{
+    PLASSERT( n_modules > 0 );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( output.size() == output_size );
+    PLASSERT( output_gradient.size() == output_size );
+
+    // bpropUpdate should be called just after the corresponding fprop,
+    // so values should be up-to-date.
+    modules[n_modules-1]->bpropUpdate( values[n_modules-2], output,
+                                       gradients[n_modules-2],
+                                       output_gradient );
+
+    for( int i=n_modules-2 ; i>0 ; i-- )
+        modules[i]->bpropUpdate( values[i-1], values[i],
+                                 gradients[i-1], gradients[i] );
+
+    modules[0]->bpropUpdate( input, values[0], gradients[0] );
+}
+
+//////////////////
+// bbpropUpdate //
+//////////////////
+void ModuleStackModule::bbpropUpdate(const Vec& input, const Vec& output,
+                                     Vec& input_gradient,
+                                     const Vec& output_gradient,
+                                     Vec& input_diag_hessian,
+                                     const Vec& output_diag_hessian)
+{
+    PLASSERT( n_modules > 0 );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( output.size() == output_size );
+    PLASSERT( output_gradient.size() == output_size );
+    PLASSERT( output_diag_hessian.size() == output_size );
+
+    // bbpropUpdate should be called just after the corresponding fprop,
+    // so values should be up-to-date.
+    modules[n_modules-1]->bbpropUpdate( values[n_modules-2], output,
+                                        gradients[n_modules-2], output_gradient,
+                                        diag_hessians[n_modules-2],
+                                        output_diag_hessian );
+
+    for( int i=n_modules-2 ; i>0 ; i-- )
+        modules[i]->bbpropUpdate( values[i-1], values[i],
+                                  gradients[i-1], gradients[i],
+                                  diag_hessians[i-1], diag_hessians[i] );
+
+    modules[0]->bbpropUpdate( input, values[0], input_gradient, gradients[0],
+                              input_diag_hessian, diag_hessians[0] );
+}
+
+void ModuleStackModule::bbpropUpdate(const Vec& input, const Vec& output,
+                                     const Vec& output_gradient,
+                                     const Vec& output_diag_hessian)
+{
+    PLASSERT( n_modules > 0 );
+    PLASSERT( input.size() == input_size );
+    PLASSERT( output.size() == output_size );
+    PLASSERT( output_gradient.size() == output_size );
+    PLASSERT( output_diag_hessian.size() == output_size );
+
+    // bbpropUpdate should be called just after the corresponding fprop,
+    // so values should be up-to-date.
+    modules[n_modules-1]->bbpropUpdate( values[n_modules-2], output,
+                                        gradients[n_modules-2], output_gradient,
+                                        diag_hessians[n_modules-2],
+                                        output_diag_hessian );
+
+    for( int i=n_modules-2 ; i>0 ; i-- )
+        modules[i]->bbpropUpdate( values[i-1], values[i],
+                                  gradients[i-1], gradients[i],
+                                  diag_hessians[i-1], diag_hessians[i] );
+
+    modules[0]->bbpropUpdate( input, values[0],
+                              gradients[0], diag_hessians[0] );
+}
+
+////////////
+// forget //
+////////////
+void ModuleStackModule::forget()
+{
+    for( int i=0 ; i<n_modules ; i++ )
+        modules[i]->forget();
+
+    values.clear();
+    gradients.clear();
+    diag_hessians.clear();
+}
+
+//////////////
+// finalize //
+//////////////
+void ModuleStackModule::finalize()
+{
+    for( int i=0 ; i<n_modules ; i++ )
+        modules[i]->finalize();
+}
+
+//////////////////////
+// bpropDoesNothing //
+//////////////////////
+bool ModuleStackModule::bpropDoesNothing()
+{
+    for( int i=0 ; i<n_modules ; i++ )
+        if( !(modules[i]->bpropDoesNothing()) )
+            return false;
+    return true;
+}
+
+/////////////////////
+// setLearningRate //
+/////////////////////
+void ModuleStackModule::setLearningRate(real dynamic_learning_rate)
+{
+    for( int i=0 ; i<n_modules ; i++ )
+        modules[i]->setLearningRate( dynamic_learning_rate );
+}
+
+} // end of namespace PLearn
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Added: trunk/plearn_learners/online/ModuleStackModule.h
===================================================================
--- trunk/plearn_learners/online/ModuleStackModule.h	2007-01-24 04:43:21 UTC (rev 6603)
+++ trunk/plearn_learners/online/ModuleStackModule.h	2007-01-24 05:13:12 UTC (rev 6604)
@@ -0,0 +1,185 @@
+// -*- C++ -*-
+
+// ModuleStackModule.h
+//
+// Copyright (C) 2007 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file ModuleStackModule.h */
+
+
+#ifndef ModuleStackModule_INC
+#define ModuleStackModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+namespace PLearn {
+
+/**
+ * Wraps a stack of layered OnlineLearningModule into a single one.
+ *
+ * The OnlineLearningModule's are disposed like superposed layers:
+ * outputs of module i are the inputs of module (i+1), the last layer is
+ * the output layer.
+ *
+ */
+class ModuleStackModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! The underlying modules
+    TVec< PP<OnlineLearningModule> > modules;
+
+    //! The number of modules
+    int n_modules;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    ModuleStackModule();
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Adapt based on the output gradient, and obtain the input gradient.
+    //! This method should only be called just after a corresponding
+    //! fprop; it should be called with the same arguments as fprop
+    //! for the first two arguments (and output should not have been
+    //! modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient);
+
+    //! This version does not obtain the input gradient.
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian);
+
+    //! This version does not obtain the input gradient and diag_hessian.
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+
+
+    //! Reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    //! Perform some processing after training, or after a series of
+    //! fprop/bpropUpdate calls to prepare the model for truly out-of-sample
+    //! operation.
+    virtual void finalize();
+
+    //! In case bpropUpdate does not do anything, make it known
+    virtual bool bpropDoesNothing();
+
+    //! If this class has a learning rate (or something close to it), set it
+    virtual void setLearningRate(real dynamic_learning_rate);
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(ModuleStackModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+    //! values[i] represents the value of the output of module i and the input
+    //! of module i+1. No need for values[n_modules-1] because it's the output.
+    //! gradients[i] and diag_hessians[i] works just the same, and there is no
+    //! need for gradients[-1] because it is input_gradient.
+    mutable TVec<Vec> values;
+    mutable TVec<Vec> gradients;
+    mutable TVec<Vec> diag_hessians;
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(ModuleStackModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Jan 24 06:22:36 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 06:22:36 +0100
Subject: [Plearn-commits] r6605 - in trunk: commands plearn_learners/online
	plearn_learners/online/DEPRECATED
Message-ID: <200701240522.l0O5Maxb000693@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 06:22:36 +0100 (Wed, 24 Jan 2007)
New Revision: 6605

Added:
   trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.cc
   trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h
Removed:
   trunk/plearn_learners/online/SquaredErrModule.cc
   trunk/plearn_learners/online/SquaredErrModule.h
Modified:
   trunk/commands/plearn_light_inc.h
   trunk/commands/plearn_noblas_inc.h
Log:
Deprecate SquaredErrModule. Please use SquaredErrorCostModule instead.


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-01-24 05:13:12 UTC (rev 6604)
+++ trunk/commands/plearn_light_inc.h	2007-01-24 05:22:36 UTC (rev 6605)
@@ -216,7 +216,6 @@
 #include <plearn_learners/online/RBMMultinomialLayer.h>
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/SoftmaxModule.h>
-#include <plearn_learners/online/SquaredErrModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
 #include <plearn_learners/online/ModulesLearner.h>
 #include <plearn_learners/online/StackedModulesModule.h>

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-01-24 05:13:12 UTC (rev 6604)
+++ trunk/commands/plearn_noblas_inc.h	2007-01-24 05:22:36 UTC (rev 6605)
@@ -208,7 +208,6 @@
 #include <plearn_learners/online/RBMMultinomialLayer.h>
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/SoftmaxModule.h>
-#include <plearn_learners/online/SquaredErrModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
 #include <plearn_learners/online/StackedModulesModule.h>
 #include <plearn_learners/online/Subsampling2DModule.h>

Copied: trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.cc (from rev 6596, trunk/plearn_learners/online/SquaredErrModule.cc)

Copied: trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h (from rev 6596, trunk/plearn_learners/online/SquaredErrModule.h)
===================================================================
--- trunk/plearn_learners/online/SquaredErrModule.h	2007-01-23 20:43:54 UTC (rev 6596)
+++ trunk/plearn_learners/online/DEPRECATED/SquaredErrModule.h	2007-01-24 05:22:36 UTC (rev 6605)
@@ -0,0 +1,150 @@
+// -*- C++ -*-
+
+// SquaredErrModule.h
+//
+// Copyright (C) 2005 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************
+   * $Id: SquaredErrModule.h,v 1.1 2005/11/30 04:36:17 lamblinp Exp $
+   ******************************************************* */
+
+// Authors: Pascal Lamblin
+
+/*! \file SquaredErrModule.h */
+
+
+#ifndef SquaredErrModule_INC
+#define SquaredErrModule_INC
+
+#include <plearn/base/Object.h>
+#include <plearn/math/TMat_maths.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+namespace PLearn {
+
+/**
+ * Squared difference (and derivatives thereof) between the target and input.
+ *
+ * @deprecated: Use ../SquaredErrorCostModule instead
+ */
+class SquaredErrModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    SquaredErrModule();
+
+    // Your other public member functions go here
+/*
+    virtual void setTarget(const Vec the_target);
+    virtual void setTarget(real the_target);
+    virtual Vec getTarget() const;
+*/
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient, const Vec& output_gradient);
+
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian);
+
+
+    virtual void forget();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(SquaredErrModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+//    Vec target;
+    int target_size;
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(SquaredErrModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/SquaredErrModule.cc
===================================================================
--- trunk/plearn_learners/online/SquaredErrModule.cc	2007-01-24 05:13:12 UTC (rev 6604)
+++ trunk/plearn_learners/online/SquaredErrModule.cc	2007-01-24 05:22:36 UTC (rev 6605)
@@ -1,306 +0,0 @@
-// -*- C++ -*-
-
-// SquaredErrModule.cc
-//
-// Copyright (C) 2005 Pascal Lamblin
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************
-   * $Id: SquaredErrModule.cc,v 1.2 2005/12/30 19:53:56 lamblinp Exp $
-   ******************************************************* */
-
-// Authors: Pascal Lamblin
-
-/*! \file SquaredErrModule.cc */
-
-
-#include "SquaredErrModule.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    SquaredErrModule,
-    "SquaredError Module",
-    ""
-    );
-
-SquaredErrModule::SquaredErrModule() :
-    /* ### Initialize all fields to their default value */
-    target_size( 0 )
-{
-    output_size = 1;
-}
-
-
-// output = error = sum of squared difference between input and target
-void SquaredErrModule::fprop(const Vec& input, Vec& output) const
-{
-    int in_size = input.size();
-    // size check
-    if( in_size != input_size+target_size )
-    {
-        PLERROR("SquaredErrModule::fprop: 'input.size()' should be equal\n"
-                " to 'input_size' + 'target_size' (%i != %i + %i)\n",
-                in_size, input_size, target_size);
-    }
-
-    Vec target = input.subVec( input_size, target_size );
-    Vec input_ = input.subVec( 0, input_size );
-    output.resize( output_size );
-    output[0] = sumsquare( input_ - target );
-}
-
-// Don't modify class
-void SquaredErrModule::bpropUpdate(const Vec& input, const Vec& output,
-                                   const Vec& output_gradient)
-{
-    int in_size = input.size();
-    int out_size = output.size();
-    int og_size = output_gradient.size();
-
-    // size check
-    if( in_size != input_size + target_size )
-    {
-        PLWARNING("SquaredErrModule::bpropUpdate: 'input.size()' should be\n"
-                  " equal to 'input_size' + 'target_size' (%i != %i + %i)\n",
-                  in_size, input_size, target_size);
-    }
-    if( out_size != output_size )
-    {
-        PLWARNING("SquaredErrModule::bpropUpdate: output.size()' should be\n"
-                  " equal to 'output_size' (%i != %i)\n",
-                  out_size, output_size);
-    }
-    if( og_size != output_size )
-    {
-        PLWARNING("SquaredErrModule::bpropUpdate: 'output_gradient.size()'\n"
-                  " should be equal to 'output_size' (%i != %i)\n",
-                  og_size, output_size);
-    }
-}
-
-// We don't care about output_gradient, we consider we're the last variable.
-// So we compute the gradient of the error of this variable.
-void SquaredErrModule::bpropUpdate(const Vec& input, const Vec& output,
-                                   Vec& input_gradient,
-                                   const Vec& output_gradient)
-{
-    int in_size = input.size();
-    int out_size = output.size();
-    int og_size = output_gradient.size();
-    bool is_final_cost = false; // if yes, output_gradient is 1
-
-    // size check
-    if( in_size != input_size + target_size )
-    {
-        PLERROR("SquaredErrModule::bpropUpdate: 'input.size()' should be\n"
-                " equal to 'input_size' + 'target_size' (%i != %i + %i)\n",
-                in_size, input_size, target_size);
-    }
-    if( out_size != output_size )
-    {
-        PLERROR("SquaredErrModule::bpropUpdate: output.size()' should be\n"
-                " equal to 'output_size' (%i != %i)\n",
-                out_size, output_size);
-    }
-    if( og_size == 0 )
-    {
-        /*
-        PLWARNING("SquaredErrModule::bpropUpdate: you are not providing"
-                  "output_gradient.\n"
-                  "Assuming this is the final cost, and output_gradient=1.\n");
-         */
-        is_final_cost = true;
-    }
-    else if( og_size != output_size )
-    {
-        PLERROR("SquaredErrModule::bpropUpdate: 'output_gradient.size()'\n"
-                " should be equal to 'output_size' (%i != %i)\n",
-                og_size, output_size);
-    }
-
-    Vec input_ = input.subVec( 0, input_size );
-    Vec target = input.subVec( input_size, target_size );
-    input_gradient.resize( input_size );
-    for( int i=0 ; i<input_size ; i++ )
-    {
-        if( is_final_cost )
-            input_gradient[i] = 2*( input_[i] - target[i] );
-        else
-            input_gradient[i] = 2*( input_[i] - target[i] )*output_gradient[0];
-    }
-}
-
-// Does nothing (just checks and warns)
-void SquaredErrModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                    const Vec& output_gradient,
-                                    const Vec& output_diag_hessian)
-{
-    int odh_size = output_diag_hessian.size();
-    if( odh_size != output_size )
-    {
-        PLWARNING("SquaredErrModule::bbpropUpdate:"
-                  " 'output_diag_hessian.size()'\n"
-                  " should be equal to 'output_size' (%i != %i)\n",
-                  odh_size, output_size);
-    }
-
-    bpropUpdate( input, output, output_gradient );
-}
-
-// Propagates back output_gradient and output_diag_hessian
-void SquaredErrModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                    Vec& input_gradient,
-                                    const Vec& output_gradient,
-                                    Vec& input_diag_hessian,
-                                    const Vec& output_diag_hessian)
-{
-    int odh_size = output_diag_hessian.size();
-    bool is_final_cost = false; // if yes, output_diag_hessian is 0
-
-    // size check
-    // others size checks will be done in bpropUpdate()
-    if( odh_size == 0 )
-    {
-        PLWARNING("SquaredErrModule::bbpropUpdate: you are not providing"
-                  " output_diag_hessian.\n"
-                  "Assuming this is the final cost,"
-                  " and output_diag_hessian=0.\n");
-        is_final_cost = true;
-    }
-    else if( odh_size != output_size )
-    {
-        PLERROR("SquaredErrModule::bbpropUpdate:"
-                " 'output_diag_hessian.size()'\n"
-                " should be equal to 'output_size' (%i != %i)\n",
-                odh_size, output_size);
-    }
-
-    bpropUpdate( input, output, input_gradient, output_gradient );
-
-    Vec input_ = input.subVec( 0, input_size );
-    Vec target = input.subVec( input_size, target_size );
-    input_diag_hessian.resize( input_size );
-
-    // computation of term dC/dy d?y/dx?,
-    // skipped if estimate_simpler_diag_hessian, unless it is final cost
-    if( estimate_simpler_diag_hessian && !is_final_cost )
-    {
-        input_diag_hessian.clear();
-    }
-    else
-    {
-        Vec idh( input_size, 2 );
-        input_diag_hessian << idh;
-
-        if( !is_final_cost )
-            input_diag_hessian *= output_gradient[0];
-    }
-
-    // computation of term d?C/dy? (dy/dx)?,
-    // skipped if it is final cost, because then d?C/dy? == d?C/dC? == 0
-    if( !is_final_cost )
-    {
-        for( int i=0 ; i<input_size ; i++ )
-        {
-            real fprime_i = 2*(input_[i] - target[i]);
-            input_diag_hessian[i] += (fprime_i*fprime_i)
-                                       * output_diag_hessian[0];
-        }
-    }
-
-}
-
-
-
-
-//
-void SquaredErrModule::forget()
-{
-//    target = Vec( input_size );
-}
-
-
-// ### Nothing to add here, simply calls build_
-void SquaredErrModule::build()
-{
-    inherited::build();
-    build_();
-}
-
-void SquaredErrModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-//    deepCopyField(target, copies);
-}
-
-void SquaredErrModule::declareOptions(OptionList& ol)
-{
-    inherited::declareOptions(ol);
-}
-
-void SquaredErrModule::build_()
-{
-    if( input_size < 0 )
-    {
-        PLWARNING("SquaredErrModule::build_: 'input_size' is < 0.\n"
-                  "You should set it to a positive integer.\n"
-                  "Defaulting to '1' (scalar version).");
-        input_size = 1;
-    }
-    if( output_size != 1 )
-    {
-        PLWARNING("SquaredErrModule::build_: 'output_size' (%i) should be 1.\n"
-                  "Setting 'output_size' to 1.\n", output_size);
-        output_size = 1;
-    }
-
-    target_size = input_size;
-}
-
-
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/SquaredErrModule.h
===================================================================
--- trunk/plearn_learners/online/SquaredErrModule.h	2007-01-24 05:13:12 UTC (rev 6604)
+++ trunk/plearn_learners/online/SquaredErrModule.h	2007-01-24 05:22:36 UTC (rev 6605)
@@ -1,149 +0,0 @@
-// -*- C++ -*-
-
-// SquaredErrModule.h
-//
-// Copyright (C) 2005 Pascal Lamblin
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************
-   * $Id: SquaredErrModule.h,v 1.1 2005/11/30 04:36:17 lamblinp Exp $
-   ******************************************************* */
-
-// Authors: Pascal Lamblin
-
-/*! \file SquaredErrModule.h */
-
-
-#ifndef SquaredErrModule_INC
-#define SquaredErrModule_INC
-
-#include <plearn/base/Object.h>
-#include <plearn/math/TMat_maths.h>
-#include "OnlineLearningModule.h"
-
-namespace PLearn {
-
-/**
- * Squared difference (and derivatives thereof) between the target and input.
- *
- */
-class SquaredErrModule : public OnlineLearningModule
-{
-    typedef OnlineLearningModule inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    SquaredErrModule();
-
-    // Your other public member functions go here
-/*
-    virtual void setTarget(const Vec the_target);
-    virtual void setTarget(real the_target);
-    virtual Vec getTarget() const;
-*/
-    virtual void fprop(const Vec& input, Vec& output) const;
-
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             const Vec& output_gradient);
-
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
-
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              const Vec& output_gradient,
-                              const Vec& output_diag_hessian);
-
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              Vec& input_gradient,
-                              const Vec& output_gradient,
-                              Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
-
-
-    virtual void forget();
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    PLEARN_DECLARE_OBJECT(SquaredErrModule);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-protected:
-    //#####  Protected Options  ###############################################
-//    Vec target;
-    int target_size;
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    static void declareOptions(OptionList& ol);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(SquaredErrModule);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Jan 24 06:56:30 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 06:56:30 +0100
Subject: [Plearn-commits] r6606 - in trunk/plearn_learners/online: .
	DEPRECATED
Message-ID: <200701240556.l0O5uUXj014781@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 06:56:29 +0100 (Wed, 24 Jan 2007)
New Revision: 6606

Added:
   trunk/plearn_learners/online/DEPRECATED/multi_layer_rbm.txt
Removed:
   trunk/plearn_learners/online/multi_layer_rbm.txt
Log:
Deprecate file presenting equations and pseudo-code for the DBNs.
The equations are at
<http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Neurones/DBNEquations>.
The pseudo-code is in $PLEARNDIR/doc/machine_learning.tex


Copied: trunk/plearn_learners/online/DEPRECATED/multi_layer_rbm.txt (from rev 6596, trunk/plearn_learners/online/multi_layer_rbm.txt)

Deleted: trunk/plearn_learners/online/multi_layer_rbm.txt
===================================================================
--- trunk/plearn_learners/online/multi_layer_rbm.txt	2007-01-24 05:22:36 UTC (rev 6605)
+++ trunk/plearn_learners/online/multi_layer_rbm.txt	2007-01-24 05:56:29 UTC (rev 6606)
@@ -1,507 +0,0 @@
-
-                      Contrastive divergence multi-layer RBMs
-
-
-Boltzmann machines:
-
-X (may be) observed, H is (always) hidden r.v.
-Their joint is given by the Boltzmann distribution
-associated with an energy function energy(x,h):
-  P(X=x,H=h) = exp(-energy(x,h)) / Z
-where Z is the appropriate normalization constant:
-  Z = sum_{x,h} exp(-energy(x,h)).
-
-In ordinary Boltzmann machines, the energy function is a 
-quadratic polynomial. Let z=(x,h), then
-  energy(z) = sum_i b_i z_i    +    sum_{ij} w_{ij} z_i z_j
-where z_i \in {0,1}. There is no need for a constant term 
-since it would cancel out in Z.
-
-If X=x is observed while H remains hidden, the likelihood
-involves a sum over all configurations of H:
-
-  P(X=x) = sum_h P(X=x,H=h) = sum_h exp(-energy(x,h)) / Z
-
-Unfortunately, the exact gradient of log P(x) wrt b or w
-is intractable since it involves two intractable sums
-(the one over h for the numerator of the above, and the
-one over h and x, for Z, the denominator).
-
-The gradient can be written as a sum of the corresponding two terms:
-
-  d(-log P(x))/dtheta =
-  sum_h      P(H=h|X=x) d_energy(h,x)/dtheta          [this is the POSITIVE phase contribution]
-- sum_{h,x}  P(H=h,X=x) d_energy(h,x)/dtheta          [this is the NEGATIVE phase contribution]
-
-The derivation of this result is easy (we do a similar derivation
-for the restricted Boltzmann machine below).
-
-The standard way to estimate the gradient, to avoid these sums, is
-to perform an MCMC scheme to obtain one or more samples from P(h|x)
-with x ~ training set and from P(x,h).
-
-For a reason that I do not remember exactly, the w matrix
-is normally constrained to be symmetric. I believe it has
-to do with the MCMC scheme normally used for sampling
-from P(X) or from P(H|X). However, we will not be using
-that scheme, so I do not think that this restriction
-is important (to be verified somehow).
-
-Restricted Boltzmann machines:
-
-If we set the weight between h_i and h_j to 0 
-and the weight between x_i and x_j to 0, we obtain a RBM.
-The advantage of an RBM is that all the H_i's become independent
-when conditioning on X, and (symmetrically) all the X_i become independent
-when conditioning on H.
-
-
-Energy functions for Restricted Boltzmann Machines:
-(Note that w_{ij} = w_{ji})
-
- * energy term for binomial unit i with value v_i and inputs u_j, parameters (b_i, w_{i.}): 
-     
-    b_i v_i + sum_j w_{ij} v_i u_j
-   
-       ==> P(v_i=1 | u) = exp(- b_i - sum_j w_{ij} u_j) / (1 + exp(- b_i - sum_j w_{ij} u_j)) = sigmoid(-b_i - sum_j w_{ij} u_j)
-       NOTE THE MINUS
-
- * energy term for fixed-variance Gaussian unit i with value v_i and inputs u_j, parameters (a_i,b_i, w_{i,.}):
-
-    b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j
-       ==> P(v_i | u) = (1/Z) exp(-(b_i v_i + a_i^2 v_i^2 + sum_j w_{ij} v_i u_j)) = (1/Z)(-0.5(v_i - mu)/sigma^2)
-       ==> P(v_i | u) = N(v_i; mu, sigma^2) with sigma^2 = 0.5/a_i^2, mu = -0.5(b_i + sum_j w_{ij} u_j)/a_i^2
-       NOTE HOW THESE BLOW UP WHEN a_i IS TOO SMALL. MAY WANT TO USE (a_i + epsilon)^2 INSTEAD, with epsilon fixed.
-
- * energy term for softmax units i with value v_i and inputs u_j, parameters (b_i, w_{i.}): 
-     
-    b_i v_i + sum_j w_{ij} v_i u_j
-   
-       ==> P(v_i=1 | u) = exp(- b_i - sum_j w_{ij} u_j) / sum_i " = softmax(- b_i - sum_j w_{ij} u_j)
-       NOTE THE MINUS
-
-Likelihood gradient for a RBM with observed inputs x, hidden outputs y:
-
- use Z = sum_{x,y} exp(-energy(x,y))
-    P(x,y) = exp(-energy(x,y))/Z
-    P(y|x) = exp(-energy(x,y)) / sum_y exp(-energy(x,y))
-
- For ANY energy-based (Boltzmann) distribution:
-  d/dtheta (- log P(x)) = d/dtheta (- log sum_y P(x,y)) = d/dtheta (- log sum_y exp(-energy(x,y))/Z)
-                        = (Z/(sum_y exp(-energy(x,y)))) sum_y exp(-energy(x,y))/Z (d_energy(x,y)/dtheta + (1/Z) dZ/dtheta)
-                        = sum_y P(y|x) d_energy(x,y)/dtheta - (1/Z) sum_{x,y} exp(-energy(x,y)) d_energy(x,y)/dtheta
-                        = sum_y P(y|x) d_energy(x,y)/dtheta - sum_{x,y} P(x,y) d_energy(x,y)/dtheta
-                        = E[d_energy(x,y)/dtheta | x] - E[d_energy(x,y)/dtheta]
-                          where E is over the model's distribution
-                        = "positive phase contribution" - "negative phase contribution"
- The positive phase tries to lower the energy of observed x while the negative phase tries to increase the energy of all x ~ P.
-
- For a RBM, P(y|x) factorizes into P(y_i|x) and energy is a sum over energy_i(x,y_i), and theta={theta_i} so that 
-  sum_y P(y|x) d_energy(x,y)/dtheta_i = sum_{y_i} P(y_i|x) d_energy_i(x,y_i)/dtheta_i
- with theta_i the parameters associated with y_i.
-  
- With Contrastive Divergence we replace the expectation over (x,y) by a sample taken after 1 (or more) Gibbs sampling steps
-    observed x = x0 -- (P(y|x0)) --> y0 -- (P(x|y0)) --> x1 -- (P(y|x1)) --> y1
- and the pair (x1,y1) serves as that sample in the case of 1 step (= "CD1").
-
-  * output binomial unit i <-> input binomial unit j
-      weight w_{ij}:
-       positive phase contribution: P(y_{0i}=1|x_0) 1*x_{0j} + (1 - P(y_{0i}=1|x_0)) 0*x_{0j} = P(y_{0i}=1|x_0) x_{0j}
-       negative phase contribution: P(y_{1i}=1|x_1) 1*x_{1j} + (1 - P(y_{1i}=1|x_1)) 0*x_{1j} = P(y_{1i}=1|x_1) x_{1j}
-      bias b_i:
-       positive phase contribution: P(y_{0i}=1|x_0) 
-       negative phase contribution: P(y_{1i}=1|x_1) 
-
-  * output binomial unit i <-> input Gaussian unit j
-      bias b_i and weight w_{ij} as above
-      parameter a_j:
-       positive phase contribution: 2 a_j x_{0j}^2
-       negative phase contribution: 2 a_j x_{1j}^2
-      
-  * output softmax unit i <-> input binomial unit j
-      same formulas as for binomial units, except that P(y_i=1|x) is computed differently (with softmax instead of sigmoid)
-
-
-Basic operations in each RBM layer:
-
- * layer::upActivation: compute parameters of p(output_variables | input_variables)
-             i.e. compute activation (weighted sum) in the case of binomial units, and (mu,sigma) in the case of Gaussian units
- * layer::fprop(input,E[output|input]): find E(output_variables | input_variables). This ALWAYS calls upActivation.
-             call upActivation and then compute & return corresponding expectation (p for binomial = sigmoid(activation), mu for Gaussian)
- * layer::upSample(): sample output_variables from p(output_variables | input_variables). upActivation MUST have been called before.
-             sample from binomial or from Gaussian
- * layer::downActivation(): compute parameters of p(input_variables | output_variables)
-             similar to upActivation, but downward, i.e. on input variables
- * layer::downExpectation(): find E(input_variables | output_variables). This ALWAYS calls downActivation.
-             similar to upExpectation but downward, i.e. on input variables
- * layer::downSample(): sample input_variables from p(input_variables | output_variables). downActivation MUST have been called before.
-             similar to upSample but on input variables
- * layer::accumulatePosStats(): accumulate positive phase statistics
-             for binomial and softmax output units: accumulate input_variable[i]*output_prob[j] in pos_corr[i][j]
-                                                    accumulate output_prob[j] in pos_mean[j]
-             for Gaussian input units: accumulate 2 a[i] input_variable[i]^2 in pos_square[i]
-             for all: increment pos_count
- * layer::accumulateNegStats(): accumulate negative phase statistics
-             for binomial output units: accumulate input_variable[i]*output_prob[j] in neg_corr[i][j]
-             for Gaussian input units: accumulate 2 a[i] input_variable[i]^2 in neg_square[i]
-             for all: increment neg_count
- * layer::layerCDupdate(): update parameters using positive and negative phase statistics
-             for binomial and softmax output units: 
-                  gradient estimator for b_j = pos_mean[j]/pos_count - neg_corr[i][j]/neg_count
-                  gradient estimator for w_ij = pos_corr[i][j]/pos_count - neg_corr[i][j]/neg_count
-                  clear pos_corr, neg_corr
-             for Gaussian input units:
-                  gradient estimator for a_i = pos_square[i]/pos_count - neg_square[i]/neg_count
-                  clear pos_square
-             clear pos_count,neg_count
-
-
-Combined operations, in a supervised RBM network:
- A supervised RBM network is an OnlineLearningModule that has a sequence of layers. 
- The last_layer is fully supervised. The 2nd to last is the last_hidden_layer.
- There is also a cost_layer that follows the last_layer, which computes the supervised cost to minimize.
-
- * network::computeRepresentation(input_variables):
-     v <- input_variables
-     for each layer except the last:
-        layer->input_variables <- v.copy()
-        v.resize(output_variables.size())
-        layer->fprop(layer->input_variables,v)
-     return v
-
- * network::fprop(input_variables)
-     computeRepresentation(input_variables)
-     if last_layer_takes_activations: (e.g. for undirected softmax last_layer)
-       last_layer->fprop(last_hidden_layer->output_activation,last_layer->output)
-     else
-       last_layer->fprop(last_hidden_layer->output,last_layer->output)
-     return last_layer->output
-
- * network::unsupervised_learning_from_example(input_variables):
-     v <- input_variables
-     for each layer except the last:
-        layer->input_variables <- v.copy()
-        layer->fprop(layer->input_variables,layer->output)
-        v <- layer->output
-        layer->upSample() 
-        layer->accumulatePosStats()
-        layer->downSample()
-        layer->upSample()
-        layer->accumulateNegStats()
-        layer->CDupdate()
-
- * network::supervised_learning_from_example(input_variables, output_variables):
-     unsupervised_learning_from_example(input_variables)
-     if last_layer_takes_activations: (e.g. for undirected softmax last_layer)
-       last_layer->fprop(v=last_hidden_layer->output_activation,last_layer->output)
-     else
-       last_layer->fprop(v=last_hidden_layer->output,last_layer->output)
-     cost_layer->fprop((last_layer->output,output_variables),cost)
-     cost_layer->bpropUpdate((last_layer->output,output_variables),cost,(dout,tmp), 1)
-     last_layer->bpropUpdate(v,out,din,dout)
-     for each layer except the last, going backwards:
-        dout <- din.copy()
-        layer->bpropUpdate(layer->input_variables,layer->output, din, dout)
-
-
- * network::unconditionalGenerate(): sample an input, unconditionally
-     last_layer->clearActivations() // so that the first upSample() only samples from the biases
-     repeat n_MCMC_sampling_iterations
-       last_layer->upSample()
-       last_layer->downActivation()
-       last_hidden_layer->output_activation_external_contribution <- last_layer->input_activation.copy()
-       last_hidden_layer->upActivation() // adds activation from inputs with external contribution from last_layer
-       last_hidden_layer->upSample()
-       last_hidden_layer->downActivation()
-       last_hidden_layer->downSample()
-       last_layer->input_variables <- last_hidden_layer->output_variables.copy()
-       last_layer->upActivation()
-     v <- last_hidden_layer->output_variables
-     for each layer except the last, going backwards:
-       layer->output_variables <- v.copy()
-       layer->downActivation()
-       layer->downSample()
-       v <- layer->input_variables
-     return v
-
-
- * network::conditionalGenerate(output): sample an input, conditional on the given output value
-     last_layer->output_variables <- output.copy()
-     last_layer->downActivation() // compute part of the activation of last hidden units due to output units
-     last_hidden_layer->output_activation_external_contribution <- last_layer->input_activation.copy()
-     repeat n_MCMC_sampling_iterations
-       last_hidden_layer->downActivation()
-       last_hidden_layer->downSample()
-       last_hidden_layer->upActivation()
-       last_hidden_layer->upSample()
-     v <- last_hidden_layer->output_variables
-     for each layer except the last, going backwards:
-       layer->output_variables <- v.copy()
-       layer->downActivation()
-       layer->downSample()
-       v <- layer->input_variables
-     last_hidden_layer->output_activation_external_contribution.clear()
-     return v
-
--------------------------------------------------------------------------------
-Using and training the last layers
--------------------------------------------------------------------------------
-
-Using
------
-If we train the network in a supervised fashion, we introduce a layer
-containing the outputs (or targets), Y.
-Let's call the last layer L and the previous layer P, and define:
-
- R.V. (=sample) of the last layer = L_i
- R.V. (=sample) of the previous (next-to-last) layer = P_j
- R.V. of the supervised layer = Y_k
- energy parameters between P_j and L_i = V,C: energies V_ij L_i P_j + C_i L_i
- energy parameters between L_i and Y_k = W,B: energies W_ki Y_k L_i + B_k Y_k
- "output" (expectation) of next-to-last layer = p(P) (given the inputs of the
-network)
-
- * The activation of Y is computed from p(P), not p(L):
-
-    actY_k = -B_k + sum_i softplus(-(W_ki + C_i + sum_j V_ij p(P_j)))
-    with softplus: x -> log(1 + exp(x))
-    (see below for explanation)
-
- * The expectations of Y, from the activations (Y is a multinomial units set)
-       P(Y|inputs) = softmax(actY)
-
- * fprop = expectations ( activations )
-
-Training
---------
-There are two ways of learning the parameters V, C, W and B
-
- * By simple gradient descent:
-We compute the cost = - log P(Y=onehot(k)|inputs), where k is the
-observed target, and we backprop all the way.
-
- * By using contrastive divergence:
-    - we consider [ Y, P ] = X a big layer, beyond L
-    - we put [onehot(k), p(P)] = x in X, as input of L
-    - we compute the expectations of L given x:
-        p(L_i) = sigmoid(C_i + W_ki + sum_j V_ij p(P_j))
-    - accumulatePosStats(x, p(L))
-    - sample L according to p(L)
-    - compute p(P) given L
-    - sample P according to p(P)
-    - compute p(L) again, using P as input (and NOT p(P))
-    - accumulateNegStats([onehot(k), P], p(L))
-    - update
-
-Or a combination of the two above. See rbm_todo_and_ideas.txt for variants.
-
-Why?
-----
-The output probabilities are computed as follows:
-    P(Y=onehot(k)|inputs) = exp(-B_k + sum_i softplus(-(W_ki + C_i + sum_j V_ij p(P_j))) / Z
-
-This formula can be derived by considering that P, L, and Y are binary
-random variables following the Boltzmann distribution with energy:
-    E(L,Y,P) = B'Y + Y'WL + C'L + P'V'L
-
-During training, both P and Y are observed, so that E is linear in L,
-i.e. P(L|P,Y) is a product of P(L_i|P,Y):
-the L_i are conditionally independant given P and Y.
-
-This corresponds to an undirected graphical model with full connectivity
-between each L_i and each Y_k (and similarly between L_i and each P_j),
-but no connection among the L_i or among the Y_k's. Because of this
-factorization we obtain that
-    P(Y|P) = sum_L exp(-E(L,Y,P)) / Z
-and
-   sum_L exp(-E(L,Y,P)) = exp(-B'Y) prod_i(exp(-E_i(1,Y,P)) + exp(-E_i(0,Y,P)))
-
-where E_i(l,Y,P) = the term in L_i=l in the energy
-                 = l (sum_k Y_k W_ki + C_i + sum_j V_ij P_j).
-
-Since E_i(0,Y,P) = 0, we obtain that
-    sum_L exp(-E(L,Y,P)) = exp(-B'Y) exp(sum_i log(1 + exp(-E_i(1,Y,P))))
-                         = exp(-B'Y + sum_i softplus(-sum_k Y_k W_ki + C_i + sum_j V_ij P_j))
-
-which gives the above formula for P(Y=onehot(k)|inputs), if we replace P by
-its expectation p(P).
-
-
--------------------------------------------------------------------------------
-Other architecture and pseudo-code proposed
--------------------------------------------------------------------------------
-
-The units themselves, stored in layers (class RBMLayer), are separated
-from the weighted links between layers (class RBMParameters, inheriting
-from OnlineLearningModule). The top-level architecture of the network (say,
-DeepBeliefNetwork) is a PLearner (maybe a PDistribution) with functions for
-sampling.
-
-RBMLayer stores:
-  - activations: values allowing to know the distribution (the activation
-    value (before sigmoid) for a binomial input, the couple (mu,sigma) for a
-    gaussian).
-  - expectations: expectation of each unit
-  - sample: sample from the distribution
-  - some flags to know what is up-to-date
-
-RBMParameters stores (and has to update):
-  - a string describing the type of upper units, "l" if the energy function is
-    linear (binomial or part of multinomial), "q" if quadratic (gaussian)
-  - the same thing for the lower units
-  - weights: weights between up and down layers
-  - TVec<Vec> up_unit_params: the Vec up_unit_params[i] contains the
-    bias (and parameters like the quadratic term, in gaussian units) for
-    unit i in up layer
-  - TVec<Vec> down_unit_params: the same for down error
-  - input_vec: a pointer to its current input vector (sample or expectation),
-    and a flag to know if it is up or down
-
-  - weights_pos_stats, weight_neg_stats: statistics accumulated during
-    positive (respectively negative) phase
-  - up_unit_params_pos_stats, up_unit_params_neg_stats
-  - down_unit_params_pos_stats, down_unit_params_neg_stats
-  - pos_count and neg_count: two counters of the numbers of statistics
-    accumulated
-  - a learning rate (possibly a second one specially for error gradient)
-
-The methods are:
-  * RBMParameters::setAsUpInput( Vec ) : set the input vector, and flag to 'up'
-  * RBMParameters::setAsDownInput( Vec ) : same, but 'down'
-  * RBMParameters::accumulatePosStats( Vec down_values, Vec up_values ) :
-      weights_pos_stats += up_values * down_values';
-      for i in up units:
-        up_unit_params_pos_stats[i][0] += up_values[i]
-      for i in "g" up units:
-        up_unit_params_pos_stats[i][1] += 2 * up_unit_params[i][1] * up_values[i]^2
-      for i in down units:
-        down_unit_params_pos_stats[i][0] += down_values[i]
-      for i in "g" down units:
-        down_unit_params_pos_stats[i][1] += 2 * down_unit_params[i][1] * down_values[i]^2
-      pos_count++;
-
-  * RBMParameters::accumulateNegStats( Vec down_values, Vec up_values ) :
-      weights_neg_stats += up_values * down_values';
-      for i in up units:
-        up_unit_params_neg_stats[i][0] += up_values[i]
-      for i in "g" up units:
-        up_unit_params_neg_stats[i][1] += 2 * up_unit_params[i][1] * up_values[i]^2
-      for i in down units:
-        down_unit_params_neg_stats[i][0] += down_values[i]
-      for i in "g" down units:
-        down_unit_params_neg_stats[i][1] += 2 * down_unit_params[i][1] * down_values[i]^2
-      neg_count++;
-
-  * RBMParameters::update() (from statistics accumulated)
-      # are the signs OK?
-      weights -= learning_rate * (weights_pos_stats/pos_count - weight_neg_stats/neg_count);
-      up_unit_params -= learning_rate * (up_unit_params_pos_stats/pos_count - up_unit_params_neg_stats/neg_count);
-      down_unit_params -= learning_rate * (down_unit_params_pos_stats/pos_count - down_unit_params_neg_stats/neg_count);
-      # reset
-      weights_pos_stats.clear();
-      weights_neg_stats.clear();
-      up_unit_params_pos_stats.clear();
-      up_unit_params_neg_stats.clear();
-      down_unit_params_pos_stats.clear();
-      down_unit_params_neg_stats.clear();
-      pos_count = 0;
-      neg_count = 0;
-
-  * implement fprop(), bpropUpdate() (how?)
-
-  * RBMParameters::computeUnitActivations( int i, const Vec& activations ) :
-    put in activations the parameters describing the distribution of
-    unit i (i is an index in the up layer if input vector is 'down', and
-    vice-versa), and invalidates 'sample' and 'expectation'
-
-  * RBMParameters::computeUnitActivations( const Vec& all_activations ) :
-    same thing, for every unit in the appropriate layer
-
-For class RBMLayer:
-
-  * RBMLayer::getUnitActivations( int i, RBMParams rbmp ) :
-      calls rbmp->computeUnitActivations( i, layer_params.subvec(...) )
-  * RBMLayer::getUnitActivations( RBMParams rbmp ) :
-      calls rbmp->computeUnitActivations( layer_params ) by default, or
-      rbmp->computeUnitActivations( indices, layer_params ),
-        indices being a subset of units, for "sparse" layers
-  * RBMLayer::generateSample()
-      get 'sample' from 'activations'
-  * RBMLayer::computeExpectation()
-      get 'expectation' from 'activations'
-
-  * string RBMLayer::getUnitsTypes()
-      returns a string with each character encoding the type of corresponding
-      unit, 'l' for linear and 'g' for gaussian
-
-If a DeepBeliefNetwork has two RBMLayer's (input and output) and one
-RBMParameters (rbmp) linking both of them, a simple up propagation of
-input_vec to a sample (out_sample) would typically call:
-
-    input.sample << input_vec;
-    rbmp.setAsDownInput( input.sample );
-    output.getUnitActivations( rbmp );
-    output.generateSample();
-    out_sample << output.sample;
-
-If we wanted the expectation (out_exp) instead of a sample, it would have
-been:
-
-    input.sample << input_vec;
-    rbmp.setAsDownInput( input.sample );
-    output.getUnitActivations( rbmp );
-    output.computeExpectation();
-    out_exp << output.expectation;
-
-The learning of one example (input_vec) by contrastive divergence:
-
-    // positive phase
-    input.sample << input_vec;
-    rbmp.setAsDownInput( input.sample );
-    output.getUnitActivations( rbmp );
-    output.generateSample();
-    rbmp.accumulatePosStats( input.sample, output.sample );
-
-    // negative phase
-    rbmp.setAsUpInput( output.sample );
-    input.getUnitActivations( rbmp );
-    input.generateSample();
-    rbmp.setAsDownInput( input.sample );
-    output.getUnitActivations( rbmp );
-    output.generateSample();
-    rbmp.accumulateNegStats( input.sample, output.sample );
-
-    // update
-    rbmp.update();
-
-If we have three sequential layers (input, hidden, output) and two
-RBMParameters:
-  - rbmp_ih between input and hidden, frozen (because learned previously);
-  - rbmp_ho between hidden and output, which we train;
-the learning of one example (input_vec) will look like:
-
-    // propagation to hidden
-    input.sample << input_vec;
-    rbmp_ih.setAsDownInput( input.sample );
-    hidden.getUnitActivations( rbmp_ih );
-    hidden.computeExpectation(); // not generateSample()
-
-    // positive phase
-    rbmp_ho.setAsDownInput( hidden.expectation );
-    output.getUnitActivations( rbmp_ho );
-    output.generateSample();
-    rbmp_ho.accumulatePosStats( hidden.expectation, output.sample );
-
-    // negative phase
-    rbmp_ho.setAsUpInput( output.sample );
-    hidden.getUnitActivations( rbmp_ho );
-    hidden.getExpectation();
-    rbmp_ho.setAsDownInput( hidden.expectation );
-    output.getUnitActivations( rbmp_ho );
-    output.generateSample();
-    rbmp_ho.accumulateNegStats( hidden.expectation, output.sample );
-
-    // update
-    rbmp_ho.update();
-
-



From lamblin at mail.berlios.de  Wed Jan 24 07:11:41 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 07:11:41 +0100
Subject: [Plearn-commits] r6607 - trunk/plearn_learners/online
Message-ID: <200701240611.l0O6BfwX030093@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 07:11:41 +0100 (Wed, 24 Jan 2007)
New Revision: 6607

Modified:
   trunk/plearn_learners/online/rbm_todo_and_ideas.txt
Log:
Update this file with the results we've had.
TODO: where do we put such files?


Modified: trunk/plearn_learners/online/rbm_todo_and_ideas.txt
===================================================================
--- trunk/plearn_learners/online/rbm_todo_and_ideas.txt	2007-01-24 05:56:29 UTC (rev 6606)
+++ trunk/plearn_learners/online/rbm_todo_and_ideas.txt	2007-01-24 06:11:41 UTC (rev 6607)
@@ -5,19 +5,29 @@
 
 * Implement DeepBeliefNet as a PDistribution, trained the way Hinton
   trains its network
+    -> PDistribution was a bad idea (too complicated).
+    -> It is implemented as a PLearner, in $PLEARNDIR/plearn_learners/online.
+    -> It still lacks the generating functions.
 
 * Make an OnlineLearningModule stacking RBMLayers and RBMParameters doing the
   same thing? Encapsulating DeepBeliefNet? Make DeepBeliefNet encapsulate the
   module?
+    -> Nope.
+    -> We can use ModuleStackModule if we only want to use the mean-field
+approximation.
 
+
 Questions
 =========
 
 * How do we initialize the weights? Is the sampling process enough to break
   the symmetry if all weights are initialized to 0? Should we initialize them
   randomly ?
+    -> Initializing to 0 can lead to sub-optimal results. Initializing
+    randomly experimentally led to better solutions.
 
-* When do we update the weights? After each sampe? At the end of a batch?
+* When do we update the weights? After each sample? At the end of a batch?
+    -> After each sample seems to be OK.
 
 * How to learn the weights V between Last layer and Previous layer, and W
   between Last layer and output/target layer (Y) if trained in a supervised
@@ -29,10 +39,17 @@
   - Learn V by contrastive divergence, but clamping Y to the true target
     during both positive and negative phases
 
-* Can we compute the predicted probability (or density?) of an input? Of the
-  label given the input? The input given the label?
+    -> Second solutions seems to work OK, but Hinton uses V unsupervisedly and
+    then fine-tunes the whole network (no pre-training of W).
 
+* Can we compute the predicted probability (or density?) of an input?
+    -> Not without sampling, or summing an exponential number of terms
+  Of the label given the input?
+    -> Yes
+  The input given the label?
+    -> Nope (see 1.)
 
+
 Ideas
 =====
 
@@ -40,8 +57,11 @@
 the up (and down) layer have the same type ('l' or 'q'), because
 RBMGenericParameters has to check every unit. So we could use matrix-matrix
 products instead of n matrix-vector products.
+    -> Obsoleted by new version. RBMConnection (RBMParameter's successor) does
+    not depend on the type of the units anymore.
 
 * How to combine error gradient and likelihood gradient?
+    -> Add them with two different learning rates.
 
 * Implement global wake-sleep (on every layer at one time) learning of the
   weights ==> this would seem necessary in order to have simultaneous
@@ -49,22 +69,31 @@
 
 * See if we share biases (and quadratic term) of one layer between the "upper"
   and "lower" RBMParameters modules
+    -> Yes. They are now integrated into RBMLayer, which is shared between the
+    wo RBMs.
 
 * Is there a way to compute analytically the "undirected softmax" if the
   output layer has Gaussian units?
-  -> maybe if only one Gaussian, probably not otherwise
+    -> maybe if only one Gaussian, probably not otherwise
 
 * See when we should sample from a layer, and when we should compute the
   expectation. Is it a problem if some statistics during the positive phase
   are expectations, and are samples during the negative phase (or vice-versa)?
+    -> The solution that works best experimentally is:
+        v_0: expectation (or original input)
+        h_0: expectation, but initialize the Markov chain with a sample
+        v_1: sample
+        h_1: expectation
 
 * introducing temporal structure into the model:
-   - time delays in the connections
-   - recurrent connections (e.g. from layer i at t to layer i at t+1)
-   - supervised targets (and corresponding gradients) from the task of
-     predicting layer i at t+1 from layer j at t
+  - time delays in the connections
+  - recurrent connections (e.g. from layer i at t to layer i at t+1)
+  - supervised targets (and corresponding gradients) from the task of
+    predicting layer i at t+1 from layer j at t
 
+    -> See $PLEARNDIR/plearn_learners_experimental/DynamicallyLinkedRBMsModel
+    -> Still lots of things to do.
 
+    -> Where do we put this document?
 
 
-



From lamblin at mail.berlios.de  Wed Jan 24 07:21:11 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 07:21:11 +0100
Subject: [Plearn-commits] r6608 - in trunk: commands plearn_learners/online
	plearn_learners/online/DEPRECATED
Message-ID: <200701240621.l0O6LBuE012213@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 07:21:10 +0100 (Wed, 24 Jan 2007)
New Revision: 6608

Added:
   trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.cc
   trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h
Removed:
   trunk/plearn_learners/online/StackedModulesModule.cc
   trunk/plearn_learners/online/StackedModulesModule.h
Modified:
   trunk/commands/plearn_light_inc.h
   trunk/commands/plearn_noblas_inc.h
   trunk/plearn_learners/online/ModulesLearner.h
Log:
Deprecate StackedModulesModules.
Please use ModuleStackModule (if you didn't use last_layer_is_cost),
ProcessInputCostModule or a combination of both (if you did).


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-01-24 06:11:41 UTC (rev 6607)
+++ trunk/commands/plearn_light_inc.h	2007-01-24 06:21:10 UTC (rev 6608)
@@ -201,6 +201,8 @@
 #include <plearn_learners/online/CostModule.h>
 #include <plearn_learners/online/DeepBeliefNet.h>
 #include <plearn_learners/online/GradNNetLayerModule.h>
+#include <plearn_learners/online/ModulesLearner.h>
+#include <plearn_learners/online/ModuleStackModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
 #include <plearn_learners/online/NLLErrModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
@@ -217,8 +219,6 @@
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
-#include <plearn_learners/online/ModulesLearner.h>
-#include <plearn_learners/online/StackedModulesModule.h>
 #include <plearn_learners/online/Subsampling2DModule.h>
 #include <plearn_learners/online/Supersampling2DModule.h>
 #include <plearn_learners/online/TanhModule.h>

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-01-24 06:11:41 UTC (rev 6607)
+++ trunk/commands/plearn_noblas_inc.h	2007-01-24 06:21:10 UTC (rev 6608)
@@ -209,7 +209,6 @@
 #include <plearn_learners/online/RBMTruncExpLayer.h>
 #include <plearn_learners/online/SoftmaxModule.h>
 #include <plearn_learners/online/SquaredErrorCostModule.h>
-#include <plearn_learners/online/StackedModulesModule.h>
 #include <plearn_learners/online/Subsampling2DModule.h>
 #include <plearn_learners/online/Supersampling2DModule.h>
 #include <plearn_learners/online/TanhModule.h>

Copied: trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.cc (from rev 6596, trunk/plearn_learners/online/StackedModulesModule.cc)

Copied: trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h (from rev 6596, trunk/plearn_learners/online/StackedModulesModule.h)
===================================================================
--- trunk/plearn_learners/online/StackedModulesModule.h	2007-01-23 20:43:54 UTC (rev 6596)
+++ trunk/plearn_learners/online/DEPRECATED/StackedModulesModule.h	2007-01-24 06:21:10 UTC (rev 6608)
@@ -0,0 +1,216 @@
+// -*- C++ -*-
+
+// StackedModulesModule.h
+//
+// Copyright (C) 2006 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+// Authors: Pascal Lamblin
+
+/*! \file StackedModulesModule.h */
+
+
+#ifndef StackedModulesModule_INC
+#define StackedModulesModule_INC
+
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+namespace PLearn {
+
+/**
+ * Wraps a stack of OnlineLearningModule, which are layers.
+ * The OnlineLearningModule's are disposed like superposed layers:
+ * outputs of module i are the inputs of module (i+1), the last layer is
+ * the output layer.
+ *
+ * @deprecated: use ../ModuleStackModule or ../ProcessInputCostModule instead
+ */
+class StackedModulesModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+    //! ### declare public option fields (such as build options) here
+    //! Start your comments with Doxygen-compatible comments such as //!
+
+    //! Underlying layers of the Module
+    TVec< PP<OnlineLearningModule> > modules;
+
+    //! Indicates if the last layer is a cost layer (taking input and target as
+    //! input, and outputing the cost we will minimize), allowing this module
+    //! to behave the same way.
+    bool last_layer_is_cost;
+
+    //! If last_layer_is_cost, the size of the target
+    int target_size;
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    // ### Make sure the implementation in the .cc
+    // ### initializes all fields to reasonable default values.
+    StackedModulesModule();
+
+    // Your other public member functions go here
+
+    //! given the input, compute the output (possibly resize it  appropriately)
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    //! Adapt based on the output gradient: this method should only
+    //! be called just after a corresponding fprop; it should be
+    //! called with the same arguments as fprop for the first two arguments
+    //! (and output should not have been modified since then).
+    //! Since sub-classes are supposed to learn ONLINE, the object
+    //! is 'ready-to-be-used' just after any bpropUpdate.
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
+    //! JUST CALLS
+    //!     bpropUpdate(input, output, input_gradient, output_gradient)
+    //! AND IGNORES INPUT GRADIENT.
+    // virtual void bpropUpdate(const Vec& input, const Vec& output,
+    //                          const Vec& output_gradient);
+
+    //! this version allows to obtain the input gradient as well
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient,
+                             const Vec& output_gradient);
+
+    //! Similar to bpropUpdate, but adapt based also on the estimation
+    //! of the diagonal of the Hessian matrix, and propagates this
+    //! back. If these methods are defined, you can use them INSTEAD of
+    //! bpropUpdate(...)
+    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
+    //! WHICH JUST CALLS
+    //!     bbpropUpdate(input, output, input_gradient, output_gradient,
+    //!                  out_hess, in_hess)
+    //! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
+    // virtual void bbpropUpdate(const Vec& input, const Vec& output,
+    //                           const Vec& output_gradient,
+    //                           const Vec& output_diag_hessian);
+
+    //! this version allows to obtain the input gradient and diag_hessian
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian);
+
+    //! reset the parameters to the state they would be BEFORE starting
+    //! training.  Note that this method is necessarily called from
+    //! build().
+    virtual void forget();
+
+
+    //! optionally perform some processing after training, or after a
+    //! series of fprop/bpropUpdate calls to prepare the model for truly
+    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
+    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
+    // virtual void finalize();
+
+    //! in case bpropUpdate does not do anything, make it known
+    //! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false;
+    // virtual bool bpropDoesNothing();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    // ### If your class is not instantiatable (it has pure virtual methods)
+    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
+    PLEARN_DECLARE_OBJECT(StackedModulesModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+
+    //! Number of module layers
+    int nmodules;
+
+    //####  Not Options  ######################################################
+
+public: // hack
+    //! stores the input and output values of the functions
+    TVec<Vec> values;
+
+    //! stores the input of the last module, and the target if there is one
+    Vec cost_layer_input;
+
+    //! stores the gradients
+    TVec<Vec> gradients;
+
+    //! stores the diagonal of Hessians
+    TVec<Vec> diag_hessians;
+
+protected:
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+    void buildOptions();
+    void buildLayers();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(StackedModulesModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Modified: trunk/plearn_learners/online/ModulesLearner.h
===================================================================
--- trunk/plearn_learners/online/ModulesLearner.h	2007-01-24 06:11:41 UTC (rev 6607)
+++ trunk/plearn_learners/online/ModulesLearner.h	2007-01-24 06:21:10 UTC (rev 6608)
@@ -53,8 +53,6 @@
  * In order to stack layers, you can use StackedModulesModule,
  * and in order to compute several costs, you can use CombinedCostsModule.
  *
- * @todo Finish this class...
- *
  */
 class ModulesLearner : public PLearner
 {

Deleted: trunk/plearn_learners/online/StackedModulesModule.cc
===================================================================
--- trunk/plearn_learners/online/StackedModulesModule.cc	2007-01-24 06:11:41 UTC (rev 6607)
+++ trunk/plearn_learners/online/StackedModulesModule.cc	2007-01-24 06:21:10 UTC (rev 6608)
@@ -1,303 +0,0 @@
-// -*- C++ -*-
-
-// StackedModulesModule.cc
-//
-// Copyright (C) 2006 Pascal Lamblin
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Pascal Lamblin
-
-/*! \file StackedModulesModule.cc */
-
-
-
-#include "StackedModulesModule.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    StackedModulesModule,
-    "Wraps a stack of OnlineLearningModule, which are layers",
-    "The OnlineLearningModule's are disposed like superposed layers:\n"
-    "outputs of module i are the inputs of module (i+1), the last layer is\n"
-    "the output layer.\n");
-
-StackedModulesModule::StackedModulesModule() :
-    last_layer_is_cost( false ),
-    target_size( 0 ),
-    nmodules( 0 )
-{
-}
-
-void StackedModulesModule::declareOptions(OptionList& ol)
-{
-    /*
-    declareOption(ol, "", &StackedModulesModule::,
-                  OptionBase::buildoption,
-                  "");
-     */
-    declareOption(ol, "modules", &StackedModulesModule::modules,
-                  OptionBase::buildoption,
-                  "Underlying layers of the Module");
-
-    declareOption(ol, "last_layer_is_cost",
-                  &StackedModulesModule::last_layer_is_cost,
-                  OptionBase::buildoption,
-                  "Indicates if the last layer is a cost layer (taking input"
-                  " and target\n"
-                  "as input, and outputing the cost we will minimize),"
-                  " allowing this\n"
-                  "module to behave the same way.\n");
-
-    declareOption(ol, "target_size", &StackedModulesModule::target_size,
-                  OptionBase::buildoption,
-                  "If last_layer_is_cost, the size of the target");
-
-    declareOption(ol, "nmodules", &StackedModulesModule::nmodules,
-                  OptionBase::learntoption,
-                  "Number of module layers");
-
-    // Now call the parent class' declareOptions
-    inherited::declareOptions(ol);
-}
-
-void StackedModulesModule::build_()
-{
-    // initialize random generator from seed
-    if( !random_gen )
-        random_gen = new PRandom();
-    else
-        random_gen->manual_seed( random_gen->seed_ );
-
-    // get some options
-    nmodules = modules.length();
-    if( nmodules == 0 )
-        return;
-
-    if( last_layer_is_cost && target_size <= 0 )
-        PLERROR("StackedModulesModule::build_() - Please provide a target_size"
-                "  > 0\n"
-                "(is '%d').\n", target_size );
-    if( !last_layer_is_cost )
-        target_size = 0;
-
-    PLASSERT( modules[0]->input_size >= 0 );
-    input_size = modules[0]->input_size + target_size;
-
-//    int last_module_output_size = modules[nmodules-1]->output_size;
-//    if( last_layer_is_cost )
-//        last_module_output_size = 1;
-
-    output_size = modules[nmodules-1]->output_size;
-
-    // build the modules
-    buildLayers();
-
-}
-
-void StackedModulesModule::buildLayers()
-{
-    // first values will be "input" values
-    int size = input_size - target_size;
-    values.resize( nmodules+1 );
-    values[0].resize( size );
-    gradients.resize( nmodules+1 );
-    gradients[0].resize( size );
-    // TODO: use it only if we actually use bbprop?
-    diag_hessians.resize( nmodules+1 );
-    diag_hessians[0].resize( size );
-
-    for( int i=0 ; i<nmodules ; i++ )
-    {
-        modules[i]->estimate_simpler_diag_hessian =
-            estimate_simpler_diag_hessian;
-        modules[i]->random_gen = random_gen;
-        modules[i]->build();
-
-        size = modules[i]->output_size;
-        values[i+1].resize( size );
-        gradients[i+1].resize( size );
-        diag_hessians[i+1].resize( size );
-    }
-
-    // stores the input of the last module, and the target if there is one
-    cost_layer_input = values[nmodules-1];
-    if( last_layer_is_cost )
-        cost_layer_input.resize( cost_layer_input.size() + target_size );
-}
-
-void StackedModulesModule::build()
-{
-    inherited::build();
-    build_();
-}
-
-
-void StackedModulesModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-    deepCopyField(modules, copies);
-    deepCopyField(values, copies);
-    deepCopyField(cost_layer_input, copies);
-    deepCopyField(gradients, copies);
-    deepCopyField(diag_hessians, copies);
-}
-
-//! given the input, compute the output (possibly resize it  appropriately)
-void StackedModulesModule::fprop(const Vec& input, Vec& output) const
-{
-    PLASSERT( input.size() == input_size );
-    PLASSERT( modules[0]->input_size + target_size == input_size );
-    int last_input_size = values[nmodules-1].size();
-
-    values[0] << input.subVec(0, input_size - target_size );
-
-    for( int i=0 ; i<nmodules-1 ; i++ )
-        modules[i]->fprop( values[i], values[i+1] );
-
-    if( last_layer_is_cost )
-    {
-        cost_layer_input.subVec( last_input_size, target_size )
-            << input.subVec( input_size - target_size, target_size );
-    }
-
-    modules[nmodules-1]->fprop( cost_layer_input, values[nmodules] );
-    output.resize( output_size );
-    output << values[ nmodules ];
-}
-
-//! this version allows to obtain the input gradient as well
-//! N.B. THE DEFAULT IMPLEMENTATION IN SUPER-CLASS JUST RAISES A PLERROR.
-void StackedModulesModule::bpropUpdate(const Vec& input, const Vec& output,
-                                       Vec& input_gradient,
-                                       const Vec& output_gradient)
-{
-    // If last_layer_is_cost, the gradient wrt it is 1
-    if( last_layer_is_cost )
-        gradients[nmodules][0] = 1;
-    else
-        gradients[nmodules] << output_gradient;
-
-    // values should have the values given by fprop(), so
-    // values[nmodules] should already be equal to output
-    modules[nmodules-1]->bpropUpdate( cost_layer_input, values[nmodules],
-                                      gradients[nmodules-1],
-                                      gradients[nmodules] );
-
-    for( int i=nmodules-2 ; i>=0 ; i-- )
-        modules[i]->bpropUpdate( values[i], values[i+1],
-                                 gradients[i], gradients[i+1] );
-
-    input_gradient = gradients[0].copy();
-}
-
-//! reset the parameters to the state they would be BEFORE starting training.
-//! Note that this method is necessarily called from build().
-void StackedModulesModule::forget()
-{
-    random_gen->manual_seed( random_gen->seed_ );
-
-    // reset inputs
-    values[0].clear();
-    gradients[0].clear();
-    diag_hessians[0].clear();
-
-    // reset modules and outputs
-    for( int i=0 ; i<nmodules ; i++ )
-    {
-        modules[i]->forget();
-        values[i+1].clear();
-        gradients[i+1].clear();
-        diag_hessians[i+1].clear();
-    }
-}
-
-/* THIS METHOD IS OPTIONAL
-//! reset the parameters to the state they would be BEFORE starting training.
-//! Note that this method is necessarily called from build().
-//! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS DOES NOT DO
-//! ANYTHING.
-void StackedModulesModule::finalize()
-{
-}
-*/
-
-//! Similar to bpropUpdate, but adapt based also on the estimation
-//! of the diagonal of the Hessian matrix, and propagates this
-//! back. If these methods are defined, you can use them INSTEAD of
-//! bpropUpdate(...)
-//! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-//! RAISES A PLERROR.
-void StackedModulesModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                        Vec& input_gradient,
-                                        const Vec& output_gradient,
-                                        Vec& input_diag_hessian,
-                                        const Vec& output_diag_hessian)
-{
-    // If last_layer_is_cost, the gradient wrt it is 1 and hessian is 0
-    if( last_layer_is_cost )
-    {
-        gradients[nmodules][0] = 1;
-        diag_hessians[nmodules][0] = 1;
-    }
-    else
-    {
-        gradients[nmodules] << output_gradient;
-        diag_hessians[nmodules] << output_diag_hessian;
-    }
-
-    // values should have the values given by fprop(), so
-    // values[nmodules] should already be equal to output
-    for( int i=nmodules-1 ; i>=0 ; i-- )
-        modules[i]->bbpropUpdate( values[i], values[i+1],
-                                  gradients[i], gradients[i+1],
-                                  diag_hessians[i], diag_hessians[i+1] );
-
-    input_gradient = gradients[0].copy();
-    input_diag_hessian = diag_hessians[0].copy();
-}
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/StackedModulesModule.h
===================================================================
--- trunk/plearn_learners/online/StackedModulesModule.h	2007-01-24 06:11:41 UTC (rev 6607)
+++ trunk/plearn_learners/online/StackedModulesModule.h	2007-01-24 06:21:10 UTC (rev 6608)
@@ -1,214 +0,0 @@
-// -*- C++ -*-
-
-// StackedModulesModule.h
-//
-// Copyright (C) 2006 Pascal Lamblin
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-// Authors: Pascal Lamblin
-
-/*! \file StackedModulesModule.h */
-
-
-#ifndef StackedModulesModule_INC
-#define StackedModulesModule_INC
-
-#include <plearn_learners/online/OnlineLearningModule.h>
-
-namespace PLearn {
-
-/**
- * Wraps a stack of OnlineLearningModule, which are layers.
- * The OnlineLearningModule's are disposed like superposed layers:
- * outputs of module i are the inputs of module (i+1), the last layer is
- * the output layer.
- */
-class StackedModulesModule : public OnlineLearningModule
-{
-    typedef OnlineLearningModule inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-    //! ### declare public option fields (such as build options) here
-    //! Start your comments with Doxygen-compatible comments such as //!
-
-    //! Underlying layers of the Module
-    TVec< PP<OnlineLearningModule> > modules;
-
-    //! Indicates if the last layer is a cost layer (taking input and target as
-    //! input, and outputing the cost we will minimize), allowing this module
-    //! to behave the same way.
-    bool last_layer_is_cost;
-
-    //! If last_layer_is_cost, the size of the target
-    int target_size;
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    // ### Make sure the implementation in the .cc
-    // ### initializes all fields to reasonable default values.
-    StackedModulesModule();
-
-    // Your other public member functions go here
-
-    //! given the input, compute the output (possibly resize it  appropriately)
-    virtual void fprop(const Vec& input, Vec& output) const;
-
-    //! Adapt based on the output gradient: this method should only
-    //! be called just after a corresponding fprop; it should be
-    //! called with the same arguments as fprop for the first two arguments
-    //! (and output should not have been modified since then).
-    //! Since sub-classes are supposed to learn ONLINE, the object
-    //! is 'ready-to-be-used' just after any bpropUpdate.
-    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS, WHICH
-    //! JUST CALLS
-    //!     bpropUpdate(input, output, input_gradient, output_gradient)
-    //! AND IGNORES INPUT GRADIENT.
-    // virtual void bpropUpdate(const Vec& input, const Vec& output,
-    //                          const Vec& output_gradient);
-
-    //! this version allows to obtain the input gradient as well
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient,
-                             const Vec& output_gradient);
-
-    //! Similar to bpropUpdate, but adapt based also on the estimation
-    //! of the diagonal of the Hessian matrix, and propagates this
-    //! back. If these methods are defined, you can use them INSTEAD of
-    //! bpropUpdate(...)
-    //! N.B. A DEFAULT IMPLEMENTATION IS PROVIDED IN THE SUPER-CLASS,
-    //! WHICH JUST CALLS
-    //!     bbpropUpdate(input, output, input_gradient, output_gradient,
-    //!                  out_hess, in_hess)
-    //! AND IGNORES INPUT HESSIAN AND INPUT GRADIENT.
-    // virtual void bbpropUpdate(const Vec& input, const Vec& output,
-    //                           const Vec& output_gradient,
-    //                           const Vec& output_diag_hessian);
-
-    //! this version allows to obtain the input gradient and diag_hessian
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              Vec& input_gradient,
-                              const Vec& output_gradient,
-                              Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
-
-    //! reset the parameters to the state they would be BEFORE starting
-    //! training.  Note that this method is necessarily called from
-    //! build().
-    virtual void forget();
-
-
-    //! optionally perform some processing after training, or after a
-    //! series of fprop/bpropUpdate calls to prepare the model for truly
-    //! out-of-sample operation.  THE DEFAULT IMPLEMENTATION PROVIDED IN
-    //! THE SUPER-CLASS DOES NOT DO ANYTHING.
-    // virtual void finalize();
-
-    //! in case bpropUpdate does not do anything, make it known
-    //! THE DEFAULT IMPLEMENTATION PROVIDED IN THE SUPER-CLASS RETURNS false;
-    // virtual bool bpropDoesNothing();
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    // ### If your class is not instantiatable (it has pure virtual methods)
-    // ### you should replace this by PLEARN_DECLARE_ABSTRACT_OBJECT
-    PLEARN_DECLARE_OBJECT(StackedModulesModule);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-protected:
-    //#####  Protected Options  ###############################################
-
-    //! Number of module layers
-    int nmodules;
-
-    //####  Not Options  ######################################################
-
-public: // hack
-    //! stores the input and output values of the functions
-    TVec<Vec> values;
-
-    //! stores the input of the last module, and the target if there is one
-    Vec cost_layer_input;
-
-    //! stores the gradients
-    TVec<Vec> gradients;
-
-    //! stores the diagonal of Hessians
-    TVec<Vec> diag_hessians;
-
-protected:
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    static void declareOptions(OptionList& ol);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    void build_();
-
-    void buildOptions();
-    void buildLayers();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(StackedModulesModule);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From lamblin at mail.berlios.de  Wed Jan 24 07:33:33 2007
From: lamblin at mail.berlios.de (lamblin at BerliOS)
Date: Wed, 24 Jan 2007 07:33:33 +0100
Subject: [Plearn-commits] r6609 - in trunk: commands plearn_learners/online
	plearn_learners/online/DEPRECATED
Message-ID: <200701240633.l0O6XXoY012761@sheep.berlios.de>

Author: lamblin
Date: 2007-01-24 07:33:33 +0100 (Wed, 24 Jan 2007)
New Revision: 6609

Added:
   trunk/plearn_learners/online/DEPRECATED/NLLErrModule.cc
   trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h
Removed:
   trunk/plearn_learners/online/NLLErrModule.cc
   trunk/plearn_learners/online/NLLErrModule.h
Modified:
   trunk/commands/plearn_light_inc.h
   trunk/commands/plearn_noblas_inc.h
Log:
Deprecate NLLErrModule.
Please use NLLCostModule if you only want the NLL computation,
or SoftmaxModule and NLLCostModule (possibly combined in a
ProcessInputCostModule) if you want softmax + NLL.


Modified: trunk/commands/plearn_light_inc.h
===================================================================
--- trunk/commands/plearn_light_inc.h	2007-01-24 06:21:10 UTC (rev 6608)
+++ trunk/commands/plearn_light_inc.h	2007-01-24 06:33:33 UTC (rev 6609)
@@ -204,7 +204,6 @@
 #include <plearn_learners/online/ModulesLearner.h>
 #include <plearn_learners/online/ModuleStackModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
-#include <plearn_learners/online/NLLErrModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn_learners/online/RBMBinomialLayer.h>
 #include <plearn_learners/online/RBMClassificationModule.h>

Modified: trunk/commands/plearn_noblas_inc.h
===================================================================
--- trunk/commands/plearn_noblas_inc.h	2007-01-24 06:21:10 UTC (rev 6608)
+++ trunk/commands/plearn_noblas_inc.h	2007-01-24 06:33:33 UTC (rev 6609)
@@ -193,7 +193,6 @@
 #include <plearn_learners/online/ModulesLearner.h>
 #include <plearn_learners/online/ModuleStackModule.h>
 #include <plearn_learners/online/NLLCostModule.h>
-#include <plearn_learners/online/NLLErrModule.h>
 #include <plearn_learners/online/OnlineLearningModule.h>
 #include <plearn_learners/online/ProcessInputCostModule.h>
 #include <plearn_learners/online/RBMBinomialLayer.h>

Copied: trunk/plearn_learners/online/DEPRECATED/NLLErrModule.cc (from rev 6596, trunk/plearn_learners/online/NLLErrModule.cc)

Copied: trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h (from rev 6596, trunk/plearn_learners/online/NLLErrModule.h)
===================================================================
--- trunk/plearn_learners/online/NLLErrModule.h	2007-01-23 20:43:54 UTC (rev 6596)
+++ trunk/plearn_learners/online/DEPRECATED/NLLErrModule.h	2007-01-24 06:33:33 UTC (rev 6609)
@@ -0,0 +1,157 @@
+// -*- C++ -*-
+
+// NLLErrModule.h
+//
+// Copyright (C) 2005 Pascal Lamblin
+//
+// Redistribution and use in source and binary forms, with or without
+// modification, are permitted provided that the following conditions are met:
+//
+//  1. Redistributions of source code must retain the above copyright
+//     notice, this list of conditions and the following disclaimer.
+//
+//  2. Redistributions in binary form must reproduce the above copyright
+//     notice, this list of conditions and the following disclaimer in the
+//     documentation and/or other materials provided with the distribution.
+//
+//  3. The name of the authors may not be used to endorse or promote
+//     products derived from this software without specific prior written
+//     permission.
+//
+// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
+// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
+// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+//
+// This file is part of the PLearn library. For more information on the PLearn
+// library, go to the PLearn Web site at www.plearn.org
+
+/* *******************************************************
+   * $Id: NLLErrModule.h,v 1.1 2005/11/30 04:36:17 lamblinp Exp $
+   ******************************************************* */
+
+// Authors: Pascal Lamblin
+
+/*! \file NLLErrModule.h */
+
+
+#ifndef NLLErrModule_INC
+#define NLLErrModule_INC
+
+#include <plearn/base/Object.h>
+#include <plearn/math/pl_math.h>
+#include <plearn/math/TMat_maths.h>
+#include <plearn_learners/online/OnlineLearningModule.h>
+
+namespace PLearn {
+
+/**
+ * NLL (and derivatives thereof) between the target and input.
+ * This class computes the Negative Log-Likelihood of the input, given the
+ * desired 'target'. Also propagates gradient and diagonal of Hessian
+ * backwards.
+ *
+ * @deprecated: Use ../NLLCostModule if you only want the NLL,
+ *  or SoftmaxModule and NLLCostModule (possibly in a ProcessInputCostModule)
+ *  if you want softmax + NLL
+ */
+class NLLErrModule : public OnlineLearningModule
+{
+    typedef OnlineLearningModule inherited;
+
+public:
+    //#####  Public Build Options  ############################################
+
+
+public:
+    //#####  Public Member Functions  #########################################
+
+    //! Default constructor
+    NLLErrModule();
+
+    // Your other public member functions go here
+
+    virtual int getTarget( const Vec& input ) const;
+
+    virtual void fprop(const Vec& input, Vec& output) const;
+
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             const Vec& output_gradient);
+
+    virtual void bpropUpdate(const Vec& input, const Vec& output,
+                             Vec& input_gradient, const Vec& output_gradient);
+
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              const Vec& output_gradient,
+                              const Vec& output_diag_hessian);
+
+    virtual void bbpropUpdate(const Vec& input, const Vec& output,
+                              Vec& input_gradient,
+                              const Vec& output_gradient,
+                              Vec& input_diag_hessian,
+                              const Vec& output_diag_hessian);
+
+
+    virtual void forget();
+
+    //#####  PLearn::Object Protocol  #########################################
+
+    // Declares other standard object methods.
+    PLEARN_DECLARE_OBJECT(NLLErrModule);
+
+    // Simply calls inherited::build() then build_()
+    virtual void build();
+
+    //! Transforms a shallow copy into a deep copy
+    // (PLEASE IMPLEMENT IN .cc)
+    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
+
+protected:
+    //#####  Protected Options  ###############################################
+    int target_size;
+    //#####  Protected Member Functions  ######################################
+
+    //! Declares the class options.
+    static void declareOptions(OptionList& ol);
+
+private:
+    //#####  Private Member Functions  ########################################
+
+    //! This does the actual building.
+    void build_();
+
+private:
+    //#####  Private Data Members  ############################################
+
+    // The rest of the private stuff goes here
+
+    // Holds the result of the fprop's softmax
+    mutable Vec fp_sm;
+
+};
+
+// Declares a few other classes and functions related to this class
+DECLARE_OBJECT_PTR(NLLErrModule);
+
+} // end of namespace PLearn
+
+#endif
+
+
+/*
+  Local Variables:
+  mode:c++
+  c-basic-offset:4
+  c-file-style:"stroustrup"
+  c-file-offsets:((innamespace . 0)(inline-open . 0))
+  indent-tabs-mode:nil
+  fill-column:79
+  End:
+*/
+// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/NLLErrModule.cc
===================================================================
--- trunk/plearn_learners/online/NLLErrModule.cc	2007-01-24 06:21:10 UTC (rev 6608)
+++ trunk/plearn_learners/online/NLLErrModule.cc	2007-01-24 06:33:33 UTC (rev 6609)
@@ -1,345 +0,0 @@
-// -*- C++ -*-
-
-// NLLErrModule.cc
-//
-// Copyright (C) 2005 Pascal Lamblin
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************
-   * $Id: NLLErrModule.cc,v 1.5 2005/12/30 19:53:56 lamblinp Exp $
-   ******************************************************* */
-
-// Authors: Pascal Lamblin
-
-/*! \file NLLErrModule.cc */
-
-
-#include "NLLErrModule.h"
-
-namespace PLearn {
-using namespace std;
-
-PLEARN_IMPLEMENT_OBJECT(
-    NLLErrModule,
-    "NLLError Module",
-    "This class computes the Negative Log-Likelihood of the input, given the\n"
-    "desired 'target'. Also propagates gradient and diagonal of Hessian\n"
-    "backwards.\n"
-    "If output_size = 2, the second output is the classification error.\n"
-    );
-
-NLLErrModule::NLLErrModule():
-    target_size(1)
-    /* ### Initialize all fields to their default value */
-{
-    output_size = 1;
-}
-
-
-// retrieve target from input vector
-int NLLErrModule::getTarget(const Vec& input) const
-{
-    int t_size = input.size() - input_size;
-    int target = -1;
-
-    // size check
-    if( t_size == 1 )
-    {
-        target = (int) input[ input_size ];
-    }
-    else if( t_size == input_size )
-    {
-        /*
-        PLWARNING("NLLErrModule::getTarget: You're giving a target the same\n"
-                  "size as the input, instead of an integer. Checking if\n"
-                  "this is a one-hot vector from this integer.\n");
-         */
-
-        Vec the_target = input.subVec( input_size, t_size );
-        // get position of '1'
-        target = argmax( the_target );
-
-#ifdef BOUNDCHECK
-        // check if vector matches with a one-hot one
-        PLASSERT( is_equal( the_target[target], 1. ) ) ;
-        for( int i=0 ; i<input_size ; i++ )
-            PLASSERT( is_equal( the_target[i], 0. ) || i == target );
-#endif
-    }
-    else
-    {
-        PLERROR("NLLErrModule::getTarget: target.size() is %i,\n"
-                " but should be 1. 'target' should contain an integer.\n",
-                t_size);
-    }
-
-    if( target < 0 || target >= input_size )
-        PLERROR("NLLErrModule::getTarget: target should be between 0 and"
-                "input_size (%i).\n", input_size);
-
-    return target;
-}
-
-// output = error = log(softmax(input))[target]
-void NLLErrModule::fprop(const Vec& input, Vec& output) const
-{
-    int target = getTarget( input );
-    // size check is done in getTarget()
-
-    Vec input_ = input.subVec( 0, input_size );
-    output.resize( output_size );
-
-    fp_sm = softmax( input_ );
-    output[0] = - pl_log( fp_sm[target] );
-
-
-    if( output_size > 1 )
-        output[1] = ( argmax( input_ ) == target ) ? 0 : 1;
-}
-
-// Don't modify class
-void NLLErrModule::bpropUpdate(const Vec& input, const Vec& output,
-                               const Vec& output_gradient)
-{
-    int out_size = output.size();
-    int og_size = output_gradient.size();
-
-    // for size check
-    getTarget( input );
-
-    // size check
-    if( out_size != output_size )
-    {
-        PLWARNING("NLLErrModule::bpropUpdate: output.size()' should be\n"
-                  " equal to 'output_size' (%i != %i)\n",
-                  out_size, output_size);
-    }
-    if( og_size != output_size )
-    {
-        PLWARNING("NLLErrModule::bpropUpdate: 'output_gradient.size()'\n"
-                  " should be equal to 'output_size' (%i != %i)\n",
-                  og_size, output_size);
-    }
-}
-
-// We don't care about output_gradient, we consider we're the last variable.
-// So we compute the gradient of the error of this variable.
-void NLLErrModule::bpropUpdate(const Vec& input, const Vec& output,
-                               Vec& input_gradient,
-                               const Vec& output_gradient)
-{
-    int out_size = output.size();
-    int og_size = output_gradient.size();
-    int target = getTarget( input );
-    bool is_final_cost = false; // if yes, output_gradient is 1
-
-    // size check
-    if( out_size != output_size )
-    {
-        PLERROR("NLLErrModule::bpropUpdate: output.size()' should be\n"
-                " equal to 'output_size' (%i != %i)\n",
-                out_size, output_size);
-    }
-    if( og_size == 0 )
-    {
-        /*
-        PLWARNING("NLLErrModule::bpropUpdate: you are not providing"
-                  "output_gradient.\n"
-                  "Assuming this is the final cost, and output_gradient=1.\n");
-         */
-        is_final_cost = true;
-    }
-    else if( og_size != output_size )
-    {
-        PLERROR("NLLErrModule::bpropUpdate: 'output_gradient.size()'\n"
-                " should be equal to 'output_size' (%i != %i)\n",
-                og_size, output_size);
-    }
-
-    // input_gradient[i] = output_gradient*( softmax(input)[i] ) if i!=target
-    // input_gradient[target] = output_gradient*( softmax(input)[target] )
-    input_gradient.resize( input_size );
-    input_gradient << fp_sm;
-
-    input_gradient[target] -= 1;
-    if( !is_final_cost )
-        input_gradient *= output_gradient[0];
-
-}
-
-// Does nothing (just checks and warns)
-void NLLErrModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                const Vec& output_gradient,
-                                const Vec& output_diag_hessian)
-{
-    int odh_size = output_diag_hessian.size();
-    if( odh_size != output_size )
-    {
-        PLWARNING("NLLErrModule::bbpropUpdate:"
-                  " 'output_diag_hessian.size()'\n"
-                  " should be equal to 'output_size' (%i != %i)\n",
-                  odh_size, output_size);
-    }
-
-    bpropUpdate( input, output, output_gradient );
-}
-
-// Propagates back output_gradient and output_diag_hessian
-void NLLErrModule::bbpropUpdate(const Vec& input, const Vec& output,
-                                Vec& input_gradient,
-                                const Vec& output_gradient,
-                                Vec& input_diag_hessian,
-                                const Vec& output_diag_hessian)
-{
-    int odh_size = output_diag_hessian.size();
-    int target = getTarget( input );
-    bool is_final_cost = false; // if yes, output_diag_hessian is 0
-
-    // size check
-    // others size checks will be done in bpropUpdate()
-    if( odh_size == 0 )
-    {
-        PLWARNING("NLLErrModule::bbpropUpdate: you are not providing"
-                  " output_diag_hessian.\n"
-                  "Assuming this is the final cost,"
-                  " and output_diag_hessian=0.\n");
-        is_final_cost = true;
-    }
-    else if( odh_size != output_size )
-    {
-        PLERROR("NLLErrModule::bbpropUpdate:"
-                " 'output_diag_hessian.size()'\n"
-                " should be equal to 'output_size' (%i != %i)\n",
-                odh_size, output_size);
-    }
-
-    bpropUpdate( input, output, input_gradient, output_gradient );
-
-    Vec input_ = input.subVec( 0, input_size );
-    input_diag_hessian.resize( input_size );
-    Vec softmax_in = softmax( input_ );
-
-    // computation of term dC/dy d?y/dx?,
-    // skipped if estimate_simpler_diag_hessian, unless it is final cost
-    if( estimate_simpler_diag_hessian && !is_final_cost )
-    {
-        input_diag_hessian.clear();
-    }
-    else
-    {
-        for( int i=0 ; i<input_size ; i++ )
-        {
-            real sm_i = softmax_in[i];
-            input_diag_hessian[i] = sm_i*( 1-sm_i);
-        }
-
-        if( !is_final_cost )
-            input_diag_hessian *= output_gradient[0];
-
-    }
-
-    // computation of term d?C/dy? (dy/dx)?,
-    // skipped if it is final cost, because then d?C/dy? == d?C/dC? == 0
-    if( !is_final_cost )
-    {
-        Vec fprime = softmax_in;
-        fprime[target] -= 1;
-        fprime *= fprime;
-
-        input_diag_hessian += output_diag_hessian[0] * fprime;
-    }
-
-}
-
-
-
-
-//
-void NLLErrModule::forget()
-{
-}
-
-
-// ### Nothing to add here, simply calls build_
-void NLLErrModule::build()
-{
-    inherited::build();
-    build_();
-}
-
-void NLLErrModule::makeDeepCopyFromShallowCopy(CopiesMap& copies)
-{
-    inherited::makeDeepCopyFromShallowCopy(copies);
-
-}
-
-void NLLErrModule::declareOptions(OptionList& ol)
-{
-    inherited::declareOptions(ol);
-}
-
-void NLLErrModule::build_()
-{
-    if( input_size < 0 )
-    {
-        PLWARNING("NLLErrModule::build_: 'input_size' is < 0.\n"
-                  "You should set it to a positive integer.\n"
-                  "Defaulting to '1' (like a sigmoid function ?)\n");
-        input_size = 1;
-    }
-    if( output_size != 1 && output_size != 2 )
-    {
-        PLWARNING("NLLErrModule::build_: 'output_size' (%i) should be 1.\n"
-                  "Setting 'output_size' to 1.\n", output_size);
-        output_size = 1;
-    }
-
-    target_size = 1;
-
-    fp_sm.resize(input_size);
-}
-
-
-
-
-} // end of namespace PLearn
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :

Deleted: trunk/plearn_learners/online/NLLErrModule.h
===================================================================
--- trunk/plearn_learners/online/NLLErrModule.h	2007-01-24 06:21:10 UTC (rev 6608)
+++ trunk/plearn_learners/online/NLLErrModule.h	2007-01-24 06:33:33 UTC (rev 6609)
@@ -1,154 +0,0 @@
-// -*- C++ -*-
-
-// NLLErrModule.h
-//
-// Copyright (C) 2005 Pascal Lamblin
-//
-// Redistribution and use in source and binary forms, with or without
-// modification, are permitted provided that the following conditions are met:
-//
-//  1. Redistributions of source code must retain the above copyright
-//     notice, this list of conditions and the following disclaimer.
-//
-//  2. Redistributions in binary form must reproduce the above copyright
-//     notice, this list of conditions and the following disclaimer in the
-//     documentation and/or other materials provided with the distribution.
-//
-//  3. The name of the authors may not be used to endorse or promote
-//     products derived from this software without specific prior written
-//     permission.
-//
-// THIS SOFTWARE IS PROVIDED BY THE AUTHORS ``AS IS'' AND ANY EXPRESS OR
-// IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
-// OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN
-// NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
-// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
-// TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
-// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
-// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
-// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
-// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-//
-// This file is part of the PLearn library. For more information on the PLearn
-// library, go to the PLearn Web site at www.plearn.org
-
-/* *******************************************************
-   * $Id: NLLErrModule.h,v 1.1 2005/11/30 04:36:17 lamblinp Exp $
-   ******************************************************* */
-
-// Authors: Pascal Lamblin
-
-/*! \file NLLErrModule.h */
-
-
-#ifndef NLLErrModule_INC
-#define NLLErrModule_INC
-
-#include <plearn/base/Object.h>
-#include <plearn/math/pl_math.h>
-#include <plearn/math/TMat_maths.h>
-#include "OnlineLearningModule.h"
-
-namespace PLearn {
-
-/**
- * NLL (and derivatives thereof) between the target and input.
- * This class computes the Negative Log-Likelihood of the input, given the
- * desired 'target'. Also propagates gradient and diagonal of Hessian
- * backwards.
- *
- */
-class NLLErrModule : public OnlineLearningModule
-{
-    typedef OnlineLearningModule inherited;
-
-public:
-    //#####  Public Build Options  ############################################
-
-
-public:
-    //#####  Public Member Functions  #########################################
-
-    //! Default constructor
-    NLLErrModule();
-
-    // Your other public member functions go here
-
-    virtual int getTarget( const Vec& input ) const;
-
-    virtual void fprop(const Vec& input, Vec& output) const;
-
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             const Vec& output_gradient);
-
-    virtual void bpropUpdate(const Vec& input, const Vec& output,
-                             Vec& input_gradient, const Vec& output_gradient);
-
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              const Vec& output_gradient,
-                              const Vec& output_diag_hessian);
-
-    virtual void bbpropUpdate(const Vec& input, const Vec& output,
-                              Vec& input_gradient,
-                              const Vec& output_gradient,
-                              Vec& input_diag_hessian,
-                              const Vec& output_diag_hessian);
-
-
-    virtual void forget();
-
-    //#####  PLearn::Object Protocol  #########################################
-
-    // Declares other standard object methods.
-    PLEARN_DECLARE_OBJECT(NLLErrModule);
-
-    // Simply calls inherited::build() then build_()
-    virtual void build();
-
-    //! Transforms a shallow copy into a deep copy
-    // (PLEASE IMPLEMENT IN .cc)
-    virtual void makeDeepCopyFromShallowCopy(CopiesMap& copies);
-
-protected:
-    //#####  Protected Options  ###############################################
-    int target_size;
-    //#####  Protected Member Functions  ######################################
-
-    //! Declares the class options.
-    static void declareOptions(OptionList& ol);
-
-private:
-    //#####  Private Member Functions  ########################################
-
-    //! This does the actual building.
-    void build_();
-
-private:
-    //#####  Private Data Members  ############################################
-
-    // The rest of the private stuff goes here
-
-    // Holds the result of the fprop's softmax
-    mutable Vec fp_sm;
-
-};
-
-// Declares a few other classes and functions related to this class
-DECLARE_OBJECT_PTR(NLLErrModule);
-
-} // end of namespace PLearn
-
-#endif
-
-
-/*
-  Local Variables:
-  mode:c++
-  c-basic-offset:4
-  c-file-style:"stroustrup"
-  c-file-offsets:((innamespace . 0)(inline-open . 0))
-  indent-tabs-mode:nil
-  fill-column:79
-  End:
-*/
-// vim: filetype=cpp:expandtab:shiftwidth=4:tabstop=8:softtabstop=4:encoding=utf-8:textwidth=79 :



From chrish at mail.berlios.de  Wed Jan 24 20:21:42 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 24 Jan 2007 20:21:42 +0100
Subject: [Plearn-commits] r6610 - trunk/plearn_learners/testers
Message-ID: <200701241921.l0OJLgTw023820@sheep.berlios.de>

Author: chrish
Date: 2007-01-24 20:21:41 +0100 (Wed, 24 Jan 2007)
New Revision: 6610

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Cleanup in PTester:perform in preparation for bug fixes:
* Use const when declaring an int or bool variable that is constant.
* Indentation and whitespace cleanup.
Change should have no impact on generated object code.


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-01-24 06:33:33 UTC (rev 6609)
+++ trunk/plearn_learners/testers/PTester.cc	2007-01-24 19:21:41 UTC (rev 6610)
@@ -407,44 +407,43 @@
 /////////////
 Vec PTester::perform(bool call_forget)
 {
-    if(!learner)
+    if (!learner)
         PLERROR("No learner specified for PTester.");
-    if(!splitter)
+    if (!splitter)
         PLERROR("No splitter specified for PTester");
 
-    int nstats;
-    nstats = statnames_processed.length();
+    const int nstats = statnames_processed.length();
     Vec global_result(nstats);
 
     {
-        if(expdir!="")
+        if (expdir != "")
         {
-            if(pathexists(expdir) && enforce_clean_expdir)
+            if (pathexists(expdir) && enforce_clean_expdir)
                 PLERROR("Directory (or file) %s already exists.\n"
                         "First move it out of the way.", expdir.c_str());
-            if(!force_mkdir(expdir))
+            if (!force_mkdir(expdir))
                 PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
             expdir = expdir.absolute() / "";
 
             // Save this tester description in the expdir
-            if(save_initial_tester)
-                PLearn::save( expdir / "tester.psave", *this);
+            if (save_initial_tester)
+                PLearn::save(expdir / "tester.psave", *this);
         }
 
         splitter->setDataSet(dataset);
 
-        int nsplits = splitter->nsplits();
-        if(nsplits>1)
+        const int nsplits = splitter->nsplits();
+        if (nsplits > 1)
             call_forget = true;
 
         TVec<string> testcostnames = learner->getTestCostNames();
         TVec<string> traincostnames = learner->getTrainCostNames();
 
-        int nsets = splitter->nSetsPerSplit();
+        const int nsets = splitter->nSetsPerSplit();
 
         // Stats collectors for individual sets of a split:
         TVec< PP<VecStatsCollector> > stcol(nsets);
-        for(int setnum=0; setnum<nsets; setnum++)
+        for (int setnum = 0; setnum < nsets; setnum++)
         {
             if (template_stats_collector)
             {
@@ -454,7 +453,7 @@
             else
                 stcol[setnum] = new VecStatsCollector();
 
-            if(setnum==0)
+            if (setnum == 0)
                 stcol[setnum]->setFieldNames(traincostnames);
             else
                 stcol[setnum]->setFieldNames(testcostnames);
@@ -480,9 +479,11 @@
 
         // Stat specs
         TVec<StatSpec> statspecs(nstats);
-        for(int k=0; k<nstats; k++) {
+        for(int k = 0; k < nstats; k++)
+        {
             statspecs[k].init(statnames_processed[k]);
         }
+        
         // Hack to accumulate statistics over splits. We store in 'acc' the sets
         // which need to accumulate statistics.
         TVec<int> acc;
@@ -497,69 +498,73 @@
             else if (acc.find(statspecs[k].setnum) != -1)
                 PLERROR("In PTester::perform - You can't have stats with and without 'ACC' for set %d", statspecs[k].setnum);
 
-        // int traincostsize = traincostnames.size();
-        // int testcostsize = testcostnames.size();
-
-        VMat global_stats_vm;    // the vmat in which to save global result stats specified in statnames
-        VMat split_stats_vm;   // the vmat in which to save per split result stats
-        if(expdir!="" && report_stats)
+        // The vmat in which to save global result stats specified in statnames
+        VMat global_stats_vm;
+        // The vmat in which to save per split result stats
+        VMat split_stats_vm;
+        
+        if (expdir != "" && report_stats)
         {
-            saveStringInFile(expdir/"train_cost_names.txt", join(traincostnames,"\n")+"\n");
-            saveStringInFile(expdir/"test_cost_names.txt", join(testcostnames,"\n")+"\n");
+            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
+            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
 
             global_stats_vm = new FileVMatrix(expdir/"global_stats.pmat", 1, nstats);
-            for(int k=0; k<nstats; k++)
-                global_stats_vm->declareField(k,statspecs[k].statName());
+            for (int k = 0; k < nstats; k++)
+                global_stats_vm->declareField(k, statspecs[k].statName());
             global_stats_vm->saveFieldInfos();
 
-            split_stats_vm = new FileVMatrix(expdir/"split_stats.pmat", 0, 1+nstats);
-            split_stats_vm->declareField(0,"splitnum");
-            for(int k=0; k<nstats; k++)
-                split_stats_vm->declareField(k+1,statspecs[k].setname + "." + statspecs[k].intstatname);
+            split_stats_vm = new FileVMatrix(expdir / "split_stats.pmat", 0, 1+nstats);
+            split_stats_vm->declareField(0, "splitnum");
+            for (int k = 0; k < nstats; k++)
+                split_stats_vm->declareField(k+1, statspecs[k].setname + "." + statspecs[k].intstatname);
             split_stats_vm->saveFieldInfos();
         }
 
-        for(int splitnum=0; splitnum<nsplits; splitnum++)
+        for (int splitnum = 0; splitnum < nsplits; splitnum++)
         {
             PPath splitdir;
             bool is_splitdir = false;
-            if(!expdir.isEmpty()) {
-                splitdir = expdir / ("Split"+tostring(splitnum));
+            if(!expdir.isEmpty())
+            {
+                splitdir = expdir / ("Split" + tostring(splitnum));
                 is_splitdir = true;
             }
 
             TVec<VMat> dsets = splitter->getSplit(splitnum);
             VMat trainset = dsets[0];
-            if(is_splitdir && save_data_sets)
-                PLearn::save(splitdir/"training_set.psave",trainset);
+            if (is_splitdir && save_data_sets)
+                PLearn::save(splitdir / "training_set.psave", trainset);
 
-            if(should_train && provide_learner_expdir)
+            if (should_train && provide_learner_expdir)
             {
-                if(is_splitdir)
-                    learner->setExperimentDirectory( splitdir/"LearnerExpdir/" );
+                if (is_splitdir)
+                    learner->setExperimentDirectory(splitdir / "LearnerExpdir/");
                 else
                     learner->setExperimentDirectory("");
             }
 
             learner->setTrainingSet(trainset, call_forget && should_train);
-            if(dsets.size()>1)
+            if (dsets.size() > 1)
                 learner->setValidationSet(dsets[1]);
 
-            int outputsize = learner->outputsize();
+            const int outputsize = learner->outputsize();
 
-
             if (should_train)
             {
-                if(is_splitdir && save_initial_learners)
-                    PLearn::save(splitdir/"initial_learner.psave",learner);
+                if (is_splitdir && save_initial_learners)
+                    PLearn::save(splitdir / "initial_learner.psave", learner);
 
                 train_stats->forget();
                 learner->train();
                 train_stats->finalize();
-                if(is_splitdir && save_stat_collectors)
-                    PLearn::save(splitdir/"train_stats.psave",train_stats);
-                if(is_splitdir && save_learners)
-                    PLearn::save(splitdir/"final_learner.psave",learner);
+
+                if (is_splitdir)
+                {
+                    if (save_stat_collectors)
+                        PLearn::save(splitdir / "train_stats.psave", train_stats);
+                    if (save_learners)
+                        PLearn::save(splitdir / "final_learner.psave", learner);
+                }
             }
             else
                 learner->build();
@@ -569,46 +574,52 @@
             TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
 
             // Perform the test if required
-            if (should_test) {
-                for(int setnum=1; setnum<dsets.length(); setnum++)
+            if (should_test)
+            {
+                for (int setnum = 1; setnum < dsets.length(); setnum++)
                 {
                     VMat testset = dsets[setnum];
                     PP<VecStatsCollector> test_stats = stcol[setnum];
-                    string setname = "test"+tostring(setnum);
-                    if(is_splitdir && save_data_sets)
-                        PLearn::save(splitdir/(setname+"_set.psave"),testset);
+                    const string setname = "test" + tostring(setnum);
+                    if (is_splitdir && save_data_sets)
+                        PLearn::save(splitdir / (setname + "_set.psave"), testset);
                     VMat test_outputs;
                     VMat test_costs;
                     VMat test_confidence;
                     if (is_splitdir)
                         force_mkdir(splitdir); // TODO Why is this done so late?
 
-                    if(is_splitdir && save_test_outputs)
-                        test_outputs = new FileVMatrix(splitdir/(setname+"_outputs.pmat"),0,learner->getOutputNames());
-                    //test_outputs = new FileVMatrix(splitdir/(setname+"_outputs.pmat"),0,outputsize);
-                    else if(!perf_evaluators.empty()) // we don't want to save test outputs to disk, but we need them for pef_evaluators
-                    { // So let's store them in a MemoryVMatrix
+                    if (is_splitdir && save_test_outputs)
+                        test_outputs = new FileVMatrix(splitdir / (setname + "_outputs.pmat"),
+                                                       0, learner->getOutputNames());
+                    else if (!perf_evaluators.empty())
+                    {
+                        // We don't want to save test outputs to disk, but we
+                        // need them for pef_evaluators. So let's store them in
+                        // a MemoryVMatrix
                         Mat data(testset.length(),outputsize);
                         data.resize(0,outputsize);
                         test_outputs = new MemoryVMatrix(data);
                         test_outputs->declareFieldNames(learner->getOutputNames());
                     }
 
-                    if(is_splitdir && save_test_costs)
-                        test_costs = new FileVMatrix(splitdir/(setname+"_costs.pmat"),0,learner->getTestCostNames());
-                    //test_costs = new FileVMatrix(splitdir/(setname+"_costs.pmat"),0,testcostsize);
-                    if(is_splitdir && save_test_confidence)
-                        test_confidence = new FileVMatrix(splitdir/(setname+"_confidence.pmat"),
-                                                          0,2*outputsize);
+                    if (is_splitdir)
+                    {
+                        if (save_test_costs)
+                            test_costs = new FileVMatrix(splitdir / (setname + "_costs.pmat"),
+                                                         0, learner->getTestCostNames());
+                        if (save_test_confidence)
+                            test_confidence = new FileVMatrix(splitdir / (setname + "_confidence.pmat"),
+                                                              0, 2 * outputsize);
+                    }
 
-                    bool reset_stats = (acc.find(setnum) == -1);
-
-                    //perr << "reset_stats= " << reset_stats << endl;
-
+                    const bool reset_stats = (acc.find(setnum) == -1);
                     if (reset_stats)
                         test_stats->forget();
-                    if (testset->length()==0)
-                        PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",setname.c_str());
+                    
+                    if (testset->length() == 0)
+                        PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",
+                                  setname.c_str());
 
                     // Before each test set, reset the internal state of the learner
                     learner->resetInternalState();
@@ -616,22 +627,22 @@
                     learner->test(testset, test_stats, test_outputs, test_costs);
                     if (reset_stats)
                         test_stats->finalize();
-                    if(is_splitdir && save_stat_collectors)
-                        PLearn::save(splitdir/(setname+"_stats.psave"),test_stats);
+                    if (is_splitdir && save_stat_collectors)
+                        PLearn::save(splitdir / (setname + "_stats.psave"), test_stats);
 
                     perf_evaluators_t::iterator it = perf_evaluators.begin();
-                    perf_evaluators_t::iterator itend = perf_evaluators.end();
-                    while(it!=itend)
+                    const perf_evaluators_t::iterator itend = perf_evaluators.end();
+                    while(it != itend)
                     {
                         PPath perf_eval_dir;
-                        if(is_splitdir)
-                            perf_eval_dir = splitdir/setname/("perfeval_"+it->first);
+                        if (is_splitdir)
+                            perf_eval_dir = splitdir / setname / ("perfeval_" + it->first);
                         Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
                         TVec<string> perf_costnames = it->second->getCostNames();
-                        if(perf_costvals.length()!=perf_costnames.length())
+                        if (perf_costvals.length()!=perf_costnames.length())
                             PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
                         map<string, real>& costmap = perf_eval_costs[setnum][it->first];
-                        for(int costi = 0; costi<perf_costnames.length(); costi++)
+                        for (int costi = 0; costi < perf_costnames.length(); costi++)
                             costmap[perf_costnames[costi]] = perf_costvals[costi];
                         ++it;
                     }
@@ -639,10 +650,10 @@
                 }
             }
 
-            Vec splitres(1+nstats);
+            Vec splitres(1 + nstats);
             splitres[0] = splitnum;
 
-            for(int k=0; k<nstats; k++)
+            for (int k = 0; k < nstats; k++)
             {
                 // If we ask for a test-set that's beyond what's currently
                 // available, OR we are asking for test-statistics in
@@ -659,14 +670,17 @@
                     {
                         string left, right;
                         split_on_first(sp.intstatname, ".",left,right);
-                        if(right!="" && perf_evaluators.find(left)!=perf_evaluators.end())
-                        { // looks like a cost from a performance evaluator
+                        if (right != "" && perf_evaluators.find(left) != perf_evaluators.end())
+                        {
+                            // looks like a cost from a performance evaluator
                             map<string, real>& costmap = perf_eval_costs[sp.setnum][left];
-                            if(costmap.find(right)==costmap.end())
-                                PLERROR("No cost named %s appears to be returned by evaluator %s",right.c_str(),left.c_str());
+                            if (costmap.find(right) == costmap.end())
+                                PLERROR("No cost named %s appears to be returned by evaluator %s",
+                                        right.c_str(), left.c_str());
                             splitres[k+1] = costmap[right];
                         }
-                        else // must be a cost from a stats collector
+                        else
+                            // must be a cost from a stats collector
                             splitres[k+1] = stcol[sp.setnum]->getStat(sp.intstatname);
                     }
                     else
@@ -674,37 +688,39 @@
                 }
             }
 
-            if(split_stats_vm)
+            if (split_stats_vm)
             {
                 split_stats_vm->appendRow(splitres);
                 split_stats_vm->flush();
             }
 
-            global_statscol->update(splitres.subVec(1,nstats));
+            global_statscol->update(splitres.subVec(1, nstats));
         }
 
 
         global_statscol->finalize();
-        for(int k=0; k<nstats; k++) {
+        for (int k = 0; k < nstats; k++)
+        {
             if (acc.find(statspecs[k].setnum) == -1)
                 global_result[k] = global_statscol->getStats(k).getStat(statspecs[k].extstat);
-            else {
-                int j = statspecs[k].setnum;
+            else
+            {
+                const int j = statspecs[k].setnum;
                 stcol[j]->finalize();
                 global_result[k] = stcol[j]->getStat(statspecs[k].intstatname);
             }
         }
 
-        if(global_stats_vm)
+        if (global_stats_vm)
             global_stats_vm->appendRow(global_result);
-
     }
 
 #if USING_MPI
-    if (PLMPI::rank==0)
+    if (PLMPI::rank == 0)
 #endif
     // Perform the final commands provided in final_commands.
-    for (int i = 0; i < final_commands.length(); i++) {
+    for (int i = 0; i < final_commands.length(); i++)
+    {
         system(final_commands[i].c_str());
     }
 



From ducharme at mail.berlios.de  Wed Jan 24 21:23:25 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 24 Jan 2007 21:23:25 +0100
Subject: [Plearn-commits] r6611 - trunk/python_modules/plearn/utilities
Message-ID: <200701242023.l0OKNPqf028069@sheep.berlios.de>

Author: ducharme
Date: 2007-01-24 21:23:24 +0100 (Wed, 24 Jan 2007)
New Revision: 6611

Added:
   trunk/python_modules/plearn/utilities/thread_utils.py
Log:
Permet d'envoyer une exception a une thread qui roule.


Added: trunk/python_modules/plearn/utilities/thread_utils.py
===================================================================
--- trunk/python_modules/plearn/utilities/thread_utils.py	2007-01-24 19:21:41 UTC (rev 6610)
+++ trunk/python_modules/plearn/utilities/thread_utils.py	2007-01-24 20:23:24 UTC (rev 6611)
@@ -0,0 +1,18 @@
+import inspect
+import ctypes
+ 
+ 
+## Allows one thread to raise exceptions in the context of another thread
+def _async_raise(tid, exctype):
+    """Raises the exception, performs cleanup if needed"""
+
+    if not inspect.isclass(exctype):
+        raise TypeError("Only types can be raised (not instances)")
+    res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))
+    if res == 0:
+        raise ValueError("Invalid thread id")
+    elif res != 1:
+        # """if it returns a number greater than one, you're in trouble, 
+        # and you should call it again with exc=NULL to revert the effect"""
+        ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, 0)
+        raise SystemError("PyThreadState_SetAsyncExc failed")



From ducharme at mail.berlios.de  Wed Jan 24 21:25:33 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Wed, 24 Jan 2007 21:25:33 +0100
Subject: [Plearn-commits] r6612 - trunk/python_modules/plearn/io
Message-ID: <200701242025.l0OKPXqK028211@sheep.berlios.de>

Author: ducharme
Date: 2007-01-24 21:25:32 +0100 (Wed, 24 Jan 2007)
New Revision: 6612

Modified:
   trunk/python_modules/plearn/io/server.py
Log:
Ajout d'une methode "kill" pour tuer le server.


Modified: trunk/python_modules/plearn/io/server.py
===================================================================
--- trunk/python_modules/plearn/io/server.py	2007-01-24 20:23:24 UTC (rev 6611)
+++ trunk/python_modules/plearn/io/server.py	2007-01-24 20:25:32 UTC (rev 6612)
@@ -41,15 +41,23 @@
 
 def launch_plearn_server(command = 'plearn server', logger=None):
     if logger: logger.info('LAUNCHING PLEARN SERVER: command = '+command)                
-    to_server, from_server = os.popen2(command, 'b')
-    return RemotePLearnServer(from_server, to_server, logger)
+    ## If available, we use module "subprocess" instead of os.popen2 so that
+    ## we can have information on the child pid
+    try:
+        from subprocess import Popen, PIPE
+        p = Popen([command], shell=True, stdin=PIPE, stdout=PIPE, close_fds=True)
+        (to_server, from_server, child_pid) = (p.stdin, p.stdout, p.pid)
+    except:
+        to_server, from_server = os.popen2(command, 'b')
+        child_pid = -1
+    return RemotePLearnServer(from_server, to_server, pid=child_pid, logger=logger)
 
 def connect_to_plearn_server(hostname, port, logger=None):
     if logger: logger.info('CONNECTING TO PLEARN SERVER ON HOST '+hostname+', PORT '+str(port))                
     s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
     s.connect((hostname, port))
     io = s.makefile()
-    return RemotePLearnServer(io, io, logger)    
+    return RemotePLearnServer(io, io, logger=logger)
         
 class RemotePLearnServer:
 
@@ -73,13 +81,14 @@
         helpMethod(classname, methodname)
         """        
 
-    def __init__(self, from_server, to_server, logger=None):
+    def __init__(self, from_server, to_server, pid=-1, logger=None):
         """from_server and to_server are expected to be two file-like objects
         (supporting read, write, flush).
-        If you wish to log debugging info, pass ad logger an instance of a logging.Logger
+        If you wish to log debugging info, pass at logger an instance of a logging.Logger
         as returned for ex. by logging.getLogger()
         """
         self.io = plearn.io.serialize.PLearnIO(from_server, to_server)
+        self.pid = pid
         self.log = logger
         self.reserved_ids = []
         self.nextid = 1
@@ -99,7 +108,7 @@
 
         self.callFunction("binary")
         self.callFunction("implicit_storage",True)        
-        
+
     def set_return_lists(self, ret_lists=True):
         """If this is called with True, PLearn sequences will be
         returned as lists (rather than as arrays, which is the default
@@ -338,7 +347,13 @@
                 
     def __del__(self):
         self.close()
-        
+
+    def kill(self):
+        """Kill the process with the TERM signal."""
+        if self.pid == -1:
+            raise RuntimeError, 'The "kill" method is only available with Python 2.4 and above.'
+        os.kill(self.pid, 15)
+
 class RemotePObject:
     
     def __init__(self, serv, objid):



From chrish at mail.berlios.de  Wed Jan 24 22:16:48 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Wed, 24 Jan 2007 22:16:48 +0100
Subject: [Plearn-commits] r6613 - trunk/plearn_learners/testers
Message-ID: <200701242116.l0OLGmwS032091@sheep.berlios.de>

Author: chrish
Date: 2007-01-24 22:16:48 +0100 (Wed, 24 Jan 2007)
New Revision: 6613

Modified:
   trunk/plearn_learners/testers/PTester.cc
Log:
Remove spurious {} level around almost all of PTester::perform() method.

Make should_train = False case work correctly:
* When not training, don't call setTrainingSet, etc. on model.
* Retrieving outputsize of model needs to be done after build!


Modified: trunk/plearn_learners/testers/PTester.cc
===================================================================
--- trunk/plearn_learners/testers/PTester.cc	2007-01-24 20:25:32 UTC (rev 6612)
+++ trunk/plearn_learners/testers/PTester.cc	2007-01-24 21:16:48 UTC (rev 6613)
@@ -415,127 +415,130 @@
     const int nstats = statnames_processed.length();
     Vec global_result(nstats);
 
+    if (expdir != "")
     {
-        if (expdir != "")
-        {
-            if (pathexists(expdir) && enforce_clean_expdir)
-                PLERROR("Directory (or file) %s already exists.\n"
-                        "First move it out of the way.", expdir.c_str());
-            if (!force_mkdir(expdir))
-                PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
-            expdir = expdir.absolute() / "";
+        if (pathexists(expdir) && enforce_clean_expdir)
+            PLERROR("Directory (or file) %s already exists.\n"
+                    "First move it out of the way.", expdir.c_str());
+        if (!force_mkdir(expdir))
+            PLERROR("In PTester Could not create experiment directory %s",expdir.c_str());
+        expdir = expdir.absolute() / "";
 
-            // Save this tester description in the expdir
-            if (save_initial_tester)
-                PLearn::save(expdir / "tester.psave", *this);
-        }
+        // Save this tester description in the expdir
+        if (save_initial_tester)
+            PLearn::save(expdir / "tester.psave", *this);
+    }
 
-        splitter->setDataSet(dataset);
+    splitter->setDataSet(dataset);
 
-        const int nsplits = splitter->nsplits();
-        if (nsplits > 1)
-            call_forget = true;
+    const int nsplits = splitter->nsplits();
+    if (nsplits > 1)
+        call_forget = true;
 
-        TVec<string> testcostnames = learner->getTestCostNames();
-        TVec<string> traincostnames = learner->getTrainCostNames();
+    TVec<string> testcostnames = learner->getTestCostNames();
+    TVec<string> traincostnames = learner->getTrainCostNames();
 
-        const int nsets = splitter->nSetsPerSplit();
+    const int nsets = splitter->nSetsPerSplit();
 
-        // Stats collectors for individual sets of a split:
-        TVec< PP<VecStatsCollector> > stcol(nsets);
-        for (int setnum = 0; setnum < nsets; setnum++)
+    // Stats collectors for individual sets of a split:
+    TVec< PP<VecStatsCollector> > stcol(nsets);
+    for (int setnum = 0; setnum < nsets; setnum++)
+    {
+        if (template_stats_collector)
         {
-            if (template_stats_collector)
-            {
-                CopiesMap copies;
-                stcol[setnum] = template_stats_collector->deepCopy(copies);
-            }
-            else
-                stcol[setnum] = new VecStatsCollector();
+            CopiesMap copies;
+            stcol[setnum] = template_stats_collector->deepCopy(copies);
+        }
+        else
+            stcol[setnum] = new VecStatsCollector();
 
-            if (setnum == 0)
-                stcol[setnum]->setFieldNames(traincostnames);
-            else
-                stcol[setnum]->setFieldNames(testcostnames);
+        if (setnum == 0)
+            stcol[setnum]->setFieldNames(traincostnames);
+        else
+            stcol[setnum]->setFieldNames(testcostnames);
 
-            stcol[setnum]->build();
-            stcol[setnum]->forget();
-        }
+        stcol[setnum]->build();
+        stcol[setnum]->forget();
+    }
 
-        PP<VecStatsCollector> train_stats = stcol[0];
-        learner->setTrainStatsCollector(train_stats);
+    PP<VecStatsCollector> train_stats = stcol[0];
+    learner->setTrainStatsCollector(train_stats);
 
-        // Global stats collector
-        PP<VecStatsCollector> global_statscol;
-        if (global_template_stats_collector)
-        {
-            CopiesMap copies;
-            global_statscol = global_template_stats_collector->deepCopy(copies);
-            global_statscol->build();
-            global_statscol->forget();
-        }
-        else
-            global_statscol = new VecStatsCollector();
+    // Global stats collector
+    PP<VecStatsCollector> global_statscol;
+    if (global_template_stats_collector)
+    {
+        CopiesMap copies;
+        global_statscol = global_template_stats_collector->deepCopy(copies);
+        global_statscol->build();
+        global_statscol->forget();
+    }
+    else
+        global_statscol = new VecStatsCollector();
 
-        // Stat specs
-        TVec<StatSpec> statspecs(nstats);
-        for(int k = 0; k < nstats; k++)
+    // Stat specs
+    TVec<StatSpec> statspecs(nstats);
+    for(int k = 0; k < nstats; k++)
+    {
+        statspecs[k].init(statnames_processed[k]);
+    }
+        
+    // Hack to accumulate statistics over splits. We store in 'acc' the sets
+    // which need to accumulate statistics.
+    TVec<int> acc;
+    for (int k = 0; k < nstats; k++)
+        if (statspecs[k].extstat == "ACC")
         {
-            statspecs[k].init(statnames_processed[k]);
+            if (statspecs[k].setnum == 0)
+                PLERROR("In PTester::perform - For now, you cannot accumulate train stats");
+            if (acc.find(statspecs[k].setnum) == -1)
+                acc.append(statspecs[k].setnum);
         }
-        
-        // Hack to accumulate statistics over splits. We store in 'acc' the sets
-        // which need to accumulate statistics.
-        TVec<int> acc;
-        for (int k = 0; k < nstats; k++)
-            if (statspecs[k].extstat == "ACC")
-            {
-                if (statspecs[k].setnum == 0)
-                    PLERROR("In PTester::perform - For now, you cannot accumulate train stats");
-                if (acc.find(statspecs[k].setnum) == -1)
-                    acc.append(statspecs[k].setnum);
-            }
-            else if (acc.find(statspecs[k].setnum) != -1)
-                PLERROR("In PTester::perform - You can't have stats with and without 'ACC' for set %d", statspecs[k].setnum);
+        else if (acc.find(statspecs[k].setnum) != -1)
+            PLERROR("In PTester::perform - You can't have stats with and without 'ACC' for set %d", statspecs[k].setnum);
 
-        // The vmat in which to save global result stats specified in statnames
-        VMat global_stats_vm;
-        // The vmat in which to save per split result stats
-        VMat split_stats_vm;
+    // The vmat in which to save global result stats specified in statnames
+    VMat global_stats_vm;
+    // The vmat in which to save per split result stats
+    VMat split_stats_vm;
         
-        if (expdir != "" && report_stats)
-        {
-            saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
-            saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
+    if (expdir != "" && report_stats)
+    {
+        saveStringInFile(expdir / "train_cost_names.txt", join(traincostnames, "\n") + "\n");
+        saveStringInFile(expdir / "test_cost_names.txt", join(testcostnames, "\n") + "\n");
 
-            global_stats_vm = new FileVMatrix(expdir/"global_stats.pmat", 1, nstats);
-            for (int k = 0; k < nstats; k++)
-                global_stats_vm->declareField(k, statspecs[k].statName());
-            global_stats_vm->saveFieldInfos();
+        global_stats_vm = new FileVMatrix(expdir / "global_stats.pmat",
+                                          1, nstats);
+        for (int k = 0; k < nstats; k++)
+            global_stats_vm->declareField(k, statspecs[k].statName());
+        global_stats_vm->saveFieldInfos();
 
-            split_stats_vm = new FileVMatrix(expdir / "split_stats.pmat", 0, 1+nstats);
-            split_stats_vm->declareField(0, "splitnum");
-            for (int k = 0; k < nstats; k++)
-                split_stats_vm->declareField(k+1, statspecs[k].setname + "." + statspecs[k].intstatname);
-            split_stats_vm->saveFieldInfos();
-        }
+        split_stats_vm = new FileVMatrix(expdir / "split_stats.pmat",
+                                         0, 1 + nstats);
+        split_stats_vm->declareField(0, "splitnum");
+        for (int k = 0; k < nstats; k++)
+            split_stats_vm->declareField(k+1, statspecs[k].setname + "." + statspecs[k].intstatname);
+        split_stats_vm->saveFieldInfos();
+    }
 
-        for (int splitnum = 0; splitnum < nsplits; splitnum++)
+    for (int splitnum = 0; splitnum < nsplits; splitnum++)
+    {
+        PPath splitdir;
+        bool is_splitdir = false;
+        if (!expdir.isEmpty())
         {
-            PPath splitdir;
-            bool is_splitdir = false;
-            if(!expdir.isEmpty())
-            {
-                splitdir = expdir / ("Split" + tostring(splitnum));
-                is_splitdir = true;
-            }
+            splitdir = expdir / ("Split" + tostring(splitnum));
+            is_splitdir = true;
+        }
 
-            TVec<VMat> dsets = splitter->getSplit(splitnum);
+        TVec<VMat> dsets = splitter->getSplit(splitnum);
+
+        if (should_train) {
             VMat trainset = dsets[0];
             if (is_splitdir && save_data_sets)
                 PLearn::save(splitdir / "training_set.psave", trainset);
-
-            if (should_train && provide_learner_expdir)
+            
+            if (provide_learner_expdir)
             {
                 if (is_splitdir)
                     learner->setExperimentDirectory(splitdir / "LearnerExpdir/");
@@ -547,174 +550,177 @@
             if (dsets.size() > 1)
                 learner->setValidationSet(dsets[1]);
 
-            const int outputsize = learner->outputsize();
+            if (is_splitdir && save_initial_learners)
+                PLearn::save(splitdir / "initial_learner.psave", learner);
 
-            if (should_train)
+            train_stats->forget();
+            learner->train();
+            train_stats->finalize();
+
+            if (is_splitdir)
             {
-                if (is_splitdir && save_initial_learners)
-                    PLearn::save(splitdir / "initial_learner.psave", learner);
+                if (save_stat_collectors)
+                    PLearn::save(splitdir / "train_stats.psave", train_stats);
+                if (save_learners)
+                    PLearn::save(splitdir / "final_learner.psave", learner);
+            }
+        }
+        else
+            learner->build();
 
-                train_stats->forget();
-                learner->train();
-                train_stats->finalize();
+        // This needs to be after the SetTrainingSet() / build() call to the
+        // learner.
+        const int outputsize = learner->outputsize();
 
+        // perf_eval_costs[setnum][perf_evaluator_name][costname] will contain value
+        // of the given cost returned by the given perf_evaluator on the given setnum
+        TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
+
+        // Perform the test if required
+        if (should_test)
+        {
+            for (int setnum = 1; setnum < dsets.length(); setnum++)
+            {
+                VMat testset = dsets[setnum];
+                VMat test_outputs;
+                VMat test_costs;
+                VMat test_confidence;
+
+                PP<VecStatsCollector> test_stats = stcol[setnum];
+                const string setname = "test" + tostring(setnum);
+                if (is_splitdir && save_data_sets)
+                    PLearn::save(splitdir / (setname + "_set.psave"), testset);
+
+                // QUESTION Why is this done so late? Can't it be moved
+                // somewhere earlier? At least before the save_data_sets?
                 if (is_splitdir)
+                    force_mkdir(splitdir);
+
+                if (is_splitdir && save_test_outputs)
+                    test_outputs = new FileVMatrix(splitdir / (setname + "_outputs.pmat"),
+                                                   0, learner->getOutputNames());
+                else if (!perf_evaluators.empty())
                 {
-                    if (save_stat_collectors)
-                        PLearn::save(splitdir / "train_stats.psave", train_stats);
-                    if (save_learners)
-                        PLearn::save(splitdir / "final_learner.psave", learner);
+                    // We don't want to save test outputs to disk, but we
+                    // need them for pef_evaluators. So let's store them in
+                    // a MemoryVMatrix
+                    Mat data(testset.length(), outputsize);
+                    data.resize(0, outputsize);
+                    test_outputs = new MemoryVMatrix(data);
+                    test_outputs->declareFieldNames(learner->getOutputNames());
                 }
-            }
-            else
-                learner->build();
 
-            // perf_eval_costs[setnum][perf_evaluator_name][costname] will contain value
-            // of the given cost returned by the given perf_evaluator on the given setnum
-            TVec< map<string, map<string, real> > > perf_eval_costs(dsets.length());
-
-            // Perform the test if required
-            if (should_test)
-            {
-                for (int setnum = 1; setnum < dsets.length(); setnum++)
+                if (is_splitdir)
                 {
-                    VMat testset = dsets[setnum];
-                    PP<VecStatsCollector> test_stats = stcol[setnum];
-                    const string setname = "test" + tostring(setnum);
-                    if (is_splitdir && save_data_sets)
-                        PLearn::save(splitdir / (setname + "_set.psave"), testset);
-                    VMat test_outputs;
-                    VMat test_costs;
-                    VMat test_confidence;
-                    if (is_splitdir)
-                        force_mkdir(splitdir); // TODO Why is this done so late?
+                    if (save_test_costs)
+                        test_costs = new FileVMatrix(splitdir / (setname + "_costs.pmat"),
+                                                     0, learner->getTestCostNames());
+                    if (save_test_confidence)
+                        test_confidence = new FileVMatrix(splitdir / (setname + "_confidence.pmat"),
+                                                          0, 2 * outputsize);
+                }
 
-                    if (is_splitdir && save_test_outputs)
-                        test_outputs = new FileVMatrix(splitdir / (setname + "_outputs.pmat"),
-                                                       0, learner->getOutputNames());
-                    else if (!perf_evaluators.empty())
-                    {
-                        // We don't want to save test outputs to disk, but we
-                        // need them for pef_evaluators. So let's store them in
-                        // a MemoryVMatrix
-                        Mat data(testset.length(),outputsize);
-                        data.resize(0,outputsize);
-                        test_outputs = new MemoryVMatrix(data);
-                        test_outputs->declareFieldNames(learner->getOutputNames());
-                    }
-
-                    if (is_splitdir)
-                    {
-                        if (save_test_costs)
-                            test_costs = new FileVMatrix(splitdir / (setname + "_costs.pmat"),
-                                                         0, learner->getTestCostNames());
-                        if (save_test_confidence)
-                            test_confidence = new FileVMatrix(splitdir / (setname + "_confidence.pmat"),
-                                                              0, 2 * outputsize);
-                    }
-
-                    const bool reset_stats = (acc.find(setnum) == -1);
-                    if (reset_stats)
-                        test_stats->forget();
+                const bool reset_stats = (acc.find(setnum) == -1);
+                if (reset_stats)
+                    test_stats->forget();
                     
-                    if (testset->length() == 0)
-                        PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",
-                                  setname.c_str());
+                if (testset->length() == 0)
+                    PLWARNING("PTester:: test set %s is of length 0, costs will be set to -1",
+                              setname.c_str());
 
-                    // Before each test set, reset the internal state of the learner
-                    learner->resetInternalState();
+                // Before each test set, reset the internal state of the learner
+                learner->resetInternalState();
 
-                    learner->test(testset, test_stats, test_outputs, test_costs);
-                    if (reset_stats)
-                        test_stats->finalize();
-                    if (is_splitdir && save_stat_collectors)
-                        PLearn::save(splitdir / (setname + "_stats.psave"), test_stats);
+                learner->test(testset, test_stats, test_outputs, test_costs);
+                if (reset_stats)
+                    test_stats->finalize();
+                if (is_splitdir && save_stat_collectors)
+                    PLearn::save(splitdir / (setname + "_stats.psave"), test_stats);
 
-                    perf_evaluators_t::iterator it = perf_evaluators.begin();
-                    const perf_evaluators_t::iterator itend = perf_evaluators.end();
-                    while(it != itend)
-                    {
-                        PPath perf_eval_dir;
-                        if (is_splitdir)
-                            perf_eval_dir = splitdir / setname / ("perfeval_" + it->first);
-                        Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
-                        TVec<string> perf_costnames = it->second->getCostNames();
-                        if (perf_costvals.length()!=perf_costnames.length())
-                            PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
-                        map<string, real>& costmap = perf_eval_costs[setnum][it->first];
-                        for (int costi = 0; costi < perf_costnames.length(); costi++)
-                            costmap[perf_costnames[costi]] = perf_costvals[costi];
-                        ++it;
-                    }
-                    computeConfidence(testset, test_confidence);
+                perf_evaluators_t::iterator it = perf_evaluators.begin();
+                const perf_evaluators_t::iterator itend = perf_evaluators.end();
+                while (it != itend)
+                {
+                    PPath perf_eval_dir;
+                    if (is_splitdir)
+                        perf_eval_dir = splitdir / setname / ("perfeval_" + it->first);
+                    Vec perf_costvals = it->second->evaluatePerformance(learner, testset, test_outputs, perf_eval_dir);
+                    TVec<string> perf_costnames = it->second->getCostNames();
+                    if (perf_costvals.length()!=perf_costnames.length())
+                        PLERROR("vector of costs returned by performance evaluator differ in size with its vector of costnames");
+                    map<string, real>& costmap = perf_eval_costs[setnum][it->first];
+                    for (int costi = 0; costi < perf_costnames.length(); costi++)
+                        costmap[perf_costnames[costi]] = perf_costvals[costi];
+                    ++it;
                 }
+                computeConfidence(testset, test_confidence);
             }
+        }
 
-            Vec splitres(1 + nstats);
-            splitres[0] = splitnum;
+        Vec splitres(1 + nstats);
+        splitres[0] = splitnum;
 
-            for (int k = 0; k < nstats; k++)
+        for (int k = 0; k < nstats; k++)
+        {
+            // If we ask for a test-set that's beyond what's currently
+            // available, OR we are asking for test-statistics in
+            // train-only mode, then the statistic is MISSING_VALUE.
+            StatSpec& sp = statspecs[k];
+            if (sp.setnum>=stcol.length() ||
+                (! should_test && sp.setnum > 0))
             {
-                // If we ask for a test-set that's beyond what's currently
-                // available, OR we are asking for test-statistics in
-                // train-only mode, then the statistic is MISSING_VALUE.
-                StatSpec& sp = statspecs[k];
-                if (sp.setnum>=stcol.length() ||
-                    (! should_test && sp.setnum > 0))
+                splitres[k+1] = MISSING_VALUE;
+            }
+            else
+            {
+                if (acc.find(sp.setnum) == -1)
                 {
-                    splitres[k+1] = MISSING_VALUE;
-                }
-                else
-                {
-                    if (acc.find(sp.setnum) == -1)
+                    string left, right;
+                    split_on_first(sp.intstatname, ".",left,right);
+                    if (right != "" && perf_evaluators.find(left) != perf_evaluators.end())
                     {
-                        string left, right;
-                        split_on_first(sp.intstatname, ".",left,right);
-                        if (right != "" && perf_evaluators.find(left) != perf_evaluators.end())
-                        {
-                            // looks like a cost from a performance evaluator
-                            map<string, real>& costmap = perf_eval_costs[sp.setnum][left];
-                            if (costmap.find(right) == costmap.end())
-                                PLERROR("No cost named %s appears to be returned by evaluator %s",
-                                        right.c_str(), left.c_str());
-                            splitres[k+1] = costmap[right];
-                        }
-                        else
-                            // must be a cost from a stats collector
-                            splitres[k+1] = stcol[sp.setnum]->getStat(sp.intstatname);
+                        // looks like a cost from a performance evaluator
+                        map<string, real>& costmap = perf_eval_costs[sp.setnum][left];
+                        if (costmap.find(right) == costmap.end())
+                            PLERROR("No cost named %s appears to be returned by evaluator %s",
+                                    right.c_str(), left.c_str());
+                        splitres[k+1] = costmap[right];
                     }
                     else
-                        splitres[k+1] = MISSING_VALUE;
+                        // must be a cost from a stats collector
+                        splitres[k+1] = stcol[sp.setnum]->getStat(sp.intstatname);
                 }
+                else
+                    splitres[k+1] = MISSING_VALUE;
             }
+        }
 
-            if (split_stats_vm)
-            {
-                split_stats_vm->appendRow(splitres);
-                split_stats_vm->flush();
-            }
-
-            global_statscol->update(splitres.subVec(1, nstats));
+        if (split_stats_vm)
+        {
+            split_stats_vm->appendRow(splitres);
+            split_stats_vm->flush();
         }
 
+        global_statscol->update(splitres.subVec(1, nstats));
+    }
 
-        global_statscol->finalize();
-        for (int k = 0; k < nstats; k++)
+
+    global_statscol->finalize();
+    for (int k = 0; k < nstats; k++)
+    {
+        if (acc.find(statspecs[k].setnum) == -1)
+            global_result[k] = global_statscol->getStats(k).getStat(statspecs[k].extstat);
+        else
         {
-            if (acc.find(statspecs[k].setnum) == -1)
-                global_result[k] = global_statscol->getStats(k).getStat(statspecs[k].extstat);
-            else
-            {
-                const int j = statspecs[k].setnum;
-                stcol[j]->finalize();
-                global_result[k] = stcol[j]->getStat(statspecs[k].intstatname);
-            }
+            const int j = statspecs[k].setnum;
+            stcol[j]->finalize();
+            global_result[k] = stcol[j]->getStat(statspecs[k].intstatname);
         }
-
-        if (global_stats_vm)
-            global_stats_vm->appendRow(global_result);
     }
 
+    if (global_stats_vm)
+        global_stats_vm->appendRow(global_result);
+
 #if USING_MPI
     if (PLMPI::rank == 0)
 #endif



From ducharme at mail.berlios.de  Thu Jan 25 15:48:09 2007
From: ducharme at mail.berlios.de (ducharme at BerliOS)
Date: Thu, 25 Jan 2007 15:48:09 +0100
Subject: [Plearn-commits] r6614 - tags
Message-ID: <200701251448.l0PEm9JD001849@sheep.berlios.de>

Author: ducharme
Date: 2007-01-25 15:48:08 +0100 (Thu, 25 Jan 2007)
New Revision: 6614

Added:
   tags/OPAL-3.1/
Log:
Tag pour release OPAL 3.1

Copied: tags/OPAL-3.1 (from rev 6613, trunk)



From chrish at mail.berlios.de  Thu Jan 25 16:57:53 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 25 Jan 2007 16:57:53 +0100
Subject: [Plearn-commits] r6615 - trunk/python_modules/plearn/pyplearn
Message-ID: <200701251557.l0PFvrOB008950@sheep.berlios.de>

Author: chrish
Date: 2007-01-25 16:57:53 +0100 (Thu, 25 Jan 2007)
New Revision: 6615

Modified:
   trunk/python_modules/plearn/pyplearn/plargs.py
Log:
Don't complain if passing the same pyplearn command-line option twice with
different values. Just use the last value, like every other command-line parser
doesn.


Modified: trunk/python_modules/plearn/pyplearn/plargs.py
===================================================================
--- trunk/python_modules/plearn/pyplearn/plargs.py	2007-01-25 14:48:08 UTC (rev 6614)
+++ trunk/python_modules/plearn/pyplearn/plargs.py	2007-01-25 15:57:53 UTC (rev 6615)
@@ -444,10 +444,6 @@
     def addCmdLineOverride(holder_name, optname, value):
         context = actualContext(plopt)
         hldr_key = (holder_name, optname)
-        if hldr_key in context.plopt_cmdline_overrides:
-            from plearn.pyplearn import PyPLearnError
-            raise PyPLearnError("Duplicate command-line value for option '%s'"%optname)
-
         # Use the 'unique' key to store command-line override's value
         context.plopt_cmdline_overrides[hldr_key] = value
     addCmdLineOverride = staticmethod(addCmdLineOverride)



From chrish at mail.berlios.de  Thu Jan 25 17:15:00 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 25 Jan 2007 17:15:00 +0100
Subject: [Plearn-commits] r6616 - tags
Message-ID: <200701251615.l0PGF0Dl010368@sheep.berlios.de>

Author: chrish
Date: 2007-01-25 17:15:00 +0100 (Thu, 25 Jan 2007)
New Revision: 6616

Added:
   tags/exlearn-0.4.0/
Log:
Tag pour release exlearn 0.4.0

Copied: tags/exlearn-0.4.0 (from rev 6615, trunk)



From chrish at mail.berlios.de  Thu Jan 25 18:15:58 2007
From: chrish at mail.berlios.de (chrish at BerliOS)
Date: Thu, 25 Jan 2007 18:15:58 +0100
Subject: [Plearn-commits] r6617 - tags/exlearn-0.4.0
Message-ID: <200701251715.l0PHFwAh015496@sheep.berlios.de>

Author: chrish
Date: 2007-01-25 18:15:58 +0100 (Thu, 25 Jan 2007)
New Revision: 6617

Added:
   tags/exlearn-0.4.0/PLearn/
Log:
Tag pour release exlearn 0.4.0

Copied: tags/exlearn-0.4.0/PLearn (from rev 6616, trunk)



From chapados at mail.berlios.de  Fri Jan 26 15:26:12 2007
From: chapados at mail.berlios.de (chapados at BerliOS)
Date: Fri, 26 Jan 2007 15:26:12 +0100
Subject: [Plearn-commits] r6618 - trunk/python_modules/plearn/math
Message-ID: <200701261426.l0QEQCZO029883@sheep.berlios.de>

Author: chapados
Date: 2007-01-26 15:26:12 +0100 (Fri, 26 Jan 2007)
New Revision: 6618

Added:
   trunk/python_modules/plearn/math/ltqnorm.py
Log:
Added function to compute quantile of standard normal distribution that does not rely on anything else than standard python "math" module

Added: trunk/python_modules/plearn/math/ltqnorm.py
===================================================================
--- trunk/python_modules/plearn/math/ltqnorm.py	2007-01-25 17:15:58 UTC (rev 6617)
+++ trunk/python_modules/plearn/math/ltqnorm.py	2007-01-26 14:26:12 UTC (rev 6618)
@@ -0,0 +1,62 @@
+import math
+
+def ltqnorm( p ):
+    """
+    Lower tail quantile for standard normal distribution function.
+
+    Modified from the author's original perl code (original comments follow below)
+    by dfield at yahoo-inc.com.  May 3, 2004.
+
+    This function returns an approximation of the inverse cumulative
+    standard normal distribution function.  I.e., given P, it returns
+    an approximation to the X satisfying P = Pr{Z <= X} where Z is a
+    random variable from the standard normal distribution.
+
+    The algorithm uses a minimax approximation by rational functions
+    and the result has a relative error whose absolute value is less
+    than 1.15e-9.
+
+    Author:      Peter J. Acklam
+    Time-stamp:  2000-07-19 18:26:14
+    E-mail:      pjacklam at online.no
+    WWW URL:     http://home.online.no/~pjacklam
+    """
+
+    if p <= 0 or p >= 1:
+        # The original perl code exits here, we'll throw an exception instead
+        raise ValueError( "Argument to ltqnorm %f must be in open interval (0,1)" % p )
+
+    # Coefficients in rational approximations.
+    a = (-3.969683028665376e+01,  2.209460984245205e+02, \
+         -2.759285104469687e+02,  1.383577518672690e+02, \
+         -3.066479806614716e+01,  2.506628277459239e+00)
+    b = (-5.447609879822406e+01,  1.615858368580409e+02, \
+         -1.556989798598866e+02,  6.680131188771972e+01, \
+         -1.328068155288572e+01 )
+    c = (-7.784894002430293e-03, -3.223964580411365e-01, \
+         -2.400758277161838e+00, -2.549732539343734e+00, \
+          4.374664141464968e+00,  2.938163982698783e+00)
+    d = ( 7.784695709041462e-03,  3.224671290700398e-01, \
+          2.445134137142996e+00,  3.754408661907416e+00)
+
+    # Define break-points.
+    plow  = 0.02425
+    phigh = 1 - plow
+
+    # Rational approximation for lower region:
+    if p < plow:
+       q  = math.sqrt(-2*math.log(p))
+       return (((((c[0]*q+c[1])*q+c[2])*q+c[3])*q+c[4])*q+c[5]) / \
+               ((((d[0]*q+d[1])*q+d[2])*q+d[3])*q+1)
+
+    # Rational approximation for upper region:
+    if phigh < p:
+       q  = math.sqrt(-2*math.log(1-p))
+       return -(((((c[0]*q+c[1])*q+c[2])*q+c[3])*q+c[4])*q+c[5]) / \
+                ((((d[0]*q+d[1])*q+d[2])*q+d[3])*q+1)
+
+    # Rational approximation for central region:
+    q = p - 0.5
+    r = q*q
+    return (((((a[0]*r+a[1])*r+a[2])*r+a[3])*r+a[4])*r+a[5])*q / \
+           (((((b[0]*r+b[1])*r+b[2])*r+b[3])*r+b[4])*r+1)



